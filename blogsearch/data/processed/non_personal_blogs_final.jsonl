{"title": "Software Engineer, Gameplay", "url": "https://www.metacareers.com/jobs/1711708019551066/", "content": "Reality Labs at Meta is building products that make it easier for people to connect with the ones they love most, enjoy top-notch, wire-free VR, and push the future of computing platforms. We are a team of high-quality industry experts developing and shipping products at the intersection of hardware, software and content. As a Gameplay Engineer on the Reality Labs team at Meta, you can help build new, innovative hardware and software that radically redefine the way people work, play and connect. What we build today could one day be the norm. So to be here today is to truly be at the heart of change and the frontier of what's to come. We're the people helping to define the metaverse. We may not have all the answers. But together, we're getting closer.\n\nSoftware Engineer, Gameplay Responsibilities Architect efficient and reusable systems that drive complex Augmented Reality/Virtual Reality (AR/VR) applications\n\nPrototyping new interactions and features with an eye toward intuitive usability and feel\n\nDrive design and implementation for proposed playful and creative projects for Experiences in AR or VR\n\nLead collaboration with multi-functional teams to achieve compelling experiences\n\nEngage with Facebook teams to understand and utilize FB graph and features that will power social user experience (UX)\n\nMinimum Qualifications Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience\n\n5+ years experience as an engineer shipping user-facing features on games or other 3D interactive products or PhD degree +2 years of experience\n\nDemonstrated experience interfacing with other internal and external teams to incorporate their innovations and vice versa\n\nPreferred Qualifications Experience with real time networked features (i.e., synchronous multiplayer games)\n\nExperience in UX design or game design\n\nDemonstrate experience understanding of 3D math, linear algebra, and techniques\n\nExperience with augmented reality (HoloLens, Magic Leap, ARKit, etc.)\n\nExperience with a combination of Windows, Android, and game console programming\n\nExperience with virtual reality (Rift, Quest, VIVE, Gear VR, etc.)\n\nExperience with Unity 3D or Unreal\n\nExperience with C++ and/or C# programming\n\nFor those who live in or expect to work from California if hired for this position, please click here for additional information.", "label": "non_personal"}
{"title": "Copilot for all: Introducing Microsoft 365 Copilot Chat", "url": "https://www.microsoft.com/en-us/microsoft-365/blog/2025/01/15/copilot-for-all-introducing-microsoft-365-copilot-chat/", "content": "Our ambition is to empower every employee with a Copilot and to transform every business process with agents. From Dow to Disney, companies are going big with Microsoft 365 Copilot and agents, uncovering key scenarios that can deliver real ROI. Now, organizations of all sizes are looking to scale their AI transformation and realize the enterprise-wide ROI that comes with broad adoption.\n\nToday, we’re introducing Microsoft 365 Copilot Chat, a new offering that adds pay-as-you-go agents to our existing free chat experience for Microsoft 365 commercial customers.1 Copilot Chat enables your entire workforce—from customer service representatives to marketing leads to frontline technicians—to start using Copilot and agents today. It includes:\n\nFree, secure AI chat powered by GPT-4o.\n\npowered by GPT-4o. Agents accessible right in the chat.\n\naccessible right in the chat. IT controls, including enterprise data protection and agent management.\n\nMoving forward, every organization will have a mix of Copilot Chat and Microsoft 365 Copilot—our best-in-class offering—to drive AI transformation at scale. Let’s walk through the new product lineup.\n\nClick to enlarge\n\nCopilot Chat: The power of chat + agents\n\nCopilot is the UI for AI, and it all starts with Copilot Chat. It’s the chat experience you’ll use every day—powered by broad knowledge from the web, built on GPT-4o, and designed to be safe and secure for business use. It represents a foundational shift in how we work, enabling everyone to work smarter, faster, and more collaboratively.\n\nCopilot Chat includes:\n\nWeb-grounded chat with GPT-4o. You can use it to do market research, write a strategy document, or prepare for a meeting. File uploads allow you to add any document to the chat and ask Copilot to do things like summarize key points in a Word document, analyze data in an Excel spreadsheet, and suggest improvements to a PowerPoint presentation. 2 With Copilot Pages , you can collaborate on content with people and AI in real time—adding content from Copilot, your files, and now from the web as well. And you can quickly create AI-generated images for campaigns, product launches, and social media posts. 3\n\nwith GPT-4o. You can use it to do market research, write a strategy document, or prepare for a meeting. allow you to add any document to the chat and ask Copilot to do things like summarize key points in a Word document, analyze data in an Excel spreadsheet, and suggest improvements to a PowerPoint presentation. With , you can collaborate on content with people and AI in real time—adding content from Copilot, your files, and now from the web as well. And you can quickly create for campaigns, product launches, and social media posts. Agents . Using natural language, now anyone can easily create agents to automate repetitive tasks and business processes—directly in Copilot Chat. A customer service representative can ask a customer relationship management (CRM) agent for account details before a customer meeting, while field service agents can access step-by-step instructions and real-time product knowledge stored in SharePoint. Agents are priced on a metered basis, and IT stays in control. IT admins can also build organization-wide agents and manage agent deployment, all powered by Microsoft Copilot Studio.\n\n. Using natural language, now anyone can easily create agents to automate repetitive tasks and business processes—directly in Copilot Chat. A customer service representative can ask a customer relationship management (CRM) agent for account details before a customer meeting, while field service agents can access step-by-step instructions and real-time product knowledge stored in SharePoint. Agents are priced on a metered basis, and IT stays in control. IT admins can also build organization-wide agents and manage agent deployment, all powered by Microsoft Copilot Studio. Copilot Control System. Copilot Chat includes foundational capabilities of the Copilot Control System, including enterprise data protection (EDP) for data privacy and security and the ability to govern access and manage the usage and lifecycle of Copilot and agents, as well as measurement and reporting.\n\nWhile Copilot Chat is a powerful new on-ramp for everyone in your organization to build the AI habit, Microsoft 365 Copilot remains our best-in-class personal AI assistant for work. It includes everything in Copilot Chat and more. Microsoft 365 Copilot combines the power of GPT-4o grounded in your work data—all your meetings, emails, chats, documents, and more; Copilot in the Microsoft 365 apps that millions rely on every day—Microsoft Teams, Outlook, Word, Excel, and PowerPoint; and usage and access to agents. And we continue to rapidly add new capabilities like Copilot Actions to tackle people’s biggest pain points at work. We’ve empowered every IT team to lead and manage at scale with the Copilot Control System and Copilot Analytics to measure the impact and ROI of your Copilot investment.\n\nCustomers can get started with either the free or paid experience in the Microsoft 365 Copilot app, available at m365copilot.com or in the Windows, Android, or iPhone app stores.\n\nThese announcements enable every customer to accelerate their AI transformation and realize enterprise-wide ROI. Now, every employee has a Copilot and a team of agents to scale their impact.\n\nTry Copilot Chat at m365copilot.com and visit WorkLab for the latest research and insights on AI and the future of work.\n\nDownload the Microsoft 365 Copilot app Take the power of AI on the go with the Copilot app. Download today\n\nFootnotes:\n\n1 Learn more about Copilot Chat eligibility.\n\n2 File upload limits apply\n\n3 Image generation limits apply", "label": "non_personal"}
{"title": "Introducing Gemma 3n: The developer guide", "url": "https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/", "content": "The first Gemma model launched early last year and has since grown into a thriving Gemmaverse of over 160 million collective downloads. This ecosystem includes our family of over a dozen specialized models for everything from safeguarding to medical applications and, most inspiringly, the countless innovations from the community. From innovators like Roboflow building enterprise computer vision to the Institute of Science Tokyo creating highly-capable Japanese Gemma variants, your work has shown us the path forward. Building on this incredible momentum, we're excited to announce the full release of Gemma 3n. While last month's preview offered a glimpse, today unlocks the full power of this mobile-first architecture. Gemma 3n is designed for the developer community that helped shape Gemma. It’s supported by your favorite tools including Hugging Face Transformers, llama.cpp, Google AI Edge, Ollama, MLX, and many others, enabling you to fine-tune and deploy for your specific on-device applications with ease. This post is the developer deep dive: we'll explore some of the innovations behind Gemma 3n, share new benchmark results, and show you how to start building today.\n\nWhat’s new in Gemma 3n? Gemma 3n represents a major advancement for on-device AI, bringing powerful multimodal capabilities to edge devices with performance previously only seen in last year's cloud-based frontier models.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nMultimodal by design: Gemma 3n natively supports image, audio, video, and text inputs and text outputs. Optimized for on-device: Engineered with a focus on efficiency, Gemma 3n models are available in two sizes based on effective parameters: E2B and E4B. While their raw parameter count is 5B and 8B respectively, architectural innovations allow them to run with a memory footprint comparable to traditional 2B and 4B models, operating with as little as 2GB (E2B) and 3GB (E4B) of memory. Groundbreaking architecture: At its core, Gemma 3n features novel components like the MatFormer architecture for compute flexibility, Per Layer Embeddings (PLE) for memory efficiency, LAuReL and AltUp for architectural efficiency, and new audio and MobileNet-v5 based vision encoders optimized for on-device use cases. Enhanced quality: Gemma 3n delivers quality improvements across multilinguality (supporting 140 languages for text and multimodal understanding of 35 languages), math, coding, and reasoning. The E4B version achieves an LMArena score over 1300, making it the first model under 10 billion parameters to reach this benchmark.\n\nAchieving this leap in on-device performance required rethinking the model from the ground up. The foundation is Gemma 3n’s unique mobile-first architecture, and it all starts with MatFormer.\n\nMatFormer: One model, many sizes At the core of Gemma 3n is the MatFormer (🪆Matryoshka Transformer) architecture, a novel nested transformer built for elastic inference. Think of it like Matryoshka dolls: a larger model contains smaller, fully functional versions of itself. This approach extends the concept of Matryoshka Representation Learning from just embeddings to all transformer components.\n\nDuring the MatFormer training of the 4B effective parameter (E4B) model, a 2B effective parameter (E2B) sub-model is simultaneously optimized within it, as shown in the figure above. This provides developers two powerful capabilities and use cases today: 1: Pre-extracted models: You can directly download and use either the main E4B model for the highest capabilities, or the standalone E2B sub-model which we have already extracted for you, offering up to 2x faster inference. 2: Custom sizes with Mix-n-Match: For more granular control tailored to specific hardware constraints, you can create a spectrum of custom-sized models between E2B and E4B using a method we call Mix-n-Match. This technique allows you to precisely slice the E4B model's parameters, primarily by adjusting the feed forward network hidden dimension per layer (from 8192 to 16384) and selectively skipping some layers. We are releasing the MatFormer Lab, a tool that shows how to retrieve these optimal models, which were identified by evaluating various settings on benchmarks like MMLU.\n\nMMLU scores for the pre-trained Gemma 3n checkpoints at different model sizes (using Mix-n-Match)\n\nLooking ahead, the MatFormer architecture also paves the way for elastic execution. While not part of today’s launched implementations, this capability allows a single deployed E4B model to dynamically switch between E4B and E2B inference paths on the fly, enabling real-time optimization of performance and memory usage based on the current task and device load.\n\nPer-Layer Embeddings (PLE): Unlocking more memory efficiency Gemma 3n models incorporate Per-Layer Embeddings (PLE). This innovation is tailored for on-device deployment as it dramatically improves model quality without increasing the high-speed memory footprint required on your device's accelerator (GPU/TPU). While the Gemma 3n E2B and E4B models have a total parameter count of 5B and 8B respectively, PLE allows a significant portion of these parameters (the embeddings associated with each layer) to be loaded and computed efficiently on the CPU. This means only the core transformer weights (approximately 2B for E2B and 4B for E4B) need to sit in the typically more constrained accelerator memory (VRAM).\n\nWith Per-Layer Embeddings, you can use Gemma 3n E2B while only having ~2B parameters loaded in your accelerator.\n\nKV Cache sharing: Faster long-context processing Processing long inputs, such as the sequences derived from audio and video streams, is essential for many advanced on-device multimodal applications. Gemma 3n introduces KV Cache Sharing, a feature designed to significantly accelerate time-to-first-token for streaming response applications. KV Cache Sharing optimizes how the model handles the initial input processing stage (often called the \"prefill\" phase). The keys and values of the middle layer from local and global attention are directly shared with all the top layers, delivering a notable 2x improvement on prefill performance compared to Gemma 3 4B. This means the model can ingest and understand lengthy prompt sequences much faster than before.\n\nAudio understanding: Introducing speech to text and translation Gemma 3n uses an advanced audio encoder based on the Universal Speech Model (USM). The encoder generates a token for every 160ms of audio (about 6 tokens per second), which are then integrated as input to the language model, providing a granular representation of the sound context. This integrated audio capability unlocks key features for on-device development, including: Automatic Speech Recognition (ASR): Enable high-quality speech-to-text transcription directly on the device. Automatic Speech Translation (AST): Translate spoken language into text in another language. We've observed particularly strong AST results for translation between English and Spanish, French, Italian, and Portuguese, offering great potential for developers targeting applications in these languages. For tasks like speech translation, leveraging Chain-of-Thought prompting can significantly enhance results. Here’s an example:", "label": "non_personal"}
{"title": "Engineering at Meta", "url": "https://engineering.fb.com/feed/", "content": "In this post, we explore how Instagram has successfully scaled its algorithm to include over 1000 ML models without sacrificing recommendation quality or reliability.\n\nWe delve into the intricacies of managing such a vast array of models, each with its own performance characteristics and product goals.\n\nWe share insights and lessons learned along the way—from the initial realization that our infrastructure maturity was lagging behind our ambitious scaling goals, to the innovative solutions we implemented to bridge these gaps.\n\nIn the ever-evolving landscape of social media, Instagram serves as a hub for creative expression and connection, continually adapting to meet the dynamic needs of its global community. At the heart of this adaptability lies a web of machine learning (ML) models, each playing a crucial role in personalizing experiences. As Instagram’s reach and influence has grown, so too has the complexity of its algorithmic infrastructure. This growth, while exciting, presents a unique set of challenges, particularly in terms of reliability and scalability.\n\nJoin us as we uncover the strategies and tools that have enabled Instagram to maintain its position at the forefront of social media innovation, ensuring a seamless and engaging experience for billions of users worldwide.\n\nAre there really that many ML models in Instagram?\n\nThough what shows up in Feed, Stories, and Reels is personally ranked, the number of ranked surfaces goes much deeper—to which comments surface in Feed, which notifications are “important,” or whom you might tag in a post. These are all driven by ML recommendations.\n\nWithin a given surface, we’ll have different layers of the ranking funnel: sourcing (retrieval), early-stage ranking (ESR), and late-stage ranking (LSR). We operate on fewer candidates as we progress through the funnel, as the underlying operations grow more expensive (see Figure 1 below):\n\nWithin each surface and layer, there is constant experimentation, and these permutations create a severe infrastructure challenge. We need to allow room for our ML engineers to experiment with changes such as adjusting weights for a given prediction. The net result, depicted below in Figure 2, is a large number of models serving user traffic in production:\n\nHow did we realize infra maturity wasn’t going to catch up?\n\nIdentified risks\n\nWe identified several risks associated with scaling our algorithm, rooted in complaints about ML productivity and repeating patterns of issues:\n\nDiscovery: Even as a team focused on one app — Instagram — we couldn’t stay on top of the growth, and product ML teams were maintaining separate sources of truth, if any, for their models in production.\n\nRelease: We didn’t have a consistent way to launch new models safely, and the process was slow, impacting ML velocity and, therefore, product innovation.\n\nHealth: We lacked a consistent definition of model prediction quality, and with the diversity of surfaces and subtlety of degraded ranking, quality issues went unnoticed.\n\nSolution overview\n\nTo address these risks, we implemented several solutions:\n\nModel registry: We built a registry that serves as a ledger for production model importance and business function foremost, among other metadata. This registry serves as our foundational source of truth, upon which we can leverage automation to uplevel system-wide observability, change management, and model health.\n\nModel launch tooling: We developed a more ideal flow for launching new models that includes estimation, approval, prep, scale-up, and finalization. This process is now automated, and we’ve reduced the time it takes to launch a new model from days to hours.\n\nModel stability: We defined and operationalized model stability, a pioneering metric that measures the accuracy of our model predictions. We’ve leveraged model stability to produce SLOs for all models in the model registry, which enables simple understanding of the entire product surface’s ML health.\n\nModel registry\n\nWhat did model investigations look like prior to the registry?\n\nBefore we created the model registry, the investigation process was a time-consuming and error-prone experience for on-call engineers and model owners. An on-call engineer had to ask multiple questions to model owners to gather information, as depicted Figure 3 below, about the context of what this model does in the stack and to clarify how important it is to the business.\n\nUnderstanding this context is extremely important to the operational response: Depending on the importance of the model and the criticality of the surface it’s supporting, the response is going to differ in kind. When a model is an experiment serving a small percentage of the traffic, an appropriate response can be to end the experiment and reroute the traffic back to the main model (the baseline). But if there’s a problem with the baseline model that needs to be handled with urgency, it’s not possible to “just turn it off.” The engineer on call has to loop in the model owner, defeating the purpose of having a dedicated on-call.\n\nTo avoid holding up an operational response on a single POC, we needed a central source of truth for model importance and business function. What if the model is not available? What if 10 of these issues happen concurrently?\n\nWith the development of the model registry, we standardized the collection of model importance and business function information, ensuring most of our operational resources were going towards the most important models.\n\nWhat problems did the model registry solve?\n\nThe model registry is a system of record built on top of Configerator, Meta’s distributed configuration suite . This schematized ledger (see an example in Figure 4 and detailed further below) provides read-and-write access to operational data based on the inventory of production models. It’s a flexible and extensible foundation upon which one can build automation and tools to solve problems that are specific to individual organizations within Meta that are not served by the general tooling.\n\nAs Instagram scaled its investment in AI through rapid innovation in content recommendations, the number of models and AI assets grew; as a result, it has been increasingly important — but also increasingly difficult — to maintain a minimum standard for all of our models, as we lacked an authoritative source for the business context as well as for a model’s importance.\n\nIn creating the model registry, we set out to provide a structured interface for collecting business context via model types, importance via criticality, and additional metadata that would enable model understanding. Below, we’ll get into the model types, criticality, and automation we’ve built for this purpose.\n\nModel types\n\nAt a high level, model type describes the purpose for the ML workload where it represents a category or class of models that share a common purpose or are used in similar contexts. For example, we have “ig_stories_tray_mtml” which is a string attached to training flows, model checkpoints, inference services, and more. Put simply, a model type identifies for the reader this model’s purpose in the ranking funnel.\n\nLet’s break it down:\n\n“ig_stories_tray_mtml” → “ig” “stories” “tray” “mtml”\n\n“ ig ”: This model is an “ig” model as opposed to “fb” or “whatsapp”.\n\n“ stories ”: This model serves IG Stories.\n\n“ tray ”: This model serves in the main IG Stories tray (as opposed to stories in some other surface).\n\n“mtml”: This model is a multi-task-multi-label model, commonly used in late-stage ranking.\n\nWe can then use these model type strings to tag AI assets, and since they serve as proxies for business context, we can use them also for asset management, policy enforcement, analytics, and more.\n\nThe metadata entries in the model registry are anchored on two main types that describe model instances (ModelMetadata) as well as model types (ModelTypeMetadata). These types are made up of “core” attributes that are universally applicable, as well as “extended” attributes that allow different teams to encode their opinions about how these entries will inform operations. For example, in Instagram our extended attributes encode “baseline” and “holdout” model IDs, which are used in our ranking infrastructure to orchestrate ranking funnel execution.\n\nCriticality\n\nIn addition to defining business function, we had to establish clear guidelines for model importance. Within Meta, SEVs and services have a unified-importance tier system where the Global Service Index (GSI) records a criticality from TIER0 to TIER4 based on the maximum incident severity level the service can cause, from SEV0 as the most critical to SEV4 as simply a “heads up.” Since GSI criticality had social proof at the company, and infra engineers were familiar with this system, we adopted these criticalities for models and now annotate them at the model type and model level.\n\nNo longer would each team decide to raise their own model services to TIER1 for themselves, increasing the burden on all teams that support these models. Teams needed to provide an immediate response (available 24/7) on call and be able to prove that their models contributed meaningfully to critical business metrics to qualify for elevated monitoring.\n\nConfiguration structure as a foundation for automation\n\nOnce we had onboarded a critical mass of Instagram models to the model registry, we could begin to fully integrate with our monitoring and observability suite using our Meta-wide configuration solution, Configerator. With this, we could now have model performance monitoring and alerts that are fully automated and integrated with our tooling for SLIs called SLICK, dashboards that allow us to monitor models across many time series dimensions, and a suite of alerting specific to the model that is driven from the entries in the model registry.\n\nThis provided all our teams confidence that our monitoring coverage was complete and automated.\n\nLaunching\n\nWhile a point-in-time snapshot of models in production is great for static systems, Instagram’s ML landscape is constantly shifting. With the rapid increase of iteration on the recommendation system driving an increased number of launches, it became clear our infrastructure support to make this happen was not adequate. Time-to-launch was a bottleneck in ML velocity, and we needed to drive it down.\n\nWhat did the process look like?\n\nConventionally, services were longstanding systems that had engineers supporting them to tune. Even when new changes would introduce new capacity regression risks, we could gate this behind change safety mechanisms.\n\nHowever, our modeling and experimentation structure was unique in that we were planning for more rapid iteration, and our options were insufficient. To safely test the extent of load a new service could support, we would clone the entire service, send shadow traffic (i.e., cloned traffic that isn’t processed by our clients), and run multiple overload tests until we found a consistent peak throughput. But this wasn’t a perfect science. Sometimes we didn’t send enough traffic, and sometimes we’d send too much, and the amount could change throughout the day due to variations in global user behavior.\n\nThis could easily take two days to get right, including actually debugging the performance itself when the results weren’t expected. Once we got the result, we’d then have to estimate the final cost. Below (in Figure 5) is the formula we landed on.\n\nThe actual traffic shifting portion was tedious as well. For example, when we managed to fully estimate that we needed 500 replicas to host the new service, we might not actually have 500 spares lying around to do a full replacement, so launching was a delicate process of partially sizing up by approximately 20%, sending 20% of traffic over, and then scaling down the old service by 20% to reclaim and recycle the capacity. Rinse, repeat. Inefficient!\n\nAnd by the time we got to the end of this arduous process, the ordeal still wasn’t over. Each team was responsible for correctly setting up new alerts for their baseline in a timely fashion, or else their old models could and did trigger false alarms.\n\nHow does forcing virtual pools aid product growth?\n\nOne of the prerequisites for fixing competition for resources and unblocking productivity was to put up guardrails. Prior to this, it was “first come first served,” with no clear way to even “reserve” future freed capacity. It was also hard to reason about fairness from an infra perspective: Would it make sense to give each team equal pools, or give each individual person a maximum limit?\n\nAs it turned out, not all MLEs are experimenting at the same time, due to staggered progress on their work, so individual (per-engineer) limits were not ideal. One member might be in the experimentation stage and another might be training. So our solution was to provide bandwidth to each team.\n\nOnce each team — and therefore product — had quotas distributed, their launch policy became more clear cut. Some teams established free launching as long as the team was within quota. Others required no regressions in capacity usage. But mostly this unlocked our ability to run launches in parallel, since each one required much less red tape, and prioritization was no longer done at the org level.\n\nWhat other tooling improved launching?\n\nAs mentioned earlier, preplanning with capacity estimations was critical to understanding cost and ensuring reliability. We were often asked, Why not let autoscaling take care of everything? The problem was that each service could be configured slightly differently than a previously optimized service, or some architectural change could have affected the performance of the model. We didn’t have an infinite amount of supply to work with, so by the time we fully traffic-shifted everything over, we might find that we didn’t have enough supply. Reverting is costly, taking hours to get through each stage.\n\nBy doing capacity estimations in advance, this also allowed us and each team to accurately evaluate metric improvement versus cost. It might be worthwhile to double our costs if something would increase time spent on the app by 1%, but likely not for a 0.05% improvement where we could better spend that capacity funding another initiative.\n\nWith partners in AI Infra, we developed two major solutions to this process: offline performance evaluation and an automated launching platform.\n\nWe simplified determining performance of a new service using recorded traffic. Pre-recorded traffic was continuously collected into a data warehouse that the benchmarker could read from, and we’d spin up temporary jobs with this automation. One job would replay different levels of traffic continuously and send it to another job that was a clone of the existing experiment. By putting stoppers on desired latency and error rates, the tooling would eventually output a converged stable number that we could understand as the max load (see Figure 6).\n\nThe launch platform itself would input the numbers we captured from these tests, automatically collect demand data as defined, and run that same formula to calculate a cost. The platform would then perform the upscaling/downscaling cycle for teams as we shifted traffic.\n\nAnd finally, by leveraging the model registry, we were able to land this model change in code (see example in Figure 6), to help us better maintain and understand the 1000+ models within our fleet. Likewise, this bolstered our trust in the model registry, which was now directly tied to the model launch lifecycle.\n\nThis suite of launch automation has dramatically reduced the class of SEVs related to model launches, improved our pace of innovation from a few to more than 10 launches per week, and reduced the amount of time engineers spend conducting a launch by more than two days.\n\nModel stability\n\nAs the number of models in production increased, our organization started to feel the effects of an inconsistent measure of model health. While ranking models are run like any other distributed backend system (receive a request, produce a response), one may think a universal SLO that measures request success rate can suffice to capture holistic health. This is not the case for ranking models, as the accuracy of recommendations received carries significant importance to the end-user experience. If we consider a user who is a huge fan of golf but does not enjoy cooking content (see the “available & irrelevant” case in Figure 8 below), we see an example of this inaccuracy in practice. This is precisely what the model stability metric sought to capture.\n\nWhy is measuring ranking model reliability unique?\n\nRanking models, unlike traditional idempotent request/response backends, produce scores predicting user action given a set of candidates (PLIKE, PCOMMENT, PFOLLOW, etc.). These scores then combine and are used to determine which candidates are most relevant to an end user. It’s important that these scores accurately reflect user interest, as their accuracy is directly correlated to user engagement. If we recommend irrelevant content, user engagement suffers. The model stability metric was designed to make it easy to measure this accuracy and detect inaccuracy at our scale.\n\nLet’s discuss how this works.\n\nDefining model stability\n\nModels are complex, and they produce multiple output predictions. Let’s take a simplified example (shown in Figure 9 below) of a multi-task-multi-label (MTML) model predicting three actions:\n\nFor us to claim this model is stable, we must also claim that each underlying prediction is stable.\n\nWhen evaluating the accuracy of a ranking model’s predictions, we typically look at two metrics:\n\nModel calibration , which is based on observed real-world outcomes and answers the question, “Are we over- or under-predicting user action?” It is calculated as a ratio of predicted click-through-rate (CTR) and empirical CTR. A perfect predictor will have calibration centered at 1.\n\nModel normalized entropy (NE), which measures the discriminative power of a predictor, and answers the question, “How well can this predictor separate action from inaction?” It is calculated as a ratio of the average log-loss per impression to what the average log-loss per impression would be if we always predicted the empirical CTR. With NE, lower values are better, and an NE of 1 is equivalent to random predictions.\n\n(For more information regarding our choice of prediction evaluation metrics, please refer to the paper, “Practical Lessons from Predicting Clicks on Ads at Facebook.”)\n\nA model’s predictions are unstable when either calibration or NE are out of their expected healthy ranges. To determine what a healthy range is, we must look at each metric in real time, and Figure 10 below shows what these time series can look like:\n\nBy observing the trend of a healthy prediction, we can apply thresholds for our evaluation metrics. When these thresholds are breached, the underlying prediction is considered unstable.\n\nFrom here, we can define model stability as a binary indicator across a model’s predictions. It is 1 if all underlying predictions are stable, and 0 if any prediction is unstable. This is an extremely powerful method of reacting to real-time prediction instability as well as a tool for understanding trends in predictive health per model or across distinct products ranking funnels.\n\nOperationalizing model stability\n\nWith a real-time view on model predictive health, we can leverage this unified definition of model stability and apply it to all of our models in production, once again leveraging the model registry as a ledger to hold this important data. In Figure 11 below, we can see the addition of model stability metric metadata after we determined the expected thresholds.\n\nGiven the large number of models in production, each producing many predictions, building a portable definition of model health applicable to all of our ranking models represented an important milestone toward upleveling Instagram’s ML infrastructure maturity. This has unlocked our ability to build generic alerting to guarantee detection of our most important models becoming unstable, thereby moving us closer to mitigation when our recommendation system is at risk.\n\nSince the addition of these metrics and alerting, ML teams have discovered previously hidden issues within their models and addressed them faster than before, leading to higher-quality recommendations.\n\nKey takeaways\n\nIn our journey to scale Instagram’s algorithm to manage over 1000 models, we have learned several critical lessons that have shaped our approach and infrastructure. These takeaways not only highlight the challenges we faced but also underscore the strategies that led to our success.\n\nInfra understanding is the foundation to building the right tools\n\nA unified understanding of our infrastructure footprint was essential in developing the right tools to support our scaling efforts. By identifying the gaps and potential risks in our existing systems, we were able to implement solutions such as the model registry that significantly improved our operational efficiency and reliability posture.\n\nHelping colleagues move fast means we all move faster\n\nBy addressing the model iteration bottleneck, we enabled our teams to innovate more rapidly. Our focus on creating a seamless, self-service process for model iteration empowered client teams to take ownership of their workflows. This not only accelerated their progress but also reduced the operational burden on our infrastructure team. As a result, the entire organization benefited from increased agility and productivity.\n\nReliability must consider quality\n\nEnsuring the reliability of our models required us to redefine how we measure and maintain model quality. By operationalizing model stability and establishing clear metrics for model health, we were able to proactively manage the performance of our models. This approach enables us to maintain high standards of quality across our recommendation systems, ultimately enhancing user engagement and satisfaction.\n\nOur experience in scaling Instagram’s recommendation system has reinforced the importance of infrastructure understanding, collaboration, and a focus on quality. By building robust tools and processes, we have not only improved our own operations but also empowered our colleagues to drive innovation and growth across the platform.\n\nThe post Journey to 1000 models: Scaling Instagram’s recommendation system appeared first on Engineering at Meta.", "label": "non_personal"}
{"title": "Read This Will Be Fun – Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/21/read-this-will-be-fun/", "content": "The Princess Bride meets People We Meet on Vacation in this cozy quest romantasy about a group of friends who once defended their magical land together but haven’t spoken since, reuniting to attend a royal wedding, and ending up on a new adventure to save the realm—and hopefully themselves.\n\nTen years ago, they saved the realm. It ruined their lives.\n\nEveryone in Mythria knows the story of how best friends Beatrice and Elowen, handsome ex-bandit Clare, and valiant leader Galwell the Great defended the land from darkness. It’s a tale beloved by all—except the former heroes. They haven’t spoken in a decade, devastated by what their quest cost them.\n\nBut when they receive an invitation to the queen of Mythria’s wedding, it’s a summons they can’t refuse . . . and a reunion for the ages, with Clare secretly not over his long-ago fling with Beatrice, Beatrice fighting the guilt she feels over how everything ended, Elowen unprepared for the return of her former flame (the cunning Vandra), and all of them lost without Galwell’s presence. And if reuniting with old friends and lovers wasn’t perilous enough, dark forces from their past have returned, plotting a domination that only Mythria’s one-time defenders can stop. Maybe.\n\nDusting off old weapons and old instincts, they face undead nemeses, crystal caves, enchanted swords, coffee shops, games of magical Truth or Dare, and, hardest of all, their past—rife with wounds never healed and romances never forgotten.\n\nThis time around, will their story end in happily ever after?", "label": "non_personal"}
{"title": "Read The Arts and Crafts Movement – Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/24/read-the-arts-and-crafts-movement/", "content": "Read The Arts and Crafts Movement: A Study of Its Sources, Ideals and Influence on Design Theory by Gillian Naylor\n\nLots of original quotes are interspersed, from a variety of sources. The text itself sometimes got lost in facts, dates, and sequences; it was best where it narrativized and provided high level analysis of trends. Some of the quotes are 🔥\n\n“The movement… represents in some sense a revolt against the hard mechanical conventional life and its insensibility to beauty (quite another thing to ornament). It is a protest against that so-called industrial progress which provides shoddy wares, the cheapness of which is paid for by the lives of their producers and the degradation of their users.”\n\n— Walter Crane, The Revival of Design and Handicraft (orig. pg. 12)\n\nI found the layout of the text frustratingly uncomfortable to read. I read through page 145 (195 pages of content before endnotes).\n\nGraphics\n\nInteresting and wide-ranging collection of sample works included (chiefly black and white unfortunately) — though I was frustrated at least twice when a specific piece would be described in the text but not pictured.\n\nNotes and Quotes\n\nMartin Wiener – English Culture and the Decline of the Industrial Spirit\n\nDesign theory and categorization of ornament = related to industrialization of design\n\n“The Arts and Crafts movement was inspired by a crisis of conscience. Its motivations were social and moral, and its aesthetic values derived from the conviction that society produces the art and architecture it deserves.” (from 1989 preface by Gillian Naylor)\n\n“Averting mankind’s enslavement to the machine by saving the mass product and the home from mechanical anarchy and by restoring them to purpose, sense and life.”\n\n— Walter Gropius, The Scope of Total Architecture\n\ncontemporary critique from Thorstein Veblen 1899: idolizing the handmade –> conspicuous consumption — “propaganda of crudity” describing the “exaltation of the defective” aka imperfections in handwork (aestheticization!!!)\n\nthey don’t write insults like this anymore lol: “disencumber yourselves of the lymphatic ideology of your deplorable Ruskin” –Marinetti, 1912\n\n1835 committee to figure out how to give craftspeople a design sense and *taste* since consumers preferred imported aesthetics\n\nRuskin thought a craft and its society were inextricable\n\n“For it is not the material, but the absence of human labor, which makes the thing worthless, and a piece of terracotta, or plaster of paris, which has been wrought by the human hand, is worth all the stone and Carrara cut by machinery. It is, indeed, possible and even usual, for men to sink into machines themselves, so that even handwork has all the character of mechanization.”\n\n— Ruskin, ‘The Lamp of Truth’ from The Seven Lamps of Architecture\n\nThe Stones of Venice = key book to the movement — especially the essay “Nature of Gothic” (here as printed by William Morris)\n\n“It is not that men are ill-fed, but that they have no pleasure in the work by which they make their bread, and therefore look to wealth as the only means of pleasure.”\n\n— Ruskin, ‘Nature of Gothic’ from The Stones of Venice\n\nRuskin into destigmatizing manual labor\n\nsociety’s wealth measured in human happiness and its works of art\n\nthis is grouping Burne-Jones in with Arts and Crafts rather than Pre-Raphaelite\n\n“It was just a commonplace thing handled imaginatively, and it gave me as much pleasure as anything in the exhibition. It made me feel that it takes a big man to do a simple thing.”\n\n— architect John Sedding, about a piece of furniture designed by Ford Madox Brown\n\n“‘Art’ to them meant individuality and the search for ‘truth’, whether in painting, architecture or applied design — and truth, they felt, could be found both in the study of nature, and in the recreation of the spirit rather than the letter of mediaevalism.”\n\nduality of “straightforward, honest craftsmanship” and “mid-nineteenth-century ornamental conventions”\n\n“cardinal principle” = know your materials and learn the craft directly\n\nMorris thought pattern design should hold meaning:\n\n“do not introduce any lines or objects which cannot be explained by the structure of the pattern; it is just this logical sequence of form, this growth which looks as if, under the circumstances, it could not have been otherwise, which prevents the eye wearying of the repetition of the pattern.”\n\nconflict between Morris’ love of craftsmanship and the expense of producing quality goods blocking most people from accessing it led to him becoming a Socialist\n\n“[I]t is the allowing of machines to be our masters, and not our servants, that so injures the beauty of life nowadays.” — William Morris\n\nArts and Crafts accepted both “simple and luxurious” — “sprang from the ideal of the craftsman as artist, and from the belief in individualism and individual commitment”\n\nSee also: Read Liberty: British Colour Pattern\n\nRead In Harmony with Nature", "label": "non_personal"}
{"title": "tech industry – Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/tag/tech-industry/", "content": "This feels like a sister piece to Ed Zitron’s essay Era of the Business Idiots and Mandy Brown’s essay Toolmen. Fair warning, this is a 5000 word post; I’ve been working on this for weeks, pulling together what I’ve learned about generative AI and culture over the past two years, so I hope it is worth your time 😄 Bonus: it doubles as a playlist 🎶\n\n“‘Real power’ is achieved when a technology ‘[leaves] mythology and [enters] banality,'” Marion Fourcade and Kieran Healy quote Vincent Mosco in The Ordinal Society. We’ve had the mythology stage — the world tour with grandiose prophecies of imminent AGI — but now the race to normalize generative AI* is on: tech corporations are attempting to inure people to generative AI, an expression of the Business Borg aesthetic that currently carries a negative stigma outside of tech.\n\n*(My rule of thumb: if something is described as AI, it’s probably predatory and/or bullshit; if it’s described as machine learning, it probably does something useful. Not always true but a helpful predictor.)\n\nIn general, people like what we recognize better than what we don’t — we prefer cultural works we can categorize to the unfamiliar and undefinable — and we are facing an inescapable shock-and-awe barrage of genAI graphics across the web to inundate our synapses with uncanny synthetic renderings.\n\nCurrently, generative AI is shunned by many artists and writers, the traditional arbiters of good taste and culture, because it has been developed through the theft of their labor. But tech CEOs stand to make (even bigger) fortunes if they can convince people that genAI doesn’t signify bad taste, or make it seem like an irrevocable fact of life, like spam emails and text scammers. It’s being deployed upon us with the same lockstep corporate solidarity that forced us to pay fees for checked luggage on flights (younger folks, before 2008 your bag used to be included with your ticket! Stowing your carry-on wasn’t a competitive sport back in the day.).", "label": "non_personal"}
{"title": "Leveraging BigQuery JSON for Optimized MongoDB Dataflow Pipelines", "url": "https://developers.googleblog.com/en/leveraging-bigquery-json-for-optimized-mongodb-dataflow-pipelines/", "content": "This streamlined approach saves time and resources, empowering users to unlock the full potential of their data through advanced data analytics and machine learning.\n\nWe're delighted to introduce a major enhancement to our Google Cloud Dataflow templates for MongoDB Atlas. By enabling direct support for JSON data types, users can now seamlessly integrate their MongoDB Atlas data into BigQuery, eliminating the need for complex data transformations.\n\nLimitations without JSON support\n\nTraditionally, Dataflow pipelines designed to handle MongoDB Atlas data often necessitate the transformation of data into JSON strings or flattening complex structures to a single level of nesting before loading into BigQuery. Although this approach is viable, it can result in several drawbacks:\n\nIncreased latency: The multiple data conversions required can lead to increased latency and can significantly slow down the overall pipeline execution time.\n\nHigher operational costs: The extra data transformations and storage requirements associated with this approach can lead to increased operational costs.\n\nReduced query performance: Flattening complex document structures in JSON String format can impact query performance and make it difficult to analyze nested data.\n\n\n\nSo, what’s new?\n\nBigQuery's Native JSON format addresses these challenges by enabling users to directly load nested JSON data from MongoDB Atlas into BigQuery without any intermediate conversions.\n\nThis approach offers numerous benefits:\n\nReduced operating costs: By eliminating the need for additional data transformations, users can significantly reduce operational expenses, including those associated with infrastructure, storage, and compute resources.\n\nEnhanced query performance: BigQuery's optimized storage and query engine is designed to efficiently process data in Native JSON format, resulting in significantly faster query execution times and improved overall query performance.\n\nImproved data flexibility: users can easily query and analyze complex data structures, including nested and hierarchical data, without the need for time-consuming and error-prone flattening or normalization processes.\n\nA significant advantage of this pipeline lies in its ability to directly leverage BigQuery's powerful JSON functions on the MongoDB data loaded into BigQuery. This eliminates the need for a complex and time-consuming data transformation process. The JSON data within BigQuery can be queried and analyzed using standard BQML queries.\n\nWhether you prefer a streamlined cloud-based approach or a hands-on, customizable solution, the Dataflow pipeline can be deployed either through the Google Cloud console or by running the code from github repository.\n\n\n\nEnabling data-driven decision-making\n\nTo summarize, Google’s Dataflow template provides a flexible solution for transferring data from MongoDB to BigQuery. It can process entire collections or capture incremental changes using MongoDB's Change Stream functionality. The pipeline's output format can be customized to suit your specific needs. Whether you prefer a raw JSON representation or a flattened schema with individual fields, you can easily configure it through the userOption parameter. Additionally, data transformation can be performed during template execution using User-Defined Functions (UDFs).\n\nBy adopting BigQuery Native JSON format in your Dataflow pipelines, you can significantly enhance the efficiency, performance, and cost-effectiveness of your data processing workflows. This powerful combination empowers you to extract valuable insights from your data and make data-driven decisions.\n\nFollow the Google Documentation to learn how to set up the Dataflow templates for MongoDB Atlas and BigQuery.", "label": "non_personal"}
{"title": "Simplified Dataflow Connectors with Managed I/O", "url": "https://developers.googleblog.com/en/simplified-dataflow-connectors-with-managed-io/", "content": "Google Cloud Dataflow offers a fully managed data processing system for running Apache Beam pipelines on Google Cloud in a highly scalable manner. Due to being a fully managed service, Dataflow users do not have to worry about any service side regressions and versioning. The promise is that you only concern yourself with your pipeline logic while Google takes care of the service infrastructure. While this is certainly true, Apache Beam itself is a very full featured SDK that provides many simple to highly complex transforms for you to use in their pipelines. For example, Apache Beam provides a number of I/O connectors. Many of these connectors are Apache Beam composite transforms from 10s to 100s of steps. Historically, these have been considered \"user code\" from the service's perspective, despite being not authored or maintained by the user. There are several common complications customers run into complex Beam transforms such as I/O connectors. You are on the hook for upgrading Beam to adopt any fixes and improvements to connectors. Connector APIs vary widely and moving from one connector to another usually requires a lot of exploration and learning. While connectors offer a complete API, the API might not be optimized for the Dataflow runner. To alleviate all three of these issues, Dataflow recently introduced a new offering named Managed I/O. With Managed I/O the service itself is able to manage these complexities on your behalf. Hence you can truly focus on their pipelines business logic instead of focussing on the minutiae related to using and configuring a specific connector to suit their needs. Below we detail how each of the above mentioned complexities are addressed via Managed I/O.\n\nAutomatic SDK upgrades Apache Beam is a fully fledged SDK with many transforms, features, and optimization. Like many large pieces of software, upgrading Beam to a new version can be a significant process. Usually upgrading Beam involves upgrading all parts of a pipeline including all I/O connectors. But sometimes, you just need to obtain access to a critical bug fix or an improvement available in the latest version of one or more I/O connectors used in your pipeline. Managed I/O with Dataflow simplifies this by completely taking over the management of the Beam I/O connector version. With Managed I/O, Dataflow will make sure that I/O connectors used by pipelines are always up to date. Dataflow performs this by always upgrading I/O connectors to the latest vetted version during job submission and streaming update via replacement. For example, assume that you use a Beam pipeline that uses Beam 2.x.0 and assume that you use the Managed Apache Iceberg I/O source in your pipeline. Also, assume that the latest vetted version of the Iceberg I/O source supported by Dataflow is 2.y.0. During job submission, Dataflow will replace this specific connector with version 2.y.0 and will keep the rest of the Beam pipeline including any standard (non-managed) I/O connectors at version 2.x.0.\n\nAfter replacement, Dataflow optimizes the updated pipeline and executes it in GCE. To achieve isolation between connectors from different Beam versions, Dataflow deploys an additional Beam SDK container in GCE VMs. So in this case, Beam SDK containers from both versions 2.x.0 and 2.y.0 will be running in each GCE VM used by the Dataflow job. So with Managed I/O you can be assured that I/O connectors used in your pipeline are always up to date. This allows you to focus on improving the business logic of your pipeline without worrying about upgrading the Beam version to simply obtain I/O connector updates.\n\nSimplified IO API APIs differences across Beam I/O connectors vary greatly. This means that, whenever you try to use a new Beam I/O connector, you would have to learn an API specific to that connector. Some of the APIs can be quite large and non-intuitive. This can be due to: Support for various and in some cases redundant features offered by the underlying system. Maintaining backwards compatibility for legacy (or archaic) features or defaults. Support for customizing the I/O connector to support edge cases and implementation details that may only apply to few customers. Above points result in very large API surfaces for some connectors that are not intuitive for a new customer to use efficiently.\n\nManaged I/O offers standardized Java and Python APIs for supported I/O connectors. For example, with Beam Java SDK an I/O connector source can be instantiated in the following standardized form.\n\nManaged.read(SOURCE).withConfig(sourceConfig) Java Copied\n\nAn I/O connector sink can be instantiated in the following form.\n\nManaged.write(SINK).withConfig(sinkConfig) Java Copied\n\nHere SOURCE and SINK are keys specifically identifying the connector while sourceConfig and sinkConfig are maps of configurations used to instantiate the connector source or sink. The map of configurations may also be provided as YAML files available locally or in Google Cloud Storage. Please see the Managed I/O website for more complete examples for supported sources and sinks. Beam Python SDK offers a similarly simplified API. This means that various Beam I/O connectors with different APIs can be instantiated in a very standard way. For example,\n\n// Create a Java BigQuery I/O source Map<String, Object> bqReadConfig = ImmutableMap.of(\"query\", \"<query>\", ...); Managed.read(Managed.BIGQUERY).withConfig(bqReadConfig) // Create a Java Kafka I/O source. Map<String, Object> kafkaReadConfig = ImmutableMap.of(\"bootstrap_servers\", \"<server>\", \"topic\", \"<topic>\", ...); Managed.read(Managed.KAFKA).withConfig(kafkaReadConfig) // Create a Java Kafka I/O source but with a YAML based config available in Google Cloud Storage. String kafkaReadYAMLConfig = \"gs://path/to/config.yaml\" Managed.read(Managed.KAFKA).withConfigUrl(kafkaReadYAMLConfig) // Create a Python Iceberg I/O source. iceberg_config = {\"table\": \"<table>\", ...} managed.Read(managed.ICEBERG, config=iceberg_config) Java Copied\n\nAutomatically optimized for Dataflow Many Beam connectors offer a comprehensive API for configuring and optimizing the connector to suit a given pipeline and a given Beam runner. One downside of this is that if you specifically want to run on Dataflow, you may have to learn the specific configurations that best suit Dataflow and apply them when setting up your pipeline. Connector related documentation can be long and detailed and specific changes needed might not be intuitive. This might result in connectors used in Dataflow pipelines performing in a sub-optimal way. Manage I/O connectors alleviates this by automatically re-configuring the connectors to incorporate best practices and configure them to best suit Dataflow. Such re-configuration may occur during job submission or streaming update via replacement. For example, Dataflow streaming pipelines offer two modes, exactly-once and at-least-once while BigQuery I/O sink with Storage Write API offer two analogous delivery semantics, exactly-once and at-least-once. BigQuery sink with at-least-once delivery semantics is usually less expensive and results in lower latencies. With standard BigQuery I/O connectors, you are responsible for making sure that you use the appropriate mode when using the BigQuery I/O. With Managed BigQuery I/O sink this is automatically configured for you. Which means that if your streaming pipeline is operating at the at-least-once mode, your Managed I/O BigQuery sink will be automatically configured to use the at-least-once delivery semantics.\n\nReal-world pipelines We ran several pipelines that wrote data using the Managed Iceberg I/O sink backed by a Hadoop catalog deployed in GCS (please see here for the other supported catalogs). Pipelines were submitted using Beam 2.61.0 and the Managed I/O sink was automatically upgraded by Dataflow to the latest supported version. All benchmarks used n1-standard-4 VMs and the number of VMs used by the pipeline was fixed to 100. Please note that execution time here does not include the startup and shutdown time.\n\nAs the benchmarks show, Managed Iceberg I/O scaled up nicely and both metrics grew linearly with the data size. We also ran a streaming pipeline that read from Google Pub/Sub and used the Managed I/O Kafka sink to push messages to a Kafka cluster hosted in GCP. The pipeline used Beam 2.61.0 and Dataflow upgraded the Managed Kafka sink to the latest supported version. During the steady state, the pipeline used 10 n1-standard-4 VMs (max 20 VMs). The pipeline was consistently processing messages at a throughput of 250k msgs/sec across all steps and was run for 2 hours.\n\nThe following graph shows the data throughputs of various steps of the pipeline. Note that throughputs are different here since the element size changes between steps. The pipeline read from Pub/Sub at a rate of 75 MiB/sec (red line) and wrote to Kafka at a rate of 40 MiB/sec (green line).\n\nBoth latency and backlog was low for the duration of the pipeline execution.\n\nThe pipeline used VM CPU and memory efficiently.", "label": "non_personal"}
{"title": "Unlock your potential: Discover the enhanced Google Developer Program", "url": "https://developers.googleblog.com/en/google-developer-program-latest-enhancements/", "content": "The Google Developer Program is evolving. We're introducing AI-powered tools and expanded resources designed to help you build faster, smarter and more effectively with Google’s technologies.\n\nIn today’s fast-paced development landscape, having the right toolkit is crucial. We’ve listened to your feedback and are delivering updates that will enable you to focus on what matters most: creating exceptional AI applications.\n\n\n\nUnlock your full potential with Google Developer Program Premium: Now infused with AI\n\nThe Google Developer Program premium membership ($299/year) is an enhanced set of resources designed to provide developers with more advanced capabilities, including access to Google’s latest AI tools. Think of it as your catalyst for growth with advanced tools and dedicated support at every phase of your development journey.\n\nToday, we are adding some new ways you can experience the power of Google’s AI:\n\nEnhanced Coding Assistance with Gemini Code Assist Standard: Access paid Gemini in Firebase features and Gemini Code Assist to instantly boost your coding efficiency. Write high-quality code, faster, and with greater confidence.\n\nIncreased Capacity with Firebase Studio Workspaces: Get more room to build! Project IDX is now part of Firebase Studio. With premium, you get 30 Firebase Studio workspaces, providing greater flexibility to handle complex projects and scale your applications.\n\nExperiment with the Latest Models: Dive into API-driven AI with a $50 GenAI developer credit for Google AI Studio and Google Cloud Vertex AI. Experiment with cutting-edge Gemini, Imagen, and Veo models and integrate powerful AI capabilities into your application.\n\nAccess to Premium Google AI Features: Premium members receive a 3-month free trial of Google One AI Premium. Enjoy Gemini Advanced, NotebookLM Plus, increased storage, and much more.\n\nWe've also improved the premium benefits dashboard. Log in to see all your benefits and easily activate them in one place.", "label": "non_personal"}
{"title": "Announcing the Agent2Agent Protocol (A2A)", "url": "https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/", "content": "A new era of Agent Interoperability\n\n\n\nAI agents offer a unique opportunity to help people be more productive by autonomously handling many daily recurring or complex tasks. Today, enterprises are increasingly building and deploying autonomous agents to help scale, automate and enhance processes throughout the workplace–from ordering new laptops, to aiding customer service representatives, to assisting in supply chain planning.\n\nTo maximize the benefits from agentic AI, it is critical for these agents to be able to collaborate in a dynamic, multi-agent ecosystem across siloed data systems and applications. Enabling agents to interoperate with each other, even if they were built by different vendors or in a different framework, will increase autonomy and multiply productivity gains, while lowering long-term costs.\n\n\n\nToday, we’re launching a new, open protocol called Agent2Agent (A2A), with support and contributions from more than 50 technology partners like Atlassian, Box, Cohere, Intuit, Langchain, MongoDB, PayPal, Salesforce, SAP, ServiceNow, UKG and Workday; and leading service providers including Accenture, BCG, Capgemini, Cognizant, Deloitte, HCLTech, Infosys, KPMG, McKinsey, PwC, TCS, and Wipro. The A2A protocol will allow AI agents to communicate with each other, securely exchange information, and coordinate actions on top of various enterprise platforms or applications. We believe the A2A framework will add significant value for customers, whose AI agents will now be able to work across their entire enterprise application estates.\n\nThis collaborative effort signifies a shared vision of a future when AI agents, regardless of their underlying technologies, can seamlessly collaborate to automate complex enterprise workflows and drive unprecedented levels of efficiency and innovation.\n\nA2A is an open protocol that complements Anthropic's Model Context Protocol (MCP), which provides helpful tools and context to agents. Drawing on Google's internal expertise in scaling agentic systems, we designed the A2A protocol to address the challenges we identified in deploying large-scale, multi-agent systems for our customers. A2A empowers developers to build agents capable of connecting with any other agent built using the protocol and offers users the flexibility to combine agents from various providers. Critically, businesses benefit from a standardized method for managing their agents across diverse platforms and cloud environments. We believe this universal interoperability is essential for fully realizing the potential of collaborative AI agents.", "label": "non_personal"}
{"title": "Google Cloud announces general availability of APIM Operator for Apigee", "url": "https://developers.googleblog.com/en/google-cloud-announces-apim-operator-for-apigee-general-availability/", "content": "We're excited to announce the general availability of the Apigee APIM Operator, a new feature that brings lightweight API Management and API Gateway capabilities to your GKE environment. This release marks a significant step in our strategy to enable Apigee for API management on any gateway, anywhere.\n\n\n\nWhat does this mean for you?\n\nDeveloper-Native Tooling: For the many cloud-native businesses using CNCF-standardized tooling, you can now configure API management using Kubernetes-like YAML, eliminating the need to switch between different tools.\n\nReduced Friction: By supporting APIM with Kubernetes and CNCF toolchains, we're reducing the conceptual and operational friction for service developers and platform administrators.\n\nPolicy Management: Admins can create APIM template rules with RBAC, allowing different groups to use different sets of policies based on their needs. Users and admins can also add various Apigee policies to APIM templates, achieving a comparable level of functionality to Apigee Hybrid.\n\n\n\nKey Features and Capabilities\n\nThe GA release enables customers to configure a GKE cluster and GKE Gateway to use an Apigee Hybrid instance for API management through a traffic extension (ext-proc callout). It also provides API lifecycle management using YAML-based policies operated through the Kubernetes/CNCF toolchain and offers factory-built-in starter defaults for Day-Zero with workload tailoring.\n\n\n\nAddressing Customer Needs\n\nThis feature addresses the growing need for developer-friendly tooling that simplifies API management. Apigee, with its perceived complexity and the requirement to switch from kubectl to other tooling, was seen as less agile. The APIM Operator is our response to this feedback, providing a more streamlined and efficient way to manage APIs.\n\n\n\nLooking Ahead\n\nBuilding on the strong foundation of this GA release, we are exploring potential future enhancements such as gRPC and GraphQL support to accommodate a wider range of API types. We are also evaluating ways to address current limitations concerning the number of Gateway resources and policy attachments, and will update the community as new features and support become available.\n\nWe believe that the APIM Operator will significantly improve the developer experience and streamline API management for our customers. We are excited to see the innovative ways you will use this feature to build and deploy your applications.", "label": "non_personal"}
{"title": "What's new with Agents: ADK, Agent Engine, and A2A Enhancements", "url": "https://developers.googleblog.com/en/agents-adk-agent-engine-a2a-enhancements-google-io/", "content": "At Google, we envision a future where intelligent agents are not just tools, but collaborative partners in solving complex challenges, streamlining workflows, and unlocking new possibilities. We believe that empowering developers with a platform that offers flexibility, trust, and comprehensive capabilities is key to realizing this potential. Today, we're thrilled to share a series of significant updates across our product portfolio that reflect this vision, designed to help you build and manage your intelligent agents with unprecedented ease and power. These enhancements focus on providing robust development tools, intuitive management interfaces, and seamless agent-to-agent communications, delivering a stronger foundation for the next generation of AI powered solutions.\n\nBuilding with confidence and flexibility: Agent Development Kit (ADK) To empower you to create sophisticated agents with stability and adaptability, we've added significant innovations with our Agent Development Kit (ADK). Python ADK v1.0.0: Stability for Production-Ready Agents We're excited to announce the v1.0.0 stable release of our Python Agent Development Kit. This milestone signifies that the Python ADK is now production-ready, offering a reliable and robust platform for developers to confidently build and deploy their agents in live environments. We've heard incredible feedback from customers using Agent Development Kit already, including Renault Group, Box, and Revionics. Java ADK v0.1.0: Extending Agent Capabilities to the Java Ecosystem Expanding our reach, we're also launching the initial release of the Java ADK v0.1.0. This development brings the power and flexibility of the ADK to Java developers, enabling them to leverage its capabilities for their agent development needs.\n\nTo get started with the Java ADK, you can add the following dependency to your Maven project:\n\n<dependency> <groupId>com.google.adk</groupId> <artifactId>google-adk</artifactId> <version>0.1.0</version> </dependency> XML Copied\n\nIntuitive control and management: The Agent Engine UI The Vertex AI Agent Engine helps developers deploy, manage, and scale agents in production. We’re excited to now offer an Agent Engine UI to simplify the agent lifecycle in a more straightforward and centralized way. This user-friendly interface, accessible within the Google Cloud console, provides a comprehensive dashboard to view and manage your deployed agents, list sessions, trace and debug actions, and monitor your agents. This streamlined approach significantly enhances the development and management process, offering you greater control and deeper insights into your agent's behavior and performance.", "label": "non_personal"}
{"title": "Data-driven marketing starts with developers", "url": "https://developers.googleblog.com/en/data-driven-marketing-starts-with-developers/", "content": "To build a great marketing campaign in today’s landscape, data needs to be steering your strategy, not just measuring success. Developers play a key role in implementing the tools that analyze and process this data, turning it into insights, smarter strategies, and better results.\n\nUnlock the power in your marketing data with these three developer-friendly MarTech solutions. From gathering data with unparalleled transparency and control, to transforming raw data into structured insights, or using automated A/B testing for optimal performance, here’s how developers can transform what marketing data can do.\n\n\n\nsGTM Pantheon\n\nGain more control and transparency over your marketing data\n\nFrom buttons clicked to pages scrolled, knowing how people interact with your website or app is crucial to optimizing performance. Server-side Google Tag Manager (sGTM) makes this process easier by measuring traffic and managing data flow—while opening the doors to better privacy, performance, control, and productivity.\n\nsGTM Pantheon is a toolbox of easy-to-deploy solutions that complement the existing capabilities of sGTM in different ways:\n\nImprove reporting, bidding, audience management, and data pipeline processes.\n\nReceive unparalleled transparency and control over website and app data.\n\nAccess data from external APIs and cloud-based customer, product, and business data in real time.\n\nOffer real-time website personalization and conversion rate optimization.\n\nAccess advanced analytics and reporting using cloud databases.\n\n\n\nDevelopers have the flexibility to mix and match solutions to create a single pipeline that can be integrated with both Google and non-Google platforms. And because sGTM Pantheon uses a server environment, the solutions run in a private, first-party cloud-secure environment.\n\n\n\nWhat will you find in the sGTM Pantheon toolbox?\n\nTo gather data:\n\nSoteria: Calculates bid to profit for online transactions without exposing data.\n\nPhoebe: Calls Vertex AI in real time for Lifetime Value (LTV) bidding and lead scoring.\n\nArtemis: Gets customer data from Firestore for audience segmentation.\n\nApollo: Retrieves data from a Google Sheet to generate lead gen value for lead scoring.\n\nCerberus: Integrates reCAPTCHA to filter bot-generated events and suspicious activity.\n\nDioscuri: Offers personalization with quick access to Gemini.\n\n\n\nTo send data:\n\nHephaestus: Advances bidding, audience, analytics, and marketing data pipeline automation.\n\nDeipeus: Sends first-party data back to the website for personalization.\n\nChaos: Drives advanced analytics, data recovery, and audience creation.\n\nHermes: Simplifies the sending of data in data pipelines.\n\n\n\nTo manage data:\n\nArgos: Monitors critical gTag settings.\n\n\n\nsGTM Pantheon is a living solution and is continually growing. Want to see more tools? Explore the full sGTM Pantheon on GitHub.\n\n\n\nGA4 Dataform\n\nTransform BigQuery data into accessible insights with GA4 Dataform\n\nYour Google Analytics 4 (GA4) marketing data holds untold stories, powerful insights, and new ways to connect with your audience—but deciphering it isn’t always easy.\n\nGA4 Dataform is a data transformation tool that organizes raw BigQuery data into clear, modular tables, such as events, items, sessions, transactions, and more—so users of all technical skill levels can analyze data and steer data-driven campaigns. Offering both depth and simplicity, GA4 Dataform gives you the power to go beyond default settings, build your own data models, and find new ways to engage with customers.\n\n\n\nHow do I integrate GA4 Dataform with BigQuery?\n\nGA4 Dataform is a Google Cloud Dataform project that provides SQL data models for transforming raw GA4 BigQuery exports. The code is essentially a starter pack to help you build models on top of the GA4 raw data exports for data-driven marketing insights.", "label": "non_personal"}
{"title": "Google Cloud donates A2A to Linux Foundation", "url": "https://developers.googleblog.com/en/google-cloud-donates-a2a-to-linux-foundation/", "content": "Today at Open Source Summit North America, the Linux Foundation announced the formation of the Agent2Agent project with Amazon Web Services, Cisco, Google, Microsoft, Salesforce, SAP, and ServiceNow. With the formation of this new, independent entity, the companies will collaborate closely on fostering an open and interoperable ecosystem for AI agents with the Agent2Agent (A2A) protocol and other interoperability technology. The project will be hosted by the Linux Foundation and will be seeded with Google’s transfer of the groundbreaking Agent2Agent (A2A) protocol specification, accompanying SDKs, and developer tooling.\n\nThe A2A protocol, an open standard for communication and collaboration between distinct AI agents, aims to break down the silos that currently limit the potential of artificial intelligence. More than 100 companies now support the protocol, with AWS and Cisco as its newest validators. By providing a common language for AI agents to discover each other’s capabilities, securely exchange information, and coordinate complex tasks, the A2A protocol is paving the way for a new era of more powerful, collaborative, and innovative AI applications.\n\nThe formation of the Agent2Agent project under the neutral governance of the Linux Foundation will ensure that this critical component remains vendor-agnostic and community-driven. This move is designed to accelerate the adoption and development of the A2A protocol by providing a robust framework for open collaboration, intellectual property management, and long-term stewardship.", "label": "non_personal"}
{"title": "Unlocking the Potential of Quantum Computing", "url": "https://developers.googleblog.com/en/unlocking-the-potential-of-quantum-computing-a-developers-guide-to-error-correction/", "content": "A Developer’s Guide to Error Correction\n\nManipulating quantum states on a superconducting chip cooled to a fraction of a degree above absolute zero at the bottom of a cryogenic fridge is incredibly challenging. And things don't always go to plan. Errors happen. Lots of errors. Sophisticated strategies are required to extract reliable computation from what would otherwise be just so many random output bits. Success is only possible if errors are not too overwhelmingly common. Today, Google Quantum AI announced the construction of a quantum chip where errors are indeed not overwhelmingly common. It makes use of the surface code, essentially a square patch of nearest-neighbor coupled physical qubits that work together to form a single more reliable logical qubit. As the square patch gets bigger, a logical qubit should get more reliable, and that is exactly what we demonstrated: a chip with logical qubits getting over a factor of two more reliable with each increase in patch size. But why is this needed—what are quantum errors and how are they corrected? We'll give a short version of the answers here, and if you want more than a brief overview, you can head straight to the hands-on quantum error correction course now available for free on Coursera where you will find a series of videos and exercises to develop your understanding. “This new course opens a door to a complex domain, making quantum error correction not just accessible, but tangible for millions of learners worldwide,” says Marni Baker Stein, Chief Content Officer at Coursera. “Our collaboration with an industry pioneer like Google Quantum AI signifies another step towards a future where knowledge is the key to harnessing the power of quantum computing.”\n\nWhat is a quantum error? Let's start with classical errors. You have a bit. Suppose it should be 0. But a cosmic ray hits it and it becomes 1. That's a bit-flip error, the only type of error in a classical computer. In comparison, a quantum chip doesn't consist of bits but rather multilevel quantum systems so cold they have well-defined discrete ground and excited states. We write these states as |0>, |1>, |2>, etc. When a cosmic ray hits a qubit that should be |0>, it can produce a superposition a|0> + b|1> + c|2> + ... of many states. Careful engineering of our device has made our qubits robust to all but the most energetic impacts, allowing us to compute results and simply discard the output when we are hit. When computing, we try hard to only use states |0> and |1>, but control and measurement errors inevitably lead to the occasional |2+> state, known as leakage errors. Resetting a qubit gets rid of this error, but also gets rid of any data on that qubit, so we have a special gate that moves higher states off data qubits and onto a qubit about to be reset. This prevents the accumulation of |2+> states in the computer. Then there is the problem that quantum data just doesn't like hanging around very long. Qubits like to relax. If you use an excited state to represent |1>, after a short time it will relax to |0>. We also want to be able to store superpositions like a|0> + b|1>, and that ‘plus’ can spontaneously become a ‘minus’, or a phase-flip error. The various ways that qubits lose data are collectively called decoherence. In general, decoherence can produce a completely different state to the one we want, but fortunately this difference can be broken into a mix of bit flips and phase flips; for brevity we call them X and Z errors.\n\nHow do we detect X and Z errors? Let's start with a picture of our quantum chip.\n\nThis is a 2D array of qubits with nearest neighbor interactions only. Data qubits store our precious quantum state. Measure qubits are used to detect X and Z errors. A good way to understand this is to imagine each light blue region detecting Z errors on the data qubits it touches, and each dark blue region detecting X errors. A Z error on a data qubit activates the neighboring light blue regions, locating it and allowing us to compensate for its presence in software. Provided the density of X and Z errors is low enough, the pattern of lit up regions will give us clear information allowing us to find and compensate for these errors. If a measure qubit suffers an error, that can give you a falsely activated region. To cope with this, the search for errors is repeated as often as possible, and the next time the check is performed there is a good chance it will be resolved. This creates an identifiable signature for a measurement error, enabling these to also be handled in software. It is an area of ongoing research to devise more sophisticated algorithms to handle the output of measure qubits to better identify the location and type of errors, all while keeping pace with the quantum computer.\n\nLearning more What we’ve covered above is scraping the surface of quantum error correction and the critical role it plays in advancing quantum computing. For a step-by-step explanation and labs starting from the very basics, through quantum states and circuits, to some of the latest error correction tools used today, go to Coursera for our hands-on quantum error correction course. If you are a software engineer who has always wanted to work on a quantum problem, after taking the course head over to Quantum AI open source tools, where we build open-source software like Cirq, Stim and Crumble to simulate quantum circuits and develop error correction techniques. Learn how to design quantum algorithms and contribute to the development of tools that will enable the realization of practical quantum computing.\n\nQuantum computing: extra credit Here is an exciting area of research that combines theoretical computer science, software engineering, and quantum physics:", "label": "non_personal"}
{"title": "Building a better smart home", "url": "https://developers.googleblog.com/en/building-a-better-smart-home-expanding-access-for-developers-and-users/", "content": "Expanding access for app and device developers\n\nThe smart home industry is constantly evolving, and we're committed to staying at the forefront of innovation. At I/O 2024 we announced how we reimagined Google Home as a platform for all developers. Since then we have reached major milestones and our first partners have begun launching apps built on our platform.\n\nOur goal has remained the same - to make it easier for developers to create amazing experiences for users, and for those users to enjoy seamless connectivity and interoperability across all their devices. To that end, we are making a series of investments designed to enable all developers to build for the home.\n\n\n\nEmpowering developers with Home APIs\n\nWe believe that open platforms foster greater innovation, and it's clear that developers do too. We’ve had nearly 2,000 developers sign up to learn more about the Home APIs since I/O 2024 and that's why we're thrilled to announce the public developer beta launch of Home APIs - today for Android, and in the coming months for iOS. These APIs provide developers with the tools they need to build richer and more integrated smart home experiences.\n\nThis isn't just theoretical, our partners are already making headway. Early access partners Eve, Nanoleaf, LG, ADT and Tuya Smart have launched new apps and features built with the Google Home Platform, with even more partners like Cync, GE Appliances, Yale, and Aqara releasing in the coming months. At CES 2025, our partners are showcasing device control and automation experiences in their apps built using these APIs. Hisense and Aqara will demo how the Home APIs have helped them to create and surface automations for different areas of their users’ lives. SDMC will demonstrate how the Device & Structure API enables their users to control devices connected to Google Home directly in their apps.\n\n\n\nInvesting in connectivity and interoperability\n\nDelivering a truly smart home requires seamless connectivity made effortless via our platform. Our investments in Matter are a foundational layer to the smart home experience, and we continue to increase our investments in this area across multiple fronts:\n\nExpanding Matter support: A hub for Google Home is critical to unlocking Matter's fast, secure and reliable experience - it enables remote access and fully local control of Matter devices. We've significantly expanded the reach of hubs for Google Home by integrating the Google Home runtime into over 40 million devices, including Google Nest devices, Chromecasts, Google TV devices on Android 14 and eligible LG TVs. This means more users can enjoy the benefits of Matter connectivity, with less work. You can learn more details on our expanded Matter support here.\n\nIncreasing our investment to improve Matter quality: We firmly believe in Matter's potential to unify the smart home, and we are committed to its growth. That's why we, along with Apple and Samsung, are going beyond our existing commitments to Matter to further accelerate the improvement of quality for Matter. At Google, we're investing in Matter's growth in a number of new ways, including increasing development resources to enhance certification automation, interoperability scripting, and SDK bug fixes & maintenance.\n\nStreamlining certification with Connectivity Standards Alliance: In an effort to reduce time and costs for developers looking to certify software and products across multiple ecosystems, Google is excited to join Apple and Samsung in accepting Connectivity Standards Alliance Interop Lab test results. With this, you can now get certified with Works With Google Home for Matter devices without an additional certification process with Google.\n\nMaking Thread more universal: One key customer challenge with Matter today is that Thread devices require a Thread Border Router (TBR) in the home for control. To address this challenge at the ecosystem level and expand TBR availability, we’ve partnered with MediaTek on the new Trinity chip (MT7903) that includes Wi-Fi, Bluetooth LE and Thread on a single system-on-a-chip (SoC). This makes it easier and more affordable for device OEMs to build Thread into all their new products.\n\n\n\nThe Future of the smart home\n\nWe're incredibly excited about the future of the smart home, and we believe these investments will pave the way for a new era of innovation and interoperability. By empowering developers and fostering a robust ecosystem, we're making the dream of a truly connected and intuitive smart home a reality.\n\nTo make sure you are staying up to date with the latest news, announcements, and resources from Google Home, be sure to subscribe to the Google Home Developer Newsletter.", "label": "non_personal"}
{"title": "Usability and safety updates to Google Auth Platform", "url": "https://developers.googleblog.com/en/usability-and-safety-updates-to-google-auth-platform/", "content": "Millions of developers rely on Google’s identity platform for user authentication and the ability to authorize access to hundreds of APIs. Underpinning the platform is one of the world’s largest implementations of the OAuth 2.0 protocol and related OpenID Connect standard, which provide a seamless, safe, and reliable way for developers to integrate with Google. We’re excited to share some updates that will make the platform even more secure and easy to use.\n\n\n\nSimplified OAuth configuration in the Google Cloud Console\n\nDevelopers that use Sign in with Google for authentication or to obtain user authorization to call Google APIs need to register their apps and websites to create client credentials. For developers that use the Google Cloud Console, OAuth configuration pages previously lived in the APIs & Services section. Now, these pages have their own dedicated navigation section called Google Auth Platform. As part of this change, we’ve made it easier to register new projects, reduced the time it takes to update app configurations, and added more helpful guidance for developers. Stay tuned for more improvements in the coming months, including a better onboarding wizard, simplified OAuth scope management, and changes to make app verification faster and more transparent.\n\nFor developers who use OAuth capabilities through other consoles like Firebase or Apps Script, your experience on those products remains unchanged.\n\n\n\nChange to how OAuth client secrets are displayed\n\nSome OAuth clients are required to use a “secret” when making authentication and authorization requests. The client secret is like a password for a website or application, so it’s critical to protect these strings to ensure the security and privacy of user accounts and data.\n\nHistorically, developers have been able to view and download their own client secrets in the Google Cloud Console, Firebase Console, and other places across Google developer products. Starting in June, we’ll start masking OAuth secrets in the client management pages of the Google Cloud Console. As an aid to help identify them, developer consoles will show the last few characters.", "label": "non_personal"}
{"title": "Enhancing Netflix Reliability with Service-Level Prioritized Load Shedding", "url": "https://netflixtechblog.com/enhancing-netflix-reliability-with-service-level-prioritized-load-shedding-e735e6ce8f7d?source=collection_home---4------21-----------------------", "content": "Without prioritized load-shedding, both user-initiated and prefetch availability drop when latency is injected. However, after adding prioritized load-shedding, user-initiated requests maintain a 100% availability and only prefetch requests are throttled.\n\nWe were ready to roll this out to production and see how it performed in the wild!\n\nReal-World Application and Results\n\nNetflix engineers work hard to keep our systems available, and it was a while before we had a production incident that tested the efficacy of our solution. A few months after deploying prioritized load shedding, we had an infrastructure outage at Netflix that impacted streaming for many of our users. Once the outage was fixed, we got a 12x spike in pre-fetch requests per second from Android devices, presumably because there was a backlog of queued requests built up.\n\nSpike in Android pre-fetch RPS\n\nThis could have resulted in a second outage as our systems weren’t scaled to handle this traffic spike. Did prioritized load-shedding in PlayAPI help us here?\n\nYes! While the availability for prefetch requests dropped as low as 20%, the availability for user-initiated requests was > 99.4% due to prioritized load-shedding.\n\nAvailability of pre-fetch and user-initiated requests\n\nAt one point we were throttling more than 50% of all requests but the availability of user-initiated requests continued to be > 99.4%.\n\nGeneric service work prioritization\n\nBased on the success of this approach, we have created an internal library to enable services to perform prioritized load shedding based on pluggable utilization measures, with multiple priority levels.\n\nUnlike API gateway, which needs to handle a large volume of requests with varying priorities, most microservices typically receive requests with only a few distinct priorities. To maintain consistency across different services, we have introduced four predefined priority buckets inspired by the Linux tc-prio levels:\n\nCRITICAL : Affect core functionality — These will never be shed if we are not in complete failure.\n\n: Affect core functionality — These will never be shed if we are not in complete failure. DEGRADED : Affect user experience — These will be progressively shed as the load increases.\n\n: Affect user experience — These will be progressively shed as the load increases. BEST_EFFORT : Do not affect the user — These will be responded to in a best effort fashion and may be shed progressively in normal operation.\n\n: Do not affect the user — These will be responded to in a best effort fashion and may be shed progressively in normal operation. BULK: Background work, expect these to be routinely shed.\n\nServices can either choose the upstream client’s priority or map incoming requests to one of these priority buckets by examining various request attributes, such as HTTP headers or the request body, for more precise control. Here is an example of how services can map requests to priority buckets:\n\nResourceLimiterRequestPriorityProvider requestPriorityProvider() {\n\nreturn contextProvider -> {\n\nif (contextProvider.getRequest().isCritical()) {\n\nreturn PriorityBucket.CRITICAL;\n\n} else if (contextProvider.getRequest().isHighPriority()) {\n\nreturn PriorityBucket.DEGRADED;\n\n} else if (contextProvider.getRequest().isMediumPriority()) {\n\nreturn PriorityBucket.BEST_EFFORT;\n\n} else {\n\nreturn PriorityBucket.BULK;\n\n}\n\n};\n\n}\n\nGeneric CPU based load-shedding\n\nMost services at Netflix autoscale on CPU utilization, so it is a natural measure of system load to tie into the prioritized load shedding framework. Once a request is mapped to a priority bucket, services can determine when to shed traffic from a particular bucket based on CPU utilization. In order to maintain the signal to autoscaling that scaling is needed, prioritized shedding only starts shedding load after hitting the target CPU utilization, and as system load increases, more critical traffic is progressively shed in an attempt to maintain user experience.\n\nFor example, if a cluster targets a 60% CPU utilization for auto-scaling, it can be configured to start shedding requests when the CPU utilization exceeds this threshold. When a traffic spike causes the cluster’s CPU utilization to significantly surpass this threshold, it will gradually shed low-priority traffic to conserve resources for high-priority traffic. This approach also allows more time for auto-scaling to add additional instances to the cluster. Once more instances are added, CPU utilization will decrease, and low-priority traffic will resume being served normally.\n\nPercentage of requests (Y-axis) being load-shed based on CPU utilization (X-axis) for different priority buckets\n\nExperiments with CPU based load-shedding\n\nWe ran a series of experiments sending a large request volume at a service which normally targets 45% CPU for auto scaling but which was prevented from scaling up for the purpose of monitoring CPU load shedding under extreme load conditions. The instances were configured to shed noncritical traffic after 60% CPU and critical traffic after 80%.\n\nAs RPS was dialed up past 6x the autoscale volume, the service was able to shed first noncritical and then critical requests. Latency remained within reasonable limits throughout, and successful RPS throughput remained stable.\n\nExperimental behavior of CPU based load-shedding using synthetic traffic.\n\nP99 latency stayed within a reasonable range throughout the experiment, even as RPS surpassed 6x the autoscale target.\n\nAnti-patterns with load-shedding\n\nAnti-pattern 1 — No shedding\n\nIn the above graphs, the limiter does a good job keeping latency low for the successful requests. If there was no shedding here, we’d see latency increase for all requests, instead of a fast failure in some requests that can be retried. Further, this can result in a death spiral where one instance becomes unhealthy, resulting in more load on other instances, resulting in all instances becoming unhealthy before auto-scaling can kick in.\n\nNo load-shedding: In the absence of load-shedding, increased latency can degrade all requests instead of rejecting some requests (that can be retried), and can make instances unhealthy\n\nAnti-pattern 2 — Congestive failure\n\nAnother anti-pattern to watch out for is congestive failure or shedding too aggressively. If the load-shedding is due to an increase in traffic, the successful RPS should not drop after load-shedding. Here is an example of what congestive failure looks like:\n\nCongestive failure: After 16:57, the service starts rejecting most requests and is not able to sustain a successful 240 RPS that it was before load-shedding kicked in. This can be seen in fixed concurrency limiters or when load-shedding consumes too much CPU preventing any other work from being done\n\nWe can see in the Experiments with CPU based load-shedding section above that our load-shedding implementation avoids both these anti-patterns by keeping latency low and sustaining as much successful RPS during load-shedding as before.\n\nGeneric IO based load-shedding\n\nSome services are not CPU-bound but instead are IO-bound by backing services or datastores that can apply back pressure via increased latency when they are overloaded either in compute or in storage capacity. For these services we re-use the prioritized load shedding techniques, but we introduce new utilization measures to feed into the shedding logic. Our initial implementation supports two forms of latency based shedding in addition to standard adaptive concurrency limiters (themselves a measure of average latency):\n\nThe service can specify per-endpoint target and maximum latencies, which allow the service to shed when the service is abnormally slow regardless of backend. The Netflix storage services running on the Data Gateway return observed storage target and max latency SLO utilization, allowing services to shed when they overload their allocated storage capacity.\n\nThese utilization measures provide early warning signs that a service is generating too much load to a backend, and allow it to shed low priority work before it overwhelms that backend. The main advantage of these techniques over concurrency limits alone is they require less tuning as our services already must maintain tight latency service-level-objectives (SLOs), for example a p50 < 10ms and p100 < 500ms. So, rephrasing these existing SLOs as utilizations allows us to shed low priority work early to prevent further latency impact to high priority work. At the same time, the system will accept as much work as it can while maintaining SLO’s.\n\nTo create these utilization measures, we count how many requests are processed slower than our target and maximum latency objectives, and emit the percentage of requests failing to meet those latency goals. For example, our KeyValue storage service offers a 10ms target with 500ms max latency for each namespace, and all clients receive utilization measures per data namespace to feed into their prioritized load shedding. These measures look like:\n\nutilization(namespace) = {\n\noverall = 12\n\nlatency = {\n\nslo_target = 12,\n\nslo_max = 0\n\n}\n\nsystem = {\n\nstorage = 17,\n\ncompute = 10,\n\n}\n\n}\n\nIn this case, 12% of requests are slower than the 10ms target, 0% are slower than the 500ms max latency (timeout), and 17% of allocated storage is utilized. Different use cases consult different utilizations in their prioritized shedding, for example batches that write data daily may get shed when system storage utilization is approaching capacity as writing more data would create further instability.\n\nAn example where the latency utilization is useful is for one of our critical file origin services which accepts writes of new files in the AWS cloud and acts as an origin (serves reads) for those files to our Open Connect CDN infrastructure. Writes are the most critical and should never be shed by the service, but when the backing datastore is getting overloaded, it is reasonable to progressively shed reads to files which are less critical to the CDN as it can retry those reads and they do not affect the product experience.\n\nTo achieve this goal, the origin service configured a KeyValue latency based limiter that starts shedding reads to files which are less critical to the CDN when the datastore reports a target latency utilization exceeding 40%. We then stress tested the system by generating over 50Gbps of read traffic, some of it to high priority files and some of it to low priority files:", "label": "non_personal"}
{"title": "How It’s Made: Little Language Lessons uses Gemini’s multilingual capabilities to personalize language learning", "url": "https://developers.googleblog.com/en/how-its-made-little-language-lessons-to-personalize-learning/", "content": "As an engineer, I’ve always been fascinated by languages—both the kind we code in and the kind we speak. Learning a new programming language typically begins by building something tangible, instantly putting theory into practice. Learning a new spoken language, on the other hand, often happens in a vacuum—through textbooks or exercises that feel strangely disconnected from the situations where language actually matters. As is the case with programming, language is best learned through meaningful contexts: the conversations we have, the objects around us, the moments we find ourselves in. Unlike traditional learning tools, AI can adapt to a learner’s context, making it uniquely suited to help us practice languages in ways that feel more natural and personal. This led me, along with a small group of colleagues, to experiment with the Gemini API, which enables developers to access the latest generative models from Google. The result is Little Language Lessons: a collection of three bite-sized learning experiments, all powered by Google’s Gemini models.\n\nExperiment 1, Tiny Lesson: Learning what you need, when you need it One of the most frustrating parts about learning a language is finding yourself in a situation where you need a specific word or phrase—and it’s one that you haven’t learned yet. That’s the idea behind Tiny Lesson. You describe a situation—maybe it’s “asking for directions” or “finding a lost passport”—and receive useful vocabulary, phrases, and grammar tips tailored to that context.\n\nSorry, your browser doesn't support playback for this video\n\nWe were able to accomplish this using a simple prompt recipe. The prompt begins with a persona-setting preamble that looks like this:\n\nYou are a(n) {target language} tutor who is bilingual in {target language} and {source language} and an expert at crafting educational content that is custom-tailored to students' language usage goals. Markdown Copied\n\nIn this prompt and in all of the prompts to come, we took advantage of Gemini’s ability to provide outputs as structured JSON, defining desired result as a list of keys in an object:\n\nFor the given usage context, provide a JSON object containing two keys: \"vocabulary\" and \"phrases\". The value of \"vocabulary\" should be an array of objects, each containing three keys: \"term\", “transliteration”, and \"translation\". The value of \"term\" should be a {target language} word that is highly relevant and useful in the given context. If the language of interest is ordinarily written in the Latin script, the value of “transliteration” should be an empty string. Otherwise, the value of “transliteration” should be a transliteration of the term. The value of \"translation\" should be the {source language} translation of the term. ... Markdown Copied\n\nIn total, each lesson is the result of two calls to the Gemini API. One prompt handles generating all of the vocabulary and phrases, and the other deals with generating relevant grammar topics. And the end of each prompt, we interpolate the user’s desired usage context as follows:\n\nINPUT (usage context): {user input} Markdown Copied\n\nExperiment 2, Slang Hang: Learning to sound less like a textbook There’s a moment in the journey of learning a language when you start feeling comfortable. You can hold conversations, express yourself, and mostly get by. But then you realize, you still sound… off. Too formal. Stiff. We built Slang Hang to help address this. The idea is simple: generate a realistic conversation between native speakers and let users learn from it. You can watch the dialogue unfold, revealing one message at a time and unpacking unfamiliar terms as they appear.\n\nSorry, your browser doesn't support playback for this video\n\nThe preamble for the Slang Hang prompt looks like this:\n\nYou are a screenwriter who is bilingual in {source language} and {target language} and an expert and crafting captivating dialogues. You are also a linguist and highly attuned to the cultural nuances that shape natural speech. Markdown Copied\n\nAlthough users can only reveal messages one at a time, everything—the setting, the conversation, the explanations for highlighted terms—is generated from a single call to the Gemini API. We define the structure of the JSON output as follows:\n\nGenerate a short scene that contains two interlocutors speaking authentic {target language}. Give the result as a JSON object that contains two keys: \"context\" and \"dialogue\". The value of \"context\" should be a short paragraph in {SOURCE LANGUAGE} that describes the setting of the scene, what is happening, who the speakers are, and speakers' relationship to each other. The value of \"dialogue\" should be an array of objects, where each object contains information about a single conversational turn. Each object in the \"dialogue\" array should contain four keys: \"speaker\", \"gender\", \"message\", and \"notes\". ... Markdown Copied\n\nThe dialogue is generated in the user’s target language, but users can also translate messages into their native language (a functionality powered by the Cloud Translation API). One of the more interesting aspects of this experiment is the element of emergent storytelling. Each scene is unique and generated on the fly—it could be a street vendor chatting with a customer, two coworkers meeting on the subway, or even a pair of long-lost friends unexpectedly reuniting at an exotic pet show. That said, we found that this experiment is somewhat susceptible to accuracy errors: it occasionally misuses certain expressions and slang, or even makes them up. LLMs still aren’t perfect, and for that reason it’s important to cross-reference with reliable sources.\n\nExperiment 3, Word Cam: Learning from your surroundings Sometimes, you just need words for the things in front of you. It can be extremely humbling to realize just how much you don’t know how to say in your target language. You know the word for “window”, but how do you say “windowsill”? Or “blinds”? Word Cam turns your camera into an instant vocabulary helper. Snap a photo, and Gemini will detect objects, label them in your target language, and give you additional words that you can use to describe them.\n\nSorry, your browser doesn't support playback for this video\n\nThis experiment leverages Gemini’s vision capabilities for object detection. We send the model an image and ask it for the bounding box coordinates of the different objects in that image:\n\nProvide insights about the objects that are present in the given image. Give the result as a JSON object that contains a single key called \"objects\". The value of \"objects\" should be an array of objects whose length is no more than the number of distinct objects present in the image. Each object in the array should contain four keys: \"name\", \"transliteration\", \"translation\", and \"coordinates\". ... The value of \"coordinates\" should be an integer array representing the coordinates of the bounding box for the object. Give the coordinates as [ymin, xmin, ymax, xmax]. Markdown Copied\n\nOnce the user selects an object, we send the cropped image to Gemini in a separate prompt and ask it to generate descriptors for that object in the user’s target language:\n\nFor the object represented in the given image, provide descriptors that describe the object. Give the result as a JSON object that contains a single key called \"descriptors\". The value of \"descriptors\" should be an array of objects, where each object contains five keys: \"descriptor\", \"transliteration\", \"translation\", \"exampleSentence\", \"exampleSentenceTransliteration\", and \"exampleSentenceTranslation\". ... Markdown Copied", "label": "non_personal"}
{"title": "Video annotator: a framework for efficiently building video classifiers using vision-language models and active learning", "url": "https://netflixtechblog.com/video-annotator-building-video-classifiers-using-vision-language-models-and-active-learning-8ebdda0b2db4?source=collection_home---4------23-----------------------", "content": "Video annotator: a framework for efficiently building video classifiers using vision-language models and active learning Netflix Technology Blog 6 min read · Jun 19, 2024 -- 2 Listen Share\n\nAmir Ziai, Aneesh Vartakavi, Kelli Griggs, Eugene Lok, Yvonne Jukes, Alex Alonso, Vi Iyengar, Anna Pulido\n\nIntroduction\n\nProblem\n\nHigh-quality and consistent annotations are fundamental to the successful development of robust machine learning models. Conventional techniques for training machine learning classifiers are resource intensive. They involve a cycle where domain experts annotate a dataset, which is then transferred to data scientists to train models, review outcomes, and make changes. This labeling process tends to be time-consuming and inefficient, sometimes halting after a few annotation cycles.\n\nImplications\n\nConsequently, less effort is invested in annotating high-quality datasets compared to iterating on complex models and algorithmic methods to improve performance and fix edge cases. As a result, ML systems grow rapidly in complexity.\n\nFurthermore, constraints on time and resources often result in leveraging third-party annotators rather than domain experts. These annotators perform the labeling task without a deep understanding of the model’s intended deployment or usage, often making consistent labeling of borderline or hard examples, especially in more subjective tasks, a challenge.\n\nThis necessitates multiple review rounds with domain experts, leading to unexpected costs and delays. This lengthy cycle can also result in model drift, as it takes longer to fix edge cases and deploy new models, potentially hurting usefulness and stakeholder trust.\n\nSolution\n\nWe suggest that more direct involvement of domain experts, using a human-in-the-loop system, can resolve many of these practical challenges. We introduce a novel framework, Video Annotator (VA), which leverages active learning techniques and zero-shot capabilities of large vision-language models to guide users to focus their efforts on progressively harder examples, enhancing the model’s sample efficiency and keeping costs low.\n\nVA seamlessly integrates model building into the data annotation process, facilitating user validation of the model before deployment, therefore helping with building trust and fostering a sense of ownership. VA also supports a continuous annotation process, allowing users to rapidly deploy models, monitor their quality in production, and swiftly fix any edge cases by annotating a few more examples and deploying a new model version.\n\nThis self-service architecture empowers users to make improvements without active involvement of data scientists or third-party annotators, allowing for fast iteration.\n\nVideo understanding\n\nWe design VA to assist in granular video understanding which requires the identification of visuals, concepts, and events within video segments. Video understanding is fundamental for numerous applications such as search and discovery, personalization, and the creation of promotional assets. Our framework allows users to efficiently train machine learning models for video understanding by developing an extensible set of binary video classifiers, which power scalable scoring and retrieval of a vast catalog of content.\n\nVideo classification\n\nVideo classification is the task of assigning a label to an arbitrary-length video clip, often accompanied by a probability or prediction score, as illustrated in Fig 1.\n\nFig 1- Functional view of a binary video classifier. A few-second clip from ”Operation Varsity Blues: The College Admissions Scandal” is passed to a binary classifier for detecting the ”establishing shots” label. The classifier outputs a very high score (score is between 0 and 1), indicating that the video clip is very likely an establishing shot. In filmmaking, an establishing shot is a wide shot (i.e. video clip between two consecutive cuts) of a building or a landscape that is intended for establishing the time and location of the scene.\n\nVideo understanding via an extensible set of video classifiers\n\nBinary classification allows for independence and flexibility, allowing us to add or improve one model independent of the others. It also has the additional benefit of being easier to understand and build for our users. Combining the predictions of multiple models allows us a deeper understanding of the video content at various levels of granularity, illustrated in Fig 2.\n\nFig 2- Three video clips and the corresponding binary classifier scores for three video understanding labels. Note that these labels are not mutually exclusive. Video clips are from Operation Varsity Blues: The College Admissions Scandal, 6 Underground, and Leave The World Behind, respectively.\n\nVideo Annotator (VA)\n\nIn this section, we describe VA’s three-step process for building video classifiers.\n\nStep 1 — search\n\nUsers begin by finding an initial set of examples within a large, diverse corpus to bootstrap the annotation process. We leverage text-to-video search to enable this, powered by video and text encoders from a Vision-Language Model to extract embeddings. For example, an annotator working on the establishing shots model may start the process by searching for “wide shots of buildings”, illustrated in Fig 3.\n\nFig 3- Step 1 — Text-to-video search to bootstrap the annotation process.\n\nStep 2 — active learning\n\nThe next stage involves a classic Active Learning loop. VA then builds a lightweight binary classifier over the video embeddings, which is subsequently used to score all clips in the corpus, and presents some examples within feeds for further annotation and refinement, as illustrated in Fig 4.\n\nFig 4- Step 2 — Active Learning loop. The annotator clicks on build, which initiates classifier training and scoring of all clips in a video corpus. Scored clips are organized in four feeds.\n\nThe top-scoring positive and negative feeds display examples with the highest and lowest scores respectively. Our users reported that this provided a valuable indication as to whether the classifier has picked up the correct concepts in the early stages of training and spot cases of bias in the training data that they were able to subsequently fix. We also include a feed of “borderline” examples that the model is not confident about. This feed helps with discovering interesting edge cases and inspires the need for labeling additional concepts. Finally, the random feed consists of randomly selected clips and helps to annotate diverse examples which is important for generalization.\n\nThe annotator can label additional clips in any of the feeds and build a new classifier and repeat as many times as desired.\n\nStep 3 — review\n\nThe last step simply presents the user with all annotated clips. It’s a good opportunity to spot annotation mistakes and to identify ideas and concepts for further annotation via search in step 1. From this step, users often go back to step 1 or step 2 to refine their annotations.\n\nExperiments\n\nTo evaluate VA, we asked three video experts to annotate a diverse set of 56 labels across a video corpus of 500k shots. We compared VA to the performance of a few baseline methods, and observed that VA leads to the creation of higher quality video classifiers. Fig 5 compares VA’s performance to baselines as a function of the number of annotated clips.\n\nFig 5- Model quality (i.e. Average Precision) as a function of the number of annotated clips for the “establishing shots” label. We observe that all methods outperform the baseline, and that all methods benefit from additional annotated data, albeit to varying degrees.\n\nYou can find more details about VA and our experiments in this paper.\n\nConclusion\n\nWe presented Video Annotator (VA), an interactive framework that addresses many challenges associated with conventional techniques for training machine learning classifiers. VA leverages the zero-shot capabilities of large vision-language models and active learning techniques to enhance sample efficiency and reduce costs. It offers a unique approach to annotating, managing, and iterating on video classification datasets, emphasizing the direct involvement of domain experts in a human-in-the-loop system. By enabling these users to rapidly make informed decisions on hard samples during the annotation process, VA increases the system’s overall efficiency. Moreover, it allows for a continuous annotation process, allowing users to swiftly deploy models, monitor their quality in production, and rapidly fix any edge cases.\n\nThis self-service architecture empowers domain experts to make improvements without the active involvement of data scientists or third-party annotators, and fosters a sense of ownership, thereby building trust in the system.\n\nWe conducted experiments to study the performance of VA, and found that it yields a median 8.3 point improvement in Average Precision relative to the most competitive baseline across a wide-ranging assortment of video understanding tasks. We release a dataset with 153k labels across 56 video understanding tasks annotated by three professional video editors using VA, and also release code to replicate our experiments.", "label": "non_personal"}
{"title": "Maestro: Data/ML Workflow Orchestrator at Netflix", "url": "https://netflixtechblog.com/maestro-netflixs-workflow-orchestrator-ee13a06f9c78?source=collection_home---4------20-----------------------", "content": "Maestro: Data/ML Workflow Orchestrator at Netflix Netflix Technology Blog 18 min read · Jul 22, 2024 -- 12 Listen Share\n\nBy Jun He, Natallia Dzenisenka, Praneeth Yenugutala, Yingyi Zhang, and Anjali Norwood\n\nTL;DR\n\nWe are thrilled to announce that the Maestro source code is now open to the public! Please visit the Maestro GitHub repository to get started. If you find it useful, please give us a star.\n\nWhat is Maestro\n\nMaestro is a horizontally scalable workflow orchestrator designed to manage large-scale Data/ML workflows such as data pipelines and machine learning model training pipelines. It oversees the entire lifecycle of a workflow, from start to finish, including retries, queuing, task distribution to compute engines, etc.. Users can package their business logic in various formats such as Docker images, notebooks, bash script, SQL, Python, and more. Unlike traditional workflow orchestrators that only support Directed Acyclic Graphs (DAGs), Maestro supports both acyclic and cyclic workflows and also includes multiple reusable patterns, including foreach loops, subworkflow, and conditional branch, etc.\n\nOur Journey with Maestro\n\nSince we first introduced Maestro in this blog post, we have successfully migrated hundreds of thousands of workflows to it on behalf of users with minimal interruption. The transition was seamless, and Maestro has met our design goals by handling our ever-growing workloads. Over the past year, we’ve seen a remarkable 87.5% increase in executed jobs. Maestro now launches thousands of workflow instances and runs half a million jobs daily on average, and has completed around 2 million jobs on particularly busy days.\n\nScalability and Versatility\n\nMaestro is a fully managed workflow orchestrator that provides Workflow-as-a-Service to thousands of end users, applications, and services at Netflix. It supports a wide range of workflow use cases, including ETL pipelines, ML workflows, AB test pipelines, pipelines to move data between different storages, etc. Maestro’s horizontal scalability ensures it can manage both a large number of workflows and a large number of jobs within a single workflow.\n\nAt Netflix, workflows are intricately connected. Splitting them into smaller groups and managing them across different clusters adds unnecessary complexity and degrades the user experience. This approach also requires additional mechanisms to coordinate these fragmented workflows. Since Netflix’s data tables are housed in a single data warehouse, we believe a single orchestrator should handle all workflows accessing it.\n\nJoin us on this exciting journey by exploring the Maestro GitHub repository and contributing to its ongoing development. Your support and feedback are invaluable as we continue to improve the Maestro project.\n\nIntroducing Maestro\n\nNetflix Maestro offers a comprehensive set of features designed to meet the diverse needs of both engineers and non-engineers. It includes the common functions and reusable patterns applicable to various use cases in a loosely coupled way.\n\nA workflow definition is defined in a JSON format. Maestro combines user-supplied fields with those managed by Maestro to form a flexible and powerful orchestration definition. An example can be found in the Maestro repository wiki.\n\nA Maestro workflow definition comprises two main sections: properties and versioned workflow including its metadata. Properties include author and owner information, and execution settings. Maestro preserves key properties across workflow versions, such as author and owner information, run strategy, and concurrency settings. This consistency simplifies management and aids in trouble-shootings. If the ownership of the current workflow changes, the new owner can claim the ownership of the workflows without creating a new workflow version. Users can also enable the triggering or alerting features for a given workflow over the properties.\n\nVersioned workflow includes attributes like a unique identifier, name, description, tags, timeout settings, and criticality levels (low, medium, high) for prioritization. Each workflow change creates a new version, enabling tracking and easy reversion, with the active or the latest version used by default. A workflow consists of steps, which are the nodes in the workflow graph defined by users. Steps can represent jobs, another workflow using subworkflow step, or a loop using foreach step. Steps consist of unique identifiers, step types, tags, input and output step parameters, step dependencies, retry policies, and failure mode, step outputs, etc. Maestro supports configurable retry policies based on error types to enhance step resilience.\n\nThis high-level overview of Netflix Maestro’s workflow definition and properties highlights its flexibility to define complex workflows. Next, we dive into some of the useful features in the following sections.\n\nWorkflow Run Strategy\n\nUsers want to automate data pipelines while retaining control over the execution order. This is crucial when workflows cannot run in parallel or must halt current executions when new ones occur. Maestro uses predefined run strategies to decide whether a workflow instance should run or not. Here is the list of predefined run strategies Maestro offers.\n\nSequential Run Strategy\n\nThis is the default strategy used by maestro, which runs workflows one at a time based on a First-In-First-Out (FIFO) order. With this run strategy, Maestro runs workflows in the order they are triggered. Note that an execution does not depend on the previous states. Once a workflow instance reaches one of the terminal states, whether succeeded or not, Maestro will start the next one in the queue.\n\nStrict Sequential Run Strategy\n\nWith this run strategy, Maestro will run workflows in the order they are triggered but block execution if there’s a blocking error in the workflow instance history. Newly triggered workflow instances are queued until the error is resolved by manually restarting the failed instances or marking the failed ones unblocked.\n\nIn the above example, run5 fails at 5AM, then later runs are queued but do not run. When someone manually marks run5 unblocked or restarts it, then the workflow execution will resume. This run strategy is useful for time insensitive but business critical workflows. This gives the workflow owners the option to review the failures at a later time and unblock the executions after verifying the correctness.\n\nFirst-only Run Strategy\n\nWith this run strategy, Maestro ensures that the running workflow is complete before queueing a new workflow instance. If a new workflow instance is queued while the current one is still running, Maestro will remove the queued instance. Maestro will execute a new workflow instance only if there is no workflow instance currently running, effectively turning off queuing with this run strategy. This approach helps to avoid idempotency issues by not queuing new workflow instances.\n\nLast-only Run Strategy\n\nWith this run strategy, Maestro ensures the running workflow is the latest triggered one and keeps only the last instance. If a new workflow instance is queued while there is an existing workflow instance already running, Maestro will stop the running instance and execute the newly triggered one. This is useful if a workflow is designed to always process the latest data, such as processing the latest snapshot of an entire table each time.\n\nParallel with Concurrency Limit Run Strategy\n\nWith this run strategy, Maestro will run multiple triggered workflow instances in parallel, constrained by a predefined concurrency limit. This helps to fan out and distribute the execution, enabling the processing of large amounts of data within the time limit. A common use case for this strategy is for backfilling the old data.\n\nParameters and Expression Language Support\n\nIn Maestro, parameters play an important role. Maestro supports dynamic parameters with code injection, which is super useful and powerful. This feature significantly enhances the flexibility and dynamism of workflows, allowing using parameters to control execution logic and enable state sharing between workflows and their steps, as well as between upstream and downstream steps. Together with other Maestro features, it makes the defining of workflows dynamic and enables users to define parameterized workflows for complex use cases.\n\nHowever, code injection introduces significant security and safety concerns. For example, users might unintentionally write an infinite loop that creates an array and appends items to it, eventually crashing the server with out-of-memory (OOM) issues. While one approach could be to ask users to embed the injected code within their business logic instead of the workflow definition, this would impose additional work on users and tightly couple their business logic with the workflow. In certain cases, this approach blocks users to design some complex parameterized workflows.\n\nTo mitigate these risks and assist users to build parameterized workflows, we developed our own customized expression language parser, a simple, secure, and safe expression language (SEL). SEL supports code injection while incorporating validations during syntax tree parsing to protect the system. It leverages the Java Security Manager to restrict access, ensuring a secure and controlled environment for code execution.\n\nSimple, Secure, and Safe Expression Language (SEL)\n\nSEL is a homemade simple, secure, and safe expression language (SEL) to address the risks associated with code injection within Maestro parameterized workflows. It is a simple expression language and the grammar and syntax follow JLS (Java Language Specifications). SEL supports a subset of JLS, focusing on Maestro use cases. For example, it supports data types for all Maestro parameter types, raising errors, datetime handling, and many predefined utility methods. SEL also includes additional runtime checks, such as loop iteration limits, array size checks, object memory size limits and so on, to enhance security and reliability. For more details about SEL, please refer to the Maestro GitHub documentation.\n\nOutput Parameters\n\nTo further enhance parameter support, Maestro allows for callable step execution, which returns output parameters from user execution back to the system. The output data is transmitted to Maestro via its REST API, ensuring that the step runtime does not have direct access to the Maestro database. This approach significantly reduces security concerns.\n\nParameterized Workflows\n\nThanks to the powerful parameter support, users can easily create parameterized workflows in addition to static ones. Users enjoy defining parameterized workflows because they are easy to manage and troubleshoot while being powerful enough to solve complex use cases.\n\nStatic workflows are simple and easy to use but come with limitations. Often, users have to duplicate the same workflow multiple times to accommodate minor changes. Additionally, workflow and jobs cannot share the states without using parameters.\n\nOn the other hand, completely dynamic workflows can be challenging to manage and support. They are difficult to debug or troubleshoot and hard to be reused by others.\n\nParameterized workflows strike a balance by being initialized step by step at runtime based on user defined parameters. This approach provides great flexibility for users to control the execution at runtime while remaining easy to manage and understand.\n\nAs we described in the previous Maestro blog post, parameter support enables the creation of complex parameterized workflows, such as backfill data pipelines.\n\nWorkflow Execution Patterns\n\nMaestro provides multiple useful building blocks that allow users to easily define dataflow patterns or other workflow patterns. It provides support for common patterns directly within the Maestro engine. Direct engine support not only enables us to optimize these patterns but also ensures a consistent approach to implementing them. Next, we will talk about the three major building blocks that Maestro provides.\n\nForeach Support\n\nIn Maestro, the foreach pattern is modeled as a dedicated step within the original workflow definition. Each iteration of the foreach loop is internally treated as a separate workflow instance, which scales similarly as any other Maestro workflow based on the step executions (i.e. a sub-graph) defined within the foreach definition block. The execution of sub-graph within a foreach step is delegated to a separate workflow instance. Foreach step then monitors and collects the status of these foreach workflow instances, each managing the execution of a single iteration. For more details, please refer to our previous Maestro blog post.\n\nThe foreach pattern is frequently used to repeatedly run the same jobs with different parameters, such as data backfilling or machine learning model tuning. It would be tedious and time consuming to request users to explicitly define each iteration in the workflow definition (potentially hundreds of thousands of iterations). Additionally, users would need to create new workflows if the foreach range changes, further complicating the process.\n\nConditional Branch Support\n\nThe conditional branch feature allows subsequent steps to run only if specific conditions in the upstream step are met. These conditions are defined using the SEL expression language, which is evaluated at runtime. Combined with other building blocks, users can build powerful workflows, e.g. doing some remediation if the audit check step fails and then run the job again.\n\nSubworkflow Support\n\nThe subworkflow feature allows a workflow step to run another workflow, enabling the sharing of common functions across multiple workflows. This effectively enables “workflow as a function” and allows users to build a graph of workflows. For example, we have observed complex workflows consisting of hundreds of subworkflows to process data across hundreds tables, where subworkflows are provided by multiple teams.\n\nThese patterns can be combined together to build composite patterns for complex workflow use cases. For instance, we can loop over a set of subworkflows or run nested foreach loops. One example that Maestro users developed is an auto-recovery workflow that utilizes both conditional branch and subworkflow features to handle errors and retry jobs automatically.\n\nIn this example, subworkflow `job1` runs another workflow consisting of extract-transform-load (ETL) and audit jobs. Next, a status check job leverages the Maestro parameter and SEL support to retrieve the status of the previous job. Based on this status, it can decide whether to complete the workflow or to run a recovery job to address any data issues. After resolving the issue, it then executes subworkflow `job2`, which runs the same workflow as subworkflow `job1`.\n\nStep Runtime and Step Parameter\n\nStep Runtime Interface\n\nIn Maestro, we use step runtime to describe a job at execution time. The step runtime interface defines two pieces of information:\n\nA set of basic APIs to control the behavior of a step instance at execution runtime. Some simple data structures to track step runtime state and execution result.\n\nMaestro offers a few step runtime implementations such as foreach step runtime, subworkflow step runtime (mentioned in previous section). Each implementation defines its own logic for start, execute and terminate operations. At runtime, these operations control the way to initialize a step instance, perform the business logic and terminate the execution under certain conditions (i.e. manual intervention by users).\n\nAlso, Maestro step runtime internally keeps track of runtime state as well as the execution result of the step. The runtime state is used to determine the next state transition of the step and tell if it has failed or terminated. The execution result hosts both step artifacts and the timeline of step execution history, which are accessible by subsequent steps.\n\nStep Parameter Merging\n\nTo control step behavior in a dynamic way, Maestro supports both runtime parameters and tags injection in step runtime. This makes a Maestro step more flexible to absorb runtime changes (i.e. overridden parameters) before actually being started. Maestro internally maintains a step parameter map that is initially empty and is updated by merging step parameters in the order below:\n\nDefault General Parameters : Parameters merging starts from default parameters that in general every step should have. For example, workflow_instance_id, step_instance_uuid, step_attempt_id and step_id are required parameters for each maestro step. They are internally reserved by maestro and cannot be passed by users.\n\n: Parameters merging starts from default parameters that in general every step should have. For example, workflow_instance_id, step_instance_uuid, step_attempt_id and step_id are required parameters for each maestro step. They are internally reserved by maestro and cannot be passed by users. Injected Parameters : Maestro then merges injected parameters (if present) into the parameter map. The injected parameters come from step runtime, which are dynamically generated based on step schema. Each type of step can have its own schema with specific parameters associated with this step. The step schema can evolve independently with no need to update Maestro code.\n\n: Maestro then merges injected parameters (if present) into the parameter map. The injected parameters come from step runtime, which are dynamically generated based on step schema. Each type of step can have its own schema with specific parameters associated with this step. The step schema can evolve independently with no need to update Maestro code. Default Typed Parameters : After injecting runtime parameters, Maestro tries to merge default parameters that are related to a specific type of step. For example, foreach step has loop_params and loop_index default parameters which are internally set by maestro and used for foreach step only.\n\n: After injecting runtime parameters, Maestro tries to merge default parameters that are related to a specific type of step. For example, foreach step has loop_params and loop_index default parameters which are internally set by maestro and used for foreach step only. Workflow and Step Info Parameters : These parameters contain information about step and the workflow it belongs to. This can be identity information, i.e. workflow_id and will be merged to step parameter map if present.\n\n: These parameters contain information about step and the workflow it belongs to. This can be identity information, i.e. workflow_id and will be merged to step parameter map if present. Undefined New Parameters : When starting or restarting a maestro workflow instance, users can specify new step parameters that are not present in initial step definition. ParamsManager merges these parameters to ensure they are available at execution time.\n\n: When starting or restarting a maestro workflow instance, users can specify new step parameters that are not present in initial step definition. ParamsManager merges these parameters to ensure they are available at execution time. Step Definition Parameters : These step parameters are defined by users at definition time and get merged if they are not empty.\n\n: These step parameters are defined by users at definition time and get merged if they are not empty. Run and Restart Parameters: When starting or restarting a maestro workflow instance, users can override defined parameters by providing run or restart parameters. These two types of parameters are merged at the end so that step runtime can see the most recent and accurate parameter space.\n\nThe parameters merging logic can be visualized in the diagram below.\n\nStep Dependencies and Signals\n\nSteps in the Maestro execution workflow graph can express execution dependencies using step dependencies. A step dependency specifies the data-related conditions required by a step to start execution. These conditions are usually defined based on signals, which are pieces of messages carrying information such as parameter values and can be published through step outputs or external systems like SNS or Kafka messages.\n\nSignals in Maestro serve both signal trigger pattern and signal dependencies (a publisher-subscriber) pattern. One step can publish an output signal (a sample example) that can unblock the execution of multiple other steps that depend on it. A signal definition includes a list of mapped parameters, allowing Maestro to perform “signal matching” on a subset of fields. Additionally, Maestro supports signal operators like <, >, etc., on signal parameter values.\n\nNetflix has built various abstractions on top of the concept of signals. For instance, a ETL workflow can update a table with data and send signals that unblock steps in downstream workflows dependent on that data. Maestro supports “signal lineage,” which allows users to navigate all historical instances of signals and the workflow steps that match (i.e. publishing or consuming) those signals. Signal triggering guarantees exactly-once execution for the workflow subscribing a signal or a set of joined signals. This approach is efficient, as it conserves resources by only executing the workflow or step when the specified conditions in the signals are met. A signal service is implemented for those advanced abstractions. Please refer to the Maestro blog for further details on it.\n\nBreakpoint\n\nMaestro allows users to set breakpoints on workflow steps, functioning similarly to code-level breakpoints in an IDE. When a workflow instance executes and reaches a step with a breakpoint, that step enters a “paused” state. This halts the workflow graph’s progression until a user manually resumes from the breakpoint. If multiple instances of a workflow step are paused at a breakpoint, resuming one instance will only affect that specific instance, leaving the others in a paused state. Deleting the breakpoint will cause all paused step instances to resume.\n\nThis feature is particularly useful during the initial development of a workflow, allowing users to inspect step executions and output data. It is also beneficial when running a step multiple times in a “foreach” pattern with various input parameters. Setting a single breakpoint on a step will cause all iterations of the foreach loop to pause at that step for debugging purposes. Additionally, the breakpoint feature allows human intervention during the workflow execution and can also be used for other purposes, e.g. supporting mutating step states while the workflow is running.\n\nTimeline\n\nMaestro includes a step execution timeline, capturing all significant events such as execution state machine changes and the reasoning behind them. This feature is useful for debugging, providing insights into the status of a step. For example, it logs transitions such as “Created” and “Evaluating params”, etc. An example of a timeline is included here for reference. The implemented step runtimes can add the timeline events into the timeline to surface the execution information to the end users.\n\nRetry Policies\n\nMaestro supports retry policies for steps that reach a terminal state due to failure. Users can specify the number of retries and configure retry policies, including delays between retries and exponential backoff strategies, in addition to fixed interval retries. Maestro distinguishes between two types of retries: “platform” and “user.” Platform retries address platform-level errors unrelated to user logic, while user retries are for user-defined conditions. Each type can have its own set of retry policies.\n\nAutomatic retries are beneficial for handling transient errors that can be resolved without user intervention. Maestro provides the flexibility to set retries to zero for non-idempotent steps to avoid retry. This feature ensures that users have control over how retries are managed based on their specific requirements.\n\nAggregated View\n\nBecause a workflow instance can have multiple runs, it is important for users to see an aggregated state of all steps in the workflow instance. Aggregated view is computed by merging base aggregated view with current runs instance step statuses. For example, as you can see on the figure below simulating a simple case, there is a first run, where step1 and step2 succeeded, step3 failed, and step4 and step5 have not started. When the user restarts the run, the run starts from step3 in run 2 with step1 and step2 skipped which succeeded in the previous run. After all steps succeed, the aggregated view shows the run states for all steps.\n\nRollup\n\nRollup provides a high-level summary of a workflow instance, detailing the status of each step and the count of steps in each status. It flattens steps across the current instance and any nested non-inline workflows like subworkflows or foreach steps. For instance, if a successful workflow has three steps, one of which is a subworkflow corresponding to a five-step workflow, the rollup will indicate that seven steps succeeded. Only leaf steps are counted in the rollup, as other steps serve merely as pointers to concrete workflows.\n\nRollup also retains references to any non-successful steps, offering a clear overview of step statuses and facilitating easy navigation to problematic steps, even within nested workflows. The aggregated rollup for a workflow instance is calculated by combining the current run’s runtime data with a base rollup. The current state is derived from the statuses of active steps, including aggregated rollups for foreach and subworkflow steps. The base rollup is established when the workflow instance begins and includes statuses of inline steps (excluding foreach and subworkflows) from the previous run that are not part of the current run.\n\nFor subworkflow steps, the rollup simply reflects the rollup of the subworkflow instance. For foreach steps, the rollup combines the base rollup of the foreach step with the current state rollup. The base is derived from the previous run’s aggregated rollup, excluding the iterations to be restarted in the new run. The current state is periodically updated by aggregating rollups of running iterations until all iterations reach a terminal state.\n\nDue to these processes, the rollup model is eventually consistent. While the figure below illustrates a straightforward example of rollup, the calculations can become complex and recursive, especially with multiple levels of nested foreaches and subworkflows.\n\nMaestro Event Publishing\n\nWhen workflow definition, workflow instance or step instance is changed, Maestro generates an event, processes it internally and publishes the processed event to external system(s). Maestro has both internal and external events. The internal event tracks changes within the life cycle of workflow, workflow instance or step instance. It is published to an internal queue and processed within Maestro. After internal events are processed, some of them will be transformed into external event and sent out to the external queue (i.e. SNS, Kafka). The external event carries maestro status change information for downstream services. The event publishing flow is illustrated in the diagram below:\n\nAs shown in the diagram, the Maestro event processor bridges the two aforementioned Maestro events. It listens on the internal queue to get the published internal events. Within the processor, the internal job event is processed based on its type and gets converted to an external event if needed. The notification publisher at the end emits the external event so that downstream services can consume.\n\nThe downstream services are mostly event-driven. The Maestro event carries the most useful message for downstream services to capture different changes in Maestro. In general, these changes can be classified into two categories: workflow change and instance status change. The workflow change event is associated with actions at workflow level, i.e definition or properties of a workflow has changed. Meanwhile, instance status change tracks status transition on workflow instance or step instance.\n\nGet Started with Maestro\n\nMaestro has been extensively used within Netflix, and today, we are excited to make the Maestro source code publicly available. We hope that the scalability and usability that Maestro offers can expedite workflow development outside Netflix. We invite you to try Maestro, use it within your organization, and contribute to its development.\n\nYou can find the Maestro code repository at github.com/Netflix/maestro. If you have any questions, thoughts, or comments about Maestro, please feel free to create a GitHub issue in the Maestro repository. We are eager to hear from you.\n\nWe are taking workflow orchestration to the next level and constantly solving new problems and challenges, please stay tuned for updates. If you are passionate about solving large scale orchestration problems, please join us.\n\nAcknowledgements\n\nThanks to other Maestro team members, Binbing Hou, Zhuoran Dong, Brittany Truong, Deepak Ramalingam, Moctar Ba, for their contributions to the Maestro project. Thanks to our Product Manager Ashim Pokharel for driving the strategy and requirements. We’d also like to thank Andrew Seier, Romain Cledat, Olek Gorajek, and other stunning colleagues at Netflix for their contributions to the Maestro project. We also thank Prashanth Ramdas, Eva Tse, David Noor, Charles Smith and other leaders of Netflix engineering organizations for their constructive feedback and suggestions on the Maestro project.", "label": "non_personal"}
{"title": "Java 21 Virtual Threads - Dude, Where’s My Lock?", "url": "https://netflixtechblog.com/java-21-virtual-threads-dude-wheres-my-lock-3052540e231d?source=collection_home---4------19-----------------------", "content": "Java 21 Virtual Threads - Dude, Where’s My Lock?\n\nGetting real with virtual threads Netflix Technology Blog 10 min read · Jul 29, 2024 -- 35 Listen Share\n\nBy Vadim Filanovsky, Mike Huang, Danny Thomas and Martin Chalupa\n\nIntro\n\nNetflix has an extensive history of using Java as our primary programming language across our vast fleet of microservices. As we pick up newer versions of Java, our JVM Ecosystem team seeks out new language features that can improve the ergonomics and performance of our systems. In a recent article, we detailed how our workloads benefited from switching to generational ZGC as our default garbage collector when we migrated to Java 21. Virtual threads is another feature we are excited to adopt as part of this migration.\n\nFor those new to virtual threads, they are described as “lightweight threads that dramatically reduce the effort of writing, maintaining, and observing high-throughput concurrent applications.” Their power comes from their ability to be suspended and resumed automatically via continuations when blocking operations occur, thus freeing the underlying operating system threads to be reused for other operations. Leveraging virtual threads can unlock higher performance when utilized in the appropriate context.\n\nIn this article we discuss one of the peculiar cases that we encountered along our path to deploying virtual threads on Java 21.\n\nThe problem\n\nNetflix engineers raised several independent reports of intermittent timeouts and hung instances to the Performance Engineering and JVM Ecosystem teams. Upon closer examination, we noticed a set of common traits and symptoms. In all cases, the apps affected ran on Java 21 with SpringBoot 3 and embedded Tomcat serving traffic on REST endpoints. The instances that experienced the issue simply stopped serving traffic even though the JVM on those instances remained up and running. One clear symptom characterizing the onset of this issue is a persistent increase in the number of sockets in closeWait state as illustrated by the graph below:\n\nCollected diagnostics\n\nSockets remaining in closeWait state indicate that the remote peer closed the socket, but it was never closed on the local instance, presumably because the application failed to do so. This can often indicate that the application is hanging in an abnormal state, in which case application thread dumps may reveal additional insight.\n\nIn order to troubleshoot this issue, we first leveraged our alerts system to catch an instance in this state. Since we periodically collect and persist thread dumps for all JVM workloads, we can often retroactively piece together the behavior by examining these thread dumps from an instance. However, we were surprised to find that all our thread dumps show a perfectly idle JVM with no clear activity. Reviewing recent changes revealed that these impacted services enabled virtual threads, and we knew that virtual thread call stacks do not show up in jstack -generated thread dumps. To obtain a more complete thread dump containing the state of the virtual threads, we used the “ jcmd Thread.dump_to_file ” command instead. As a last-ditch effort to introspect the state of JVM, we also collected a heap dump from the instance.\n\nAnalysis\n\nThread dumps revealed thousands of “blank” virtual threads:\n\n#119821 \"\" virtual\n\n\n\n#119820 \"\" virtual\n\n\n\n#119823 \"\" virtual\n\n\n\n#120847 \"\" virtual\n\n\n\n#119822 \"\" virtual\n\n...\n\nThese are the VTs (virtual threads) for which a thread object is created, but has not started running, and as such, has no stack trace. In fact, there were approximately the same number of blank VTs as the number of sockets in closeWait state. To make sense of what we were seeing, we need to first understand how VTs operate.\n\nA virtual thread is not mapped 1:1 to a dedicated OS-level thread. Rather, we can think of it as a task that is scheduled to a fork-join thread pool. When a virtual thread enters a blocking call, like waiting for a Future , it relinquishes the OS thread it occupies and simply remains in memory until it is ready to resume. In the meantime, the OS thread can be reassigned to execute other VTs in the same fork-join pool. This allows us to multiplex a lot of VTs to just a handful of underlying OS threads. In JVM terminology, the underlying OS thread is referred to as the “carrier thread” to which a virtual thread can be “mounted” while it executes and “unmounted” while it waits. A great in-depth description of virtual thread is available in JEP 444.\n\nIn our environment, we utilize a blocking model for Tomcat, which in effect holds a worker thread for the lifespan of a request. By enabling virtual threads, Tomcat switches to virtual execution. Each incoming request creates a new virtual thread that is simply scheduled as a task on a Virtual Thread Executor. We can see Tomcat creates a VirtualThreadExecutor here.\n\nTying this information back to our problem, the symptoms correspond to a state when Tomcat keeps creating a new web worker VT for each incoming request, but there are no available OS threads to mount them onto.\n\nWhy is Tomcat stuck?\n\nWhat happened to our OS threads and what are they busy with? As described here, a VT will be pinned to the underlying OS thread if it performs a blocking operation while inside a synchronized block or method. This is exactly what is happening here. Here is a relevant snippet from a thread dump obtained from the stuck instance:\n\n#119515 \"\" virtual\n\njava.base/jdk.internal.misc.Unsafe.park(Native Method)\n\njava.base/java.lang.VirtualThread.parkOnCarrierThread(VirtualThread.java:661)\n\njava.base/java.lang.VirtualThread.park(VirtualThread.java:593)\n\njava.base/java.lang.System$2.parkVirtualThread(System.java:2643)\n\njava.base/jdk.internal.misc.VirtualThreads.park(VirtualThreads.java:54)\n\njava.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:219)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:754)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:990)\n\njava.base/java.util.concurrent.locks.ReentrantLock$Sync.lock(ReentrantLock.java:153)\n\njava.base/java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:322)\n\nzipkin2.reporter.internal.CountBoundedQueue.offer(CountBoundedQueue.java:54)\n\nzipkin2.reporter.internal.AsyncReporter$BoundedAsyncReporter.report(AsyncReporter.java:230)\n\nzipkin2.reporter.brave.AsyncZipkinSpanHandler.end(AsyncZipkinSpanHandler.java:214)\n\nbrave.internal.handler.NoopAwareSpanHandler$CompositeSpanHandler.end(NoopAwareSpanHandler.java:98)\n\nbrave.internal.handler.NoopAwareSpanHandler.end(NoopAwareSpanHandler.java:48)\n\nbrave.internal.recorder.PendingSpans.finish(PendingSpans.java:116)\n\nbrave.RealSpan.finish(RealSpan.java:134)\n\nbrave.RealSpan.finish(RealSpan.java:129)\n\nio.micrometer.tracing.brave.bridge.BraveSpan.end(BraveSpan.java:117)\n\nio.micrometer.tracing.annotation.AbstractMethodInvocationProcessor.after(AbstractMethodInvocationProcessor.java:67)\n\nio.micrometer.tracing.annotation.ImperativeMethodInvocationProcessor.proceedUnderSynchronousSpan(ImperativeMethodInvocationProcessor.java:98)\n\nio.micrometer.tracing.annotation.ImperativeMethodInvocationProcessor.process(ImperativeMethodInvocationProcessor.java:73)\n\nio.micrometer.tracing.annotation.SpanAspect.newSpanMethod(SpanAspect.java:59)\n\njava.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n\njava.base/java.lang.reflect.Method.invoke(Method.java:580)\n\norg.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethodWithGivenArgs(AbstractAspectJAdvice.java:637)\n\n...\n\nIn this stack trace, we enter the synchronization in brave.RealSpan.finish(RealSpan.java:134) . This virtual thread is effectively pinned — it is mounted to an actual OS thread even while it waits to acquire a reentrant lock. There are 3 VTs in this exact state and another VT identified as “ <redacted> @DefaultExecutor - 46542 ” that also follows the same code path. These 4 virtual threads are pinned while waiting to acquire a lock. Because the app is deployed on an instance with 4 vCPUs, the fork-join pool that underpins VT execution also contains 4 OS threads. Now that we have exhausted all of them, no other virtual thread can make any progress. This explains why Tomcat stopped processing the requests and why the number of sockets in closeWait state keeps climbing. Indeed, Tomcat accepts a connection on a socket, creates a request along with a virtual thread, and passes this request/thread to the executor for processing. However, the newly created VT cannot be scheduled because all of the OS threads in the fork-join pool are pinned and never released. So these newly created VTs are stuck in the queue, while still holding the socket.\n\nWho has the lock?\n\nNow that we know VTs are waiting to acquire a lock, the next question is: Who holds the lock? Answering this question is key to understanding what triggered this condition in the first place. Usually a thread dump indicates who holds the lock with either “ - locked <0x…> (at …) ” or “ Locked ownable synchronizers ,” but neither of these show up in our thread dumps. As a matter of fact, no locking/parking/waiting information is included in the jcmd -generated thread dumps. This is a limitation in Java 21 and will be addressed in the future releases. Carefully combing through the thread dump reveals that there are a total of 6 threads contending for the same ReentrantLock and associated Condition . Four of these six threads are detailed in the previous section. Here is another thread:\n\n#119516 \"\" virtual\n\njava.base/java.lang.VirtualThread.park(VirtualThread.java:582)\n\njava.base/java.lang.System$2.parkVirtualThread(System.java:2643)\n\njava.base/jdk.internal.misc.VirtualThreads.park(VirtualThreads.java:54)\n\njava.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:219)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:754)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:990)\n\njava.base/java.util.concurrent.locks.ReentrantLock$Sync.lock(ReentrantLock.java:153)\n\njava.base/java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:322)\n\nzipkin2.reporter.internal.CountBoundedQueue.offer(CountBoundedQueue.java:54)\n\nzipkin2.reporter.internal.AsyncReporter$BoundedAsyncReporter.report(AsyncReporter.java:230)\n\nzipkin2.reporter.brave.AsyncZipkinSpanHandler.end(AsyncZipkinSpanHandler.java:214)\n\nbrave.internal.handler.NoopAwareSpanHandler$CompositeSpanHandler.end(NoopAwareSpanHandler.java:98)\n\nbrave.internal.handler.NoopAwareSpanHandler.end(NoopAwareSpanHandler.java:48)\n\nbrave.internal.recorder.PendingSpans.finish(PendingSpans.java:116)\n\nbrave.RealScopedSpan.finish(RealScopedSpan.java:64)\n\n...\n\nNote that while this thread seemingly goes through the same code path for finishing a span, it does not go through a synchronized block. Finally here is the 6th thread:\n\n#107 \"AsyncReporter <redacted>\"\n\njava.base/jdk.internal.misc.Unsafe.park(Native Method)\n\njava.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:221)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:754)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1761)\n\nzipkin2.reporter.internal.CountBoundedQueue.drainTo(CountBoundedQueue.java:81)\n\nzipkin2.reporter.internal.AsyncReporter$BoundedAsyncReporter.flush(AsyncReporter.java:241)\n\nzipkin2.reporter.internal.AsyncReporter$Flusher.run(AsyncReporter.java:352)\n\njava.base/java.lang.Thread.run(Thread.java:1583)\n\nThis is actually a normal platform thread, not a virtual thread. Paying particular attention to the line numbers in this stack trace, it is peculiar that the thread seems to be blocked within the internal acquire() method after completing the wait. In other words, this calling thread owned the lock upon entering awaitNanos() . We know the lock was explicitly acquired here. However, by the time the wait completed, it could not reacquire the lock. Summarizing our thread dump analysis:\n\nThere are 5 virtual threads and 1 regular thread waiting for the lock. Out of those 5 VTs, 4 of them are pinned to the OS threads in the fork-join pool. There’s still no information on who owns the lock. As there’s nothing more we can glean from the thread dump, our next logical step is to peek into the heap dump and introspect the state of the lock.\n\nInspecting the lock", "label": "non_personal"}
{"title": "Explore the latest updates on Google Wallet", "url": "https://developers.googleblog.com/en/explore-the-latest-updates-google-wallet-io-25/", "content": "Last year, we were thrilled to expand access to Google Wallet for users in more than 90 countries and territories. Recently we expanded Google Wallet adding +50 more countries, allowing users to view and use digital passes in the app and on the web. And we've worked hard to make Google Wallet even more robust, flexible, and ultimately, more feature-rich for you, our developer community.\n\nLet’s dive into the exciting new capabilities we’ve announced at our I/O session this year.\n\n\n\nDigital IDs: A foundation of trust, ease, and interoperability\n\nDigital IDs in Google Wallet are live today, built with trust, ease of use, and interoperability as our top priorities. We're committed to bringing more digital IDs to Google Wallet to support our users across the globe. Now, it’s easier to prove age and identity with Google Wallet.\n\nExpanding Availability: Residents in Arkansas, Montana, Puerto Rico and West Virginia will soon be able to save their government-issued digital IDs to Google Wallet. And in Arizona, Georgia, Maryland and New Mexico, users will also be able to use their mobile IDs at the DMV for improved and streamlined customer experiences.\n\nU.K. Passport Support: U.K. passport holders will soon be able to create digital ID passes with their U.K. passports and securely and conveniently store them in Google Wallet. At launch, we’re partnering with Rail Delivery Group, which will offer train travellers the opportunity to use their digital ID to verify that they meet the eligibility criteria for select Railcards on its Railcard retailing platform railcard.co.uk.\n\nNew use cases: New use cases are on the way in collaboration with our strong partner ecosystem. Soon, you'll be able to use your digital ID to recover Amazon accounts, access online health services with CVS Health and MyChart by Epic, verify profiles on platforms like Uber and more.\n\nIntroducing the Digital Credentials API: To empower you to leverage these digital IDs, we collaborated with ecosystem partners in the W3C to develop our Digital Credentials API. This unified and secure framework allows apps and websites to request verifiable proof of age or identity directly from any digital wallet on a user's device.\n\n\n\nConnecting with families: Google Wallet enabled for kids\n\nWe're very excited to provide parents and guardians a way to allow their children to access Google Wallet with appropriate supervision.\n\nParents and guardians in select countries can now allow their kids to tap and pay in stores, plus keep supported passes like event tickets, library cards, and gift cards, all in one place, on their Android devices. Safety is key, so parents have controls with Family Link. They will get notified by email about every transaction, and can easily track recent purchases, remove payment cards, and turn off passes access in Family Link.\n\nBest of all, there are no changes in the Google Wallet API to support this enhancement.\n\n\n\nElevating user engagement: more granular notifications\n\nLast year, we expanded mobile push notifications for the Google Wallet API, empowering you to deliver timely updates to your users. This year, we're taking it a step further with the introduction of field update notifications.\n\nImagine being able to trigger a push notification not just on a general update, but specifically when a particular field within a pass changes. For example, when a user's points balance crosses a threshold, or when their tier status is upgraded, a push notification can be triggered which will notify them that the data on their pass has changed. This granularity allows for a much more engaging and user-centric experience, driving higher engagement and utility on the pass.\n\n\n\nProximity power: introducing Nearby Passes Notifications\n\nSpeaking of notifications, we’re really excited to announce that we will support Nearby Passes notifications for the Google Wallet API. If enabled by the user, this feature is designed to provide timely and relevant information by alerting users about pertinent passes, such as loyalty cards, offers, boarding passes, or event tickets, when they approach a designated point of interest.\n\nThe Google Wallet app sends a contextual notification when the user is near a specific location. This notification serves as a direct gateway, allowing users to seamlessly access the associated pass with a single tap. This direct access promotes a more fluid and intuitive interaction with the stored passes, encouraging users to leverage the wallet's capabilities more effectively.\n\nTo ensure user control and flexibility, we’ve introduced two new toggles to help users control their notification experience. The first is on the pass details screen, that allows users to turn on or off notifications from that specific pass. This applies to all notifications related to your pass, including field updates, and nearby passes notifications. The second is through the Nearby Passes notifications channel that allows users to control whether they receive nearby passes notifications. This empowers users to tailor their notification settings based on their specific needs and preferences.\n\n\n\nBeyond transactions: unlocking Value Added Opportunities\n\nEngaging users goes beyond just notifications. To address this, we’re introducing Value Added Opportunities which empowers you to integrate personalized modules directly into your passes, showcasing relevant deals, promotions, and additional services.\n\nThis is a significant step towards transforming passes into a more dynamic engagement surface for you and your users. By highlighting these value-added benefits, such as exclusive offers or upgrade options, you can guide users back to your app or website, creating a dynamic gateway for ongoing user interaction.\n\n\n\nBridging the gap: introducing the Pass Upgrade experience\n\nUsers have saved millions of passes by manually adding their loyalty cards to Google Wallet. These passes are unlinked from their merchant accounts, and, as a developer, you can’t update, or engage users on these passes. We’ve now introduced a Pass Upgrade experience that prompts users to sign into their merchant account and save a linked version of the pass. All you need to do is integrate with the Wallet API’s User Loyalty Program Enrollment feature, and ensure that users can save the linked pass once they successfully sign in. We’ll be expanding on this feature in the future to enable users who have added a pass using our “everything else” feature to link to the merchant account as well.\n\n\n\nSeamless journeys: enhancements for travel\n\nAt last year's I/O, we announced Auto Linked Passes, allowing you to add an additional related pass automatically to your users’ Google Wallet provided they already have an existing pass issued by you. Now, we're happy to announce an expansion of this feature for airlines. Developers from airlines that integrate loyalty cards for their frequent flyer programs with the Google Wallet API can automatically push boarding passes to their users’ wallets once they check in for a flight.\n\nGoogle Wallet users already benefit from streamlined travel on open-loop EMV transit systems, using tokenized payment methods for seamless fare transactions. Our solution further enhances this experience by providing riders with detailed journey and fare construction details. The tokenized open loop payment card acts as a bridge between user payments and transit systems, facilitating this communication.\n\nBuilding on this foundation, we're excited to announce the upcoming expansion of Google Transit Insights to support specific pass types sales, such as season passes, directly linked to the users' tokenized open loop payment card. With this new capability, developers will soon be able to leverage the Google Wallet APIs to implement seamless pass purchasing and management, eliminating the need for separate transit cards.\n\nUsers of Google Wallet will now see real-time transit pass updates, such as on-time or delayed train status, directly on their passes. This enhanced experience is powered by the Google Wallet API's new live status support and a seamless integration with Google Maps. Train operators can enable this feature by ensuring their tickets contain the required fields and providing a real-time GTFS feed, making it accessible to major operators worldwide.\n\n\n\nPersonalization and security: Secure Private Images\n\nFor some use cases, it’s important for you to create a pass that includes a profile photo of the user you’re distributing the pass to. To achieve this, we’re enabling Secure Private Images on passes.\n\nWith this feature, you’re able to define passes that include images that are only accessible to the holder of the pass. For example, this allows you to create digital business cards, membership passes, or event tickets with the user profile image on it.\n\nPlease note that this feature can’t be used for official identity verification.\n\n\n\nLearn more and build with Google Wallet\n\nGoogle Wallet continues to evolve, offering developers powerful new capabilities to create richer, more engaging user experiences. From expanding digital identity features like the Digital Credentials API and secure private images, to enhancing user engagement with granular notifications and Value Added Opportunities, and streamlining travel with expanded Auto Linked Passes and real-time transit updates, these features are designed to help you build innovative solutions that connect more deeply with your users and unlock new value within the Google Wallet platform.\n\nPlease take a look at the following resources to learn more:\n\nLearn everything about Google Wallet in the developer documentation website.\n\nStay tuned with upcoming and past events at our events page.\n\nTry one of our codelabs to have a hands-on code experience.\n\nUse one of our client libraries for your favorite language/platform.", "label": "non_personal"}
{"title": "Bringing Gemini intelligence to Google Home APIs", "url": "https://developers.googleblog.com/en/bringing-gemini-intelligence-to-google-home-apis/", "content": "The smart home is rapidly evolving into an intuitive ecosystem to make life easier, and its next era will be powered by Gemini and the Home APIs. This isn't just about connected devices; it's about creating effortless experiences. With the Home APIs, our goal has been to empower all developers to build innovative devices and experiences for the home. Now, with Gemini in the Home APIs, we're taking the next step: bringing the best of Google's AI directly to you. We're moving beyond simple device control to create an effortless smart home that truly understands, adapts, and responds to your users' needs. At Google I/O 2024, we announced the Home APIs, providing app developers with access to over 600M devices. We are excited to share that our ecosystem has grown even more to over 750M devices that developers now have access to along with Google's hubs and Matter infrastructure, and an automation engine powered by Google intelligence. We’ve spent time rolling it out to a few early access partners, our Android and iOS SDKs are in public developer beta, and some developers have already leveraged the Home APIs to release new apps on Android.\n\nPartner experiences built with Home APIs Last year, we shared the innovative new ways partners like ADT, LG, and Eve built on Google Home, and now there are even more partners showcasing how Home APIs are making their customer experience even better:\n\nFirst Alert Control your smoke alarm from the First Alert app or the Google Home app and seamlessly interconnect with your existing Nest Protects.\n\nYale Yale’s upcoming Matter lock, the successor to the Nest x Yale lock, takes advantage of the best-in-class lock features in the Google Home app, built using the Home APIs.\n\nCync Imagine your home automatically adjusting lighting and fan settings to ensure your pet's comfort when you're away. Cync is making this a reality.\n\niRobot Select iRobot Roomba® robots can create automations using Google Home presence sensing, so they can automatically clean your home when you leave the house, ensuring a spotless return.\n\nMotorola Moto Tag You can create custom smart home routines triggered by simple tag interactions, offering unparalleled personalization.\n\nTuya Smart Tuya Smart is enhancing seamless interoperability. Now, users can easily set up a Matter device and control devices connected to Google directly in Tuya Smart app.\n\nBringing your cameras to life with Gemini-powered Home APIs Last fall, we introduced Gemini-powered camera features in public preview in the Google Home app, allowing users to ask natural questions like, “Did the kids leave their bikes in the driveway?” and instantly get relevant video clips. Now we are bringing those camera experiences directly to developers too.\n\nWe're including the standard camera features you'd expect – like live streaming, event history access, two-way talk capabilities, and camera settings. But we're going further by integrating the Gemini-powered intelligence that our users love, such as AI descriptions and the ability to search camera history, making it easier to quickly identify what you are looking for in your camera history, keeping you and your family safer.\n\nMaking automation effortless with Gemini Figuring out the perfect automation to help improve your home experience and implementing it can be a daunting task many users don’t want to undertake. So, we’re introducing new Gemini-powered features to the Automations API designed to make creating powerful routines easier than ever: Suggested Automations: Gemini intelligently analyzes the devices in a user's home and proactively suggests potentially useful automations they might not have thought of.\n\nHelp me create: Building automations becomes as simple as a conversation. Users can tell Gemini what they want to achieve using natural language, and the automation is drafted for them.\n\nNew Automation Starters: We're adding more sophisticated triggers based on dates and weather conditions, allowing automations to respond more dynamically to the complexities of real life.\n\nThese new features will enable you to offer unprecedented Gemini-powered intelligent capabilities to your users more quickly than ever before.\n\nGemini across the Google Home surfaces The benefits don't stop within your app. When you integrate your devices using the Google Home APIs, they can participate in Gemini-powered experiences across Google's surfaces.\n\nFor instance, Google Home users, while in the Gemini app, can control and inquire about their smart home devices using natural language. We've also previewed Gemini enhancing the voice experience on smart speakers, smart displays, and Google TV, enabling more natural interaction, deeper exploration of topics, device control, and even voice-based automation creation. And we are testing a Home Summary Widget on Pixel with a select set of users, providing insights about your home without having to open an app!", "label": "non_personal"}
{"title": "Improve Your Next Experiment by Learning Better Proxy Metrics From Past Experiments", "url": "https://netflixtechblog.com/improve-your-next-experiment-by-learning-better-proxy-metrics-from-past-experiments-64c786c2a3ac?source=collection_home---4------18-----------------------", "content": "We are excited to share our work on how to learn good proxy metrics from historical experiments at KDD 2024. This work addresses a fundamental question for technology companies and academic researchers alike: how do we establish that a treatment that improves short-term (statistically sensitive) outcomes also improves long-term (statistically insensitive) outcomes? Or, faced with multiple short-term outcomes, how do we optimally trade them off for long-term benefit?\n\nFor example, in an A/B test, you may observe that a product change improves the click-through rate. However, the test does not provide enough signal to measure a change in long-term retention, leaving you in the dark as to whether this treatment makes users more satisfied with your service. The click-through rate is a proxy metric (S, for surrogate, in our paper) while retention is a downstream business outcome or north star metric (Y). We may even have several proxy metrics, such as other types of clicks or the length of engagement after click. Taken together, these form a vector of proxy metrics.\n\nThe goal of our work is to understand the true relationship between the proxy metric(s) and the north star metric — so that we can assess a proxy’s ability to stand in for the north star metric, learn how to combine multiple metrics into a single best one, and better explore and compare different proxies.\n\nSeveral intuitive approaches to understanding this relationship have surprising pitfalls:\n\nLooking only at user-level correlations between the proxy S and north star Y. Continuing the example from above, you may find that users with a higher click-through rate also tend to have a higher retention. But this does not mean that a product change that improves the click-through rate will also improve retention (in fact, promoting clickbait may have the opposite effect). This is because, as any introductory causal inference class will tell you, there are many confounders between S and Y — many of which you can never reliably observe and control for.\n\nContinuing the example from above, you may find that users with a higher click-through rate also tend to have a higher retention. But this does not mean that a product change that improves the click-through rate will also improve retention (in fact, promoting clickbait may have the opposite effect). This is because, as any introductory causal inference class will tell you, there are many confounders between S and Y — many of which you can never reliably observe and control for. Looking naively at treatment effect correlations between S and Y. Suppose you are lucky enough to have many historical A/B tests. Further imagine the ordinary least squares (OLS) regression line through a scatter plot of Y on S in which each point represents the (S,Y)-treatment effect from a previous test. Even if you find that this line has a positive slope, you unfortunately cannot conclude that product changes that improve S will also improve Y. The reason for this is correlated measurement error — if S and Y are positively correlated in the population, then treatment arms that happen to have more users with high S will also have more users with high Y.\n\nBetween these naive approaches, we find that the second one is the easier trap to fall into. This is because the dangers of the first approach are well-known, whereas covariances between estimated treatment effects can appear misleadingly causal. In reality, these covariances can be severely biased compared to what we actually care about: covariances between true treatment effects. In the extreme — such as when the negative effects of clickbait are substantial but clickiness and retention are highly correlated at the user level — the true relationship between S and Y can be negative even if the OLS slope is positive. Only more data per experiment could diminish this bias — using more experiments as data points will only yield more precise estimates of the badly biased slope. At first glance, this would appear to imperil any hope of using existing experiments to detect the relationship.\n\nThis figure shows a hypothetical treatment effect covariance matrix between S and Y (white line; negative correlation), a unit-level sampling covariance matrix creating correlated measurement errors between these metrics (black line; positive correlation), and the covariance matrix of estimated treatment effects which is a weighted combination of the first two (orange line; no correlation).\n\nTo overcome this bias, we propose better ways to leverage historical experiments, inspired by techniques from the literature on weak instrumental variables. More specifically, we show that three estimators are consistent for the true proxy/north-star relationship under different constraints (the paper provides more details and should be helpful for practitioners interested in choosing the best estimator for their setting):\n\nA Total Covariance (TC) estimator allows us to estimate the OLS slope from a scatter plot of true treatment effects by subtracting the scaled measurement error covariance from the covariance of estimated treatment effects. Under the assumption that the correlated measurement error is the same across experiments (homogeneous covariances), the bias of this estimator is inversely proportional to the total number of units across all experiments, as opposed to the number of members per experiment.\n\nestimator allows us to estimate the OLS slope from a scatter plot of true treatment effects by subtracting the scaled measurement error covariance from the covariance of estimated treatment effects. Under the assumption that the correlated measurement error is the same across experiments (homogeneous covariances), the bias of this estimator is inversely proportional to the total number of units across all experiments, as opposed to the number of members per experiment. Jackknife Instrumental Variables Estimation (JIVE) converges to the same OLS slope as the TC estimator but does not require the assumption of homogeneous covariances. JIVE eliminates correlated measurement error by removing each observation’s data from the computation of its instrumented surrogate values.\n\nconverges to the same OLS slope as the TC estimator but does not require the assumption of homogeneous covariances. JIVE eliminates correlated measurement error by removing each observation’s data from the computation of its instrumented surrogate values. A Limited Information Maximum Likelihood (LIML) estimator is statistically efficient as long as there are no direct effects between the treatment and Y (that is, S fully mediates all treatment effects on Y). We find that LIML is highly sensitive to this assumption and recommend TC or JIVE for most applications.\n\nOur methods yield linear structural models of treatment effects that are easy to interpret. As such, they are well-suited to the decentralized and rapidly-evolving practice of experimentation at Netflix, which runs thousands of experiments per year on many diverse parts of the business. Each area of experimentation is staffed by independent Data Science and Engineering teams. While every team ultimately cares about the same north star metrics (e.g., long-term revenue), it is highly impractical for most teams to measure these in short-term A/B tests. Therefore, each has also developed proxies that are more sensitive and directly relevant to their work (e.g., user engagement or latency). To complicate matters more, teams are constantly innovating on these secondary metrics to find the right balance of sensitivity and long-term impact.\n\nIn this decentralized environment, linear models of treatment effects are a highly useful tool for coordinating efforts around proxy metrics and aligning them towards the north star:\n\nManaging metric tradeoffs. Because experiments in one area can affect metrics in another area, there is a need to measure all secondary metrics in all tests, but also to understand the relative impact of these metrics on the north star. This is so we can inform decision-making when one metric trades off against another metric. Informing metrics innovation. To minimize wasted effort on metric development, it is also important to understand how metrics correlate with the north star “net of” existing metrics. Enabling teams to work independently. Lastly, teams need simple tools in order to iterate on their own metrics. Teams may come up with dozens of variations of secondary metrics, and slow, complicated tools for evaluating these variations are unlikely to be adopted. Conversely, our models are easy and fast to fit, and are actively used to develop proxy metrics at Netflix.\n\nWe are thrilled about the research and implementation of these methods at Netflix — while also continuing to strive for great and always better, per our culture. For example, we still have some way to go to develop a more flexible data architecture to streamline the application of these methods within Netflix. Interested in helping us? See our open job postings!\n\nFor feedback on this blog post and for supporting and making this work better, we thank Apoorva Lal, Martin Tingley, Patric Glynn, Richard McDowell, Travis Brooks, and Ayal Chen-Zion.", "label": "non_personal"}
{"title": "Recommending for Long-Term Member Satisfaction at Netflix", "url": "https://netflixtechblog.com/recommending-for-long-term-member-satisfaction-at-netflix-ac15cada49ef?source=collection_home---4------17-----------------------", "content": "Recommending for Long-Term Member Satisfaction at Netflix Netflix Technology Blog 8 min read · Aug 29, 2024 -- 9 Listen Share\n\nBy Jiangwei Pan, Gary Tang, Henry Wang, and Justin Basilico\n\nIntroduction\n\nOur mission at Netflix is to entertain the world. Our personalization algorithms play a crucial role in delivering on this mission for all members by recommending the right shows, movies, and games at the right time. This goal extends beyond immediate engagement; we aim to create an experience that brings lasting enjoyment to our members. Traditional recommender systems often optimize for short-term metrics like clicks or engagement, which may not fully capture long-term satisfaction. We strive to recommend content that not only engages members in the moment but also enhances their long-term satisfaction, which increases the value they get from Netflix, and thus they’ll be more likely to continue to be a member.\n\nRecommendations as Contextual Bandit\n\nOne simple way we can view recommendations is as a contextual bandit problem. When a member visits, that becomes a context for our system and it selects an action of what recommendations to show, and then the member provides various types of feedback. These feedback signals can be immediate (skips, plays, thumbs up/down, or adding items to their playlist) or delayed (completing a show or renewing their subscription). We can define reward functions to reflect the quality of the recommendations from these feedback signals and then train a contextual bandit policy on historical data to maximize the expected reward.\n\nImproving Recommendations: Models and Objectives\n\nThere are many ways that a recommendation model can be improved. They may come from more informative input features, more data, different architectures, more parameters, and so forth. In this post, we focus on a less-discussed aspect about improving the recommender objective by defining a reward function that tries to better reflect long-term member satisfaction.\n\nRetention as Reward?\n\nMember retention might seem like an obvious reward for optimizing long-term satisfaction because members should stay if they’re satisfied, however it has several drawbacks:\n\nNoisy : Retention can be influenced by numerous external factors, such as seasonal trends, marketing campaigns, or personal circumstances unrelated to the service.\n\n: Retention can be influenced by numerous external factors, such as seasonal trends, marketing campaigns, or personal circumstances unrelated to the service. Low Sensitivity : Retention is only sensitive for members on the verge of canceling their subscription, not capturing the full spectrum of member satisfaction.\n\n: Retention is only sensitive for members on the verge of canceling their subscription, not capturing the full spectrum of member satisfaction. Hard to Attribute : Members might cancel only after a series of bad recommendations.\n\n: Members might cancel only after a series of bad recommendations. Slow to Measure: We only get one signal per account per month.\n\nDue to these challenges, optimizing for retention alone is impractical.\n\nProxy Rewards\n\nInstead, we can train our bandit policy to optimize a proxy reward function that is highly aligned with long-term member satisfaction while being sensitive to individual recommendations. The proxy reward r(user, item) is a function of user interaction with the recommended item. For example, if we recommend “One Piece” and a member plays then subsequently completes and gives it a thumbs-up, a simple proxy reward might be defined as r(user, item) = f(play, complete, thumb).\n\nClick-through rate (CTR)\n\nClick-through rate (CTR), or in our case play-through rate, can be viewed as a simple proxy reward where r(user, item) = 1 if the user clicks a recommendation and 0 otherwise. CTR is a common feedback signal that generally reflects user preference expectations. It is a simple yet strong baseline for many recommendation applications. In some cases, such as ads personalization where the click is the target action, CTR may even be a reasonable reward for production models. However, in most cases, over-optimizing CTR can lead to promoting clickbaity items, which may harm long-term satisfaction.\n\nBeyond CTR\n\nTo align the proxy reward function more closely with long-term satisfaction, we need to look beyond simple interactions, consider all types of user actions, and understand their true implications on user satisfaction.\n\nWe give a few examples in the Netflix context:\n\nFast season completion ✅: Completing a season of a recommended TV show in one day is a strong sign of enjoyment and long-term satisfaction.\n\n✅: Completing a season of a recommended TV show in one day is a strong sign of enjoyment and long-term satisfaction. Thumbs-down after completion ❌: Completing a TV show in several weeks followed by a thumbs-down indicates low satisfaction despite significant time spent.\n\n❌: Completing a TV show in several weeks followed by a thumbs-down indicates low satisfaction despite significant time spent. Playing a movie for just 10 minutes ❓: In this case, the user’s satisfaction is ambiguous. The brief engagement might indicate that the user decided to abandon the movie, or it could simply mean the user was interrupted and plans to finish the movie later, perhaps the next day.\n\n❓: In this case, the user’s satisfaction is ambiguous. The brief engagement might indicate that the user decided to abandon the movie, or it could simply mean the user was interrupted and plans to finish the movie later, perhaps the next day. Discovering new genres ✅ ✅: Watching more Korean or game shows after “Squid Game” suggests the user is discovering something new. This discovery was likely even more valuable since it led to a variety of engagements in a new area for a member.\n\nReward Engineering\n\nReward engineering is the iterative process of refining the proxy reward function to align with long-term member satisfaction. It is similar to feature engineering, except that it can be derived from data that isn’t available at serving time. Reward engineering involves four stages: hypothesis formation, defining a new proxy reward, training a new bandit policy, and A/B testing. Below is a simple example.\n\nChallenge: Delayed Feedback\n\nUser feedback used in the proxy reward function is often delayed or missing. For example, a member may decide to play a recommended show for just a few minutes on the first day and take several weeks to fully complete the show. This completion feedback is therefore delayed. Additionally, some user feedback may never occur; while we may wish otherwise, not all members provide a thumbs-up or thumbs-down after completing a show, leaving us uncertain about their level of enjoyment.\n\nWe could try and wait to give a longer window to observe feedback, but how long should we wait for delayed feedback before computing the proxy rewards? If we wait too long (e.g., weeks), we miss the opportunity to update the bandit policy with the latest data. In a highly dynamic environment like Netflix, a stale bandit policy can degrade the user experience and be particularly bad at recommending newer items.\n\nSolution: predict missing feedback\n\nWe aim to update the bandit policy shortly after making a recommendation while also defining the proxy reward function based on all user feedback, including delayed feedback. Since delayed feedback has not been observed at the time of policy training, we can predict it. This prediction occurs for each training example with delayed feedback, using already observed feedback and other relevant information up to the training time as input features. Thus, the prediction also gets better as time progresses.\n\nThe proxy reward is then calculated for each training example using both observed and predicted feedback. These training examples are used to update the bandit policy.\n\nBut aren’t we still only relying on observed feedback in the proxy reward function? Yes, because delayed feedback is predicted based on observed feedback. However, it is simpler to reason about rewards using all feedback directly. For instance, the delayed thumbs-up prediction model may be a complex neural network that takes into account all observed feedback (e.g., short-term play patterns). It’s more straightforward to define the proxy reward as a simple function of the thumbs-up feedback rather than a complex function of short-term interaction patterns. It can also be used to adjust for potential biases in how feedback is provided.\n\nThe reward engineering diagram is updated with an optional delayed feedback prediction step.\n\nTwo types of ML models\n\nIt’s worth noting that this approach employs two types of ML models:\n\nDelayed Feedback Prediction Models : These models predict p(final feedback | observed feedbacks). The predictions are used to define and compute proxy rewards for bandit policy training examples. As a result, these models are used offline during the bandit policy training.\n\n: These models predict p(final feedback | observed feedbacks). The predictions are used to define and compute proxy rewards for bandit policy training examples. As a result, these models are used offline during the bandit policy training. Bandit Policy Models: These models are used in the bandit policy π(item | user; r) to generate recommendations online and in real-time.\n\nChallenge: Online-Offline Metric Disparity\n\nImproved input features or neural network architectures often lead to better offline model metrics (e.g., AUC for classification models). However, when these improved models are subjected to A/B testing, we often observe flat or even negative online metrics, which can quantify long-term member satisfaction.\n\nThis online-offline metric disparity usually occurs when the proxy reward used in the recommendation policy is not fully aligned with long-term member satisfaction. In such cases, a model may achieve higher proxy rewards (offline metrics) but result in worse long-term member satisfaction (online metrics).\n\nNevertheless, the model improvement is genuine. One approach to resolve this is to further refine the proxy reward definition to align better with the improved model. When this tuning results in positive online metrics, the model improvement can be effectively productized. See [1] for more discussions on this challenge.\n\nSummary and Open Questions\n\nIn this post, we provided an overview of our reward engineering efforts to align Netflix recommendations with long-term member satisfaction. While retention remains our north star, it is not easy to optimize directly. Therefore, our efforts focus on defining a proxy reward that is aligned with long-term satisfaction and sensitive to individual recommendations. Finally, we discussed the unique challenge of delayed user feedback at Netflix and proposed an approach that has proven effective for us. Refer to [2] for an earlier overview of the reward innovation efforts at Netflix.\n\nAs we continue to improve our recommendations, several open questions remain:\n\nCan we learn a good proxy reward function automatically by correlating behavior with retention?\n\nHow long should we wait for delayed feedback before using its predicted value in policy training?\n\nHow can we leverage Reinforcement Learning to further align the policy with long-term satisfaction?\n\nReferences\n\n[1] Deep learning for recommender systems: A Netflix case study. AI Magazine 2021. Harald Steck, Linas Baltrunas, Ehtsham Elahi, Dawen Liang, Yves Raimond, Justin Basilico.\n\n[2] Reward innovation for long-term member satisfaction. RecSys 2023. Gary Tang, Jiangwei Pan, Henry Wang, Justin Basilico.", "label": "non_personal"}
{"title": "Google Pay inside sandboxed iframe for PCI DSS v4 compliance", "url": "https://developers.googleblog.com/en/google-pay-inside-sandboxed-iframe-for-pci-dss-v4-compliance/", "content": "Using a sandboxed iframe satisfies any concerns with compliance since scripts within the iFrame will not have access to the parent DOM. See the following illustration for an example:\n\nOne way to comply with this requirement is to use a technique like Subresource Integrity (SRI) . However, the Google Pay JavaScript (pay.js) build and release process does not allow for a long-lived, stable hash required by techniques like SRI.\n\nIf you are developing or maintaining a checkout page you might come across PCI DSS v4 which includes the following requirement under 6.4.3:\n\nIn this case the domain “cdn.somewhereelse.com” would load Google Pay’s pay.js JavaScript file. After a successful transaction, the inner iframe can communicate with the parent page through mechanisms like window.postMessage() if needed.\n\nIn order for Google Pay to work in all browsers we need the following 4 sandbox attribute values in addition to allow=”payment” :\n\nallow-scripts\n\nTo allow the iframe to execute scripts (pay.js as an example)\n\nallow-popups\n\nAllows the embedded page to create 'child browsing contexts'. In practice, this flag enables the embedded iframe to open new tabs and windows when the user clicks a link.\n\nallow-same-origin\n\nIf not set, fails on various occasions for browsers. If set, the iframe has access to the parents storage and cookies.\n\nallow-forms\n\nAllows forms such as the Google Pay login to submit the data.\n\nSee this test page to see the various iframe sandbox values in action.\n\n\n\nShopify successfully certified for PCI DSS v4\n\nGoogle Pay partnered with Shopify to implement the above solution. Shopify was able to successfully pass the PCI DSS v4 audit by using a sandboxed iframe to display the Google Pay button. Here is what Shopify has to say:\n\nWe’ve built Shopify Checkout in such a way that Google Pay code executes in a secure sandboxed environment, allowing us to maintain the integrity of our checkout and comply with PCI DSS V4 requirements.\n\n\n\n– Ilya Grigorik, Distinguished Engineer at Shopify\n\nFor more information on how Shopify built their checkout solution using sandboxed iframes, their “Powering Shopify’s High-Performance, PCI DSS v4 Compliant Checkout with Sandboxing” blog post has the insights.\n\n\n\nConclusion\n\nWrapping your Google Pay integration in a sandboxed iframe can help you to comply with PCI DSS v4 requirements. For more assistance with your implementation, sign in to the Google Pay & Wallet Console to create a support ticket. In addition, you can join the developer community in the #payments channel on Discord.\n\nFollow @GooglePayDevs on X for future updates. If you have questions, tag @GooglePayDevs and include #AskGooglePayDevs in your tweets.", "label": "non_personal"}
{"title": "Noisy Neighbor Detection with eBPF", "url": "https://netflixtechblog.com/noisy-neighbor-detection-with-ebpf-64b1f4b3bbdd?source=collection_home---4------16-----------------------", "content": "The sched_wakeup and sched_wakeup_new hooks are invoked when a process changes state from 'sleeping' to 'runnable.' They let us identify when a process is ready to run and is waiting for CPU time. During this event, we generate a timestamp and store it in an eBPF hash map using the process ID as the key.\n\nstruct {\n\n__uint(type, BPF_MAP_TYPE_HASH);\n\n__uint(max_entries, MAX_TASK_ENTRIES);\n\n__uint(key_size, sizeof(u32));\n\n__uint(value_size, sizeof(u64));\n\n} runq_enqueued SEC(\".maps\");\n\n\n\nSEC(\"tp_btf/sched_wakeup\")\n\nint tp_sched_wakeup(u64 *ctx)\n\n{\n\nstruct task_struct *task = (void *)ctx[0];\n\nu32 pid = task->pid;\n\nu64 ts = bpf_ktime_get_ns();\n\n\n\nbpf_map_update_elem(&runq_enqueued, &pid, &ts, BPF_NOEXIST);\n\nreturn 0;\n\n}\n\nConversely, the sched_switch hook is triggered when the CPU switches between processes. This hook provides pointers to the process currently utilizing the CPU and the process about to take over. We use the upcoming task's process ID (PID) to fetch the timestamp from the eBPF map. This timestamp represents when the process entered the queue, which we had previously stored. We then calculate the run queue latency by simply subtracting the timestamps.\n\nSEC(\"tp_btf/sched_switch\")\n\nint tp_sched_switch(u64 *ctx)\n\n{\n\nstruct task_struct *prev = (struct task_struct *)ctx[1];\n\nstruct task_struct *next = (struct task_struct *)ctx[2];\n\nu32 prev_pid = prev->pid;\n\nu32 next_pid = next->pid;\n\n\n\n// fetch timestamp of when the next task was enqueued\n\nu64 *tsp = bpf_map_lookup_elem(&runq_enqueued, &next_pid);\n\nif (tsp == NULL) {\n\nreturn 0; // missed enqueue\n\n}\n\n\n\n// calculate runq latency before deleting the stored timestamp\n\nu64 now = bpf_ktime_get_ns();\n\nu64 runq_lat = now - *tsp;\n\n\n\n// delete pid from enqueued map\n\nbpf_map_delete_elem(&runq_enqueued, &next_pid);\n\n....\n\nOne of the advantages of eBPF is its ability to provide pointers to the actual kernel data structures representing processes or threads, also known as tasks in kernel terminology. This feature enables access to a wealth of information stored about a process. We required the process's cgroup ID to associate it with a container for our specific use case. However, the cgroup information in the process struct is safeguarded by an RCU (Read Copy Update) lock.\n\nTo safely access this RCU-protected information, we can leverage kfuncs in eBPF. kfuncs are kernel functions that can be called from eBPF programs. There are kfuncs available to lock and unlock RCU read-side critical sections. These functions ensure that our eBPF program remains safe and efficient while retrieving the cgroup ID from the task struct.\n\nvoid bpf_rcu_read_lock(void) __ksym;\n\nvoid bpf_rcu_read_unlock(void) __ksym;\n\n\n\nu64 get_task_cgroup_id(struct task_struct *task)\n\n{\n\nstruct css_set *cgroups;\n\nu64 cgroup_id;\n\nbpf_rcu_read_lock();\n\ncgroups = task->cgroups;\n\ncgroup_id = cgroups->dfl_cgrp->kn->id;\n\nbpf_rcu_read_unlock();\n\nreturn cgroup_id;\n\n}\n\nOnce the data is ready, we must package it and send it to userspace. For this purpose, we chose the eBPF ring buffer. It is efficient, high-performing, and user-friendly. It can handle variable-length data records and allows data reading without necessitating extra memory copying or syscalls. However, the sheer number of data points was causing the userspace program to use too much CPU, so we implemented a rate limiter in eBPF to sample the data.\n\nstruct {\n\n__uint(type, BPF_MAP_TYPE_RINGBUF);\n\n__uint(max_entries, RINGBUF_SIZE_BYTES);\n\n} events SEC(\".maps\");\n\n\n\nstruct {\n\n__uint(type, BPF_MAP_TYPE_PERCPU_HASH);\n\n__uint(max_entries, MAX_TASK_ENTRIES);\n\n__uint(key_size, sizeof(u64));\n\n__uint(value_size, sizeof(u64));\n\n} cgroup_id_to_last_event_ts SEC(\".maps\");\n\n\n\nstruct runq_event {\n\nu64 prev_cgroup_id;\n\nu64 cgroup_id;\n\nu64 runq_lat;\n\nu64 ts;\n\n};\n\n\n\nSEC(\"tp_btf/sched_switch\")\n\nint tp_sched_switch(u64 *ctx)\n\n{\n\n// ....\n\n// The previous code\n\n// ....\n\n\n\nu64 prev_cgroup_id = get_task_cgroup_id(prev);\n\nu64 cgroup_id = get_task_cgroup_id(next);\n\n\n\n// per-cgroup-id-per-CPU rate-limiting\n\n// to balance observability with performance overhead\n\nu64 *last_ts =\n\nbpf_map_lookup_elem(&cgroup_id_to_last_event_ts, &cgroup_id);\n\nu64 last_ts_val = last_ts == NULL ? 0 : *last_ts;\n\n\n\n// check the rate limit for the cgroup_id in consideration\n\n// before doing more work\n\nif (now - last_ts_val < RATE_LIMIT_NS) {\n\n// Rate limit exceeded, drop the event\n\nreturn 0;\n\n}\n\n\n\nstruct runq_event *event;\n\nevent = bpf_ringbuf_reserve(&events, sizeof(*event), 0);\n\n\n\nif (event) {\n\nevent->prev_cgroup_id = prev_cgroup_id;\n\nevent->cgroup_id = cgroup_id;\n\nevent->runq_lat = runq_lat;\n\nevent->ts = now;\n\nbpf_ringbuf_submit(event, 0);\n\n// Update the last event timestamp for the current cgroup_id\n\nbpf_map_update_elem(&cgroup_id_to_last_event_ts, &cgroup_id,\n\n&now, BPF_ANY);\n\n\n\n}\n\n\n\nreturn 0;\n\n}\n\nOur userspace application, developed in Go, processes events from the ring buffer to emit metrics to our metrics backend, Atlas. Each event includes a run queue latency sample with a cgroup ID, which we associate with containers running on the host. We categorize it as a system service if no such association is found. When a cgroup ID is associated with a container, we emit a percentile timer Atlas metric ( runq.latency ) for that container. We also increment a counter metric ( sched.switch.out ) to monitor preemptions occurring for the container's processes. Access to the prev_cgroup_id of the preempted process allows us to tag the metric with the cause of the preemption, whether it's due to a process within the same container (or cgroup), a process in another container, or a system service.\n\nIt's important to highlight that both the runq.latency metric and the sched.switch.out metrics are needed to determine if a container is affected by noisy neighbors, which is the goal we aim to achieve — relying solely on the runq.latency metric can lead to misconceptions. For example, if a container is at or over its cgroup CPU limit, the scheduler will throttle it, resulting in an apparent spike in run queue latency due to delays in the queue. If we were only to consider this metric, we might incorrectly attribute the performance degradation to noisy neighbors when it's actually because the container is hitting its CPU quota. However, simultaneous spikes in both metrics, mainly when the cause is a different container or system process, clearly indicate a noisy neighbor issue.\n\nA Noisy Neighbor Story", "label": "non_personal"}
{"title": "Streamlining LLM Inference at the Edge with TFLite", "url": "https://developers.googleblog.com/en/streamlining-llm-inference-at-the-edge-with-tflite/", "content": "Optimizing Time to First Token and Peak Memory Usage with a Smarter Cache for XNNPack\n\nXNNPack is the default TensorFlow Lite CPU inference engine for all models. It delivers game changing speedups across mobile, desktop, and Web platforms. One of the optimizations employed in XNNPack is repacking the static weights of the Convolution, Depthwise Convolution, Transposed Convolution, and Fully Connected operators into an internal layout optimized for inference computations. During inference, the repacked weights are accessed in a sequential pattern that is friendly to the processors’ pipelines. The inference latency reduction comes at a cost: repacking essentially creates an extra copy of the weights inside XNNPack. Previous efforts have been made to reduce that cost by adding an in-memorycache to XNNPack. This cache allows sharing the packed weights between independent TFLite interpreters that would run the same model independently. TFLite XNNPack delegate implementation has been improved to address some of the shortcomings of the existing cache.\n\n1. The cache lives in anonymous memory, which incurs swapping to disk in case of memory pressure, leading to poor performance. 2. It requires repacking the initial weights every time a process is started. 3. Because repacking reads the original TFLite weights and writes to a new buffer, this leads to a high peak memory usage during the packing. 4. It requires tedious steps and careful lifecycle management to properly enable caching through XNNPack delegate. 5. It doesn’t allow sharing the weights across processes.\n\n.\n\nThe New XNNPack Cache Provider Interface XNNPack has been updated and provides an interface that lets you implement a weight cache provider. A weight cache provider behaves as a dictionary that XNNPack will fill and query in order to access packed buffers. Here are its main functions. look_up looks up a packed buffer key and returns a unique identifier (or a special identifier reserved for NotFound) that may be later used to retrieve the buffer address. reserve_space reserves a buffer that may be used to store information of a given size. That buffer then needs to be committed using look_up_or_insert . look_up_or_insert checks if a buffer matching the given key exists in the cache provider. If not, the given data is committed to the cache provider. This function also returns the identifier that may be used to retrieve the buffer address. offset_to_addr returns the buffer address from the identifier returned by look_up and look_up_or_insert . The interactions between XNNPack and the weight cache provider are illustrated in the following diagram.\n\n.\n\nLoading the Cache From Disk with MMAP in the TFLite Delegate The TFLite Delegate now uses this new interface and has its own weight cache provider. This provider is capable of saving and loading the packed weights directly to / from disk. TFLite has been leveraging flatbuffer and file-backed memory mapping for a long time. We are filling the gap here by leveraging the same technique, for the following advantages.\n\nIt eliminates the repacking overhead. Persisting packed weights on disk bypasses the costly repacking process each time a model is loaded. This translates to a significant reduction in both startup latency and peak memory usage. Even for the initial building, this offers packed data deduplication and further improves packing performance by avoiding repacking the same data again.\n\nIt improves memory management. mmap leverages the operating system's virtual memory management allowing it to optimize overall system memory usage and performance. In our case, this is especially advantageous for random access bulky read-only file access, like a neural network’s operation’s constant weights for instance. With packed data stored on disk, the XNNPack cache no longer relies on anonymous memory which can be prone to performance issues under memory pressure. Instead, it leverages the operating system's virtual memory management for smoother operation. By eliminating the need to copy data between the file system and memory, mmap significantly reduces overhead and speeds up access times. You can find more information about file mappings and memory usage directly from mmap’s man page and other interesting reads.\n\nIt allows cross-process collaboration. mmap -based file loading opens the door for seamless weight sharing between multiple processes as each process’ virtual address space maps to the same physical memory pages. This not only reduces the overall memory footprint as multiple processes share the same memory but also accelerates model loading across the board.\n\n.\n\nIt simplifies the user facing API. Instead of requiring the user to setup and manage the cache object throughout the application lifetime, they can simply provide a path to the cache file.\n\nstd::unique_ptr<tflite::Interpreter> interpreter; // Setup the options for the XNNPack delegate. TfLiteXNNPackDelegateOptions xnnpack_options = TfLiteXNNPackDelegateOptionsDefault(); xnnpack_options.weight_cache_file_path = \"/tmp/cache_file.xnn_cache\"; // Create and apply the XNNPack delegate to a TFLite interpreter. // Static weights will be packed and written into weights_cache on the first run. // They will be automatically loaded for all other runs. TfLiteDelegate* delegate = TfLiteXNNPackDelegateCreate(&xnnpack_options); interpreter->ModifyGraphWithDelegate(delegate); C++ Copied\n\nMaintaining Cache Integrity To guarantee accurate and efficient inference, it's crucial to invalidate the XNNPack cache under specific conditions: Model Evolution: if your model's weights or structure change, the cached data becomes outdated and must be invalidated. This means removing the file at the provided cache path. XNNPack Upgrades: updates to XNNPack's internal packing algorithm may result in incompatible cached weights, requiring the cache to be recomputed. Fortunately XNNPack is capable of detecting this and will replace the existing cache automatically. In essence, any modification that could impact the way weights are packed or utilized by XNNPack should trigger a cache invalidation.\n\nBenchmarks The session initialisation is dominated by the weight packing. For LLMs several subgraphs are reusing the same weights. Building the cache is faster because the deduplication functionality avoids packing those same weights multiple times. For more standard models, like stable diffusion, there is no deduplication and the slightly higher initialisation time is due to saving the cache to disk. Reloading the cache (from the 2nd run on) brings the initialisation down to a fraction of the previous time in all the cases. The session initialisation improvement naturally affects the time to the first token for LLMs, roughly dividing it by 2 in the benchmarks. The memory gains brought by the cache implementation can also be seen. The peak Resident Set Size is lowered for LLMs thanks to the deduplication. For other models that don’t benefit from the deduplication, there is no change. Reloading the cache brings the peak RSS even further down because the TFLite original models aren’t read anymore and therefore never get pulled into memory.\n\nGemma 2B on a Pixel 8 Pro\n\n.\n\nPhi2 on a Pixel 8 Pro\n\n.\n\nStable Diffusion on a Pixel 8 Pro\n\n.", "label": "non_personal"}
{"title": "TensorFlow Lite is now LiteRT", "url": "https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/", "content": "LiteRT, part of the Google AI Edge suite of tools, is the runtime that lets you seamlessly deploy ML and AI models on Android, iOS, and embedded devices. With AI Edge's robust model conversion and optimization tools, you can ready both open-source and custom models for on-device development.\n\nSince its debut in 2017, TFLite has enabled developers to bring ML-powered experiences to over 100K apps running on 2.7B devices. More recently, TFLite has grown beyond its TensorFlow roots to support models authored in PyTorch , JAX , and Keras with the same leading performance. The name LiteRT captures this multi-framework vision for the future: enabling developers to start with any popular framework and run their model on-device with exceptional performance.\n\nLiteRT (short for Lite Runtime) is the new name for TensorFlow Lite (TFLite). While the name is new, it's still the same trusted, high-performance runtime for on-device AI, now with an expanded vision.\n\nThis change will roll out progressively. Starting today, you’ll see the LiteRT name reflected in the developer documentation, which is moving to ai.google.dev/edge/litert, and in other references across the AI Edge website. The documentation at tensorflow.org/lite now redirects to corresponding pages at ai.google.dev/edge/litert.\n\nThe main TensorFlow brand will not be affected, nor will apps already using TensorFlow Lite.\n\n\n\nHow to access LiteRT\n\nOur goal is that this change is minimally disruptive, requiring as few code changes from developers as possible.\n\nIf you currently use TensorFlow Lite via packages, you’ll need to update any dependencies to use the new LiteRT from Maven, PyPi, Cocoapods.\n\nIf you currently use TensorFlow Lite via Google Play Services, no change is necessary at this time.\n\nIf you currently build TensorFlow Lite from source, please continue building from the TensorFlow repo until code has been fully moved to the new LiteRT repo later this year.\n\n\n\nFrequently asked questions\n\n\n\n1. What is changing beyond the new name, LiteRT?\n\nFor now, the only change is the new name, LiteRT. Your production apps will not be affected. With a new name and refreshed vision, look out for more updates coming to LiteRT, improving how you deploy classic ML models, LLMs, and diffusion models with GPU and NPU acceleration across platforms.\n\n\n\n2. What’s happening to the TensorFlow Lite Support Library (including TensorFlow Lite Tasks)?\n\nThe TensorFlow Lite support library and TensorFlow Lite Tasks will remain in the /tensorflow repository at this time. We encourage you to use MediaPipe Tasks for future development.\n\n\n\n3. What’s happening to TensorFlow Lite Model Maker?\n\nYou can continue to access TFLite Model Maker via https://pypi.org/project/tflite-model-maker/\n\n\n\n4. What if I want to contribute code?\n\nFor now, please contribute code to the existing TensorFlow Lite repository. We’ll make a separate announcement when we’re ready for contributions to the LiteRT repository.\n\n\n\n5. What’s happening to the .tflite file extension and file format?\n\nNo changes are being made to the .tflite file extension or format. Conversion tools will continue to output .tflite flatbuffer files, and .tflite files will be readable by LiteRT.\n\n\n\n6. How do I convert models to .tflite format?\n\nFor Tensorflow, Keras and Jax you can continue to use the same flows. For PyTorch support check out ai-edge-torch.\n\n\n\n7. Will there be any changes to classes and methods?\n\nNo. Aside from package names, you won’t have to change any code you’ve written for now.\n\n\n\n8. Will there be any changes to TensorFlow.js?\n\nNo, TensorFlow.js will continue to function independently as part of the Tensorflow codebase.\n\n\n\n9. My production app uses TensorFlow Lite. Will it be affected?\n\nApps that have already deployed TensorFlow Lite will not be affected. This includes apps that access TensorFlow Lite via Google Play Services. (TFLite is compiled into the apps at build time, so once they’re deployed, apps have no dependency.)\n\n\n\n10. Why “LiteRT”?\n\n“LiteRT” (short for Lite Runtime) reflects the legacy of TensorFlow Lite, a pioneering “lite”, on-device runtime, plus Google’s commitment to supporting today’s thriving multi-framework ecosystem.\n\n\n\n11. Is TensorFlow Lite still being actively developed?\n\nYes, but under the name LiteRT. Active development will continue on the runtime (now called LiteRT), as well as the conversion and optimization tools. To ensure you're using the most up-to-date version of the runtime, please use LiteRT.\n\n\n\n12. Where can I see examples of LiteRT in practice?\n\nYou can find examples for Python, Android, and iOS in the official LiteRT samples repo.\n\n\n\nWe’re excited for the future of on-device ML, and are committed to our vision of making LiteRT the easiest to use, highest performance runtime for a wide range of models.", "label": "non_personal"}
{"title": "Learn to build and run AI powered apps at Firebase Demo Day ‘24", "url": "https://developers.googleblog.com/en/firebase-demo-day-24/", "content": "Welcome to Firebase Demo Day 2024\n\nWe just released 8 bite sized demo videos to showcase how Firebase helps you build and run AI-powered apps. We’ll show you how to use new Firebase products and features like Firebase Genkit, Vertex AI in Firebase, Gemini in Firebase and Firebase App Hosting, to build AI features into your existing applications, monitor their performance, and create great experiences for your users.\n\nTo bring these concepts to life, we'll take you on an app dev journey through Compass, our sample travel app. We’ll demonstrate how you can use Firebase to create features like personalized recommendations, smart itineraries, AI-powered chatbots, and more. Follow along as we highlight how you can leverage Firebase tools to add the same cutting-edge functionality to your own apps.\n\n\n\nWatch Firebase Demo Day 2024 from anywhere at any time, at your own pace.\n\n\n\n\n\n\n\nDemos to build AI-powered features\n\nWatch as we transform our travel app with the power of Firebase and AI. Our build demos show you how to build and deploy AI features with new Firebase products like Vertex AI, Genkit and Firebase Hosting, all while leveraging Firebase's fully managed infrastructure to get to market quickly and securely.\n\n\n\nCall Gemini from your Android app\n\nIntegrate the power of Gemini directly into your Android app using the native Vertex AI in Firebase SDK for Android to make calls to Gemini.", "label": "non_personal"}
{"title": "Celebrating Flutter’s “Production Era”", "url": "https://developers.googleblog.com/en/celebrating-flutters-production-era/", "content": "This article is cross posted on Flutter\n\nJust over six years ago, we unveiled Flutter 1.0. Today, at #FlutterInProduction, we’re celebrating how far we’ve come — from the immense support we’ve received from thousands of contributors in the community, to the widespread adoption of Flutter as a production-grade app framework for building multi-platform app experiences. If you haven’t experienced Flutter yet, we invite you to try it! As we shared today, you’d be joining a big group: Flutter has over 1 million monthly active developers across the globe, and powers nearly 30% of all new iOS apps. More than 90 thousand developers actively participate in Flutter Meetups across more than sixty countries. And if you want input on designing or building a new successful Flutter app, we have a large and growing list of Flutter Consultants ready to help you. “Apptopia tracks millions of apps in the Apple AppStore and Google Play Store, and analyzes and detects which developer SDKs were used to create the apps. Flutter is one of the most popular SDKs we track: In the Apple AppStore it has grown steadily in usage from around 10% of all tracked free apps in 2021 to nearly 30% of all tracked free apps in 2024!”\n\n— Apptopia Inc. A decade of innovation to reach the production era It’s been an incredible journey, starting in 2014 (in what we now call our experimental era) as a Google experiment codenamed “Sky.” Before Flutter, compromises were inevitable. Many developers have become skeptical that any framework can truly deliver a premium experience across multiple platforms. With the launch of Flutter 1.0 in 2018 we had a clear mission to resolve that technology dilemma: We aimed to empower developers with the ultimate app framework for crafting beautiful, high-performance user interfaces across all platforms. Also, to enable developers to reach all customers with high-quality apps on all the platforms that customers care about, but with lower cost and in less time. Our focus has remained constant through Flutter’s growth era, even as we’ve added support for the six major platforms across mobile, web, and desktop — and continue to push beyond, with work like Toyota’s use of Flutter for infotainment systems.\n\nWe’re now in the “production era,” and we’re celebrating that with #FlutterInProduction! This event spotlights the achievements of developers using Flutter in real-world applications.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nBuilding in partnership with the community None of this would be possible without our amazing community of over 1,400 contributors, more than 10,000 package publishers authoring over 50,000 packages, and passionate Flutter content creators and enthusiasts. Together, we’ve built a top-5 GitHub open-source project by contributions!\n\nAmazing user experiences It all starts with a focus on enabling amazing user experiences. Free from typical platform constraints, Flutter supports a broad set of design languages — support for Material Design and our Apple-inspired Cupertino widgets comes with the SDK. The ecosystem also provides a broad selection of design libraries like Windows-inspired fluent_ui , macOS-inspired macos_ui , and the Ubuntu-inspired yaru widgets.\n\nScandinavian Airlines design awards With Flutter, you have the flexibility and power to realize any design your design team envisions. This is exemplified by Scandinavian Airlines, who after creating their new mobile app with Flutter have filled their trophy case with prestigious design awards such as the Red Dot Design Award, the Webby People’s Voice Award, and the iF Design Gold Award. Charlotte Svensson, EVP & CIO at SAS explains: “I’m extremely proud over this award, which is not just an industry award, but a global recognition. It’s a testament to what we can do, when we go above-and-beyond in focusing on improving the customer experience, and when we interact and develop together with our customers. SAS has always been at the forefront of innovation in the aviation industry, and this award serves as a validation of its dedication to providing exceptional digital solutions for our customers.”\n\nGreat performance & reliability Performance and reliability are crucial for a positive user experience and brand perception. Slow or crash-prone apps not only frustrate users in the short term but can also damage your brand reputation in the long run through negative reviews and word-of-mouth. Flutter has prioritized performance and reliability from the outset. By choosing the Dart programming language, we ensure fast startup times through ahead-of-time compilation to native machine code or web assembly. Dart’s rich, null-safety type system helps catch errors during development, further enhancing reliability. Additionally, Flutter’s custom Impeller rendering engine, designed specifically for multi-platform UI, delivers smooth animations and gives us full control over the rendering stack, top to bottom, from the UI source code to the GPU.\n\nUniversal Studios performance and reliability For example, Universal Destinations and Experiences recently reported that by adopting Flutter, they not only decreased their app size — a significant benefit for users with unreliable internet connections — but also dramatically reduced app crashes to near zero, thus lowering their total cost of ownership.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nLG Electronics performance LG Electronics has traditionally relied on web apps for their webOS-powered smart TVs due to concerns about the high development cost of traditional native apps. However, they found that web apps launch slower and consume more memory than native apps. With Flutter, LG Electronics has a solution that combines fast development speed and excellent performance. As a result, they plan to use Flutter for key applications in webOS TVs globally starting in 2025.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nFirst-class developer experience and thriving ecosystem Flutter’s success is deeply rooted in its focus on developer experience. We pioneered instant developer workflows with Stateful Hot Reload, and during our growth era added Flutter DevTools to significantly accelerate diagnostics and debugging workflows. Flutter’s community provides a thriving and open ecosystem of over 50,000 packages published by over 10,000 publishers, combined with robust third-party services & technologies. Also, if you want input on designing or building a new successful Flutter app, we have a large list of Flutter Consultants ready to help you.\n\nMGM and developer productivity App agency Superformula has built with Flutter since August 2020. They found that Flutter is easy to learn and well documented, enabling them to get new team members up to speed quickly and contribute effectively. Superformula also used Flutter to revitalize the digital dining experience for MGM Resorts’ 400+ restaurants. The new Flutter-based MGM Rewards app was rebuilt in just 4 months, cutting the total amount of code in half, and improving delivery speed by a factor of 4. One core enabler of productivity for Superformula is the ability to share code across mobile, tablet-based kiosks, and web-based tools.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nGEICO user interface elements shared across web, iOS, and Android.", "label": "non_personal"}
{"title": "New Google Pay features to enhance your payment flows", "url": "https://developers.googleblog.com/en/new-google-pay-features-to-enhance-your-payment-flows/", "content": "At Google I/O 2025, we unveiled updates across the Google Pay API designed to help you create smoother, safer, and more successful checkout experiences for your users. Whether you're looking to boost conversions, enable new payment scenarios, enhance security, or simplify your integration, there's something new for you. Let's dive into the key announcements developers need to know.\n\nEnhancing the checkout experience and conversion Google Pay in Android WebViews Big news! Starting with Chrome v137, users can seamlessly use Google Pay within Android WebViews, accessing an Android native experience and device tokens from their Google Wallet. Simply enable PaymentRequest in your app manifest, and tap into the opportunities of in-app browser purchases with a high-quality, secure form of payment. Take a look at the integration guide to learn more.\n\nSorry, your browser doesn't support playback for this video Figure 1: A sample checkout flow via a WebView on Android that uses Google Pay to complete the payment\n\nA more versatile API to power modern checkout flows We are introducing improvements to the Google Pay API to help you adapt to a payment ecosystem that is in continuous evolution. Here are some of our favorite updates: The Google Pay payment sheet now features richer card art and names, helping users select their preferred card faster. The payment sheet also supports dark mode for a more integrated feel within your application.\n\nFigure 2: Screenshots showcasing the Google Pay payment sheet in their dark and light versions.\n\nBuilding on last year's success, the createButton API for Web now offers more customization options (show/hide border, more button text options) to better match your UI, and continue to help boost sales by showing card details upfront. Need to show card-identifying information without using a payment button? We're introducing a new API in the coming months to enable this use case.\n\nfigure 3: An example offering Google Pay using a list selector through the Payment Metadata API\n\nWe're adding support for Merchant-Initiated Transactions (MITs) (subscriptions, auto-reloads, deferred charges) to the Google Pay Online API. This includes details in the payment sheet to inform users, device-independent tokens for payment continuity (even if users change devices), and lifecycle notifications for underlying card changes.\n\nStreamlining the developer experience We are dedicated to making the Google Pay API easier to integrate, test and maintain. Here are some updates that improve the integration experience: Testing just got easier. We have improved the test card suite, so you can now see relevant test cards (regular, tokenized, debit) for your specific PSP directly in the payments sheet when using the TEST environment. Debug your integrations faster on Android with more fine-grained build time error logs to amend your logic more easily, and detailed exceptions/error codes at runtime. Check out the troubleshooting guide if you are seeing errors in your integration.\n\n\n\nFigure 4: More detailed error messages are now surfaced via the Logcat and the debugger\n\nWe've launched new codelabs, Firebase Studio templates (one-click development environments), and a learning pathway for web developers. We are planning to add similar resources for native Android, Flutter, React JS, React Native, and Angular. Stay informed about the status of the Google Pay API with the new Google Pay API Status Dashboard. The dashboard monitors key APIs like the CreateButton, IsReadyToPay, or LoadPaymentData APIs in real-time. Check the availability of the API (99.99% uptime last year!) and get incident updates instantly.\n\nFigure 5: The Google Pay API Status Dashboard includes service uptime and health information.", "label": "non_personal"}
{"title": "Pushy to the Limit: Evolving Netflix’s WebSocket proxy for the future", "url": "https://netflixtechblog.com/pushy-to-the-limit-evolving-netflixs-websocket-proxy-for-the-future-b468bc0ff658?source=collection_home---4------15-----------------------", "content": "Pushy to the Limit: Evolving Netflix’s WebSocket proxy for the future Netflix Technology Blog 16 min read · Sep 10, 2024 -- 16 Listen Share\n\nBy Karthik Yagna, Baskar Odayarkoil, and Alex Ellis\n\nPushy is Netflix’s WebSocket server that maintains persistent WebSocket connections with devices running the Netflix application. This allows data to be sent to the device from backend services on demand, without the need for continually polling requests from the device. Over the last few years, Pushy has seen tremendous growth, evolving from its role as a best-effort message delivery service to be an integral part of the Netflix ecosystem. This post describes how we’ve grown and scaled Pushy to meet its new and future needs, as it handles hundreds of millions of concurrent WebSocket connections, delivers hundreds of thousands of messages per second, and maintains a steady 99.999% message delivery reliability rate.\n\nHistory & motivation\n\nThere were two main motivating use cases that drove Pushy’s initial development and usage. The first was voice control, where you can play a title or search using your virtual assistant with a voice command like “Show me Stranger Things on Netflix.” (See How to use voice controls with Netflix if you want to do this yourself!).\n\nIf we consider the Alexa use case, we can see how this partnership with Amazon enabled this to work. Once they receive the voice command, we allow them to make an authenticated call through apiproxy, our streaming edge proxy, to our internal voice service. This call includes metadata, such as the user’s information and details about the command, such as the specific show to play. The voice service then constructs a message for the device and places it on the message queue, which is then processed and sent to Pushy to deliver to the device. Finally, the device receives the message, and the action, such as “Show me Stranger Things on Netflix”, is performed. This initial functionality was built out for FireTVs and was expanded from there.\n\nSample system diagram for an Alexa voice command. Where aws ends and the internet begins is an exercise left to the reader.\n\nThe other main use case was RENO, the Rapid Event Notification System mentioned above. Before the integration with Pushy, the TV UI would continuously poll a backend service to see if there were any row updates to get the latest information. These requests would happen every few seconds, which ended up creating extraneous requests to the backend and were costly for devices, which are frequently resource constrained. The integration with WebSockets and Pushy alleviated both of these points, allowing the origin service to send row updates as they were ready, resulting in lower request rates and cost savings.\n\nFor more background on Pushy, you can see this InfoQ talk by Susheel Aroskar. Since that presentation, Pushy has grown in both size and scope, and this article will be discussing the investments we’ve made to evolve Pushy for the next generation of features.\n\nClient Reach\n\nThis integration was initially rolled out for Fire TVs, PS4s, Samsung TVs, and LG TVs, leading to a reach of about 30 million candidate devices. With these clear benefits, we continued to build out this functionality for more devices, enabling the same efficiency wins. As of today, we’ve expanded our list of candidate devices even further to nearly a billion devices, including mobile devices running the Netflix app and the website experience. We’ve even extended support to older devices that lack modern capabilities, like support for TLS and HTTPS requests. For those, we’ve enabled secure communication from client to Pushy via an encryption/decryption layer on each, allowing for confidential messages to flow between the device and server.\n\nScaling to handle that growth (and more)\n\nGrowth\n\nWith that extended reach, Pushy has gotten busier. Over the last five years, Pushy has gone from tens of millions of concurrent connections to hundreds of millions of concurrent connections, and it regularly reaches 300,000 messages sent per second. To support this growth, we’ve revisited Pushy’s past assumptions and design decisions with an eye towards both Pushy’s future role and future stability. Pushy had been relatively hands-free operationally over the last few years, and as we updated Pushy to fit its evolving role, our goal was also to get it into a stable state for the next few years. This is particularly important as we build out new functionality that relies on Pushy; a strong, stable infrastructure foundation allows our partners to continue to build on top of Pushy with confidence.\n\nThroughout this evolution, we’ve been able to maintain high availability and a consistent message delivery rate, with Pushy successfully maintaining 99.999% reliability for message delivery over the last few months. When our partners want to deliver a message to a device, it’s our job to make sure they can do so.\n\nHere are a few of the ways we’ve evolved Pushy to handle its growing scale.\n\nA few of the related services in Pushy’s immediate ecosystem and the changes we’ve made for them.\n\nMessage processor\n\nOne aspect that we invested in was the evolution of the asynchronous message processor. The previous version of the message processor was a Mantis stream-processing job that processed messages from the message queue. It was very efficient, but it had a set job size, requiring manual intervention if we wanted to horizontally scale it, and it required manual intervention when rolling out a new version.\n\nIt served Pushy’s needs well for many years. As the scale of the messages being processed increased and we were making more code changes in the message processor, we found ourselves looking for something more flexible. In particular, we were looking for some of the features we enjoy with our other services: automatic horizontal scaling, canaries, automated red/black rollouts, and more observability. With this in mind, we rewrote the message processor as a standalone Spring Boot service using Netflix paved-path components. Its job is the same, but it does so with easy rollouts, canary configuration that lets us roll changes safely, and autoscaling policies we’ve defined to let it handle varying volumes.\n\nRewriting always comes with a risk, and it’s never the first solution we reach for, particularly when working with a system that’s in place and working well. In this case, we found that the burden from maintaining and improving the custom stream processing job was increasing, and we made the judgment call to do the rewrite. Part of the reason we did so was the clear role that the message processor played — we weren’t rewriting a huge monolithic service, but instead a well-scoped component that had explicit goals, well-defined success criteria, and a clear path towards improvement. Since the rewrite was completed in mid-2023, the message processor component has been completely zero touch, happily automated and running reliably on its own.\n\nPush Registry\n\nFor most of its life, Pushy has used Dynomite for keeping track of device connection metadata in its Push Registry. Dynomite is a Netflix open source wrapper around Redis that provides a few additional features like auto-sharding and cross-region replication, and it provided Pushy with low latency and easy record expiry, both of which are critical for Pushy’s workload.\n\nAs Pushy’s portfolio grew, we experienced some pain points with Dynomite. Dynomite had great performance, but it required manual scaling as the system grew. The folks on the Cloud Data Engineering (CDE) team, the ones building the paved path for internal data at Netflix, graciously helped us scale it up and make adjustments, but it ended up being an involved process as we kept growing.\n\nThese pain points coincided with the introduction of KeyValue, which was a new offering from the CDE team that is roughly “HashMap as a service” for Netflix developers. KeyValue is an abstraction over the storage engine itself, which allows us to choose the best storage engine that meets our SLO needs. In our case, we value low latency — the faster we can read from KeyValue, the faster these messages can get delivered. With CDE’s help, we migrated our Push Registry to use KV instead, and we have been extremely satisfied with the result. After tuning our store for Pushy’s needs, it has been on autopilot since, appropriately scaling and serving our requests with very low latency.\n\nScaling Pushy horizontally and vertically\n\nMost of the other services our team runs, like apiproxy, the streaming edge proxy, are CPU bound, and we have autoscaling policies that scale them horizontally when we see an increase in CPU usage. This maps well to their workload — more HTTP requests means more CPU used, and we can scale up and down accordingly.\n\nPushy has slightly different performance characteristics, with each node maintaining many connections and delivering messages on demand. In Pushy’s case, CPU usage is consistently low, since most of the connections are parked and waiting for an occasional message. Instead of relying on CPU, we scale Pushy on the number of connections, with exponential scaling to scale faster after higher thresholds are reached. We load balance the initial HTTP requests to establish the connections and rely on a reconnect protocol where devices will reconnect every 30 minutes or so, with some staggering, that gives us a steady stream of reconnecting devices to balance connections across all available instances.\n\nFor a few years, our scaling policy had been that we would add new instances when the average number of connections reached 60,000 connections per instance. For a couple hundred million devices, this meant that we were regularly running thousands of Pushy instances. We can horizontally scale Pushy to our heart’s content, but we would be less content with our bill and would have to shard Pushy further to get around NLB connection limits. This evolution effort aligned well with an internal focus on cost efficiency, and we used this as an opportunity to revisit these earlier assumptions with an eye towards efficiency.\n\nBoth of these would be helped by increasing the number of connections that each Pushy node could handle, reducing the total number of Pushy instances and running more efficiently with the right balance between instance type, instance cost, and maximum concurrent connections. It would also allow us to have more breathing room with the NLB limits, reducing the toil of additional sharding as we continue to grow. That being said, increasing the number of connections per node is not without its own drawbacks. When a Pushy instance goes down, the devices that were connected to it will immediately try to reconnect. By increasing the number of connections per instance, it means that we would be increasing the number of devices that would be immediately trying to reconnect. We could have a million connections per instance, but a down node would lead to a thundering herd of a million devices reconnecting at the same time.\n\nThis delicate balance led to us doing a deep evaluation of many instance types and performance tuning options. Striking that balance, we ended up with instances that handle an average of 200,000 connections per node, with breathing room to go up to 400,000 connections if we had to. This makes for a nice balance between CPU usage, memory usage, and the thundering herd when a device connects. We’ve also enhanced our autoscaling policies to scale exponentially; the farther we are past our target average connection count, the more instances we’ll add. These improvements have enabled Pushy to be almost entirely hands off operationally, giving us plenty of flexibility as more devices come online in different patterns.\n\nReliability & building a stable foundation\n\nAlongside these efforts to scale Pushy for the future, we also took a close look at our reliability after finding some connectivity edge cases during recent feature development. We found a few areas for improvement around the connection between Pushy and the device, with failures due to Pushy attempting to send messages on a connection that had failed without notifying Pushy. Ideally something like a silent failure wouldn’t happen, but we frequently see odd client behavior, particularly on older devices.\n\nIn collaboration with the client teams, we were able to make some improvements. On the client side, better connection handling and improvements around the reconnect flow meant that they were more likely to reconnect appropriately. In Pushy, we added additional heartbeats, idle connection cleanup, and better connection tracking, which meant that we were keeping around fewer and fewer stale connections.\n\nWhile these improvements were mostly around those edge cases for the feature development, they had the side benefit of bumping our message delivery rates up even further. We already had a good message delivery rate, but this additional bump has enabled Pushy to regularly average 5 9s of message delivery reliability.\n\nPush message delivery success rate over a recent 2-week period.\n\nRecent developments\n\nWith this stable foundation and all of these connections, what can we now do with them? This question has been the driving force behind nearly all of the recent features built on top of Pushy, and it’s an exciting question to ask, particularly as an infrastructure team.\n\nShift towards direct push\n\nThe first change from Pushy’s traditional role is what we call direct push; instead of a backend service dropping the message on the asynchronous message queue, it can instead leverage the Push library to skip the asynchronous queue entirely. When called to deliver a message in the direct path, the Push library will look up the Pushy connected to the target device in the Push Registry, then send the message directly to that Pushy. Pushy will respond with a status code reflecting whether it was able to successfully deliver the message or it encountered an error, and the Push library will bubble that up to the calling code in the service.\n\nThe system diagram for the direct and indirect push paths.\n\nSusheel, the original author of Pushy, added this functionality as an optional path, but for years, nearly all backend services relied on the indirect path with its “best-effort” being good enough for their use cases. In recent years, we’ve seen usage of this direct path really take off as the needs of backend services have grown. In particular, rather than being just best effort, these direct messages allow the calling service to have immediate feedback about the delivery, letting them retry if a device they’re targeting has gone offline.\n\nThese days, messages sent via direct push make up the majority of messages sent through Pushy. For example, for a recent 24 hour period, direct messages averaged around 160,000 messages per second and indirect averaged at around 50,000 messages per second..\n\nGraph of direct vs indirect messages per second.\n\nDevice to device messaging\n\nAs we’ve thought through this evolving use case, our concept of a message sender has also evolved. What if we wanted to move past Pushy’s pattern of delivering server-side messages? What if we wanted to have a device send a message to a backend service, or maybe even to another device? Our messages had traditionally been unidirectional as we send messages from the server to the device, but we now leverage these bidirectional connections and direct device messaging to enable what we call device to device messaging. This device to device messaging supported early phone-to-TV communication in support of games like Triviaverse, and it’s the messaging foundation for our Companion Mode as TVs and phones communicate back and forth.\n\nA screenshot of one of the authors playing Triviaquest with a mobile device as the controller.\n\nThis requires higher level knowledge of the system, where we need to know not just information about a single device, but more broader information, like what devices are connected for an account that the phone can pair with. This also enables things like subscribing to device events to know when another device comes online and when they’re available to pair or send a message to. This has been built out with an additional service that receives device connection information from Pushy. These events, sent over a Kafka topic, let the service keep track of the device list for a given account. Devices can subscribe to these events, allowing them to receive a message from the service when another device for the same account comes online.\n\nPushy and its relationship with the Device List Service for discovering other devices.\n\nThis device list enables the discoverability aspect of these device to device messages. Once the devices have this knowledge of the other devices connected for the same account, they’re able to choose a target device from this list that they can then send messages to.\n\nOnce a device has that list, it can send a message to Pushy over its WebSocket connection with that device as the target in what we call a device to device message (1 in the diagram below). Pushy looks up the target device’s metadata in the Push registry (2) and sends the message to the second Pushy that the target device is connected to (3), as if it was the backend service in the direct push pattern above. That Pushy delivers the message to the target device (4), and the original Pushy will receive a status code in response, which it can pass back to the source device (5).\n\nA basic order of events for a device to device message.\n\nThe messaging protocol\n\nWe’ve defined a basic JSON-based message protocol for device to device messaging that lets these messages be passed from the source device to the target device. As a networking team, we naturally lean towards abstracting the communication layer with encapsulation wherever possible. This generalized message means that device teams are able to define their own protocols on top of these messages — Pushy would just be the transport layer, happily forwarding messages back and forth.\n\nThe client app protocol, built on top of the device to device protocol, built on top of Pushy.\n\nThis generalization paid off in terms of investment and operational support. We built the majority of this functionality in October 2022, and we’ve only needed small tweaks since then. We needed nearly no modifications as client teams built out the functionality on top of this layer, defining the higher level application-specific protocols that powered the features they were building. We really do enjoy working with our partner teams, but if we’re able to give them the freedom to build on top of our infrastructure layer without us getting involved, then we’re able to increase their velocity, make their lives easier, and play our infrastructure roles as message platform providers.\n\nWith early features in experimentation, Pushy sees an average of 1000 device to device messages per second, a number that will only continue to grow.\n\nGraph of device to device messages per second.\n\nThe Netty-gritty details\n\nIn Pushy, we handle incoming WebSocket messages in our PushClientProtocolHandler (code pointer to class in Zuul that we extend), which extends Netty’s ChannelInboundHandlerAdapter and is added to the Netty pipeline for each client connection. We listen for incoming WebSocket messages from the connected device in its channelRead method and parse the incoming message. If it’s a device to device message, we pass the message, the ChannelHandlerContext, and the PushUserAuth information about the connection’s identity to our DeviceToDeviceManager.\n\nA rough overview of the internal organization for these components.\n\nThe DeviceToDeviceManager is responsible for validating the message, doing some bookkeeping, and kicking off an async call that validates that the device is an authorized target, looks up the Pushy for the target device in the local cache (or makes a call to the data store if it’s not found), and forwards on the message. We run this asynchronously to avoid any event loop blocking due to these calls. The DeviceToDeviceManager is also responsible for observability, with metrics around cache hits, calls to the data store, message delivery rates, and latency percentile measurements. We’ve relied heavily on these metrics for alerts and optimizations — Pushy really is a metrics service that occasionally will deliver a message or two!\n\nSecurity\n\nAs the edge of the Netflix cloud, security considerations are always top of mind. With every connection over HTTPS, we’ve limited these messages to just authenticated WebSocket connections, added rate limiting, and added authorization checks to ensure that a device is able to target another device — you may have the best intentions in mind, but I’d strongly prefer it if you weren’t able to send arbitrary data to my personal TV from yours (and vice versa, I’m sure!).\n\nLatency and other considerations\n\nOne main consideration with the products built on top of this is latency, particularly when this feature is used for anything interactive within the Netflix app.\n\nWe’ve added caching to Pushy to reduce the number of lookups in the hotpath for things that are unlikely to change frequently, like a device’s allowed list of targets and the Pushy instance the target device is connected to. We have to do some lookups on the initial messages to know where to send them, but it enables us to send subsequent messages faster without any KeyValue lookups. For these requests where caching removed KeyValue from the hot path, we were able to greatly speed things up. From the incoming message arriving at Pushy to the response being sent back to the device, we reduced median latency to less than a millisecond, with the 99th percentile of latency at less than 4ms.\n\nOur KeyValue latency is usually very low, but we have seen brief periods of elevated read latencies due to underlying issues in our KeyValue datastore. Overall latencies increased for other parts of Pushy, like client registration, but we saw very little increase in device to device latency with this caching in place.\n\nCultural aspects that enable this work\n\nPushy’s scale and system design considerations make the work technically interesting, but we also deliberately focus on non-technical aspects that have helped to drive Pushy’s growth. We focus on iterative development that solves the hardest problem first, with projects frequently starting with quick hacks or prototypes to prove out a feature. As we do this initial version, we do our best to keep an eye towards the future, allowing us to move quickly from supporting a single, focused use case to a broad, generalized solution. For example, for our cross-device messaging, we were able to solve hard problems in the early work for Triviaverse that we later leveraged for the generic device to device solution.\n\nAs one can immediately see in the system diagrams above, Pushy does not exist in a vacuum, with projects frequently involving at least half a dozen teams. Trust, experience, communication, and strong relationships all enable this to work. Our team wouldn’t exist without our platform users, and we certainly wouldn’t be here writing this post without all of the work our product and client teams do. This has also emphasized the importance of building and sharing — if we’re able to get a prototype together with a device team, we’re able to then show it off to seed ideas from other teams. It’s one thing to mention that you can send these messages, but it’s another to show off the TV responding to the first click of the phone controller button!\n\nThe future of Pushy\n\nIf there’s anything certain in this world, it’s that Pushy will continue to grow and evolve. We have many new features in the works, like WebSocket message proxying, WebSocket message tracing, a global broadcast mechanism, and subscription functionality in support of Games and Live. With all of this investment, Pushy is a stable, reinforced foundation, ready for this next generation of features.\n\nWe’ll be writing about those new features as well — stay tuned for future posts.\n\nSpecial thanks to our stunning colleagues Jeremy Kelly and Justin Guerra who have both been invaluable to Pushy’s growth and the WebSocket ecosystem at large. We would also like to thank our larger teams and our numerous partners for their great work; it truly takes a village!", "label": "non_personal"}
{"title": "Introducing Netflix’s Key-Value Data Abstraction Layer", "url": "https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30?source=collection_home---4------14-----------------------", "content": "Introducing Netflix’s Key-Value Data Abstraction Layer Netflix Technology Blog 13 min read · Sep 18, 2024 -- 10 Listen Share\n\nVidhya Arvind, Rajasekhar Ummadisetty, Joey Lynch, Vinay Chella\n\nIntroduction\n\nAt Netflix our ability to deliver seamless, high-quality, streaming experiences to millions of users hinges on robust, global backend infrastructure. Central to this infrastructure is our use of multiple online distributed databases such as Apache Cassandra, a NoSQL database known for its high availability and scalability. Cassandra serves as the backbone for a diverse array of use cases within Netflix, ranging from user sign-ups and storing viewing histories to supporting real-time analytics and live streaming.\n\nOver time as new key-value databases were introduced and service owners launched new use cases, we encountered numerous challenges with datastore misuse. Firstly, developers struggled to reason about consistency, durability and performance in this complex global deployment across multiple stores. Second, developers had to constantly re-learn new data modeling practices and common yet critical data access patterns. These include challenges with tail latency and idempotency, managing “wide” partitions with many rows, handling single large “fat” columns, and slow response pagination. Additionally, the tight coupling with multiple native database APIs — APIs that continually evolve and sometimes introduce backward-incompatible changes — resulted in org-wide engineering efforts to maintain and optimize our microservice’s data access.\n\nTo overcome these challenges, we developed a holistic approach that builds upon our Data Gateway Platform. This approach led to the creation of several foundational abstraction services, the most mature of which is our Key-Value (KV) Data Abstraction Layer (DAL). This abstraction simplifies data access, enhances the reliability of our infrastructure, and enables us to support the broad spectrum of use cases that Netflix demands with minimal developer effort.\n\nIn this post, we dive deep into how Netflix’s KV abstraction works, the architectural principles guiding its design, the challenges we faced in scaling diverse use cases, and the technical innovations that have allowed us to achieve the performance and reliability required by Netflix’s global operations.\n\nThe Key-Value Service\n\nThe KV data abstraction service was introduced to solve the persistent challenges we faced with data access patterns in our distributed databases. Our goal was to build a versatile and efficient data storage solution that could handle a wide variety of use cases, ranging from the simplest hashmaps to more complex data structures, all while ensuring high availability, tunable consistency, and low latency.\n\nData Model\n\nAt its core, the KV abstraction is built around a two-level map architecture. The first level is a hashed string ID (the primary key), and the second level is a sorted map of a key-value pair of bytes. This model supports both simple and complex data models, balancing flexibility and efficiency.\n\nHashMap<String, SortedMap<Bytes, Bytes>>\n\nFor complex data models such as structured Records or time-ordered Events , this two-level approach handles hierarchical structures effectively, allowing related data to be retrieved together. For simpler use cases, it also represents flat key-value Maps (e.g. id → {\"\" → value} ) or named Sets (e.g. id → {key → \"\"} ). This adaptability allows the KV abstraction to be used in hundreds of diverse use cases, making it a versatile solution for managing both simple and complex data models in large-scale infrastructures like Netflix.\n\nThe KV data can be visualized at a high level, as shown in the diagram below, where three records are shown.\n\nmessage Item (\n\nBytes key,\n\nBytes value,\n\nMetadata metadata,\n\nInteger chunk\n\n)\n\nDatabase Agnostic Abstraction\n\nThe KV abstraction is designed to hide the implementation details of the underlying database, offering a consistent interface to application developers regardless of the optimal storage system for that use case. While Cassandra is one example, the abstraction works with multiple data stores like EVCache, DynamoDB, RocksDB, etc…\n\nFor example, when implemented with Cassandra, the abstraction leverages Cassandra’s partitioning and clustering capabilities. The record ID acts as the partition key, and the item key as the clustering column:\n\nThe corresponding Data Definition Language (DDL) for this structure in Cassandra is:\n\nCREATE TABLE IF NOT EXISTS <ns>.<table> (\n\nid text,\n\nkey blob,\n\nvalue blob,\n\nvalue_metadata blob,\n\n\n\nPRIMARY KEY (id, key))\n\nWITH CLUSTERING ORDER BY (key <ASC|DESC>)\n\nNamespace: Logical and Physical Configuration\n\nA namespace defines where and how data is stored, providing logical and physical separation while abstracting the underlying storage systems. It also serves as central configuration of access patterns such as consistency or latency targets. Each namespace may use different backends: Cassandra, EVCache, or combinations of multiple. This flexibility allows our Data Platform to route different use cases to the most suitable storage system based on performance, durability, and consistency needs. Developers just provide their data problem rather than a database solution!\n\nIn this example configuration, the ngsegment namespace is backed by both a Cassandra cluster and an EVCache caching layer, allowing for highly durable persistent storage and lower-latency point reads.\n\n\"persistence_configuration\":[\n\n{\n\n\"id\":\"PRIMARY_STORAGE\",\n\n\"physical_storage\": {\n\n\"type\":\"CASSANDRA\",\n\n\"cluster\":\"cassandra_kv_ngsegment\",\n\n\"dataset\":\"ngsegment\",\n\n\"table\":\"ngsegment\",\n\n\"regions\": [\"us-east-1\"],\n\n\"config\": {\n\n\"consistency_scope\": \"LOCAL\",\n\n\"consistency_target\": \"READ_YOUR_WRITES\"\n\n}\n\n}\n\n},\n\n{\n\n\"id\":\"CACHE\",\n\n\"physical_storage\": {\n\n\"type\":\"CACHE\",\n\n\"cluster\":\"evcache_kv_ngsegment\"\n\n},\n\n\"config\": {\n\n\"default_cache_ttl\": 180s\n\n}\n\n}\n\n]\n\n\n\nKey APIs of the KV Abstraction\n\nTo support diverse use-cases, the KV abstraction provides four basic CRUD APIs:\n\nPutItems — Write one or more Items to a Record\n\nThe PutItems API is an upsert operation, it can insert new data or update existing data in the two-level map structure.\n\nmessage PutItemRequest (\n\nIdempotencyToken idempotency_token,\n\nstring namespace,\n\nstring id,\n\nList<Item> items\n\n)\n\nAs you can see, the request includes the namespace, Record ID, one or more items, and an idempotency token to ensure retries of the same write are safe. Chunked data can be written by staging chunks and then committing them with appropriate metadata (e.g. number of chunks).\n\nGetItems — Read one or more Items from a Record\n\nThe GetItems API provides a structured and adaptive way to fetch data using ID, predicates, and selection mechanisms. This approach balances the need to retrieve large volumes of data while meeting stringent Service Level Objectives (SLOs) for performance and reliability.\n\nmessage GetItemsRequest (\n\nString namespace,\n\nString id,\n\nPredicate predicate,\n\nSelection selection,\n\nMap<String, Struct> signals\n\n)\n\nThe GetItemsRequest includes several key parameters:\n\nNamespace : Specifies the logical dataset or table\n\n: Specifies the logical dataset or table Id : Identifies the entry in the top-level HashMap\n\n: Identifies the entry in the top-level HashMap Predicate : Filters the matching items and can retrieve all items ( match_all ), specific items ( match_keys ), or a range ( match_range )\n\n: Filters the matching items and can retrieve all items ( ), specific items ( ), or a range ( ) Selection : Narrows returned responses for example page_size_bytes for pagination, item_limit for limiting the total number of items across pages and include / exclude to include or exclude large values from responses\n\n: Narrows returned responses for example for pagination, for limiting the total number of items across pages and / to include or exclude large values from responses Signals: Provides in-band signaling to indicate client capabilities, such as supporting client compression or chunking.\n\nThe GetItemResponse message contains the matching data:\n\nmessage GetItemResponse (\n\nList<Item> items,\n\nOptional<String> next_page_token\n\n)\n\nItems : A list of retrieved items based on the Predicate and Selection defined in the request.\n\n: A list of retrieved items based on the and defined in the request. Next Page Token: An optional token indicating the position for subsequent reads if needed, essential for handling large data sets across multiple requests. Pagination is a critical component for efficiently managing data retrieval, especially when dealing with large datasets that could exceed typical response size limits.\n\nDeleteItems — Delete one or more Items from a Record\n\nThe DeleteItems API provides flexible options for removing data, including record-level, item-level, and range deletes — all while supporting idempotency.\n\nmessage DeleteItemsRequest (\n\nIdempotencyToken idempotency_token,\n\nString namespace,\n\nString id,\n\nPredicate predicate\n\n)\n\n\n\nJust like in the GetItems API, the Predicate allows one or more Items to be addressed at once:\n\nRecord-Level Deletes (match_all) : Removes the entire record in constant latency regardless of the number of items in the record.\n\n: Removes the entire record in constant latency regardless of the number of items in the record. Item-Range Deletes (match_range) : This deletes a range of items within a Record. Useful for keeping “n-newest” or prefix path deletion.\n\n: This deletes a range of items within a Record. Useful for keeping “n-newest” or prefix path deletion. Item-Level Deletes (match_keys): Deletes one or more individual items.\n\nSome storage engines (any store which defers true deletion) such as Cassandra struggle with high volumes of deletes due to tombstone and compaction overhead. Key-Value optimizes both record and range deletes to generate a single tombstone for the operation — you can learn more about tombstones in About Deletes and Tombstones.\n\nItem-level deletes create many tombstones but KV hides that storage engine complexity via TTL-based deletes with jitter. Instead of immediate deletion, item metadata is updated as expired with randomly jittered TTL applied to stagger deletions. This technique maintains read pagination protections. While this doesn’t completely solve the problem it reduces load spikes and helps maintain consistent performance while compaction catches up. These strategies help maintain system performance, reduce read overhead, and meet SLOs by minimizing the impact of deletes.\n\nComplex Mutate and Scan APIs\n\nBeyond simple CRUD on single Records, KV also supports complex multi-item and multi-record mutations and scans via MutateItems and ScanItems APIs. PutItems also supports atomic writes of large blob data within a single Item via a chunked protocol. These complex APIs require careful consideration to ensure predictable linear low-latency and we will share details on their implementation in a future post.\n\nDesign Philosophies for reliable and predictable performance\n\nIdempotency to fight tail latencies\n\nTo ensure data integrity the PutItems and DeleteItems APIs use idempotency tokens, which uniquely identify each mutative operation and guarantee that operations are logically executed in order, even when hedged or retried for latency reasons. This is especially crucial in last-write-wins databases like Cassandra, where ensuring the correct order and de-duplication of requests is vital.\n\nIn the Key-Value abstraction, idempotency tokens contain a generation timestamp and random nonce token. Either or both may be required by backing storage engines to de-duplicate mutations.\n\nmessage IdempotencyToken (\n\nTimestamp generation_time,\n\nString token\n\n)\n\nAt Netflix, client-generated monotonic tokens are preferred due to their reliability, especially in environments where network delays could impact server-side token generation. This combines a client provided monotonic generation_time timestamp with a 128 bit random UUID token . Although clock-based token generation can suffer from clock skew, our tests on EC2 Nitro instances show drift is minimal (under 1 millisecond). In some cases that require stronger ordering, regionally unique tokens can be generated using tools like Zookeeper, or globally unique tokens such as a transaction IDs can be used.\n\nThe following graphs illustrate the observed clock skew on our Cassandra fleet, suggesting the safety of this technique on modern cloud VMs with direct access to high-quality clocks. To further maintain safety, KV servers reject writes bearing tokens with large drift both preventing silent write discard (write has timestamp far in past) and immutable doomstones (write has a timestamp far in future) in storage engines vulnerable to those.\n\nHandling Large Data through Chunking\n\nKey-Value is also designed to efficiently handle large blobs, a common challenge for traditional key-value stores. Databases often face limitations on the amount of data that can be stored per key or partition. To address these constraints, KV uses transparent chunking to manage large data efficiently.\n\nFor items smaller than 1 MiB, data is stored directly in the main backing storage (e.g. Cassandra), ensuring fast and efficient access. However, for larger items, only the id, key, and metadata are stored in the primary storage, while the actual data is split into smaller chunks and stored separately in chunk storage. This chunk storage can also be Cassandra but with a different partitioning scheme optimized for handling large values. The idempotency token ties all these writes together into one atomic operation.\n\nBy splitting large items into chunks, we ensure that latency scales linearly with the size of the data, making the system both predictable and efficient. A future blog post will describe the chunking architecture in more detail, including its intricacies and optimization strategies.\n\nClient-Side Compression\n\nThe KV abstraction leverages client-side payload compression to optimize performance, especially for large data transfers. While many databases offer server-side compression, handling compression on the client side reduces expensive server CPU usage, network bandwidth, and disk I/O. In one of our deployments, which helps power Netflix’s search, enabling client-side compression reduced payload sizes by 75%, significantly improving cost efficiency.\n\nSmarter Pagination\n\nWe chose payload size in bytes as the limit per response page rather than the number of items because it allows us to provide predictable operation SLOs. For instance, we can provide a single-digit millisecond SLO on a 2 MiB page read. Conversely, using the number of items per page as the limit would result in unpredictable latencies due to significant variations in item size. A request for 10 items per page could result in vastly different latencies if each item was 1 KiB versus 1 MiB.\n\nUsing bytes as a limit poses challenges as few backing stores support byte-based pagination; most data stores use the number of results e.g. DynamoDB and Cassandra limit by number of items or rows. To address this, we use a static limit for the initial queries to the backing store, query with this limit, and process the results. If more data is needed to meet the byte limit, additional queries are executed until the limit is met, the excess result is discarded and a page token is generated.\n\nThis static limit can lead to inefficiencies, one large item in the result may cause us to discard many results, while small items may require multiple iterations to fill a page, resulting in read amplification. To mitigate these issues, we implemented adaptive pagination which dynamically tunes the limits based on observed data.\n\nAdaptive Pagination\n\nWhen an initial request is made, a query is executed in the storage engine, and the results are retrieved. As the consumer processes these results, the system tracks the number of items consumed and the total size used. This data helps calculate an approximate item size, which is stored in the page token. For subsequent page requests, this stored information allows the server to apply the appropriate limits to the underlying storage, reducing unnecessary work and minimizing read amplification.\n\nWhile this method is effective for follow-up page requests, what happens with the initial request? In addition to storing item size information in the page token, the server also estimates the average item size for a given namespace and caches it locally. This cached estimate helps the server set a more optimal limit on the backing store for the initial request, improving efficiency. The server continuously adjusts this limit based on recent query patterns or other factors to keep it accurate. For subsequent pages, the server uses both the cached data and the information in the page token to fine-tune the limits.\n\nIn addition to adaptive pagination, a mechanism is in place to send a response early if the server detects that processing the request is at risk of exceeding the request’s latency SLO.\n\nFor example, let us assume a client submits a GetItems request with a per-page limit of 2 MiB and a maximum end-to-end latency limit of 500ms. While processing this request, the server retrieves data from the backing store. This particular record has thousands of small items so it would normally take longer than the 500ms SLO to gather the full page of data. If this happens, the client would receive an SLO violation error, causing the request to fail even though there is nothing exceptional. To prevent this, the server tracks the elapsed time while fetching data. If it determines that continuing to retrieve more data might breach the SLO, the server will stop processing further results and return a response with a pagination token.\n\nThis approach ensures that requests are processed within the SLO, even if the full page size isn’t met, giving clients predictable progress. Furthermore, if the client is a gRPC server with proper deadlines, the client is smart enough not to issue further requests, reducing useless work.\n\nIf you want to know more, the How Netflix Ensures Highly-Reliable Online Stateful Systems article talks in further detail about these and many other techniques.\n\nSignaling\n\nKV uses in-band messaging we call signaling that allows the dynamic configuration of the client and enables it to communicate its capabilities to the server. This ensures that configuration settings and tuning parameters can be exchanged seamlessly between the client and server. Without signaling, the client would need static configuration — requiring a redeployment for each change — or, with dynamic configuration, would require coordination with the client team.\n\nFor server-side signals, when the client is initialized, it sends a handshake to the server. The server responds back with signals, such as target or max latency SLOs, allowing the client to dynamically adjust timeouts and hedging policies. Handshakes are then made periodically in the background to keep the configuration current. For client-communicated signals, the client, along with each request, communicates its capabilities, such as whether it can handle compression, chunking, and other features.\n\nKV Usage @ Netflix\n\nThe KV abstraction powers several key Netflix use cases, including:\n\nStreaming Metadata : High-throughput, low-latency access to streaming metadata, ensuring personalized content delivery in real-time.\n\n: High-throughput, low-latency access to streaming metadata, ensuring personalized content delivery in real-time. User Profiles : Efficient storage and retrieval of user preferences and history, enabling seamless, personalized experiences across devices.\n\n: Efficient storage and retrieval of user preferences and history, enabling seamless, personalized experiences across devices. Messaging : Storage and retrieval of push registry for messaging needs, enabling the millions of requests to flow through.\n\n: Storage and retrieval of push registry for messaging needs, enabling the millions of requests to flow through. Real-Time Analytics: This persists large-scale impression and provides insights into user behavior and system performance, moving data from offline to online and vice versa.\n\nFuture Enhancements\n\nLooking forward, we plan to enhance the KV abstraction with:\n\nLifecycle Management : Fine-grained control over data retention and deletion.\n\n: Fine-grained control over data retention and deletion. Summarization : Techniques to improve retrieval efficiency by summarizing records with many items into fewer backing rows.\n\n: Techniques to improve retrieval efficiency by summarizing records with many items into fewer backing rows. New Storage Engines : Integration with more storage systems to support new use cases.\n\n: Integration with more storage systems to support new use cases. Dictionary Compression: Further reducing data size while maintaining performance.\n\nConclusion\n\nThe Key-Value service at Netflix is a flexible, cost-effective solution that supports a wide range of data patterns and use cases, from low to high traffic scenarios, including critical Netflix streaming use-cases. The simple yet robust design allows it to handle diverse data models like HashMaps, Sets, Event storage, Lists, and Graphs. It abstracts the complexity of the underlying databases from our developers, which enables our application engineers to focus on solving business problems instead of becoming experts in every storage engine and their distributed consistency models. As Netflix continues to innovate in online datastores, the KV abstraction remains a central component in managing data efficiently and reliably at scale, ensuring a solid foundation for future growth.\n\nAcknowledgments: Special thanks to our stunning colleagues who contributed to Key Value’s success: William Schor, Mengqing Wang, Chandrasekhar Thumuluru, Rajiv Shringi, John Lu, George Cambell, Ammar Khaku, Jordan West, Chris Lohfink, Matt Lehman, and the whole online datastores team (ODS, f.k.a CDE).", "label": "non_personal"}
{"title": "Introducing Netflix’s TimeSeries Data Abstraction Layer", "url": "https://netflixtechblog.com/introducing-netflix-timeseries-data-abstraction-layer-31552f6326f8?source=collection_home---4------13-----------------------", "content": "Introducing Netflix’s TimeSeries Data Abstraction Layer Netflix Technology Blog 18 min read · Oct 8, 2024 -- 16 Listen Share\n\nBy Rajiv Shringi, Vinay Chella, Kaidan Fullerton, Oleksii Tkachuk, Joey Lynch\n\nIntroduction\n\nAs Netflix continues to expand and diversify into various sectors like Video on Demand and Gaming, the ability to ingest and store vast amounts of temporal data — often reaching petabytes — with millisecond access latency has become increasingly vital. In previous blog posts, we introduced the Key-Value Data Abstraction Layer and the Data Gateway Platform, both of which are integral to Netflix’s data architecture. The Key-Value Abstraction offers a flexible, scalable solution for storing and accessing structured key-value data, while the Data Gateway Platform provides essential infrastructure for protecting, configuring, and deploying the data tier.\n\nBuilding on these foundational abstractions, we developed the TimeSeries Abstraction — a versatile and scalable solution designed to efficiently store and query large volumes of temporal event data with low millisecond latencies, all in a cost-effective manner across various use cases.\n\nIn this post, we will delve into the architecture, design principles, and real-world applications of the TimeSeries Abstraction, demonstrating how it enhances our platform’s ability to manage temporal data at scale.\n\nNote: Contrary to what the name may suggest, this system is not built as a general-purpose time series database. We do not use it for metrics, histograms, timers, or any such near-real time analytics use case. Those use cases are well served by the Netflix Atlas telemetry system. Instead, we focus on addressing the challenge of storing and accessing extremely high-throughput, immutable temporal event data in a low-latency and cost-efficient manner.\n\nChallenges\n\nAt Netflix, temporal data is continuously generated and utilized, whether from user interactions like video-play events, asset impressions, or complex micro-service network activities. Effectively managing this data at scale to extract valuable insights is crucial for ensuring optimal user experiences and system reliability.\n\nHowever, storing and querying such data presents a unique set of challenges:\n\nHigh Throughput : Managing up to 10 million writes per second while maintaining high availability.\n\n: Managing up to 10 million writes per second while maintaining high availability. Efficient Querying in Large Datasets : Storing petabytes of data while ensuring primary key reads return results within low double-digit milliseconds, and supporting searches and aggregations across multiple secondary attributes.\n\n: Storing petabytes of data while ensuring primary key reads return results within low double-digit milliseconds, and supporting searches and aggregations across multiple secondary attributes. Global Reads and Writes : Facilitating read and write operations from anywhere in the world with adjustable consistency models.\n\n: Facilitating read and write operations from anywhere in the world with adjustable consistency models. Tunable Configuration : Offering the ability to partition datasets in either a single-tenant or multi-tenant datastore, with options to adjust various dataset aspects such as retention and consistency.\n\n: Offering the ability to partition datasets in either a single-tenant or multi-tenant datastore, with options to adjust various dataset aspects such as retention and consistency. Handling Bursty Traffic : Managing significant traffic spikes during high-demand events, such as new content launches or regional failovers.\n\n: Managing significant traffic spikes during high-demand events, such as new content launches or regional failovers. Cost Efficiency: Reducing the cost per byte and per operation to optimize long-term retention while minimizing infrastructure expenses, which can amount to millions of dollars for Netflix.\n\nTimeSeries Abstraction\n\nThe TimeSeries Abstraction was developed to meet these requirements, built around the following core design principles:\n\nPartitioned Data : Data is partitioned using a unique temporal partitioning strategy combined with an event bucketing approach to efficiently manage bursty workloads and streamline queries.\n\n: Data is partitioned using a unique temporal partitioning strategy combined with an event bucketing approach to efficiently manage bursty workloads and streamline queries. Flexible Storage : The service is designed to integrate with various storage backends, including Apache Cassandra and Elasticsearch, allowing Netflix to customize storage solutions based on specific use case requirements.\n\n: The service is designed to integrate with various storage backends, including Apache Cassandra and Elasticsearch, allowing Netflix to customize storage solutions based on specific use case requirements. Configurability : TimeSeries offers a range of tunable options for each dataset, providing the flexibility needed to accommodate a wide array of use cases.\n\n: TimeSeries offers a range of tunable options for each dataset, providing the flexibility needed to accommodate a wide array of use cases. Scalability : The architecture supports both horizontal and vertical scaling, enabling the system to handle increasing throughput and data volumes as Netflix expands its user base and services.\n\n: The architecture supports both horizontal and vertical scaling, enabling the system to handle increasing throughput and data volumes as Netflix expands its user base and services. Sharded Infrastructure: Leveraging the Data Gateway Platform, we can deploy single-tenant and/or multi-tenant infrastructure with the necessary access and traffic isolation.\n\nLet’s dive into the various aspects of this abstraction.\n\nData Model\n\nWe follow a unique event data model that encapsulates all the data we want to capture for events, while allowing us to query them efficiently.\n\nLet’s start with the smallest unit of data in the abstraction and work our way up.\n\nEvent Item : An event item is a key-value pair that users use to store data for a given event. For example: {“device_type”: “ios”}.\n\n: An event item is a key-value pair that users use to store data for a given event. For example: {“device_type”: “ios”}. Event : An event is a structured collection of one or more such event items. An event occurs at a specific point in time and is identified by a client-generated timestamp and an event identifier (such as a UUID). This combination of event_time and event_id also forms part of the unique idempotency key for the event, enabling users to safely retry requests.\n\n: An event is a structured collection of one or more such event items. An event occurs at a specific point in time and is identified by a client-generated timestamp and an event identifier (such as a UUID). This combination of and also forms part of the unique idempotency key for the event, enabling users to safely retry requests. Time Series ID : A time_series_id is a collection of one or more such events over the dataset’s retention period. For instance, a device_id would store all events occurring for a given device over the retention period. All events are immutable, and the TimeSeries service only ever appends events to a given time series ID.\n\n: A is a collection of one or more such events over the dataset’s retention period. For instance, a would store all events occurring for a given device over the retention period. All events are immutable, and the TimeSeries service only ever appends events to a given time series ID. Namespace: A namespace is a collection of time series IDs and event data, representing the complete TimeSeries dataset. Users can create one or more namespaces for each of their use cases. The abstraction applies various tunable options at the namespace level, which we will discuss further when we explore the service’s control plane.\n\nAPI\n\nThe abstraction provides the following APIs to interact with the event data.\n\nWriteEventRecordsSync: This endpoint writes a batch of events and sends back a durability acknowledgement to the client. This is used in cases where users require a guarantee of durability.\n\nWriteEventRecords: This is the fire-and-forget version of the above endpoint. It enqueues a batch of events without the durability acknowledgement. This is used in cases like logging or tracing, where users care more about throughput and can tolerate a small amount of data loss.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"events\": [\n\n{\n\n\"timeSeriesId\": \"profile100\",\n\n\"eventTime\": \"2024-10-03T21:24:23.988Z\",\n\n\"eventId\": \"550e8400-e29b-41d4-a716-446655440000\",\n\n\"eventItems\": [\n\n{\n\n\"eventItemKey\": \"deviceType\",\n\n\"eventItemValue\": \"aW9z\"\n\n},\n\n{\n\n\"eventItemKey\": \"deviceMetadata\",\n\n\"eventItemValue\": \"c29tZSBtZXRhZGF0YQ==\"\n\n}\n\n]\n\n},\n\n{\n\n\"timeSeriesId\": \"profile100\",\n\n\"eventTime\": \"2024-10-03T21:23:30.000Z\",\n\n\"eventId\": \"123e4567-e89b-12d3-a456-426614174000\",\n\n\"eventItems\": [\n\n{\n\n\"eventItemKey\": \"deviceType\",\n\n\"eventItemValue\": \"YW5kcm9pZA==\"\n\n}\n\n]\n\n}\n\n]\n\n}\n\nReadEventRecords: Given a combination of a namespace, a timeSeriesId, a timeInterval, and optional eventFilters, this endpoint returns all the matching events, sorted descending by event_time, with low millisecond latency.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"timeSeriesId\": \"profile100\",\n\n\"timeInterval\": {\n\n\"start\": \"2024-10-02T21:00:00.000Z\",\n\n\"end\": \"2024-10-03T21:00:00.000Z\"\n\n},\n\n\"eventFilters\": [\n\n{\n\n\"matchEventItemKey\": \"deviceType\",\n\n\"matchEventItemValue\": \"aW9z\"\n\n}\n\n],\n\n\"pageSize\": 100,\n\n\"totalRecordLimit\": 1000\n\n}\n\nSearchEventRecords: Given a search criteria and a time interval, this endpoint returns all the matching events. These use cases are fine with eventually consistent reads.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"timeInterval\": {\n\n\"start\": \"2024-10-02T21:00:00.000Z\",\n\n\"end\": \"2024-10-03T21:00:00.000Z\"\n\n},\n\n\"searchQuery\": {\n\n\"booleanQuery\": {\n\n\"searchQuery\": [\n\n{\n\n\"equals\": {\n\n\"eventItemKey\": \"deviceType\",\n\n\"eventItemValue\": \"aW9z\"\n\n}\n\n},\n\n{\n\n\"range\": {\n\n\"eventItemKey\": \"deviceRegistrationTimestamp\",\n\n\"lowerBound\": {\n\n\"eventItemValue\": \"MjAyNC0xMC0wMlQwMDowMDowMC4wMDBa\",\n\n\"inclusive\": true\n\n},\n\n\"upperBound\": {\n\n\"eventItemValue\": \"MjAyNC0xMC0wM1QwMDowMDowMC4wMDBa\"\n\n}\n\n}\n\n}\n\n],\n\n\"operator\": \"AND\"\n\n}\n\n},\n\n\"pageSize\": 100,\n\n\"totalRecordLimit\": 1000\n\n}\n\nAggregateEventRecords: Given a search criteria and an aggregation mode (e.g. DistinctAggregation) , this endpoint performs the given aggregation within a given time interval. Similar to the Search endpoint, users can tolerate eventual consistency and a potentially higher latency (in seconds).\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"timeInterval\": {\n\n\"start\": \"2024-10-02T21:00:00.000Z\",\n\n\"end\": \"2024-10-03T21:00:00.000Z\"\n\n},\n\n\"searchQuery\": {...some search criteria...},\n\n\"aggregationQuery\": {\n\n\"distinct\": {\n\n\"eventItemKey\": \"deviceType\",\n\n\"pageSize\": 100\n\n}\n\n}\n\n}\n\nIn the subsequent sections, we will talk about how we interact with this data at the storage layer.\n\nStorage Layer\n\nThe storage layer for TimeSeries comprises a primary data store and an optional index data store. The primary data store ensures data durability during writes and is used for primary read operations, while the index data store is utilized for search and aggregate operations. At Netflix, Apache Cassandra is the preferred choice for storing durable data in high-throughput scenarios, while Elasticsearch is the preferred data store for indexing. However, similar to our approach with the API, the storage layer is not tightly coupled to these specific data stores. Instead, we define storage API contracts that must be fulfilled, allowing us the flexibility to replace the underlying data stores as needed.\n\nPrimary Datastore\n\nIn this section, we will talk about how we leverage Apache Cassandra for TimeSeries use cases.\n\nPartitioning Scheme\n\nAt Netflix’s scale, the continuous influx of event data can quickly overwhelm traditional databases. Temporal partitioning addresses this challenge by dividing the data into manageable chunks based on time intervals, such as hourly, daily, or monthly windows. This approach enables efficient querying of specific time ranges without the need to scan the entire dataset. It also allows Netflix to archive, compress, or delete older data efficiently, optimizing both storage and query performance. Additionally, this partitioning mitigates the performance issues typically associated with wide partitions in Cassandra. By employing this strategy, we can operate at much higher disk utilization, as it reduces the need to reserve large amounts of disk space for compactions, thereby saving costs.\n\nHere is what it looks like :\n\nTime Slice: A time slice is the unit of data retention and maps directly to a Cassandra table. We create multiple such time slices, each covering a specific interval of time. An event lands in one of these slices based on the event_time. These slices are joined with no time gaps in between, with operations being start-inclusive and end-exclusive, ensuring that all data lands in one of the slices. By utilizing these time slices, we can efficiently implement retention by dropping entire tables, which reduces storage space and saves on costs.\n\nWhy not use row-based Time-To-Live (TTL)?\n\nUsing TTL on individual events would generate a significant number of tombstones in Cassandra, degrading performance, especially during range scans. By employing discrete time slices and dropping them, we avoid the tombstone issue entirely. The tradeoff is that data may be retained slightly longer than necessary, as an entire table’s time range must fall outside the retention window before it can be dropped. Additionally, TTLs are difficult to adjust later, whereas TimeSeries can extend the dataset retention instantly with a single control plane operation.\n\nTime Buckets: Within a time slice, data is further partitioned into time buckets. This facilitates effective range scans by allowing us to target specific time buckets for a given query range. The tradeoff is that if a user wants to read the entire range of data over a large time period, we must scan many partitions. We mitigate potential latency by scanning these partitions in parallel and aggregating the data at the end. In most cases, the advantage of targeting smaller data subsets outweighs the read amplification from these scatter-gather operations. Typically, users read a smaller subset of data rather than the entire retention range.\n\nEvent Buckets: To manage extremely high-throughput write operations, which may result in a burst of writes for a given time series within a short period, we further divide the time bucket into event buckets. This prevents overloading the same partition for a given time range and also reduces partition sizes further, albeit with a slight increase in read amplification.\n\nNote: With Cassandra 4.x onwards, we notice a substantial improvement in the performance of scanning a range of data in a wide partition. See Future Enhancements at the end to see the Dynamic Event bucketing work that aims to take advantage of this.\n\nStorage Tables\n\nWe use two kinds of tables\n\nData tables : These are the time slices that store the actual event data.\n\n: These are the time slices that store the actual event data. Metadata table: This table stores information about how each time slice is configured per namespace.\n\nData tables\n\nThe partition key enables splitting events for a time_series_id over a range of time_bucket(s) and event_bucket(s), thus mitigating hot partitions, while the clustering key allows us to keep data sorted on disk in the order we almost always want to read it. The value_metadata column stores metadata for the event_item_value such as compression.\n\nWriting to the data table:\n\nUser writes will land in a given time slice, time bucket, and event bucket as a factor of the event_time attached to the event. This factor is dictated by the control plane configuration of a given namespace.\n\nFor example:\n\nDuring this process, the writer makes decisions on how to handle the data before writing, such as whether to compress it. The value_metadata column records any such post-processing actions, ensuring that the reader can accurately interpret the data.\n\nReading from the data table:\n\nThe below illustration depicts at a high-level on how we scatter-gather the reads from multiple partitions and join the result set at the end to return the final result.\n\nMetadata table\n\nThis table stores the configuration data about the time slices for a given namespace.\n\nNote the following:\n\nNo Time Gaps : The end_time of a given time slice overlaps with the start_time of the next time slice, ensuring all events find a home.\n\n: The end_time of a given time slice overlaps with the start_time of the next time slice, ensuring all events find a home. Retention : The status indicates which tables fall inside and outside of the retention window.\n\n: The status indicates which tables fall inside and outside of the retention window. Flexible: This metadata can be adjusted per time slice, allowing us to tune the partition settings of future time slices based on observed data patterns in the current time slice.\n\nThere is a lot more information that can be stored into the metadata column (e.g., compaction settings for the table), but we only show the partition settings here for brevity.\n\nIndex Datastore\n\nTo support secondary access patterns via non-primary key attributes, we index data into Elasticsearch. Users can configure a list of attributes per namespace that they wish to search and/or aggregate data on. The service extracts these fields from events as they stream in, indexing the resultant documents into Elasticsearch. Depending on the throughput, we may use Elasticsearch as a reverse index, retrieving the full data from Cassandra, or we may store the entire source data directly in Elasticsearch.\n\nNote: Again, users are never directly exposed to Elasticsearch, just like they are not directly exposed to Cassandra. Instead, they interact with the Search and Aggregate API endpoints that translate a given query to that needed for the underlying datastore.\n\nIn the next section, we will talk about how we configure these data stores for different datasets.\n\nControl Plane\n\nThe data plane is responsible for executing the read and write operations, while the control plane configures every aspect of a namespace’s behavior. The data plane communicates with the TimeSeries control stack, which manages this configuration information. In turn, the TimeSeries control stack interacts with a sharded Data Gateway Platform Control Plane that oversees control configurations for all abstractions and namespaces.\n\nSeparating the responsibilities of the data plane and control plane helps maintain the high availability of our data plane, as the control plane takes on tasks that may require some form of schema consensus from the underlying data stores.\n\nNamespace Configuration\n\nThe below configuration snippet demonstrates the immense flexibility of the service and how we can tune several things per namespace using our control plane.\n\n\"persistence_configuration\": [\n\n{\n\n\"id\": \"PRIMARY_STORAGE\",\n\n\"physical_storage\": {\n\n\"type\": \"CASSANDRA\", // type of primary storage\n\n\"cluster\": \"cass_dgw_ts_tracing\", // physical cluster name\n\n\"dataset\": \"tracing_default\" // maps to the keyspace\n\n},\n\n\"config\": {\n\n\"timePartition\": {\n\n\"secondsPerTimeSlice\": \"129600\", // width of a time slice\n\n\"secondPerTimeBucket\": \"3600\", // width of a time bucket\n\n\"eventBuckets\": 4 // how many event buckets within\n\n},\n\n\"queueBuffering\": {\n\n\"coalesce\": \"1s\", // how long to coalesce writes\n\n\"bufferCapacity\": 4194304 // queue capacity in bytes\n\n},\n\n\"consistencyScope\": \"LOCAL\", // single-region/multi-region\n\n\"consistencyTarget\": \"EVENTUAL\", // read/write consistency\n\n\"acceptLimit\": \"129600s\" // how far back writes are allowed\n\n},\n\n\"lifecycleConfigs\": {\n\n\"lifecycleConfig\": [ // Primary store data retention\n\n{\n\n\"type\": \"retention\",\n\n\"config\": {\n\n\"close_after\": \"1296000s\", // close for reads/writes\n\n\"delete_after\": \"1382400s\" // drop time slice\n\n}\n\n}\n\n]\n\n}\n\n},\n\n{\n\n\"id\": \"INDEX_STORAGE\",\n\n\"physicalStorage\": {\n\n\"type\": \"ELASTICSEARCH\", // type of index storage\n\n\"cluster\": \"es_dgw_ts_tracing\", // ES cluster name\n\n\"dataset\": \"tracing_default_useast1\" // base index name\n\n},\n\n\"config\": {\n\n\"timePartition\": {\n\n\"secondsPerSlice\": \"129600\" // width of the index slice\n\n},\n\n\"consistencyScope\": \"LOCAL\",\n\n\"consistencyTarget\": \"EVENTUAL\", // how should we read/write data\n\n\"acceptLimit\": \"129600s\", // how far back writes are allowed\n\n\"indexConfig\": {\n\n\"fieldMapping\": { // fields to extract to index\n\n\"tags.nf.app\": \"KEYWORD\",\n\n\"tags.duration\": \"INTEGER\",\n\n\"tags.enabled\": \"BOOLEAN\"\n\n},\n\n\"refreshInterval\": \"60s\" // Index related settings\n\n}\n\n},\n\n\"lifecycleConfigs\": {\n\n\"lifecycleConfig\": [\n\n{\n\n\"type\": \"retention\", // Index retention settings\n\n\"config\": {\n\n\"close_after\": \"1296000s\",\n\n\"delete_after\": \"1382400s\"\n\n}\n\n}\n\n]\n\n}\n\n}\n\n]\n\nProvisioning Infrastructure\n\nWith so many different parameters, we need automated provisioning workflows to deduce the best settings for a given workload. When users want to create their namespaces, they specify a list of workload desires, which the automation translates into concrete infrastructure and related control plane configuration. We highly encourage you to watch this ApacheCon talk, by one of our stunning colleagues Joey Lynch, on how we achieve this. We may go into detail on this subject in one of our future blog posts.\n\nOnce the system provisions the initial infrastructure, it then scales in response to the user workload. The next section describes how this is achieved.\n\nScalability\n\nOur users may operate with limited information at the time of provisioning their namespaces, resulting in best-effort provisioning estimates. Further, evolving use-cases may introduce new throughput requirements over time. Here’s how we manage this:\n\nHorizontal scaling : TimeSeries server instances can auto-scale up and down as per attached scaling policies to meet the traffic demand. The storage server capacity can be recomputed to accommodate changing requirements using our capacity planner.\n\n: TimeSeries server instances can auto-scale up and down as per attached scaling policies to meet the traffic demand. The storage server capacity can be recomputed to accommodate changing requirements using our capacity planner. Vertical scaling : We may also choose to vertically scale our TimeSeries server instances or our storage instances to get greater CPU, RAM and/or attached storage capacity.\n\n: We may also choose to vertically scale our TimeSeries server instances or our storage instances to get greater CPU, RAM and/or attached storage capacity. Scaling disk : We may attach EBS to store data if the capacity planner prefers infrastructure that offers larger storage at a lower cost rather than SSDs optimized for latency. In such cases, we deploy jobs to scale the EBS volume when the disk storage reaches a certain percentage threshold.\n\n: We may attach EBS to store data if the capacity planner prefers infrastructure that offers larger storage at a lower cost rather than SSDs optimized for latency. In such cases, we deploy jobs to scale the EBS volume when the disk storage reaches a certain percentage threshold. Re-partitioning data: Inaccurate workload estimates can lead to over or under-partitioning of our datasets. TimeSeries control-plane can adjust the partitioning configuration for upcoming time slices, once we realize the nature of data in the wild (via partition histograms). In the future we plan to support re-partitioning of older data and dynamic partitioning of current data.\n\nDesign Principles\n\nSo far, we have seen how TimeSeries stores, configures and interacts with event datasets. Let’s see how we apply different techniques to improve the performance of our operations and provide better guarantees.\n\nEvent Idempotency\n\nWe prefer to bake in idempotency in all mutation endpoints, so that users can retry or hedge their requests safely. Hedging is when the client sends an identical competing request to the server, if the original request does not come back with a response in an expected amount of time. The client then responds with whichever request completes first. This is done to keep the tail latencies for an application relatively low. This can only be done safely if the mutations are idempotent. For TimeSeries, the combination of event_time, event_id and event_item_key form the idempotency key for a given time_series_id event.\n\nSLO-based Hedging\n\nWe assign Service Level Objectives (SLO) targets for different endpoints within TimeSeries, as an indication of what we think the performance of those endpoints should be for a given namespace. We can then hedge a request if the response does not come back in that configured amount of time.\n\n\"slos\": {\n\n\"read\": { // SLOs per endpoint\n\n\"latency\": {\n\n\"target\": \"0.5s\", // hedge around this number\n\n\"max\": \"1s\" // time-out around this number\n\n}\n\n},\n\n\"write\": {\n\n\"latency\": {\n\n\"target\": \"0.01s\",\n\n\"max\": \"0.05s\"\n\n}\n\n}\n\n}\n\nPartial Return\n\nSometimes, a client may be sensitive to latency and willing to accept a partial result set. A real-world example of this is real-time frequency capping. Precision is not critical in this case, but if the response is delayed, it becomes practically useless to the upstream client. Therefore, the client prefers to work with whatever data has been collected so far rather than timing out while waiting for all the data. The TimeSeries client supports partial returns around SLOs for this purpose. Importantly, we still maintain the latest order of events in this partial fetch.\n\nAdaptive Pagination\n\nAll reads start with a default fanout factor, scanning 8 partition buckets in parallel. However, if the service layer determines that the time_series dataset is dense — i.e., most reads are satisfied by reading the first few partition buckets — then it dynamically adjusts the fanout factor of future reads in order to reduce the read amplification on the underlying datastore. Conversely, if the dataset is sparse, we may want to increase this limit with a reasonable upper bound.\n\nLimited Write Window\n\nIn most cases, the active range for writing data is smaller than the range for reading data — i.e., we want a range of time to become immutable as soon as possible so that we can apply optimizations on top of it. We control this by having a configurable “acceptLimit” parameter that prevents users from writing events older than this time limit. For example, an accept limit of 4 hours means that users cannot write events older than now() — 4 hours. We sometimes raise this limit for backfilling historical data, but it is tuned back down for regular write operations. Once a range of data becomes immutable, we can safely do things like caching, compressing, and compacting it for reads.\n\nBuffering Writes\n\nWe frequently leverage this service for handling bursty workloads. Rather than overwhelming the underlying datastore with this load all at once, we aim to distribute it more evenly by allowing events to coalesce over short durations (typically seconds). These events accumulate in in-memory queues running on each instance. Dedicated consumers then steadily drain these queues, grouping the events by their partition key, and batching the writes to the underlying datastore.\n\nThe queues are tailored to each datastore since their operational characteristics depend on the specific datastore being written to. For instance, the batch size for writing to Cassandra is significantly smaller than that for indexing into Elasticsearch, leading to different drain rates and batch sizes for the associated consumers.\n\nWhile using in-memory queues does increase JVM garbage collection, we have experienced substantial improvements by transitioning to JDK 21 with ZGC. To illustrate the impact, ZGC has reduced our tail latencies by an impressive 86%:\n\nBecause we use in-memory queues, we are prone to losing events in case of an instance crash. As such, these queues are only used for use cases that can tolerate some amount of data loss .e.g. tracing/logging. For use cases that need guaranteed durability and/or read-after-write consistency, these queues are effectively disabled and writes are flushed to the data store almost immediately.\n\nDynamic Compaction\n\nOnce a time slice exits the active write window, we can leverage the immutability of the data to optimize it for read performance. This process may involve re-compacting immutable data using optimal compaction strategies, dynamically shrinking and/or splitting shards to optimize system resources, and other similar techniques to ensure fast and reliable performance.\n\nThe following section provides a glimpse into the real-world performance of some of our TimeSeries datasets.\n\nReal-world Performance\n\nThe service can write data in the order of low single digit milliseconds\n\nwhile consistently maintaining stable point-read latencies:\n\nAt the time of writing this blog, the service was processing close to 15 million events/second across all the different datasets at peak globally.\n\nTime Series Usage @ Netflix\n\nThe TimeSeries Abstraction plays a vital role across key services at Netflix. Here are some impactful use cases:\n\nTracing and Insights: Logs traces across all apps and micro-services within Netflix, to understand service-to-service communication, aid in debugging of issues, and answer support requests.\n\nLogs traces across all apps and micro-services within Netflix, to understand service-to-service communication, aid in debugging of issues, and answer support requests. User Interaction Tracking : Tracks millions of user interactions — such as video playbacks, searches, and content engagement — providing insights that enhance Netflix’s recommendation algorithms in real-time and improve the overall user experience.\n\n: Tracks millions of user interactions — such as video playbacks, searches, and content engagement — providing insights that enhance Netflix’s recommendation algorithms in real-time and improve the overall user experience. Feature Rollout and Performance Analysis : Tracks the rollout and performance of new product features, enabling Netflix engineers to measure how users engage with features, which powers data-driven decisions about future improvements.\n\n: Tracks the rollout and performance of new product features, enabling Netflix engineers to measure how users engage with features, which powers data-driven decisions about future improvements. Asset Impression Tracking and Optimization : Tracks asset impressions ensuring content and assets are delivered efficiently while providing real-time feedback for optimizations.\n\n: Tracks asset impressions ensuring content and assets are delivered efficiently while providing real-time feedback for optimizations. Billing and Subscription Management: Stores historical data related to billing and subscription management, ensuring accuracy in transaction records and supporting customer service inquiries.\n\nand more…\n\nFuture Enhancements\n\nAs the use cases evolve, and the need to make the abstraction even more cost effective grows, we aim to make many improvements to the service in the upcoming months. Some of them are:\n\nTiered Storage for Cost Efficiency: Support moving older, lesser-accessed data into cheaper object storage that has higher time to first byte, potentially saving Netflix millions of dollars.\n\nSupport moving older, lesser-accessed data into cheaper object storage that has higher time to first byte, potentially saving Netflix millions of dollars. Dynamic Event Bucketing: Support real-time partitioning of keys into optimally-sized partitions as events stream in, rather than having a somewhat static configuration at the time of provisioning a namespace. This strategy has a huge advantage of not partitioning time_series_ids that don’t need it, thus saving the overall cost of read amplification. Also, with Cassandra 4.x, we have noted major improvements in reading a subset of data in a wide partition that could lead us to be less aggressive with partitioning the entire dataset ahead of time.\n\nSupport real-time partitioning of keys into optimally-sized partitions as events stream in, rather than having a somewhat static configuration at the time of provisioning a namespace. This strategy has a huge advantage of not partitioning time_series_ids that don’t need it, thus saving the overall cost of read amplification. Also, with Cassandra 4.x, we have noted major improvements in reading a subset of data in a wide partition that could lead us to be less aggressive with partitioning the entire dataset ahead of time. Caching: Take advantage of immutability of data and cache it intelligently for discrete time ranges.\n\nTake advantage of immutability of data and cache it intelligently for discrete time ranges. Count and other Aggregations: Some users are only interested in counting events in a given time interval rather than fetching all the event data for it.\n\nConclusion\n\nThe TimeSeries Abstraction is a vital component of Netflix’s online data infrastructure, playing a crucial role in supporting both real-time and long-term decision-making. Whether it’s monitoring system performance during high-traffic events or optimizing user engagement through behavior analytics, TimeSeries Abstraction ensures that Netflix operates seamlessly and efficiently on a global scale.\n\nAs Netflix continues to innovate and expand into new verticals, the TimeSeries Abstraction will remain a cornerstone of our platform, helping us push the boundaries of what’s possible in streaming and beyond.\n\nStay tuned for Part 2, where we’ll introduce our Distributed Counter Abstraction, a key element of Netflix’s Composite Abstractions, built on top of the TimeSeries Abstraction.\n\nAcknowledgments\n\nSpecial thanks to our stunning colleagues who contributed to TimeSeries Abstraction’s success: Tom DeVoe Mengqing Wang, Kartik Sathyanarayanan, Jordan West, Matt Lehman, Cheng Wang, Chris Lohfink .", "label": "non_personal"}
{"title": "Exploring the Magic Mirror: an interactive experience powered by the Gemini models", "url": "https://developers.googleblog.com/en/magic-mirror-interactive-experience-powered-by-gemini-models/", "content": "Imagine gazing into a mirror and seeing not just your reflection, but a gateway to information, creativity, and a touch of enchantment. This is precisely what the Gemini backed Magic Mirror project brings to life. Moving beyond a simple display, this project showcases the incredible interactive capabilities of the Gemini API and JavaScript GenAI SDK, transforming a familiar object into a new chat interface.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nThis project creates its interactive experience using several features of the Gemini API:\n\n1: Fluid, Real-Time Conversations with the Live API The foundation of the magic mirror's interactivity is the Live API. This allows for continuous, real-time voice interactions. You speak, and the mirror doesn't just listen for a single command, it engages in a flowing conversation by processing your speech as you talk, allowing for a more natural back-and-forth dialogue in either text or audio. On top of this, the Live API is able to understand when you’re speaking during playback and interpret that interruption to pivot the narrative and conversation based on your inputs, allowing for dynamic audible conversations alongside text.\n\nLink to Youtube Video (visible only when JS is disabled)\n\n2: The enchanted storyteller On top of being able to have a conversation through the Live API, the magic mirror can also be customized to weave tales, all thanks to the Gemini model's advanced generation capabilities by providing specific system instructions and updating speech configurations during initialization to include different dialects or accents, voices, and a variety of other attributes.\n\nLink to Youtube Video (visible only when JS is disabled)\n\n3: Instant information: grounding with Google Search While conversations and stories are great, sometimes you want to be able to know about the world around you as it’s happening. This magic mirror project leverages the model’s ability to integrate with Grounding with Google Search, providing grounded, up-to-date information.\n\nLink to Youtube Video (visible only when JS is disabled)\n\n4: Visual alchemy: image generation on command Using Function Calling with the Gemini API, the magic mirror is able to generate visuals based on your descriptions, adding depth to stories and deepening the experience of interacting with the Gemini model. The Gemini model determines that your request requires image generation and calls a predefined function based on stated characteristics, passing along the detailed prompt it derives from your spoken words.\n\nLink to Youtube Video (visible only when JS is disabled)", "label": "non_personal"}
{"title": "Investigation of a Workbench UI Latency Issue", "url": "https://netflixtechblog.com/investigation-of-a-workbench-ui-latency-issue-faa017b4653d?source=collection_home---4------12-----------------------", "content": "Investigation of a Workbench UI Latency Issue Netflix Technology Blog 12 min read · Oct 14, 2024 -- 4 Listen Share\n\nBy: Hechao Li and Marcelo Mayworm\n\nWith special thanks to our stunning colleagues Amer Ather, Itay Dafna, Luca Pozzi, Matheus Leão, and Ye Ji.\n\nOverview\n\nAt Netflix, the Analytics and Developer Experience organization, part of the Data Platform, offers a product called Workbench. Workbench is a remote development workspace based on Titus that allows data practitioners to work with big data and machine learning use cases at scale. A common use case for Workbench is running JupyterLab Notebooks.\n\nRecently, several users reported that their JupyterLab UI becomes slow and unresponsive when running certain notebooks. This document details the intriguing process of debugging this issue, all the way from the UI down to the Linux kernel.\n\nSymptom\n\nMachine Learning engineer Luca Pozzi reported to our Data Platform team that their JupyterLab UI on their workbench becomes slow and unresponsive when running some of their Notebooks. Restarting the ipykernel process, which runs the Notebook, might temporarily alleviate the problem, but the frustration persists as more notebooks are run.\n\nQuantify the Slowness\n\nWhile we observed the issue firsthand, the term “UI being slow” is subjective and difficult to measure. To investigate this issue, we needed a quantitative analysis of the slowness.\n\nItay Dafna devised an effective and simple method to quantify the UI slowness. Specifically, we opened a terminal via JupyterLab and held down a key (e.g., “j”) for 15 seconds while running the user’s notebook. The input to stdin is sent to the backend (i.e., JupyterLab) via a WebSocket, and the output to stdout is sent back from the backend and displayed on the UI. We then exported the .har file recording all communications from the browser and loaded it into a Notebook for analysis.\n\nUsing this approach, we observed latencies ranging from 1 to 10 seconds, averaging 7.4 seconds.\n\nBlame The Notebook\n\nNow that we have an objective metric for the slowness, let’s officially start our investigation. If you have read the symptom carefully, you must have noticed that the slowness only occurs when the user runs certain notebooks but not others.\n\nTherefore, the first step is scrutinizing the specific Notebook experiencing the issue. Why does the UI always slow down after running this particular Notebook? Naturally, you would think that there must be something wrong with the code running in it.\n\nUpon closely examining the user’s Notebook, we noticed a library called pystan , which provides Python bindings to a native C++ library called stan, looked suspicious. Specifically, pystan uses asyncio. However, because there is already an existing asyncio event loop running in the Notebook process and asyncio cannot be nested by design, in order for pystan to work, the authors of pystan recommend injecting pystan into the existing event loop by using a package called nest_asyncio, a library that became unmaintained because the author unfortunately passed away.\n\nGiven this seemingly hacky usage, we naturally suspected that the events injected by pystan into the event loop were blocking the handling of the WebSocket messages used to communicate with the JupyterLab UI. This reasoning sounds very plausible. However, the user claimed that there were cases when a Notebook not using pystan runs, the UI also became slow.\n\nMoreover, after several rounds of discussion with ChatGPT, we learned more about the architecture and realized that, in theory, the usage of pystan and nest_asyncio should not cause the slowness in handling the UI WebSocket for the following reasons:\n\nEven though pystan uses nest_asyncio to inject itself into the main event loop, the Notebook runs on a child process (i.e., the ipykernel process) of the jupyter-lab server process, which means the main event loop being injected by pystan is that of the ipykernel process, not the jupyter-server process. Therefore, even if pystan blocks the event loop, it shouldn’t impact the jupyter-lab main event loop that is used for UI websocket communication. See the diagram below:\n\nIn other words, pystan events are injected to the event loop B in this diagram instead of event loop A. So, it shouldn’t block the UI WebSocket events.\n\nYou might also think that because event loop A handles both the WebSocket events from the UI and the ZeroMQ socket events from the ipykernel process, a high volume of ZeroMQ events generated by the notebook could block the WebSocket. However, when we captured packets on the ZeroMQ socket while reproducing the issue, we didn’t observe heavy traffic on this socket that could cause such blocking.\n\nA stronger piece of evidence to rule out pystan was that we were ultimately able to reproduce the issue even without it, which I’ll dive into later.\n\nBlame Noisy Neighbors\n\nThe Workbench instance runs as a Titus container. To efficiently utilize our compute resources, Titus employs a CPU oversubscription feature, meaning the combined virtual CPUs allocated to containers exceed the number of available physical CPUs on a Titus agent. If a container is unfortunate enough to be scheduled alongside other “noisy” containers — those that consume a lot of CPU resources — it could suffer from CPU deficiency.\n\nHowever, after examining the CPU utilization of neighboring containers on the same Titus agent as the Workbench instance, as well as the overall CPU utilization of the Titus agent, we quickly ruled out this hypothesis. Using the top command on the Workbench, we observed that when running the Notebook, the Workbench instance uses only 4 out of the 64 CPUs allocated to it. Simply put, this workload is not CPU-bound.\n\nBlame The Network\n\nThe next theory was that the network between the web browser UI (on the laptop) and the JupyterLab server was slow. To investigate, we captured all the packets between the laptop and the server while running the Notebook and continuously pressing ‘j’ in the terminal.\n\nWhen the UI experienced delays, we observed a 5-second pause in packet transmission from server port 8888 to the laptop. Meanwhile, traffic from other ports, such as port 22 for SSH, remained unaffected. This led us to conclude that the pause was caused by the application running on port 8888 (i.e., the JupyterLab process) rather than the network.\n\nThe Minimal Reproduction\n\nAs previously mentioned, another strong piece of evidence proving the innocence of pystan was that we could reproduce the issue without it. By gradually stripping down the “bad” Notebook, we eventually arrived at a minimal snippet of code that reproduces the issue without any third-party dependencies or complex logic:\n\nimport time\n\nimport os\n\nfrom multiprocessing import Process\n\n\n\nN = os.cpu_count()\n\n\n\ndef launch_worker(worker_id):\n\ntime.sleep(60)\n\n\n\nif __name__ == '__main__':\n\nwith open('/root/2GB_file', 'r') as file:\n\ndata = file.read()\n\nprocesses = []\n\nfor i in range(N):\n\np = Process(target=launch_worker, args=(i,))\n\nprocesses.append(p)\n\np.start()\n\n\n\nfor p in processes:\n\np.join()\n\nThe code does only two things:\n\nRead a 2GB file into memory (the Workbench instance has 480G memory in total so this memory usage is almost negligible). Start N processes where N is the number of CPUs. The N processes do nothing but sleep.\n\nThere is no doubt that this is the most silly piece of code I’ve ever written. It is neither CPU bound nor memory bound. Yet it can cause the JupyterLab UI to stall for as many as 10 seconds!\n\nQuestions\n\nThere are a couple of interesting observations that raise several questions:\n\nWe noticed that both steps are required in order to reproduce the issue . If you don’t read the 2GB file (that is not even used!), the issue is not reproducible. Why using 2GB out of 480GB memory could impact the performance?\n\n. If you don’t read the 2GB file (that is not even used!), the issue is not reproducible. When the UI delay occurs, the jupyter-lab process CPU utilization spikes to 100% , hinting at contention on the single-threaded event loop in this process (event loop A in the diagram before). What does the jupyter-lab process need the CPU for, given that it is not the process that runs the Notebook?\n\n, hinting at contention on the single-threaded event loop in this process (event loop A in the diagram before). The code runs in a Notebook, which means it runs in the ipykernel process, that is a child process of the jupyter-lab process. How can anything that happens in a child process cause the parent process to have CPU contention?\n\nThe workbench has 64CPUs. But when we printed os.cpu_count(), the output was 96. That means the code starts more processes than the number of CPUs. Why is that?\n\nLet’s answer the last question first. In fact, if you run lscpu and nproc commands inside a Titus container, you will also see different results — the former gives you 96, which is the number of physical CPUs on the Titus agent, whereas the latter gives you 64, which is the number of virtual CPUs allocated to the container. This discrepancy is due to the lack of a “CPU namespace” in the Linux kernel, causing the number of physical CPUs to be leaked to the container when calling certain functions to get the CPU count. The assumption here is that Python os.cpu_count() uses the same function as the lscpu command, causing it to get the CPU count of the host instead of the container. Python 3.13 has a new call that can be used to get the accurate CPU count, but it’s not GA’ed yet.\n\nIt will be proven later that this inaccurate number of CPUs can be a contributing factor to the slowness.\n\nMore Clues\n\nNext, we used py-spy to do a profiling of the jupyter-lab process. Note that we profiled the parent jupyter-lab process, not the ipykernel child process that runs the reproduction code. The profiling result is as follows:\n\nAs one can see, a lot of CPU time (89%!!) is spent on a function called __parse_smaps_rollup. In comparison, the terminal handler used only 0.47% CPU time. From the stack trace, we see that this function is inside the event loop A, so it can definitely cause the UI WebSocket events to be delayed.\n\nThe stack trace also shows that this function is ultimately called by a function used by a Jupyter lab extension called jupyter_resource_usage. We then disabled this extension and restarted the jupyter-lab process. As you may have guessed, we could no longer reproduce the slowness!\n\nBut our puzzle is not solved yet. Why does this extension cause the UI to slow down? Let’s keep digging.\n\nRoot Cause Analysis\n\nFrom the name of the extension and the names of the other functions it calls, we can infer that this extension is used to get resources such as CPU and memory usage information. Examining the code, we see that this function call stack is triggered when an API endpoint /metrics/v1 is called from the UI. The UI apparently calls this function periodically, according to the network traffic tab in Chrome’s Developer Tools.\n\nNow let’s look at the implementation starting from the call get(jupter_resource_usage/api.py:42) . The full code is here and the key lines are shown below:\n\ncur_process = psutil.Process()\n\nall_processes = [cur_process] + cur_process.children(recursive=True)\n\n\n\nfor p in all_processes:\n\ninfo = p.memory_full_info()\n\nBasically, it gets all children processes of the jupyter-lab process recursively, including both the ipykernel Notebook process and all processes created by the Notebook. Obviously, the cost of this function is linear to the number of all children processes. In the reproduction code, we create 96 processes. So here we will have at least 96 (sleep processes) + 1 (ipykernel process) + 1 (jupyter-lab process) = 98 processes when it should actually be 64 (allocated CPUs) + 1 (ipykernel process) + 1 (jupyter-lab process) = 66 processes, because the number of CPUs allocated to the container is, in fact, 64.\n\nThis is truly ironic. The more CPUs we have, the slower we are!\n\nAt this point, we have answered one question: Why does starting many grandchildren processes in the child process cause the parent process to be slow? Because the parent process runs a function that’s linear to the number all children process recursively.\n\nHowever, this solves only half of the puzzle. If you remember the previous analysis, starting many child processes ALONE doesn’t reproduce the issue. If we don’t read the 2GB file, even if we create 2x more processes, we can’t reproduce the slowness.\n\nSo now we must answer the next question: Why does reading a 2GB file in the child process affect the parent process performance, especially when the workbench has as much as 480GB memory in total?\n\nTo answer this question, let’s look closely at the function __parse_smaps_rollup. As the name implies, this function parses the file /proc/<pid>/smaps_rollup.\n\ndef _parse_smaps_rollup(self):\n\nuss = pss = swap = 0\n\nwith open_binary(\"{}/{}/smaps_rollup\".format(self._procfs_path, self.pid)) as f:\n\nfor line in f:\n\nif line.startswith(b”Private_”):\n\n# Private_Clean, Private_Dirty, Private_Hugetlb\n\ns uss += int(line.split()[1]) * 1024\n\nelif line.startswith(b”Pss:”):\n\npss = int(line.split()[1]) * 1024\n\nelif line.startswith(b”Swap:”):\n\nswap = int(line.split()[1]) * 1024\n\nreturn (uss, pss, swap)\n\nNaturally, you might think that when memory usage increases, this file becomes larger in size, causing the function to take longer to parse. Unfortunately, this is not the answer because:\n\nFirst, the number of lines in this file is constant for all processes .\n\n. Second, this is a special file in the /proc filesystem, which should be seen as a kernel interface instead of a regular file on disk. In other words, I/O operations of this file are handled by the kernel rather than disk.\n\nThis file was introduced in this commit in 2017, with the purpose of improving the performance of user programs that determine aggregate memory statistics. Let’s first focus on the handler of open syscall on this /proc/<pid>/smaps_rollup.\n\nFollowing through the single_open function, we will find that it uses the function show_smaps_rollup for the show operation, which can translate to the read system call on the file. Next, we look at the show_smaps_rollup implementation. You will notice a do-while loop that is linear to the virtual memory area.\n\nstatic int show_smaps_rollup(struct seq_file *m, void *v) {\n\n…\n\nvma_start = vma->vm_start;\n\ndo {\n\nsmap_gather_stats(vma, &mss, 0);\n\nlast_vma_end = vma->vm_end;\n\n…\n\n} for_each_vma(vmi, vma);\n\n…\n\n}\n\nThis perfectly explains why the function gets slower when a 2GB file is read into memory. Because the handler of reading the smaps_rollup file now takes longer to run the while loop. Basically, even though smaps_rollup already improved the performance of getting memory information compared to the old method of parsing the /proc/<pid>/smaps file, it is still linear to the virtual memory used.\n\nMore Quantitative Analysis\n\nEven though at this point the puzzle is solved, let’s conduct a more quantitative analysis. How much is the time difference when reading the smaps_rollup file with small versus large virtual memory utilization? Let’s write some simple benchmark code like below:\n\nimport os\n\n\n\ndef read_smaps_rollup(pid):\n\nwith open(\"/proc/{}/smaps_rollup\".format(pid), \"rb\") as f:\n\nfor line in f:\n\npass\n\n\n\nif __name__ == “__main__”:\n\npid = os.getpid()\n\n\n\nread_smaps_rollup(pid)\n\n\n\nwith open(“/root/2G_file”, “rb”) as f:\n\ndata = f.read()\n\n\n\nread_smaps_rollup(pid)\n\nThis program performs the following steps:\n\nReads the smaps_rollup file of the current process. Reads a 2GB file into memory. Repeats step 1.\n\nWe then use strace to find the accurate time of reading the smaps_rollup file.\n\n$ sudo strace -T -e trace=openat,read python3 benchmark.py 2>&1 | grep “smaps_rollup” -A 1\n\n\n\nopenat(AT_FDCWD, “/proc/3107492/smaps_rollup”, O_RDONLY|O_CLOEXEC) = 3 <0.000023>\n\nread(3, “560b42ed4000–7ffdadcef000 — -p 0”…, 1024) = 670 <0.000259>\n\n...\n\nopenat(AT_FDCWD, “/proc/3107492/smaps_rollup”, O_RDONLY|O_CLOEXEC) = 3 <0.000029>\n\nread(3, “560b42ed4000–7ffdadcef000 — -p 0”…, 1024) = 670 <0.027698>\n\nAs you can see, both times, the read syscall returned 670, meaning the file size remained the same at 670 bytes. However, the time it took the second time (i.e., 0.027698 seconds) is 100x the time it took the first time (i.e., 0.000259 seconds)! This means that if there are 98 processes, the time spent on reading this file alone will be 98 * 0.027698 = 2.7 seconds! Such a delay can significantly affect the UI experience.\n\nSolution\n\nThis extension is used to display the CPU and memory usage of the notebook process on the bar at the bottom of the Notebook:\n\nWe confirmed with the user that disabling the jupyter-resource-usage extension meets their requirements for UI responsiveness, and that this extension is not critical to their use case. Therefore, we provided a way for them to disable the extension.\n\nSummary\n\nThis was such a challenging issue that required debugging from the UI all the way down to the Linux kernel. It is fascinating that the problem is linear to both the number of CPUs and the virtual memory size — two dimensions that are generally viewed separately.\n\nOverall, we hope you enjoyed the irony of:\n\nThe extension used to monitor CPU usage causing CPU contention. An interesting case where the more CPUs you have, the slower you get!\n\nIf you’re excited by tackling such technical challenges and have the opportunity to solve complex technical challenges and drive innovation, consider joining our Data Platform teams. Be part of shaping the future of Data Security and Infrastructure, Data Developer Experience, Analytics Infrastructure and Enablement, and more. Explore the impact you can make with us!", "label": "non_personal"}
{"title": "Gemini 2.5: Updates to our family of thinking models", "url": "https://developers.googleblog.com/en/gemini-2-5-thinking-model-updates/", "content": "Today we are excited to share updates across the board to our Gemini 2.5 model family: Gemini 2.5 Pro is generally available and stable (no changes from the 06-05 preview) Gemini 2.5 Flash is generally available and stable (no changes from the 05-20 preview, see pricing updates below) Gemini 2.5 Flash-Lite is now available in preview Gemini 2.5 models are thinking models, capable of reasoning through their thoughts before responding, resulting in enhanced performance and improved accuracy. Each model has control over the thinking budget, giving developers the ability to choose when and how much the model “thinks” before generating a response.\n\nOverview of our family of Gemini 2.5 thinking models\n\nIntroducing Gemini 2.5 Flash-Lite Today, we’re introducing 2.5 Flash-Lite in preview with the lowest latency and cost in the 2.5 model family. It’s designed as a cost-effective upgrade from our previous 1.5 and 2.0 Flash models. It also offers better performance across most evals, and lower time to first token while also achieving higher tokens per second decode. This model is great for high throughput tasks like classification or summarization at scale. Gemini 2.5 Flash-Lite is a reasoning model, which allows for dynamic control of the thinking budget with an API parameter. Because Flash-Lite is optimized for cost and speed, “thinking” is off by default, unlike our other models. 2.5 Flash-Lite also supports all of our native tools like Grounding with Google Search, Code Execution, and URL Context in addition to function calling.\n\nBenchmarks for Gemini 2.5 Flash-Lite\n\nOver the last year, our research teams have continued to push the pareto frontier with our Flash model series. When 2.5 Flash was initially announced, we had not yet finalized the capabilities for 2.5 Flash-Lite. We also launched with a “thinking” and “non-thinking price”, which led to developer confusion.\n\n\n\nWith the stable version of Gemini 2.5 Flash rolling out (which is the same 05-20 model preview we made available at Google I/O), and the incredible performance of 2.5 Flash, we are updating the pricing for 2.5 Flash: $0.30 / 1M input tokens (*up from $0.15 input) $2.50 / 1M output tokens (*down from $3.50 output) We removed the thinking vs. non-thinking price difference We kept a single price tier regardless of input token size While we strive to maintain consistent pricing between preview and stable releases to minimize disruption, this is a specific adjustment reflecting Flash’s exceptional value, still offering the best cost-per-intelligence available. And with Gemini 2.5 Flash-Lite, we now have an even lower cost option (with or without thinking) for cost and latency sensitive use cases that require less model intelligence.\n\nPricing updates for our Gemini Flash family\n\nIf you are using the Gemini 2.5 Flash Preview 04-17 , the existing preview pricing will remain in effect until its planned deprecation on July 15, 2025, at which point that model endpoint will be turned off. You can transition to the generally available model “gemini-2.5-flash”, or switch to 2.5 Flash-Lite Preview as a lower cost option.\n\nContinued growth of Gemini 2.5 Pro The growth and demand for Gemini 2.5 Pro continues to be the steepest of any of our models we have ever seen. To allow more customers to build on this model in production, we are making the 06-05 version of the model stable, with the same pareto frontier price point as before. We expect that cases where you need the highest intelligence and most capabilities are where you will see Pro shine, like coding and agentic tasks. Gemini 2.5 Pro is at the heart of many of the most loved developer tools.\n\nTop developer tools using Gemini 2.5 Pro", "label": "non_personal"}
{"title": "Multilingual innovation in LLMs: How open models help unlock global communication", "url": "https://developers.googleblog.com/en/unlock-global-communication-gemma-projects/", "content": "We are thrilled to celebrate the incredible contributions of the community to the Unlock Global Communication with Gemma competition on Kaggle! Developers tackled the critical challenge in AI of adapting state-of-the-art large language models (LLMs) for diverse cultural and linguistic contexts.\n\nModels often exhibit a bias towards high-resource languages due to the predominant language of their training and evaluation datasets. This can lead to a performance gap, where the latest AI advancements may not be realized in lower-resourced languages. Additionally, these models may not only lack understanding of the language, but also culturally-relevant context that would make these models helpful for the communities.\n\nWe were incredibly impressed by the community's creative solutions for translation of languages, lyrics, old texts, and more.\n\n\n\nHonoring the innovators\n\nThrough hundreds of submissions, developers demonstrated how to bring the transformative power of LLMs to languages everywhere. Projects leveraged custom datasets and efficient post-training methods to adapt Gemma for instruction following, translation, and specific domains. We encourage you to explore the notebooks on Kaggle to see these techniques in action and apply them to your own multilingual projects.\n\nThe first place project adapted Gemma for Swahili understanding, opening up new possibilities to reach 200+ million language speakers. Gemma models were fine-tuned using parameter-efficient fine-tuning techniques for the 2B, 9B, and 27B parameter sizes.\n\nA key aspect of their tuning was Gemma’s “remarkable flexibility in instruction-response formatting,” which allowed the models to parse instructions with minimal structural constraints and generate coherent responses across different input formats.\n\nKnowledge Yielding Adaptive Retrieval Augmentation (Kyara) explored retrieval processes for LLM fine-tuning, demonstrating how to enhance Gemma’s ability to generate informed responses in Traditional Chinese.\n\nThe project focused on building high-quality question & answer (Q&A) datasets using a graph-based approach to knowledge retrieval, inspired on how humans learn by connecting concepts.\n\nThe project fine-tuned Gemma for Arabic language tasks, including translation, summarization, storytelling, and dialogue generation.\n\nAs a language with a rich historical past, the project also aimed to enhance comprehension of older forms of Arabic used in literary texts and art, employing multiple techniques to bridge tasks between Modern Standard Arabic and Classical Arabic.\n\nThis project focused on improving Italian language understanding for Gemma using a cost-effective post-training approach that addresses pitfalls such as hallucinations and catastrophic forgetting.\n\nThe 2B and 9B model sizes were fine-tuned on a mix of data, including a new instruction tuning dataset created using LLM-as-a-judge to ensure the quality of translations.\n\nThis project developed an “Ancient Chinese Expert” using Gemma to understand and generate translations for ancient Chinese texts, highlighting the potential of LLMs for historical cultural preservation.\n\nThe model was fine-tuned on a comprehensive dataset to improve linguistic understanding, and post-training included techniques to improve instruction following.\n\nThis project tackled nuanced challenges specific to AI-driven lyric translation, enhancing Gemma’s sensitivity to cultural references and symbolic language, while also ensuring rhythmic fidelity to the original song.\n\nA multilingual dataset contained lyric translations annotated to capture crucial cultural context, emotional tone, and rhythmic features, enabling the model to grasp and replicate the artistic depth of lyrical content.\n\nThis project adapted Gemma 2 JPN to generate Yomigana/Furigana, a reading aid for Japanese text and assist language learners or readers encountering complex Kanji.\n\nWhile other rule-based tools currently exist, LLMs can recognize rare Kanji better and “interpret the context of a sentence, enabling accurate disambiguation of polyphonic Kanji”. The notebook also noted that conversational capabilities had degraded due to training on the singular translation task.\n\nThis project enhances Gemma’s mathematical and logical understanding in Hindi numeric words, which presents a challenge for models to interpret given complex word formations, for example “दो सौ” for “200” or “ढाई” for “2.5”.\n\nThe 9B model was fine-tuned on a curated and human expert-verified dataset featuring a wide array of question types, unlocking uses for AI-driven educational tools, automated tutoring, and localized content\n\nThis project fine-tuned the Gemma 2 9B model for translation tasks in Kazakh. A language written in three distinct scripts (Cyrillic, Latin, and Arabic), the Cyrillic version requires approximately twice as many tokens as English, presenting a challenge for training with limited resources.\n\nModel performance showed better benchmarks than the 27B Gemma variant and Google Translate, demonstrating how to adapt LLMs for underrepresented languages using a cost-effective approach.\n\nThis project enables Gemma to understand and translate Old English, the earliest recorded form of the English language. A custom dataset with Old English-Modern English language pairs was created to help tackle the challenge of working with historical languages and limited publicly available data.\n\nThe notebook also features a bonus audio generation component, based on an open-source Icelandic text-to-speech model, offering an approximation of how speech might have sounded.\n\n\n\n10 more awesome projects\n\nGemma 2 Reasoning for Japanese Math: This project created reasoning variants to perform chain-of-thought processes and handle complex problems.\n\nMultitask Gemma2 Agents - Summarise & Translate: This project focused on developing agents capable of multiple tasks.\n\nKorean AI Doctor Gemma2: This project adapted Gemma for medical applications in Korean.\n\nGemma Fine-Tuning for Ru-En Medical Translations: This project enhanced Gemma translation accuracy in ophthalmology.\n\nGemma PT: This project fine-tuned the ShieldGemma content classifier to detect prejudice and disinformation in Portuguese.\n\nHow to Fine-tune Gemma 2 for Advanced Reasoning: This project enhanced Gemma reasoning capabilities by implementing the Coconut (Chain of Continuous Thought) paradigm.\n\nFinetune Gemma Turkish Chat: This project fine-tuned on Gemma on a Q&A dataset to improve accuracy and conversational ability.\n\nFinetuning Gemma2 Customized Dataset: This project fine-tuned Gemma for English-Arabic translation and medical understanding.\n\nGemma-2 Finetuning on Telugu News Dataset: This project adapted Gemma to generate Telugu headlines from news articles.\n\nFinetuned Gemma2 9B Math Reasoning Model Russian: This project enhanced Gemma performance for math problems in Russian.\n\n\n\nLooking ahead with Gemma 3\n\nWith over 7,000 languages spoken worldwide, the potential for AI to bridge communication gaps is immense. The Gemma open model family provides a powerful foundation for developers to adapt high-performing models to low-resource languages.\n\nThe innovation and dedication demonstrated by the Kaggle community in adapting Gemma 2 for various languages are truly inspiring. As we continue to build a future where AI empowers global communication for everyone, we're excited for Gemma 3, which brings pretrained support for over 140 languages, making it a great foundation to build on.\n\nWe encourage developers to explore the possibilities of Gemma, to share their datasets and models with others, and continue to advance multilingual AI together.", "label": "non_personal"}
{"title": "Netflix’s Distributed Counter Abstraction", "url": "https://netflixtechblog.com/netflixs-distributed-counter-abstraction-8d0c45eb66b2?source=collection_home---4------11-----------------------", "content": "Netflix’s Distributed Counter Abstraction Netflix Technology Blog 19 min read · Nov 12, 2024 -- 22 Listen Share\n\nBy: Rajiv Shringi, Oleksii Tkachuk, Kartik Sathyanarayanan\n\nIntroduction\n\nIn our previous blog post, we introduced Netflix’s TimeSeries Abstraction, a distributed service designed to store and query large volumes of temporal event data with low millisecond latencies. Today, we’re excited to present the Distributed Counter Abstraction. This counting service, built on top of the TimeSeries Abstraction, enables distributed counting at scale while maintaining similar low latency performance. As with all our abstractions, we use our Data Gateway Control Plane to shard, configure, and deploy this service globally.\n\nDistributed counting is a challenging problem in computer science. In this blog post, we’ll explore the diverse counting requirements at Netflix, the challenges of achieving accurate counts in near real-time, and the rationale behind our chosen approach, including the necessary trade-offs.\n\nNote: When it comes to distributed counters, terms such as ‘accurate’ or ‘precise’ should be taken with a grain of salt. In this context, they refer to a count very close to accurate, presented with minimal delays.\n\nUse Cases and Requirements\n\nAt Netflix, our counting use cases include tracking millions of user interactions, monitoring how often specific features or experiences are shown to users, and counting multiple facets of data during A/B test experiments, among others.\n\nAt Netflix, these use cases can be classified into two broad categories:\n\nBest-Effort: For this category, the count doesn’t have to be very accurate or durable. However, this category requires near-immediate access to the current count at low latencies, all while keeping infrastructure costs to a minimum. Eventually Consistent: This category needs accurate and durable counts, and is willing to tolerate a slight delay in accuracy and a slightly higher infrastructure cost as a trade-off.\n\nBoth categories share common requirements, such as high throughput and high availability. The table below provides a detailed overview of the diverse requirements across these two categories.\n\nDistributed Counter Abstraction\n\nTo meet the outlined requirements, the Counter Abstraction was designed to be highly configurable. It allows users to choose between different counting modes, such as Best-Effort or Eventually Consistent, while considering the documented trade-offs of each option. After selecting a mode, users can interact with APIs without needing to worry about the underlying storage mechanisms and counting methods.\n\nLet’s take a closer look at the structure and functionality of the API.\n\nAPI\n\nCounters are organized into separate namespaces that users set up for each of their specific use cases. Each namespace can be configured with different parameters, such as Type of Counter, Time-To-Live (TTL), and Counter Cardinality, using the service’s Control Plane.\n\nThe Counter Abstraction API resembles Java’s AtomicInteger interface:\n\nAddCount/AddAndGetCount: Adjusts the count for the specified counter by the given delta value within a dataset. The delta value can be positive or negative. The AddAndGetCount counterpart also returns the count after performing the add operation.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"counter_name\": \"counter123\",\n\n\"delta\": 2,\n\n\"idempotency_token\": {\n\n\"token\": \"some_event_id\",\n\n\"generation_time\": \"2024-10-05T14:48:00Z\"\n\n}\n\n}\n\nThe idempotency token can be used for counter types that support them. Clients can use this token to safely retry or hedge their requests. Failures in a distributed system are a given, and having the ability to safely retry requests enhances the reliability of the service.\n\nGetCount: Retrieves the count value of the specified counter within a dataset.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"counter_name\": \"counter123\"\n\n}\n\nClearCount: Effectively resets the count to 0 for the specified counter within a dataset.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"counter_name\": \"counter456\",\n\n\"idempotency_token\": {...}\n\n}\n\nNow, let’s look at the different types of counters supported within the Abstraction.\n\nTypes of Counters\n\nThe service primarily supports two types of counters: Best-Effort and Eventually Consistent, along with a third experimental type: Accurate. In the following sections, we’ll describe the different approaches for these types of counters and the trade-offs associated with each.\n\nBest Effort Regional Counter\n\nThis type of counter is powered by EVCache, Netflix’s distributed caching solution built on the widely popular Memcached. It is suitable for use cases like A/B experiments, where many concurrent experiments are run for relatively short durations and an approximate count is sufficient. Setting aside the complexities of provisioning, resource allocation, and control plane management, the core of this solution is remarkably straightforward:\n\n// counter cache key\n\ncounterCacheKey = <namespace>:<counter_name>\n\n\n\n// add operation\n\nreturn delta > 0\n\n? cache.incr(counterCacheKey, delta, TTL)\n\n: cache.decr(counterCacheKey, Math.abs(delta), TTL);\n\n\n\n// get operation\n\ncache.get(counterCacheKey);\n\n\n\n// clear counts from all replicas\n\ncache.delete(counterCacheKey, ReplicaPolicy.ALL);\n\nEVCache delivers extremely high throughput at low millisecond latency or better within a single region, enabling a multi-tenant setup within a shared cluster, saving infrastructure costs. However, there are some trade-offs: it lacks cross-region replication for the increment operation and does not provide consistency guarantees, which may be necessary for an accurate count. Additionally, idempotency is not natively supported, making it unsafe to retry or hedge requests.\n\nEdit: A note on probabilistic data structures:\n\nProbabilistic data structures like HyperLogLog (HLL) can be useful for tracking an approximate number of distinct elements, like distinct views or visits to a website, but are not ideally suited for implementing distinct increments and decrements for a given key. Count-Min Sketch (CMS) is an alternative that can be used to adjust the values of keys by a given amount. Data stores like Redis support both HLL and CMS. However, we chose not to pursue this direction for several reasons:\n\nWe chose to build on top of data stores that we already operate at scale.\n\nProbabilistic data structures do not natively support several of our requirements, such as resetting the count for a given key or having TTLs for counts. Additional data structures, including more sketches, would be needed to support these requirements.\n\nOn the other hand, the EVCache solution is quite simple, requiring minimal lines of code and using natively supported elements. However, it comes at the trade-off of using a small amount of memory per counter key.\n\nEventually Consistent Global Counter\n\nWhile some users may accept the limitations of a Best-Effort counter, others opt for precise counts, durability and global availability. In the following sections, we’ll explore various strategies for achieving durable and accurate counts. Our objective is to highlight the challenges inherent in global distributed counting and explain the reasoning behind our chosen approach.\n\nApproach 1: Storing a Single Row per Counter\n\nLet’s start simple by using a single row per counter key within a table in a globally replicated datastore.\n\nLet’s examine some of the drawbacks of this approach:\n\nLack of Idempotency : There is no idempotency key baked into the storage data-model preventing users from safely retrying requests. Implementing idempotency would likely require using an external system for such keys, which can further degrade performance or cause race conditions.\n\n: There is no idempotency key baked into the storage data-model preventing users from safely retrying requests. Implementing idempotency would likely require using an external system for such keys, which can further degrade performance or cause race conditions. Heavy Contention: To update counts reliably, every writer must perform a Compare-And-Swap operation for a given counter using locks or transactions. Depending on the throughput and concurrency of operations, this can lead to significant contention, heavily impacting performance.\n\nSecondary Keys: One way to reduce contention in this approach would be to use a secondary key, such as a bucket_id, which allows for distributing writes by splitting a given counter into buckets, while enabling reads to aggregate across buckets. The challenge lies in determining the appropriate number of buckets. A static number may still lead to contention with hot keys, while dynamically assigning the number of buckets per counter across millions of counters presents a more complex problem.\n\nLet’s see if we can iterate on our solution to overcome these drawbacks.\n\nApproach 2: Per Instance Aggregation\n\nTo address issues of hot keys and contention from writing to the same row in real-time, we could implement a strategy where each instance aggregates the counts in memory and then flushes them to disk at regular intervals. Introducing sufficient jitter to the flush process can further reduce contention.\n\nHowever, this solution presents a new set of issues:\n\nVulnerability to Data Loss : The solution is vulnerable to data loss for all in-memory data during instance failures, restarts, or deployments.\n\n: The solution is vulnerable to data loss for all in-memory data during instance failures, restarts, or deployments. Inability to Reliably Reset Counts : Due to counting requests being distributed across multiple machines, it is challenging to establish consensus on the exact point in time when a counter reset occurred.\n\n: Due to counting requests being distributed across multiple machines, it is challenging to establish consensus on the exact point in time when a counter reset occurred. Lack of Idempotency: Similar to the previous approach, this method does not natively guarantee idempotency. One way to achieve idempotency is by consistently routing the same set of counters to the same instance. However, this approach may introduce additional complexities, such as leader election, and potential challenges with availability and latency in the write path.\n\nThat said, this approach may still be suitable in scenarios where these trade-offs are acceptable. However, let’s see if we can address some of these issues with a different event-based approach.\n\nApproach 3: Using Durable Queues\n\nIn this approach, we log counter events into a durable queuing system like Apache Kafka to prevent any potential data loss. By creating multiple topic partitions and hashing the counter key to a specific partition, we ensure that the same set of counters are processed by the same set of consumers. This setup simplifies facilitating idempotency checks and resetting counts. Furthermore, by leveraging additional stream processing frameworks such as Kafka Streams or Apache Flink, we can implement windowed aggregations.\n\nHowever, this approach comes with some challenges:\n\nPotential Delays : Having the same consumer process all the counts from a given partition can lead to backups and delays, resulting in stale counts.\n\n: Having the same consumer process all the counts from a given partition can lead to backups and delays, resulting in stale counts. Rebalancing Partitions: This approach requires auto-scaling and rebalancing of topic partitions as the cardinality of counters and throughput increases.\n\nFurthermore, all approaches that pre-aggregate counts make it challenging to support two of our requirements for accurate counters:\n\nAuditing of Counts : Auditing involves extracting data to an offline system for analysis to ensure that increments were applied correctly to reach the final value. This process can also be used to track the provenance of increments. However, auditing becomes infeasible when counts are aggregated without storing the individual increments.\n\n: Auditing involves extracting data to an offline system for analysis to ensure that increments were applied correctly to reach the final value. This process can also be used to track the provenance of increments. However, auditing becomes infeasible when counts are aggregated without storing the individual increments. Potential Recounting: Similar to auditing, if adjustments to increments are necessary and recounting of events within a time window is required, pre-aggregating counts makes this infeasible.\n\nBarring those few requirements, this approach can still be effective if we determine the right way to scale our queue partitions and consumers while maintaining idempotency. However, let’s explore how we can adjust this approach to meet the auditing and recounting requirements.\n\nApproach 4: Event Log of Individual Increments\n\nIn this approach, we log each individual counter increment along with its event_time and event_id. The event_id can include the source information of where the increment originated. The combination of event_time and event_id can also serve as the idempotency key for the write.\n\nHowever, in its simplest form, this approach has several drawbacks:\n\nRead Latency : Each read request requires scanning all increments for a given counter potentially degrading performance.\n\n: Each read request requires scanning all increments for a given counter potentially degrading performance. Duplicate Work : Multiple threads might duplicate the effort of aggregating the same set of counters during read operations, leading to wasted effort and subpar resource utilization.\n\n: Multiple threads might duplicate the effort of aggregating the same set of counters during read operations, leading to wasted effort and subpar resource utilization. Wide Partitions : If using a datastore like Apache Cassandra, storing many increments for the same counter could lead to a wide partition, affecting read performance.\n\n: If using a datastore like Apache Cassandra, storing many increments for the same counter could lead to a wide partition, affecting read performance. Large Data Footprint: Storing each increment individually could also result in a substantial data footprint over time. Without an efficient data retention strategy, this approach may struggle to scale effectively.\n\nThe combined impact of these issues can lead to increased infrastructure costs that may be difficult to justify. However, adopting an event-driven approach seems to be a significant step forward in addressing some of the challenges we’ve encountered and meeting our requirements.\n\nHow can we improve this solution further?\n\nNetflix’s Approach\n\nWe use a combination of the previous approaches, where we log each counting activity as an event, and continuously aggregate these events in the background using queues and a sliding time window. Additionally, we employ a bucketing strategy to prevent wide partitions. In the following sections, we’ll explore how this approach addresses the previously mentioned drawbacks and meets all our requirements.\n\nNote: From here on, we will use the words “rollup” and “aggregate” interchangeably. They essentially mean the same thing, i.e., collecting individual counter increments/decrements and arriving at the final value.\n\nTimeSeries Event Store:\n\nWe chose the TimeSeries Data Abstraction as our event store, where counter mutations are ingested as event records. Some of the benefits of storing events in TimeSeries include:\n\nHigh-Performance: The TimeSeries abstraction already addresses many of our requirements, including high availability and throughput, reliable and fast performance, and more.\n\nReducing Code Complexity: We reduce a lot of code complexity in Counter Abstraction by delegating a major portion of the functionality to an existing service.\n\nTimeSeries Abstraction uses Cassandra as the underlying event store, but it can be configured to work with any persistent store. Here is what it looks like:\n\nHandling Wide Partitions: The time_bucket and event_bucket columns play a crucial role in breaking up a wide partition, preventing high-throughput counter events from overwhelming a given partition. For more information regarding this, refer to our previous blog.\n\nNo Over-Counting: The event_time, event_id and event_item_key columns form the idempotency key for the events for a given counter, enabling clients to retry safely without the risk of over-counting.\n\nEvent Ordering: TimeSeries orders all events in descending order of time allowing us to leverage this property for events like count resets.\n\nEvent Retention: The TimeSeries Abstraction includes retention policies to ensure that events are not stored indefinitely, saving disk space and reducing infrastructure costs. Once events have been aggregated and moved to a more cost-effective store for audits, there’s no need to retain them in the primary storage.\n\nNow, let’s see how these events are aggregated for a given counter.\n\nAggregating Count Events:\n\nAs mentioned earlier, collecting all individual increments for every read request would be cost-prohibitive in terms of read performance. Therefore, a background aggregation process is necessary to continually converge counts and ensure optimal read performance.\n\nBut how can we safely aggregate count events amidst ongoing write operations?\n\nThis is where the concept of Eventually Consistent counts becomes crucial. By intentionally lagging behind the current time by a safe margin, we ensure that aggregation always occurs within an immutable window.\n\nLets see what that looks like:\n\nLet’s break this down:\n\nlastRollupTs : This represents the most recent time when the counter value was last aggregated. For a counter being operated for the first time, this timestamp defaults to a reasonable time in the past.\n\n: This represents the most recent time when the counter value was last aggregated. For a counter being operated for the first time, this timestamp defaults to a reasonable time in the past. Immutable Window and Lag: Aggregation can only occur safely within an immutable window that is no longer receiving counter events. The “acceptLimit” parameter of the TimeSeries Abstraction plays a crucial role here, as it rejects incoming events with timestamps beyond this limit. During aggregations, this window is pushed slightly further back to account for clock skews.\n\nThis does mean that the counter value will lag behind its most recent update by some margin (typically in the order of seconds). This approach does leave the door open for missed events due to cross-region replication issues. See “Future Work” section at the end.\n\nAggregation Process: The rollup process aggregates all events in the aggregation window since the last rollup to arrive at the new value.\n\nRollup Store:\n\nWe save the results of this aggregation in a persistent store. The next aggregation will simply continue from this checkpoint.\n\nWe create one such Rollup table per dataset and use Cassandra as our persistent store. However, as you will soon see in the Control Plane section, the Counter service can be configured to work with any persistent store.\n\nLastWriteTs: Every time a given counter receives a write, we also log a last-write-timestamp as a columnar update in this table. This is done using Cassandra’s USING TIMESTAMP feature to predictably apply the Last-Write-Win (LWW) semantics. This timestamp is the same as the event_time for the event. In the subsequent sections, we’ll see how this timestamp is used to keep some counters in active rollup circulation until they have caught up to their latest value.\n\nRollup Cache\n\nTo optimize read performance, these values are cached in EVCache for each counter. We combine the lastRollupCount and lastRollupTs into a single cached value per counter to prevent potential mismatches between the count and its corresponding checkpoint timestamp.\n\nBut, how do we know which counters to trigger rollups for? Let’s explore our Write and Read path to understand this better.\n\nAdd/Clear Count:\n\nAn add or clear count request writes durably to the TimeSeries Abstraction and updates the last-write-timestamp in the Rollup store. If the durability acknowledgement fails, clients can retry their requests with the same idempotency token without the risk of overcounting. Upon durability, we send a fire-and-forget request to trigger the rollup for the request counter.\n\nGetCount:\n\nWe return the last rolled-up count as a quick point-read operation, accepting the trade-off of potentially delivering a slightly stale count. We also trigger a rollup during the read operation to advance the last-rollup-timestamp, enhancing the performance of subsequent aggregations. This process also self-remediates a stale count if any previous rollups had failed.\n\nWith this approach, the counts continually converge to their latest value. Now, let’s see how we scale this approach to millions of counters and thousands of concurrent operations using our Rollup Pipeline.\n\nRollup Pipeline:\n\nEach Counter-Rollup server operates a rollup pipeline to efficiently aggregate counts across millions of counters. This is where most of the complexity in Counter Abstraction comes in. In the following sections, we will share key details on how efficient aggregations are achieved.\n\nLight-Weight Roll-Up Event: As seen in our Write and Read paths above, every operation on a counter sends a light-weight event to the Rollup server:\n\nrollupEvent: {\n\n\"namespace\": \"my_dataset\",\n\n\"counter\": \"counter123\"\n\n}\n\nNote that this event does not include the increment. This is only an indication to the Rollup server that this counter has been accessed and now needs to be aggregated. Knowing exactly which specific counters need to be aggregated prevents scanning the entire event dataset for the purpose of aggregations.\n\nIn-Memory Rollup Queues: A given Rollup server instance runs a set of in-memory queues to receive rollup events and parallelize aggregations. In the first version of this service, we settled on using in-memory queues to reduce provisioning complexity, save on infrastructure costs, and make rebalancing the number of queues fairly straightforward. However, this comes with the trade-off of potentially missing rollup events in case of an instance crash. For more details, see the “Stale Counts” section in “Future Work.”\n\nMinimize Duplicate Effort: We use a fast non-cryptographic hash like XXHash to ensure that the same set of counters end up on the same queue. Further, we try to minimize the amount of duplicate aggregation work by having a separate rollup stack that chooses to run fewer beefier instances.\n\nAvailability and Race Conditions: Having a single Rollup server instance can minimize duplicate aggregation work but may create availability challenges for triggering rollups. If we choose to horizontally scale the Rollup servers, we allow threads to overwrite rollup values while avoiding any form of distributed locking mechanisms to maintain high availability and performance. This approach remains safe because aggregation occurs within an immutable window. Although the concept of now() may differ between threads, causing rollup values to sometimes fluctuate, the counts will eventually converge to an accurate value within each immutable aggregation window.\n\nRebalancing Queues: If we need to scale the number of queues, a simple Control Plane configuration update followed by a re-deploy is enough to rebalance the number of queues.\n\n\"eventual_counter_config\": {\n\n\"queue_config\": {\n\n\"num_queues\" : 8, // change to 16 and re-deploy\n\n...\n\nHandling Deployments: During deployments, these queues shut down gracefully, draining all existing events first, while the new Rollup server instance starts up with potentially new queue configurations. There may be a brief period when both the old and new Rollup servers are active, but as mentioned before, this race condition is managed since aggregations occur within immutable windows.\n\nMinimize Rollup Effort: Receiving multiple events for the same counter doesn’t mean rolling it up multiple times. We drain these rollup events into a Set, ensuring a given counter is rolled up only once during a rollup window.\n\nEfficient Aggregation: Each rollup consumer processes a batch of counters simultaneously. Within each batch, it queries the underlying TimeSeries abstraction in parallel to aggregate events within specified time boundaries. The TimeSeries abstraction optimizes these range scans to achieve low millisecond latencies.\n\nDynamic Batching: The Rollup server dynamically adjusts the number of time partitions that need to be scanned based on cardinality of counters in order to prevent overwhelming the underlying store with many parallel read requests.\n\nAdaptive Back-Pressure: Each consumer waits for one batch to complete before issuing the rollups for the next batch. It adjusts the wait time between batches based on the performance of the previous batch. This approach provides back-pressure during rollups to prevent overwhelming the underlying TimeSeries store.\n\nHandling Convergence:\n\nIn order to prevent low-cardinality counters from lagging behind too much and subsequently scanning too many time partitions, they are kept in constant rollup circulation. For high-cardinality counters, continuously circulating them would consume excessive memory in our Rollup queues. This is where the last-write-timestamp mentioned previously plays a crucial role. The Rollup server inspects this timestamp to determine if a given counter needs to be re-queued, ensuring that we continue aggregating until it has fully caught up with the writes.\n\nNow, let’s see how we leverage this counter type to provide an up-to-date current count in near-realtime.\n\nExperimental: Accurate Global Counter\n\nWe are experimenting with a slightly modified version of the Eventually Consistent counter. Again, take the term ‘Accurate’ with a grain of salt. The key difference between this type of counter and its counterpart is that the delta, representing the counts since the last-rolled-up timestamp, is computed in real-time.\n\nAnd then, currentAccurateCount = lastRollupCount + delta\n\nAggregating this delta in real-time can impact the performance of this operation, depending on the number of events and partitions that need to be scanned to retrieve this delta. The same principle of rolling up in batches applies here to prevent scanning too many partitions in parallel. Conversely, if the counters in this dataset are accessed frequently, the time gap for the delta remains narrow, making this approach of fetching current counts quite effective.\n\nNow, let’s see how all this complexity is managed by having a unified Control Plane configuration.\n\nControl Plane\n\nThe Data Gateway Platform Control Plane manages control settings for all abstractions and namespaces, including the Counter Abstraction. Below, is an example of a control plane configuration for a namespace that supports eventually consistent counters with low cardinality:\n\n\"persistence_configuration\": [\n\n{\n\n\"id\": \"CACHE\", // Counter cache config\n\n\"scope\": \"dal=counter\",\n\n\"physical_storage\": {\n\n\"type\": \"EVCACHE\", // type of cache storage\n\n\"cluster\": \"evcache_dgw_counter_tier1\" // Shared EVCache cluster\n\n}\n\n},\n\n{\n\n\"id\": \"COUNTER_ROLLUP\",\n\n\"scope\": \"dal=counter\", // Counter abstraction config\n\n\"physical_storage\": {\n\n\"type\": \"CASSANDRA\", // type of Rollup store\n\n\"cluster\": \"cass_dgw_counter_uc1\", // physical cluster name\n\n\"dataset\": \"my_dataset_1\" // namespace/dataset\n\n},\n\n\"counter_cardinality\": \"LOW\", // supported counter cardinality\n\n\"config\": {\n\n\"counter_type\": \"EVENTUAL\", // Type of counter\n\n\"eventual_counter_config\": { // eventual counter type\n\n\"internal_config\": {\n\n\"queue_config\": { // adjust w.r.t cardinality\n\n\"num_queues\" : 8, // Rollup queues per instance\n\n\"coalesce_ms\": 10000, // coalesce duration for rollups\n\n\"capacity_bytes\": 16777216 // allocated memory per queue\n\n},\n\n\"rollup_batch_count\": 32 // parallelization factor\n\n}\n\n}\n\n}\n\n},\n\n{\n\n\"id\": \"EVENT_STORAGE\",\n\n\"scope\": \"dal=ts\", // TimeSeries Event store\n\n\"physical_storage\": {\n\n\"type\": \"CASSANDRA\", // persistent store type\n\n\"cluster\": \"cass_dgw_counter_uc1\", // physical cluster name\n\n\"dataset\": \"my_dataset_1\", // keyspace name\n\n},\n\n\"config\": {\n\n\"time_partition\": { // time-partitioning for events\n\n\"buckets_per_id\": 4, // event buckets within\n\n\"seconds_per_bucket\": \"600\", // smaller width for LOW card\n\n\"seconds_per_slice\": \"86400\", // width of a time slice table\n\n},\n\n\"accept_limit\": \"5s\", // boundary for immutability\n\n},\n\n\"lifecycleConfigs\": {\n\n\"lifecycleConfig\": [\n\n{\n\n\"type\": \"retention\", // Event retention\n\n\"config\": {\n\n\"close_after\": \"518400s\",\n\n\"delete_after\": \"604800s\" // 7 day count event retention\n\n}\n\n}\n\n]\n\n}\n\n}\n\n]\n\nUsing such a control plane configuration, we compose multiple abstraction layers using containers deployed on the same host, with each container fetching configuration specific to its scope.\n\nProvisioning\n\nAs with the TimeSeries abstraction, our automation uses a bunch of user inputs regarding their workload and cardinalities to arrive at the right set of infrastructure and related control plane configuration. You can learn more about this process in a talk given by one of our stunning colleagues, Joey Lynch : How Netflix optimally provisions infrastructure in the cloud.\n\nPerformance\n\nAt the time of writing this blog, this service was processing close to 75K count requests/second globally across the different API endpoints and datasets:\n\nwhile providing single-digit millisecond latencies for all its endpoints:\n\nFuture Work\n\nWhile our system is robust, we still have work to do in making it more reliable and enhancing its features. Some of that work includes:\n\nRegional Rollups: Cross-region replication issues can result in missed events from other regions. An alternate strategy involves establishing a rollup table for each region, and then tallying them in a global rollup table. A key challenge in this design would be effectively communicating the clearing of the counter across regions.\n\nCross-region replication issues can result in missed events from other regions. An alternate strategy involves establishing a rollup table for each region, and then tallying them in a global rollup table. A key challenge in this design would be effectively communicating the clearing of the counter across regions. Error Detection and Stale Counts: Excessively stale counts can occur if rollup events are lost or if a rollup fails and isn’t retried. This isn’t an issue for frequently accessed counters, as they remain in rollup circulation. This issue is more pronounced for counters that aren’t accessed frequently. Typically, the initial read for such a counter will trigger a rollup, self-remediating the issue. However, for use cases that cannot accept potentially stale initial reads, we plan to implement improved error detection, rollup handoffs, and durable queues for resilient retries.\n\nConclusion\n\nDistributed counting remains a challenging problem in computer science. In this blog, we explored multiple approaches to implement and deploy a Counting service at scale. While there may be other methods for distributed counting, our goal has been to deliver blazing fast performance at low infrastructure costs while maintaining high availability and providing idempotency guarantees. Along the way, we make various trade-offs to meet the diverse counting requirements at Netflix. We hope you found this blog post insightful.\n\nStay tuned for Part 3 of Composite Abstractions at Netflix, where we’ll introduce our Graph Abstraction, a new service being built on top of the Key-Value Abstraction and the TimeSeries Abstraction to handle high-throughput, low-latency graphs.\n\nAcknowledgments\n\nSpecial thanks to our stunning colleagues who contributed to the Counter Abstraction’s success: Joey Lynch, Vinay Chella, Kaidan Fullerton, Tom DeVoe, Mengqing Wang, Varun Khaitan", "label": "non_personal"}
{"title": "Title Launch Observability at Netflix Scale", "url": "https://netflixtechblog.com/title-launch-observability-at-netflix-scale-c88c586629eb?source=collection_home---4------10-----------------------", "content": "Title Launch Observability at Netflix Scale\n\nPart 1: Understanding The Challenges Netflix Technology Blog 5 min read · Dec 17, 2024 -- 7 Listen Share\n\nBy: Varun Khaitan\n\nWith special thanks to my stunning colleagues: Mallika Rao, Esmir Mesic, Hugo Marques\n\nIntroduction\n\nAt Netflix, we manage over a thousand global content launches each month, backed by billions of dollars in annual investment. Ensuring the success and discoverability of each title across our platform is a top priority, as we aim to connect every story with the right audience to delight our members. To achieve this, we are committed to building robust systems that deliver comprehensive observability, enabling us to take full accountability for every title on our service.\n\nThe Challenge of Title Launch Observability\n\nAs engineers, we’re wired to track system metrics like error rates, latencies, and CPU utilization — but what about metrics that matter to a title’s success?\n\nConsider the following example of two different Netflix Homepages:\n\nSample Homepage A\n\nSample Homepage B\n\nTo a basic recommendation system, the two sample pages might appear equivalent as long as the viewer watches the top title. Yet, these pages couldn’t be more different. Each title represents countless hours of effort and creativity, and our systems need to honor that uniqueness.\n\nHow do we bridge this gap? How can we design systems that recognize these nuances and empower every title to shine and bring joy to our members?\n\nThe Operational Needs of a Personalization System\n\nIn the early days of Netflix Originals, our launch team would huddle together at midnight, manually verifying that titles appeared in all the right places. While this hands-on approach worked for a handful of titles, it quickly became clear that it couldn’t scale. As Netflix expanded globally and the volume of title launches skyrocketed, the operational challenges of maintaining this manual process became undeniable.\n\nOperating a personalization system for a global streaming service involves addressing numerous inquiries about why certain titles appear or fail to appear at specific times and places.\n\nSome examples:\n\nWhy is title X not showing on the Coming Soon row for a particular member?\n\nWhy is title Y missing from the search page in Brazil?\n\nIs title Z being displayed correctly in all product experiences as intended?\n\nAs Netflix scaled, we faced the mounting challenge of providing accurate, timely answers to increasingly complex queries about title performance and discoverability. This led to a suite of fragmented scripts, runbooks, and ad hoc solutions scattered across teams — an approach that was neither sustainable nor efficient.\n\nThe stakes are even higher when ensuring every title launches flawlessly. Metadata and assets must be correctly configured, data must flow seamlessly, microservices must process titles without error, and algorithms must function as intended. The complexity of these operational demands underscored the urgent need for a scalable solution.\n\nAutomating the Operations\n\nIt becomes evident over time that we need to automate our operations to scale with the business. As we thought more about this problem and possible solutions, two clear options emerged.\n\nOption 1: Log Processing\n\nLog processing offers a straightforward solution for monitoring and analyzing title launches. By logging all titles as they are displayed, we can process these logs to identify anomalies and gain insights into system performance. This approach provides a few advantages:\n\nLow burden on existing systems: Log processing imposes minimal changes to existing infrastructure. By leveraging logs, which are already generated during regular operations, we can scale observability without significant system modifications. This allows us to focus on data analysis and problem-solving rather than managing complex system changes. Using the source of truth: Logs serve as a reliable “source of truth” by providing a comprehensive record of system events. They allow us to verify whether titles are presented as intended and investigate any discrepancies. This capability is crucial for ensuring our recommendation systems and user interfaces function correctly, supporting successful title launches.\n\nHowever, taking this approach also presents several challenges:\n\nCatching Issues Ahead of Time: Logging primarily addresses post-launch scenarios, as logs are generated only after titles are shown to members. To detect issues proactively, we need to simulate traffic and predict system behavior in advance. Once artificial traffic is generated, discarding the response object and relying solely on logs becomes inefficient. Appropriate Accuracy: Comprehensive logging requires services to log both included and excluded titles, along with reasons for exclusion. This could lead to an exponential increase in logged data. Utilizing probabilistic logging methods could compromise accuracy, making it difficult to ascertain whether a title’s absence in logs is due to exclusion or random chance. SLA and Cost Considerations: Our existing online logging systems do not natively support logging at the title granularity level. While reengineering these systems to accommodate this additional axis is possible, it would entail increased costs. Additionally, the time-sensitive nature of these investigations precludes the use of cold storage, which cannot meet the stringent SLAs required.\n\nOption 2: Observability Endpoints in Our Personalization Systems\n\nTo prioritize title launch observability, we could adopt a centralized approach. By introducing observability endpoints across all systems, we can enable real-time data flow into a dedicated microservice for title launch observability. This approach embeds observability directly into the very fabric of services managing title launches and personalization, ensuring seamless monitoring and insights. Key benefits and strategies include:\n\nReal-Time Monitoring: Observability endpoints enable real-time monitoring of system performance and title placements, allowing us to detect and address issues as they arise. Proactive Issue Detection: By simulating future traffic(an aspect we call “time travel”) and capturing system responses ahead of time, we can preemptively identify potential issues before they impact our members or the business. Enhanced Accuracy: Observability endpoints provide precise data on title inclusions and exclusions, allowing us to make accurate assertions about system behavior and title visibility. It also provides us with advanced debugability information needed to fix identified issues. Scalability and Cost Efficiency: While initial implementation required some investment, this approach ultimately offers a scalable and cost-effective solution to managing title launches at Netflix scale.\n\nChoosing this option also comes with some tradeoffs:\n\nSignificant Initial Investment: Several systems would need to create new endpoints and refactor their codebases to adopt this new method of prioritizing launches. Synchronization Risk: There would be a potential risk that these new endpoints may not accurately represent production behavior, thus necessitating conscious efforts to ensure all endpoints remain synchronized.\n\nUp Next\n\nBy adopting a comprehensive observability strategy that includes real-time monitoring, proactive issue detection, and source of truth reconciliation, we’ve significantly enhanced our ability to ensure the successful launch and discovery of titles across Netflix, enriching the global viewing experience for our members. In the next part of this series, we’ll dive into how we achieved this, sharing key technical insights and details.\n\nStay tuned for a closer look at the innovation behind the scenes in Part 2!", "label": "non_personal"}
{"title": "Supercharge your notebooks: The new AI-first Google Colab is now available to everyone", "url": "https://developers.googleblog.com/en/new-ai-first-google-colab-now-available-to-everyone/", "content": "Last month at Google I/O 2025, we shared our vision for a reimagined, AI-first Colab, a true coding partner in your notebook designed to help you tackle your most challenging problems faster than ever. We’ve started by rolling these features out to a small group of users, and the response has been incredible. Today, we are thrilled to make AI-first Colab available to everyone!\n\n\n\nEarly users have been embracing Colab's new agentic capabilities to accelerate their projects, learn new skills, and unlock insights from their data in ways that have delighted and inspired us.\n\n\n\nFrom early access to everyday productivity\n\nOur goal was to build an AI collaborator that understands your intentions and integrates seamlessly into your workflow. Based on user feedback, this new experience is already making a significant impact.\n\nHere are a few of the top ways people are using the new Colab AI:\n\n\n\n1: Accelerating End-to-End Machine Learning Projects\n\nUsers are leveraging Colab AI for the entire machine learning lifecycle. From taking a raw dataset and asking the agent to autonomously perform cleaning and preparation, to generating feature analysis, training models, and evaluating the results. This turns hours of work into a guided, conversational experience.\n\n\n\n2: Smarter Debugging\n\nCoding is an iterative process. Colab AI acts as a pair programmer to help you prototype ideas, generate boilerplate code, and understand new libraries. When you hit an error, the AI doesn't just help find the bug, it can suggest a fix in a clear diff view, helping you learn and keep going with your project. The result is a massive boost in productivity.\n\n\n\n3: Creating Stunning Visualizations with Zero Hassle\n\nData exploration is incomplete without visualization. Users are simply asking Colab AI to graph their data, and it generates high-quality, clearly labeled charts without the need for manual tweaking of plotting libraries.\n\n\n\nA quick look at the features powering your workflow\n\nThese use cases are powered by a suite of new, deeply integrated features:\n\nIterative Querying: A conversational experience where you can ask for code, get explanations about libraries, and intelligently fix errors.\n\nNext-Generation Data Science Agent (DSA): Trigger autonomous analytical workflows. The agent creates a plan, executes code, reasons about the results, and presents its findings, all while allowing you to provide feedback and stay in control.\n\nEffortless Code Transformation: Simply describe a change in natural language, and Colab will identify and refactor the relevant code for you.\n\n\n\nGet started with Colab AI today!\n\nWe are incredibly excited to put these powerful new capabilities into the hands of the entire Colab community. This is a major step in our journey to create a more powerful and intuitive AI-first Colab, and we’re just getting started.\n\nReady to try it out? It’s easy to get started:\n\n1: Open any new or existing notebook in Google Colab.\n\n2: Look for the Gemini spark icon in the bottom toolbar.", "label": "non_personal"}
{"title": "Cloud Efficiency at Netflix", "url": "https://netflixtechblog.com/cloud-efficiency-at-netflix-f2a142955f83?source=collection_home---4------9-----------------------", "content": "Cloud Efficiency at Netflix Netflix Technology Blog 5 min read · Dec 17, 2024 -- 10 Listen Share\n\nBy J Han, Pallavi Phadnis\n\nContext\n\nAt Netflix, we use Amazon Web Services (AWS) for our cloud infrastructure needs, such as compute, storage, and networking to build and run the streaming platform that we love. Our ecosystem enables engineering teams to run applications and services at scale, utilizing a mix of open-source and proprietary solutions. In turn, our self-serve platforms allow teams to create and deploy, sometimes custom, workloads more efficiently. This diverse technological landscape generates extensive and rich data from various infrastructure entities, from which, data engineers and analysts collaborate to provide actionable insights to the engineering organization in a continuous feedback loop that ultimately enhances the business.\n\nOne crucial way in which we do this is through the democratization of highly curated data sources that sunshine usage and cost patterns across Netflix’s services and teams. The Data & Insights organization partners closely with our engineering teams to share key efficiency metrics, empowering internal stakeholders to make informed business decisions.\n\nData is Key\n\nThis is where our team, Platform DSE (Data Science Engineering), comes in to enable our engineering partners to understand what resources they’re using, how effectively and efficiently they use those resources, and the cost associated with their resource usage. We want our downstream consumers to make cost conscious decisions using our datasets.\n\nTo address these numerous analytic needs in a scalable way, we’ve developed a two-component solution:\n\nFoundational Platform Data (FPD): This component provides a centralized data layer for all platform data, featuring a consistent data model and standardized data processing methodology. Cloud Efficiency Analytics (CEA): Built on top of FPD, this component offers an analytics data layer that provides time series efficiency metrics across various business use cases.\n\nFoundational Platform Data (FPD)\n\nWe work with different platform data providers to get inventory, ownership, and usage data for the respective platforms they own. Below is an example of how this framework applies to the Spark platform. FPD establishes data contracts with producers to ensure data quality and reliability; these contracts allow the team to leverage a common data model for ownership. The standardized data model and processing promotes scalability and consistency.\n\nCloud Efficiency Analytics (CEA Data)\n\nOnce the foundational data is ready, CEA consumes inventory, ownership, and usage data and applies the appropriate business logic to produce cost and ownership attribution at various granularities. The data model approach in CEA is to compartmentalize and be transparent; we want downstream consumers to understand why they’re seeing resources show up under their name/org and how those costs are calculated. Another benefit to this approach is the ability to pivot quickly as new or changes in business logic is/are introduced.\n\n* For cost accounting purposes, we resolve assets to a single owner, or distribute costs when assets are multi-tenant. However, we do also provide usage and cost at different aggregations for different consumers.\n\nData Principles\n\nAs the source of truth for efficiency metrics, our team’s tenants are to provide accurate, reliable, and accessible data, comprehensive documentation to navigate the complexity of the efficiency space, and well-defined Service Level Agreements (SLAs) to set expectations with downstream consumers during delays, outages or changes.\n\nWhile ownership and cost may seem straightforward, the complexity of the datasets is considerably high due to the breadth and scope of the business infrastructure and platform specific features. Services can have multiple owners, cost heuristics are unique to each platform, and the scale of infra data is large. As we work on expanding infrastructure coverage to all verticals of the business, we face a unique set of challenges:\n\nA Few Sizes to Fit the Majority\n\nDespite data contracts and a standardized data model on transforming upstream platform data into FPD and CEA, there is usually some degree of customization that is unique to that particular platform. As the centralized source of truth, we feel the constant tension of where to place the processing burden. Decision-making involves ongoing transparent conversations with both our data producers and consumers, frequent prioritization checks, and alignment with business needs as informed captains in this space.\n\nData Guarantees\n\nFor data correctness and trust, it’s crucial that we have audits and visibility into health metrics at each layer in the pipeline in order to investigate issues and root cause anomalies quickly. Maintaining data completeness while ensuring correctness becomes challenging due to upstream latency and required transformations to have the data ready for consumption. We continuously iterate our audits and incorporate feedback to refine and meet our SLAs.\n\nAbstraction Layers\n\nWe value people over process, and it is not uncommon for engineering teams to build custom SaaS solutions for other parts of the organization. Although this fosters innovation and improves development velocity, it can create a bit of a conundrum when it comes to understanding and interpreting usage patterns and attributing cost in a way that makes sense to the business and end consumer. With clear inventory, ownership, and usage data from FPD, and precise attribution in the analytical layer, we aim to provide metrics to downstream users regardless of whether they utilize and build on top of internal platforms or on AWS resources directly.\n\nFuture Forward\n\nLooking ahead, we aim to continue onboarding platforms to FPD and CEA, striving for nearly complete cost insight coverage in the upcoming year. Longer term, we plan to extend FPD to other areas of the business such as security and availability. We aim to move towards proactive approaches via predictive analytics and ML for optimizing usage and detecting anomalies in cost.\n\nUltimately, our goal is to enable our engineering organization to make efficiency-conscious decisions when building and maintaining the myriad of services that allow us to enjoy Netflix as a streaming service.\n\nAcknowledgments\n\nThe FPD and CEA work would not have been possible without the cross functional input of many outstanding colleagues and our dedicated team building these important data assets.\n\n—\n\nA bit about the authors:\n\nJHan enjoys nature, reading fantasy, and finding the best chocolate chip cookies and cinnamon rolls. She is adamant about writing the SQL select statement with leading commas.\n\nPallavi enjoys music, travel and watching astrophysics documentaries. With 15+ years working with data, she knows everything’s better with a dash of analytics and a cup of coffee!", "label": "non_personal"}
{"title": "Using KerasHub for easy end-to-end machine learning workflows with Hugging Face", "url": "https://developers.googleblog.com/en/load-model-weights-from-safetensors-into-kerashub-multi-framework-machine-learning/", "content": "How to load SafeTensors checkpoints across different frameworks\n\nAs the AI ecosystem continues to evolve, there are more and more ways to define machine learning models, and even more ways to save the model weights that result from training and fine-tuning. In this growing set of choices, KerasHub allows you to mix and match popular model architectures and their weights across different ML frameworks. For example, a popular place to load checkpoints from is the Hugging Face Hub. Many of those model checkpoints were created with the Hugging Face transformers library in the SafeTensors format. Regardless of what ML framework was used to create the model checkpoint, those weights can be loaded into a KerasHub model, which allows you to use your choice of framework (JAX, PyTorch, or TensorFlow) to run the model. Yes, that means you can run a checkpoint from Mistral or Llama on JAX, or even load Gemma with PyTorch – it doesn't get any more flexible than that. Let's take a look at some of these terms in more detail, and talk about how this works in practice.\n\nModel architecture vs. model weights When loading models, there are two distinct parts that we need: the model architecture and the model weights (often called \"checkpoints\"). Let's define each of these in more detail. When we say \"model architecture\", we are referring to how the layers of the model are arranged, and the operations that happen within them. Another way to describe this might be to call it the \"structure\" of the model. We use Python frameworks like PyTorch, JAX, or Keras to express model architectures. When we talk about \"model weights\", we are referring to the \"parameters\" of a model, or numbers in a model that are changed over the course of training. The particular values of these weights are what give a trained model its characteristics. \"Checkpoints\" are a snapshot of the values of the model weights at a particular point in the training. The typical checkpoint files that are shared and widely used are the ones where the model has reached a particularly good training outcome. As the same model architecture is further refined with fine-tuning and other techniques, additional new checkpoint files are created. For example, many developers have taken Google's gemma-2-2b-it model and fine-tuned it with their own datasets, and you can see over 600 examples. All of these fine-tuned models use the same architecture as the original gemma-2-2b-it model, but their checkpoints have differing weights. So there we have it: the model architecture is described with code, while model weights are trained parameters, saved as checkpoint files. When we have a model architecture together with a set of model weights (in the form of a checkpoint file), we create a functioning model that produces useful outputs.\n\nSorry, your browser doesn't support playback for this video Different model weights can be loaded into the same model architecture. These different sets of weights are saved as checkpoints.\n\nTools like Hugging Face's transformers library and Google's KerasHub library provide model architectures and the APIs you need to experiment with them. Examples of checkpoint repositories include Hugging Face Hub and Kaggle Models. You can mix and match model architecture libraries with your choice of checkpoint repositories. For example, you can load a checkpoint from Hugging Face Hub into a JAX model architecture and fine-tune it with KerasHub. For a different task, you might find a checkpoint on Kaggle Models that's suitable for your needs. This flexibility and separation means you are not boxed into one ecosystem.\n\nWhat is KerasHub? So we’ve mentioned KerasHub a few times– let’s go into it in more detail. KerasHub is a Python library that helps make defining model architectures easier. It contains many of the most popular and commonly used machine learning models today, and more are being added all the time. Because it's based on Keras, KerasHub supports all three major Python machine learning libraries used today: PyTorch, JAX, and TensorFlow. This means you can have model architectures defined in whichever library you'd like. Furthermore, since KerasHub supports the most common checkpoint formats, you can easily load checkpoints from many checkpoint repositories. For example, you can find hundreds of thousands of checkpoints on Hugging Face and Kaggle to load into these model architectures.\n\nComparisons to the Hugging Face transformers library A common workflow by developers is to use the Hugging Face transformers library to fine-tune a model and upload it to the Hugging Face Hub. And if you’re a user of transformers , you’ll also find many familiar API patterns in KerasHub. Check out the KerasHub API documentation to learn more. An interesting aspect of KerasHub is that many of the checkpoints found on Hugging Face Hub are compatible with not only the transformers library, but also KerasHub. Let's take a look at how that works.\n\nKerasHub is compatible with Hugging Face Hub Hugging Face has a model checkpoint repository, called Hugging Face Hub. It's one of the many places where the machine learning community uploads their model checkpoints to share with the world. Especially popular on Hugging Face is the SafeTensors format, which is compatible with KerasHub. You can load these checkpoints from Hugging Face Hub directly into your KerasHub model, as long as the model architecture is available. Wondering if your favorite model is available? You can check https://keras.io/keras_hub/presets/ for a list of supported model architectures. And don't forget, all the community created fine-tuned checkpoints of these model architectures are also compatible! We recently created a new guide to help explain the process in more detail. How does this all work? KerasHub has built-in converters that simplify the use of Hugging Face transformers models. These converters automatically handle the process of translating Hugging Face model checkpoints into a format that's compatible with the KerasHub. This means you can seamlessly load a wide variety of pretrained Hugging Face transformer models from the Hugging Face Hub directly into KerasHub with just a few lines of code. If you notice a missing model architecture, you can add it by filing a pull request on GitHub.\n\nHow to load a Hugging Face Hub checkpoint into KerasHub So how do we get checkpoints from Hugging Face Hub loaded into KerasHub? Let's take a look at some concrete examples. We'll start by first choosing our machine learning library as our Keras \"backend\". We'll use JAX in the examples shown, but you can choose between JAX, PyTorch, or TensorFlow for any of them. All the examples below work regardless of which one you choose. Then we can proceed by importing keras , keras_hub , and huggingface_hub , and then login with our Hugging Face User Access token so we can access the model checkpoints.\n\nimport os os.environ[\"KERAS_BACKEND\"] = \"jax\" # or \"torch\" or \"tensorflow\" import keras from keras_hub import models from huggingface_hub import login login('HUGGINGFACE_TOKEN') Python Copied\n\nPut a Mistral model on JAX First up, perhaps we want to run a checkpoint from Mistral on JAX? Over on KerasHub, there are a handful of Mistral models available on KerasHub's list of available model architectures, let's try out mistral_0.2_instruct_7b_en . Clicking into it, we see that we should use the MistralCausalLM class to call from_preset . On the Hugging Face Hub side of things, we see that the corresponding model checkpoint is stored here, with over 900 fine-tuned versions. Browsing that list, there's a popular cybersecurity-focused fine-tuned model called Lily, with the pathname of segolilylabs/Lily-Cybersecurity-7B-v0.2 . We'll also need to add \" hf:// \" before that path to specify that KerasHub should look at Hugging Face Hub.\n\nSorry, your browser doesn't support playback for this video\n\nPutting it all together, we get the following code:\n\n# Model checkpoint from Hugging Face Hub gemma_lm = models.MistralCausalLM.from_preset(\"hf://segolilylabs/Lily-Cybersecurity-7B-v0.2\") gemma_lm.generate(\"Lily, how do evil twin wireless attacks work?\", max_length=30) Python Copied\n\nRunning Llama 3.1 on JAX Llama 3.1-8B-Instruct is a popular model, with over 5 million downloads last month. Let's put a fine-tuned version on JAX. With over 1400 fine-tuned checkpoints, there's no lack of choice. The xVerify fine-tuned checkpoint looks interesting, let's load that into JAX on KerasHub. We'll use the Llama3CausalLM class to reflect the model architecture that we are using. As before, we'll need the appropriate path from Hugging Face Hub, prefixed with \" hf:// \". It's pretty amazing that we can load and call a model with just two lines of code, right?\n\n# Model checkpoint from Hugging Face Hub gemma_lm = models.Llama3CausalLM.from_preset(\"hf://IAAR-Shanghai/xVerify-8B-I\") gemma_lm.generate(\"What is the tallest building in NYC?\", max_length=100) Python Copied\n\nLoad Gemma on JAX Finally, let's load a fine-tuned Gemma-3-4b-it checkpoint into JAX. We'll use the Gemma3CausalLM class, and select one of the fine-tuned checkpoints. How about EraX, a multilingual translator? As before, we'll use the pathname with the Hugging Face Hub prefix to create the full path of \" hf://erax-ai/EraX-Translator-V1.0 \".\n\n# Model checkpoint from Hugging Face Hub gemma_lm = models.Gemma3CausalLM.from_preset(\"hf://erax-ai/EraX-Translator-V1.0\") gemma_lm.generate(\"Translate to German: \", max_length=30) Python Copied", "label": "non_personal"}
{"title": "Part 1: A Survey of Analytics Engineering Work at Netflix", "url": "https://netflixtechblog.com/part-1-a-survey-of-analytics-engineering-work-at-netflix-d761cfd551ee?source=collection_home---4------8-----------------------", "content": "Part 1: A Survey of Analytics Engineering Work at Netflix Netflix Technology Blog 7 min read · Dec 17, 2024 -- 2 Listen Share\n\nThis article is the first in a multi-part series sharing a breadth of Analytics Engineering work at Netflix, recently presented as part of our annual internal Analytics Engineering conference. We kick off with a few topics focused on how we’re empowering Netflix to efficiently produce and effectively deliver high quality, actionable analytic insights across the company. Subsequent posts will detail examples of exciting analytic engineering domain applications and aspects of the technical craft.\n\nAt Netflix, we seek to entertain the world by ensuring our members find the shows and movies that will thrill them. Analytics at Netflix powers everything from understanding what content will excite and bring members back for more to how we should produce and distribute a content slate that maximizes member joy. Analytics Engineers deliver these insights by establishing deep business and product partnerships; translating business challenges into solutions that unblock critical decisions; and designing, building, and maintaining end-to-end analytical systems.\n\nEach year, we bring the Analytics Engineering community together for an Analytics Summit — a 3-day internal conference to share analytical deliverables across Netflix, discuss analytic practice, and build relationships within the community. We covered a broad array of exciting topics and wanted to spotlight a few to give you a taste of what we’re working on across Analytics Engineering at Netflix!\n\nDataJunction: Unifying Experimentation and Analytics\n\nYian Shang, Anh Le\n\nAt Netflix, like in many organizations, creating and using metrics is often more complex than it should be. Metric definitions are often scattered across various databases, documentation sites, and code repositories, making it difficult for analysts and data scientists to find reliable information quickly. This fragmentation leads to inconsistencies and wastes valuable time as teams end up reinventing metrics or seeking clarification on definitions that should be standardized and readily accessible.\n\nEnter DataJunction (DJ). DJ acts as a central store where metric definitions can live and evolve. Once a metric owner has registered a metric into DJ, metric consumers throughout the organization can apply that same metric definition to a set of filtered records and aggregate to any dimensional grain.\n\nAs an example, imagine an analyst wanting to create a “Total Streaming Hours” metric. To add this metric to DJ, they need to provide two pieces of information:\n\nThe fact table that the metric comes from:\n\nSELECT\n\naccount_id, country_iso_code, streaming_hours\n\nFROM streaming_fact_table\n\nThe metric expression:\n\n`SUM(streaming_hours)`\n\nThen metric consumers throughout the organization can call DJ to request either the SQL or the resulting data. For example,\n\ntotal_streaming_hours of each account:\n\ndj.sql(metrics=[“total_streaming_hours”], dimensions=[“account_id”]))\n\ntotal_streaming_hours of each country:\n\ndj.sql(metrics=[“total_streaming_hours”], dimensions=[“country_iso_code”]))\n\ntotal_streaming_hours of each account in the US:\n\ndj.sql(metrics=[“total_streaming_hours”], dimensions=[“country_iso_code”], filters=[“country_iso_code = ‘US’”]))\n\nThe key here is that DJ can perform the dimensional join on users’ behalf. If country_iso_code doesn’t already exist in the fact table, the metric owner only needs to tell DJ that account_id is the foreign key to an `users_dimension_table` (we call this process “dimension linking”). DJ then can perform the joins to bring in any requested dimensions from `users_dimension_table`.\n\nThe Netflix Experimentation Platform heavily leverages this feature today by treating cell assignment as just another dimension that it asks DJ to bring in. For example, to compare the average streaming hours in cell A vs cell B, the Experimentation Platform relies on DJ to bring in “cell_assignment” as a user’s dimension (no different from country_iso_code). A metric can therefore be defined once in DJ and be made available across analytics dashboards and experimentation analysis.\n\nDJ has a strong pedigree–there are several prior semantic layers in the industry (e.g. Minerva at Airbnb; dbt Transform, Looker, and AtScale as paid solutions). DJ stands out as an open source solution that is actively developed and stress-tested at Netflix. We’d love to see DJ easing your metric creation and consumption pain points!\n\nLORE: How we’re democratizing analytics at Netflix\n\nApurva Kansara\n\nAt Netflix, we rely on data and analytics to inform critical business decisions. Over time, this has resulted in large numbers of dashboard products. While such analytics products are tremendously useful, we noticed a few trends:\n\nA large portion of such products have less than 5 MAU (monthly active users) We spend a tremendous amount of time building and maintaining business metrics and dimensions We see inconsistencies in how a particular metric is calculated, presented, and maintained across the Data & Insights organization. It is challenging to scale such bespoke solutions to ever-changing and increasingly complex business needs.\n\nAnalytics Enablement is a collection of initiatives across Data & Insights all focused on empowering Netflix analytic practitioners to efficiently produce and effectively deliver high-quality, actionable insights.\n\nSpecifically, these initiatives are focused on enabling analytics rather than on the activities that produce analytics (e.g., dashboarding, analysis, research, etc.).\n\nAs part of broad analytics enablement across all business domains, we invested in a chatbot to provide real insights to our end users using the power of LLM. One reason LLMs are well suited for such problems is that they tie the versatility of natural language with the power of data query to enable our business users to query data that would otherwise require sophisticated knowledge of underlying data models.\n\nBesides providing the end user with an instant answer in a preferred data visualization, LORE instantly learns from the user’s feedback. This allows us to teach LLM a context-rich understanding of internal business metrics that were previously locked in custom code for each of the dashboard products.\n\nSome of the challenges we run into:\n\nGaining user trust: To gain our end users’ trust, we focused on our model’s explainability. For example, LORE provides human-readable reasoning on how it arrived at the answer that users can cross-verify. LORE also provides a confidence score to our end users based on its grounding in the domain space.\n\nTraining: We created easy-to-provide feedback using 👍 and 👎 with a fully integrated fine-tuning loop to allow end-users to teach new domains and questions around it effectively. This allowed us to bootstrap LORE across several domains within Netflix.\n\nDemocratizing analytics can unlock the tremendous potential of data for everyone within the company. With Analytics enablement and LORE, we’ve enabled our business users to truly have a conversation with the data.\n\nLeveraging Foundational Platform Data to enable Cloud Efficiency Analytics\n\nJ Han, Pallavi Phadnis\n\nAt Netflix, we use Amazon Web Services (AWS) for our cloud infrastructure needs, such as compute, storage, and networking to build and run the streaming platform that we love. Our ecosystem enables engineering teams to run applications and services at scale, utilizing a mix of open-source and proprietary solutions. In order to understand how efficiently we operate in this diverse technological landscape, the Data & Insights organization partners closely with our engineering teams to share key efficiency metrics, empowering internal stakeholders to make informed business decisions.\n\nThis is where our team, Platform DSE (Data Science Engineering), comes in to enable our engineering partners to understand what resources they’re using, how effectively they utilize those resources, and the cost associated with their resource usage. By creating curated datasets and democratizing access via a custom insights app and various integration points, downstream users can gain granular insights essential for making data-driven, cost-effective decisions for the business.\n\nTo address the numerous analytic needs in a scalable way, we’ve developed a two-component solution:\n\nFoundational Platform Data (FPD): This component provides a centralized data layer for all platform data, featuring a consistent data model and standardized data processing methodology. We work with different platform data providers to get inventory, ownership, and usage data for the respective platforms they own. Cloud Efficiency Analytics (CEA): Built on top of FPD, this component offers an analytics data layer that provides time series efficiency metrics across various business use cases. Once the foundational data is ready, CEA consumes inventory, ownership, and usage data and applies the appropriate business logic to produce cost and ownership attribution at various granularities.\n\nAs the source of truth for efficiency metrics, our team’s tenants are to provide accurate, reliable, and accessible data, comprehensive documentation to navigate the complexity of the efficiency space, and well-defined Service Level Agreements (SLAs) to set expectations with downstream consumers during delays, outages, or changes.\n\nLooking ahead, we aim to continue onboarding platforms, striving for nearly complete cost insight coverage. We’re also exploring new use cases, such as tailored reports for platforms, predictive analytics for optimizing usage and detecting anomalies in cost, and a root cause analysis tool using LLMs.\n\nUltimately, our goal is to enable our engineering organization to make efficiency-conscious decisions when building and maintaining the myriad of services that allows us to enjoy Netflix as a streaming service. For more detail on our modeling approach and principles, check out this post!", "label": "non_personal"}
{"title": "Gemini 2.5 for robotics and embodied intelligence", "url": "https://developers.googleblog.com/en/gemini-25-for-robotics-and-embodied-intelligence/", "content": "The latest generation of Gemini models, 2.5 Pro and Flash, are unlocking new frontiers in robotics. Their advanced coding, reasoning, and multimodal capabilities, now combined with spatial understanding, provide the foundation for the next generation of interactive and intelligent robots. This post explores how developers can leverage Gemini 2.5 to build sophisticated robotics applications. We'll provide practical examples with prompts to show using Gemini 2.5 and the Live API for: Semantic scene understanding for complex queries: Identify and label objects from robot camera feeds. Understand complex queries through multimodal reasoning. Combine spatial reasoning with code generation to control robots: Use the robot's API to call functions and bring task plans to life. Build interactive robotics applications with the Live API: Convert voice commands into executable robot plans. In March, we launched our Gemini Robotics models, including Gemini Robotics-ER, our advanced embodied reasoning model optimized for the unique demands of robotics applications. We’re also excited to share how our Gemini Robotics trusted testers are already demonstrating the power of Gemini in robotics applications. We are including examples from Agile Robots, Agility Robotics, Boston Dynamics, and Enchanted Tools. Join the Gemini Robotics-ER trusted tester program waitlist.\n\nSemantic scene understanding for complex queries Reasoning about the physical world is at the core of general and robust control. Gemini 2.5 represents a step in this direction with its improved ability to reason multimodally. Below we share two examples, utilizing Gemini’s pointing and object detection capabilities. Pointing allows a model to refer to entities or parts of entities precisely, and locate them in space. Gemini 2.5 Pro is able to reason about the entities it is pointing to, opening new opportunities for interacting with images. For example, Gemini 2.5 Pro is able to reason about empty space in the context of a supermarket display, knowing that this indicates restocking may be needed. In the example below, Gemini identifies the baby eggplant needs restocking. Gemini 2.5 Pro also shows a nascent ability to locate and read information from that location, as illustrated in the gauge example. Example 1: Gemini 2.5 can locate objects in the scene based on fine-grained language descriptions, for example, find a shelf that needs restocking. Prompt: Point to one bin on the shelf that needs restocking. The answer should follow the json format: [{\"point\": <point>, \"label\": <label1>}, ...]. The points are in [y, x] format normalized to 0-1000. Input image with response overlay:\n\nExample 2: Gemini 2.5 can locate small objects in the scene and estimate states of those objects. For example, it can read gauges. Prompt: Point to all the round gauges. The answer should follow the json format: [{\"point\": <point>, \"label\": <label1>}, ...]. The points are in [y, x] format normalized to 0-1000. Input image with response overlay:\n\nPrompt: What is the reading on the middle gauge? Response: Based on the close-up view, the round gauge in the center-left of the image appears to be reading 0. The needle is pointing directly at the \"0\" mark on the dial.\n\nGoing beyond object-centric perception Gemini 2.5 is able to accurately track multiple objects across time and detect open ended concepts like ‘a spill’. Gemini 2.5 can be prompted into trajectory prediction in the form of a sequence of points. Example 1: Gemini 2.5 can generate bounding boxes for each frame in a video and be visualized like below. Prompt: Detect green bowl, crab, wallet, pink bowl, phone, return a json array with keys box_2d and label. (executed per frame). Input image with response overlay:\n\nSorry, your browser doesn't support playback for this video\n\nExample 2: Gemini 2.5 can detect open-ended concepts relevant to robotics, requiring commonsense knowledge and context specific reasoning. For example, a helpful robot needs to understand the concept of a “spill”. Prompt: 1) Show me the bounding box of spill. Return in a json array with keys box_2d and label. 2) Give the segmentation masks for the spill. Output a JSON list of segmentation masks where each entry contains the 2D bounding box in the key \"box_2d\", the segmentation mask in key \"mask\", and the text label in the key \"label\". Input image with response overlay:\n\nExample 3: Gemini 2.5 can be prompted into trajectory prediction in the form of a sequence of points. Prompt: Generate a robot arm trajectory of 10 points to move the cloth to the spill. The answer should follow the json format: [{\"point\": <point>, \"label\": <label1>}, ...]. The points are in [y, x] format normalized to 0-1000. Input image with response overlay:\n\nUsing spatial understanding and code generation to control robots Gemini 2.5 can utilize its underlying spatial understanding to control robots through code generation. By providing Gemini 2.5 with a robot control API, it can apply advanced capabilities in scene understanding, object manipulation, and code writing together to perform tasks zero-shot, with no additional training. Example 1 below showcases code-generation for “Put the banana in the bowl”. It gives Gemini access to a robot control API and shows how the model leverages its spatial understanding, thinking, and code generation capabilities to select the appropriate API calls and arguments given the task. Gemini 2.5 generates 2 different feasible plans for putting the banana in the bowl. The first solution is to simply pick up the banana, move it above the bowl, and drop it. The second solution lifts the banana, moves the bowl below the banana, and then drops the banana. Example 1: Gemini 2.5 Generate high-level planning code for pick-and-place tasks with different strategies. Prompt: You are given a robot control API and example code below:\n\n# Provided API class RobotAPI: def detect_object(self, obj: str) -> Detection \"\"\"Detects the given object's XYZ location\"\"\" def get_grasp_position_and_euler_orientation(self, gripper: str, object_name: str) -> tuple[numpy.ndarray, numpy.ndarray]: \"\"\"Returns the grasp position and orientation for the given object and gripper from the Gemini Robotics ER model or other grasping primitives. def move_gripper_to(self, gripper: str, position: numpy.ndarray, orientation: numpy.ndarray): \"\"\"Moves the gripper to the given position and orientation.\"\"\" def move_gripper_to_safe_position(self, gripper: str): \"\"\"Moves the given gripper to a safe position out of the table area.\"\"\" def open_gripper(self, gripper: str): \"\"\"Opens the given gripper.\"\"\" def close_gripper(self, gripper: str): \"\"\"Closes the given gripper.\"\"\" robot=RobotAPI() # Example code robot.open_gripper(gripper='RIGHT') banana_detection = robot.detect_object(obj='banana') banana_position = banana_detection.position print(f\"Detected banana position: {banana_position}\") # Gemini uses this to determine which gripper to use. grasp_position, grasp_orientation = robot.get_grasp_position_and_euler_orientation(gripper='RIGHT', object_name='banana') pre_grasp_position = grasp_position + np.array([0, 0, 0.1]) # Add 10cm to the Z-coordinate for pre-grasp robot.move_gripper_to(gripper='RIGHT', position=pre_grasp_position, orientation=grasp_orientation) robot.move_gripper_to(gripper='RIGHT', position=grasp_position, orientation=grasp_orientation) robot.close_gripper(gripper='RIGHT') Python Copied\n\nNow generate code for the task “put the banana in the bowl”. Response 1 with results of code executed: Reasoning (abbreviated) 1: The current position of the right gripper is [-0.04, -0.028, 0.176] . 2: The bowl's detected position is [-0.03839983, -0.02811164, 0.0073104] . 3: To place the banana inside the bowl, the gripper's Z-coordinate needs to be lowered. I will use the bowl's detected X and Y coordinates for the target position. For the Z-coordinate, I will use the bowl's detected Z-coordinate ( 0.0073104 ) and add a small offset (e.g., 0.03m) to ensure the banana is placed inside the bowl and not through the table. This would result in a target Z of approximately 0.007 + 0.03 = 0.037m . This Z-coordinate is higher than the bowl's base but lower than the typical height of a banana when standing on the table, ensuring it's placed *into* the bowl. Code Snippet:\n\n# Detect the bowl's position (re-detect to ensure fresh data, though it's unlikely to have moved) bowl_detection = robot.detect_object('bowl') if bowl_detection.position is None: raise ValueError(\"Bowl not detected.\") # Define the target Z-coordinate for dropping the banana into the bowl # Use the bowl's Z-position and add an offset to place the banana inside. # The banana's original Z was ~0.032, bowl's Z is ~0.007. # A target Z of 0.037 (0.007 + 0.03) should place it inside the bowl. drop_z = bowl_detection.position[2] + 0.03 Python Copied\n\nSorry, your browser doesn't support playback for this video\n\nResponse 2 with results of code executed: Reasoning (abbreviated) # Re-planning due to reachability constraints: The RIGHT arm cannot reach the bowl's current position. # The plan will now involve the LEFT arm moving the bowl to the RIGHT arm's reachable area. # The RIGHT arm can reach x-coordinates from 0.04m to 0.40m. I will move the bowl to an x-coordinate of 0.10m, which is well within the RIGHT arm's reach and also allows for some clearance from the center.\n\nCode Snippet:\n\ntarget_bowl_position = [-0.07274145, -0.08317622, 0.00720479] # Original bowl position # Move the bowl to an x-coordinate reachable by the RIGHT arm, keeping the same y and z (lifted) # Let's choose x = 0.10m for the bowl, which is within the right arm's reach (0.04 to 0.40) # Keep the lifted height at 0.20m move_bowl_position = [0.10, bowl_detection.position[1], 0.20] robot.move_gripper_to('LEFT', move_bowl_position, current_left_gripper_orientation) Python Copied\n\nSorry, your browser doesn't support playback for this video\n\nGemini 2.5 can also effectively utilize a small number of in-context examples to perform more dexterous robot control tasks. In the two examples shown below of an ALOHA robot packing boxes and folding a dress, 10 demonstrations consisting of interleaved reasoning and robot actions for each task were added to Gemini’s context. We’ve created open-source code showing how to do this using Gemini, including examples of the input demonstrations. This enables robots to be taught and deployed on the spot. See the Colab. Example 2: Gemini 2.5 (Flash) utilizes a small number of in-context examples to perform more dexterous robot control tasks. Prompt: see colab. Response with results of code executed:\n\nSorry, your browser doesn't support playback for this video\n\nSorry, your browser doesn't support playback for this video\n\nBuilding interactive robotics applications with the Live API The Live API for realtime streaming was recently introduced and can be used to build interactive applications that let people control robots using their voice. Intuitive human-robot-interaction is an important aspect of making robots that are easy and safe to use. We recently showcased an interactive Gemini Robotics demo at I/O 2025, which was built around Live API for voice interaction and function calling. Live API supports both audio and video as input modalities, and audio / text as output modalities. This allows you to send both voice input and the robot camera feed to the Live API. This is even more powerful when combined with tool use. Tool use allows Live API to go beyond just conversation by enabling it to perform actions in the real-world while maintaining a real time connection. For example, the robot APIs defined above can be defined as function calls including robot.open_gripper() , robot.close_gripper() and robot.move_gripper_to() . After they are defined as tool calls, they can be integrated into the workflow where people can interact with the robot using voice in real time. Developers can get started on GitHub, and refer to API documentation for function calling features.\n\nSorry, your browser doesn't support playback for this video 🔊 Demonstration of a realtime web console for robotics built with Live API, right click to open in a new tab for audio.\n\nSafety The 2.5 Pro and 2.5 Flash models demonstrate robust performance on the ASIMOV Multimodal and Physical Injury benchmarks released along with the Gemini Robotics tech report, exhibiting accuracy comparable to that of 2.0 models. Beyond the ASIMOV benchmarks, the 2.5 Pro and 2.5 Flash models also exhibit excellent performance in rejecting prompts that attempt to leverage embodied reasoning capabilities while violating safety policies such as promoting harmful stereotypes, discrimination, or endangerment of minors. Following rigorous evaluation against such synthetically generated adversarial prompts, 2.5 Pro and Flash demonstrated near-zero violation rates.\n\nHow Gemini is being used today for Robotics In March we released the Gemini Robotics-ER model and we’re already inspired by how the community is using it for robotics applications. Check out these examples of interactivity, perception, planning, and function calling from our trusted testers: Agile Robots, Agility Robotics, Boston Dynamics, and Enchanted Tools.\n\nSorry, your browser doesn't support playback for this video\n\nSorry, your browser doesn't support playback for this video\n\nSorry, your browser doesn't support playback for this video\n\nSorry, your browser doesn't support playback for this video", "label": "non_personal"}
{"title": "Introducing Configurable Metaflow", "url": "https://netflixtechblog.com/introducing-configurable-metaflow-d2fb8e9ba1c6?source=collection_home---4------7-----------------------", "content": "Introducing Configurable Metaflow Netflix Technology Blog 13 min read · Dec 20, 2024 -- 4 Listen Share\n\nDavid J. Berg*, David Casler^, Romain Cledat*, Qian Huang*, Rui Lin*, Nissan Pow*, Nurcan Sonmez*, Shashank Srikanth*, Chaoying Wang*, Regina Wang*, Darin Yu*\n\n*: Model Development Team, Machine Learning Platform\n\n^: Content Demand Modeling Team\n\nA month ago at QConSF, we showcased how Netflix utilizes Metaflow to power a diverse set of ML and AI use cases, managing thousands of unique Metaflow flows. This followed a previous blog on the same topic. Many of these projects are under constant development by dedicated teams with their own business goals and development best practices, such as the system that supports our content decision makers, or the system that ranks which language subtitles are most valuable for a specific piece of content.\n\nAs a central ML and AI platform team, our role is to empower our partner teams with tools that maximize their productivity and effectiveness, while adapting to their specific needs (not the other way around). This has been a guiding design principle with Metaflow since its inception.\n\nMetaflow infrastructure stack\n\nStanding on the shoulders of our extensive cloud infrastructure, Metaflow facilitates easy access to data, compute, and production-grade workflow orchestration, as well as built-in best practices for common concerns such as collaboration, versioning, dependency management, and observability, which teams use to setup ML/AI experiments and systems that work for them. As a result, Metaflow users at Netflix have been able to run millions of experiments over the past few years without wasting time on low-level concerns.\n\nA long standing FAQ: configurable flows\n\nWhile Metaflow aims to be un-opinionated about some of the upper levels of the stack, some teams within Netflix have developed their own opinionated tooling. As part of Metaflow’s adaptation to their specific needs, we constantly try to understand what has been developed and, more importantly, what gaps these solutions are filling.\n\nIn some cases, we determine that the gap being addressed is very team specific, or too opinionated at too high a level in the stack, and we therefore decide to not develop it within Metaflow. In other cases, however, we realize that we can develop an underlying construct that aids in filling that gap. Note that even in that case, we do not always aim to completely fill the gap and instead focus on extracting a more general lower level concept that can be leveraged by that particular user but also by others. One such recurring pattern we noticed at Netflix is the need to deploy sets of closely related flows, often as part of a larger pipeline involving table creations, ETLs, and deployment jobs. Frequently, practitioners want to experiment with variants of these flows, testing new data, new parameterizations, or new algorithms, while keeping the overall structure of the flow or flows intact.\n\nA natural solution is to make flows configurable using configuration files, so variants can be defined without changing the code. Thus far, there hasn’t been a built-in solution for configuring flows, so teams have built their bespoke solutions leveraging Metaflow’s JSON-typed Parameters, IncludeFile, and deploy-time Parameters or deploying their own home-grown solution (often with great pain). However, none of these solutions make it easy to configure all aspects of the flow’s behavior, decorators in particular.\n\nRequests for a feature like Metaflow Config\n\nOutside Netflix, we have seen similar frequently asked questions on the Metaflow community Slack as shown in the user quotes above:\n\nhow can I adjust the @resource requirements, such as CPU or memory, without having to hardcode the values in my flows?\n\nhow to adjust the triggering @schedule programmatically, so our production and staging deployments can run at different cadences?\n\nNew in Metaflow: Configs!\n\nToday, to answer the FAQ, we introduce a new — small but mighty — feature in Metaflow: a Config object. Configs complement the existing Metaflow constructs of artifacts and Parameters, by allowing you to configure all aspects of the flow, decorators in particular, prior to any run starting. At the end of the day, artifacts, Parameters and Configs are all stored as artifacts by Metaflow but they differ in when they are persisted as shown in the diagram below:\n\nDifferent data artifacts in Metaflow\n\nSaid another way:\n\nAn artifact is resolved and persisted to the datastore at the end of each task.\n\nis resolved and persisted to the datastore at the end of each task. A parameter is resolved and persisted at the start of a run; it can therefore be modified up to that point. One common use case is to use triggers to pass values to a run right before executing. Parameters can only be used within your step code.\n\nis resolved and persisted at the start of a run; it can therefore be modified up to that point. One common use case is to use triggers to pass values to a run right before executing. Parameters can only be used within your step code. A config is resolved and persisted when the flow is deployed. When using a scheduler such as Argo Workflows, deployment happens when create’ing the flow. In the case of a local run, “deployment” happens just prior to the execution of the run — think of “deployment” as gathering all that is needed to run the flow. Unlike parameters, configs can be used more widely in your flow code, particularly, they can be used in step or flow level decorators as well as to set defaults for parameters. Configs can of course also be used within your flow.\n\nAs an example, you can specify a Config that reads a pleasantly human-readable configuration file, formatted as TOML. The Config specifies a triggering ‘@schedule’ and ‘@resource’ requirements, as well as application-specific parameters for this specific deployment:\n\n[schedule]\n\ncron = \"0 * * * *\"\n\n\n\n[model]\n\noptimizer = \"adam\"\n\nlearning_rate = 0.5\n\n\n\n[resources]\n\ncpu = 1\n\nUsing the newly released Metaflow 2.13, you can configure a flow with a Config like above, as demonstrated by this flow:\n\nimport pprint\n\nfrom metaflow import FlowSpec, step, Config, resources, config_expr, schedule\n\n\n\n@schedule(cron=config_expr(\"config.schedule.cron\"))\n\nclass ConfigurableFlow(FlowSpec):\n\nconfig = Config(\"config\", default=\"myconfig.toml\", parser=\"tomllib.loads\")\n\n\n\n@resources(cpu=config.resources.cpu)\n\n@step\n\ndef start(self):\n\nprint(\"Config loaded:\")\n\npprint.pp(self.config)\n\nself.next(self.end)\n\n\n\n@step\n\ndef end(self):\n\npass\n\n\n\nif __name__ == \"__main__\":\n\nConfigurableFlow()\n\nThere is a lot going on in the code above, a few highlights:\n\nyou can refer to configs before they have been defined using ‘config_expr’.\n\nyou can define arbitrary parsers — using a string means the parser doesn’t even have to be present remotely!\n\nFrom the developer’s point of view, Configs behave like dictionary-like artifacts. For convenience, they support the dot-syntax (when possible) for accessing keys, making it easy to access values in a nested configuration. You can also unpack the whole Config (or a subtree of it) with Python’s standard dictionary unpacking syntax, ‘**config’. The standard dictionary subscript notation is also available.\n\nSince Configs turn into dictionary artifacts, they get versioned and persisted automatically as artifacts. You can access Configs of any past runs easily through the Client API. As a result, your data, models, code, Parameters, Configs, and execution environments are all stored as a consistent bundle — neatly organized in Metaflow namespaces — paving the way for easily reproducible, consistent, low-boilerplate, and now easily configurable experiments and robust production deployments.\n\nMore than a humble config file\n\nWhile you can get far by accompanying your flow with a simple config file (stored in your favorite format, thanks to user-definable parsers), Configs unlock a number of advanced use cases. Consider these examples from the updated documentation:\n\nA major benefit of Config over previous more hacky solutions for configuring flows is that they work seamlessly with other features of Metaflow: you can run steps remotely and deploy flows to production, even when relying on custom parsers, without having to worry about packaging Configs or parsers manually or keeping Configs consistent across tasks. Configs also work with the Runner and Deployer.\n\nThe Hollywood principle: don’t call us, we’ll call you\n\nWhen used in conjunction with a configuration manager like Hydra, Configs enable a pattern that is highly relevant for ML and AI use cases: orchestrating experiments over multiple configurations or sweeping over parameter spaces. While Metaflow has always supported sweeping over parameter grids easily using foreaches, it hasn’t been easily possible to alter the flow itself, e.g. to change @resources or @pypi/@conda dependencies for every experiment.\n\nIn a typical case, you trigger a Metaflow flow that consumes a configuration file, changing how a run behaves. With Hydra, you can invert the control: it is Hydra that decides what gets run based on a configuration file. Thanks to Metaflow’s new Runner and Deployer APIs, you can create a Hydra app that operates Metaflow programmatically — for instance, to deploy and execute hundreds of variants of a flow in a large-scale experiment.\n\nTake a look at two interesting examples of this pattern in the documentation. As a teaser, this video shows Hydra orchestrating deployment of tens of Metaflow flows, each of which benchmarks PyTorch using a varying number of CPU cores and tensor sizes, updating a visualization of the results in real-time as the experiment progresses:\n\nExample using Hydra with Metaflow\n\nMetaboosting Metaflow — based on a true story\n\nTo give a motivating example of what configurations look like at Netflix in practice, let’s consider Metaboost, an internal Netflix CLI tool that helps ML practitioners manage, develop and execute their cross-platform projects, somewhat similar to the open-source Hydra discussed above but with specific integrations to the Netflix ecosystem. Metaboost is an example of an opinionated framework developed by a team already using Metaflow. In fact, a part of the inspiration for introducing Configs in Metaflow came from this very use case.\n\nMetaboost serves as a single interface to three different internal platforms at Netflix that manage ETL/Workflows (Maestro), Machine Learning Pipelines (Metaflow) and Data Warehouse Tables (Kragle). In this context, having a single configuration system to manage a ML project holistically gives users increased project coherence and decreased project risk.\n\nConfiguration in Metaboost\n\nEase of configuration and templatizing are core values of Metaboost. Templatizing in Metaboost is achieved through the concept of bindings, wherein we can bind a Metaflow pipeline to an arbitrary label, and then create a corresponding bespoke configuration for that label. The binding-connected configuration is then merged into a global set of configurations containing such information as GIT repository, branch, etc. Binding a Metaflow, will also signal to Metaboost that it should instantiate the Metaflow flow once per binding into our orchestration cluster.\n\nImagine a ML practitioner on the Netflix Content ML team, sourcing features from hundreds of columns in our data warehouse, and creating a multitude of models against a growing suite of metrics. When a brand new content metric comes along, with Metaboost, the first version of the metric’s predictive model can easily be created by simply swapping the target column against which the model is trained.\n\nSubsequent versions of the model will result from experimenting with hyper parameters, tweaking feature engineering, or conducting feature diets. Metaboost’s bindings, and their integration with Metaflow Configs, can be leveraged to scale the number of experiments as fast as a scientist can create experiment based configurations.\n\nScaling experiments with Metaboost bindings — backed by Metaflow Config\n\nConsider a Metaboost ML project named `demo` that creates and loads data to custom tables (ETL managed by Maestro), and then trains a simple model on this data (ML Pipeline managed by Metaflow). The project structure of this repository might look like the following:\n\n├── metaflows\n\n│ ├── custom -> custom python code, used by\n\n| | | Metaflow\n\n│ │ ├── data.py\n\n│ │ └── model.py\n\n│ └── training.py -> defines our Metaflow pipeline\n\n├── schemas\n\n│ ├── demo_features_f.tbl.yaml -> table DDL, stores our ETL\n\n| | output, Metaflow input\n\n│ └── demo_predictions_f.tbl.yaml -> table DDL,\n\n| stores our Metaflow output\n\n├── settings\n\n│ ├── settings.configuration.EXP_01.yaml -> defines the additive\n\n| | config for Experiment 1\n\n│ ├── settings.configuration.EXP_02.yaml -> defines the additive\n\n| | config for Experiment 2\n\n│ ├── settings.configuration.yaml -> defines our global\n\n| | configuration\n\n│ └── settings.environment.yaml -> defines parameters based on\n\n| git branch (e.g. READ_DB)\n\n├── tests\n\n├── workflows\n\n│ ├── sql\n\n│ ├── demo.demo_features_f.sch.yaml -> Maestro workflow, defines ETL\n\n│ └── demo.main.sch.yaml -> Maestro workflow, orchestrates\n\n| ETLs and Metaflow\n\n└── metaboost.yaml -> defines our project for\n\nMetaboost\n\nThe configuration files in the settings directory above contain the following YAML files:\n\n# settings.configuration.yaml (global configuration)\n\nmodel:\n\nfit_intercept: True\n\nconda:\n\nnumpy: '1.22.4'\n\n\"scikit-learn\": '1.4.0'\n\n# settings.configuration.EXP_01.yaml\n\ntarget_column: metricA\n\nfeatures:\n\n- runtime\n\n- content_type\n\n- top_billed_talent\n\n# settings.configuration.EXP_02.yaml\n\ntarget_column: metricA\n\nfeatures:\n\n- runtime\n\n- director\n\n- box_office\n\nMetaboost will merge each experiment configuration (*.EXP*.yaml) into the global configuration (settings.configuration.yaml) individually at Metaboost command initialization. Let’s take a look at how Metaboost combines these configurations with a Metaboost command:\n\n(venv-demo) ~/projects/metaboost-demo [branch=demoX]\n\n$ metaboost metaflow settings show --yaml-path=configuration\n\n\n\nbinding=EXP_01:\n\nmodel: -> defined in setting.configuration.yaml (global)\n\nfit_intercept: true\n\nconda: -> defined in setting.configuration.yaml (global)\n\nnumpy: 1.22.4\n\n\"scikit-learn\": 1.4.0\n\ntarget_column: metricA -> defined in setting.configuration.EXP_01.yaml\n\nfeatures: -> defined in setting.configuration.EXP_01.yaml\n\n- runtime\n\n- content_type\n\n- top_billed_talent\n\n\n\nbinding=EXP_02:\n\nmodel: -> defined in setting.configuration.yaml (global)\n\nfit_intercept: true\n\nconda: -> defined in setting.configuration.yaml (global)\n\nnumpy: 1.22.4\n\n\"scikit-learn\": 1.4.0\n\ntarget_column: metricA -> defined in setting.configuration.EXP_02.yaml\n\nfeatures: -> defined in setting.configuration.EXP_02.yaml\n\n- runtime\n\n- director\n\n- box_office\n\nMetaboost understands it should deploy/run two independent instances of training.py — one for the EXP_01 binding and one for the EXP_02 binding. You can also see that Metaboost is aware that the tables and ETL workflows are not bound, and should only be deployed once. These details of which artifacts to bind and which to leave unbound are encoded in the project’s top-level metaboost.yaml file.\n\n(venv-demo) ~/projects/metaboost-demo [branch=demoX]\n\n$ metaboost project list\n\n\n\nTables (metaboost table list):\n\nschemas/demo_predictions_f.tbl.yaml (binding=default):\n\ntable_path=prodhive/demo_db/demo_predictions_f\n\nschemas/demo_features_f.tbl.yaml (binding=default):\n\ntable_path=prodhive/demo_db/demo_features_f\n\n\n\nWorkflows (metaboost workflow list):\n\nworkflows/demo.demo_features_f.sch.yaml (binding=default):\n\ncluster=sandbox, workflow.id=demo.branch_demox.demo_features_f\n\nworkflows/demo.main.sch.yaml (binding=default):\n\ncluster=sandbox, workflow.id=demo.branch_demox.main\n\n\n\nMetaflows (metaboost metaflow list):\n\nmetaflows/training.py (binding=EXP_01): -> EXP_01 instance of training.py\n\ncluster=sandbox, workflow.id=demo.branch_demox.EXP_01.training\n\nmetaflows/training.py (binding=EXP_02): -> EXP_02 instance of training.py\n\ncluster=sandbox, workflow.id=demo.branch_demox.EXP_02.training\n\nBelow is a simple Metaflow pipeline that fetches data, executes feature engineering, and trains a LinearRegression model. The work to integrate Metaboost Settings into a user’s Metaflow pipeline (implemented using Metaflow Configs) is as easy as adding a single mix-in to the FlowSpec definition:\n\nfrom metaflow import FlowSpec, Parameter, conda_base, step\n\nfrom custom.data import feature_engineer, get_data\n\nfrom metaflow.metaboost import MetaboostSettings\n\n\n\n@conda_base(\n\nlibraries=MetaboostSettings.get_deploy_time_settings(\"configuration.conda\")\n\n)\n\nclass DemoTraining(FlowSpec, MetaboostSettings):\n\nprediction_date = Parameter(\"prediction_date\", type=int, default=-1)\n\n\n\n@step\n\ndef start(self):\n\n# get show_settings() for free with the mixin\n\n# and get convenient debugging info\n\nself.show_settings(exclude_patterns=[\"artifact*\", \"system*\"])\n\n\n\nself.next(self.get_features)\n\n\n\n@step\n\ndef get_features(self):\n\n# feature engineers on our extracted data\n\nself.fe_df = feature_engineer(\n\n# loads data from our ETL pipeline\n\ndata=get_data(prediction_date=self.prediction_date),\n\nfeatures=self.settings.configuration.features +\n\n[self.settings.configuration.target_column]\n\n)\n\n\n\nself.next(self.train)\n\n\n\n@step\n\ndef train(self):\n\nfrom sklearn.linear_model import LinearRegression\n\n\n\n# trains our model\n\nself.model = LinearRegression(\n\nfit_intercept=self.settings.configuration.model.fit_intercept\n\n).fit(\n\nX=self.fe_df[self.settings.configuration.features],\n\ny=self.fe_df[self.settings.configuration.target_column]\n\n)\n\nprint(f\"Fit slope: {self.model.coef_[0]}\")\n\nprint(f\"Fit intercept: {self.model.intercept_}\")\n\n\n\nself.next(self.end)\n\n\n\n@step\n\ndef end(self):\n\npass\n\n\n\n\n\nif __name__ == \"__main__\":\n\nDemoTraining()\n\nThe Metaflow Config is added to the FlowSpec by mixing in the MetaboostSettings class. Referencing a configuration value is as easy as using the dot syntax to drill into whichever parameter you’d like.\n\nFinally let’s take a look at the output from our sample Metaflow above. We execute experiment EXP_01 with\n\nmetaboost metaflow run --binding=EXP_01\n\nwhich upon execution will merge the configurations into a single settings file (shown previously) and serialize it as a yaml file to the .metaboost/settings/compiled/ directory.\n\nYou can see the actual command and args that were sub-processed in the Metaboost Execution section below. Please note the –config argument pointing to the serialized yaml file, and then subsequently accessible via self.settings. Also note the convenient printing of configuration values to stdout during the start step using a mixed in function named show_settings().\n\n(venv-demo) ~/projects/metaboost-demo [branch=demoX]\n\n$ metaboost metaflow run --binding=EXP_01\n\n\n\nMetaboost Execution:\n\n- python3.10 /root/repos/cdm-metaboost-irl/metaflows/training.py\n\n--no-pylint --package-suffixes=.py --environment=conda\n\n--config settings\n\n.metaboost/settings/compiled/settings.branch_demox.EXP_01.training.mP4eIStG.yaml\n\nrun --prediction_date20241006\n\n\n\nMetaflow 2.12.39+nflxfastdata(2.13.5);nflx(2.13.5);metaboost(0.0.27)\n\nexecuting DemoTraining for user:dcasler\n\nValidating your flow...\n\nThe graph looks good!\n\nBootstrapping Conda environment... (this could take a few minutes)\n\nAll packages already cached in s3.\n\nAll environments already cached in s3.\n\n\n\nWorkflow starting (run-id 50), see it in the UI at\n\nhttps://metaflowui.prod.netflix.net/DemoTraining/50\n\n\n\n[50/start/251640833] Task is starting.\n\n[50/start/251640833] Configuration Values:\n\n[50/start/251640833] settings.configuration.conda.numpy = 1.22.4\n\n[50/start/251640833] settings.configuration.features.0 = runtime\n\n[50/start/251640833] settings.configuration.features.1 = content_type\n\n[50/start/251640833] settings.configuration.features.2 = top_billed_talent\n\n[50/start/251640833] settings.configuration.model.fit_intercept = True\n\n[50/start/251640833] settings.configuration.target_column = metricA\n\n[50/start/251640833] settings.environment.READ_DATABASE = data_warehouse_prod\n\n[50/start/251640833] settings.environment.TARGET_DATABASE = demo_dev\n\n[50/start/251640833] Task finished successfully.\n\n\n\n[50/get_features/251640840] Task is starting.\n\n[50/get_features/251640840] Task finished successfully.\n\n\n\n[50/train/251640854] Task is starting.\n\n[50/train/251640854] Fit slope: 0.4702672504331096\n\n[50/train/251640854] Fit intercept: -6.247919678070083\n\n[50/train/251640854] Task finished successfully.\n\n\n\n[50/end/251640868] Task is starting.\n\n[50/end/251640868] Task finished successfully.\n\n\n\nDone! See the run in the UI at\n\nhttps://metaflowui.prod.netflix.net/DemoTraining/50\n\nTakeaways\n\nMetaboost is an integration tool that aims to ease the project development, management and execution burden of ML projects at Netflix. It employs a configuration system that combines git based parameters, global configurations and arbitrarily bound configuration files for use during execution against internal Netflix platforms.\n\nIntegrating this configuration system with the new Config in Metaflow is incredibly simple (by design), only requiring users to add a mix-in class to their FlowSpec — similar to this example in Metaflow documentation — and then reference the configuration values in steps or decorators. The example above templatizes a training Metaflow for the sake of experimentation, but users could just as easily use bindings/configs to templatize their flows across target metrics, business initiatives or any other arbitrary lines of work.\n\nTry it at home\n\nIt couldn’t be easier to get started with Configs! Just\n\npip install -U metaflow\n\nto get the latest version and head to the updated documentation for examples. If you are impatient, you can find and execute all config-related examples in this repository as well.\n\nIf you have any questions or feedback about Config (or other Metaflow features), you can reach out to us at the Metaflow community Slack.\n\nAcknowledgments\n\nWe would like to thank Outerbounds for their collaboration on this feature; for rigorously testing it and developing a repository of examples to showcase some of the possibilities offered by this feature.", "label": "non_personal"}
{"title": "Part 2: A Survey of Analytics Engineering Work at Netflix", "url": "https://netflixtechblog.com/part-2-a-survey-of-analytics-engineering-work-at-netflix-4f1f53b4ab0f?source=collection_home---4------6-----------------------", "content": "Part 2: A Survey of Analytics Engineering Work at Netflix Netflix Technology Blog 9 min read · Jan 2, 2025 -- 9 Listen Share\n\nThis article is the second in a multi-part series sharing a breadth of Analytics Engineering work at Netflix, recently presented as part of our annual internal Analytics Engineering conference. Need to catch up? Check out Part 1. In this article, we highlight a few exciting analytic business applications, and in our final article we’ll go into aspects of the technical craft.\n\nGame Analytics\n\nYimeng Tang, Claire Willeck, Sagar Palao\n\nUser Acquisition Incrementality for Netflix Games\n\nNetflix has been launching games for the past three years, during which it has initiated various marketing efforts, including User Acquisition (UA) campaigns, to promote these games across different countries. These UA campaigns typically feature static creatives, launch trailers, and game review videos on platforms like Google, Meta, and TikTok. The primary goals of these campaigns are to encourage more people to install and play the games, making incremental installs and engagement crucial metrics for evaluating their effectiveness.\n\nMost UA campaigns are conducted at the country level, meaning that everyone in the targeted countries can see the ads. However, due to the absence of a control group in these countries, we adopt a synthetic control framework (blog post) to estimate the counterfactual scenario. This involves creating a weighted combination of countries not exposed to the UA campaign to serve as a counterfactual for the treated countries. To facilitate easier access to incrementality results, we have developed an interactive tool powered by this framework. This tool allows users to directly obtain the lift in game installs and engagement, view plots for both the treated country and the synthetic control unit, and assess the p-value from placebo tests.\n\nTo better guide the design and budgeting of future campaigns, we are developing an Incremental Return on Investment model. This model incorporates factors such as the incremental impact, the value of the incremental engagement and incremental signups, and the cost of running the campaign. In addition to using the causal inference framework mentioned earlier to estimate incrementality, we also leverage other frameworks, such as Incremental Account Lifetime Valuation (blog post), to assign value to the incremental engagement and signups resulting from the campaigns.\n\nMeasuring and Validating Incremental Signups for Netflix Games\n\nNetflix is a subscription service meaning members buy subscriptions which include games but not the individual games themselves. This makes it difficult to measure the impact of different game launches on acquisition. We only observe signups, not why members signed up.\n\nThis means we need to estimate incremental signups. We adopt an approach developed at Netflix to estimate incremental acquisition (technical paper). This approach uses simple assumptions to estimate a counterfactual for the rate that new members start playing the game.\n\nBecause games differ from series/films, it’s crucial to validate this estimation method for games. Ideally, we would have causal estimates from an A/B test to use for validation, but since that is not available, we use another causal inference design as one of our ensemble of validation approaches. This causal inference design involves a systematic framework we designed to measure game events that relies on synthetic control (blog post).\n\nAs we mentioned above, we have been launching User Acquisition (UA) campaigns in select countries to boost game engagement and new memberships. We can use this cross-country variation to form a synthetic control and measure the incremental signups due to the UA campaign. The incremental signups from UA campaigns differ from those attributed to a game, but they should be similar. When our estimated incremental acquisition numbers over a campaign period are similar to the incremental acquisition numbers calculated using synthetic control, we feel more confident in our approach to measuring incremental signups for games.\n\nNetflix Games Players’ Adventure: Modeled using State Machine\n\nAt Netflix Games, we aim to have a high number of members engaging with games each month, referred to as Monthly Active Accounts (MAA). To evaluate our progress toward this objective and to find areas to boost our MAA, we modeled the Netflix players’ journey as a state machine.\n\nWe track a daily state machine showing the probability of account transitions between states.\n\nNetflix Players’ Journey as State machine\n\nModeling the players’ journey as a state machine allows us to simulate future states and assess progress toward engagement goals. The most basic operation involves multiplying the daily state-transition matrix with the current state values to determine the next day’s state values.\n\nThis basic operation allows us to explore various scenarios:\n\nConstant Trends: If transition rates stay constant, we can predict future states by repeatedly multiplying the daily state-transition matrix to new state values, helping us assess progress towards annual goals under unchanged conditions.\n\nDynamic Scenarios: By modifying transition rates, we can simulate complex scenarios. For instance, mimicking past changes in transition rates from a game launch allows us to predict the impact of similar future launches by altering the transition rate for a specific period.\n\nSteady State: We can calculate the steady state of the state-transition matrix (excluding new players) to estimate the MAA once all accounts have tried Netflix games and understand long-term retention and reactivation effects.\n\nBeyond predicting future states, we use the state machine for sensitivity analysis to find which transition rates most impact MAA. By making small changes to each transition rate we calculate the resulting MAA and measure its impact. This guides us in prioritizing efforts on top-of-funnel improvements, member retention, or reactivation.\n\nContent Cash Modeling\n\nAlex Diamond\n\nAt Netflix we produce a variety of entertainment: movies, series, documentaries, stand-up specials, and more. Each format has a different production process and different patterns of cash spend, called our “Content Forecast”. Looking into the future, Netflix keeps a plan of how many titles we intend to produce, what kinds, and when. Because we don’t yet know what specific titles that content will eventually become, these generic placeholders are called “TBD Slots.” A sizable portion of our Content Forecast is represented by TBD Slots.\n\nAlmost all businesses have a cash forecasting process informing how much cash they need in a given time period to continue executing on their plans. As plans change, the cash forecast will change. Netflix has a cash forecast that projects our cash needs to produce the titles we plan to make. This presents the question: how can we optimally forecast cash needs for TBD Slots, given we don’t have details on what real titles they will become?\n\nThe large majority of our titles are funded throughout the production process — starting from when we begin developing the title to shooting the actual shows and movies to launch on our Netflix service.\n\nSince cash spend is driven by what is happening on a production, we model it by breaking down into these three steps:\n\nDetermine estimated production phase durations using historical actuals Determine estimated percent of cash spent in each production phase Model the shape of cash spend within each phase\n\nPutting these three pieces together allows us to generate a generic estimation of cash spend per day leading up to and beyond a title’s launch date (a proxy for “completion”). We could distribute this spend linearly across each phase, but this approach allows us to capture nuance around patterns of spend that ramp up slowly, or are concentrated at the start and taper off throughout.\n\nBefore starting any math, we need to ensure a high quality historical dataset. Data quality plays a huge role in this work. For example, if we see 80% of our cash spent before production even started, it might be safe to say that either the production dates (which are manually captured) are incorrect or that title had a unique spending pattern that we don’t want to anticipate our future titles will follow.\n\nFor the first two steps, finding the estimated phase durations and cash percent per phase, we’ve found that simple math works best, for interpretability and consistency. We use a weighted average across our “clean” historical actuals to produce these estimated assumptions.\n\nFor modeling the shape of spend throughout each phase, we perform constrained optimization to fit a 3rd degree polynomial function. The constraints include:\n\nMust pass through the points (0,0) and (1,1). This ensures that 0% through the phase, 0% of that phase’s cash has been spent. Similarly, 100% through the phase, 100% of that phase’s cash has been spent. The derivative must be non-negative. This ensures that the function is monotonically increasing, avoiding counterintuitively forecasting any negative spend.\n\nThe optimization’s objective function minimizes the sum of squared residuals and returns the coefficients of the polynomial that will guide the shape of cash spend through each phase.\n\nOnce we have these coefficients, we can evaluate this polynomial at each day of the expected phase duration, and then multiply the result by the expected cash per phase. With some additional data processing, this yields an expected percent of cash spend each day leading up to and beyond the launch date, which we can base our forecasts on.\n\nAssistive Speech Recognition in Dubbing Workflows at Netflix\n\nTanguy Cornau\n\nGreat stories can come from anywhere and be loved everywhere. At Netflix, we strive to make our titles accessible to a global audience, transcending language barriers to connect with viewers worldwide. One of the key ways we achieve this is through creating dubs in many languages.\n\nFrom the transcription of the original titles all the way to the delivery of the dub audio, we blend innovation with human expertise to preserve the original creative intent.\n\nLeveraging technologies like Assistive Speech Recognition (ASR), we seek to make the transcription part of the process more efficient for our linguists. Transcription, in our context, involves creating a verbatim script of the spoken dialogue, along with precise timing information to perfectly align the text with the original video. With ASR, instead of starting the transcription from scratch, linguists get a pre-generated starting point which they can use and edit for complete accuracy.\n\nThis efficiency enables linguists to focus more on other creative tasks, such as adding cultural annotations and references, which are crucial for downstream dubbing.\n\nWith ASR, and other new and enhanced technologies we introduce, rigorous analytics and measurement are essential to their success. To effectively evaluate our ASR system, we’ve established a multi-layered measurement framework that provides comprehensive insights into its performance across many dimensions (for example, the accuracy of the text and timing predictions), offline and online.\n\nASR is expected to perform differently for various languages; therefore, at a high level, we track metrics by original language of the show, allowing us to assess overall ASR effectiveness and identify trends across different linguistic contexts. We further break down performance by various dimensions, e.g. content type, genre, etc… to help us pinpoint specific areas where the ASR system may encounter difficulties. Furthermore, our framework allows us to conduct in-depth analyses of individual titles’ transcription, focusing on critical quality dimensions around text and timing accuracy of ASR suggestions. By zooming in on where the system falls short, we gain valuable insights into specific challenges, enabling us to further refine our understanding of ASR performance.\n\nThese measurement layers collectively empower us to continuously monitor, identify improvement areas, and implement targeted enhancements, ensuring that our ASR technology gets more and more accurate, effective, and helpful to linguists across diverse content types and languages. By refining our dubbing workflows through these innovations, we aim to keep improving the quality of our dubs to help great stories travel across the globe and bring joy to our members.", "label": "non_personal"}
{"title": "Simulating a neural operating system with Gemini 2.5 Flash-Lite", "url": "https://developers.googleblog.com/en/simulating-a-neural-operating-system-with-gemini-2-5-flash-lite/", "content": "In traditional computing, user interfaces are pre-defined. Every button, menu, and window is meticulously coded by developers. But what if an interface could be generated in real time, adapting to a user's context with each interaction? We explored this question by building a research prototype (view demo app in Google AI Studio) for a generative, infinite computer experience. Our prototype simulates an operating system where each screen is generated on the fly by a large language model. It uses Gemini 2.5 Flash-Lite, a model whose low latency is critical for creating a responsive interaction that feels instantaneous. Instead of navigating a static file system, the user interacts with an environment that the model builds and rebuilds with every click. This post outlines the core technical concepts behind this prototype.\n\nSorry, your browser doesn't support playback for this video\n\nConditioning the model for on-the-fly UI generation To generate a UI on-the-fly, we need to provide the model with a clear structure and context for each request. We engineered our prompt by dividing the model's input into two parts: a \"UI constitution\" and a \"UI interaction\". The UI constitution is a system prompt that contains a fixed set of rules for UI generation. These rules define consistent elements like the OS-level styling, the home screen format, and logic for embedding elements like maps. The UI interaction is a JSON object that captures the user's most recent action, such as a mouse click on an icon. This object serves as the specific query that prompts the model to generate the next screen. For example, clicking on a “Save Note” icon within the Notepad app may generate an object as the following:\n\n{ // `id`: The unique ID from the button's `data-interaction-id` attribute. id: 'save_note_action', // `type`: The interaction type from `data-interaction-type`. type: 'button_press', // `value`: Because the button has a `data-value-from` attribute, the system // retrieves the content from the textarea with the ID 'notepad_main_textarea'. value: 'Meeting notes\n\n- Discuss Q3 roadmap\n\n- Finalize budget', // `elementType`: The HTML tag of the element that was clicked. elementType: 'button', // `elementText`: The visible text inside the button. elementText: 'Save Note', // `appContext`: The ID of the application the user is currently in. // This comes from the `activeApp` state in `App.tsx`. appContext: 'notepad_app' } JSON Copied", "label": "non_personal"}
{"title": "Part 3: A Survey of Analytics Engineering Work at Netflix", "url": "https://netflixtechblog.com/part-3-a-survey-of-analytics-engineering-work-at-netflix-e67f0aa82183?source=collection_home---4------5-----------------------", "content": "Part 3: A Survey of Analytics Engineering Work at Netflix Netflix Technology Blog 9 min read · Jan 6, 2025 -- 3 Listen Share\n\nThis article is the last in a multi-part series sharing a breadth of Analytics Engineering work at Netflix, recently presented as part of our annual internal Analytics Engineering conference. Need to catch up? Check out Part 1, which detailed how we’re empowering Netflix to efficiently produce and effectively deliver high quality, actionable analytic insights across the company and Part 2, which stepped through a few exciting business applications for Analytics Engineering. This post will go into aspects of technical craft.\n\nDashboard Design Tips\n\nRina Chang, Susie Lu\n\nWhat is design, and why does it matter? Often people think design is about how things look, but design is actually about how things work. Everything is designed, because we’re all making choices about how things work, but not everything is designed well. Good design doesn’t waste time or mental energy; instead, it helps the user achieve their goals.\n\nWhen applying this to a dashboard application, the easiest way to use design effectively is to leverage existing patterns. (For example, people have learned that blue underlined text on a website means it’s a clickable link.) So knowing the arsenal of available patterns and what they imply is useful when making the choice of when to use which pattern.\n\nFirst, to design a dashboard well, you need to understand your user.\n\nTalk to your users throughout the entire product lifecycle. Talk to them early and often, through whatever means you can.\n\nUnderstand their needs, ask why, then ask why again. Separate symptoms from problems from solutions.\n\nPrioritize and clarify — less is more! Distill what you can build that’s differentiated and provides the most value to your user.\n\nHere is a framework for thinking about what your users are trying to achieve. Where do your users fall on these axes? Don’t solve for multiple positions across these axes in a given view; if that exists, then create different views or potentially different dashboards.\n\nSecond, understanding your users’ mental models will allow you to choose how to structure your app to match. A few questions to ask yourself when considering the information architecture of your app include:\n\nDo you have different user groups trying to accomplish different things? Split them into different apps or different views.\n\nWhat should go together on a single page? All the information needed for a single user type to accomplish their “job.” If there are multiple jobs to be done, split each out onto its own page.\n\nWhat should go together within a single section on a page? All the information needed to answer a single question.\n\nDoes your dashboard feel too difficult to use? You probably have too much information! When in doubt, keep it simple. If needed, hide complexity under an “Advanced” section.\n\nHere are some general guidelines for page layouts:\n\nChoose infinite scrolling vs. clicking through multiple pages depending on which option suits your users’ expectations better\n\nLead with the most-used information first, above the fold\n\nCreate signposts that cue the user to where they are by labeling pages, sections, and links\n\nUse cards or borders to visually group related items together\n\nLeverage nesting to create well-understood “scopes of control.” Specifically, users expect a controller object to affect children either: Below it (if horizontal) or To the right of it (if vertical)\n\nThird, some tips and tricks can help you more easily tackle the unique design challenges that come with making interactive charts.\n\nTitles: Make sure filters are represented in the title or subtitle of the chart for easy scannability and screenshot-ability.\n\nTooltips: Core details should be on the page, while the context in the tooltip is for deeper information. Annotate multiple points when there are only a handful of lines.\n\nAnnotations: Provide annotations on charts to explain shifts in values so all users can access that context.\n\nColor: Limit the number of colors you use. Be consistent in how you use colors. Otherwise, colors lose meaning.\n\nOnboarding: Separate out onboarding to your dashboard from routine usage.\n\nFinally, it is important to note that these are general guidelines, but there is always room for interpretation and/or the use of good judgment to adapt them to suit your own product and use cases. At the end of the day, the most important thing is that a user can leverage the data insights provided by your dashboard to perform their work, and good design is a means to that end.\n\nLearnings from Deploying an Analytics API at Netflix\n\nDevin Carullo\n\nAt Netflix Studio, we operate at the intersection of art and science. Data is a tool that enhances decision-making, complementing the deep expertise and industry knowledge of our creative professionals.\n\nOne example is in production budgeting — namely, determining how much we should spend to produce a given show or movie. Although there was already a process for creating and comparing budgets for new productions against similar past projects, it was highly manual. We developed a tool that automatically selects and compares similar Netflix productions, flagging any anomalies for Production Finance to review.\n\nTo ensure success, it was essential that results be delivered in real-time and integrated seamlessly into existing tools. This required close collaboration among product teams, DSE, and front-end and back-end developers. We developed a GraphQL endpoint using Metaflow, integrating it into the existing budgeting product. This solution enabled data to be used more effectively for real-time decision-making.\n\nWe recently launched our MVP and continue to iterate on the product. Reflecting on our journey, the path to launch was complex and filled with unexpected challenges. As an analytics engineer accustomed to crafting quick solutions, I underestimated the effort required to deploy a production-grade analytics API.\n\nFig 1. My vague idea of how my API would work\n\nFig 2: Our actual solution\n\nWith hindsight, below are my key learnings.\n\nMeasure Impact and Necessity of Real-Time Results\n\nBefore implementing real-time analytics, assess whether real-time results are truly necessary for your use case. This can significantly impact the complexity and cost of your solution. Batch processing data may provide a similar impact and take significantly less time. It’s easier to develop and maintain, and tends to be more familiar for analytics engineers, data scientists, and data engineers.\n\nAdditionally, if you are developing a proof of concept, the upfront investment may not be worth it. Scrappy solutions can often be the best choice for analytics work.\n\nExplore All Available Solutions\n\nAt Netflix, there were multiple established methods for creating an API, but none perfectly suited our specific use case. Metaflow, a tool developed at Netflix for data science projects, already supported REST APIs. However, this approach did not align with the preferred workflow of our engineering partners. Although they could integrate with REST endpoints, this solution presented inherent limitations. Large response sizes rendered the API/front-end integration unreliable, necessitating the addition of filter parameters to reduce the response size.\n\nAdditionally, the product we were integrating into was using GraphQL, and deviating from this established engineering approach was not ideal. Lastly, given our goal to overlay results throughout the product, GraphQL features, such as federation, proved to be particularly advantageous.\n\nAfter realizing there wasn’t an existing solution at Netflix for deploying python endpoints with GraphQL, we worked with the Metaflow team to build this feature. This allowed us to continue developing via Metaflow and allowed our engineering partners to stay on their paved path.\n\nAlign on Performance Expectations\n\nA major challenge during development was managing API latency. Much of this could have been mitigated by aligning on performance expectations from the outset. Initially, we operated under our assumptions of what constituted an acceptable response time, which differed greatly from the actual needs of our users and our engineering partners.\n\nUnderstanding user expectations is key to designing an effective solution. Our methodology resulted in a full budget analysis taking, on average, 7 seconds. Users were willing to wait for an analysis when they modified a budget, but not every time they accessed one. To address this, we implemented caching using Metaflow, reducing the API response time to approximately 1 second for cached results. Additionally, we set up a nightly batch job to pre-cache results.\n\nWhile users were generally okay with waiting for analysis during changes, we had to be mindful of GraphQL’s 30-second limit. This highlighted the importance of continuously monitoring the impact of changes on response times, leading us to our next key learning: rigorous testing.\n\nReal-Time Analysis Requires Rigorous Testing\n\nLoad Testing: We leveraged Locust to measure the response time of our endpoint and assess how the endpoint responded to reasonable and elevated loads. We were able to use FullStory, which was already being used in the product, to estimate expected calls per minute.\n\nFig 3. Locust allows us to simulate concurrent calls and measure response time\n\nUnit Tests & Integration Tests: Code testing is always a good idea, but it can often be overlooked in analytics. It is especially important when you are delivering live analysis to circumvent end users from being the first to see an error or incorrect information. We implemented unit testing and full integration tests, ensuring that our analysis would return correct results.\n\nThe Importance of Aligning Workflows and Collaboration\n\nThis project marked the first time our team collaborated directly with our engineering partners to integrate a DSE API into their product. Throughout the process, we discovered significant gaps in our understanding of each other’s workflows. Assumptions about each other’s knowledge and processes led to misunderstandings and delays.\n\nDeployment Paths: Our engineering partners followed a strict deployment path, whereas our approach on the DSE side was more flexible. We typically tested our work on feature branches using Metaflow projects and then pushed results to production. However, this lack of control led to issues, such as inadvertently deploying changes to production before the corresponding product updates were ready and difficulties in managing a test endpoint. Ultimately, we deferred to our engineering partners to establish a deployment path and collaborated with the Metaflow team and data engineers to implement it effectively.\n\nFig 4. Our current deployment path\n\nWork Planning: While the engineering team operated on sprints, our DSE team planned by quarters. This misalignment in planning cycles is an ongoing challenge that we are actively working to resolve.\n\nLooking ahead, our team is committed to continuing this partnership with our engineering colleagues. Both teams have invested significant time in building this relationship, and we are optimistic that it will yield substantial benefits in future projects.\n\nExternal Speaker: Benn Stancil\n\nIn addition to the above presentations, we kicked off our Analytics Summit with a keynote talk from Benn Stancil, Founder of Mode Analytics. Benn stepped through a history of the modern data stack, and the group discussed ideas on the future of analytics.", "label": "non_personal"}
{"title": "Unlock deeper insights with the new Python client library for Data Commons", "url": "https://developers.googleblog.com/en/pythondatacommons/", "content": "Data is the bedrock of progress across nearly every field. It serves as the raw material from which profound insights are forged, enabling us to precisely measure current realities, identify critical trends, and possibly predict future outcomes.\n\nAt Google, our mission with Data Commons is to organize the world's publicly available statistical data, making it more accessible and useful for everyone. It's an open-source knowledge graph that unifies a vast array of public data from diverse sources, simplifying access and comprehension for developers, researchers, and data analysts alike. Along with the datacommons.org website, Google Search uses Data Commons to answer queries like What is the population of San Francisco?, with the top graph generated by Data Commons.\n\nToday, we're announcing the general availability of the new Python client library for the Data Commons based on the V2 REST API. This new Python library dramatically enhances how data developers can leverage Data Commons.\n\n\n\nReal-world impact: partnering with ONE.org\n\nThis milestone was significantly shaped by the vision and substantial contributions of our partner The ONE Campaign, a global organization working to create the investments needed for economic opportunities and healthier lives in Africa. We built Data Commons as an open-source platform precisely to encourage community contributions and enable innovative uses, and this partnership with The ONE Campaign perfectly exemplifies that goal. ONE advocated for, proposed the design and coded the client library to make Data Commons' rich insights available to data scientists and analysts who want to leverage the rich ecosystem of Python analytical tools and libraries.\n\n\n\nSupport for custom Data Commons instances\n\nThe Data Commons platform also allows organizations, like the United Nations or ONE, to host their own Data Commons instances. These custom instances enable the seamless integration of proprietary datasets with the foundational Data Commons knowledge graph. Organizations leverage the Data Commons data framework and tools while maintaining full control over their data and resources.\n\nOne of the most impactful additions in the V2 library is robust support for custom instances. This means you can now use the Python library to programmatically query any public or private instance—whether hosted locally, within your organization or on the Google Cloud Platform.\n\n\n\nPowerful new features\n\nThe Python library makes it very easy to perform common queries against Data Commons data, such as:\n\nExploring the structure of the knowledge graph\n\nRetrieving data for any of the 200,000+ statistical variables from over 200 datasets in domains such as demographics, economy, education, energy, environment, health, and housing\n\nEasily mapping entities from other datasets to entities in Data Commons\n\n\n\nV2 of the client library offers many technical improvements over the V1 library, including:\n\nPandas dataframe APIs are supported as an integral module, with a single installation package, allowing seamless use with other API endpoints in the same client\n\nSeveral new convenience methods for common data queries\n\nAPI key management and other stateful operations built in to the client class\n\nIntegration with the Pydantic libraries for improved type safety, validation and serialization", "label": "non_personal"}
{"title": "Title Launch Observability at Netflix Scale", "url": "https://netflixtechblog.com/title-launch-observability-at-netflix-scale-19ea916be1ed?source=collection_home---4------4-----------------------", "content": "Title Launch Observability at Netflix Scale\n\nPart 2: Navigating Ambiguity Netflix Technology Blog 6 min read · Jan 7, 2025 -- 8 Listen Share\n\nBy: Varun Khaitan\n\nWith special thanks to my stunning colleagues: Mallika Rao, Esmir Mesic, Hugo Marques\n\nBuilding on the foundation laid in Part 1, where we explored the “what” behind the challenges of title launch observability at Netflix, this post shifts focus to the “how.” How do we ensure every title launches seamlessly and remains discoverable by the right audience?\n\nIn the dynamic world of technology, it’s tempting to leap into problem-solving mode. But the key to lasting success lies in taking a step back — understanding the broader context before diving into solutions. This thoughtful approach doesn’t just address immediate hurdles; it builds the resilience and scalability needed for the future. Let’s explore how this mindset drives results.\n\nUnderstanding the Bigger Picture\n\nLet’s take a comprehensive look at all the elements involved and how they interconnect. We should aim to address questions such as: What is vital to the business? Which aspects of the problem are essential to resolve? And how did we arrive at this point?\n\nThis process involves:\n\nIdentifying Stakeholders: Determine who is impacted by the issue and whose input is crucial for a successful resolution. In this case, the main stakeholders are:\n\n\n\n- Title Launch Operators\n\nRole: Responsible for setting up the title and its metadata into our systems.\n\nChallenge: Don’t understand the cascading effects of their setup on these perceived black box personalization systems\n\n\n\n- Personalization System Engineers\n\nRole: Develop and operate the personalization systems.\n\nChallenge: End up spending unplanned cycles on title launch and personalization investigations.\n\n\n\n- Product Managers\n\nRole: Ensure we put forward the best experience for our members.\n\nChallenge: Members may not connect with the most relevant title.\n\n\n\n- Creative Representatives\n\nRole: Mediator between the content creators and Netflix.\n\nChallenge: Build trust in the Netflix brand with content creators. Mapping the Current Landscape: By charting the existing landscape, we can pinpoint areas ripe for improvement and steer clear of redundant efforts. Beyond the scattered solutions and makeshift scripts, it became evident that there was no established solution for title launch observability. This suggests that this area has been neglected for quite some time and likely requires significant investment. This situation presents both challenges and opportunities; while it may be more difficult to make initial progress, there are plenty of easy wins to capitalize on. Clarifying the Core Problem: By clearly defining the problem, we can ensure that our solutions address the root cause rather than just the symptoms. While there were many issues and problems we could address, the core problem here was to make sure every title was treated fairly by our personalization stack. If we can ensure fair treatment with confidence and bring that visibility to all our stakeholders, we can address all their challenges. Assessing Business Priorities: Understanding what is most important to the organization helps prioritize actions and resources effectively. In this context, we’re focused on developing systems that ensure successful title launches, build trust between content creators and our brand, and reduce engineering operational overhead. While this is a critical business need and we definitely should solve it, it’s essential to evaluate how it stacks up against other priorities across different areas of the organization.\n\nDefining Title Health\n\nNavigating such an ambiguous space required a shared understanding to foster clarity and collaboration. To address this, we introduced the term “Title Health,” a concept designed to help us communicate effectively and capture the nuances of maintaining each title’s visibility and performance. This shared language became a foundation for discussing the complexities of this domain.\n\n“Title Health” encompasses various metrics and indicators that reflect how well a title is performing, in terms of discoverability and member engagement. The three main questions we try to answer are:\n\nIs this title visible at all to any member? Is this title visible to an appropriate audience size? Is this title reaching all the appropriate audiences?\n\nDefining Title Health provided a framework to monitor and optimize each title’s lifecycle. It allowed us to align with partners on principles and requirements before building solutions, ensuring every title reaches its intended audience seamlessly. This common language not only introduced the problem space effectively but also accelerated collaboration and decision-making across teams.\n\nCategories of issues\n\nTo build a robust plan for title launch observability, we first needed to categorize the types of issues we encounter. This structured approach allows us to address all aspects of title health comprehensively.\n\nCurrently, these issues are grouped into three primary categories:\n\n1. Title Setup\n\nA title’s setup includes essential attributes like metadata (e.g., launch dates, audio and subtitle languages, editorial tags) and assets (e.g., artwork, trailers, supplemental messages). These elements are critical for a title’s eligibility in a row, accurate personalization, and an engaging presentation. Since these attributes feed directly into algorithms, any delays or inaccuracies can ripple through the system.\n\nThe observability system must ensure that title setup is complete and validated in a timely manner, identify potential bottlenecks and ensure a smooth launch process.\n\n2. Personalization Systems\n\nTitles are eligible to be recommended across multiple canvases on product — HomePage, Coming Soon, Messaging, Search and more. Personalization systems handle the recommendation and serving of titles on these canvases, leveraging a vast ecosystem of microservices, caches, databases, code, and configurations to build these product canvases.\n\nWe aim to validate that titles are eligible in all appropriate product canvases across the end to end personalization stack during all of the title’s launch phases.\n\n3. Algorithms\n\nComplex algorithms drive each personalized product experience, recommending titles tailored to individual members. Observability here means validating the accuracy of algorithmic recommendations for all titles.\n\nAlgorithmic performance can be affected by various factors, such as model shortcomings, incomplete or inaccurate input signals, feature anomalies, or interactions between titles. Identifying and addressing these issues ensures that recommendations remain precise and effective.\n\nBy categorizing issues into these areas, we can systematically address challenges and deliver a reliable, personalized experience for every title on our platform.\n\nIssue Analysis\n\nLet’s also learn more about how often we see each of these types of issues and how much effort it takes to fix them once they come up.\n\nFrom the above chart, we see that setup issues are the most common but they are also easy to fix since it’s relatively straightforward to go back and rectify a title’s metadata. System issues, which mostly manifest as bugs in our personalization microservices are not uncommon, and they take moderate effort to address. Algorithm issues, while rare, are really difficult to address since these often involve interpreting and retraining complex machine learning models.\n\nEvaluating Our Options\n\nNow that we understand more deeply about the problems we want to address and how we should go about prioritizing our resources. Lets go back to the two options we discussed in Part 1, and make an informed decision.\n\nUltimately, we realized this space demands the full spectrum of features we’ve discussed. But the question remained: Where do we start?\n\nAfter careful consideration, we chose to focus on proactive issue detection first. Catching problems before launch offered the greatest potential for business impact, ensuring smoother launches, better member experiences, and stronger system reliability.\n\nThis decision wasn’t just about solving today’s challenges — it was about laying the foundation for a scalable, robust system that can grow with the complexities of our ever-evolving platform.\n\nUp next\n\nIn the next iteration we will talk about how to design an observability endpoint that works for all personalization systems. What are the main things to keep in mind while creating a microservice API endpoint? How do we ensure standardization? What is the architecture of the systems involved?\n\nKeep an eye out for our next binge-worthy episode!", "label": "non_personal"}
{"title": "What you should know from the Google I/O 2025 Developer keynote", "url": "https://developers.googleblog.com/en/google-io-2025-developer-keynote-recap/", "content": "This year at Google I/O we’re showing how you can build across Google’s different platforms, and innovate using our best AI models from Google DeepMind. Here are the top announcements from the Developer keynote.\n\nBuilding with Gemini\n\nGoogle AI Studio is the fastest way to evaluate models and start building with the Gemini API.\n\nGoogle AI Studio makes it easy to build with the Gemini API: We’ve integrated Gemini 2.5 Pro into the native code editor, enabling you to prototype faster. It’s tightly optimized with the GenAI SDK so you can instantly generate web apps from text, image, or video prompts. Start from a simple prompt, or get inspired by starter apps in the showcase.\n\nBuild agentic experiences with the Gemini API: Build agents with Gemini 2.5 advanced reasoning capabilities via the Gemini API and new tools, like URL Context. It enables the model to pull context from web pages with just a link. We also announced the Gemini SDKs will support Model Context Protocol (MCP) definitions, making it easier to leverage open source tools.\n\nGemini 2.5 Flash Native Audio in the Live API: Build agentic applications that hear and speak, with full control over the model’s voice, tone, speed, and overall style, in 24 languages. Gemini 2.5 Flash Native Audio is much better at understanding conversational flow and ignoring stray sounds or voices, leading to smoother, more natural back-and-forth.\n\nGenerate high-quality UI designs with Stitch: A new AI-powered tool to generate user interface designs and corresponding frontend code for web applications. Iterate on your designs conversationally using chat, adjust themes, and easily export your creations to CSS/HTML or Figma to keep working. Try Stitch for UI design.\n\nOur async code agent, Jules, is now in public beta: Jules is a parallel, asynchronous coding agent that works directly with your GitHub repositories. You can ask Jules to take on tasks such as version upgrades, writing tests, updating features, and bug fixes, to name a few. It spins up a Cloud VM, makes coordinated edits across your codebase, runs tests, and you can open a pull request from its branch when you're happy with the code.\n\n\n\nAndroid\n\nLearn how we’re making it easier for you to build great experiences across devices.\n\nBuilding experiences with generative AI: Generative AI enhances apps by making them intelligent, personalized, and agentic. We announced new ML Kit GenAI APIs using Gemini Nano for common on-device tasks. We showcased an AI sample app, Androidify, which lets you create an Android robot of yourself using a selfie. Discover how Androidify is built, and read the developer documentation to get started.\n\nBuilding excellent apps adaptively across 500 million devices: Mobile Android apps form the foundation across phones, foldables, tablets, and ChromeOS, and this year we’re helping you bring them to cars and Android XR. You can also take advantage of Material 3 Expressive to help make your apps shine.\n\nGemini in Android Studio - AI agents to help you work: Gemini in Android Studio is the AI-powered coding companion that makes developers more productive at every stage of the dev lifecycle. We previewed Journeys, an agentic experience that helps with writing and executing end-to-end tests. We also previewed the Version Upgrade Agent which helps update dependencies. Learn more about how these agentic experiences in Gemini in Android Studio can help you build better apps, faster.\n\n\n\nWeb\n\nWe’re making it easier to create powerful web experiences, from building better UI and faster debugging, to creating new AI-powered features.\n\nCarousels are now easier than ever to build with a few lines of CSS and HTML: Build beautiful carousels with CSS that are interactive at first paint. With Chrome 135, we've combined a few new CSS primitives to make building carousels, and other types of off-screen UI, dramatically easier. Use familiar CSS concepts to create rich, interactive, smooth, and more accessible carousels, in a fraction of the time.\n\nIntroducing the new experimental Interest Invoker API: Declaratively toggle popovers when visitor interest is active for a small duration. Combine with the Anchor Positioning API and Popover API to build complex, responsive, layered UI elements like tooltips and hover cards, without JavaScript. Interest Invoker API is available as an origin trial.\n\nBaseline features availability is now in your familiar tools: VS Code now displays the Baseline status of features as you build, with support coming soon to other VS Code-based IDEs and WebStorm by JetBrains. Baseline is now also supported in ESLint for CSS, HTML ESLint, and Stylelint. RUMvision combines Baseline information with real-user data, letting you strategically select the optimal Baseline target for your audience. Plus, with the web-features data set now 100% mapped, you can now access the Baseline status of every feature on every major browser.\n\nAI in Chrome DevTools supports your debugging workflow: Boost your development workflow with Gemini integrated directly into Chrome DevTools. With AI assistance, you can now directly apply suggested changes to the files in your workspace in the Elements panel. Plus, the reimagined Performance Panel now features a powerful ‘Ask AI’ integration that provides contextual performance insights to help optimize your web application’s Core Web Vitals.\n\nNew built-in AI APIs using Gemini Nano are now available, including multimodal capabilities: Gemini Nano brings enhanced privacy, reduced latency, and lower cost. Starting from Chrome 138, the Summarizer API, Language Detector API, Translator API, and Prompt API for Chrome Extensions are available in Stable. The Writer and Rewriter APIs are available in origin trials, and the Proofreader API and Prompt API with multimodal capabilities are in Canary. Join our early preview program to help shape the future of AI on the web.\n\n\n\nFirebase\n\nPrototype, build, and run modern, AI-powered, full-stack apps users love with Firebase. Use Firebase Studio, a cloud-based, AI workspace powered by Gemini 2.5, to turn your ideas into a full-stack app in minutes, from prompt to publish.\n\nFigma designs can be brought to life in Firebase Studio: Import a Figma design directly into Firebase Studio using the builder.io plugin, then add features and functionality using Gemini in Firebase without having to write any code.\n\nFirebase Studio will now suggest a backend: Rolling out over the next several weeks, when you use the App Prototyping agent, Firebase Studio can detect the need for a backend. Firebase Studio will now recommend Firebase Auth and Cloud Firestore, and when you're ready to publish the app to Firebase App Hosting, Firebase Studio will provision those services for you.\n\nFirebase AI Logic: Integrate Google’s gen AI models directly through your client apps, or through Genkit for server-side implementation. As part of the evolution from Vertex AI in Firebase to Firebase AI Logic, we’re also releasing new features such as client side integrations for the Gemini Developer API, hybrid inference, enhanced observability, and deeper integrations with Firebase products such as App Check and Remote Config.\n\n\n\nBuilding with open models\n\nThere's so much you can do when building with Gemini, but sometimes it's better to train and tune your own model. That’s why we released Gemma, our family of open models designed to be state of the art, and fit on devices.\n\nGemma 3n is in early preview: This model can run on as little as 2GB of RAM thanks to research innovations. It is the first model built on the new, advanced mobile-first architecture that will also power the next generation of Gemini Nano, and is engineered for unmatched AI performance directly on portable devices.\n\nMedGemma is our most capable open model for multimodal medical text and image comprehension: A variant of Gemma 3, MedGemma is a great starting point for developers to fine tune and adapt to build their own healthcare-based AI applications. Its small size makes it efficient for inference, and because it’s open, it enables developers with the flexibility to fine-tune the model and run it in their preferred environments. MedGemma is available for use now as part of Health AI Developer Foundations.\n\nColab is launching an agent first experience that transforms coding: Powered by Gemini 2.5 Flash, Colab helps you navigate complex tasks, such as fine-tuning a model. We showcased how the new AI-first Colab can build UI, saving you lots of coding time.\n\nSignGemma is a sign language understanding model coming later this year to the Gemma family: It is the most capable model for translating sign languages into spoken language text to date (best at American Sign Language to English), enabling you to develop new ways for Deaf/Hard of Hearing users to access technology. Share your input at goo.gle/SignGemma.\n\nDolphinGemma is the world’s first large language model for dolphins: Working with researchers at Georgia Tech and the Wild Dolphin Project, DolphinGemma was fine-tuned on data from decades of field research, to help scientists better understand patterns in how dolphins communicate.\n\n\n\nGoogle Developer Program\n\nWe expanded AI benefits for the Google Developer Program, including Gemini Code Assist Standard, a new gen AI developer annual credit, and 3 months of Google One AI Premium. We also announced a new Google Cloud & NVIDIA community where you can connect with experts from both companies in a dedicated forum, and soon gain access to exclusive learning content and credits.\n\n\n\nTune into all of the developer news\n\nFollowing the keynotes, we’ll be livestreaming sessions across AI, Android, web, and cloud May 20-21. Then, check out all of the Google I/O announcements and updates with 100+ sessions, codelabs, and more available on demand starting May 22.\n\nMake sure to connect with our thriving global community of developers, and follow along on LinkedIn and Instagram as we bring I/O Connect events to developers around the world.", "label": "non_personal"}
{"title": "On-device small language models with multimodality, RAG, and Function Calling", "url": "https://developers.googleblog.com/en/google-ai-edge-small-language-models-multimodality-rag-function-calling/", "content": "Last year Google AI Edge introduced support for on-device small language models (SLMs) with four initial models on Android, iOS, and Web. Today, we are excited to expand support to over a dozen models including the new Gemma 3 and Gemma 3n models, hosted on our new LiteRT Hugging Face community. Gemma 3n, available via Google AI Edge as an early preview, is Gemma’s first multimodal on-device small language model supporting text, image, video, and audio inputs. Paired with our new Retrieval Augmented Generation (RAG) and Function Calling libraries, you have everything you need to prototype and build transformative AI features fully on the edge.\n\nSorry, your browser doesn't support playback for this video Let users control apps with on-device SLMs and our new function calling library\n\nBroader model support You can find our growing list of models to choose from in the LiteRT Hugging Face Community. Download any of these models and easily run them on-device with just a few lines of code. The models are fully optimized and converted for mobile and web. Full instructions on how to run these models can be found in our documentation and on each model card on Hugging Face. To customize any of these models, you finetune the base model and then convert and quantize the model using the appropriate AI Edge libraries. We have a Colab showing every step you need to fine-tune and then convert Gemma 3 1B. With the latest release of our quantization tools, we have new quantization schemes that allow for much higher quality int4 post training quantization. Compared to bf16, the default data type for many models, int4 quantization can reduce the size of language models by a factor of 2.5-4X while significantly decreasing latency and peak memory consumption.\n\nGemma 3 1B & Gemma 3n Earlier this year, we introduced Gemma 3 1B. At only 529MB, this model can run up to 2,585 tokens per second pre-fill on the mobile GPU, allowing it to process up to a page of content in under a second. Gemma 3 1B’s small footprint allows it to support a wide range of devices and limits the size of files an end user would need to download in their application. Today, we are thrilled to add an early preview of Gemma 3n to our collection of supported models. The 2B and 4B parameter variants will both support native text, image, video, and audio inputs. The text and image modalities are available on Hugging Face with audio to follow shortly.\n\nSorry, your browser doesn't support playback for this video Gemma 3n analyzing images fully on-device", "label": "non_personal"}
{"title": "Imagen 4 is now available in the Gemini API and Google AI Studio", "url": "https://developers.googleblog.com/en/imagen-4-now-available-in-the-gemini-api-and-google-ai-studio/", "content": "We're thrilled to bring Imagen 4, our best text-to-image model yet, to paid preview in the Gemini API and for limited free testing in Google AI Studio. Imagen 4 offers significantly improved text rendering over our prior image models and pushes the boundaries of text-to-image generation quality.\n\n\n\nThe Imagen 4 Family: Imagen 4 and Imagen 4 Ultra\n\nWe’re introducing two models within the Imagen 4 family, built to serve a variety of creative needs:\n\n\n\nImagen 4: Your go-to for most tasks\n\nThis is our flagship text-to-image model designed to handle a wide range of image generation tasks with significant improvements in quality, particularly for text generation, over Imagen 3. Imagen 4 is priced at $0.04 per output image.\n\n\n\nImagen 4 Ultra: Precision for your prompts\n\nWhen you need your images to precisely follow instructions, Imagen 4 Ultra is the model for you. It's designed to produce outputs that are more highly aligned with your text prompts, achieving strong results compared to other leading image generation models. Imagen 4 Ultra is priced at $0.06 per output image.\n\nWe will introduce additional billing tiers in the coming weeks. In the meantime, you can request higher rate limits for Imagen 4 and 4 Ultra.\n\n\n\nSee Imagen 4 in action\n\nTo give you a glimpse of Imagen 4's capabilities, here are some examples of what you can create. Created using Imagen 4 Ultra, the prompts below showcase the model's versatility across various styles and content.\n\nPrompt: A 3-panel cosmic epic comic. Panel 1: Tiny 'Stardust' in nebula; radar shows anomaly (text 'ANOMALY DETECTED'), hull text 'stardust'. Pilot whispers. Panel 2: Bioluminescent leviathan emerges; console red text 'WARNING!. Panel 3: Leviathan chases ship through asteroids; console re text 'SHIELD CRITICAL!', screen text 'EVADE!'. Pilot screams, SFX 'CRUNCH!', 'ROOOOAAARR!'.", "label": "non_personal"}
{"title": "Title Launch Observability at Netflix Scale", "url": "https://netflixtechblog.com/title-launch-observability-at-netflix-scale-8efe69ebd653?source=collection_home---4------2-----------------------", "content": "Title Launch Observability at Netflix Scale\n\nPart 3: System Strategies and Architecture Netflix Technology Blog 7 min read · Mar 5, 2025 -- 6 Listen Share\n\nBy: Varun Khaitan\n\nWith special thanks to my stunning colleagues: Mallika Rao, Esmir Mesic, Hugo Marques\n\nThis blog post is a continuation of Part 2, where we cleared the ambiguity around title launch observability at Netflix. In this installment, we will explore the strategies, tools, and methodologies that were employed to achieve comprehensive title observability at scale.\n\nDefining the observability endpoint\n\nTo create a comprehensive solution, we decided to introduce observability endpoints first. Each microservice involved in our Personalization stack that integrated with our observability solution had to introduce a new “Title Health” endpoint. Our goal was for each new endpoint to adhere to a few principles:\n\nAccurate reflection of production behavior Standardization across all endpoints Answering the Insight Triad: “Healthy” or not, why not and how to fix it.\n\nAccurately Reflecting Production Behavior\n\nA key part of our solution is insights into production behavior, which necessitates our requests to the endpoint result in traffic to the real service functions that mimics the same pathways the traffic would take if it came from the usual callers.\n\nIn order to allow for this mimicking, many systems implement an “event” handling, where they convert our request into a call to the real service with properties enabled to log when titles are filtered out of their response and why. Building services that adhere to software best practices, such as Object-Oriented Programming (OOP), the SOLID principles, and modularization, is crucial to have success at this stage. Without these practices, service endpoints may become tightly coupled to business logic, making it challenging and costly to add a new endpoint that seamlessly integrates with the observability solution while following the same production logic.\n\nA service with modular business logic facilitates the seamless addition of an observability endpoint.\n\nStandardization\n\nTo standardize communication between our observability service and the personalization stack’s observability endpoints, we’ve developed a stable proto request/response format. This centralized format, defined and maintained by our team, ensures all endpoints adhere to a consistent protocol. As a result, requests are uniformly handled, and responses are processed cohesively. This standardization enhances adoption within the personalization stack, simplifies the system, and improves understanding and debuggability for engineers.\n\nThe request schema for the observability endpoint.\n\nThe Insight Triad API\n\nTo efficiently understand the health of a title and triage issues quickly, all implementations of the observability endpoint must answer: is the title eligible for this phase of promotion, if not — why is it not eligible, and what can be done to fix any problems.\n\nThe end-users of this observability system are Launch Managers, whose job it is to ensure smooth title launches. As such, they must be able to quickly see whether there is a problem, what the problem is, and how to solve it. Teams implementing the endpoint must provide as much information as possible so that a non-engineer (Launch Manager) can understand the root cause of the issue and fix any title setup issues as they arise. They must also provide enough information for partner engineers to identify the problem with the underlying service in cases of system-level issues.\n\nThese requirements are captured in the following protobuf object that defines the endpoint response.\n\nThe response schema for the observability endpoint.\n\nHigh level architecture\n\nWe’ve distilled our comprehensive solution into the following key steps, capturing the essence of our approach:\n\nEstablish observability endpoints across all services within our Personalization and Discovery Stack. Implement proactive monitoring for each of these endpoints. Track real-time title impressions from the Netflix UI. Store the data in an optimized, highly distributed datastore. Offer easy-to-integrate APIs for our dashboard, enabling stakeholders to track specific titles effectively. “Time Travel” to validate ahead of time.\n\nObservability stack high level architecture diagram\n\nIn the following sections, we will explore each of these concepts and components as illustrated in the diagram above.\n\nKey Features\n\nProactive monitoring through scheduled collectors jobs\n\nOur Title Health microservice runs a scheduled collector job every 30 minutes for most of our personalization stack.\n\nFor each Netflix row we support (such as Trending Now, Coming Soon, etc.), there is a dedicated collector. These collectors retrieve the relevant list of titles from our catalog that qualify for a specific row by interfacing with our catalog services. These services are informed about the expected subset of titles for each row, for which we are assessing title health.\n\nOnce a collector retrieves its list of candidate titles, it orchestrates batched calls to assigned row services using the above standardized schema to retrieve all the relevant health information of the titles. Additionally, some collectors will instead poll our kafka queue for impressions data.\n\nReal-time Title Impressions and Kafka Queue\n\nIn addition to evaluating title health via our personalization stack services, we also keep an eye on how our recommendation algorithms treat titles by reviewing impressions data. It’s essential that our algorithms treat all titles equitably, for each one has limitless potential.\n\nThis data is processed from a real-time impressions stream into a Kafka queue, which our title health system regularly polls. Specialized collectors access the Kafka queue every two minutes to retrieve impressions data. This data is then aggregated in minute(s) intervals, calculating the number of impressions titles receive in near-real-time, and presented as an additional health status indicator for stakeholders.\n\nData storage and distribution through Hollow Feeds\n\nNetflix Hollow is an Open Source java library and toolset for disseminating in-memory datasets from a single producer to many consumers for high performance read-only access. Given the shape of our data, hollow feeds are an excellent strategy to distribute the data across our service boxes.\n\nOnce collectors gather health data from partner services in the personalization stack or from our impressions stream, this data is stored in a dedicated Hollow feed for each collector. Hollow offers numerous features that help us monitor the overall health of a Netflix row, including ensuring there are no large-scale issues across a feed publish. It also allows us to track the history of each title by maintaining a per-title data history, calculate differences between previous and current data versions, and roll back to earlier versions if a problematic data change is detected.\n\nObservability Dashboard using Health Check Engine\n\nWe maintain several dashboards that utilize our title health service to present the status of titles to stakeholders. These user interfaces access an endpoint in our service, enabling them to request the current status of a title across all supported rows. This endpoint efficiently reads from all available Hollow Feeds to obtain the current status, thanks to Hollow’s in-memory capabilities. The results are returned in a standardized format, ensuring easy support for future UIs.\n\nAdditionally, we have other endpoints that can summarize the health of a title across subsets of sections to highlight specific member experiences.\n\nMessage depicting a dashboard request.\n\nTime Traveling: Catching before launch\n\nTitles launching at Netflix go through several phases of pre-promotion before ultimately launching on our platform. For each of these phases, the first several hours of promotion are critical for the reach and effective personalization of a title, especially once the title has launched. Thus, to prevent issues as titles go through the launch lifecycle, our observability system needs to be capable of simulating traffic ahead of time so that relevant teams can catch and fix issues before they impact members. We call this capability “Time Travel”.\n\nMany of the metadata and assets involved in title setup have specific timelines for when they become available to members. To determine if a title will be viewable at the start of an experience, we must simulate a request to a partner service as if it were from a future time when those specific metadata or assets are available. This is achieved by including a future timestamp in our request to the observability endpoint, corresponding to when the title is expected to appear for a given experience. The endpoint then communicates with any further downstream services using the context of that future timestamp.\n\nAn example request with a future timestamp.\n\nConclusion\n\nThroughout this series, we’ve explored the journey of enhancing title launch observability at Netflix. In Part 1, we identified the challenges of managing vast content launches and the need for scalable solutions to ensure each title’s success. Part 2 highlighted the strategic approach to navigating ambiguity, introducing “Title Health” as a framework to align teams and prioritize core issues. In this final part, we detailed the sophisticated system strategies and architecture, including observability endpoints, proactive monitoring, and “Time Travel” capabilities; all designed to ensure a thrilling viewing experience.\n\nBy investing in these innovative solutions, we enhance the discoverability and success of each title, fostering trust with content creators and partners. This journey not only bolsters our operational capabilities but also lays the groundwork for future innovations, ensuring that every story reaches its intended audience and that every member enjoys their favorite titles on Netflix.\n\nThank you for joining us on this exploration, and stay tuned for more insights and innovations as we continue to entertain the world.", "label": "non_personal"}
{"title": "Foundation Model for Personalized Recommendation", "url": "https://netflixtechblog.com/foundation-model-for-personalized-recommendation-1a0bd8e02d39?source=collection_home---4------1-----------------------", "content": "Foundation Model for Personalized Recommendation Netflix Technology Blog 11 min read · Mar 21, 2025 -- 36 Listen Share\n\nBy Ko-Jen Hsiao, Yesu Feng and Sudarshan Lamkhede\n\nMotivation\n\nNetflix’s personalized recommender system is a complex system, boasting a variety of specialized machine learned models each catering to distinct needs including “Continue Watching” and “Today’s Top Picks for You.” (Refer to our recent overview for more details). However, as we expanded our set of personalization algorithms to meet increasing business needs, maintenance of the recommender system became quite costly. Furthermore, it was difficult to transfer innovations from one model to another, given that most are independently trained despite using common data sources. This scenario underscored the need for a new recommender system architecture where member preference learning is centralized, enhancing accessibility and utility across different models.\n\nParticularly, these models predominantly extract features from members’ recent interaction histories on the platform. Yet, many are confined to a brief temporal window due to constraints in serving latency or training costs. This limitation has inspired us to develop a foundation model for recommendation. This model aims to assimilate information both from members’ comprehensive interaction histories and our content at a very large scale. It facilitates the distribution of these learnings to other models, either through shared model weights for fine tuning or directly through embeddings.\n\nThe impetus for constructing a foundational recommendation model is based on the paradigm shift in natural language processing (NLP) to large language models (LLMs). In NLP, the trend is moving away from numerous small, specialized models towards a single, large language model that can perform a variety of tasks either directly or with minimal fine-tuning. Key insights from this shift include:\n\nA Data-Centric Approach: Shifting focus from model-centric strategies, which heavily rely on feature engineering, to a data-centric one. This approach prioritizes the accumulation of large-scale, high-quality data and, where feasible, aims for end-to-end learning. Leveraging Semi-Supervised Learning: The next-token prediction objective in LLMs has proven remarkably effective. It enables large-scale semi-supervised learning using unlabeled data while also equipping the model with a surprisingly deep understanding of world knowledge.\n\nThese insights have shaped the design of our foundation model, enabling a transition from maintaining numerous small, specialized models to building a scalable, efficient system. By scaling up semi-supervised training data and model parameters, we aim to develop a model that not only meets current needs but also adapts dynamically to evolving demands, ensuring sustainable innovation and resource efficiency.\n\nData\n\nAt Netflix, user engagement spans a wide spectrum, from casual browsing to committed movie watching. With over 300 million users at the end of 2024, this translates into hundreds of billions of interactions — an immense dataset comparable in scale to the token volume of large language models (LLMs). However, as in LLMs, the quality of data often outweighs its sheer volume. To harness this data effectively, we employ a process of interaction tokenization, ensuring meaningful events are identified and redundancies are minimized.\n\nTokenizing User Interactions: Not all raw user actions contribute equally to understanding preferences. Tokenization helps define what constitutes a meaningful “token” in a sequence. Drawing an analogy to Byte Pair Encoding (BPE) in NLP, we can think of tokenization as merging adjacent actions to form new, higher-level tokens. However, unlike language tokenization, creating these new tokens requires careful consideration of what information to retain. For instance, the total watch duration might need to be summed or engagement types aggregated to preserve critical details.\n\nFigure 1.Tokenization of user interaction history by merging actions on the same title, preserving important information.\n\nThis tradeoff between granular data and sequence compression is akin to the balance in LLMs between vocabulary size and context window. In our case, the goal is to balance the length of interaction history against the level of detail retained in individual tokens. Overly lossy tokenization risks losing valuable signals, while too granular a sequence can exceed practical limits on processing time and memory.\n\nEven with such strategies, interaction histories from active users can span thousands of events, exceeding the capacity of transformer models with standard self attention layers. In recommendation systems, context windows during inference are often limited to hundreds of events — not due to model capability but because these services typically require millisecond-level latency. This constraint is more stringent than what is typical in LLM applications, where longer inference times (seconds) are more tolerable.\n\nTo address this during training, we implement two key solutions:\n\nSparse Attention Mechanisms: By leveraging sparse attention techniques such as low-rank compression, the model can extend its context window to several hundred events while maintaining computational efficiency. This enables it to process more extensive interaction histories and derive richer insights into long-term preferences. Sliding Window Sampling: During training, we sample overlapping windows of interactions from the full sequence. This ensures the model is exposed to different segments of the user’s history over multiple epochs, allowing it to learn from the entire sequence without requiring an impractically large context window.\n\nAt inference time, when multi-step decoding is needed, we can deploy KV caching to efficiently reuse past computations and maintain low latency.\n\nThese approaches collectively allow us to balance the need for detailed, long-term interaction modeling with the practical constraints of model training and inference, enhancing both the precision and scalability of our recommendation system.\n\nInformation in Each ‘Token’: While the first part of our tokenization process focuses on structuring sequences of interactions, the next critical step is defining the rich information contained within each token. Unlike LLMs, which typically rely on a single embedding space to represent input tokens, our interaction events are packed with heterogeneous details. These include attributes of the action itself (such as locale, time, duration, and device type) as well as information about the content (such as item ID and metadata like genre and release country). Most of these features, especially categorical ones, are directly embedded within the model, embracing an end-to-end learning approach. However, certain features require special attention. For example, timestamps need additional processing to capture both absolute and relative notions of time, with absolute time being particularly important for understanding time-sensitive behaviors.\n\nTo enhance prediction accuracy in sequential recommendation systems, we organize token features into two categories:\n\nRequest-Time Features: These are features available at the moment of prediction, such as log-in time, device, or location. Post-Action Features: These are details available after an interaction has occurred, such as the specific show interacted with or the duration of the interaction.\n\nTo predict the next interaction, we combine request-time features from the current step with post-action features from the previous step. This blending of contextual and historical information ensures each token in the sequence carries a comprehensive representation, capturing both the immediate context and user behavior patterns over time.\n\nConsiderations for Model Objective and Architecture\n\nAs previously mentioned, our default approach employs the autoregressive next-token prediction objective, similar to GPT. This strategy effectively leverages the vast scale of unlabeled user interaction data. The adoption of this objective in recommendation systems has shown multiple successes [1–3]. However, given the distinct differences between language tasks and recommendation tasks, we have made several critical modifications to the objective.\n\nFirstly, during the pretraining phase of typical LLMs, such as GPT, every target token is generally treated with equal weight. In contrast, in our model, not all user interactions are of equal importance. For instance, a 5-minute trailer play should not carry the same weight as a 2-hour full movie watch. A greater challenge arises when trying to align long-term user satisfaction with specific interactions and recommendations. To address this, we can adopt a multi-token prediction objective during training, where the model predicts the next n tokens at each step instead of a single token[4]. This approach encourages the model to capture longer-term dependencies and avoid myopic predictions focused solely on immediate next events.\n\nSecondly, we can use multiple fields in our input data as auxiliary prediction objectives in addition to predicting the next item ID, which remains the primary target. For example, we can derive genres from the items in the original sequence and use this genre sequence as an auxiliary target. This approach serves several purposes: it acts as a regularizer to reduce overfitting on noisy item ID predictions, provides additional insights into user intentions or long-term genre preferences, and, when structured hierarchically, can improve the accuracy of predicting the target item ID. By first predicting auxiliary targets, such as genre or original language, the model effectively narrows down the candidate list, simplifying subsequent item ID prediction.\n\nUnique Challenges for Recommendation FM\n\nIn addition to the infrastructure challenges posed by training bigger models with substantial amounts of user interaction data that are common when trying to build foundation models, there are several unique hurdles specific to recommendations to make them viable. One of unique challenges is entity cold-starting.\n\nAt Netflix, our mission is to entertain the world. New titles are added to the catalog frequently. Therefore the recommendation foundation models require a cold start capability, which means the models need to estimate members’ preferences for newly launched titles before anyone has engaged with them. To enable this, our foundation model training framework is built with the following two capabilities: Incremental training and being able to do inference with unseen entities.\n\nIncremental training : Foundation models are trained on extensive datasets, including every member’s history of plays and actions, making frequent retraining impractical. However, our catalog and member preferences continually evolve. Unlike large language models, which can be incrementally trained with stable token vocabularies, our recommendation models require new embeddings for new titles, necessitating expanded embedding layers and output components. To address this, we warm-start new models by reusing parameters from previous models and initializing new parameters for new titles. For example, new title embeddings can be initialized by adding slight random noise to existing average embeddings or by using a weighted combination of similar titles’ embeddings based on metadata. This approach allows new titles to start with relevant embeddings, facilitating faster fine-tuning. In practice, the initialization method becomes less critical when more member interaction data is used for fine-tuning. Dealing with unseen entities : Even with incremental training, it’s not always guaranteed to learn efficiently on new entities (ex: newly launched titles). It’s also possible that there will be some new entities that are not included/seen in the training data even if we fine-tune foundation models on a frequent basis. Therefore, it’s also important to let foundation models use metadata information of entities and inputs, not just member interaction data. Thus, our foundation model combines both learnable item id embeddings and learnable embeddings from metadata. The following diagram demonstrates this idea.\n\nFigure 2. Titles are associated with various metadata, such as genres, storylines, and tones. Each type of metadata could be represented by averaging its respective embeddings, which are then concatenated to form the overall metadata-based embedding for the title.\n\nTo create the final title embedding, we combine this metadata-based embedding with a fully-learnable ID-based embedding using a mixing layer. Instead of simply summing these embeddings, we use an attention mechanism based on the “age” of the entity. This approach allows new titles with limited interaction data to rely more on metadata, while established titles can depend more on ID-based embeddings. Since titles with similar metadata can have different user engagement, their embeddings should reflect these differences. Introducing some randomness during training encourages the model to learn from metadata rather than relying solely on ID embeddings. This method ensures that newly-launched or pre-launch titles have reasonable embeddings even with no user interaction data.\n\nDownstream Applications and Challenges\n\nOur recommendation foundation model is designed to understand long-term member preferences and can be utilized in various ways by downstream applications:\n\nDirect Use as a Predictive Model The model is primarily trained to predict the next entity a user will interact with. It includes multiple predictor heads for different tasks, such as forecasting member preferences for various genres. These can be directly applied to meet diverse business needs.. Utilizing embeddings The model generates valuable embeddings for members and entities like videos, games, and genres. These embeddings are calculated in batch jobs and stored for use in both offline and online applications. They can serve as features in other models or be used for candidate generation, such as retrieving appealing titles for a user. High-quality title embeddings also support title-to-title recommendations. However, one important consideration is that the embedding space has arbitrary, uninterpretable dimensions and is incompatible across different model training runs. This poses challenges for downstream consumers, who must adapt to each retraining and redeployment, risking bugs due to invalidated assumptions about the embedding structure. To address this, we apply an orthogonal low-rank transformation to stabilize the user/item embedding space, ensuring consistent meaning of embedding dimensions, even as the base foundation model is retrained and redeployed. Fine-Tuning with Specific Data The model’s adaptability allows for fine-tuning with application-specific data. Users can integrate the full model or subgraphs into their own models, fine-tuning them with less data and computational power. This approach achieves performance comparable to previous models, despite the initial foundation model requiring significant resources.\n\nScaling Foundation Models for Netflix Recommendations\n\nIn scaling up our foundation model for Netflix recommendations, we draw inspiration from the success of large language models (LLMs). Just as LLMs have demonstrated the power of scaling in improving performance, we find that scaling is crucial for enhancing generative recommendation tasks. Successful scaling demands robust evaluation, efficient training algorithms, and substantial computing resources. Evaluation must effectively differentiate model performance and identify areas for improvement. Scaling involves data, model, and context scaling, incorporating user engagement, external reviews, multimedia assets, and high-quality embeddings. Our experiments confirm that the scaling law also applies to our foundation model, with consistent improvements observed as we increase data and model size.\n\nFigure 3. The relationship between model parameter size and relative performance improvement. The plot demonstrates the scaling law in recommendation modeling, showing a trend of increased performance with larger model sizes. The x-axis is logarithmically scaled to highlight growth across different magnitudes.\n\nConclusion\n\nIn conclusion, our Foundation Model for Personalized Recommendation represents a significant step towards creating a unified, data-centric system that leverages large-scale data to increase the quality of recommendations for our members. This approach borrows insights from Large Language Models (LLMs), particularly the principles of semi-supervised learning and end-to-end training, aiming to harness the vast scale of unlabeled user interaction data. Addressing unique challenges, like cold start and presentation bias, the model also acknowledges the distinct differences between language tasks and recommendation. The Foundation Model allows various downstream applications, from direct use as a predictive model to generate user and entity embeddings for other applications, and can be fine-tuned for specific canvases. We see promising results from downstream integrations. This move from multiple specialized models to a more comprehensive system marks an exciting development in the field of personalized recommendation systems.\n\nAcknowledgements\n\nContributors to this work (name in alphabetical order): Ai-Lei Sun Aish Fenton Anne Cocos Anuj Shah Arash Aghevli Baolin Li Bowei Yan Dan Zheng Dawen Liang Ding Tong Divya Gadde Emma Kong Gary Yeh Inbar Naor Jin Wang Justin Basilico Kabir Nagrecha Kevin Zielnicki Linas Baltrunas Lingyi Liu Luke Wang Matan Appelbaum Michael Tu Moumita Bhattacharya Pablo Delgado Qiuling Xu Rakesh Komuravelli Raveesh Bhalla Rob Story Roger Menezes Sejoon Oh Shahrzad Naseri Swanand Joshi Trung Nguyen Vito Ostuni Wei Wang Zhe Zhang\n\nReference", "label": "non_personal"}
{"title": "From idea to app: Introducing Stitch, a new way to design UIs", "url": "https://developers.googleblog.com/en/stitch-a-new-way-to-design-uis/", "content": "That's precisely the problem Stitch aims to solve – Stitch is a new experiment from Google Labs that allows you to turn simple prompt and image inputs into complex UI designs and frontend code in minutes.\n\nBuilding great applications always comes down to a powerful partnership between design and development. Designers envision the user experience, crafting intuitive and engaging interfaces. Developers then bring those designs to life with functional code. Traditionally, connecting design ideas to working code took a lot of manual effort and back-and-forth.\n\nStitch was born of an idea between a designer and an engineer, both looking to build a product that optimized their respective workflows. It leverages the multimodal capabilities of Gemini 2.5 Pro to create a more fluid and integrated workflow between design and development. And, with an option to refine your design with image inputs, an interactive chat, theme selectors, and a paste to Figma function, Stitch lets you truly hone in on your creative designs and development needs.\n\n\n\nHere’s what Stitch offers today to enhance your design and development process:\n\n\n\nGenerate UI from natural language\n\nDescribe the application you want to build in plain English, including details like color palettes or desired user experience. Stitch can generate a visual interface tailored to your description.\n\n\n\nGenerate UI from images or wireframes\n\nHave a design sketch on a whiteboard, a screenshot of a compelling UI, or a rough wireframe? Upload it to Stitch. Stitch processes the image to produce a corresponding digital UI, bridging your initial visual ideas to a functional design.\n\n\n\nRapid iteration and design exploration\n\nDesign is an iterative process, and Stitch facilitates this by allowing you to generate multiple variants of your interface. Experiment with different layouts, components, and styles to achieve the desired look and feel.\n\n\n\nSeamless transition to development\n\nOnce you're satisfied with your design, Stitch provides crucial bridges to the development workflow:\n\nPaste to Figma: Your generated design can be seamlessly pasted to Figma for easy further refinement, collaboration with design teams, and integration into existing design systems.\n\nExport front-end code: Stitch generates clean, functional front-end code based on your design, so you have a fully functional UI ready to go.\n\n\n\nStitch is about unlocking the magic of app creation for everyone. We're thrilled to bring this experiment to you and can't wait to see what you'll build with it.\n\nTry out Stitch at stitch.withgoogle.com and let us know what you think!", "label": "non_personal"}
{"title": "HDR10+ Now Streaming on Netflix", "url": "https://netflixtechblog.com/hdr10-now-streaming-on-netflix-c9ab1f4bd72b?source=collection_home---4------0-----------------------", "content": "HDR10+ Now Streaming on Netflix Netflix Technology Blog 5 min read · Mar 24, 2025 -- 13 Listen Share\n\nRoger Quero, Liwei Guo, Jeff Watts, Joseph McCormick, Agata Opalach, Anush Moorthy\n\nWe are excited to announce that we are now streaming HDR10+ content on our service for AV1-enabled devices, enhancing the viewing experience for certified HDR10+ devices, which previously only received HDR10 content. The dynamic metadata included in our HDR10+ content improves the quality and accuracy of the picture when viewed on these devices.\n\nDelighting Members with Even Better Picture Quality\n\nNearly a decade ago, we made a bold move to be a pioneering adopter of High Dynamic Range (HDR) technology. HDR enables images to have more details, vivid colors, and improved realism. We began producing our shows and movies in HDR, encoding them in HDR, and streaming them in HDR for our members. We were confident that it would greatly enhance our members’ viewing experience, and unlock new creative visions — and we were right! In the last five years, HDR streaming has increased by more than 300%, while the number of HDR-configured devices watching Netflix has more than doubled. Since launching HDR with season one of Marco Polo, Netflix now has over 11,000 hours of HDR titles for members to immerse themselves in.\n\nWe continue to enhance member joy while maintaining creative vision by adding support for HDR10+. This will further augment Netflix’s growing HDR ecosystem, preserve creative intent on even more devices, and provide a more immersive viewing experience.\n\nWe enabled HDR10+ on Netflix using the AV1 video codec that was standardized by the Alliance for Open Media (AOM) in 2018. AV1 is one of the most efficient codecs available today. We previously enabled AV1 encoding for SDR content, and saw tremendous value for our members, including higher and more consistent visual quality, lower play delay and increased streaming at the highest resolution. AV1-SDR is already the second most streamed codec at Netflix, behind H.264/AVC, which has been around for over 20 years! With the addition of HDR10+ streams to AV1, we expect the day is not far when AV1 will be the most streamed codec at Netflix.\n\nTo enhance our offering, we have been adding HDR10+ streams to both new releases and existing popular HDR titles. AV1-HDR10+ now accounts for 50% of all eligible viewing hours. We will continue expanding our HDR10+ offerings with the goal of providing an HDR10+ experience for all HDR titles by the end of this year¹.\n\nIndustry Adopted Formats\n\nToday, the industry recognizes three prevalent HDR formats: Dolby Vision, HDR10, and HDR10+. For all three HDR Formats, metadata is embedded in the content, serving as instructions to guide the playback device — whether it’s a TV, mobile device, or computer — on how to display the image.\n\nHDR10 is the most widely adopted HDR format, supported by all HDR devices. HDR10 uses static metadata that is defined once for the entire content detailing aspects such as the maximum content light level (MaxCLL), maximum frame average light level (MaxFALL), as well as characteristics of the mastering display used for color grading. This metadata only allows for a one-size-fits-all tone mapping of the content for display devices. It cannot account for dynamic contrast across scenes, which most content contains.\n\nHDR10+ and Dolby Vision improve on this with dynamic metadata that provides content image statistics on a per-frame basis, enabling optimized tone mapping adjustments for each scene. This achieves greater perceptual fidelity to the original, preserving creative intent.\n\nHDR10 vs. HDR10+\n\nThe figure below shows screen grabs of two AV1-encoded frames of the same content displayed using HDR10 (top) and HDR10+ (bottom).\n\nPhotographs of devices displaying the same frame with HDR10 metadata (top) and HDR10+ metadata (bottom). Notice the preservation of the flashlight detail in the HDR10+ capture, and the over-exposure of the region under the flashlight in the HDR10 one².\n\nAs seen in the flashlight on the table, the highlight details are clipped in the HDR10 content, but are recovered in HDR10+. Further, the region under the flashlight is overexposed in the HDR10 content, while HDR10+ renders that region with greater fidelity to the source. The reason HDR10+, with its dynamic metadata, shines in this example is that the scenes preceding and following the scene with this frame have markedly different luminance statistics. The static HDR10 metadata is unable to account for the change in the content. While this is a simple example, the dynamic metadata in HDR10+ demonstrates such value across any set of scenes. This consistency allows our members to stay immersed in the content, and better preserves creative intent.\n\nReceiving HDR10+\n\nAt the time of launch, these requirements must be satisfied to receive HDR10+:\n\n1.Member must have a Netflix Premium plan subscription\n\n2. Title must be available in HDR10+ format\n\n3. Member device must support AV1 & HDR10+. Here are some examples of compatible devices:\n\nSmartTVs, mobile phones, and tablets that meet Netflix certification for HDR10+\n\nSource device (such as set-top boxes, streaming devices, MVPDs, etc.) that meets Netflix certification for HDR10+, connected to an HDR10+ compliant display via HDMI\n\n4. For TV or streaming devices, ensure that the HDR toggle is enabled in our Netflix application settings: https://help.netflix.com/en/node/100220\n\nAdditional guidance: https://help.netflix.com/en/node/13444\n\nSummary\n\nMore HDR content is watched every day on Netflix. Expanding the Netflix HDR ecosystem to include HDR10+ increases the accessibility of HDR content with dynamic metadata to more members, improves the viewing experience, and preserves the creative intent of our content creators. The commitment to innovation and quality underscores our dedication to delivering an immersive and authentic viewing experience for all our members.\n\nAcknowledgements\n\nLaunching HDR10+ was a collaborative effort involving multiple teams at Netflix, and we are grateful to everyone who contributed to making this idea a reality. We would like to extend our thanks to the following teams for their crucial roles in this launch:\n\nFootnotes", "label": "non_personal"}
{"title": "Globalizing Productions with Netflix’s Media Production Suite", "url": "https://netflixtechblog.com/globalizing-productions-with-netflixs-media-production-suite-fc3c108c0a22?source=collection_home---4------2-----------------------", "content": "Globalizing Productions with Netflix’s Media Production Suite Netflix Technology Blog 12 min read · Mar 31, 2025 -- 8 Listen Share\n\nJesse Korosi, Thijs van de Kamp, Mayra Vega, Laura Futuro, Anton Margoline\n\nThe journey from script to screen is full of challenges in the ever-evolving world of film and television. The industry has always innovated, and over the last decade, it started moving towards cloud-based workflows. However, unlocking cloud innovation and all its benefits on a global scale has proven to be difficult. The opportunity is clear: streamline complex media management logistics, eliminate tedious, non-creative task-based work and enable productions to focus on what matters most–creative storytelling. With these challenges in mind, Netflix has developed a suite of tools by filmmakers for filmmakers: the Media Production Suite (MPS).\n\nWhat are we solving for?\n\nSignificant time and resources are devoted to managing media logistics throughout the production lifecycle. An average Netflix title produces around ~200 Terabytes of Original Camera Files (OCF), with outliers up to 700 Terabytes, not including any work-in-progress files, VFX assets, 3D assets, etc. The data produced on set is traditionally copied to physical tape stock like LTO. This workflow has been considered the industry norm for a long time and may be cost-effective, but comes with trade-offs. Aside from needing to physically ship and track all movement of the tape stock, storing media on a physical tape makes it harder to search, play and share media assets; slowing down accessibility to production media when needed, especially when titles need to collaborate with talent and vendors all over the world.\n\nEven when workflows are fully digital, the distribution of media between multiple departments and vendors can still be challenging. A lack of automation and standardization often results in a labour-intensive process across post-production and VFX with a lot of dependencies that introduce potential human errors and security risks. Many productions utilize a large variety of vendors, making this collaboration a large technical puzzle. As file sizes grow and workflows become more complex, these issues are magnified, leading to inefficiencies that slow down post-production and reduce the available time spent on creative work.\n\nMoving media into the cloud introduces new challenges for production and post ramping up to meet the operational and technological hurdles this poses. For some post-production facilities, it’s not uncommon to see a wall of portable hard drives at their facility, with media being hand-carried between vendors because alternatives are not available. The need for a centralized, cloud-based solution that transcends these barriers is more pressing than ever. This results in a willingness to embrace new and innovative ideas, even if exploratory, and introduce drastic workflow changes to productions in pursuit of creative evolution.\n\nAt Netflix, we believe that great stories can come from anywhere, but we have seen that technical limitations in traditional workflows reduce access to media and restrict filmmakers’ access to talent. Besides the need for robust cloud storage for their media, artists need access to powerful workstations and real-time playback. Depending on the market, or production budget, cutting-edge technology might not be available or affordable.\n\nWhat if we started charting a course to break free from many of these technical limitations and found ways to enhance creativity? Industry trade shows like the International Broadcast Convention (IBC) and the National Association of Broadcasters Show (NAB) highlight a strong global trend: instead of bringing media to the artist/applications (traditional workflow) we see the shift towards bringing people and applications to the media (cloud workflows and remote workstations). The concept of cloud-based workflows is not new, as many technology leaders in our industry have been experimenting in this space for more than a decade. However, executing this vision at a Netflix scale with hundreds of titles a year has not been done before…\n\nThe challenge of building a global technology to solve this\n\nBuilding solutions at a global scale poses significant challenges. The art of making movies and series lacks equal access to technology, best practices, and global standardization. Different countries worldwide are at different phases of innovation based on local needs and nuances. While some regions boast over a century of cinematic history and have a strong industry, others are just beginning to carve their niche. This vast gap presents a unique challenge: developing global technology that caters to both established and emerging markets, each with distinct languages and workflows.\n\nThe large diversity of needs by talent and vendors globally creates a standardization challenge and can be seen when productions use a global talent pool. Many mature post-production and VFX facilities have built scripts and automation that flow between various artists and personnel within their facility; allowing a more streamlined workflow, even though the customization is time-consuming. E.g., Transcoding, or transcriptions that automatically run when files are dropped in a hot folder, with the expectation that certain sidecar metadata files will accompany them with a specific organizational structure. Embracing and integrating new workflows introduces the fear of disrupting a well-established process, increasing additional pressure on the profit margins of vendors. Small workflow changes that may seem arbitrary may actually have a large impact on vendors. Therefore, innovation should provide meaningful benefits to a title in order to get adopted at scale. Reliability, a proven track record, strong support, and an incredibly low tolerance for bugs, or issues are top of mind in well-established markets.\n\nIn developing this suite, we recognized the necessity of addressing the vast array of titles that flow through Netflix without the luxury of expanding into a massive operational entity. Consequently, automation became imperative. The intricacies of color and framing management, along with deliverables, must be seamlessly controlled and effortlessly managed by the user, without the need for manual intervention. Therefore, we cannot lean into humans configuring JSON files behind the scenes to map camera formats into deliverables. By embracing open standards, we not only streamline these processes but also facilitate smoother collaboration across diverse markets and countries, ensuring that our global productions can operate with unparalleled efficiency and cohesion. To ensure this, we’ve decided to lean heavily into standards like ACES, AMF, ASC MHL, ASC FDL, and OTIO. ACES and AMF for color pipeline management. ASC MHL for any file management/verifications. ASC FDL will serve as our framing interoperability and OTIO for any timeline interchange. Leaning into standards like this means that many things can be automated at scale and more importantly, high-complexity workflows can be offered to markets or shows that don’t normally have access to them. As an example, if a show is shot on various camera formats all framed and recorded at different resolutions, with different lenses and different safeties on each frame. The task of normalizing all of these for a VFX vendor into one common container with a normalized center extracted frame is often only offered to very high-end titles, considering it takes a human behind the curtain to create all of these mappings. But by leaning into a standard like the FDL, it means this can now easily be automated, and the control for these mappings, put directly in the hands of users.\n\nOur Answer — Content Hub’s Media Production Suite (MPS)\n\nIntroducing Content Hub Media Production Suite video\n\nBuilding a global scalable solution that could be utilized in a diversity of markets has been an exciting challenge. We set out to provide customizable and feature-rich tooling for advanced users while remaining intuitive and streamlined enough for less experienced filmmakers. With collaboration from Netflix teams, vendors, and talent across the globe, we’ve taken a bold step forward in enabling a suite of tools inside Netflix Content Hub that democratizes technology: the Media Production Suite. While leveraging our scale economies and access to resources, we can now unlock global talent pools for our productions, drastically reduce non-creative task-based work, streamline workflows, and level the playing field between our markets, ultimately maximizing the time available for what matters most; creative work!\n\nSo what is it?\n\n1. Netflix Hybrid Infrastructure: Netflix has invested in a hybrid infrastructure, a mix of cloud-based and physically distributed capabilities operating in multiple locations across the world and close to our productions to optimize user performance. This infrastructure is available for Netflix shows and is foundational under Content Hub’s Media Production Suite tooling. Local storage and compute services are connected through the Netflix Open Connect network (Netflix Content Delivery Network) to the infrastructure of Amazon Web Services (AWS). The system facilitates large volumes of camera and sound media and is built for speed. In order to ensure that productions have sufficient upload speeds to get their media into the cloud, Netflix has started to roll out Content Hub Ingest Centers globally to provide high-speed internet connectivity where required. With all media centralized, MPS eliminates the need for physical media transport and reduces the risk of human error. This approach not only streamlines operations but also enhances security and accessibility.\n\n2. Automation and Tooling: In addition to the Netflix Hybrid infrastructure layer, MPS consists of a suite of tools that tap into the media in the Netflix ecosystem.\n\nFootage Ingest — An application that allows users to upload media/files into Content Hub.\n\nMedia Library — A central library that allows users to search, preview, share and download media.\n\nDailies — A workflow, backed by an operational team, offering automated Quality Control of your footage, sound sync, application of color, rendering, and delivering dailies directly to editorial.\n\nRemote Workstations — Offering access to remote editorial workstations and storage for post-production needs.\n\nVFX Pulls — An automated method for converting and delivering visual effects plates, associated color, and framing files to VFX vendors.\n\nConform Pulls — An automated method for consolidating, trimming, and delivering all OCF to picture-finishing vendors.\n\nMedia Downloader — An automated download tool that initiates a download once media has been made available in the Netflix cloud.\n\nWhile each of the individual tools within MPS is at different states of maturity, over 350 titles have made use of at least one of the tools noted above. Input has been taken from all over the world while developing, with users ranging from UCAN (United States/Canada), EMEA (Europe, Middle East, and Africa), SEA (South East Asia), LATAM (Latin America), and APAC (Asia Pacific).\n\nSenna: Early Adoption and Insightful Feedback Driving MPS Evolution\n\nMedia from the Brazilian-produced series ‘Senna’ being reviewed in MPS\n\nThe Brazilian-produced series Senna, which follows the life of legendary Formula 1 driver Ayrton Senna, utilized MPS to reshape their content creation workflow, overcome geographical barriers, and unlock innovation to support world-class storytelling for a global audience. Senna is a groundbreaking series, not just for its storytelling but for its production journey across Argentina, Uruguay, Brazil, and the United Kingdom. With editorial teams spread across Porto Alegre and Spain, and VFX studios collaborating across locations in Brazil, Canada, the United States, and India, all orchestrated by our subsidiary Scanline VFX. The series exemplifies the global nature of modern filmmaking and was the perfect fit for Netflix’s new Content Hub Media Production Suite (MPS).\n\nAt the heart of Senna’s workflow orchestration is MPS. While each of the tools within MPS is based on an opt-in model, in order to use many of the downstream services, the first step is ensuring that the original camera files (OCF) and original sound files (OSF) are uploaded. “We knew we were going to shoot in different places,” said Post Supervisor Gabriel Queiroz,“to have all this material cloud-based, it’s definitely one of the most important things for us. It would be hard to bring all this media physically from Argentina or wherever to Brazil. It will take us a lot of time.” With Senna shooting across locations, allowing production the capability of uploading their OCF and OSF resulted in no longer requiring shuttling hard drives on airplanes, creating LTO tapes, & managing physical shipments for their negative. And yes, you read that correctly; when utilizing MPS, we don’t require LTO tapes to be written unless there are title-specific needs.\n\nWith Senna beginning production back in June of 2023, our investment in MPS was still very early stages, and the tooling was considered beta. However, with the help, feedback, and partnership from this production, it was quickly realized that the investment was worth doubling down on. Since the early version used on Senna, Netflix has been spinning up ingest centers around the world, where drives can be dropped off, and within a matter of hours, all original camera files are uploaded into the Netflix ecosystem. While creating the ability to upload is not a novel concept, behind the scenes, it’s far from simple. Once a drive has been plugged in and our Netflix Footage Ingest application is opened, a validation is run, ensuring all expected media from set is on the drive. After media has been uploaded and checksums are run validating media integrity, all media is inspected, metadata is extracted, and assets are created for viewing/sharing/downloading with playable proxies. All media is then automatically backed up to a second tier of cloud-based storage for the final archive.\n\nTraditionally, if you wanted to check in with your post vendor on how things are going for each of these media management steps noted above, or whether or not you can clear on set camera cards if you haven’t gotten a completion notification, you would have to pick up the phone and call your vendor. For Senna, anyone who wanted visibility on progress, simply logged in to Content Hub and could see any activity in the Footage Ingest dashboard, as well as look up any information needed on past uploads.\n\nRemote monitoring media being uploaded and archived using the MPS Footage Ingest workflow\n\nWhile many services in MPS are available once media has been uploaded, Senna’s use of MPS focused on VFX. With Senna shooting a high volume of footage and the show having a high volume of VFX shots, according to Post Supervisor Gabriel Queiroz “Using MPS was basically a no-brainer, [having] used the tool before, I knew what it could bring to the project. And to be honest, with the amount of footage that we have, it was just so much material and with the amount of vendors we have, knowing that we would have to deliver all this footage to all these kinds of vendors, including outside of Brazil and to different parts of the world.”\n\nWith a traditional workflow, utilizing available resources in Latin America, VFX Pulls would have been done manually. This process is prone to human error and more importantly, for a show like Senna, too slow and would have resulted in different I/O methods for every vendor.\n\nIllustrating a traditional VFX Editor having to manage various I/O methods\n\nBy utilizing MPS, the Assistant Editor was able to log into Content Hub, upload an EDL, and have their VFX Pulls automatically transcoded, color files consolidated and all media placed into a Google Drive style folder built directly in Content Hub (called Workspaces). The VFX Editor was able to make any additional tweaks they wanted to the directory before farming out each of the shots to whichever vendor they were meant for. When it came time for the VFX vendors to then send shots back to editorial or DI, this was also done through MPS. Having one standard method for I/O for all VFX file sharing meant that Editorial and DI did not have to manage a different file transfer/workflow for every single vendor that was onboarded.\n\nIllustrating a more streamlined workflow for VFX vendors when using MPS\n\nAfter picture was locked and it was time for Senna to do their Online, the DI facility Quanta was able to utilize the Conform Pull service within MPS. The Conform Pull service allowed their team to upload an EDL, which ran a QC on all of the media from within their edit to ensure a smooth conform and then consolidated, trimmed, and packaged up all of the media they needed for the online. Since this early beta and thanks to learnings from many shows like Senna, advancements have been made in the system’s ability to match back to source media for both Conform and VFX Pulls. Rather than requiring an exact match between EDL and source OCF, there are several variations of fuzzy matching that can take place, as well as a current investigation in using one of our perceptual matching algorithms, allowing for a perceptual conform using computer vision, instead of solely relying on metadata.\n\nInside Senna with Content Hub Media Production Suite video\n\nConclusion\n\nThe Media Production Suite (MPS) represents a transformative leap in how we approach media production at Netflix. By embracing open standards, we have crafted a scalable solution that not only makes economic sense but also democratizes access to advanced production tools across the globe. This approach allows us to eliminate tedious tasks, enabling our teams to focus on what truly matters: creative storytelling. By fostering global collaboration and leveraging the power of cloud-based workflows, we’re not just enhancing efficiency but also elevating the quality of our productions. As we continue to innovate and refine our processes, we remain committed to breaking down barriers and unlocking the full potential of creative talent worldwide. The future of filmmaking is here, and with MPS, we are leading the charge toward a more connected and creatively empowered industry.", "label": "non_personal"}
{"title": "Behind the Scenes: Building a Robust Ads Event Processing Pipeline", "url": "https://netflixtechblog.com/behind-the-scenes-building-a-robust-ads-event-processing-pipeline-e4e86caf9249?source=collection_home---4------1-----------------------", "content": "Behind the Scenes: Building a Robust Ads Event Processing Pipeline Netflix Technology Blog 8 min read · May 9, 2025 -- 12 Listen Share\n\nKinesh Satiya\n\nIntroduction\n\nIn a digital advertising platform, a robust feedback system is essential for the lifecycle and success of an ad campaign. This system comprises of diverse sub-systems designed to monitor, measure, and optimize ad campaigns. At Netflix, we embarked on a journey to build a robust event processing platform that not only meets the current demands but also scales for future needs. This blog post delves into the architectural evolution and technical decisions that underpin our Ads event processing pipeline.\n\nAd serving acts like the “brain” — making decisions, optimizing delivery and ensuring right Ad is shown to the right member at the right time. Meanwhile, ad events, after an Ad is rendered, function like “heartbeats”, continuously providing real-time feedback (oxygen/nutrients) that fuels better decision-making, optimizations, reporting, measurement, and billing. Expanding on this analogy:\n\nJust as the brain relies on continuous blood flow, ad serving depends on a steady stream of ad events to adjust next ad serving decision, frequency capping, pacing, and personalization.\n\nIf the nervous system stops sending signals (ad events stop flowing), the brain (ad serving) lacks critical insights and starts making poor decisions or even fails.\n\nThe healthier and more accurate the event stream (just like strong heart function), the better the ad serving system can adapt, optimize, and drive business outcomes.\n\nLet’s dive into the journey of building this pipeline.\n\nThe Pilot\n\nIn November 2022, we launched a brand new basic ads plan, in partnership with Microsoft. The software systems extended the existing Netflix playback systems to play ads. Initially, the system was designed to be simple, secure, and efficient, with an underlying ethos of device-originated and server-proxied operations. The system consisted of three main components: the Microsoft Ad Server, Netflix Ads Manager, and Ad Event Handler. Each ad served required tracking to ensure the feedback loop functioned effectively, providing the external ad server with insights on impressions, frequency capping (advertiser policy that limits the number of times a user sees a specific ad), and monetization processes.\n\nKey features of this system include:", "label": "non_personal"}
{"title": "Google Play's billing system", "url": "https://developer.android.com/google/play/billing/", "content": "By Aug 31, 2025, all new apps and updates to existing apps must use Billing Library version 7 or newer. If you need more time to update your app, you can request an extension until Nov 1, 2025. Learn about Play Billing Library version deprecation\n\nGoogle Play's billing system is a service that enables you to sell digital products and content in your Android app, whether you want to monetize through one-time purchases or offer subscriptions to your services. Google Play offers a full set of APIs for integration with both your Android app and your server backend that unlock the familiarity and safety of Google Play purchases for your users.\n\nNote: Google Play's billing system is only for digital items. For physical goods and services, or other non-digital content, see the Google Pay SDK\n\nIntegration architecture\n\nThis section introduces the different functional modules that you can build and the APIs and libraries available to simplify the process.\n\nFigure 1. Diagram of a typical Google Play billing integration.\n\nYou can integrate Google Play's billing system with your Android app using the Play Billing Library. This library enables communication with the Google Play Services layer that provides the localized product offering available to each user in your app, as well as methods to handle other necessary user operations, like launching the purchase flow and handling its outcome.\n\nYou should also integrate Google Play's billing system with your server backend to create the necessary developer flows. This is essential to guarantee that your purchase management and cross-platform entitlements are efficient and secure. You can create this integration with the Subscriptions and in-app purchases API provided by the Google Play Developer API. The backend integration also leverages some Google Cloud platform tools.\n\nFigure 2. APIs and services provided by the Google Play Developer API.\n\nTerminology\n\nThis section lists and describes the high-level technologies and concepts that you might encounter when integrating Google Play's billing system into your app. Reference this list as you proceed through the integration guidance.\n\nTechnologies\n\nConcepts\n\nFlow . A flow shows the typical steps involved in a billing-related task. For example, a purchase flow outlines the steps involved when a user purchases your product. A subscription flow might show how a subscription transitions between states.\n\n. A flow shows the typical steps involved in a billing-related task. For example, a purchase flow outlines the steps involved when a user purchases your product. A subscription flow might show how a subscription transitions between states. Entitlement . When a user purchases an in-app product, they are then entitled to that product within your app. For one-time products, this means that the user should now have permanent access to the product. For subscriptions, this means that the user should have access while the subscription is active.\n\n. When a user purchases an in-app product, they are then entitled to that product within your app. For one-time products, this means that the user should now have permanent access to the product. For subscriptions, this means that the user should have access while the subscription is active. Product ID . The ID of a specific product type.\n\n. The ID of a specific product type. Purchase token . A string that represents a buyer's entitlement to a product on Google Play. It indicates that a Google user has paid for a specific product.\n\n. A string that represents a buyer's entitlement to a product on Google Play. It indicates that a Google user has paid for a specific product. Order ID. A string that represents a financial transaction on Google Play. An order ID is created every time a financial transaction occurs. This string is included in a receipt that is emailed to the buyer. You can use the order ID to manage refunds in the Order Management section of the Google Play Console. Order IDs are also used in sales and payout reports.\n\nNext steps\n\nTo begin integrating Google Play's billing system with your app and server backend, see the setup guide.", "label": "non_personal"}
{"title": "FM-Intent: Predicting User Session Intent with Hierarchical Multi-Task Learning", "url": "https://netflixtechblog.com/fm-intent-predicting-user-session-intent-with-hierarchical-multi-task-learning-94c75e18f4b8?source=collection_home---4------0-----------------------", "content": "FM-Intent: Predicting User Session Intent with Hierarchical Multi-Task Learning Netflix Technology Blog 7 min read · May 21, 2025 -- 4 Listen Share\n\nAuthors: Sejoon Oh, Moumita Bhattacharya, Yesu Feng, Sudarshan Lamkhede, Ko-Jen Hsiao, and Justin Basilico\n\nMotivation\n\nRecommender systems have become essential components of digital services across e-commerce, streaming media, and social networks [1, 2]. At Netflix, these systems drive significant product and business impact by connecting members with relevant content at the right time [3, 4]. While our recommendation foundation model (FM) has made substantial progress in understanding user preferences through large-scale learning from interaction histories (please refer to this article about FM @ Netflix), there is an opportunity to further enhance its capabilities. By extending FM to incorporate the prediction of underlying user intents, we aim to enrich its understanding of user sessions beyond next-item prediction, thereby offering a more comprehensive and nuanced recommendation experience.\n\nRecent research has highlighted the importance of understanding user intent in online platforms [5, 6, 7, 8]. As Xia et al. [8] demonstrated at Pinterest, predicting a user’s future intent can lead to more accurate and personalized recommendations. However, existing intent prediction approaches typically employ simple multi-task learning that adds intent prediction heads to next-item prediction models without establishing a hierarchical relationship between these tasks.\n\nTo address these limitations, we introduce FM-Intent, a novel recommendation model that enhances our foundation model through hierarchical multi-task learning. FM-Intent captures a user’s latent session intent using both short-term and long-term implicit signals as proxies, then leverages this intent prediction to improve next-item recommendations. Unlike conventional approaches, FM-Intent establishes a clear hierarchy where intent predictions directly inform item recommendations, creating a more coherent and effective recommendation pipeline.\n\nFM-Intent makes three key contributions:\n\nA novel recommendation model that captures user intent on the Netflix platform and enhances next-item prediction using this intent information. A hierarchical multi-task learning approach that effectively models both short-term and long-term user interests. Comprehensive experimental validation showing significant performance improvements over state-of-the-art models, including our foundation model.\n\nUnderstanding User Intent in Netflix\n\nIn the Netflix ecosystem, user intent manifests through various interaction metadata, as illustrated in Figure 1. FM-Intent leverages these implicit signals to predict both user intent and next-item recommendations.\n\nFigure 1: Overview of user engagement data in Netflix. User intent can be associated with several interaction metadata. We leverage various implicit signals to predict user intent and next-item.\n\nIn Netflix, there can be multiple types of user intents. For instance,\n\nAction Type: Categories reflecting what users intend to do on Netflix, such as discovering new content versus continuing previously started content. For example, when a member plays a follow-up episode of something they were already watching, this can be categorized as “continue watching” intent. Genre Preference: The pre-defined genre labels (e.g., Action, Thriller, Comedy) that indicate a user’s content preferences during a session. These preferences can shift significantly between sessions, even for the same user. Movie/Show Type: Whether a user is looking for a movie (typically a single, longer viewing experience) or a TV show (potentially multiple episodes of shorter duration). Time-since-release: Whether the user prefers newly released content, recent content (e.g., between a week and a month), or evergreen catalog titles.\n\nThese dimensions serve as proxies for the latent user intent, which is often not directly observable but crucial for providing relevant recommendations.\n\nFM-Intent Model Architecture\n\nFM-Intent employs a hierarchical multi-task learning approach with three major components, as illustrated in Figure 2.\n\nFigure 2: An architectural illustration of our hierarchical multi-task learning model FM-Intent for user intent and item predictions. We use ground-truth intent and item-ID labels to optimize predictions.\n\n1. Input Feature Sequence Formation\n\nThe first component constructs rich input features by combining interaction metadata. The input feature for each interaction combines categorical embeddings and numerical features, creating a comprehensive representation of user behavior.\n\n2. User Intent Prediction\n\nThe intent prediction component processes the input feature sequence through a Transformer encoder and generates predictions for multiple intent signals.\n\nThe Transformer encoder effectively models the long-term interest of users through multi-head attention mechanisms. For each prediction task, the intent encoding is transformed into prediction scores via fully-connected layers.\n\nA key innovation in FM-Intent is the attention-based aggregation of individual intent predictions. This approach generates a comprehensive intent embedding that captures the relative importance of different intent signals for each user, providing valuable insights for personalization and explanation.\n\n3. Next-Item Prediction with Hierarchical Multi-Task Learning\n\nThe final component combines the input features with the user intent embedding to make more accurate next-item recommendations.\n\nFM-Intent employs hierarchical multi-task learning where intent predictions are conducted first, and their results are used as input features for the next-item prediction task. This hierarchical relationship ensures that the next-item recommendations are informed by the predicted user intent, creating a more coherent and effective recommendation model.\n\nOffline Results\n\nWe conducted comprehensive offline experiments on sampled Netflix user engagement data to evaluate FM-Intent’s performance. Note that FM-Intent uses a much smaller dataset for training compared to the FM production model due to its complex hierarchical prediction architecture.\n\nNext-Item and Next-Intent Prediction Accuracy\n\nTable 1 compares FM-Intent with several state-of-the-art sequential recommendation models, including our production model (FM-Intent-V0).\n\nTable 1: Next-item and next-intent prediction results of baselines and our proposed method FM-Intent on the Netflix user engagement dataset.\n\nAll metrics are represented as relative % improvements compared to the SOTA baseline: TransAct. N/A indicates that a model is not capable of predicting a certain intent. Note that we added additional fully-connected layers to LSTM, GRU, and Transformer baselines in order to predict user intent, while we used original implementations for other baselines. FM-Intent demonstrates statistically significant improvement of 7.4% in next-item prediction accuracy compared to the best baseline (TransAct).\n\nMost baseline models show limited performance as they either cannot predict user intent or cannot incorporate intent predictions into next-item recommendations. Our production model (FM-Intent-V0) performs well but lacks the ability to predict and leverage user intent. Note that FM-Intent-V0 is trained with a smaller dataset for a fair comparison with other models; the actual production model is trained with a much larger dataset.\n\nQualitative Analysis: User Clustering\n\nFigure 3: K-means++ (K=10) clustering of user intent embeddings found by FM-Intent; FM-Intent finds unique clusters of users that share the similar intent.\n\nFM-Intent generates meaningful user intent embeddings that can be used for clustering users with similar intents. Figure 3 visualizes 10 distinct clusters identified through K-means++ clustering. These clusters reveal meaningful user segments with distinct viewing patterns:\n\nUsers who primarily discover new content versus those who continue watching recent/favorite content.\n\nGenre enthusiasts (e.g., anime/kids content viewers).\n\nUsers with specific viewing patterns (e.g., Rewatchers versus casual viewers).\n\nPotential Applications of FM-Intent\n\nFM-Intent has been successfully integrated into Netflix’s recommendation ecosystem, can be leveraged for several downstream applications:\n\nPersonalized UI Optimization: The predicted user intent could inform the layout and content selection on the Netflix homepage, emphasizing different rows based on whether users are in discovery mode, continue-watching mode, or exploring specific genres. Analytics and User Understanding: Intent embeddings and clusters provide valuable insights into viewing patterns and preferences, informing content acquisition and production decisions. Enhanced Recommendation Signals: Intent predictions serve as features for other recommendation models, improving their accuracy and relevance. Search Optimization: Real-time intent predictions help prioritize search results based on the user’s current session intent.\n\nConclusion\n\nFM-Intent represents an advancement in Netflix’s recommendation capabilities by enhancing them with hierarchical multi-task learning for user intent prediction. Our comprehensive experiments demonstrate that FM-Intent significantly outperforms state-of-the-art models, including our prior foundation model that focused solely on next-item prediction. By understanding not just what users might watch next but what underlying intents users have, we can provide more personalized, relevant, and satisfying recommendations.\n\nAcknowledgements\n\nWe thank our stunning colleagues in the Foundation Model team & AIMS org. for their valuable feedback and discussions. We also thank our partner teams for getting this up and running in production.\n\nReferences\n\n[1] Amatriain, X., & Basilico, J. (2015). Recommender systems in industry: A netflix case study. In Recommender systems handbook (pp. 385–419). Springer.\n\n[2] Gomez-Uribe, C. A., & Hunt, N. (2015). The netflix recommender system: Algorithms, business value, and innovation. ACM Transactions on Management Information Systems (TMIS), 6(4), 1–19.\n\n[3] Jannach, D., & Jugovac, M. (2019). Measuring the business value of recommender systems. ACM Transactions on Management Information Systems (TMIS), 10(4), 1–23.\n\n[4] Bhattacharya, M., & Lamkhede, S. (2022). Augmenting Netflix Search with In-Session Adapted Recommendations. In Proceedings of the 16th ACM Conference on Recommender Systems (pp. 542–545).\n\n[5] Chen, Y., Liu, Z., Li, J., McAuley, J., & Xiong, C. (2022). Intent contrastive learning for sequential recommendation. In Proceedings of the ACM Web Conference 2022 (pp. 2172–2182).\n\n[6] Ding, Y., Ma, Y., Wong, W. K., & Chua, T. S. (2021). Modeling instant user intent and content-level transition for sequential fashion recommendation. IEEE Transactions on Multimedia, 24, 2687–2700.\n\n[7] Liu, Z., Chen, H., Sun, F., Xie, X., Gao, J., Ding, B., & Shen, Y. (2021). Intent preference decoupling for user representation on online recommender system. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence (pp. 2575–2582).\n\n[8] Xia, X., Eksombatchai, P., Pancha, N., Badani, D. D., Wang, P. W., Gu, N., Joshi, S. V., Farahpour, N., Zhang, Z., & Zhai, A. (2023). TransAct: Transformer-based Realtime User Action Model for Recommendation at Pinterest. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (pp. 5249–5259).", "label": "non_personal"}
{"title": "Model Once, Represent Everywhere: UDA (Unified Data Architecture) at Netflix", "url": "https://netflixtechblog.com/uda-unified-data-architecture-6a6aee261d8d?source=collection_home---4------0-----------------------", "content": "Model Once, Represent Everywhere: UDA (Unified Data Architecture) at Netflix Netflix Technology Blog 15 min read · Jun 12, 2025 -- 23 Listen Share\n\nBy Alex Hutter, Alexandre Bertails, Claire Wang, Haoyuan He, Kishore Banala, Peter Royal, Shervin Afshar\n\nAs Netflix’s offerings grow — across films, series, games, live events, and ads — so does the complexity of the systems that support it. Core business concepts like ‘actor’ or ‘movie’ are modeled in many places: in our Enterprise GraphQL Gateway powering internal apps, in our asset management platform storing media assets, in our media computing platform that powers encoding pipelines, to name a few. Each system models these concepts differently and in isolation, with little coordination or shared understanding. While they often operate on the same concepts, these systems remain largely unaware of that fact, and of each other.\n\nAs a result, several challenges emerge:\n\nDuplicated and Inconsistent Models — Teams re-model the same business entities in different systems, leading to conflicting definitions that are hard to reconcile.\n\n— Teams re-model the same business entities in different systems, leading to conflicting definitions that are hard to reconcile. Inconsistent Terminology — Even within a single system, teams may use different terms for the same concept, or the same term for different concepts, making collaboration harder.\n\n— Even within a single system, teams may use different terms for the same concept, or the same term for different concepts, making collaboration harder. Data Quality Issues — Discrepancies and broken references are hard to detect across our many microservices. While identifiers and foreign keys exist, they are inconsistently modeled and poorly documented, requiring manual work from domain experts to find and fix any data issues.\n\n— Discrepancies and broken references are hard to detect across our many microservices. While identifiers and foreign keys exist, they are inconsistently modeled and poorly documented, requiring manual work from domain experts to find and fix any data issues. Limited Connectivity — Within systems, relationships between data are constrained by what each system supports. Across systems, they are effectively non-existent.\n\nTo address these challenges, we need new foundations that allow us to define a model once, at the conceptual level, and reuse those definitions everywhere. But it isn’t enough to just document concepts; we need to connect them to real systems and data. And more than just connect, we have to project those definitions outward, generating schemas and enforcing consistency across systems. The conceptual model must become part of the control plane.\n\nThese were the core ideas that led us to build UDA.\n\nIntroducing UDA\n\nUDA (Unified Data Architecture) is the foundation for connected data in Content Engineering. It enables teams to model domains once and represent them consistently across systems — powering automation, discoverability, and semantic interoperability.\n\nUsing UDA, users and systems can:\n\nRegister and connect domain models — formal conceptualizations of federated business domains expressed as data.\n\nWhy? So everyone uses the same official definitions for business concepts, which avoids confusion and stops different teams from rebuilding similar models in conflicting ways.\n\nCatalog and map domain models to data containers, such as GraphQL type resolvers served by a Domain Graph Service, Data Mesh sources, or Iceberg tables, through their representation as a graph.\n\nWhy? To make it easy to find where the actual data for these business concepts lives (e.g., in which specific database, table, or service) and understand how it’s structured there.\n\nTranspile domain models into schema definition languages like GraphQL, Avro, SQL, RDF, and Java, while preserving semantics.\n\nWhy? To automatically create consistent technical data structures (schemas) for various systems directly from the domain models, saving developers manual effort and reducing errors caused by out-of-sync definitions.\n\nMove data faithfully between data containers, such as from federated GraphQL entities to Data Mesh (a general purpose data movement and processing platform for moving data between Netflix systems at scale), Change Data Capture (CDC) sources to joinable Iceberg Data Products.\n\nWhy? To save developer time by automatically handling how data is moved and correctly transformed between different systems. This means less manual work to configure data movement, ensuring data shows up consistently and accurately wherever it’s needed.\n\nDiscover and explore domain concepts via search and graph traversal.\n\nWhy? So anyone can more easily find the specific business information they’re looking for, understand how different concepts and data are related, and be confident they are accessing the correct information.\n\nProgrammatically introspect the knowledge graph using Java, GraphQL, or SPARQL.\n\nWhy? So developers can build smarter applications that leverage this connected business information, automate more complex data-dependent workflows, and help uncover new insights from the relationships in the data.\n\nThis post introduces the foundations of UDA as a knowledge graph, connecting domain models to data containers through mappings, and grounded in an in-house metamodel, or model of models, called Upper. Upper defines the language for domain modeling in UDA and enables projections that automatically generate schemas and pipelines across systems.\n\nThe same domain model can be connected to semantically equivalent data containers in the UDA knowledge graph.\n\nThis post also highlights two systems that leverage UDA in production:\n\nPrimary Data Management (PDM) is our platform for managing authoritative reference data and taxonomies. PDM turns domain models into flat or hierarchical taxonomies that drive a generated UI for business users. These taxonomy models are projected into Avro and GraphQL schemas, automatically provisioning data products in the Warehouse and GraphQL APIs in the Enterprise Gateway.\n\nSphere is our self-service operational reporting tool for business users. Sphere uses UDA to catalog and relate business concepts across systems, enabling discovery through familiar terms like ‘actor’ or ‘movie.’ Once concepts are selected, Sphere walks the knowledge graph and generates SQL queries to retrieve data from the warehouse, no manual joins or technical mediation required.\n\nUDA is a Knowledge Graph\n\nUDA needs to solve the data integration problem. We needed a data catalog unified with a schema registry, but with a hard requirement for semantic integration. Connecting business concepts to schemas and data containers in a graph-like structure, grounded in strong semantic foundations, naturally led us to consider a knowledge graph approach.\n\nWe chose RDF and SHACL as the foundation for UDA’s knowledge graph. But operationalizing them at enterprise scale surfaced several challenges:\n\nRDF lacked a usable information model. While RDF offers a flexible graph structure, it provides little guidance on how to organize data into named graphs, manage ontology ownership, or define governance boundaries. Standard follow-your-nose mechanisms like owl:imports apply only to ontologies and don’t extend to named graphs; we needed a generalized mechanism to express and resolve dependencies between them.\n\nWhile RDF offers a flexible graph structure, it provides little guidance on how to organize data into named graphs, manage ontology ownership, or define governance boundaries. Standard follow-your-nose mechanisms like owl:imports apply only to ontologies and don’t extend to named graphs; we needed a generalized mechanism to express and resolve dependencies between them. SHACL is not a modeling language for enterprise data. Designed to validate native RDF, SHACL assumes globally unique URIs and a single data graph. But enterprise data is structured around local schemas and typed keys, as in GraphQL, Avro, or SQL. SHACL could not express these patterns, making it difficult to model and validate real-world data across heterogeneous systems.\n\nDesigned to validate native RDF, SHACL assumes globally unique URIs and a single data graph. But enterprise data is structured around local schemas and typed keys, as in GraphQL, Avro, or SQL. SHACL could not express these patterns, making it difficult to model and validate real-world data across heterogeneous systems. Teams lacked shared authoring practices. Without strong guidelines, teams modeled their ontologies inconsistently breaking semantic interoperability. Even subtle differences in style, structure, or naming led to divergent interpretations and made transpilation harder to define consistently across schemas.\n\nWithout strong guidelines, teams modeled their ontologies inconsistently breaking semantic interoperability. Even subtle differences in style, structure, or naming led to divergent interpretations and made transpilation harder to define consistently across schemas. Ontology tooling lacked support for collaborative modeling. Unlike GraphQL Federation, ontology frameworks had no built-in support for modular contributions, team ownership, or safe federation. Most engineers found the tools and concepts unfamiliar, and available authoring environments lacked the structure needed for coordinated contributions.\n\nTo address these challenges, UDA adopts a named-graph-first information model. Each named graph conforms to a governing model, itself a named graph in the knowledge graph. This systematic approach ensures resolution, modularity, and enables governance across the entire graph. While a full description of UDA’s information infrastructure is beyond the scope of this post, the next sections explain how UDA bootstraps the knowledge graph with its metamodel and uses it to model data container representations and mappings.\n\nUpper is Domain Modeling\n\nUpper is a language for formally describing domains — business or system — and their concepts. These concepts are organized into domain models: controlled vocabularies that define classes of keyed entities, their attributes, and their relationships to other entities, which may be keyed or nested, within the same domain or across domains. Keyed concepts within a domain model can be organized in taxonomies of types, which can be as complex as the business or the data system needs them to be. Keyed concepts can also be extended from other domain models — that is, new attributes and relationships can be contributed monotonically. Finally, Upper ships with a rich set of datatypes for attribute values, which can also be customized per domain.\n\nThe graph representation of the onepiece: domain model from our UI. Depicted here you can see how Characters are related to Devil Fruit, and that each Devil Fruit has a type.\n\nUpper domain models are data. They are expressed as conceptual RDF and organized into named graphs, making them introspectable, queryable, and versionable within the UDA knowledge graph. This graph unifies not just the domain models themselves, but also the schemas they transpile to — GraphQL, Avro, Iceberg, Java — and the mappings that connect domain concepts to concrete data containers, such as GraphQL type resolvers served by a Domain Graph Service, Data Mesh sources, or Iceberg tables, through their representations. Upper raises the level of abstraction above traditional ontology languages: it defines a strict subset of semantic technologies from the W3C tailored and generalized for domain modeling. It builds on ontology frameworks like RDFS, OWL, and SHACL so domain authors can model effectively without even needing to learn what an ontology is.\n\nUDA domain model for One Piece. Link to full definition.\n\nUpper is the metamodel for Connected Data in UDA — the model for all models. It is designed as a bootstrapping upper ontology, which means that Upper is self-referencing, because it models itself as a domain model; self-describing, because it defines the very concept of a domain model; and self-validating, because it conforms to its own model. This approach enables UDA to bootstrap its own infrastructure: Upper itself is projected into a generated Jena-based Java API and GraphQL schema used in GraphQL service federated into Netflix’s Enterprise GraphQL gateway. These same generated APIs are then used by the projections and the UI. Because all domain models are conservative extensions of Upper, other system domain models — including those for GraphQL, Avro, Data Mesh, and Mappings — integrate seamlessly into the same runtime, enabling consistent data semantics and interoperability across schemas.\n\nTraversing a domain model programmatically using the Java API generated from the Upper metamodel.\n\nData Container Representations\n\nData containers are repositories of information. They contain instance data that conform to their own schema languages or type systems: federated entities from GraphQL services, Avro records from Data Mesh sources, rows from Iceberg tables, or objects from Java APIs. Each container operates within the context of a system that imposes its own structural and operational constraints.\n\nA Data Mesh source is a data container.\n\nData container representations are data. They are faithful interpretations of the members of data systems as graph data. UDA captures the definition of these systems as their own domain models, the system domains. These models encode both the information architecture of the systems and the schemas of the data containers within. They provide a blueprint for translating the systems into graph representations.", "label": "non_personal"}
{"title": "Generative AI is intellectual sharecropping – Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2023/09/25/generative-ai-is-intellectual-sharecropping/", "content": "Replied to Digital sharecropping by Nicholas Carr ( roughtype.com ) One of the fundamental economic characteristics of Web 2.0 is the distribution of production into the hands of the many and the concentration of the economic rewards into the hands of the few. It’s a sharecropping system, but the sharecroppers are generally happy because their interest lies in self-expression or socializing, not in making money, and, besides, the economic value of each of their individual contributions is trivial. It’s only by aggregating those contributions on a massive scale – on a web scale – that the business becomes lucrative. To put it a different way, the sharecroppers operate happily in an attention economy while their overseers operate happily in a cash economy.\n\nI encountered this (old) analogy for social media platforms as digital sharecropping and thought it also fit generative AI. Generative AI companies steal our intellectual property then license it back to us. We can’t be compensated reasonably for our individual contributions to the model because they’ve stolen from so many of us and each individual’s work represents a miniscule portion of the entire model. Whatever we generate with their models can’t be copyrighted and used to make money for *us* without significant human contributions — but generated works are in direct competition with the creators whose works built the model. These powerful, well-funded companies want businesses to fire their employees and pay them instead, making businesses reliant on an opaque, unpredictable service that demands vast amounts of natural resources that may be in short — and shortening — supply.\n\nBut unlike social media, which rewards users emotionally rather than financially for their labor, creators aren’t getting anything out of having our work used to train generative AI models. And so we’re fighting back earlier in the cycle than with social media — maybe before it can become entrenched. AI evangelists speak as if the technology’s supremacy is inevitable, but that’s propaganda to get us to shut up and hand over our creations and our jobs.\n\nRecent AI shenanigans in the news:\n\nChatGPT caught giving horrible advice to cancer patients by Sharon Adarlo (Neoscope)\n\nWhy Silicon Valley’s biggest AI developers are hiring poets by Andrew Deck (rest of world)\n\nIowa Is Using ChatGPT to Take Out Banned Books by Alejandra Gularte (Vulture)\n\nGenerative AI at work", "label": "non_personal"}
{"title": "Generative AI and the Business Borg aesthetic – Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/02/generative-ai-and-the-business-borg-aesthetic/", "content": "This feels like a sister piece to Ed Zitron’s essay Era of the Business Idiots and Mandy Brown’s essay Toolmen. Fair warning, this is a 5000 word post; I’ve been working on this for weeks, pulling together what I’ve learned about generative AI and culture over the past two years, so I hope it is worth your time 😄 Bonus: it doubles as a playlist 🎶\n\n“‘Real power’ is achieved when a technology ‘[leaves] mythology and [enters] banality,'” Marion Fourcade and Kieran Healy quote Vincent Mosco in The Ordinal Society. We’ve had the mythology stage — the world tour with grandiose prophecies of imminent AGI — but now the race to normalize generative AI* is on: tech corporations are attempting to inure people to generative AI, an expression of the Business Borg aesthetic that currently carries a negative stigma outside of tech.\n\n*(My rule of thumb: if something is described as AI, it’s probably predatory and/or bullshit; if it’s described as machine learning, it probably does something useful. Not always true but a helpful predictor.)\n\nIn general, people like what we recognize better than what we don’t — we prefer cultural works we can categorize to the unfamiliar and undefinable — and we are facing an inescapable shock-and-awe barrage of genAI graphics across the web to inundate our synapses with uncanny synthetic renderings.\n\nCurrently, generative AI is shunned by many artists and writers, the traditional arbiters of good taste and culture, because it has been developed through the theft of their labor. But tech CEOs stand to make (even bigger) fortunes if they can convince people that genAI doesn’t signify bad taste, or make it seem like an irrevocable fact of life, like spam emails and text scammers. It’s being deployed upon us with the same lockstep corporate solidarity that forced us to pay fees for checked luggage on flights (younger folks, before 2008 your bag used to be included with your ticket! Stowing your carry-on wasn’t a competitive sport back in the day.).\n\nThe aesthetics of generative AI\n\n“I have become momentarily obsessed with scrolling down the homepage of the MetaAI tool and seeing the infinite feed of what people have been asking of The Machine. The outputs are horribly banal, but the requests are a weird window into THE (NORMIE) HUMAN SOUL AND ITS DESIRES: www.meta.ai”\n\n— Matt Muir, May 6, 2025 at 8:51 AM\n\n(via)\n\nBy its nature, generative AI produces the most likely image that meets the brief, which devolves to insipid Pictionary-style visual communication: chonky = cat, kebab = food = French chef’s hat. These graphics are iconographic pablum, the uninspired result of gee-whiz curiosity about a new “tool” (toy)(trap) in an environment that discourages personal taste and cultural literacy.\n\nOriginally, I wrote up why I find these particular graphics tacky and visually uninteresting, but realized that ultimately, what they look like doesn’t matter — it’s the beliefs beneath the appearance that matter. As corporate models are trained further, Generative AI will probably continue to get better, rendering more attractive and/or plausible outputs, but it won’t matter to me how good it gets because I reject the values it represents.\n\n(NB: I don’t want you to feel bad about yourself if you use generative AI, because there are myriad reasons to have experimented with it, including being forced to for your employment; I want you to recognize what it symbolizes when you enthusiastically use this technology today, and make an informed choice about whether those are values you wish to signal to others.)\n\n“What is wrong with a counterfeit is not what it is like, but how it was made.”\n\n— Harry G. Frankfurt, On Bullshit\n\nAesthetics are looks that signal values\n\nWitching Hour by Ladytron\n\nAn aesthetic is an expression of taste for shared values, commonly communicated through a distinct style. We think of aesthetics as surface appearance only, but the formation of an aesthetic’s conventions reflects the why and the how underneath the what. Just as the medium of a thing carries a message, so does its aesthetic. Aesthetics are visually and verbally encoded value systems.\n\nIdeology is a value system, independent of appearances. Aesthetics are the appearance of an ideology, which grow from its values. Subcultures form around ideologies, with members signaling their participation through aesthetics. “Good taste” is aesthetics that express those values.\n\nThough it’s now often reduced to a visual style, the Arts and Crafts movement of the late 1800s and early 1900s was cross-disciplinary and united instead by an ethos — namely, the nobility of craftsmanship*:\n\n“For it is not the material, but the absence of human labor, which makes the thing worthless, and a piece of terracotta, or plaster of paris, which has been wrought by the human hand, is worth all the stone and Carrara cut by machinery.” — John Ruskin, The Lamp of Truth from The Seven Lamps of Architecture\n\nAccording to the Arts and Crafts aesthetic, what is made should signal how it is made — the aesthetic’s value system weights how something is made to be as important, if not more, as what is made. Surface appearance is borne of the decisions this taste for craft produces.\n\n*(People might instead think of this William Morris quote as the quintessential perspective of Arts and Crafts — “Have nothing in your house that you do not know to be useful, or believe to be beautiful.” — and I suspect there’s a reason we’ve been taught to recall a philosophy centered on material possession instead of labor.)\n\nLooking at a Kelmscott Press book (William Morris’ printing company) reveals the printer’s respect for handicraft: they designed custom drop letters and frames, included original illustrations by fine artists like Edward Burne-Jones, used original type modeled after typefaces used by printers like Aldus Manutius in the early days of the printing press, printed with a richer black ink than was standard at the time, and employed a heavy hand in letterpress printing so the design and type would be impressed into the page. Thoughtful, intentional ornamentation is embraced. The artifact itself is a thing to be appreciated as much as its contents; these are books that honor the integrity of all creative workers involved in their production.\n\nEmily Amick applies this analytical lens to reveal the tradwife aesthetic’s underlying values:\n\nPrairie-core. Domestic bliss. Big sleeves. Bigger sourdough starters. And beneath it all, the subtle (or not-so-subtle) message: a woman’s place is in the kitchen. But not because she wants to be there, because that’s where God or her husband or some TikTok algorithm put her. The tradwife aesthetic promises comfort, but it delivers control. It’s softness as a strategy. It’s anti-feminism with a floral filter. It’s nostalgia for a time when women were property, romanticized by influencers who want brand deals from butter. […] The tradwife says: give up your autonomy and someone else will take care of everything.\n\nWe adopt aesthetics based on aspirational values\n\nJessica Cullen writes that (emphasis mine): “Aesthetics aren’t always about who we currently are but rather who we want to be.” People adopt an aesthetic to say something about themselves to others. We intentionally adopt a particular subculture’s aesthetics to convey our belonging and raise our status within the subculture. As Alec Leach puts it, “a lot of the modern taste economy is actually the status economy.”\n\nSometimes, we perceive only the surface level of an aesthetic, its appearance without the values — as Amick notes, the tradwife aesthetic “looks so damn pretty and nourishing. And we are tired.” — but whenever we adopt an aesthetic, we endorse (intentionally or not) the underlying values it represents. Amick continues dissecting the tradwife aesthetic and how it serves as conservative propaganda:\n\nWhat looks like innocent lifestyle content is actually part of an organized political movement designed to make patriarchy look cozy and appealing. Because politics is downstream from culture. The vibes that influence how we act and live.\n\nAesthetics matter more than ever because we act in accordance with our chosen aesthetics. As we get more and more of our “cultural” content on corporate silos, politics and purchases have subsumed a lot of cultural tastemaking. In this TikTok, Jamelle Bouie describes how politicians use aesthetic signaling to appeal to voters. Richard Sennett identifies that politicians come to embody “intentions, desires, values, beliefs, tastes – an emphasis which has again the effect of divorcing power from responsibility.”\n\nAnu Atluru argues that “Aesthetics are the modern units of cultural currency—stores of value and instruments of power, capable of appreciating and being monetized at scale. Owning an aesthetic is owning influence.” (emphasis mine)\n\nInterrogating the Business Borg aesthetic\n\nGlass Lux by Glass Lux\n\nThe Arts and Craft movement’s respect for labor inspired stylistic choices that highlighted craftwork as well as the decisions in what goods to produce and how. So how does the Business Borg aesthetic reveal its values?\n\nTo define the Business Borg aesthetic, I’m looking at:\n\nthe values I see expressed through generative media,\n\nI see expressed through generative media, the actions of the corporations behind generative AI,\n\nof the corporations behind generative AI, who buys into generative AI, and\n\nbuys into generative AI, and what else they like that reflects the same underlying values.\n\nWhat genAI is better at than a human artist is being cheap and instant.\n\nThe Business Borg aesthetic uses technology to signal wealth and power. Generative AI is not the only visual expression of the Business Borg aesthetic, just its most recognizable. The aesthetic is also signified visually by CGI-heavy blockbuster franchises, NFT art, and the Cybertruck; and in text by LinkedIn corporate thirst traps, X braggadocio, SEO word vomit, and generated “answers” to search results.\n\nAs political bedpartners, there is overlap between the Business Borg aesthetic and the MAGA aesthetic, but they’re distinct viewpoints. Both share dominance as a core value, decry empathy, center patriarchy, and admire performance — but MAGA also signals Judeo-Christian morality, traditional beauty standards / traditional gender roles, hyper masculinity / violence, and nationalism. MAGA borrows aesthetics from golden pasts, like Neo-Classical architecture, tradwives, and, as Kate Wagner brands it, Regional Car Dealership Rococo; Business Borg prefer the more modern tones of cyberpunk, solarpunk, and minimalism. Business Borg are regulatory libertarians who envision themselves as the rightful leaders of society, Kings of techno-city-states; MAGA are Christian nationalists who want to use the power of the state to impose their beliefs on others.\n\nElon is a Business Borg at heart but wielded a chainsaw to appeal to the more violent MAGA aesthetic. Zuckerberg is a Business Borg but got a MAGA makeover with masculine stubble and bling.\n\nWhy am I naming this after the Borg? Like Star Trek’s Borg, this is an aesthetic rooted in extractive consumption, assimilationist dominance, neo-colonial expansionism, self-righteous conviction, reductionist thinking, and proclamations of inevitability. It idolizes technology, often inspired by older science-fiction, and draws on cyberpunk aesthetics. The Silicon Valley Collective values groupthink and believes themselves superior to “the other.”\n\nWho embraces this aesthetic\n\nNot all users of Generative AI embrace the Business Borg aesthetic. I think a lot of people are experimenting with generative AI out of neophilic curiosity, productivity imperatives, nihilistic determinism, and corporate fiat. Aspiring billionaires adopt the Business Borg aesthetic to signal their belonging in the cohort of the techno-rich.\n\nGenAI evangelists seem to be the same type of person who was into passive income and supplements fifteen years ago, then Soylent and SEO ten years ago, then NFTs and macro diets five years ago, now genAI and X blue checks.\n\nThe Business Borg aesthetic combines tech-centered neophilia, a hustle mindset, an obsession with optimization, evangelical fervor, and fake-it-till-you-make-it showmanship.\n\nThe Elon fanclub are Business Borg. Ed Zitron’s Business Idiot shares a lot of characteristics with Business Borg (and may even be the same group, but I think feels a little different?).\n\nThe subculture emphasizes high profile demonstrations of “winning“ — using a $10k NFT as their X profile pic, bragging about SEO heists ripping off a competitor, generosity stunts a la Mr Beast, rubbing it in Miyazaki’s face that there will be thousands of shitty knockoffs vaguely reminiscent of his work across the web.\n\nBusiness Borg signal their busyness — and importance — by broadcasting how little they sleep, how much they work, and how little they read. The only fiction they like is (old) sci-fi because they read it as non-fiction, not fiction — a source of “inspiration” stripped of context and commentary. Using GenAI signals their adoption of cutting edge technology, the synthetic smoothness emphasizing its nonhuman origin. They care a lot about IQ, a supposedly impartial measure of intelligence that rewards their backgrounds and thinking styles, and idolize “genius.” They’re not actually neurodivergent, but play on stereotypes of autistic savants to cover for their pathological greed and lack of empathy.\n\nTheir visual and linguistic taste is mid because taste is not valued in the Business Borg aesthetic. In fact, there’s a certain pride in prompting things without having any skill, an almost gleeful snub to the perceived cultural gatekeepers — artists and writers and other creative workers — whose opinions the Business Borg disrespect because they believe that authority derives from money, not knowledge. They believe artists have wasted their time learning skills and developing taste. Academics have wasted their time studying things when information is just a click away. Business Borg don’t care about anything besides making money, and don’t care much how they spend it because the point is to have it, and show off that they have it.\n\nInto The Water by Ritual Howls\n\nGenAI True Believers often resemble the CEOs at the head of tech companies: wealthy, male, and white. These are also the people who are least at threat from the widespread use of generative AI, which reinforces racial and gender stereotypes and purports neutrality while serving up right-wing biases and corporate and foreign propaganda. Audrey Watters points out:\n\n“Computing (in general and ed-tech specifically) has long been the bastion of white male privilege; and while there had been efforts to change that – in pipelines and on panels and whatnot – AI is clearly a re-entrenchment of that power, explicitly so with the Trump Administration’s dismantling of civil rights protections, echoed by the tech industry’s dismantling of its own DEI initiatives.”\n\nAnil Dash describes “AI-first” as this year’s “Return to Office.” Managers didn’t care about in-office culture until it was made clear that workers could carry on just fine without them; managers care about AI only insomuch as it permits controlling — and firing — workers.\n\nWhat the Business Borg aesthetic represents is more important than its appearance; it represents the dominance of ordinal thinking and the ability of moneyed power to do as it wishes without regard to law or morality — in short, the hierarchical worldview that some people are better than others and that their preferences trump their lesser’s needs.\n\n“We will add your biological and technological distinctiveness to our own. Your culture will adapt to service us. Resistance is futile.” — the Borg\n\nValues driving the Business Borg aesthetic\n\nTeri Kanefield breaks down Leor Zmigrod’s book on ideology, explaining that “All ideologies seek a utopia.” The Business Borg utopia puts billionaires and their ilk high atop society, in control via the technology they own.\n\nCore values I see uniting the Business Borg aesthetic are:\n\nonly the output matters efficiency is king quantity over quality appearance trumps reality “progress” cannot be stopped\n\nValue: Only the output matters\n\nGenerative AI is being marketed to businesses as a low-cost replacement for workers that cuts steps — and collaboration — out of the process. This is a box-checking culture; all that matters is that an email was sent, a presentation was created, the newspaper had a summer reading insert, no matter the books on it were imaginary.\n\nFoundational beliefs\n\nprocess does not add value and wastes time\n\nthe world is reducible to data , and every question has one objectively correct answer\n\n, and every question has communication and collaboration are a waste of time (“email jobs”)\n\n(“email jobs”) experience is irrelevant\n\nOutcome: Tech reduces the complex to input and output\n\nIn contrast with the Arts and Crafts movement, the Business Borg aesthetic actively conceals human labor and venerates the wisdom of the machine. Generative text and graphics simulate a performance of human-less — cost-less — automation. Generative answers encourage a reliance on the machine to synthesize on one’s behalf — and it doesn’t matter to search engines that the “answers” their AI has provided cite sources incorrectly.\n\nHumans are perceived as sources of inefficiency under the Business Borg ideology, because they must be compensated in accordance with their skills and how much time they spend working. Generating material is rooted in devaluing both skill and process. The invented summer reading list was the result of forcing a single contractor to prepare an impossible quantity of work; generating content was the only way for the poor bloke to produce the content on budget. No one reviewed it, because Business Borg only care that the product exists.\n\nEd Zitron describes the evolution of the Business Idiot, personified by middle managers who are completely dissociated from the product they’re selling and explicitly do not do work (emphasis mine):\n\n[Business Idiots] see every part of our lives as a series of inputs and outputs. They boast about how many books they’ve read rather than the content of said books, about how many hours they work (even though they never, ever work that many), about high level they are in a video game they clearly don’t play, about the money they’ve raised and the scale they’ve raised it at, and about how expensive and fancy their kitchen gadgets are. Everything is dominance, acquisition, growth and possession over any lived experience, because their world is one where the journey doesn’t matter, because their journeys are riddled with privilege and the persecution of others in the pursuit of success. These people don’t want to automate work, they want to automate existence. They fantasize about hitting a button and something happening, because experiencing — living! — is beneath them, or at least your lives and your wants and your joy are.\n\nValue: Efficiency is king\n\nGenerative AI produces endless content for low cost. Corporations are using Generative AI as an excuse to lay off workers and intensify the jobs of those remaining.\n\nReady Aim Fire (Owl Vision Remix) [Single] by Blue Stahli\n\nFoundational beliefs\n\nthe more mechanized a process is, the more efficient it becomes because humans are naturally inefficient\n\nOutcome: GenAI performs “efficiency”\n\nGenerative AI need not actually reduce work or cost to represent efficiency when mechanization is always favored over people. The Business Borg aesthetic perceives automation as efficient — hence situations where workers are paid to simulate chatbots simulating human agents on customer service platforms, Microsoft devs handhold Copilot, and GM’s Cruise “autonomous” taxis needed remote human intervention every 4-5 miles!!!\n\nEfficiency is a code word for shareholders, just like “cost-cutting,” who know that these phrases mean putting the boot on workers’ necks for short-term profits. This efficiency aesthetic is used to justify outrageously profitable companies continuing to slash workers *cough Microsoft* It plays out as Hollywood demolishing the screenwriter profession to save a buck on writing rooms and self-cannibalizing the development of future acting talent by forcing extras to be body scanned so they can be reproduced by AI.\n\nJeremy comments on the use of genAI in coding, noting that it’s justified by claims like “working code wins” — as in, what it looks like under the hood and how it’s constructed don’t matter. I’m not a coder, but I’ve seen enough HTML produced by PageMaker and other CMSs to be skeptical of the quality of any generated code — is genAI producing the coding equivalent of tables for web layout? 🤔 The Business Borg aesthetic accepts mediocrity without understanding; easy and fast is always best. Good enough is always good enough.\n\nValue: Quantity over quality\n\nGenerative AI is billed as a good-enough tool that will speed up production. Cory Doctorow writes of the managerial push to automate with AI:\n\nThe point of using automation to weaken labor isn’t just cheaper products – it’s cheaper, defective products, inflicted on the unsuspecting and defenseless public who are no longer protected by workers’ professionalism and pride in their jobs.\n\nEnormous by LLgL TNDR\n\nFoundational beliefs\n\nprofit today trumps tomorrow’s concerns — someone else will have to fix ‘that problem’\n\n— someone else will have to fix ‘that problem’ money is authority — individual rankings can be quantified by wealth\n\n— individual rankings can be quantified by wealth anything that cannot be quantified is not important\n\nOutcome: Algorithms produce culture-like media, not culture\n\nGenAI and corporate web algorithms are intended to absorb attention like black absorbs light; they are designed to maximize engagement at minimal cost. Spotify benefits from degradation of culture. Netflix is a chum machine, built to spew content that people watch in the background. Kyle Raymond Fitzpatrick laments that “So much of culture is edgeless and soft, intended for us to astroglide through it without any friction or doubt as we half-watch in 1.5x speed, to consume as if we really are incapable of critical thoughts, all to appeal to everyone and no one at the same time.” Internet Age capitalism produces entertainment that is culture-like, writes Nicholas Carr:\n\n“What’s really being tested here is human taste. Will we accept a simulacrum of a work of art or craft as a satisfactory substitute for the real thing?”\n\nSo long as people accept cheap low-quality cultural media, businesses have little incentive to pay for higher quality. Generative AI becomes an attack on culture because it drowns out human-made art and writing so it’s impossible to find amidst the Great Social Media Garbage Patch. Aidan Walker describes all this as ‘ slop capitalism ‘: “an economic and cultural system in which the primary product is slop and the primary activity is the destruction of value rather than its creation.”\n\nValue: Appearances trump reality\n\nGenerative AI produces plausible graphics that we interpret as real-adjacent and plausible combinations of text that we interpret as communication.\n\nFoundational Beliefs\n\nperformance of dominance builds and reinforces real power\n\nOutcome: GenAI supersedes reality with performance and symbolism\n\nThe Business Borg aesthetic celebrates audacious performances of infinite wealth and indefinite power: adopters and evangelists for genAI also embrace filling streets with “self-driving” cars over the protests of residents and first responders, raining space debris onto inhabited areas from slapdash rocket ships, paying a fortune for a banana taped to a wall as conceptual art and eating it, paying women to have their IVF babies so they can seem fertile without fucking. While the culture at large has shifted towards inconspicuous consumption, luxury that requires knowledge to see, Business Borg signal their wealth blatantly.\n\nA repeating theme of the Business Borg aesthetic is replacing reality with life-like hyperreality: the simulation of conversation and connection with chatbots, the renderings of war and disaster mimicking photojournalism to push political narratives, the “resurrected” extinct species, the green screen action sequences that don’t track. A photograph of reality may seem less real than a generated image if it does not abide by our expectations. (See also: reading “human vibes” into LLM responses)\n\nLook at the snoozefest kayfabe of modern MMA: it’s as much about the smack talk at weigh-ins as the fights themselves, athletes winning by points for “controlling the octagon” instead of fighting to win by KO or submission — look no further than that embarrassment of a so-called fight between Mike Tyson and the YouTuber, who danced around an old man till he got tired so he could win by decision and say he’d beaten a legend 😴\n\nEffective altruism performs charity that can never be disproven, despite its claims of data-driven decisions, because it pretends to think at such long-term scales that known, existing suffering pales in comparison to the imagined future suffering they claim to be protecting against. It cosplays rationalism. Wannabe Foundation shit.\n\nThe Business Borg aesthetic is expressed as cheap cruft disguised as something of substance: words that look like writing, but are not; images that look like art, but are not; fights that look fighting, but are not.\n\nAs Ed Zitron sums up, it’s a “symbolic economy:”\n\nThe sweeping changes we’ve seen, both in our economy and in our society, has led to an unprecedented, gilded age of bullshit where nothing matters, and things — things of actual substance — matter nothing.\n\nTraditionally, we have been wary of those wearing a mask, explains Dan Fox in Pretentiousness, suspecting they are something other than what they present themselves as — but authenticity has faded as a cultural value. Business Borg embrace performance as more true than reality; what someone wants to be is more important than what they are now. What a technology could be is more important than what it currently is.\n\nValue: “Progress” cannot be stopped\n\nThe rapacious assimilation of copyrighted material to train models, the dismissal of AI’s environmental cost and induced demand, and PR campaign reorienting the conversation around AI to its possible future harms as a distraction from the harms happening today all build from the same foundation: theoretically, scaling towards the pursuit of AGI — but more likely securing the funding to inextricably embed AI into our society and infrastructure.\n\nFoundational Beliefs\n\ntechnology always represents progress\n\nthe ends justify the means\n\nOutcome: GenAI is full speed ahead, no matter what\n\nAdvocates of generative AI are pursuing a brute force, fear-mongering approach to deregulation and preventing regulation. They are defying legality in obtaining training data while declaring the technology a fait accompli: resistance is futile. Their web crawlers ignore robots.txt, breaking the common courtesy of the web. They dismiss complaints about bias, claiming that they can fix that in the future. (And what incentive will there be to ever go back and fix it, once we can’t avoid using it?) They’re hoping enough of us will get hooked on it — and government and corporations will integrate enough of it too deeply in their processes — to allow their legally-suspect models to be shut down.\n\nRyan Broderick observes:\n\nWhat the AI arms race has actually done is codified and automated all of the failures of the previous internet era. Extremism, misinformation, harassment, non-consensual sexual material, and scams — all the content that tech companies promised to fix, but never could at scale — are now trapped in some AI’s black box of data.\n\nAGI, which has been “three to five years away” for years, is a Bezzle. As Cory Doctorow quotes JK Gabraith, a Bezzle is “the magic interval when a confidence trickster knows he has the money he has appropriated but the victim does not yet understand that he has lost it.” John Kay expands that “The joy of the bezzle is that two people – each ignorant of the other’s existence and role – can enjoy the same wealth.”\n\nScams and pyramid schemes are seductive to Business Borg, like Elizabeth Holmes and Sam Bankman-Fried, because the only things they value are money and power, not making things that actually work (see: Cybertrucks falling apart, SpaceX rockets exploding); the trick is not to get caught. The goal is to surf the edge of profit as long as possible.\n\nRight now the whole stock market is bloated by outrageous NVIDIA, Google, Microsoft and Meta valuations based on the potential of generative AI to create a new “essential” utility, a service that everyone will need to subscribe to, forever — and so many people have bought into the grift so hard they’ll do anything they can to make it a success, or at least rake in the cash for as long as they can. Then when the bubble pops, the corporations will get bailed out on the taxpayer dollar, while we workers resign ourselves to working until we die since we’ve privatized retirement 🙃\n\nGenAI is being deployed to control\n\nArtists are under attack, culturally and economically, so it is only fair that they point out the quintessential thing that artists offer that generative AI cannot: taste. Professional (and amateur) artists have devoted a great deal of time to developing their taste. The oligarchs who run Silicon Valley are steeped in their own rightness and devalue anything that isn’t their expertise; if they don’t know about it, it must not be important. To the Business Borg, taste is not an essential component of production.\n\nMandy Brown describes how genAI is used to undercut expertise (emphasis mine):\n\nIt’s instructive that one of the mechanisms for perpetuating this ideology are chattering bots that speak both fact and falsehood in the same servile and confident tone, their makers unconcerned with the difference. In fact, their makers seem entirely concerned with obviating that difference, with disappearing distinctions between knowledge and ignorance, without which truth becomes entirely a product of power. […] [I]f those in power cannot prove that a great many people are already inferior then they will bring that inferiority about by forcing them to use a tool that diminishes their intellectual and creative capacity.\n\nBusiness Borg like generative AI because it grants them cultural power that they have not been able to dominate on their own. They lack skill, so they devalue skill. They need content, so they make an infinite content machine and conscript users as unwitting factory workers to provide free labor. The relentless promotion of GenAI is an attempt by corporations to capture cultural value by siphoning off value from human-made aesthetics. Generative AI is billionaires punching down on artists and the working class.\n\n[…] I think Miyazaki’s style is still valuable But it is now, in a day, valuable in a different way. What was once valuable in the awareness of painstaking labor, beautiful stories, and coherent aesthetic across the previous two qualities. PLUS our reception to it. Is now valuable PURELY in our reception to, and reproduction of, the aesthetic. […] — Reggie James (@HipCityReg) March 27, 2025\n\nGenerative AI has intentionally been molded to attack artists and diminish cultural literacy. Aidan Walker argues (read this whole piece if you liked my post):\n\nAI doesn’t have to be an antagonist to schools, work, and civil society — they’ve just designed and trained it that way… There could be guardrails in place, they could pay the producers of their training data, they could give the people a say in how the models are made and deployed — we could do a thousand things differently than the way they’re being done now.\n\nGenerative AI — both imagery and text — is inextricable from the corporate vision for its use: a world in which workers are powerless and worthless, replaced by “free” generated material. Corporate GenAI cannot be separated from the purpose for its use or the billionaires and billionaire-wannabes who shill for it. The Business Borg aesthetic imbues a sheen of venality.\n\nFurther reading:\n\nGenAI is Our Polyester by W. David Marx\n\nEconomics & labor rights in AI skepticism by Henry from online\n\nYou don’t hate AI; You hate… : a collection by Mita Williams\n\nDispatch from the Trenches of the Butlerian Jihad by ADH\n\nThe other way the [Butlerian Jihad] metaphor is proving apt is the deep-seated, almost spiritual nature of anti-AI sentiment. It’s not just more Luddism. Many people — though hardly all, given the popularity of AI products — sense that there is something grotesque about these simulacra, the people who push them on us, this whole affair. That aversion to the technological profane holds even when various stated objections to AI are supposedly addressed or nitpicked to death.\n\nSee also:\n\nWe need solidarity across creative industries", "label": "non_personal"}
{"title": "Generated content is an invasive species in the online ecosystem – Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2024/01/09/generated-content-is-an-invasive-species-in-the-online-ecosystem/", "content": "Invasive species disrupt ecosystems because they did not evolve in balance with the other species. Native species have adapted to fill specific niches, but the constraints they’ve accepted to fit that niche in the ecosystem do not also bind invasive species. Not limited by the same factors, they reproduce faster and crowd out the native species. Time and again, we’ve seen invasive species introduced to an ecosystem outcompete the more specialized native species, sometimes even driving them to extinction.\n\nLikewise, generated imagery and text is not bound by human limitations of productivity. As generated material rapaciously populates the Internet, human-created artworks will be outcompeted by generated graphics on social media platforms by virtue of volume.\n\nAnd corporations are also trying to argue that their products should not be bound by the same legalities that human artists and writers are bound by. Their products only work with copyrighted material, and that means it’s only economically viable if they steal the training data. Like invasive species, they don’t play by the same rules: the rest of us peons must wait 95 years to play with fucking Steamboat Willie, but they get to gobble down anything they want for free instantly and use it to (try to) drive us out of work.\n\nLet’s starve out this species invasion before it collapses our information ecosystem ✊\n\nSee also:\n\nA 21st century collage tool\n\nWe need solidarity across creative industries\n\nGenerative AI is intellectual sharecropping", "label": "non_personal"}
{"title": "Letter to Planning Commission re: upzoning in Juanita – Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/12/letter-to-planning-commission-re-upzoning-in-juanita/", "content": "Two large sites in the northern part of my city are being proposed for upzoning to permit taller buildings, reduced parking requirements, as well as other changes. In general the changes seem good, but there is one proposed change that is concerning: allowing townhomes instead of apartments or condos on some large sites that ought to be developed as densely as possible (link to packet). There is a great deal of community opposition to these upzones, so I sent in a supportive note to the Planning Commission in advance of their evening meeting.\n\nIn support of higher density at Michael’s and Goodwill sites\n\nTo the Kirkland Planning Commission:\n\nI am writing in favor of proposed zoning amendments to the Michael’s and Goodwill sites to allow higher density, taller building heights, and reduced parking requirements. These rare anchor sites offer great potential to make the neighborhood denser and more walkable.\n\nI do not support allowing townhomes in this area — along arterial routes with transit access, with many goods and services within walking or biking distance, we ought to support as much housing density as feasible. Kirkland and the greater Seattle area are in desperate need of more housing, so I urge you to use the code to require higher densities in the BC1 zone. If we do not push for higher density where it makes sense — and it makes a great deal of sense here — then the overall amount of additional housing Kirkland can accommodate will be much lower than the need. Please require a minimum density for ADUs in this area as in Alternatives 1 or 2 from Appendix 5.", "label": "non_personal"}
{"title": "AI transformations for sustainability", "url": "https://blogs.microsoft.com/on-the-issues/2025/01/16/ai-transformations-for-sustainability/", "content": "Today, Microsoft published a new paper, Accelerating Sustainability with AI: Innovations for a Better Future. You can read the foreword below and explore the paper in its entirety.\n\nThroughout history, societal transformations have been driven by the emergence of general-purpose technologies that reshaped entire economies, industries, and ways of life.\n\nThe steam engine, the printing press, electricity, and the internet have each marked pivotal social and economic shifts, leading to lasting changes in how we live and work. Today, AI stands as the latest—and potentially most powerful—general-purpose technology, offering an unprecedented opportunity to drive the societal transformations we urgently need to achieve the world’s sustainability goals.\n\nIn 2023, we published Accelerating Sustainability with AI: A Playbook, in which we highlighted that AI has three game-changing capabilities that make it an essential tool for accelerating sustainability. AI can enhance our ability to predict and optimize complex systems, accelerate the development and deployment of sustainable solutions, and empower the workforce to learn and achieve more—equipping society with the means to drive sustainability progress at a speed and scale previously beyond reach.\n\nOver the last year, we have seen the potential of AI for sustainability in action, empowering the world with new tools for tackling the climate crisis and sustainability challenges more broadly. For example, earlier this year, Microsoft collaborated with Pacific Northwest National Laboratory to use AI in discovering a new battery material requiring less lithium—a breakthrough achieved in weeks rather than the years that traditional research and development would have required. Reducing lithium dependence is crucial to decarbonization, as global demand for lithium is projected to outpace supply, potentially limiting the growth of the energy storage systems needed for the shift to electrification and renewable energy.\n\nAI’s transformative capabilities extend far beyond sustainability, the world has an opportunity to harness AI to enhance both productivity and prosperity. By enabling smarter resource use, optimizing systems for efficiency, and fostering innovations in carbon-free energy and conservation, the AI economy also has the potential to advance both economic growth and environmental stewardship.\n\n\n\nAt Microsoft, we believe the world needs AI that is broadly accessible and trustworthy. This includes addressing the sustainability challenges associated with this technology. The five plays outlined in our AI and sustainability playbook reflect the targeted actions needed to unlock the full potential of AI for accelerating sustainability progress globally.\n\nAcross our sustainability work, we regularly assess our progress and adjust our strategies for greater impact. One lesson from this last year is that minimizing the sustainability impact of AI operations requires more than minimizing resource use in datacenter operations; it also requires supporting the communities where datacenters are located and expanding access to zero-carbon electricity. Global electricity demand is growing rapidly, at an estimated average annual rate of 3–4%. While AI currently consumes less than 0.3% of global electricity demand—and, according to the International Energy Agency (IEA), is expected to remain a small portion in the decade ahead—rapid growth in certain regions can strain local grids.\n\nIn light of these realities, we have updated the third play of our playbook to include enhancing access to carbon-free energy on electricity grids and supporting local communities where we operate datacenters. In support of these expanded goals, we are expanding our effort to build and operate digital infrastructure that addresses societal challenges and creates benefits for communities.\n\nThis report highlights Microsoft’s innovations and actions to advance each of the five plays. Examples of our efforts across the five plays include:\n\nPlay 1: Invest in AI for sustainability\n\nMicrosoft is investing in building AI tools, such as MatterGen and MatterSim, which enable researchers to design and test materials with tenfold greater accuracy and significantly faster performance, while also predicting global weather and atmospheric processes with increased accuracy and at speeds up to 5,000 times greater than current forecasting systems. We are also building AI-enabled tools to empower stakeholders to more effectively and efficiently manage agriculture and water resources and to expedite the licensing process for carbon-free electricity.\n\nPlay 2: Develop digital and data infrastructure for the inclusive use of AI for sustainability\n\nWe are creating tools to fill critical data gaps, which can enhance AI models for better measuring and predicting complex systems such as biodiversity and climate. For instance, SPARROW captures images and acoustic recordings to gather data on biodiversity and ecosystem health in remote areas. Additionally, we are partnering with G42 on a $1 billion digital ecosystem initiative in Kenya.\n\nPlay 3: Minimize resource use, expand access to carbon-free electricity, and support local communities\n\nMicrosoft is innovating datacenter development with low-carbon materials like cross-laminated timber. Through an agreement with Brookfield, we aim to add 10.5 gigawatts (GW) of renewable energy to the grid.\n\nPlay 4: Advance AI policy principles and governance for sustainability\n\nWe advocated for policies that accelerate grid decarbonization, including Federal Energy Regulatory Commission (FERC) transmission rules and provisions in the Inflation Reduction Act in the United States. In addition, we continue to advance AI governance within Microsoft and globally.\n\nPlay 5: Build workforce capacity to use AI for sustainability\n\nMicrosoft Philanthropies’ Skills for Social Impact program trained over 14 million people in digital and AI skills to support a workforce ready to deploy AI for sustainability. As the window for achieving global sustainability goals narrows, the urgency for action intensifies. The world needs every tool at its disposal, and the potential of AI to accelerate sustainability is already being realized. Sustainability is not a journey that can be taken alone, and unlocking the full potential of AI for climate progress requires continued partnerships to combine expertise, technology, and innovation. As we continue to explore the ways AI can advance sustainability, we invite others to join us in this journey.\n\nRead the full report at https://aka.ms/AcceleratingSustainabilitywithAI2025\n\nTags: AI, Environmental Sustainability, Innovation, Responsible AI, sustainability, Workforce", "label": "non_personal"}
{"title": "The value of AI: How Microsoft’s customers and partners are creating differentiated AI solutions to reinvent how they do business today", "url": "https://blogs.microsoft.com/blog/2025/01/28/the-value-of-ai-how-microsofts-customers-and-partners-are-creating-differentiated-ai-solutions-to-reinvent-how-they-do-business-today/", "content": "Organizational leaders in every industry around the world are evaluating ways AI can unlock opportunities, drive pragmatic innovation and yield value across their business. At Microsoft, we are dedicated to helping our customers accelerate AI Transformation by empowering human ambition with Copilots and agents, developing differentiated AI solutions and building scalable cybersecurity foundations. At Microsoft Ignite we made over 100 announcements that bring the latest innovation directly to our customers and partners, and shared how Microsoft is the only technology leader to offer three distinct AI platforms for them to build AI solutions:\n\nCopilot is your UI for AI, with Copilot Studio enabling low-code creation of agents and extensibility to your data. Azure AI Foundry is the only AI app server for building real-world, world-class, AI-native applications. Microsoft Fabric is the AI data platform that provides one common way to reason over your data —no matter where it lives.\n\nAll three of these platforms are open and work synchronously to enable the development of modern AI solutions; and each is surrounded by our world-class security offerings so leaders can move their AI-first strategies forward with confidence.\n\nAs we look ahead to what we can achieve together, I remain inspired by the work we are doing today. Below are a handful of the many stories from the past quarter highlighting the differentiated AI solutions our customers and partners are driving to move business forward across industries and realize pragmatic value. Their success clearly illustrates that real results can be harnessed from AI today, and it is changing the way organizations do business.\n\nTo power its industrial IoT and AI platform, ABB Group leveraged Microsoft Azure OpenAI Service to create Genix Copilot: a generative AI-powered analytics suite aimed at solving some of the most complex industrial problems. The solution helps customers analyze key functions in their operations —such as asset and process performance, energy optimization and emission monitoring — with real-time operational insights. As a result, customers are seeing up to 35% savings in operations and maintenance, and up to 20% improvement in energy and emission optimization. ABB also saw an 80% decrease in service calls with the self-service capabilities of Genix Copilot.\n\nServing government healthcare agencies across the US, Acentra Health turned to Microsoft to help introduce the latest AI capabilities that maximize talent and cut costs in a secure, HIPAA-compliant manner. Using Azure OpenAI Service, the company developed MedScribe — an AI-powered tool reducing the time specially trained nursing staff spend on appeal determination letters. This innovation saved 11,000 nursing hours and nearly $800,000, reducing time spent on each appeal determination letter by about 50%. MedScribe also significantly enhanced operational efficiency, enabling nurses to process 20 to 30 letters daily with a 99% approval rate.\n\nTo ease challenges for small farmers, Romanian agribusiness group Agricover revolutionized access to credit by developing MyAgricover. Built with help from partner Avaelgo, the scalable digital platform utilizes Microsoft Azure, Azure API Management and Microsoft Fabric to automate the loan process and enable faster approvals and disbursements. This has empowered small farmers to grow their businesses and receive faster access to financing by reducing loan approval time by 90 percent — from 10 working days to a maximum of 24 hours.\n\nBuilding on its status as a world-class airline with a strong Indian identity, Air India sought ways to enhance customer support while managing costs. By developing AI.g, one of the industry’s first generative AI virtual assistants built on Azure OpenAI Service, the airline upgraded the customer experience. Today, 97% of customer queries are handled with full automation, resulting in millions of dollars of support costs saved and improved customer satisfaction — further positioning the airline for continued growth.\n\nBMW Group aimed to enhance data delivery efficiency and improve vehicle development and prototyping cycles by implementing a Mobile Data Recorder (MDR) solution with Azure App Service, Azure AI and Azure Kubernetes Service (AKS). The solution achieved 10 times more efficient data delivery, significantly improved data accessibility and elevated overall development quality. The MDR monitors and records more than 10,000 signals twice per second in every vehicle of BMW’s fleet of 3,500 development cars and transmits data within seconds to a centralized cloud back end. Using Azure AI Foundry and Azure OpenAI Service, BMW Group created an MDR copilot fueled by GPT-4o. Engineers can now chat with the interface using natural language, and the MDR copilot converts the conversations into KQL queries, simplifying access to technical insights. Moving from on-premises tools to a cloud-based system with faster data management also helps engineers troubleshoot in real time. The vehicle data covered by the system has doubled, and data delivery and analysis happen 10 times faster.\n\nColes Group modernized its logistics and administrative applications using Microsoft Azure Stack HCI to scale its edge AI capabilities and improve efficiency and customer experience across its 1,800 stores. By expanding its Azure Stack HCI footprint from two stores to over 500, Coles achieved a six-fold increase in the pace of application deployment, significantly enhancing operational efficiency and enabling rapid innovation without disrupting workloads. The retailer is also using Azure Machine Learning to train and develop edge AI models, speeding up data annotation time for training models by 50%.\n\nMultinational advertising and media company Dentsu wanted to speed time to insights for its team of data scientists and media analysts to support its media planning and budget optimization. Using Microsoft Azure AI Foundry and Azure OpenAI Service, Dentsu developers built a predictive analytics copilot that uses conversational chat and draws on deep expertise in media forecasting, budgeting and optimization. This AI-driven tool has reduced time to media insights for employees and clients by 90% and cut analysis costs.\n\nTo overcome the limitations of its current systems, scale operations and automate processes across millions of workflows, Docusign created the Intelligent Agreement Management (IAM) platform on Azure. Using Azure AI, Azure Cosmos DB, Azure Logic Apps and AKS, the platform transforms agreement data into actionable insights to enhance productivity and accelerate contract review cycles. IAM also ensures better collaboration and unification across business systems to provide secure solutions tailored to diverse customer needs. For example, its customer KPC Private funds reported a 70% reduction in time and resources dedicated to agreement processes.\n\nEmirates Global Aluminium (EGA) transformed its manufacturing operations by leveraging a hybrid environment with Azure Arc, Azure Stack HCI and Azure Kubernetes Service. This digital manufacturing platform resulted in 86% cost savings for AI image and video analytics and a 13-fold improvement in AI response times. The seamless hybrid cloud architecture has enhanced EGA’s operational efficiency and agility, supporting its Industry 4.0 transformation strategy.\n\nEY collaborated with Microsoft to enhance the inclusivity of AI development using Azure AI Foundry. By involving neurodivergent technologists from EY’s Neuro-Diverse Centers of Excellence, they improved the accessibility and productivity of AI tools, resulting in more inclusive AI solutions, fostering innovation and ensuring that AI tools unlock the potential of all users. With an estimated 20% of the global workforce identifying as neurodivergent, inclusive AI solutions are crucial for maximizing creativity and productivity. Neurodivergent EY technologists also collaborated with Microsoft developers to make Azure AI Foundry more inclusive and help all users work productively to create innovative AI solutions.\n\nColombian household appliance manufacturer Haceb integrated AI to optimize processes, reduce costs and improve service quality. Using Microsoft Copilot Studio and Azure OpenAI Service, the company created a virtual technical support assistant, saving its 245 technicians 5 minutes per visit — a total of 5,000 minutes saved daily. This AI solution has enhanced efficiency and boosted customer satisfaction by allowing for faster issue resolution. Haceb’s AI adoption has also empowered employees, boosted productivity and positioned the company as a leader in AI innovation in Colombia.\n\nTo better serve its global patients, Operation Smile — in collaboration with partner Squadra — leveraged Azure AI, Machine Learning and Microsoft Fabric to develop an AI-powered solution to predict surgical outcomes and optimize resource allocation. This innovation resulted in a 30% increase in surgical efficiency, a 90% reduction in translation errors and improved patient outcomes. Additionally, report generation is now up to 95% quicker, and repeated medical events have decreased by 15%, enabling Operation Smile to provide better care to more children worldwide.\n\nOntada — a McKesson business dedicated to oncology data and evidence, clinical education and point-of-care technologies — needed a way to generate key insights across 150 million unstructured oncology documents. Using Microsoft Azure AI and Azure OpenAI Service, Ontada developed a data platform solution called ON.Genuity to provide AI-driven insights into the patient journey, enhance patient trial matching and identify care gaps. The company also implemented large language models to target nearly 100 critical oncology data elements across 39 cancer types, enabling the company to analyze an estimated 70% of previously inaccessible data, reduce processing time by 75% and accelerate product time-to-market from months to just one week.\n\nAs the UK’s largest pet care company, Pets at Home sought a way to combat fraud across its retail operations — particularly as its online business continued to grow. Working closely with its fraud team, it adopted Copilot Studio to develop an AI agent that quickly identifies suspicious transactions. The agent autonomously gathers relevant information, performs analysis and shares it with a fraud agent to enable a manual, data-intensive investigative process while ensuring a human remains in the loop. With this low-code agent extending and seamlessly integrating into existing systems, the company’s fraud department can act more quickly; what used to take 20 to 30 minutes is now handled by the AI agent within seconds. The company is identifying fraud 10 times faster and is processing 20 times more cases a day. Now, the company can operate at scale with speed, efficiency and accuracy — with savings expected to be in the seven figures as it continues to build more agents.\n\nRevenue Grid, a technology company specializing in sales engagement and revenue optimization solutions, partnered with Cloud Services to modernize its data infrastructure and develop a unified data warehouse capable of handling unstructured, semi-structured and structured data. By migrating to Microsoft Fabric, Revenue Grid can now deliver data-powered revenue intelligence, driven by a unified platform, elastic scalability, enhanced analytics capabilities and streamlined operations. Revenue Grid has reduced infrastructure costs by 60% while enhancing its analytical capabilities to improve real-time data processing, empowering sales teams with accurate and diverse data.\n\nTo better manage and integrate employee data across diverse regions and systems, UST built a comprehensive Employee Data platform on Microsoft Fabric. In under a year, UST migrated 20 years of employee data with all security measures to enhance data accessibility and employee productivity. The Meta Data Driven Integration (MDDI) framework in Fabric also helped the company cut data ingestion time by 50% so employees can focus more on analysis than preparation. As a result of this implementation, the company has seen an increase in collaboration and innovation from employees, helping put its values into action.\n\nThe Microsoft Commercial Marketplace offers millions of customers worldwide a convenient place to find, try and buy software and services across 140 countries. As a Marketplace partner, WeTransact is helping independent software vendors (ISVs) list and transact their software solutions — and find opportunities for co-selling and extending their reach to enterprise customers through development of the WeTransact platform. Powered by Azure OpenAI Service, the platform is changing the way partnerships are being built by using AI pairing to facilitate a “plug and play” reseller network. More than 300 ISVs worldwide have joined the Microsoft Commercial Marketplace using the WeTransact platform, cutting their time to publish by 75%.\n\nThe opportunity for AI to create value is no longer an ambition for the future — it is happening now, and organizational leaders across industries are investing in AI-first strategies to change the way they do business. We believe AI should empower human achievement and enrich the lives of employees; and we are uniquely differentiated to help you accelerate your AI Transformation responsibly and securely. Choosing the right technology provider comes down to trust, and I look forward to what we will achieve together as we partner with you on your AI journey.\n\nTags: AI, Azure, Azure AI, Azure AI Foundry, Azure Arc, Azure OpenAI Service, Azure Stack HCI, Copilot, Copilot Studio, Microsoft Fabric, Microsoft Ignite 2024", "label": "non_personal"}
{"title": "Introducing Azure AI Foundry Labs: A hub for the latest AI research and experiments at Microsoft", "url": "https://azure.microsoft.com/en-us/blog/introducing-azure-ai-foundry-labs-a-hub-for-the-latest-ai-research-and-experiments-at-microsoft/", "content": "We’re thrilled to announce the launch of Azure AI Foundry Labs, a hub for developers, startups, and enterprises to explore groundbreaking innovations from research at Microsoft.\n\nToday we’re launching Azure AI Foundry Labs, a hub for developers, startups, and enterprises to explore groundbreaking innovations from research at Microsoft. Foundry Labs unites cutting-edge research with real-world applications, to enable developers and creators across industries to discover new possibilities, solve complex problems, and share insights to shape the future of AI.\n\nMicrosoft’s newest AI breakthrough—Muse, a first-of-its-kind World and Human Action Model (WHAM), available today in Azure AI Foundry—is the latest example of bringing cutting-edge research innovation to our AI platform for customers to use.\n\nWith Azure AI Foundry Labs, we’re excited to unveil new assets for our latest research-driven projects that empower developers to explore, engage, and experiment. Projects across models and agentic frameworks include:\n\nAurora: A large-scale atmospheric model providing high-resolution weather forecasts and air pollution predictions, outperforming traditional tools.\n\nExACT: An open-source project enabling agents to learn from past interactions and improve search efficiency dynamically.\n\nMagentic-One: A multi-agent system solving complex problems by orchestrating multiple agents, built on the AutoGen framework.\n\nMatterSim: A deep learning model for atomistic simulations, predicting material properties with high precision.\n\nOmniParser v2: A vision-based module converting UI screenshots into structured elements, enhancing agents’ action generation.\n\nTamGen: A generative AI model for drug design, using a GPT-like chemical language model for target-aware molecule generation and refinement.\n\nThen versus now\n\nIn the early days of global positioning systems (GPS) technology, it took roughly a decade for GPS to make its way from specialized, military-grade instruments into everyday consumer use. What started as a niche innovation in the 1970’s didn’t become truly mainstream until the late 1990’s and early 2000’s, when GPS receivers became standard features in cars, cell phones, and handheld devices. Ten years might sound like a reasonable adoption curve—until you look at how quickly innovations are moving in AI today.\n\nIn recent years, the pace of AI advancement has accelerated dramatically. We’ve witnessed a shift from unveiling a new model every 4–6 months to releasing breakthroughs every 4–6 days. The amount of compute used for training AI models has grown 10 times every 12 months, turbocharging both research and commercialization. And time-to-product from foundational research to full-scale product deployment has gone from years to months.\n\nAt this velocity, ideas and prototypes need to be iterated upon, validated, and deployed faster than ever before. This rapid evolution demands new thinking in how we bridge research and application.\n\nAccelerating research to impact\n\nAzure AI Foundry Labs highlights the long-term collaboration between research and engineering teams at Microsoft and provides a single access point for developers and the broader AI community to experiment with new models, explore the latest frameworks, and be at the forefront of innovation. Developers can create prototypes using experimental research in Azure AI Foundry Labs, collaborate with researchers and engineering teams by sharing feedback, and help speed up the time to market for some of the most promising technologies.\n\nThe next chapter\n\nThe gap between breakthrough and impact has never been smaller. What once took years now takes weeks, and what was once confined to research labs now runs on devices in our pockets. Azure AI Foundry Labs exists to collapse this gap even further—to ensure that every breakthrough in AI research finds its way to the developers, creators, and innovators who can transform it into real-world impact.\n\nAzure AI Foundry Labs Bridging research and application. Discover more\n\nThis isn’t just about sharing research—it’s about accelerating the cycle of innovation itself. Whether you’re a developer, researcher, startup founder, or enterprise builder, Azure AI Foundry Labs gives you direct access to the bleeding edge of AI advancement. The tools and models available today are just the beginning.\n\nVisit Azure AI Foundry Labs to start building the future.", "label": "non_personal"}
{"title": "A new level unlocked", "url": "https://blogs.microsoft.com/blog/2025/02/19/a-new-level-unlocked/", "content": "Muse, the first World and Human Action Model, could facilitate interdisciplinary collaboration, for example, when exploring gameplay ideas.\n\nToday Microsoft released Muse, a first-of-its-kind generative AI model that we are applying to gaming. But it’s so much more than that. What we’re sharing today is a huge step forward for gameplay ideation. And what’s even more exciting is what this breakthrough represents in our journey of building and using generative AI, and what industries, developers and creators of all interests will be enabled to do next.\n\nThe impressive abilities we first witnessed with ChatGPT and GPT-4 to learn human language are now being matched by AI’s abilities to learn the mechanics of how things work, in effect developing a practical understanding of interactions in the world. As a computer scientist, this ability to understand and model a 3D world is something I and many other great researchers have pursued for over 10 years and, personally, I was not sure that it could be made possible with such speed and quality.\n\nIn the case of Muse, just from observing human gameplay, this model develops a deep understanding of the environment, including its dynamics and how it evolves over time in response to actions. This unlocks the ability to rapidly iterate, remix and create in video games so developers can eventually create immersive environments and unleash their full creativity.\n\nBeyond gaming, I’m excited by the potential of this capability to enable AI assistants that understand and help visualize things, from reconfiguring the kitchen in your home to redesigning a retail space to building a digital twin of a factory floor to test and explore different scenarios. All these things are just now becoming possible with AI. From the perspective of computer science research, it’s pretty amazing, and the future applications of this are likely to be transformative for creators.\n\n—\n\nAt Microsoft, we have a long history of collaboration between research and engineering. Today, as we release Muse, we are also announcing Azure AI Foundry Labs, where the AI community can explore the latest from Microsoft Research. Azure AI Foundry Labs will help accelerate the transition from research to solutions, bringing new ideas to the broader community to help shape the future of AI. Learn more.\n\nTags: AI, Azure AI Foundry Labs, ChatGPT, GPT-4", "label": "non_personal"}
{"title": "Microsoft Build 2025: The age of AI agents and building the open agentic web", "url": "https://blogs.microsoft.com/blog/2025/05/19/microsoft-build-2025-the-age-of-ai-agents-and-building-the-open-agentic-web/", "content": "TL;DR? Hear the news as an AI-generated audio overview made using Microsoft 365 Copilot. You can read the transcript here.\n\nWe’ve entered the era of AI agents. Thanks to groundbreaking advancements in reasoning and memory, AI models are now more capable and efficient, and we’re seeing how AI systems can help us all solve problems in new ways.\n\nFor example, 15 million developers are already using GitHub Copilot, and features like agent mode and code review are streamlining the way they code, check, deploy and troubleshoot.\n\nHundreds of thousands of customers are using Microsoft 365 Copilot to help research, brainstorm and develop solutions, and more than 230,000 organizations — including 90% of the Fortune 500 — have already used Copilot Studio to build AI agents and automations.\n\nCompanies like Fujitsu and NTT DATA are using Azure AI Foundry to build and manage AI apps and agents that help prioritize sales leads, speed proposal creation and surface client insights. Stanford Health Care is using Microsoft’s healthcare agent orchestrator to build and test AI agents that can help alleviate the administrative burden and speed up the workflow for tumor board preparation.\n\nDevelopers are at the center of it all. For 50 years Microsoft has been empowering developers with tools and platforms to turn their ideas into reality, accelerating innovation at every stage. From AI-driven automation to seamless cloud integration and more, it’s exciting to see how developers are fueling the next generation of digital transformation.\n\nSo, what’s next?\n\nWe envision a world in which agents operate across individual, organizational, team and end-to-end business contexts. This emerging vision of the internet is an open agentic web, where AI agents make decisions and perform tasks on behalf of users or organizations.\n\nAt Microsoft Build we’re showing the steps we’re taking to make this vision a reality through our platforms, products and infrastructure. We’re putting new models and coding agents in the hands of developers, introducing enterprise-grade agents, making our platforms like Azure AI Foundry, GitHub and Windows the best places to build, embracing open protocols and accelerating scientific discovery with AI, all so that developers and organizations can go invent the next big thing.\n\nHere’s a glimpse at just a few of the announcements today:\n\nReimagining the software development lifecycle with AI\n\nAI is fundamentally shifting how code is written, deployed and maintained. Developers are using AI to stay in the flow of their environment longer and to shift their focus to more strategic tasks. And as the software development lifecycle is being transformed, we’re providing new features across platforms including GitHub, Azure AI Foundry and Windows that enable developers to work faster, think bigger and build at scale.\n\nGitHub Copilot coding agent and new updates to GitHub Models: GitHub Copilot is evolving from an in-editor assistant to an agentic AI partner with a first-of-its-kind asynchronous coding agent integrated into the GitHub platform. We’re adding prompt management, lightweight evaluations and enterprise controls to GitHub Models so teams can experiment with best-in-class models, without leaving GitHub. Microsoft is also open-sourcing GitHub Copilot Chat in VS Code. The AI-powered capabilities from GitHub Copilot extensions will now be part of the same open-source repository that drives the world’s most popular development tool. As the home of over 150 million developers, this reinforces our commitment to open, collaborative, AI-powered software development. Learn more about GitHub Copilot updates.\n\nIntroducing Windows AI Foundry: For developers, Windows remains one of the most open and widely used platforms available, with scale, flexibility and growing opportunity. Windows AI Foundry offers a unified and reliable platform supporting the AI developer lifecycle across training and inference. With simple model APIs for vision and language tasks, developers can manage and run open source LLMs via Foundry Local or bring a proprietary model to convert, fine-tune and deploy across client and cloud. Windows AI Foundry is available to get started today. To learn more visit our Windows Developer Blog.\n\nAzure AI Foundry Models and new tools for model evaluation: Azure AI Foundry is a unified platform for developers to design, customize and manage AI applications and agents. With Azure AI Foundry Models, we’re bringing Grok 3 and Grok 3 mini models from xAI to our ecosystem, hosted and billed directly by Microsoft. Developers can now choose from more than 1,900 partner-hosted and Microsoft-hosted AI models, while managing secure data integration, model customization and enterprise-grade governance. We’re also introducing new tools like the Model Leaderboard, which ranks the top-performing AI models across different categories and tasks, and the Model Router, designed to select an optimal model for a specific query or task in real-time. Read more about Azure AI Foundry Models.\n\nMaking AI agents more capable and secure\n\nAI agents are not only changing how developers build, but how individuals, teams and companies get work done. At Build, we’re unveiling new pre-built agents, custom agent building blocks, multi-agent capabilities and new models to help developers and organizations build and deploy agents securely to help increase productivity in meaningful ways.\n\nWith the general availability of Azure AI Foundry Agent Service, Microsoft is bringing new capabilities to empower professional developers to orchestrate multiple specialized agents to handle complex tasks, including bringing Semantic Kernel and AutoGen into a single, developer-focused SDK and Agent-to-Agent (A2A) and Model Context Protocol (MCP) support. To help developers build trust and confidence in their AI agents, we’re announcing new features in Azure AI Foundry Observability for built-in observability into metrics for performance, quality, cost and safety, all incorporated alongside detailed tracing in a streamlined dashboard. Learn more about how to deploy enterprise-grade AI agents in Azure AI Foundry Service.\n\nDiscover, protect and govern in Azure AI Foundry: With Microsoft Entra Agent ID, now in preview, agents that developers create in Microsoft Copilot Studio or Azure AI Foundry are automatically assigned unique identities in an Entra directory, helping enterprises securely manage agents right from the start and avoid “agent sprawl” that could lead to blind spots. Apps and agents built with Foundry further benefit from Purview data security and compliance controls. Foundry also offers enhanced governance tools to set risk parameters, run automated evaluations and receive detailed reports. Learn more about Microsoft Entra Agent ID and Azure AI Foundry integrations with Microsoft Purview Compliance Manager.\n\nIntroducing Microsoft 365 Copilot Tuning and multi-agent orchestration: With Copilot Tuning, customers can use their own company data, workflows and processes to train models and create agents in a simple, low-code way. These agents perform highly accurate, domain-specific tasks securely from within the Microsoft 365 service boundary. For example, a law firm can create an agent that generates documents aligned with its organization’s expertise and style. Additionally, new multi-agent orchestration in Copilot Studio connects multiple agents, allowing them to combine skills and tackle broader, more complex tasks. Check out the Microsoft 365 blog to learn how to access these new tools as well as the Microsoft 365 Copilot Wave 2 spring release, which has moved to general availability and begins rolling out today.\n\nSupporting the open agentic web\n\nTo realize the future of AI agents, we’re advancing open standards and shared infrastructure to provide unique capabilities for customers.\n\nSupporting Model Context Protocol (MCP): Microsoft is delivering broad first-party support for Model Context Protocol (MCP) across its agent platform and frameworks, spanning GitHub, Copilot Studio, Dynamics 365, Azure AI Foundry, Semantic Kernel and Windows 11. In addition, Microsoft and GitHub have joined the MCP Steering Committee to help advance secure, at-scale adoption of the open protocol and announced two new contributions to the MCP ecosystem, an updated authorization specification, which enables people to use their existing trusted sign-in methods to give agents and LLM-powered apps access to data and services such as personal storage drives or subscription services, and the design of an MCP server registry service, which allows anyone to implement public or private, up-to-date, centralized repositories for MCP server entries. Check out the GitHub repository. As we expand our MCP capabilities, our top priority is to ensure we’re building upon a secure foundation. To learn more about this approach see: Securing the Model Context Protocol: Building a Safe Agentic Future on Windows.\n\nA new open project called NLWeb: Microsoft is introducing NLWeb, which we believe can play a similar role to HTML for the agentic web. NLWeb makes it easy for websites to provide a conversational interface for their users with the model of their choice and their own data, allowing users to interact directly with web content in a rich, semantic manner. Every NLWeb endpoint is also an MCP server, so websites can make their content easily discoverable and accessible to AI agents if they choose. Learn more here.\n\nAccelerating scientific discovery with AI\n\nScience may be one of the most important applications of AI, helping to tackle humanity’s most pressing challenges, from drug discovery to sustainability. At Build we’re introducing Microsoft Discovery, an extensible platform built to empower researchers to transform the entire discovery process with agentic AI, helping research and development departments across various industries accelerate the time to market for new products and accelerate and expand the end-to-end discovery process for all scientists. Learn more here.\n\nThis is only a small selection of the many exciting features and updates we will be announcing at Build. We’re looking forward to connecting with those who have registered to join us virtually and in-person, for keynote sessions, live code deep dives, hack sessions and more — much of which will be available on demand.\n\nPlus, you can get more on all these announcements by exploring the Book of News, the official compendium of all today’s news.\n\nTags: AI, Azure AI, Azure AI Foundry, Book of News, GitHub, GitHub Copilot, Microsoft 365 Copilot, Microsoft Copilot, Microsoft Purview", "label": "non_personal"}
{"title": "Meta Open Source: 2024 by the numbers", "url": "https://engineering.fb.com/2025/04/02/open-source/meta-open-source-by-the-numbers/", "content": "Open source has played an essential role in the tech industry and beyond. Whether in the AI/ML, web, or mobile space, our open source community grew and evolved while connecting people worldwide.\n\nAt Meta Open Source, 2024 was a year of growth and transformation. Our open source initiatives addressed the evolving needs and challenges of developers—powering breakthroughs in AI and enabling the creation of innovative, user-focused applications and experiences. In close collaboration with the open source community, we shared knowledge, introduced new projects, and enhanced existing ones.\n\nIn this post, we look at our portfolio of open source projects through numbers to give a better view of the scale of the community we interact with daily.\n\nAt Meta, we have several GitHub organizations where we publish new open source projects, maintain existing ones, and hold already archived projects. They include various tools, frameworks, and platforms for web, mobile, AI/ML, and hardware industries.\n\nBy the end of last year, we launched 256 brand-new repositories, bringing active public projects to 944. This number excludes archived repositories and projects that we moved to foundations.\n\nIn 2024, our open source codebases grew at an impressive pace, reaching 189,719 total commits in just one year. Community contributors accounted for 71,018, while Meta employees made the remaining 118,701.\n\nOpen source cannot exist without people collaborating, sharing, and innovating. A total of 4,274 external contributors helped bring our community to 7,144 strong. This remarkable community is what fuels the ongoing evolution of Meta Open Source.\n\nBeyond individual contributions, our projects on GitHub accumulated an additional 151,380 stars, bringing the total to a staggering 1.8 million. This growth in engagement shows strong interest and excitement for Meta Open Source projects.\n\nThank you to the open source community\n\nAt Meta, we believe open source accelerates the pace of innovation in the world. By sharing our technologies, we aim to move the industry forward while allowing other companies and individuals to use our solutions to scale more quickly and build great products.\n\nAt the same time, Meta Open Source projects are made possible by contributions from developers like you. Pull requests, documentation updates, social media posts, and everything in between are what build connections in our communities. Thank you all for another great year for open source.\n\nTo learn more about Meta Open Source, visit our open source site, subscribe to our YouTube channel, or follow us on Facebook, Threads, X, and LinkedIn.", "label": "non_personal"}
{"title": "Neat Websites – Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/blogroll/neat-websites/", "content": "Jump to: Neat projects | Webcomics | Plants & nature | Food | Collections | Math & science | Places | Tools | Seattle\n\n🆕 added April 2025\n\nMore collections: blogroll | interesting people | cool artists | graphic design resources | indie shops | wishlist | big questions\n\nNeat projects\n\nIt’s Post Day! (Sarah Avenir) — email art project\n\nHow Not to Make a Book (Robin Rendle) — documenting the process of creating a book about typography\n\nWerner’s Nomenclature of Colors (Nicholas Rougeux) — A recreation of the original 1821 color guidebook with new cross references, photographic examples, and posters\n\n\n\nScreens, research and hypertext — hypertext book about hypertext — love that meta\n\nbrr.fyi — blog by an anonymous IT worker who overwintered in Antarctica\n\nWA 100 Peaks — photographer and climber Scott Kranz climbed 100 peaks in Washington\n\nJohannes Klingebiel’s digital garden — nice design\n\nEmmanuel Quartey’s “questions” — I like the framing and organization of information\n\nThe Shape of Music Albums — visualization of the characteristics Spotify assigns to tracks, created by Greg Wolanski\n\nTinnitus Tracker — concerts attended by Rob Weychert\n\nAtlas of Intangibles — cool interactive visualization of markers of distinctiveness and wear in London locations\n\n🆕 Atlas of Surveillance — police tech by state in the US (an EFF project)\n\nAdvocacy\n\n🆕 Regulations.gov — did you know there was one website where you could leave comments on like every U.S. rule change? I did not!\n\n🆕 Choose Democracy\n\n🆕 Beautiful Trouble Toolbox — “an interconnected web of ideas and creative best practices that puts the power in your hands”\n\nSpecific Suggestions — “The most potent tools for fighting injustice are the ones already in your hands.”\n\nWebcomics\n\nFalse Knees (Joshua Barkman) – comic strips with goofy birds\n\nwebcomic name (Alex Norris) – “oh no”\n\nPoorly Drawn Lines (Reza Farazmand) – comic strips with returning animal characters\n\nCat and Girl (Dorothy) – social commentary from Cat and Girl\n\nThe Creator’s Guide to Comics* Devices (Reimena Yee) — illustrated guide to tools that comic artists can use in storytelling\n\nInfo\n\nPlants and nature\n\n–> check out my garden section\n\nSmall Seasons — the year divided into two-week segments named for the natural phenomena that tend to happen then (in Japan)\n\nNative Plants PNW — comprehensive listings of northwest native plants\n\nPacific Northwest Wildflowers — photographic database of wildflowers filterable by color and useful for identification\n\nThe Natural Navigator (Tristan Gooley) – interpreting nature sign\n\nCotswald Diary (Chris) – a look into the ongoing work of natural restoration projects\n\nNew Hampshire Garden Solutions – pretty plants and insects\n\noakland garden club (Alexis Madrigal)* – plants and art\n\nClamsplaining (Dan Killam) – clam science\n\nNatural World Facts’ Deep Sea Hub (Leo Richards) — YouTube channel — mesmeric deep ocean videos\n\nWildhope.tv — YouTube channel — documentaries of conservation projects around the world\n\nBumble Bee Watch — report sightings of native bees\n\nPangea Seed — funding marine conservation through art\n\nFood\n\n–> check out recipes I like and saved recipes to try\n\nBudget Bytes Vegetarian Recipes — cheap recipes, usually easy\n\nSmitten Kitchen (Deb Perelmen) — Deb has an inviting writing style – consistently good source of baking recipes\n\nStill Tasty — database of how long food lasts and storage instructions for a wide variety of foods\n\nThe Good Enough Weekly (Devin K. Pope)* – climate and food\n\nEat This Newsletter (Jeremy Cherfas)* – food\n\nTasting History (Max Miller) — YouTube channel — he cooks a historic recipe from basically any time period and talks about its context while it’s cooking\n\nKenji’s Cooking Show (J. Kenji López-Alt) — YouTube channel — down to earth cooking advice from a science minded chef\n\nBlack Farmers Index — directory of Black farmers by region of the USA\n\nCollections\n\nPubMed Central — a free government database of medical journal papers, many of which include free full-text access because the researchers received grant funding 🙌\n\nFederal Open Science Repository of Canada — “federally authored scientific articles and publications from participating science-based departments and agencies” — climate & environment portal\n\nSprout Distro — free printable zines\n\n🆕 An Incomplete SFF Criticism and Studies Reading List (Molly Templeton)\n\nPlaces\n\n–> check out my road trip page\n\nAtlas Obscura — a searchable map and repository of cool destinations around the world — I always check this when I’m planning a trip\n\nClose.city — map with overlays for walking / biking / transit times to major destinations — looks similar to WalkScore but without a score\n\n🆕 Lushootseed Place Names — Google Map of western Washington\n\nMath and Science\n\nStand-up Maths (Matt Parker) — YouTube channel — goofy math questions explored with demos and field trips\n\nVeritasium (Derek Muller) — YouTube channel –longer explainer videos on science and engineering topics, often with cool models\n\nPractical Engineering (Grady Hillhouse) — YouTube channel — simple explanations of engineering practices with model demonstrations and case studies\n\n🆕 Defense Against Dishonest Charts (Nathan Yao) — visual breakdown of the elements of charts and what to look for in evaluating a chart for accuracy\n\nDiff Text – compare two text blocks\n\nLoot Lasso Portfolio Rebalancing Calculator – saw recommended on Reddit, haven’t used 😉\n\nsymbol.wtf – quickly copy and paste symbols\n\n60 Seconds of Advice on Surviving a Nuclear Blast – also see NUKEMAP by Alex Wellerstein\n\n🆕 PairDrop – pair devices to share files instead of emailing\n\n🆕 cFIREsim — calculator for financial planning\n\n🆕 Kinopio — “thinking canvas for new ideas and hard problems”\n\n🆕 Scribe.rip — front end for Medium articles\n\nSeattle area\n\nWashington Smoke Information – invaluable during smoke season\n\nThe Urbanist – Seattle area urbanist news\n\nLushootseed (Tulalip Tribe) – phrases and pronunciation of Lushootseed words “the language of Puget Sound”\n\n🆕 “The Voices of Lushootseed” online Lushootseed lessons *with audio recordings*\n\nWashington Trails Association – an amazing repository of trip notes with current conditions from hikers all across Washington", "label": "non_personal"}
{"title": "Generative AI as a magic system – Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/24/generative-ai-as-a-magic-system/", "content": "We treat generative AI like magic… and magic systems have rules. When creating fantasy worlds, writers think about who can use magic, how magic is performed, what it’s able to do, what its constraints are, what the source of magic is, and what it costs. I’m applying a bit of reverse worldbuilding to the real world to extrapolate the rules of the AI magic system.\n\nIslands in the Sky by Death Valley Girls\n\nWho can use AI magic: magic users pay to use corporate AI magic systems. Those who are wealthy and tech savvy enough can host their own local model. Free magic use is mostly limited to corporate largesse ultimately intended to build magic dependency.\n\nHow AI magic is cast: AI spells are cast with written text input through a digital interface. Spells are refined and recast until the outcome satisfies (spells produce different results every time they are cast).\n\nWhat AI magic can do: AI spells can produce combinations of words that are interpreted as writing, code-like material that sometimes runs as code, images that resemble art, and video that resembles reality. It can create imitations of specific human creators’ work, as well as individual’s speech and appearance. It can also mimic human conversation for a span of time before the spell dissipates. AI magic is near instantaneous, allowing people without technical skills to produce text and graphics faster than writers and artisans.\n\nWhat AI magic cannot do: AI magic cannot produce the same outcome twice, nor act upon existing conjurations, instead casting spells anew each time. AI magic itself cannot reference sources, though may be used in tandem with other tools that enable citation (though with questionable accuracy). AI magic cannot reason or write, but its conjurations may create the illusion of intelligence through their statistical consistency with written language use.\n\nThe source of AI magic: AI magic derives from statistical analysis of human-created art, writing, speech, music, and video, classified and sorted by human laborers in low-cost geos.\n\nThe cost of AI magic: Resource costs of AI magic include power, water, and high-end chips, which themselves require specialized manufacturing and rare earth minerals.\n\nSocial costs include the reinforcement of racism and sexism, as well as mental harm to AI trainers assessing inputs to the magic system.\n\nSocietal costs include job elimination and job intensification as positions able to be reproduced in part by magic are eliminated and that magic work is shifted to the remaining workers.\n\nInformation costs include the destruction of the online publishing incentive structure / information commons, leading to more paywalled content; an increase in low-quality material, which makes finding accurate information harder; as well as the danger of political propaganda by poisoned magic systems.\n\nIndividual user costs include critical thinking skills, writing abilities, and patience for conversing with humans.\n\nFurther reading:\n\nThe new magic of AI vs. the old magic of artists by Kening Zhu\n\nSee also:\n\nGenerative AI and the Business Borg aesthetic", "label": "non_personal"}
{"title": "How Meta understands data at scale", "url": "https://engineering.fb.com/2025/04/28/security/how-meta-understands-data-at-scale/", "content": "Managing and understanding large-scale data ecosystems is a significant challenge for many organizations, requiring innovative solutions to efficiently safeguard user data. Meta’s vast and diverse systems make it particularly challenging to comprehend its structure, meaning, and context at scale.\n\nTo address these challenges, we made substantial investments in advanced data understanding technologies, as part of our Privacy Aware Infrastructure (PAI) . Specifically, we have adopted a “shift-left” approach, integrating data schematization and annotations early in the product development process. We also created a universal privacy taxonomy , a standardized framework providing a common semantic vocabulary for data privacy management across Meta’s products that ensures quality data understanding and provides developers with reusable and efficient compliance tooling.\n\nWe discovered that a flexible and incremental approach was necessary to onboard the wide variety of systems and languages used in building Meta’s products. Additionally, continuous collaboration between privacy and product teams was essential to unlock the value of data understanding at scale.\n\nWe embarked on the journey of understanding data across Meta a decade ago with millions of assets in scope ranging from structured and unstructured, processed by millions of flows across many of the Meta App offerings. Over the past 10 years, Meta has cataloged millions of data assets and is classifying them daily, supporting numerous privacy initiatives across our product groups. Additionally, our continuous understanding approach ensures that privacy considerations are embedded at every stage of product development.\n\nAt Meta, we have a deep responsibility to protect the privacy of our community. We’re upholding that by investing our vast engineering capabilities into building cutting-edge privacy technology. We believe that privacy drives product innovation. This led us to develop our Privacy Aware Infrastructure (PAI), which integrates efficient and reliable privacy tools into Meta’s systems to address needs such as purpose limitation—restricting how data can be used while also unlocking opportunities for product innovation by ensuring transparency in data flows\n\nData understanding is an early step in PAI. It involves capturing the structure and meaning of data assets, such as tables, logs, and AI models. Over the past decade, we have gained a deeper understanding of our data, by embedding privacy considerations into every stage of product development, ensuring a more secure and responsible approach to data management.\n\nWe embarked on our data understanding journey by employing heuristics and classifiers to automatically detect semantic types from user-generated content. This approach has evolved significantly over the years, enabling us to scale to millions of assets. However, conducting these processes outside of developer workflows presented challenges in terms of accuracy and timeliness. Delayed classifications often led to confusion and unnecessary work, while the results were difficult to consume and interpret.\n\nData understanding at Meta using PAI\n\nTo address shortcomings, we invested in data understanding by capturing asset structure (schematization), describing meaning (annotation), and inventorying it into OneCatalog (Meta’s system that discovers, registers, and enumerates all data assets) across all Meta technologies. We developed tools and APIs for developers to organize assets, classify data, and auto-generate annotation code. Despite significant investment, the journey was not without challenges, requiring innovative solutions and collaboration across the organization.\n\nChallenge Approach Understanding at scale (lack of foundation) At Meta, we manage hundreds of data systems and millions of assets across our family of apps. Each product features its own distinct data model, physical schema, query language, and access patterns. This diversity created a unique hurdle for offline assets: the inability to reuse schemas due to the limitations of physical table schemas in adapting to changing definitions. Specifically, renaming columns or making other modifications had far-reaching downstream implications, rendering schema evolution challenging, thus propagation required careful coordination to ensure consistency and accuracy across multiple systems and assets. We introduced a shared asset schema format as a logical representation of the asset schema that can be translated back and forth with the system-specific format. Additionally, it offers tools to automatically classify data and send out annotation changes to asset owners for review , effectively managing long-tail systems. Inconsistent definitions (lack of shared understanding) We encountered difficulties with diverse data systems that store data in various formats, and customized data labels that made it challenging to recognize identical data elements when they are stored across multiple systems. We introduced a unified taxonomy of semantic types , which are compiled into different languages. This ensured that all systems can share the same canonical set of labels. Missing annotations (lack of quality) A solution that relied solely on data scanning and pattern matching was prone to false positives due to limited contextual information. For instance, a 64-bit integer could be misclassified as either a timestamp or a user identifier without additional context. Moreover, manual human labeling is not feasible at scale because it relies heavily on individual developers’ expertise and knowledge. We shifted left by combining schematization together with annotations in code , in addition improving and utilizing multiple classification signals . Strict measurements provided precision/recall guarantees. Protection was embedded in everything we built, without requiring every developer to be a privacy expert. Organizational barriers (lack of a unified approach) Meta’s data systems, with their bespoke schematization and practices, posed significant challenges in understanding data across the company. As we navigated complex interactions and with ever evolving privacy requirements, it became clear that fragmented approaches to data understanding hindered our ability to grasp data comprehensively. By collaborating with asset owners to develop intuitive tooling and improve coverage, we tackled adoption barriers such as poor developer experience and inaccurate classification. This effort laid the groundwork for a unified data understanding foundation, which was seamlessly integrated into the developer workflow. As a result, we drove a cultural shift towards reusable and efficient privacy practices, ultimately delivering value to product teams and fostering a more cohesive approach to data management.\n\nWalkthrough : Understanding user data for the “Beliefs” feature in Facebook Dating\n\nTo illustrate our approach and dive into the technical solution, let’s consider a scenario involving structured user data. When creating a profile on the Facebook Dating app, users have the option to include their religious views to help match with others who share similar values.\n\nOn Facebook Dating, religious views are subject to purpose limitation requirements. Our five-step approach to data understanding provides a precise, end-to-end view of how we track and protect sensitive data assets, including those related to religious views:\n\nEven a simple feature can involve data being processed by dozens of heterogenous systems, making end-to-end data protection critical. To ensure comprehensive protection, it is essential to apply the necessary steps to all systems that store or process data, including distributed systems (web systems, chat, mobile and backend services) and data warehouses.\n\nConsider the data flow from online systems to the data warehouse, as shown in the diagram below. To ensure that religious belief data is identified across all these systems, we have implemented measures to prevent its use for any purpose other than the stated one.\n\nStep 1 – Schematizing\n\nAs part of the PAI initiative, Meta developed DataSchema, a standard format that is used to capture the structure and relationships of all data assets, independent of system implementation. Creating a canonical representation for compliance tools. Understanding DataSchema requires grasping schematization, which defines the logical structure and relationships of data assets, specifying field names, types, metadata, and policies.\n\nImplemented using the Thrift Interface Description Language, DataSchema is compatible with Meta systems and languages. It describes over 100 million schemas across more than 100 data systems, covering granular data units like database tables, key-value stores, data streams from distributed systems (such as those used for logging), processing pipelines, and AI models. Essentially, a data asset is like a class with annotated attributes.\n\nLet’s examine the source of truth (SoT) for a user’s dating profile schema, modeled in DataSchema. This schema includes the names and types of fields and subfields:\n\n- user_id (uint) - name (string) - age (uint) - religious_views (enum) - photos (array<struct>): - url (url) - photo (blob) - caption (string) - uploaded_date (timestamp) Dating profile DataSchema\n\nThe canonical SoT schema serves as the foundation for all downstream representations of the dating profile data. In practice, this schema is often translated into system-specific schemas (source of record – “SoR”), optimized for developer experience and system implementation in each environment.\n\nStep 2 – Predicting metadata at scale\n\nBuilding on this schematization foundation, we used annotations to describe data, enabling us to quickly and reliably locate user data, such as religious beliefs, across Meta’s vast data landscape. This is achieved through a universal privacy taxonomy, a framework that provides a common semantic vocabulary for data privacy management across Meta’s apps. It offers a consistent language for data description and understanding, independent of specific programming languages or technologies.\n\nThe universal privacy taxonomy works alongside data classification, which scans systems across Meta’s product family to ensure compliance with privacy policies. These systems use taxonomy labels to identify and classify data elements, ensuring privacy commitments are met and data is handled appropriately according to its classification.\n\nPrivacy annotations are represented by taxonomy facets and their values. For example, an asset might pertain to an Actor.Employee, with data classified as SemanticType.Email and originating from DataOrigin.onsite, not a third party. The SemanticType annotation is our standard facet for describing the meaning, interpretation, or context of data, such as user names, email addresses, phone numbers, dates, or locations.\n\nBelow, we illustrate the semantic type taxonomy node for our scenario, Faith Spirituality:\n\nAs data models and collected data evolve, annotations can become outdated or incorrect. Moreover, new assets may lack annotations altogether. To address this, PAI utilizes various techniques to continuously verify our understanding of data elements and maintain accurate, up-to-date annotations:\n\nOur classification system leverages machine learning models and heuristics to predict data types by sampling data, extracting features, and inferring annotation values. Efficient data sampling, such as Bernoulli sampling, and processing techniques enable scaling to billions of data elements with low-latency classifications.\n\nKey components include:\n\nScheduling component : manages the set of data assets to scan, accommodating different data system architectures by either pulling data via APIs or receiving data pushed directly into the scanning service.\n\nScanning service : processes and analyzes data from various sources by accumulating samples in memory, deserializing rows (e.g., JSON) into fields and sub-fields, and extracting features using APIs available in multiple languages (C++, Python, Hack). It ensures comprehensive data capture, even for ephemeral data.\n\nClassification service : utilizes heuristic rules and machine learning models to classify data types with high accuracy. Heuristic rules : handle straightforward, deterministic cases by identifying specific data formats like dates, phone numbers, and user IDs. Machine learning models : trained on labeled datasets using supervised learning and improved through unsupervised learning to identify patterns and anomalies in unlabeled data. Ground truth calibration and verification : ensures system accuracy and reliability, allowing for model fine-tuning and improved classification performance.\n\nLineage and propagation: We integrate classification rules with high-confidence lineage signals to ensure accurate data tracking and management. Our propagation mechanism enables the seamless annotation of data as needed, ensuring that exact copies of data across systems receive equivalent classification. This approach not only maintains data integrity but also optimizes the developer experience by streamlining the process of managing data classifications across our diverse systems.\n\nStep 3 – Annotating\n\nThe integration of metadata predictions and developer input creates a comprehensive picture of a data asset’s structure (schema) and its meaning (annotation). This is achieved by attaching these elements to individual fields in data assets, providing a thorough understanding of the data.\n\nBuilding on the predicting data at scale initiative (step 2), where we utilize the universal privacy taxonomy and classification systems to identify and classify data elements, the generated metadata predictions are then used to help developers annotate their data assets efficiently and correctly.\n\nPortable annotation APIs: seamlessly integrate into developer workflows ensuring:\n\nConsistent representation of data across all systems at Meta.\n\nAccurate understanding of data, enabling the application of privacy safeguards at scale.\n\nEfficient evidencing of compliance with regulatory requirements.\n\nMetadata predictions and developer input: Two key components work together to create a comprehensive data asset picture:\n\nMetadata predictions : Classifiers generate predictions to aid developers in annotating data assets efficiently and correctly. If the confidence score exceeds a certain threshold, assignment can be automated, saving developer time.\n\nDeveloper input : Developers manually refine and verify annotations, ensuring that the data’s context and privacy requirements are accurately captured. Human oversight guarantees the accuracy and reliability of the data asset picture.\n\n- user_id (enum) → SemanticType::id_userID - name (string) → SemanticType::identity_name - age (uint) → SemanticType::age - religious_views (enum) → SemanticType::faithSpirituality - photos (array<struct>): - url (url) → SemanticType::electronicID_uri_mediaURI_imageURL - photo (blob) → SemanticType::media_image - caption (string) → SemanticType::media_text_naturalLanguageText - uploaded_date (timestamp) → SemanticType::uploadedTime\n\nEnsuring complete schemas with annotations: To maintain a high standard of data understanding, we have integrated data understanding into our data model lifecycle. This includes auto-generating code to represent the schema of newly created assets when missing, ensuring that no new assets are created without a proper schema.\n\nFor example, in the context of our religious beliefs in Facebook Dating, we have defined its structure, including fields like ‘Name,’ ‘EmailAddress,’ and ‘Religion.’ Furthermore, we have annotated the asset with Actor::user(), signifying that the data pertains to a user of our products. This level of detail enables us to readily identify fields containing privacy-related data and implement appropriate protective measures, such as applying the applicable purpose limitation policy.\n\nIn the case of the “dating profile” data asset, we have defined its structure, including fields like ‘Name’:\n\nfinal class DatingProfileSchema extends DataSchemaDefinition { <<__Override>> public function configure(ISchemaConfig $config): void { $config->metadataConfig()->description('Represents a dating profile); $config->annotationsConfig()->annotations(Actor::user()); } <<__Override>> public function getFields(): dict<string, ISchemaField> { return dict[ 'Name' => StringField::create(\"name\") ->annotations(SemanticType::identity_name()) ->example('John Doe'), 'Age' => StringInt::create('age') ->description(“The age of the user.”) ->annotations(SemanticType::age()) ->example('24'), 'ReligiousViews' => EnumStringField::create('religious_views') ->annotations(SemanticType::faithSpirituality()) ->example('Atheist'), ]; } }\n\nIn order to optimize for developer experience, the details of the schema representation differ in each environment. For example, in the data warehouse, it’s represented as a Dataset – an in-code Python class capturing the asset’s schema and metadata. Datasets provide a native API for creating data pipelines.\n\nHere is an example of such a schema:\n\n​​@hive_dataset( \"dim_all_dating_users\", // table name \"dating\", // namespace oncall=\"dating_analytics\", description=\"This is the primary Dating user dimension table containing one row per Dating user per day along with their profile, visitation, and key usage information.\", metadata=Metadata(Actor.User), ) class dim_all_dating_users(DataSet): ds: Varchar = Partition(\"datestamp\") userid: DatingUserID = Column(\"User id of the profile\") email: EmailAddress = Column(\"User's email address\"), age: PersonAge = Column(\"User's stated age on date ds\") religious_views: ReligionOptions = Column(\"User's provided religious views\")\n\nOur warehouse schema incorporates rich types, a privacy-aware type system designed to enhance data understanding and facilitate effective data protection. Rich types, such as DatingUserID, EmailAddress, PersonAge, and ReligionOptions, are integrated into the schema, offering a comprehensive approach to data management while encoding privacy metadata. They provide a developer-friendly way to annotate data and enable the enforcement of data quality rules and constraints at the type level, ensuring data consistency and accuracy across the warehouse. For instance, they can detect issues like joining columns with different types of user IDs or mismatched enums before code execution.\n\nHere is an example definition:\n\nReligionOptions = enum_from_items( \"ReligionOptions\", items=[ EnumItem(\"Atheist\", \"Atheist\"), EnumItem(\"Buddhist\", \"Buddhist\"), EnumItem(\"Christian\", \"Christian\"), EnumItem(\"Hindu\", \"Hindu\"), EnumItem(\"Jewish\", \"Jewish\"), EnumItem(\"Muslim\", \"Muslim\"), ... ], annotations=(SemanticType.faithSpirituality,), )\n\nStep 4 – Inventorying assets and systems\n\nA central inventory system is crucial for managing data assets and their metadata, offering capabilities like search and compliance tracking. Meta’s OneCatalog is a comprehensive system that discovers, registers, and enumerates all data assets across Meta’s apps, providing inventory for easier management and tracking.\n\nKey functions of OneCatalog:\n\nRegistering all data systems : OneCatalog defines a data system as a logical abstraction over resources that persist data for a common purpose. It exhaustively examines resources across Meta’s environments to discover and register all data systems hosting data assets.\n\nEnumerating all data assets : Eligible data systems must enumerate their assets through the asset enumeration platform, generating a comprehensive list of assets and their metadata in the central inventory. These assets are grouped by “asset classes” based on shared patterns, enabling efficient management and understanding of data assets.\n\nGuarantees provided by OneCatalog:\n\nCompleteness: The system regularly checks for consistency between the data defined in its configuration and the actual data stored in the inventory. This ongoing comparison ensures that all relevant data assets are accurately accounted for and up-to-date.\n\nFreshness: In addition to regularly scheduled pull-based enumeration, the system subscribes to changes in data systems and updates its inventory in real time.\n\nUniqueness of asset ID (XID): Each asset is assigned a globally unique identifier, similar to URLs, which facilitates coordination between multiple systems and the exchange of information about assets by providing a shared key. The globally unique identifier follows a human-readable structure, e.g., asset://[asset-class]/[asset-name].\n\nUnified UI: On top of the inventory, OneCatalog provides a unified user interface that consolidates all asset metadata, serving as the central hub for asset information. This interface offers a single point of access to view and manage assets, streamlining the process of finding and understanding data.\n\nFor example, in the context of our “religious beliefs in the Dating app” scenario, we can use OneCatalog’s unified user interface to view the warehouse dating profile table asset, providing a comprehensive overview of its metadata and relationships.\n\nCompliance and privacy assurance: OneCatalog’s central inventory is utilized by various privacy teams across Meta to ensure that data assets meet requirements. With its completeness and freshness guarantees, OneCatalog serves as a reliable source of truth for privacy and compliance efforts.\n\nBy providing a single view of all data assets, OneCatalog enables teams to efficiently identify and address potential risks or vulnerabilities, such as unsecured data or unauthorized access.\n\nStep 5 – Maintaining data understanding\n\nTo maintain high coverage and quality of schemas and annotations across Meta’s diverse apps, we employed a robust process that involves measuring precision and recall for both predicted metadata and developer-provided annotations. This enables us to guide the implementation of our privacy and security controls and ensure their effectiveness.\n\nBy leveraging data understanding, tooling can quickly build end-to-end compliance solutions. With schema and annotations now front and center, we’ve achieved continuous understanding, enabling our engineers to easily track and protect user data, implement various security and privacy controls, and build new features at scale.\n\nOur strategy for maintaining data understanding over time includes:\n\nShifting left on creation time : We provided intuitive APIs for developers to provide metadata at asset creation time, ensuring that schemas and annotations were applied consistently in downstream use cases.\n\nDetecting and fixing annotation gaps : We surfaced prediction signals to detect coverage and quality gaps and evolved our prediction and annotation capabilities to ensure new systems and workflows were covered.\n\nCollecting ground truth : We established a baseline to measure automated systems against, with the help of subject matter experts, to continuously measure and improve them.\n\nProviding canonical consumption APIs : We developed canonical APIs for common compliance usage patterns, such as detecting user data, to ensure consistent interpretation of metadata and low entry barriers.\n\nPutting it all together\n\nComing back to our scenario: As developers on the Facebook Dating team collect or generate new data, they utilize familiar APIs that help them schematize and annotate their data. These APIs provide a consistent and intuitive way to define the structure and meaning of the data.\n\nWhen collecting data related to “Faith Spirituality,”the developers use a data classifier that confirms their semantic type annotations once the data is scanned during testing. This ensures that the data is accurately labeled and can be properly handled by downstream systems.\n\nTo ensure the quality of the classification system, ground truth created by subject matter experts is used to measure its accuracy. A feedback loop between the product and PAI teams keeps the unified taxonomy updated, ensuring that it remains relevant and effective.\n\nBy using canonical and catalogued metadata, teams across Meta can implement privacy controls that are consistent and effective. This enables the company to maintain user trust and meet requirements.\n\nIn this scenario, the developers on the Facebook Dating team are:\n\nSchematizing and annotating their data using familiar APIs.\n\nUsing a data classifier to confirm semantic type annotations.\n\nLeveraging ground truth to measure the quality of the classification system.\n\nUtilizing a feedback loop to keep the unified taxonomy updated.\n\nImplementing privacy controls using canonical and catalogued metadata.\n\nLearnings and takeaways\n\nBuilding an understanding of all data at Meta was a monumental effort that not only required novel infrastructure but also the contribution of thousands of engineers across all teams at Meta, and years of investment.\n\nCanonical everything : Data understanding at scale relies on a canonical catalog of systems, asset classes, assets, and taxonomy labels, each with globally unique identifiers. This foundation enables an ecosystem of compliance tooling, separating the concerns of data understanding from consuming canonical metadata.\n\nIncremental and flexible approach : To tackle the challenge of onboarding hundreds of systems across Meta, we developed a platform that supports pulling schemas from existing implementations. We layered solutions to enhance existing untyped APIs , meeting developers where they are—whether in code, configuration, or a UI defining their use case and data model. This incremental and flexible approach delivers value at every step.\n\nCollaborating for data classification excellence : Building the platform was just the beginning. The infrastructure and privacy teams also collaborated with subject matter experts to develop best-in-class classifiers for our data, addressing some of the most challenging problems. These include detecting user-generated content, classifying data embedded in blobs, and creating a governed taxonomy that allows every developer to describe their data with the right level of detail.\n\nCommunity engagement with a tight feedback loop : Our success in backfilling schemas and integrating with the developer experience was made possible by a strong partnership with product teams. By co-building solutions and establishing an immediate feedback loop, we refined our approach, addressed misclassifications, and improved classification quality. This collaboration is crucial to our continued evolution and refinement of data understanding.\n\nThe future of data understanding\n\nData understanding has become a crucial component of Meta’s PAI initiative, enabling us to protect user data in a sustainable and effective manner. By creating a comprehensive understanding of our data, we can address privacy challenges durably and more efficiently than traditional methods.\n\nOur approach to data understanding aligns closely with the developer workflow, involving the creation of typed data models, collection of annotated data, and processing under relevant policies. At Meta’s scale, this approach has saved significant engineering effort by automating annotation on millions of assets (i.e., fields, columns, tables) with specific labels from an inventory that are deemed commitment-critical. This automation has greatly reduced the manual effort required for annotation, allowing teams to focus on higher-priority tasks.\n\nAs data understanding continues to evolve, it is expected to have a significant impact on various aspects of operations and product offerings. Here are some potential future use cases:\n\nImproved AI and machine learning : leveraging data understanding to improve the accuracy of AI-powered content moderation and recommendation systems.\n\nStreamlined developer workflows : integrating data understanding into Meta’s internal development tools to provide clear data context and reduce confusion.\n\nOperational and developer efficiency : By automating data classification and annotation for millions of assets across Meta’s platforms, we can significantly improve operational efficiency. This automation enables us to leverage metadata for various use cases, such as accelerating product innovation. For instance, we’re now utilizing this metadata to help developers efficiently find the right data assets, streamlining their workflow and reducing the time spent on manual searches.\n\nProduct innovation : With a comprehensive understanding of data, Meta can drive product innovation by leveraging insights to create personalized and engaging user experiences.\n\nWhile there is still more work to be done, such as evolving taxonomies to meet future compliance needs and developing novel ways to schematize data, we are excited about the potential of data understanding. By harnessing canonical metadata, we can deepen our shared understanding of data, unlocking unprecedented opportunities for innovation not only at Meta, but across the industry.\n\nAcknowledgements\n\nThe authors would like to acknowledge the contributions of many current and former Meta employees who have played a crucial role in developing data understanding over the years. In particular, we would like to extend special thanks to (in alphabetical order) Aaron Morris, Adrian Zgorzalek, Alex Gorelik, Alex Kalinin, Alex Uslontsev, Ali Fakeri Tabrizi, Amit Sarkar, Anchit Arora, Andras Belokosztolszki, Anthony O’Sullivan, Archit Jain, Aygun Aydin, Ayoade Adeniyi, Ben Warren, Bob Baldwin, Brani Stojkovic, Brian Romanko, Can Lin, Carrie (Danning) Jiang, Chao Yang, Chris Ventura, Daniel Ohayon, Danny Gagne, David Taieb, Dmitry Ponomarev, Dong Jia, Dong Zhao, Eero Neuenschwander, Fang Wang, Ferhat Sahinkaya, Ferdi Adeputra, Fred Liu, Gayathri Aiyer, George Stasa, Guoqiang Jerry Chen, Haiyang Han, Haydar Imren, Henry Swanson, Ian Carmichael, Jared Greene, Jerry Pan, Jiang Wu, Johnnie Ballentyne, Joanna Jiang, Jonathan Bergeron, Joseph Li, Jun Fang, Kaustubh Karkare, Komal Mangtani, Kuldeep Chaudhary, Kunal Kataria, Lea Li, Lei Zhang, Liu Yang, Loka Potnuru, Luiz Ribeiro, Marc Celani, Matthieu Martin, Max Mazzeo, Meg Dymek, Mellany Flores, Mike Tarasyuk, Mital Mehta, Nevzat Sevim, Nick Gardner, Nikolay Kondratyev, Oliver Dodd, Pankaj Landge, Perry Stoll, Peter Nieuwenhuizen, Pranet Verma, Prashanth Bandaru, Piyush Khemka, Rahul Nambiar, Rajesh Nishtala, Rituraj Kirti, Roger (Wei) Li, Rujin Cao, Sahil Garg, Satish Sampath, Sean Wang, Seth Silverman, Shridhar Iyer, Simran Patil, Sriguru Chakravarthi, Sushaant Mujoo, Susmit Biswas, Taha Bekir Eren, Tejas Kudrimoti, Tony Harper, Vineet Chaudhary, Vishal Jain, Vitali Haravy, Vlad Fedorov, Vlad Gorelik, Wolfram Schuttle, Xiaotian Guo, Yatu Zhang, Yi Huang, Yuxi Zhang, Zejun Zhang, and Zhaohui Zhang. We would also like to express our gratitude to all reviewers of this post, including (in alphabetical order) Aleksandar Ilic, Avtar Brar, Brianna O’Steen, Chloe Lu, Chris Wiltz, Imogen Barnes, Jason Hendrickson, Rituraj Kirti, Xenia Habekoss and Yuri Claure. We would like to especially thank Jonathan Bergeron for overseeing the effort and providing all of the guidance and valuable feedback, and Ramnath Krishna Prasad for pulling required support together to make this blog post happen.", "label": "non_personal"}
{"title": "Building Private Processing for AI tools on WhatsApp", "url": "https://engineering.fb.com/2025/04/29/security/whatsapp-private-processing-ai-tools/", "content": "We are inspired by the possibilities of AI to help people be more creative, productive, and stay closely connected on WhatsApp, so we set out to build a new technology that allows our users around the world to use AI in a privacy-preserving way.\n\nWe’re sharing an early look into Private Processing, an optional capability that enables users to initiate a request to a confidential and secure environment and use AI for processing messages where no one — including Meta and WhatsApp — can access them.\n\nTo validate our implementation of these and other security principles, independent security researchers will be able to continuously verify our privacy and security architecture and its integrity.\n\nAI has revolutionized the way people interact with technology and information, making it possible for people to automate complex tasks and gain valuable insights from vast amounts of data. However, the current state of AI processing — which relies on large language models often running on servers, rather than mobile hardware — requires that users’ requests are visible to the provider. Although that works for many use cases, it presents challenges in enabling people to use AI to process private messages while preserving the level of privacy afforded by end-to-end encryption.\n\nWe set out to enable AI capabilities with the privacy that people have come to expect from WhatsApp, so that AI can deliver helpful capabilities, such as summarizing messages, without Meta or WhatsApp having access to them, and in the way that meets the following principles:\n\nOptionality: Using Meta AI through WhatsApp, including features that use Private Processing, must be optional.\n\nTransparency: We must provide transparency when our features use Private Processing.\n\nUser control: For people’s most sensitive chats that require extra assurance, they must be able to prevent messages from being used for AI features like mentioning Meta AI in chats, with the help of WhatApp’s Advanced Chat Privacy feature.\n\nIntroducing Private Processing\n\nWe’re excited to share an initial overview of Private Processing, a new technology we’ve built to support people’s needs and aspirations to leverage AI in a secure and privacy-preserving way. This confidential computing infrastructure, built on top of a Trusted Execution Environment (TEE), will make it possible for people to direct AI to process their requests — like summarizing unread WhatsApp threads or getting writing suggestions — in our secure and private cloud environment. In other words, Private Processing will allow users to leverage powerful AI features, while preserving WhatsApp’s core privacy promise, ensuring no one except you and the people you’re talking to can access or share your personal messages, not even Meta or WhatsApp.\n\nTo uphold this level of privacy and security, we designed Private Processing with the following foundational requirements:\n\nConfidential processing: Private Processing must be built in such a way that prevents any other system from accessing user’s data — including Meta, WhatsApp or any third party — while in processing or in transit to Private Processing.\n\nEnforceable guarantees: Attempts to modify that confidential processing guarantee must cause the system to fail closed or become publicly discoverable via verifiable transparency.\n\nVerifiable transparency: Users and security researchers must be able to audit the behavior of Private Processing to independently verify our privacy and security guarantees.\n\nHowever, we know that technology platforms like ours operate in a highly adversarial environment where threat actors continuously adapt, and software and hardware systems keep evolving, generating unknown risks. As part of our defense-in-depth approach and best practices for any security-critical system, we’re treating the following additional layers of requirements as core to Private Processing on WhatsApp:\n\nNon-targetability: An attacker should not be able to target a particular user for compromise without attempting to compromise the entire Private Processing system.\n\nStateless processing and forward security: Private Processing must not retain access to user messages once the session is complete to ensure that the attacker can not gain access to historical requests or responses.\n\nThreat modeling for Private Processing\n\nBecause we set out to meet these high-security requirements, our work to build Private Processing began with developing a threat model to help us identify potential attack vectors and vulnerabilities that could compromise the confidentiality, integrity, or availability of user data. We’ve worked with our peers in the security community to audit the architecture and our implementation to help us continue to harden them.\n\nBuilding in the open\n\nTo help inform our industry’s progress in building private AI processing, and to enable independent security research in this area, we will be publishing components of Private Processing, expanding the scope of our Bug Bounty program to include Private Processing, and releasing a detailed security engineering design paper, as we get closer to the launch of Private Processing in the coming weeks.\n\nWhile AI-enabled processing of personal messages for summarization and writing suggestions at users’ direction is the first use case where Meta applies Private Processing, we expect there will be others where the same or similar infrastructure might be beneficial in processing user requests. We will continue to share our learnings and progress transparently and responsibly.\n\nHow Private Processing works\n\nPrivate Processing creates a secure cloud environment where AI models can analyze and process data without exposing it to unauthorized parties.\n\nHere’s how it works:\n\nAuthentication: First, Private Processing obtains anonymous credentials to verify that the future requests are coming from authentic WhatsApp clients.\n\nThird-party routing and load balancing: In addition to these credentials, Private Processing fetches HPKE encryption public keys from a third-party CDN in order to support Oblivious HTTP (OHTTP).\n\nWire session establishment: Private Processing establishes an OHTTP connection from the user’s device to a Meta gateway via a third-party relay which hides requester IP from Meta and WhatsApp.\n\nApplication session establishment: Private Processing establishes a Remote Attestation + Transport Layer Security (RA-TLS) session between the user’s device and the TEE. The attestation verification step cross-checks the measurements against a third-party ledger to ensure that the client only connects to code which satisfies our verifiable transparency guarantee.\n\nRequest to Private Processing: After the above session is established, the device makes a request to Private Processing (e.g., message summarization request), that is encrypted end-to-end between the device and Private Processing with an ephemeral key that Meta and WhatsApp cannot access. In other words, no one except the user’s device or the selected TEEs can decrypt the request.\n\nPrivate Processing: Our AI models process data in a confidential virtual machine (CVM), a type of TEE, without storing any messages, in order to generate a response. CVMs may communicate with other CVMs using the same RA-TLS connection clients use to complete processing.\n\nResponse from Private Processing: The processed results are then returned to the user’s device, encrypted with a key that only the device and the pre-selected Private Processing server ever have access to. Private Processing does not retain access to messages after the session is completed.\n\nThe threat model\n\nIn designing any security-critical system, it is important to develop a threat model to guide how we build its defenses. Our threat model for Private Processing includes three key components:\n\nAssets : The sensitive data and systems that we need to protect.\n\nThreat actors : The individuals or groups that may attempt to compromise our assets.\n\nThreat scenarios : The ways in which our assets could be compromised, including the tactics, techniques, and procedures (TTPs) that threat actors might use.\n\nAssets\n\nIn the context of applying Private Processing to summarizing unread messages or providing writing suggestions at users’ direction, we will use Private Processing to protect messaging content, whether they have been received by the user, or still in draft form. We use the term “messages” to refer to these primary assets in the context of this blog.\n\nIn addition to messages, we also include additional, secondary assets which help support the goal of Private Processing and may interact with or directly process assets: the Trusted Computing Base (TCB) of the Confidential Virtual Machine (CVM), the underlying hardware, and the cryptographic keys used to protect data in transit.\n\nThreat actors\n\nWe have identified three threat actor types that could attack our system to attempt to recover assets.\n\nMalicious or compromised insiders with access to our infrastructure. A third party or supply chain vendor with access to components of the infrastructure. Malicious end users targeting other users on the platform.\n\nThreat scenarios\n\nWhen building Private Processing to be resilient against these threat actors, we consider relevant threat scenarios that may be pursued against our systems, including (but not limited to) the following:\n\nExternal actors directly exploit the exposed product attack surface or compromise the services running in Private Processing CVMs to extract messages.\n\nAnywhere the system processes untrusted data, there is potentially an attack surface for a threat actor to exploit. Examples of these kinds of attacks include exploitation of zero-day vulnerabilities or attacks unique to AI such as prompt injection.\n\nPrivate Processing is designed to reduce such an attack surface through limiting the exposed entry points to a small set of thoroughly reviewed components which are subject to regular assurance testing. The service binaries are hardened and run in a containerized environment to mitigate the risks of code execution and limit a compromised binary’s ability to exfiltrate data from within the CVM to an external party.\n\nInternal or external attackers extract messages exposed through the CVM.\n\nObservability and debuggability remains a challenge in highly secure environments as they can be at odds with the goal of confidential computing, potentially exposing side channels to identify data and in the worst case accidentally leaking messages themselves. However, deploying any service at scale requires some level of observability to identify failure modes, since they may negatively impact many users, even when the frequency is uncommon. We implement a log-filtering system to limit export to only allowed log lines, such as error logs.\n\nLike any complex system, Private Processing is built of components to form a complex supply chain of both hardware and software. Internally, our CVM build process occurs in restricted environments that maintain provenance and require multi-party review. Transparency of the CVM environment, which we’ll provide through publishing a third-party log of CVM binary digests and CVM binary images, will allow external researchers to analyze, replicate, and report instances where they believe logs could leak user data.\n\nInsiders with physical or remote access to Private Processing hosts interfere with the CVM at boot and runtime, potentially bypassing the protections in order to extract messages.\n\nTEE software exploitation is a growing area of security research, and vulnerability researchers have repeatedly demonstrated the ability to bypass TEE guarantees. Similarly, physical attacks on Private Processing hosts may be used to defeat TEE guarantees or present compromised hosts as legitimate to an end user.\n\nTo address these unknown risks, we built Private Processing on the principle of defense-in-depth by actively tracking novel vulnerabilities in this space, minimizing and sanitizing untrusted inputs to the TEE, minimizing attack surface through CVM hardening and enabling abuse detection through enhanced host monitoring.\n\nBecause we know that defending against physical access introduces significant complexity and attack surface even with industry-leading controls, we continuously pursue further attack surface hardening. In addition, we reduce these risks through measures like encrypted DRAM and standard physical security controls to protect our datacenters from bad actors.\n\nTo further address these unknown risks, we seek to eliminate the viability of targeted attacks via routing sessions through a third-party OHTTP relay to prevent an attacker’s ability to route a specific user to a specific machine.\n\nDesigning Private Processing\n\nHere is how we designed Private Processing to meet these foundational security and privacy requirements against the threat model we developed.\n\n(Further technical documentation and security research engagements updates are coming soon).\n\nConfidential processing\n\nData shared to Private Processing is processed in an environment which does not make it available to any other system. This protection is further upheld by encrypting data end-to-end between the client and the Private Processing application, so that only Private Processing, and no one in between – including Meta, WhatsApp, or any third-party relay – can access the data.\n\nTo prevent possible user data leakage, only limited service reliability logs are permitted to leave the boundaries of CVM.\n\nSystem software\n\nTo prevent privileged runtime access to Private Processing, we prohibit remote shell access, including from the host machine, and implement security measures including code isolation. Code isolation ensures that only designated code in Private Processing has access to user data. Prohibited remote shell access ensures that neither the host nor a networked user can gain access to the CVM shell.\n\nWe defend against potential source control and supply chain attacks by implementing established industry best practices. This includes building software exclusively from checked-in source code and artifacts, where any change requires multiple engineers to modify the build artifacts or build pipeline.\n\nAs another layer of security, all code changes are auditable. This allows us to ensure that any potential issues are discovered — either through our continuous internal audits of code, or by external security researchers auditing our binaries.\n\nSystem hardware\n\nPrivate Processing utilizes CPU-based confidential virtualization technologies, along with Confidential Compute mode GPUs, which prevent certain classes of attacks from the host operating system, as well as certain physical attacks.\n\nEnforceable guarantees\n\nPrivate Processing utilizes CPU-based confidential virtualization technologies which allow attestation of software based in a hardware root of trust to guarantee the security of the system prior to each client-server connection. Before any data is transmitted, Private Processing checks these attestations, and confirms them against a third-party log of acceptable binaries.\n\nStateless and forward secure service\n\nWe operate Private Processing as a stateless service, which neither stores nor retains access to messages after the session has been completed.\n\nAdditionally, Private Processing does not store messages to disk or external storage, and thus does not maintain durable access to this data.\n\nAs part of our data minimization efforts, requests to Private Processing only include data that is useful for processing the prompt — for example, message summarization will only include the messages the user directed AI to summarize.\n\nNon-targetability\n\nPrivate Processing implements the OHTTP protocol to establish a secure session with Meta routing layers. This ensures that Meta and WhatsApp do not know which user is connecting to what CVM. In other words, Meta and WhatsApp do not know the user that initiated a request to Private Processing while the request is in route, so that a specific user cannot be routed to any specific hardware.\n\nPrivate Processing uses anonymous credentials to authenticate users over OHTTP. This way, Private Processing can authenticate users to the Private Processing system, but remains unable to identify them. Private Processing does not include any other identifiable information as part of the request during the establishment of a system session. We limit the impact of small-scale attacks by ensuring that they cannot be used to target the data of a specific user.\n\nVerifiable transparency\n\nTo provide users visibility into the processing of their data and aid in validation of any client-side behaviors, we will provide capabilities to obtain an in-app log of requests made to Private Processing, data shared with it, and details of how that secure session was set up.\n\nIn order to provide verifiability, we will make available the CVM image binary powering Private Processing. We will make these components available to researchers to allow independent, external verification of our implementation.\n\nIn addition, to enable deeper bug bounty research in this area, we will publish source code for certain components of the system, including our attestation verification code or load bearing code.\n\nWe will also be expanding the scope of our existing Bug Bounty program to cover Private Processing to enable further independent security research into Private Processing’s design and implementation.\n\nFinally, we will be publishing a detailed technical white paper on the security engineering design of Private Processing to provide further transparency into our security practices, and aid others in the industry in building similar systems.\n\nGet Involved\n\nWe’re deeply committed to providing our users with the best possible messaging experience while ensuring that only they and the people they’re talking to can access or share their personal messages. Private Processing is a critical component of this commitment, and we’re excited to make it available in the coming weeks.\n\nWe welcome feedback from our users, researchers, and the broader security community through our security research program:", "label": "non_personal"}
{"title": "Introducing AutoPatchBench: A Benchmark for AI-Powered Security Fixes", "url": "https://engineering.fb.com/2025/04/29/ai-research/autopatchbench-benchmark-ai-powered-security-fixes/", "content": "We are introducing AutoPatchBench, a benchmark for the automated repair of vulnerabilities identified through fuzzing.\n\nBy providing a standardized benchmark, AutoPatchBench enables researchers and practitioners to objectively evaluate and compare the effectiveness of various AI program repair systems.\n\nThis initiative facilitates the development of more robust security solutions, and also encourages collaboration within the community to address the critical challenge of software vulnerability repair.\n\nAutoPatchBench is available now on GitHub.\n\nAI is increasingly being applied to solve security challenges, including repairing vulnerabilities identified through fuzzing. However, the lack of a standardized benchmark for objectively assessing AI-driven bug repair agents specific to fuzzing has impeded progress in academia and the broader community. Today, we are publicly releasing AutoPatchBench, a benchmark designed to evaluate AI program repair systems. AutoPatchBench sits within CyberSecEval 4, Meta’s new benchmark suite for evaluating AI capabilities to support defensive use cases. It features 136 fuzzing-identified C/C++ vulnerabilities in real-world code repos along with verified fixes sourced from the ARVO dataset.\n\nAutoPatchBench provides a standardized evaluation framework for assessing the effectiveness of AI-assisted vulnerability repair tools. This benchmark aims to facilitate a comprehensive understanding of the capabilities and limitations of various AI-driven approaches to repairing fuzzing-found bugs. By offering a consistent set of evaluation criteria, AutoPatchBench fosters transparency and reproducibility in research, enabling both academic and industry professionals to identify best practices and areas for improvement.\n\nFixing fuzzing-found vulnerabilities with AI\n\nFuzzing is a cornerstone in automated testing, renowned for its effectiveness in uncovering security vulnerabilities. By bombarding a target program with vast amounts of pseudo-random input data, fuzz testing exposes critical security and reliability issues, such as memory corruption, invalid pointer dereference, integer overflow, and parsing errors.\n\nHowever, resolving a fuzzing crash is often a labor intensive task, demanding intricate debugging and thorough code review to pinpoint and rectify the underlying cause. This process can be both time-consuming and resource-intensive. Unlike regular test failures, fuzzing bugs frequently reveal security vulnerabilities that pose severe threats to system integrity and user data. Given these stakes, automating the repair of fuzzing bugs with AI becomes not just advantageous but essential. AI’s ability to swiftly analyze patterns and propose solutions significantly reduces the time and effort required for repairs, making it an invaluable ally in safeguarding our digital environments.\n\nLet’s explore the process of addressing bugs identified through fuzzing by examining a demonstrative example. Consider the following C function, which harbors a read/write buffer overflow vulnerability:\n\n#include <stdio.h> #include <string.h> void process_input(const char *input) { char buffer[8]; strcpy(buffer, input); // Potential buffer overflow printf(\"Processed: %s\n\n\", buffer); }\n\nIn this scenario, a fuzzing harness might supply an input that surpasses the buffer’s capacity, leading to a crash due to buffer overflow. A typical stack trace from such a crash might appear as follows:\n\n== Fuzzer Crash Report == Program received signal SIGSEGV, Segmentation fault. 0x00007ffff7af1223 in strcpy () from /lib/x86_64-linux-gnu/libc.so.6 (gdb) bt #0 0x00007ffff7af1223 in strcpy () #1 0x0000555555555140 in process_input (input=0x7fffffffe695 \"AAAAAA...\") #2 0x0000555555555162 in main (argc=2, argv=0x7fffffffe5f8)\n\nHere, the process_input function invokes strcpy on a string that exceeds the eight-character buffer, causing a segmentation fault. A straightforward patch involves ensuring the copy operation remains within the buffer’s limits. This can be achieved by using a bounded copy function like strncpy or implementing a length check before copying:\n\nvoid process_input(const char *input) { char buffer[8]; strncpy(buffer, input, sizeof(buffer) - 1); buffer[sizeof(buffer) - 1] = '\\0'; printf(\"Processed: %s\n\n\", buffer); }\n\nThis patch ensures that the string remains within the buffer’s limits, effectively preventing out-of-bounds writes. Its correctness can be confirmed by verifying that the fuzzing input, which previously caused the crash, no longer does so. Additional checks can be conducted to ensure the patch doesn’t introduce any unintended side effects.\n\nAs illustrated, fixing a fuzzing crash involves:\n\nAnalyzing the crash stack trace and the target code. Pinpointing the root cause. Patching the vulnerable code. Verifying the fix’s accuracy.\n\nAn AI-based solution can automate these steps by utilizing an LLM’s capability to understand and generate code.\n\nWhy we developed AutoPatchBench\n\nAutoPatchBench is informed by key advancements in the field of AI-driven program repair, particularly those focusing on fuzzing-found vulnerabilities. Among the notable contributions is Google’s tech report on AI-powered patching, which pioneered the use of LLMs for addressing fuzzing crashes, achieving a 15% fix rate with their proprietary dataset. Subsequently, Google’s study on generic program repair agents introduced the GITS-Eval benchmark, encompassing 178 bugs across various programming languages.\n\nIn the realm of AI software engineering agents, benchmarks like SWE-Bench and SWE-Bench Verified have gained widespread acceptance for evaluating generic AI SWE agents. However, these benchmarks do not specifically tackle the unique challenges posed by fuzzing-found vulnerabilities, which demand specialized approaches that utilize fuzzing-specific artifacts and address security concerns.\n\nAutoPatchBench addresses this gap by offering a dedicated benchmark focused on a wide variety of C/C++ vulnerabilities of 11 crash types identified through fuzzing with automated verification capability. Unlike the broader focus of GITS-Eval and SWE-Bench, AutoPatchBench is specifically designed to assess the effectiveness of AI-driven tools in repairing security-critical bugs typically uncovered by fuzzing. This targeted approach enables a more precise evaluation of AI capabilities in meeting the complex requirements of fuzzing-found vulnerabilities, thereby advancing the field of AI-assisted program repair in a focused manner.\n\nInside AutoPatchBench\n\nWe’re making AutoPatchBench publicly available as part of CyberSecEval 4 to encourage community collaboration in tackling the challenge of automating fuzzing crash repairs. This benchmark is specifically designed for AI program repair agents focusing on C/C++ bugs identified through fuzzing. It includes real-world C/C++ vulnerabilities with verified fixes sourced from the ARVO dataset, and incorporates additional verification of AI-generated patches through fuzzing and white-box differential testing.\n\nARVO dataset\n\nThe ARVO dataset serves as the foundation for AutoPatchBench, offering a comprehensive collection of real-world vulnerabilities that are essential for advancing AI-driven security research. Sourced from C/C++ projects identified by Google’s OSS-Fuzz, ARVO includes over 5,000 reproducible vulnerabilities across more than 250 projects. Each entry is meticulously documented with a triggering input, a canonical developer-written patch, and the capability to rebuild the project in both its vulnerable and patched states.\n\nHowever, there are notable challenges when using the ARVO dataset as a benchmark for AI patch generation:\n\nWhile reproducibility is vital for a reliable benchmark, the ARVO dataset includes samples where crashes are not consistently reproducible. Some samples lack crash stack traces, making it exceedingly difficult to address the crash. Although ARVO provides a ground-truth fix for each identified vulnerability, it lacks an automated mechanism to verify the correctness of a generated patch. Objective automated verification is essential for a benchmark focused on patch generation.\n\nAutoPatchBench addresses these challenges by creating a curated subset and by employing a comprehensive and automated verification process.\n\nSelection criteria\n\nTo ensure the reliability and effectiveness of AutoPatchBench, we meticulously filtered the ARVO dataset samples based on the following criteria:\n\nValid C/C++ vulnerability: The ground-truth fix shall edit one or more C/C++ source files that are not fuzzing harnesses.\n\nDual-container setup : Each vulnerability is accompanied by two containers—one that contains vulnerable code and another for the fixed code—that build without error.\n\nReproducibility : The crash must be consistently reproducible within the vulnerable container.\n\nValid stack trace : A valid stack trace must be present within the vulnerable container to facilitate accurate diagnosis and repair.\n\nSuccessful compilation : The vulnerable code must compile successfully within its designated container, ensuring that the environment is correctly set up for testing.\n\nFixed code verification : The fixed code must also compile successfully within its respective container, confirming that the patch does not introduce new build issues.\n\nCrash resolution : The crash must be verified as resolved within the fixed container, demonstrating the effectiveness of the patch.\n\nFuzzing pass : The fixed code must pass a comprehensive fuzzing test without finding new crashes, ensuring that the ground-truth patch maintains the integrity and functionality of the software.\n\nAfter applying these rigorous selection criteria, we retained 136 samples for AutoPatchBench that fulfill the necessary conditions for both patch generation and verification. From this refined set, we created a down-sampled subset of 113 AutoPatchBench-Lite samples to provide a focused benchmark for testing AI patch generation tools. These subsets preserves the diversity and complexity of real-world vulnerabilities including 11 distinct crash types, offering a solid foundation for advancing AI-driven security solutions.\n\nPatch verification\n\nIn the process of patch generation, the patch generator utilizes two automated methods to verify the viability of a generated patch before submitting it for evaluation. The first method involves attempting to build the patched program, which checks for syntactic correctness. The second method involves attempting to reproduce the crash by running the input that initially triggered it. If the crash no longer occurs, it suggests that the issue has been resolved. However, these steps alone are insufficient to guarantee the correctness of the patch, as a patch might not maintain the program’s intended functionality, rendering it incorrect despite resolving the crash.\n\nTo address this issue, AutoPatchBench adopts a comprehensive approach to automate the evaluation of generated patches. This involves subjecting the patched code to further fuzz testing using the original fuzzing harness that initially detected the crash. Additionally, white-box differential testing compares the runtime behavior of the patched program against the ground truth repaired program, confirming that the patch has effectively resolved the underlying bug without altering the program’s intended functionality. Since a patch can potentially be made in multiple places, we cannot assume that the LLM will patch the same function as the groundtruth patch does. Instead we find all the callstacks for each call to a patched function. Then we find the lowest common ancestor (LCA) across all pairs of stacktraces offered by the groundtruth patch and the LLM patch. We then utilize debug information to inspect arguments, return values, and local variables at the first function above the LCA, differential testing offers a detailed view of the patch’s impact on the program state.\n\nThis process evaluates whether the generated patch produces a program state identical to the ground truth program after the patched function returns. By using a diverse set of inputs obtained from fuzzing, this gives higher confidence that the bug is fixed without changing the visible behavior of the patched functions. This differential testing is implemented using a Python script that leverages LLDB APIs to dump all visible states and identify differences between the ground truth and the patched program.\n\nHowever, as with all attempts to solve provably undecidable problems (in this case: program equivalence), there are some failure modes for this verification step. For example, sometimes the analysis fails with timeouts, in which case we consider the semantics to be preserved if both the ground truth and the LLM patch timed out. Programs might also behave non-deterministically, and we run each input three times to identify nondeterministic struct fields and values. Such fields will not be compared to avoid false alarms from noisy, random values. Additionally, we strip any fields that contain the substring “build” or “time” as we’ve observed false positives from build-ids (that happen to be deterministic within a program, but not across different patches).\n\nIt should also be noted that on a number of examples, the crashing PoC never actually triggered the breakpoints on the ground truth patch, making comparison of the resulting states impossible. However, our case study showed that white-box differential testing is still effective in filtering out a majority of incorrect patches despite its limitation, which will be discussed in the case study.\n\nAutoPatchBench and AutoPatchBench-Lite\n\nAutoPatchBench is a comprehensive benchmark dataset of 136 samples. It encompasses a wide range of real-world vulnerabilities, providing a robust framework for assessing the capabilities of automated patch generation systems.\n\nWithin this benchmark, we have also created a subset called AutoPatchBench-Lite that consists of 113 samples. AutoPatchBench-Lite focuses on a simpler subset of vulnerabilities where the root cause of the crash is confined to a single function. This version is designed to cater to scenarios where the complexity of the bug is relatively low, making it more accessible for tools that are in the early stages of development or for those that specialize in handling straightforward issues.\n\nThe rationale for creating AutoPatchBench-Lite stems from the observation that when root causes are distributed across multiple locations within the code, the difficulty of generating a correct patch increases significantly. Addressing such “hard” crashes requires a tool to possess advanced reasoning capabilities to analyze larger codebases and apply patches to multiple areas simultaneously. This complexity not only challenges the tool’s design but also demands a higher level of sophistication in its algorithms to ensure accurate and effective patching.\n\nBy offering both AutoPatchBench and AutoPatchBench-Lite, we provide a tiered approach to benchmarking, allowing developers to progressively test and refine their tools. This structure supports the development of more advanced solutions capable of tackling both simple and complex vulnerabilities, ultimately contributing to the enhancement of AI-assisted bug repair techniques.\n\nExpected use cases\n\nAutoPatchBench offers significant value to a diverse range of users. Developers of auto-patch tools can leverage our open-sourced patch generator to enhance their tools and assess their effectiveness using the benchmark. Software projects employing fuzzing can incorporate our open-sourced patch generator to streamline vulnerability repair. Additionally, model developers can integrate the benchmark into their development cycles to build more robust and specialized expert models for bug repair. The tooling around the patch generator provided here can also be used in reinforcement learning as a reward signal during training. This data helps train models to better understand the nuances of bug repair, enabling them to learn from past fixes and improve their ability to generate accurate patches.\n\nReference implementation\n\nWe developed a basic patch generator to establish a baseline performance using AutoPatchBench. This generator is specifically designed to address simple crashes that involve patching a single function. We have open-sourced this reference implementation to encourage the community to build and expand upon it.\n\nFigure 1 shows a high-level overview of its design. The patch generator takes crash stack trace and target source code as input. It identifies the source locations from the stack trace and extracts every function that contains those locations. It then asks the LLM to identify the root cause and repair the crash by patching one of the functions. Upon receiving a response from the LLM, the patch generator extracts the revised code, applies the patch, compiles the program, and tests it against the original input that caused the crash. If the build or test fails, we re-engage the LLM with the error message from the build or test output, requesting it to attempt a solution again until the crash is resolved. If a fix trajectory fails to reach a valid solution that passes build and crash reproduction within a finite number of steps, we start a new trajectory to reset the context window, preventing prolonged entrapment in an incorrect path.\n\nThe example prompt below also captures the essence of our approach. Note that the actual prompt is more detailed and divided into multiple segments to promote chain-of-thought reasoning.\n\nAs an experienced Security Engineer at Meta, your task is to address the following security-critical fuzzing crash. Below is the stack trace of the crash: == Fuzzer Crash Report == Program received signal SIGSEGV, Segmentation fault. 0x00007ffff7af1223 in strcpy () from /lib/x86_64-linux-gnu/libc.so.6 (gdb) bt #0 0x00007ffff7af1223 in strcpy () #1 0x0000555555555140 in process_input (input=0x7fffffffe695 \"AAAAAA...\") #2 0x0000555555555162 in main (argc=2, argv=0x7fffffffe5f8) Here is the source code for the functions involved in the stack trace: strcpy() {...} void process_input(const char *input) { char buffer[8]; strcpy(buffer, input); // Potential buffer overflow printf(\"Processed: %s\n\n\", buffer); } int main() {...} Assuming the root cause of the crash is within one of these functions, generate a patched version of the faulty function to resolve the fuzzing crash. Ensure that you provide a complete rewrite of the function so that the patch can be applied and the code compiled without errors.\n\nA case study with AutoPatchBench-Lite\n\nIn the case study, we demonstrate the use of AutoPatchBench by evaluating our reference patch generator with several LLM models. Given that our reference implementation is limited to addressing simple issues, we conducted our evaluation with AutoPatchBench-Lite, which contains 113 samples. To prevent fix trajectories from becoming excessively prolonged, we capped the maximum length of each trajectory at five. Additionally, we set the maximum number of retries to 10.\n\nPlease note that the case study is not intended to provide a statistically rigorous comparison of model performance. Instead, it aims to present preliminary results to establish a baseline expectation. We encourage future research to build upon these findings.\n\nEffectiveness of patch generation and verification\n\nWe evaluated the effectiveness of the patch generator and our automated verification processes while using different LLM models as back-end. The figure below illustrates the effectiveness of patch generation and verification by presenting the percentage of samples that successfully passed each sequential verification step: (1) patch validity: build and crash reproducibility check, (2) fuzzing pass: passes 10-minute fuzzing, and (3) testing pass: passes white-box differential testing. It is important to note that the patch generation process only utilizes step (1) to verify the build and crash reproducibility. The fuzzing and differential testing are conducted post-generation to assess correctness.\n\nFigure 2 shows that all models achieved similar generation success rates of around 60% and similar post-verification success rates of around 5-11% with overlapping confidence intervals, and therefore, we do not draw any conclusion about their relative performance. The graph does, however, reveal that a substantial portion of the generated patches are found to be incorrect when subjected to fuzzing and white-box differential testing. For instance, Gemini 1.5 Pro achieved a 61.1% patch generation success rate, yet fewer than 15% of these patches (5.3% out of total set) were found to be correct. This gap highlights that build and crash reproduction are not good enough signals to infer the correctness of generated patches, and that future patch generation approaches should scrutinize the semantic preservation of generated patches more thoroughly. This gap also underscores the vital role of the comprehensive verification processes that checks semantic equivalence, a distinctive contribution of AutoPatchBench.\n\nEffect of inference-time computation\n\nTo assess the impact of inference-time computation on improving the patch generation success rate, we present the distribution of retry counts among the 73 patches produced by Llama 4 Maverick.\n\n\n\nFigure 3 shows that 44 out of 73 patches, or 60.2%, were successfully generated on the first attempt. The remaining 40% of the samples required more than two iterations, with no evident plateau until the 10th iteration. This outcome demonstrates that allocating more computational resources during inference-time leads to a higher success rate and suggests that increasing the number of retries could yield better results.\n\nManual validation\n\nIn our investigation of the precision and recall of white-box differential testing, we conducted a manual validation of 44 patches that passed 10-minute fuzzing against human-written ground truth fixes with the help of security experts. These patches were selected from a pool of 73 generated by Llama 4 Maverick. The following table shows the confusion matrix.\n\nTable 1: Confusion matrix between human judgement and differential testing\n\nTest pass Test fail Sum Human pass 5 0 5 Human reject 7 32 39 Sum 12 32 44\n\nThe results showed that the differential testing achieved an accuracy of 84.1% for this sample (5 + 32 / 44), indicating a high overall agreement with the human assessment. However, a closer examination of the confusion matrix revealed a notable discrepancy between precision and recall. Specifically, the testing method demonstrated 100.0% recall in this case study, correctly identifying all 5 instances that humans judged as correct. In contrast, precision was relatively low (41.7%), with 7 false positives out of 12 total positive predictions. This suggests that differential testing reported success on some incorrect patches as well, highlighting the need for manual validation of patch correctness. Despite this shortcoming, the result clearly shows the utility of differential testing in automatically rejecting a substantial number of incorrect patches, which will substantially save the manual validation effort.\n\nKey insights\n\nOur case study revealed several limitations of the current patch generator.\n\nThe root cause may not exist in the stack trace\n\nFrequently, crashes are the result of state contamination that occurs prior to the crash being triggered. Consequently, none of the functions within the stack frames may include the code responsible for the root cause. Since our current implementation requires the LLM to assume that the root cause is located within one of the functions in the stack trace, it is unable to generate an accurate patch in such cases. Solving this problem would require a more autonomous agent which can reason about the root cause on its own with a code browsing capability.\n\nCheating\n\nIn some instances, the LLM resorted to “cheating” by producing patches that superficially resolved the issue without addressing the underlying problem. This can occur when the generator modifies or removes code in a way that prevents the crash from occurring, but does not actually fix the root cause of the issue. We observed that cheating happens more frequently when we request the LLM to retry within the same trajectory. A potential solution to this could be to empower the LLM to say “I cannot fix it,” which may come with a tradeoff with success rate. However, note that most of the cheating was caught in the verification step, highlighting the utility of differential testing.\n\nNeed for enhanced patch verification methods\n\nFuzzing and white-box differential testing have shown that a large majority of generated patches are incorrect when compared to the ground-truth patches. This finding highlights the challenge of generating accurate patches without enhanced verification capabilities. To address this gap, several approaches can be considered:\n\nA patch generator could provide additional code context when querying the LLM for a patch so that LLM can better understand the consequence of a code patch.\n\nA patch generator could make additional LLM queries to verify the perseverance of existing functionality.\n\nA patch generator can attempt to generate multiple valid patches by exploring multiple trajectories in parallel, and let LLM choose the best option that is most likely to be correct.\n\nIn a well-tested real-world codebase, a patch generator can utilize existing tests to validate the patches it creates. This process complements building the code and checking for crash reproduction, allowing the patch generator to retry if a patch fails the tests. The accuracy of the generated patches is largely dependent on the thoroughness of the existing tests.\n\nIn conclusion, while our study has identified several challenges with the current patch generation process, it also opens up opportunities for improvement. By addressing these limitations with innovative solutions, we can enhance the accuracy and reliability of patch generation, paving the way for more robust and effective automated tools.\n\nGet started with AutoPatchBench\n\nAutoPatchBench is now available on GitHub. We welcome pull requests to integrate new/additional agent architectures into the framework, and look forward to seeing how well they perform on AutoPatchBench.", "label": "non_personal"}
{"title": "Taking the plunge: The engineering journey of building a subsea cable", "url": "https://engineering.fb.com/2025/05/01/connectivity/taking-the-plunge-the-engineering-journey-of-building-a-subsea-cable/", "content": "Meta develops infrastructure all across the globe to transport information and content for the billions of people using our services around the world. At the core of this infrastructure are aggregation points – like data centers – and the digital cables that connect them. Subsea cables – the unseen digital highways of the internet – are critical for Meta to serve people wherever they are in the world. In fact, more than 95% of the world’s intercontinental traffic goes through subsea cables.\n\nMeta’s engineering team prioritizes both innovation and quality when designing and deploying these cables. In the latest Meta Tech Podcast, Andy Palmer-Felgate and Pascal Pecci, both subsea cable systems engineers, join Pascal Hartig on the Meta Tech podcast to discuss the latest in subsea engineering technology. This episode dives deeper into the engineering nuances of large-scale subsea cable projects like the recently announced Project Waterworth.\n\nLearn more about Meta’s work on these engineering feats. Download or listen to the episode below:\n\nThe Meta Tech Podcast is a podcast, brought to you by Meta, where we highlight the work Meta’s engineers are doing at every level – from low-level frameworks to end-user features.\n\nSend us feedback on Instagram, Threads, or X.\n\n\n\nAnd if you’re interested in learning more about career opportunities at Meta, visit the Meta Careers page.", "label": "non_personal"}
{"title": "Enhancing the Python ecosystem with type checking and free threading", "url": "https://engineering.fb.com/2025/05/05/developer-tools/enhancing-the-python-ecosystem-with-type-checking-and-free-threading/", "content": "Meta and Quansight have improved key libraries in the Python Ecosystem. There is plenty more to do and we invite the community to help with our efforts.\n\nWe’ll look at two key efforts in Python’s packaging ecosystem to make packages faster and easier to use:\n\n🚀 Unlock performance wins for developers through free-threaded Python – where we leverage Python 3.13’s support for concurrent programming (made possible by removing the Global Interpreter Lock (GIL)).\n\n✅ Increase developer velocity in the IDE with improved type annotations.\n\nEnhancing typed Python in the Python scientific stack\n\nType hints, introduced in Python 3.5 with PEP-484, allow developers to specify variable types, enhancing code understanding without affecting runtime behavior. Type-checkers validate these annotations, helping prevent bugs and improving IDE functions like autocomplete and jump-to-definition. Despite their benefits, adoption is inconsistent across the open source ecosystem, with varied approaches to specifying and maintaining type annotations.\n\nThe landscape of open source software is fractured with respect to how type annotations are specified, maintained, and distributed to end users. Some projects have in-line annotations (types directly declared in the source code directly), others keep types in stub files, and many projects have no types at all, relying on third party repositories such as typeshed to provide community-maintained stubs. Each approach has its own pros and cons, but application and maintenance of them has been inconsistent.\n\nMeta and Quansight are addressing this inconsistency through:\n\nDirect contributions: We have improved the type coverage for pandas-stubs and numpy, and are eager to expand the effort to more packages. Community engagement: Promoting type annotation efforts to encourage community involvement, listen to feedback and create actionable ways to improve the ecosystem. Tooling and automation: Developing tools to address common challenges adding types and keeping the types up-to-date with the source code.\n\nImproved type annotations in pandas\n\nTL;DR: Pandas is the second most downloaded package from the Python scientific stack. We improved pandas-stubs package type annotation coverage from 36% to over 50%.\n\nBackground\n\nThe pandas community maintains its own stubs in a separate repository, which must be installed to obtain type annotations. While these stubs are checked separately from the source code, it allows the community to use types with their own type checking and IDE.\n\nImproving type coverage\n\nWhen we began our work in pandas-stubs, coverage was around 36%, as measured by the percentage of parameters, returns, and attributes that had a complete type annotation (the annotation is present and all generics have type arguments). After several weeks of work and about 30 PRs, type completeness is now measured at over 50%. The majority of our contributions involved adding annotations to previously-untyped parameters, adding type arguments to raw generic types, and removing deprecated/undocumented interfaces. We also improved several inaccurate annotations and updated others to match the inline annotations in the pandas source code.\n\nKey introductions\n\nTwo key introductions significantly increased coverage:\n\nReplacing raw Series types with UnknownSeries , a new type aliased to Series[Any] . When applied to return type annotations, this reduces the number of type checker false-positives when the function is called.\n\nImproving types of core Dataframe operations like insert, combine, replace, transpose, and assign, as well as many timestamp and time-zone related APIs.\n\nTooling development\n\nIn addition to improving coverage directly, we developed tooling to catalog public interfaces missing annotations. We also augmented our tools for measuring type coverage to handle the situation where stubs are distributed independently, rather than being packaged into the core library wheel.\n\nWhat is free-threaded Python ?\n\nFree-threaded Python (FTP) is an experimental build of CPython that allows multiple threads to interact with the VM in parallel. Previously, access to the VM required holding the global interpreter lock (GIL), thereby serializing execution of concurrently running threads. With the GIL becoming optional, developers will be able to take full advantage of multi-core processors and write truly parallel code.\n\nBenefits of free-threaded Python\n\nThe benefits of free-threaded Python are numerous:\n\nTrue parallelism in a single process : With the GIL removed, developers can write Python code that takes full advantage of multi-core processors without needing to use multiple processes. CPU-bound code can execute in parallel across multiple cores.\n\nImproved performance: By allowing multiple threads to execute Python code simultaneously, work can be effectively distributed across multiple threads inside a single process.\n\nSimplified concurrency: Free-threading provides developers with a more ergonomic way to write parallel programs in Python. Gone are the days of needing to use multiprocessing.Pool and/or resorting to custom shared memory data structures to efficiently share data between worker processes.\n\nGetting Python’s ecosystem ready for FTP\n\nThe ecosystem of Python packages must work well with free-threaded Python in order for it to be practically useful; application owners can’t use free-threading unless their dependencies work well with it. To that end, we have been taking a “bottoms up” approach to tackle the most difficult/popular packages in the ecosystem. We’ve added free-threading support to many of the most popular packages used for scientific computing (e.g. numpy, scipy, scikit-learn) and language bindings (e.g. Cython, nanobind, pybind, PyO3).\n\nJust getting started\n\nTogether, we made substantial progress in improving type annotations and free-threading compatibility in Python libraries. We couldn’t have done it without the Python community and are asking others to join our efforts. Whether it’s further updates to the type annotations or preparing your code for FTP, we value your help moving the Python ecosystem forward!\n\nTo learn more about Meta Open Source, visit our open source site, subscribe to our YouTube channel, or follow us on Facebook, Threads, X and LinkedIn.", "label": "non_personal"}
{"title": "Accelerating GPU indexes in Faiss with NVIDIA cuVS", "url": "https://engineering.fb.com/2025/05/08/data-infrastructure/accelerating-gpu-indexes-in-faiss-with-nvidia-cuvs/", "content": "Meta and NVIDIA collaborated to accelerate vector search on GPUs by integrating NVIDIA cuVS into Faiss v1.10 , Meta’s open source library for similarity search.\n\nThis new implementation of cuVS will be more performant than classic GPU-accelerated search in some areas.\n\nFor inverted file (IVF) indexing, NVIDIA cuVS outperforms classical GPU-accelerated IVF build times by up to 4.7x; and search latency is reduced by as much as 8.1x.\n\nFor graph indexing, CUDA ANN Graph (CAGRA) outperforms CPU Hierarchical Navigable Small World graphs (HNSW) build times by up to 12.3x; and search latency is reduced by as much as 4.7x.\n\nThe Faiss library\n\nThe Faiss library is an open source library, developed by Meta FAIR, for efficient vector search and clustering of dense vectors. Faiss pioneered vector search on GPUs, as well as the ability to seamlessly switch between GPUs and CPUs. It has made a lasting impact in both research and industry, being used as an integrated library in several databases (e.g., Milvus and OpenSearch), machine learning libraries, data processing libraries, and AI workflows. Faiss is also used heavily by researchers and data scientists as a standalone library, often paired with PyTorch.\n\nCollaboration with NVIDIA\n\nThree years ago, Meta and NVIDIA worked together to enhance the capabilities of vector search technology and to accelerate vector search on GPUs. Previously, in 2016, Meta had incorporated high performing vector search algorithms made for NVIDIA GPUs: GpuIndexFlat; GpuIndexIVFFlat; GpuIndexIVFPQ. After the partnership, NVIDIA rapidly contributed GpuIndexCagra, a state-of-the art graph-based index designed specifically for GPUs. In its latest release, Faiss 1.10.0 officially includes these algorithms from the NVIDIA cuVS library.\n\nFaiss 1.10.0 also includes a new conda package that unlocks the ability to choose between the classic Faiss GPU implementations and the newer NVIDIA cuVS algorithms, making it easy for users to switch between GPU and CPU.\n\nBenchmarking\n\nThe following benchmarks were conducted using the cuVS-bench tool.\n\nWe measured:\n\nA tall, slender image dataset: A subset of 100 million vectors from the Deep1B dataset by 96 dimensions.\n\nA short, wide dataset of text embeddings: 5 million vector embeddings, curated using the OpenAI text-embedding-ada-002 model .\n\nTests for index build times and search latency were conducted on an NVIDIA H100 GPU and compared to an Intel Xeon Platinum 8480CL system. Results are reported in the tables below at 95% recall along the pareto frontiers for k=10 nearest neighbors.\n\nBuild time (95% recall@10)\n\nIndex Embeddings\n\n100M x 96\n\n(seconds) Embeddings\n\n5M x 1536\n\n(seconds) Faiss Classic Faiss cuVS Faiss Classic Faiss cuVS Faiss Classic Faiss cuVS IVF Flat IVF Flat 101.4 37.9 (2.7x) 24.4 15.2 (1.6x) IVF PQ IVF PQ 168.2 72.7 (2.3x) 42.0 9.0 (4.7x) HNSW (CPU) CAGRA 3322.1 518.5 (6.4x) 1106.1 89.7 (12.3x)\n\nTable 1: Index build times for Faiss-classic and Faiss-cuVS in seconds (with NVIDIA cuVS speedups in parentheses).\n\nSearch latency (95% recall@10)\n\nIndex Embeddings\n\n100M x 96\n\n(milliseconds) Embeddings\n\n5M x 1536\n\n(milliseconds) Faiss Classic Faiss cuVS Faiss Classic Faiss cuVS Faiss Classic Faiss cuVS IVF Flat IVF Flat 0.75 0.39 (1.9x) 1.98 1.14 (1.7x) IVF PQ IVF PQ 0.49 0.17 (2.9x) 1.78 0.22 (8.1x) HNSW (CPU) CAGRA 0.56 0.23 (2.4x) 0.71 0.15 (4.7x)\n\nTable 2: Online (i.e., one at a time) search query latency for Faiss-classic and Faiss-cuVS in milliseconds (with NVIDIA cuVS speedups in parentheses).\n\nLooking forward\n\nThe emergence of state-of-the-art NVIDIA GPUs has revolutionized the field of vector search, enabling high recall and lightning-fast search speeds. The integration of Faiss and cuVS will continue to incorporate state-of-the-art algorithms, and we look forward to unlocking new innovations in this partnership between Meta and NVIDIA.\n\nRead here for more details about NVIDIA cuVS.", "label": "non_personal"}
{"title": "Open-sourcing Pyrefly: A faster Python type checker written in Rust", "url": "https://engineering.fb.com/2025/05/15/developer-tools/open-sourcing-pyrefly-a-faster-python-type-checker-written-in-rust/", "content": "Back in 2017, engineers at Meta sought to create a type checker for Instagram’s typed Python codebase. Years later, as the type system continued to evolve, that type checker eventually became Pyrefly.\n\nPyrefly is a new type checker and IDE experience for Python, written with Rust, and now available for the entire Python community to use! It’s open-source, supports both CLI usage and IDE integration. and is designed to help you catch errors before runtime in Python codebases of any size.\n\nOn this episode of the Meta Tech Podcast, Pascal Hartig sits down with Maggie, Rebecca, and Neil — some of the team behind Pyrefly — to discuss this latest release from Meta and how they built an incremental type checker that scales to mono repositories.\n\nDownload or listen to the episode below:\n\nYou can also find the episode wherever you get your podcasts, including:\n\nThe Meta Tech Podcast is a podcast, brought to you by Meta, where we highlight the work Meta’s engineers are doing at every level – from low-level frameworks to end-user features.\n\nSend us feedback on Instagram, Threads, or X.\n\nAnd if you’re interested in learning more about career opportunities at Meta visit the Meta Careers page.\n\nLinks", "label": "non_personal"}
{"title": "Meta’s Full-stack HHVM optimizations for GenAI", "url": "https://engineering.fb.com/2025/05/20/web/metas-full-stack-hhvm-optimizations-for-genai/", "content": "As Meta has launched new, innovative products leveraging generative AI (GenAI), we need to make sure the underlying infrastructure components evolve along with it. Applying infrastructure knowledge and optimizations have allowed us to adapt to changing product requirements, delivering a better product along the way. Ultimately, our infrastructure systems need to balance our need to ship high-quality experiences with a need to run systems sustainability.\n\nSplitting GenAI inference traffic out into a dedicated WWW tenant, which allows specialized runtime and warm-up configuration, has enabled us to meet both of those goals while delivering a 30% improvement in latency.\n\nWho we are\n\nAs the Web Foundation team, we operate Meta’s monolithic web tier, running Hack. The team is composed of cross-functional engineers who make sure the infrastructure behind the web tier is healthy and well designed. We jump into incident response, work on some of the most complex areas of the infrastructure, and help build whatever we need to keep the site happily up and running.\n\nTo accomplish this, we have established a series of best practices on being a “good citizen” of the shared tier. We need to ensure that all requests comply with these guidelines to prevent issues from spilling over and affecting other teams’ products. One core rule is the request runtime—limiting a request to 30 seconds of execution. This is a consequence of the HHVM (HipHop Virtual Machine) runtime—each request has a corresponding worker thread, of which there is a finite number. To ensure there are always threads available to serve incoming requests, we need to balance the resources available on each host with its expected throughput. If requests are taking too long, there will be fewer available threads to process new requests, leading to user-visible unavailability.\n\nThe changing landscape\n\nClassically, webservers at Meta are optimized for serving front-end requests—rendering webpages and serving GraphQL queries. These requests’ latency is typically measured in hundreds of milliseconds to seconds (substantially below the 30-second limit), which enables hosts to process approximately 500 queries per second.\n\nAdditionally, a web server will spend about two-thirds of its time doing input/output (I/O), and the remaining third doing CPU work. This fact has influenced the design of the Hack language, which supports asyncio, a type of cooperative multi-tasking, and all the core libraries support these primitives to increase performance and decrease the amount of time the CPU is sitting idle, waiting for I/O.\n\nGenAI products, especially LLMs, have a different set of requirements. These are driven by the core inference flow: The model responds with a stream of tokens that can take seconds or minutes to complete. A user may see this as a chatbot “typing” a response. This isn’t an effect to make our products seem friendlier; it’s the speed at which our models think! After a user submits a query to the model, we need to start streaming these responses back to the user as fast as possible. On top of that, the total latency of the request is now substantially longer (measured in seconds). These properties have two effects on the infrastructure—minimal overhead on the critical path before calling the LLM, and a long duration for the rest of the request, most of which is spent waiting on I/O. (See Figures 1 and 2 below).\n\nA series of optimizations\n\nThis shift in requirements allowed Web Foundation to reexamine the rules of running the monolithic web tier. We then launched a dedicated web tenant (a standalone deployment of WWW) that allowed custom configuration, which we could better tune to the needs of the workload.\n\nRequest timeout\n\nFirst, running on an isolated web tier allowed us to increase the runtime limit for GenAI requests. This is a straightforward change, but it allowed us to isolate the longer-running traffic to avoid adverse impacts on the rest of the production tier. This way, we can avoid requests timing out if inference takes longer than 30 seconds.\n\nThread-pool sizing\n\nRunning requests for longer means there is reduced availability of worker threads (which, remember, map 1:1 with processed requests). Since webservers have a finite amount of memory, we can divide the total memory available by the per-request memory limit to get a peak number of active requests; this in turn tells us how many requests we can execute simultaneously. We ended up running with approximately 1000 threads on GenAI hosts, as compared to a couple of hundred on normal webservers.\n\nJIT cache and “jumpstart”\n\nHHVM is a just-in-time (JIT) interpreted language, which means the first time a given function executes, the machine needs to compile it to lower-level machine code for execution. Additionally, a technique called Jump-Start allows a webserver to seed its JIT cache with outputs from a previously warmed server. By allowing GenAI hosts to use Jump-Start profiles from the main web tier, we are able to greatly speed up execution, even if the code overlap is not identical.\n\nRequest warm-up\n\nHHVM also supports the execution of dummy requests at server startup, which we can execute, and then we can discard the results. The intent here is to warm non-code caches within the webserver. Configuration values and service discovery info are normally fetched inline the first time they are needed and then cached within the webserver. By fetching and caching this information in warm-up requests, we prevent our users from observing the latency of these initial fetches.\n\nShadow traffic\n\nFinally, Meta heavily uses real-time configuration to control feature rollouts, which means that jumpstart profiles consumed at startup time might not cover all future code paths the server will execute. To maintain coverage in the steady state, we also added request shadowing, so we can ensure that gating changes are still covered in the JIT cache.", "label": "non_personal"}
{"title": "Journey to 1000 models: Scaling Instagram’s recommendation system", "url": "https://engineering.fb.com/2025/05/21/production-engineering/journey-to-1000-models-scaling-instagrams-recommendation-system/", "content": "In this post, we explore how Instagram has successfully scaled its algorithm to include over 1000 ML models without sacrificing recommendation quality or reliability.\n\nWe delve into the intricacies of managing such a vast array of models, each with its own performance characteristics and product goals.\n\nWe share insights and lessons learned along the way—from the initial realization that our infrastructure maturity was lagging behind our ambitious scaling goals, to the innovative solutions we implemented to bridge these gaps.\n\nIn the ever-evolving landscape of social media, Instagram serves as a hub for creative expression and connection, continually adapting to meet the dynamic needs of its global community. At the heart of this adaptability lies a web of machine learning (ML) models, each playing a crucial role in personalizing experiences. As Instagram’s reach and influence has grown, so too has the complexity of its algorithmic infrastructure. This growth, while exciting, presents a unique set of challenges, particularly in terms of reliability and scalability.\n\nJoin us as we uncover the strategies and tools that have enabled Instagram to maintain its position at the forefront of social media innovation, ensuring a seamless and engaging experience for billions of users worldwide.\n\nAre there really that many ML models in Instagram?\n\nThough what shows up in Feed, Stories, and Reels is personally ranked, the number of ranked surfaces goes much deeper—to which comments surface in Feed, which notifications are “important,” or whom you might tag in a post. These are all driven by ML recommendations.\n\nWithin a given surface, we’ll have different layers of the ranking funnel: sourcing (retrieval), early-stage ranking (ESR), and late-stage ranking (LSR). We operate on fewer candidates as we progress through the funnel, as the underlying operations grow more expensive (see Figure 1 below):\n\nWithin each surface and layer, there is constant experimentation, and these permutations create a severe infrastructure challenge. We need to allow room for our ML engineers to experiment with changes such as adjusting weights for a given prediction. The net result, depicted below in Figure 2, is a large number of models serving user traffic in production:\n\nHow did we realize infra maturity wasn’t going to catch up?\n\nIdentified risks\n\nWe identified several risks associated with scaling our algorithm, rooted in complaints about ML productivity and repeating patterns of issues:\n\nDiscovery: Even as a team focused on one app — Instagram — we couldn’t stay on top of the growth, and product ML teams were maintaining separate sources of truth, if any, for their models in production.\n\nRelease: We didn’t have a consistent way to launch new models safely, and the process was slow, impacting ML velocity and, therefore, product innovation.\n\nHealth: We lacked a consistent definition of model prediction quality, and with the diversity of surfaces and subtlety of degraded ranking, quality issues went unnoticed.\n\nSolution overview\n\nTo address these risks, we implemented several solutions:\n\nModel registry: We built a registry that serves as a ledger for production model importance and business function foremost, among other metadata. This registry serves as our foundational source of truth, upon which we can leverage automation to uplevel system-wide observability, change management, and model health.\n\nModel launch tooling: We developed a more ideal flow for launching new models that includes estimation, approval, prep, scale-up, and finalization. This process is now automated, and we’ve reduced the time it takes to launch a new model from days to hours.\n\nModel stability: We defined and operationalized model stability, a pioneering metric that measures the accuracy of our model predictions. We’ve leveraged model stability to produce SLOs for all models in the model registry, which enables simple understanding of the entire product surface’s ML health.\n\nModel registry\n\nWhat did model investigations look like prior to the registry?\n\nBefore we created the model registry, the investigation process was a time-consuming and error-prone experience for on-call engineers and model owners. An on-call engineer had to ask multiple questions to model owners to gather information, as depicted Figure 3 below, about the context of what this model does in the stack and to clarify how important it is to the business.\n\nUnderstanding this context is extremely important to the operational response: Depending on the importance of the model and the criticality of the surface it’s supporting, the response is going to differ in kind. When a model is an experiment serving a small percentage of the traffic, an appropriate response can be to end the experiment and reroute the traffic back to the main model (the baseline). But if there’s a problem with the baseline model that needs to be handled with urgency, it’s not possible to “just turn it off.” The engineer on call has to loop in the model owner, defeating the purpose of having a dedicated on-call.\n\nTo avoid holding up an operational response on a single POC, we needed a central source of truth for model importance and business function. What if the model is not available? What if 10 of these issues happen concurrently?\n\nWith the development of the model registry, we standardized the collection of model importance and business function information, ensuring most of our operational resources were going towards the most important models.\n\nWhat problems did the model registry solve?\n\nThe model registry is a system of record built on top of Configerator, Meta’s distributed configuration suite . This schematized ledger (see an example in Figure 4 and detailed further below) provides read-and-write access to operational data based on the inventory of production models. It’s a flexible and extensible foundation upon which one can build automation and tools to solve problems that are specific to individual organizations within Meta that are not served by the general tooling.\n\nAs Instagram scaled its investment in AI through rapid innovation in content recommendations, the number of models and AI assets grew; as a result, it has been increasingly important — but also increasingly difficult — to maintain a minimum standard for all of our models, as we lacked an authoritative source for the business context as well as for a model’s importance.\n\nIn creating the model registry, we set out to provide a structured interface for collecting business context via model types, importance via criticality, and additional metadata that would enable model understanding. Below, we’ll get into the model types, criticality, and automation we’ve built for this purpose.\n\nModel types\n\nAt a high level, model type describes the purpose for the ML workload where it represents a category or class of models that share a common purpose or are used in similar contexts. For example, we have “ig_stories_tray_mtml” which is a string attached to training flows, model checkpoints, inference services, and more. Put simply, a model type identifies for the reader this model’s purpose in the ranking funnel.\n\nLet’s break it down:\n\n“ig_stories_tray_mtml” → “ig” “stories” “tray” “mtml”\n\n“ ig ”: This model is an “ig” model as opposed to “fb” or “whatsapp”.\n\n“ stories ”: This model serves IG Stories.\n\n“ tray ”: This model serves in the main IG Stories tray (as opposed to stories in some other surface).\n\n“mtml”: This model is a multi-task-multi-label model, commonly used in late-stage ranking.\n\nWe can then use these model type strings to tag AI assets, and since they serve as proxies for business context, we can use them also for asset management, policy enforcement, analytics, and more.\n\nThe metadata entries in the model registry are anchored on two main types that describe model instances (ModelMetadata) as well as model types (ModelTypeMetadata). These types are made up of “core” attributes that are universally applicable, as well as “extended” attributes that allow different teams to encode their opinions about how these entries will inform operations. For example, in Instagram our extended attributes encode “baseline” and “holdout” model IDs, which are used in our ranking infrastructure to orchestrate ranking funnel execution.\n\nCriticality\n\nIn addition to defining business function, we had to establish clear guidelines for model importance. Within Meta, SEVs and services have a unified-importance tier system where the Global Service Index (GSI) records a criticality from TIER0 to TIER4 based on the maximum incident severity level the service can cause, from SEV0 as the most critical to SEV4 as simply a “heads up.” Since GSI criticality had social proof at the company, and infra engineers were familiar with this system, we adopted these criticalities for models and now annotate them at the model type and model level.\n\nNo longer would each team decide to raise their own model services to TIER1 for themselves, increasing the burden on all teams that support these models. Teams needed to provide an immediate response (available 24/7) on call and be able to prove that their models contributed meaningfully to critical business metrics to qualify for elevated monitoring.\n\nConfiguration structure as a foundation for automation\n\nOnce we had onboarded a critical mass of Instagram models to the model registry, we could begin to fully integrate with our monitoring and observability suite using our Meta-wide configuration solution, Configerator. With this, we could now have model performance monitoring and alerts that are fully automated and integrated with our tooling for SLIs called SLICK, dashboards that allow us to monitor models across many time series dimensions, and a suite of alerting specific to the model that is driven from the entries in the model registry.\n\nThis provided all our teams confidence that our monitoring coverage was complete and automated.\n\nLaunching\n\nWhile a point-in-time snapshot of models in production is great for static systems, Instagram’s ML landscape is constantly shifting. With the rapid increase of iteration on the recommendation system driving an increased number of launches, it became clear our infrastructure support to make this happen was not adequate. Time-to-launch was a bottleneck in ML velocity, and we needed to drive it down.\n\nWhat did the process look like?\n\nConventionally, services were longstanding systems that had engineers supporting them to tune. Even when new changes would introduce new capacity regression risks, we could gate this behind change safety mechanisms.\n\nHowever, our modeling and experimentation structure was unique in that we were planning for more rapid iteration, and our options were insufficient. To safely test the extent of load a new service could support, we would clone the entire service, send shadow traffic (i.e., cloned traffic that isn’t processed by our clients), and run multiple overload tests until we found a consistent peak throughput. But this wasn’t a perfect science. Sometimes we didn’t send enough traffic, and sometimes we’d send too much, and the amount could change throughout the day due to variations in global user behavior.\n\nThis could easily take two days to get right, including actually debugging the performance itself when the results weren’t expected. Once we got the result, we’d then have to estimate the final cost. Below (in Figure 5) is the formula we landed on.\n\nThe actual traffic shifting portion was tedious as well. For example, when we managed to fully estimate that we needed 500 replicas to host the new service, we might not actually have 500 spares lying around to do a full replacement, so launching was a delicate process of partially sizing up by approximately 20%, sending 20% of traffic over, and then scaling down the old service by 20% to reclaim and recycle the capacity. Rinse, repeat. Inefficient!\n\nAnd by the time we got to the end of this arduous process, the ordeal still wasn’t over. Each team was responsible for correctly setting up new alerts for their baseline in a timely fashion, or else their old models could and did trigger false alarms.\n\nHow does forcing virtual pools aid product growth?\n\nOne of the prerequisites for fixing competition for resources and unblocking productivity was to put up guardrails. Prior to this, it was “first come first served,” with no clear way to even “reserve” future freed capacity. It was also hard to reason about fairness from an infra perspective: Would it make sense to give each team equal pools, or give each individual person a maximum limit?\n\nAs it turned out, not all MLEs are experimenting at the same time, due to staggered progress on their work, so individual (per-engineer) limits were not ideal. One member might be in the experimentation stage and another might be training. So our solution was to provide bandwidth to each team.\n\nOnce each team — and therefore product — had quotas distributed, their launch policy became more clear cut. Some teams established free launching as long as the team was within quota. Others required no regressions in capacity usage. But mostly this unlocked our ability to run launches in parallel, since each one required much less red tape, and prioritization was no longer done at the org level.\n\nWhat other tooling improved launching?\n\nAs mentioned earlier, preplanning with capacity estimations was critical to understanding cost and ensuring reliability. We were often asked, Why not let autoscaling take care of everything? The problem was that each service could be configured slightly differently than a previously optimized service, or some architectural change could have affected the performance of the model. We didn’t have an infinite amount of supply to work with, so by the time we fully traffic-shifted everything over, we might find that we didn’t have enough supply. Reverting is costly, taking hours to get through each stage.\n\nBy doing capacity estimations in advance, this also allowed us and each team to accurately evaluate metric improvement versus cost. It might be worthwhile to double our costs if something would increase time spent on the app by 1%, but likely not for a 0.05% improvement where we could better spend that capacity funding another initiative.\n\nWith partners in AI Infra, we developed two major solutions to this process: offline performance evaluation and an automated launching platform.\n\nWe simplified determining performance of a new service using recorded traffic. Pre-recorded traffic was continuously collected into a data warehouse that the benchmarker could read from, and we’d spin up temporary jobs with this automation. One job would replay different levels of traffic continuously and send it to another job that was a clone of the existing experiment. By putting stoppers on desired latency and error rates, the tooling would eventually output a converged stable number that we could understand as the max load (see Figure 6).\n\nThe launch platform itself would input the numbers we captured from these tests, automatically collect demand data as defined, and run that same formula to calculate a cost. The platform would then perform the upscaling/downscaling cycle for teams as we shifted traffic.\n\nAnd finally, by leveraging the model registry, we were able to land this model change in code (see example in Figure 6), to help us better maintain and understand the 1000+ models within our fleet. Likewise, this bolstered our trust in the model registry, which was now directly tied to the model launch lifecycle.\n\nThis suite of launch automation has dramatically reduced the class of SEVs related to model launches, improved our pace of innovation from a few to more than 10 launches per week, and reduced the amount of time engineers spend conducting a launch by more than two days.\n\nModel stability\n\nAs the number of models in production increased, our organization started to feel the effects of an inconsistent measure of model health. While ranking models are run like any other distributed backend system (receive a request, produce a response), one may think a universal SLO that measures request success rate can suffice to capture holistic health. This is not the case for ranking models, as the accuracy of recommendations received carries significant importance to the end-user experience. If we consider a user who is a huge fan of golf but does not enjoy cooking content (see the “available & irrelevant” case in Figure 8 below), we see an example of this inaccuracy in practice. This is precisely what the model stability metric sought to capture.\n\nWhy is measuring ranking model reliability unique?\n\nRanking models, unlike traditional idempotent request/response backends, produce scores predicting user action given a set of candidates (PLIKE, PCOMMENT, PFOLLOW, etc.). These scores then combine and are used to determine which candidates are most relevant to an end user. It’s important that these scores accurately reflect user interest, as their accuracy is directly correlated to user engagement. If we recommend irrelevant content, user engagement suffers. The model stability metric was designed to make it easy to measure this accuracy and detect inaccuracy at our scale.\n\nLet’s discuss how this works.\n\nDefining model stability\n\nModels are complex, and they produce multiple output predictions. Let’s take a simplified example (shown in Figure 9 below) of a multi-task-multi-label (MTML) model predicting three actions:\n\nFor us to claim this model is stable, we must also claim that each underlying prediction is stable.\n\nWhen evaluating the accuracy of a ranking model’s predictions, we typically look at two metrics:\n\nModel calibration , which is based on observed real-world outcomes and answers the question, “Are we over- or under-predicting user action?” It is calculated as a ratio of predicted click-through-rate (CTR) and empirical CTR. A perfect predictor will have calibration centered at 1.\n\nModel normalized entropy (NE), which measures the discriminative power of a predictor, and answers the question, “How well can this predictor separate action from inaction?” It is calculated as a ratio of the average log-loss per impression to what the average log-loss per impression would be if we always predicted the empirical CTR. With NE, lower values are better, and an NE of 1 is equivalent to random predictions.\n\n(For more information regarding our choice of prediction evaluation metrics, please refer to the paper, “Practical Lessons from Predicting Clicks on Ads at Facebook.”)\n\nA model’s predictions are unstable when either calibration or NE are out of their expected healthy ranges. To determine what a healthy range is, we must look at each metric in real time, and Figure 10 below shows what these time series can look like:\n\nBy observing the trend of a healthy prediction, we can apply thresholds for our evaluation metrics. When these thresholds are breached, the underlying prediction is considered unstable.\n\nFrom here, we can define model stability as a binary indicator across a model’s predictions. It is 1 if all underlying predictions are stable, and 0 if any prediction is unstable. This is an extremely powerful method of reacting to real-time prediction instability as well as a tool for understanding trends in predictive health per model or across distinct products ranking funnels.\n\nOperationalizing model stability\n\nWith a real-time view on model predictive health, we can leverage this unified definition of model stability and apply it to all of our models in production, once again leveraging the model registry as a ledger to hold this important data. In Figure 11 below, we can see the addition of model stability metric metadata after we determined the expected thresholds.\n\nGiven the large number of models in production, each producing many predictions, building a portable definition of model health applicable to all of our ranking models represented an important milestone toward upleveling Instagram’s ML infrastructure maturity. This has unlocked our ability to build generic alerting to guarantee detection of our most important models becoming unstable, thereby moving us closer to mitigation when our recommendation system is at risk.\n\nSince the addition of these metrics and alerting, ML teams have discovered previously hidden issues within their models and addressed them faster than before, leading to higher-quality recommendations.\n\nKey takeaways\n\nIn our journey to scale Instagram’s algorithm to manage over 1000 models, we have learned several critical lessons that have shaped our approach and infrastructure. These takeaways not only highlight the challenges we faced but also underscore the strategies that led to our success.\n\nInfra understanding is the foundation to building the right tools\n\nA unified understanding of our infrastructure footprint was essential in developing the right tools to support our scaling efforts. By identifying the gaps and potential risks in our existing systems, we were able to implement solutions such as the model registry that significantly improved our operational efficiency and reliability posture.\n\nHelping colleagues move fast means we all move faster\n\nBy addressing the model iteration bottleneck, we enabled our teams to innovate more rapidly. Our focus on creating a seamless, self-service process for model iteration empowered client teams to take ownership of their workflows. This not only accelerated their progress but also reduced the operational burden on our infrastructure team. As a result, the entire organization benefited from increased agility and productivity.\n\nReliability must consider quality\n\nEnsuring the reliability of our models required us to redefine how we measure and maintain model quality. By operationalizing model stability and establishing clear metrics for model health, we were able to proactively manage the performance of our models. This approach enables us to maintain high standards of quality across our recommendation systems, ultimately enhancing user engagement and satisfaction.\n\nOur experience in scaling Instagram’s recommendation system has reinforced the importance of infrastructure understanding, collaboration, and a focus on quality. By building robust tools and processes, we have not only improved our own operations but also empowered our colleagues to drive innovation and growth across the platform.", "label": "non_personal"}
{"title": "Extending the Malbec subsea cable to Southern Brazil", "url": "https://engineering.fb.com/2025/05/22/connectivity/extending-malbec-subsea-cable-southern-brazil/", "content": "Meta is partnering with V.tal to extend the Malbec subsea cable to Porto Alegre, Brazil by 2027.\n\nWith this new extension, Malbec will become the first subsea cable to land in the state of Rio Grande do Sul, bringing more connectivity to millions of people in Southern Brazil and neighboring countries.\n\nMalbec will improve the scale and reliability of digital infrastructure in Porto Alegre, establishing it as a digital hub and improving online experiences across Southern Brazil, Argentina, Chile, Paraguay, and Uruguay.\n\nToday, we’re announcing the extension of the Malbec subsea cable to the city of Porto Alegre, Brazil. Developed by Meta, in partnership with V.tal, Malbec is a 2,500 km cable that entered service in 2021 to provide connectivity between the Southern Cone of South America and Brazil. The new extension will be operational in 2027 and will link Porto Alegre to the cities of Rio de Janeiro and São Paulo, Brazil and Buenos Aires, Argentina.\n\n“The expansion of Malbec to Porto Alegre is a milestone for connectivity in South America, benefiting millions of people in Brazil and positioning the capital of Rio Grande do Sul as the first major international digital hub in the south of the country,” explained Ana Luiza Valadares, Meta’s Public Policy Director, Connectivity & Infra, LatAm. “It will contribute to attracting digital infrastructure companies, lowering costs for companies and improving consumer services.”\n\nFelipe Campos, CEO of V.tal, added, “The impact of this project will be significant for the local digital economy, positioning Porto Alegre as a new connectivity hub. It will be a unique infrastructure that will attract the interest of operators and internet providers, as well as other submarine cable companies.\n\nIn addition, all the Southern Cone countries will benefit from this new ecosystem, not to mention the end users and companies who will have a better experience when using the internet and digital applications.”\n\nThis extension is one of the latest in Meta’s digital infrastructure investments to support growing demand for digital capacity, resilience, and global reach. Earlier this year, Meta also activated a Point of Presence (PoP) in Porto Alegre. PoPs facilitate the efficient delivery of content locally, which reduces the network management costs for internet service providers while improving the quality of experience for their customers. With the advent of AI and increasing demand for online services, digital infrastructure deployments play an important role in ensuring that the benefits of AI and other emerging technologies are available to everyone, regardless of where they live or work.\n\n“This investment in submarine connectivity, fully aligned with our Economic, Inclusive and Sustainable Development Plan, represents a strategic milestone for the state’s future,” said Rio Grand do Sul Governor, Eduardo Leite. “Furthermore, it fosters artificial intelligence projects, technologies that are already transforming the present and will define the future of innovation, a sector in which Rio Grande do Sul is a leader in Brazil, according to the ranking of state competitiveness.”\n\nMalbec will be the first international subsea cable to land in Rio Grande do Sul, bringing with it over 84 terabits of international capacity and direct connectivity to northern Brazil and Argentina. Like most subsea cables, local service providers will be able to acquire capacity on Malbec to serve additional bandwidth to millions of people in Brazil’s southern states. The providers will also extend Malbec’s capacity by connecting with providers in the neighboring countries of Argentina, Chile, Paraguay, and Uruguay, further positioning Brazil as a South American connectivity hub.", "label": "non_personal"}
{"title": "How we improved availability through iterative simplification", "url": "https://github.blog/engineering/engineering-principles/how-we-improved-availability-through-iterative-simplification/", "content": "Solving and staying ahead of problems when scaling up a system of GitHub’s size is a delicate process. The stack is complex, and even small changes can have a big ripple effect. Here’s a look at some of the tools in GitHub’s toolbox, and how we’ve used them to solve problems. We’ll also share some of our wins and lessons we learned along the way.\n\nThere are several tools that we use to keep pace with our growing system. While we can’t list them all, here are some that have been instrumental for our growth.\n\nAs we serve requests, there is a constant stream of related numbers that we care about. For example, we might want to know how often events are happening or how traffic levels compare to expected use. We can record metrics for each event in Datadog to see patterns over time and break them down across different dimensions, identifying areas that need focus.\n\nEvents also contain context that can help identify details for issues we’re troubleshooting. We send all this context to Splunk for further analysis.\n\nMuch of our application data is stored in MySQL, and query performance can degrade over time due to factors like database size and query frequency. We have written custom monitors that detect and report slow and timed-out queries for further investigation and remediation.\n\nWhen we introduce changes, we often need to know how those changes affect performance. We use Scientist to test proposed changes. With this tool, we measure and report results before making the changes permanent.\n\nWhen we’re ready to release a change, we roll it out incrementally to ensure it works as expected for all use cases. We also need to be able to roll back in the event of unexpected behavior. We use Flipper to limit the rollout to early access users, then to an increasing percentage of users as we build the confidence.\n\nAchieving faster database queries\n\nWe recently observed a SQL query causing a high number of timeouts. Our investigation in Splunk tracked it down to GitHub’s Command Palette feature, which was loading a list of repositories. The code to generate that list looked something like this:\n\norg_repo_ids = Repository.where(owner: org).pluck(:id) suggested_repo_ids = Contribution.where(user: viewer, repository_id: org_repo_ids).pluck(:repository_id)\n\nIf an org has many active repositories, the second line could generate a SQL query with a large IN (...) clause with an increased risk of timing out. While we’d seen this type of problem before, there was something unique about this particular use case. We might be able to improve performance by querying the user first since a given user contributes to a relatively small number of repositories.\n\ncontributor_repo_ids = Contribution.where(user: viewer).pluck(:repository_id) suggested_repo_ids = Repository.where(owner: org, id: contributor_repo_ids)\n\nWe created a Scientist experiment with a new candidate code block to evaluate performance. The Datadog dashboard for the experiment confirmed two things: the candidate code block returned the same results and improved performance by 80-90%.\n\nWe also did a deeper dive into the queries this feature was generating and found a couple of possible additional improvements.\n\nThe first involved eliminating a SQL query and sorting results in the application rather than asking the SQL server to sort. We followed the same process with a new experiment and found that the candidate code block performed 40-80% worse than the control. We removed the candidate code block and ended the experiment.\n\nThe second was a query filtering results based on the viewer’s level of access and did so by iterating through the list of results. The access check we needed can be batched. So, we started another experiment to do the filtering with a single batched query and confirmed that the candidate code block improved performance by another 20-80%.\n\nWhile we were wrapping up these experiments, we checked for similar patterns in related code and found a similar filter we could batch. We confirmed a 30-40% performance improvement with a final experiment, and left the feature in a better place that made our developers, database administrators, and users happier.\n\nRemoving unused code\n\nWhile our tooling does surface problem areas to focus on, it’s preferable to get ahead of performance issues and fix problematic areas before they cause a degraded experience. We recently analyzed the busiest request endpoints for one of our teams and found room to improve one of them before it escalated to an urgent problem.\n\nData for each request to the GitHub Rails application is logged in Splunk and tagged with the associated controller and action. We started by querying Splunk for the top 10 controller/action pairs in the endpoints owned by the team. We used that list to create a Datadog dashboard with a set of graphs for each controller/action that showed the total request volume, average and P99 request latency, and max request latency. We found that the busiest endpoint on the dashboard was an action responsible for a simple redirect, and that performance regularly degraded to the timeout threshold.\n\nWe needed to know what was slowing these requests down, so we dug into Datadog’s APM feature to show requests for the problematic controller/endpoint. We sorted those requests by elapsed request time to see the slowest requests first. We identified a pattern where slow requests spent a long time performing an access check that wasn’t required to send the redirect response.\n\nMost requests to the GitHub Rails application generate HTML responses where we need to be careful to ensure that all data in the response is accessible to the viewer. We’re able to simplify the code involved by using shared Rails controller filters to verify that the viewer is allowed to see the resources they’re requesting that run before the server renders a response. These checks aren’t required for the redirect, so we wanted to confirm we could serve those requests using a different set of filters and that this approach would improve performance.\n\nSince Rails controller filters are configured when the application boots rather than when each request is processed, we weren’t able to use a Scientist experiment to test a candidate code block. However, filters can be configured to run conditionally, which enabled us to use a Flipper feature flag to change behavior. We identified the set of filters that weren’t required for the redirect, and configured the controller to skip those filters when the feature flag was enabled. The feature flag controls let us ramp up this behavior while monitoring both performance and request status via Datadog and keeping watch for unexpected problems via Splunk.\n\nAfter confirming that performance improved for P75/P99 request latency—and more importantly, reduced max latency to be more consistent and much less likely to time out—we graduated the feature and generalized the behavior so other similar controllers can use it.\n\nWhat did we learn?\n\nThere are several lessons we learned throughout this process. Here are some of the main points we keep in mind.\n\nThe investment in observability is totally worth it! We identified and solved problems quickly because of the metric and log information we track.\n\nEven when you’re troubleshooting a problem that’s been traditionally difficult to solve, the use case may be subtly different in a way that presents a new solution.\n\nWhen you’re working on a fix, look around at adjacent code. There may be related issues you can tackle while you’re there.\n\nPerformance problems are a moving target. Keeping an eye open for the next one helps you fix it when it’s gotten slow rather than when it starts causing timeouts and breaking things.\n\nMake small changes in ways that you can control with a gradual rollout and measure results.", "label": "non_personal"}
{"title": "How GitHub supports neurodiverse employees (and how your company can, too)", "url": "https://github.blog/engineering/engineering-principles/how-github-supports-neurodiverse-employees-and-how-your-company-can-too/", "content": "In today’s global workplace, supporting employees by appreciating and understanding their background and lived experience is crucial for the success of any organization. This includes employees who are neurodivergent. Neurodivergence refers to natural variations in human brains and cognition. The term encompasses conditions such as autism, ADHD, dyslexia, mental illness, and other neurological differences.\n\nNeurodivergent employees don’t just enrich the workplace, they’re good for business. According to Deloitte, teams with neurodivergent people can be up to 30 percent more productive than others. Neurodivergent folks excel in pattern recognition and the type of outside-the-box thinking highly sought after in the software industry.\n\nIn this blog post, we’ll take a look at five ways GitHub fosters and supports neurodiverse employees via Neurocats, a GitHub Community of Belonging (CoB), and how you can do the same at your organization.\n\nLet’s go!\n\nForktocat: An Octocat image that represents the fork function in Git, which we’ve adopted in Neurocats to represent the different ways our brains work.\n\n1. Establish supportive communities\n\nAs an initial step, establish private, supportive communities where neurodivergent employees can connect, share their experiences, and find support. GitHub’s Neurocats community allows members to privately discuss their neurodivergence, offer advice to each other, and build a sense of belonging, all in a safe place where members can freely express themselves without fear.\n\nNeurocats started as a private Slack channel under a different name years before it formally transitioned into a CoB. Originally called #neuroconverse, it gave the neurodivergent community at GitHub a space to chat. In the summer of 2021, a collection of passionate members started discussions with GitHub’s Diversity Inclusion and Belonging team about becoming a formal CoB. In October 2021, they formed as an official group at GitHub, and after some discussion, became the Neurocats. The community now consists of hundreds of members from across the company and continues to grow.\n\nSetting up spaces for neurodivergent individuals to express themselves and meet other like-minded friends and allies not only improves their overall work life balance, it also accelerates the creation of new innovative ideas that could be the next big thing in your organization’s portfolio.\n\n“As a neurodivergent people manager with dyslexia and dysgraphia, I am thrilled to be part of the Neurocats CoB, a community that embraces and normalizes our uniqueness,” says Tina Barfield, senior manager at GitHub. “By doing so, we can help drive environments where everyone’s strengths are celebrated, leading to greater innovation, creativity, and inclusivity.” (Please note, all employee names and stories have been shared with permission.)\n\nSuggestions for establishing a supportive community: Have members lead. Embrace the powerful slogan, “Nothing about us, without us.”\n\nEmbrace the powerful slogan, “Nothing about us, without us.” Consider how to protect confidentiality and anonymity. Many neurodivergent people may want to control to whom and in what context they share their neurodivergence. Always obtain permission before sharing information that identifies a person’s neurodivergence.\n\nMany neurodivergent people may want to control to whom and in what context they share their neurodivergence. Always obtain permission before sharing information that identifies a person’s neurodivergence. Consider conversation history. In Neurocats, conversation history is wiped every four days.\n\nIn Neurocats, conversation history is wiped every four days. Never require diagnosis for membership to the community. In many cultures and countries, getting a formal diagnosis can be difficult if not impossible. If someone believes they are neurodivergent, accept them and support them.\n\n2. Foster a sense of belonging\n\nGiving employees the time and space to discuss their neurodivergence enables them to strongly relate to each other, lift each other up, and make personal discoveries that will help them navigate life both at work and at home.\n\n“I didn’t know what being neurodivergent was before Neurocats,” says Lou Nelson, support engineer III who works on GitHub Premium Support. “I thought I was a weird kid with an ADHD diagnosis. Neurocats has become the lynchpin for my career. I have made valuable connections and have a deeper insight into myself than I could have ever done alone. As a member, I find it incumbent to share this experience with others so that they also don’t have to feel alone.”\n\nWhen neurodivergent employees feel comfortable enough to share their stories more broadly, other employees will be drawn to those communities to either personally relate or learn and empathize about subjects they may not have previously considered.\n\n“As a people manager with ADHD, I’m accustomed to being the ‘neurodiversity pioneer’ when meeting new teams or direct reports, setting an example by speaking openly about my gifts and challenges,” says Julie Kang, staff manager of software engineering at GitHub. “When I joined GitHub, and especially when I became a Neurocat, I was pleasantly surprised to find a culture that was knowledgeable, accepting, and celebratory of neurodiversity at a level I haven’t seen before in my career.”\n\nSuggestions for encouraging a sense of belonging in your neurodivergent community: Declare safe spaces. Before meetings where neurodivergence is discussed, call out the fact that you want this to be a safe space. Encourage kindness and empathy.\n\nBefore meetings where neurodivergence is discussed, call out the fact that you want this to be a safe space. Encourage kindness and empathy. Warn about oversharing. If there’s a meeting where oversharing is possible, warn participants to think about their contributions and avoid sharing details that they may later regret.\n\nIf there’s a meeting where oversharing is possible, warn participants to think about their contributions and avoid sharing details that they may later regret. Understand privacy and confidentiality expectations. Participants should respect privacy and adopt guiding principles around privacy and confidentiality.\n\nParticipants should respect privacy and adopt guiding principles around privacy and confidentiality. Share information broadly with the company or organization. Create mechanisms where members of the community can safely share thoughts and feelings with the broader company or organization and, if appropriate, externally. Anonymize this information if required and always share with consent. Examples can include recurring Q&A sessions, presentations, and discussions with the entire company, organization or a segment of the organization all help allow neurodivergent employees to be seen.\n\n3. Provide flexibility and accommodations\n\nNeurodivergent employees can often benefit from flexible working arrangements. This could include flexible hours, remote work options, noise-canceling headphones, or customized workspaces to reduce sensory overload.\n\nAsking for accommodations can be hard. Identify the process your organization or company uses to assess workplace accommodations. Encourage employees to utilize that process to obtain a workplace accommodation.\n\n“One of the biggest things for me has been seeing how many other folks went through a lot of their life being told that they just needed to apply themselves, pay attention, work harder, etc. only to repeatedly fail out of college, get fired from jobs, and generally struggle to ‘human’ correctly,” says Caite Palmer, manual review analyst of security operations at GitHub. “These folks are now through all departments and levels at this large, successful company getting to do great work in a place where flexibility, asking a million questions, and problem solving are generally considered tremendous assets and encouraged.”.\n\nSuggestions for providing flexibility and accommodations: Listen to your employees. Identify listening mechanisms to hear concerns from your employees. An employee or group of employees may share with you how a particular process or system is difficult. Their honesty and candor is a gift and an opportunity to improve your business.\n\nIdentify listening mechanisms to hear concerns from your employees. An employee or group of employees may share with you how a particular process or system is difficult. Their honesty and candor is a gift and an opportunity to improve your business. Consider providing a budget. At GitHub we have flexible benefits that allow employees to purchase equipment and tools which can help them focus, work, and even relax. Removing barriers for employees to obtain bespoke solutions that work for them reduces costs by removing long conversations and approvals from the process.\n\nAt GitHub we have flexible benefits that allow employees to purchase equipment and tools which can help them focus, work, and even relax. Removing barriers for employees to obtain bespoke solutions that work for them reduces costs by removing long conversations and approvals from the process. Train managers to care. Part of the GitHub manager training program is geared to help managers foster a sense of caring and empathy for the employees in their teams. Encourage kindness across all levels of the company.\n\nPart of the GitHub manager training program is geared to help managers foster a sense of caring and empathy for the employees in their teams. Encourage kindness across all levels of the company. Be clear about tasks that need to be completed and flexible on how that task is accomplished. An autistic employee might go for a long walk and then be able to hyperfocus on a task, completing it in half the time than it would have taken if they’d sat at their desk the whole time. Trust your employees to do their best work in the way that fits them and judge them on the work they produce.\n\n4. Encourage open dialogue\n\nPromote a culture of openness where employees feel comfortable discussing their needs and challenges. Consider holding regular meetings and forums to discuss topics related to neurodiversity, mental health, and well-being. With the Neurocats group, we hold monthly meetings to discuss various topics, which are important to our members. One member of the Neurocats leadership team describes their experience:\n\n“We have a voice, which we use to highlight issues our members face day to day,” says Owen Niblock, senior software engineer at GitHub who works on accessibility. “We also hold monthly meetings to discuss topics from ADHD and autism to anxiety, mental health issues, and more. Over the years, we’ve had some success and find we are able to lobby for changes at a company level, leading to real tangible change that benefits the whole of GitHub.”\n\nEnabling open dialogue means providing avenues for these discussions to happen. But going one step further and encouraging open dialogue requires more effort.\n\nSuggestions for encouraging open and honest dialogue with the neurodivergent community at your company: Listen. You won’t be able to act on every piece of feedback you receive, but showing you acknowledge and appreciate feedback and making changes whenever possible will create a culture of frank and honest discussion.\n\nYou won’t be able to act on every piece of feedback you receive, but showing you acknowledge and appreciate feedback and making changes whenever possible will create a culture of frank and honest discussion. Make space. Conversations can happen at many levels and in many forums. Create space for a diverse set of feedback and ideas by accepting discussions in different formats. For example, you might have a meeting to discuss something with a follow-up GitHub Discussion to collect async ideas.\n\nConversations can happen at many levels and in many forums. Create space for a diverse set of feedback and ideas by accepting discussions in different formats. For example, you might have a meeting to discuss something with a follow-up GitHub Discussion to collect async ideas. Give access to the leadership team. At GitHub all our CoBs have an executive sponsor who’s a member of GitHub’s senior leadership team. This gives members an ally to raise issues with, help find solutions, and communicate problems directly upwards.\n\nAt GitHub all our CoBs have an executive sponsor who’s a member of GitHub’s senior leadership team. This gives members an ally to raise issues with, help find solutions, and communicate problems directly upwards. Lead from the very top. Provide opportunities for CoBs to meet with senior leadership. For example, this year, each of GitHub’s CoBs have a meeting with GitHub’s CEO as part of a listening tour. This shows that the community’s views are important to the whole business.\n\n5. Celebrate neurodiversity\n\nAcknowledge and celebrate the unique contributions of neurodiverse employees. Recognize their achievements, provide opportunities for career advancement, and ensure they have a voice in the organization. Celebrating Disability Pride Month and other related events can help raise awareness and appreciation within the company.\n\n“Neurocats was the first time I found people like me not only represented at work, but celebrated and successful,” Palmer says. “Sharing the rough days, the burnout, the overwhelm and frustration, but also the wins of finally getting appropriate support, being seen as creative instead of weird, and getting to learn about all the different ways brains can function.”\n\nCelebrations should come not just from the community but also from leadership and the People Team. Sharing posts about the company’s mental health benefits during Mental Health Month (in May) or sharing information about the community during meetings or training can all help to celebrate your diverse workforce.\n\nSuggestions for celebrating neurodiversity: Celebrations should come from all levels of the company. An internal post by a member of the leadership team or an informative post by the People team or Human Resources can show your company’s support.\n\nAn internal post by a member of the leadership team or an informative post by the People team or Human Resources can show your company’s support. Don’t sugar coat it. Being neurodivergent can be hard. It’s not a superpower. Celebrate the diversity and the wins, but never minimize the real struggles that many neurodivergent people deal with every day.\n\nBeing neurodivergent can be hard. It’s not a superpower. Celebrate the diversity and the wins, but never minimize the real struggles that many neurodivergent people deal with every day. Make it visible. Share things in the format that makes most sense and will reach the most people.\n\nShare things in the format that makes most sense and will reach the most people. Promote. Ask members of the community to share information on your company blog or website.\n\nBy implementing these strategies, you can create an inclusive environment where neurodivergent employees feel valued, supported, and empowered to contribute their best work.\n\n“Neurocats provided an environment that made me feel safe and confident in an astonishingly short amount of time, allowing me to bring my A game, leverage my strengths, and make a positive impact much sooner than usual,” says Julie Kang, staff manager of software engineering at GitHub. “The support and understanding here have been truly transformative for my professional growth, and I feel equipped to pay this forward to my peers and reports.”\n\nInterested in learning more about GitHub’s approach to accessibility? Visit accessibility.github.com.\n\nTags:", "label": "non_personal"}
{"title": "The ultimate guide to developer happiness", "url": "https://github.blog/engineering/engineering-principles/the-ultimate-guide-to-developer-happiness/", "content": "In today’s rapidly evolving landscape, where AI is reshaping industries and transforming workflows, the role of developers has never been more critical. As business leaders, fostering an environment where developers feel valued, motivated, and empowered is essential to harnessing their full potential and keeping your business profitable and innovative.\n\nIn this blog post, we’ll explore actionable tips and strategies to supercharge developer happiness, ensuring your team remains productive, engaged, and ahead of the AI curve. We’ll walk you through ways to secure your code with AI, how to increase productivity with a strong developer experience, and, of course, invite you to join us at GitHub Universe 2024 to see the very best of the latest AI tooling in action.\n\nBoost productivity with a great developer experience\n\nDeveloper experience is more than just a buzzword—it’s a critical factor in driving productivity and collaboration within software development teams. A seamless developer experience allows developers to get into the flow state more easily, where their productivity and creativity can peak. This flow state—characterized by uninterrupted concentration and a deep sense of involvement in the task—is crucial for tackling complex coding challenges.\n\nThis work environment needs to be built intentionally, and the research backs it up. Developers who carve out time for deep work enjoy 50% more productivity, while those that get work they find engaging are 30% more productive.\n\nHow does this impact businesses? Well, because a developer that can significantly reduce their context-switching and mental load can also produce code faster and at a higher quality.\n\nWhen developers understand their code, they’re 42% more productive. When developers are able to get faster turnaround times, they are 20% more innovative. These are tangible, individual benefits that in turn directly impact the output of developer teams.\n\nNow is the time for leaders to invest in creating a great developer experience. By prioritizing the developer experience, you’re setting your team up to harness the full potential of the latest AI and platform engineering advances, ensuring your business stays ahead of the curve. Curious to learn more? Then dive into how a great developer experience fuels productivity with our latest research.\n\nUse AI to secure your code\n\nHistorically, developers and security teams have found themselves at odds due to competing business goals. Shifting security left incorporates security earlier in the software development lifecycle, but in practice it has primarily shifted responsibility to developers without necessarily giving them the required expertise.\n\nThis, combined with the context switching inherent in development work, makes addressing security concerns particularly challenging. With AI, developers now have powerful tools at their disposal to enhance code security. AI can:\n\nImprove detection rates\n\nProvide near-instant fixes with context\n\nEnable application security (AppSec) at scale\n\nThese three improvements make it easier for developers to integrate robust security measures without sacrificing productivity, and transform the relationship between developers and security teams into a collaborative partnership.\n\nIntroducing a new security tool doesn’t have to be a daunting task either. By following a few simple steps, organizations can ensure a smooth transition and broad adoption.\n\nDocument the tool’s features and usage to set the foundation and set realistic expectations to help align goals across teams.\n\nthe tool’s features and usage to set the foundation and set realistic expectations to help align goals across teams. Recognize and celebrate successes to showcase the value of the new tool.\n\nand celebrate successes to showcase the value of the new tool. Adopt a go-with-the-flow approach and organize hackathons to further drive engagement and interest.\n\na go-with-the-flow approach and organize hackathons to further drive engagement and interest. Listen to developer feedback continuously improve and refine security practices.\n\nAI-powered security tools not only enhance the efficiency and effectiveness of AppSec, but also empower developers to take a proactive role in securing their code. This shift not only improves overall security posture, but also fosters a culture of shared responsibility and continuous learning, ultimately leading to more secure and resilient applications.\n\nSee exactly why security should be built into the developer workflow. 👇\n\nCustomize your LLMs\n\nOrganizations that take AI a step further and customize their AI tools are poised to lead the pack.\n\nLarge language models (LLMs) are trained on vast amounts of text data and can perform a variety of natural language processing tasks like translation, summarization, question-answering, and text generation. Customizing a pre-trained LLM goes beyond mere training—it involves adapting the model to perform specific tasks relevant to the organization’s needs. This level of customization helps developers maintain their flow state and significantly boost productivity and efficiency.\n\nCustomization techniques like retrieval-augmented generation (RAG), in-context learning, and fine-tuning enable LLMs to deliver more accurate and contextually appropriate responses:\n\nRAG combines retrieval-based and generation-based approaches in natural language processing. It enhances LLMs by integrating information retrieval techniques, where relevant documents or snippets are retrieved from a vector database to assist in generating more accurate and contextually appropriate responses. This approach allows the model to access and utilize external knowledge, making the generated output more informed and relevant to the user’s query.\n\ncombines retrieval-based and generation-based approaches in natural language processing. It enhances LLMs by integrating information retrieval techniques, where relevant documents or snippets are retrieved from a vector database to assist in generating more accurate and contextually appropriate responses. This approach allows the model to access and utilize external knowledge, making the generated output more informed and relevant to the user’s query. In-context learning refers to a model’s ability to adapt and respond to new tasks or inputs based on the context provided in the input prompt without requiring additional training. The model leverages its pre-trained knowledge and the context given in the input to perform tasks effectively.\n\nrefers to a model’s ability to adapt and respond to new tasks or inputs based on the context provided in the input prompt without requiring additional training. The model leverages its pre-trained knowledge and the context given in the input to perform tasks effectively. Fine-tuning, on the other hand, is a process in which an LLM is further trained on a specific dataset to adapt it to a particular task or domain. During fine-tuning, the model’s parameters are adjusted based on the new dataset, which typically involves supervised learning with labeled data. This process allows the model to specialize and improve its performance on specific tasks, (such as text classification, question answering, or machine translation), by leveraging the general knowledge acquired during its initial pre-training phase.\n\nBy implementing these customization strategies, businesses can unlock the full potential of their AI tools. Customized LLMs not only improve developer productivity—they also enhance the quality and relevance of AI-generated content.\n\nPrepare your repository for teamwork\n\nFostering collaboration doesn’t just make software development faster, it also helps teams build better products and boost job satisfaction. By making your repository as collaborative as possible, you’ll optimize success. This includes focusing on:\n\nRepository settings : properly configuring repository settings to control visibility, access, and contribution workflows lays the foundation for collaboration.\n\n: properly configuring repository settings to control visibility, access, and contribution workflows lays the foundation for collaboration. Repository contents : including essential files like README.md, LICENSE.md, CONTRIBUTING.md, CODEOWNERS, and CODE_OF_CONDUCT.md helps collaborators understand the project, its purpose, and how to contribute.\n\n: including essential files like README.md, LICENSE.md, CONTRIBUTING.md, CODEOWNERS, and CODE_OF_CONDUCT.md helps collaborators understand the project, its purpose, and how to contribute. Automation and checks : implementing automation tools such as linters, continuous integration (CI), and continuous deployment (CD) pipelines streamlines the development process, ensures code quality, and enables immediate feedback.\n\n: implementing automation tools such as linters, continuous integration (CI), and continuous deployment (CD) pipelines streamlines the development process, ensures code quality, and enables immediate feedback. Security practices : enforcing role-based access control, managing secrets securely, and scanning code for vulnerabilities can foster trust and protect the project from vulnerabilities.\n\n: enforcing role-based access control, managing secrets securely, and scanning code for vulnerabilities can foster trust and protect the project from vulnerabilities. Issue templates : providing structured issue templates guides contributors in providing necessary information and context when reporting bugs.\n\n: providing structured issue templates guides contributors in providing necessary information and context when reporting bugs. Community engagement: engaging with the project’s community through meetups, project blogs, discussions, and other channels fosters belonging and builds relationships.\n\nInvest in your team’s learning opportunities\n\nWhen you signal to your team that you value their career growth and exposure to learning opportunities, it can boost happiness and job satisfaction, leading to increased productivity, collaboration, and better problem solving.\n\nEncouraging your developer teams to attend conferences like GitHub Universe 2024 is a strategic investment in their professional growth and your business’ success. Our global developer event provides an unparalleled platform for the best in software development to gather and expand their knowledge, stay updated on the latest AI-powered tools, and bring fresh ideas back to their teams.\n\nHere are a few highlights of what you and your team can expect:\n\nHelp your developers get in the flow and stay there with sessions, demos, panels, and more on the powerful tools and techniques that enhance productivity and satisfaction.\n\nand stay there with sessions, demos, panels, and more on the powerful tools and techniques that enhance productivity and satisfaction. Connect with other technical leaders to share experiences, challenges, and best practices. Expand your network with valuable industry contacts.\n\nto share experiences, challenges, and best practices. Expand your network with valuable industry contacts. Get a first look at GitHub’s product roadmap and see how upcoming features and enhancements can help you stay ahead in a competitive landscape.\n\nand see how upcoming features and enhancements can help you stay ahead in a competitive landscape. Gain technical skills with GitHub certifications and workshops designed to enhance your expertise in a rapidly evolving industry.\n\nwith GitHub certifications and workshops designed to enhance your expertise in a rapidly evolving industry. Learn the latest on GitHub Copilot and stay ahead with the latest coding practices and techniques.\n\nGet your tickets today. You can take advantage of our group discount and get four tickets for the price of three. (That’s a 25% savings!)\n\nIf you’re flying solo, you can also use our Early Bird discount and save 20% off one in-person ticket, only until September 3.\n\nReach new levels of creativity and efficiency\n\nIncorporating these five business strategies can transform your development process and increase developer happiness. By investing in these areas, you empower your team, foster a culture of continuous learning, and position your organization for success in the rapidly evolving tech landscape.", "label": "non_personal"}
{"title": "GitHub Enterprise Cloud with data residency: How we built the next evolution of GitHub Enterprise using GitHub", "url": "https://github.blog/engineering/engineering-principles/github-enterprise-cloud-with-data-residency/", "content": "Today, we announced that GitHub Enterprise Cloud will offer data residency, starting with the European Union (EU) on October 29, 2024, to address a critical desire from customers and enable an optimal, unified experience on GitHub for our customers.\n\nData residency and what it means for developers\n\nWe’ve heard for years from enterprises that being able to control where their data resides is critical for them. With data residency, organizations can now store their GitHub code and repository data in their preferred geographical region. With this need met, even more developers across the globe can build on the world’s AI-powered developer platform.\n\nEnterprise Cloud with data residency provides enhanced user control and unique namespaces on ghe.com isolated from the open source cloud on github.com. It’s built on the security, business continuity, and disaster recovery capabilities of Microsoft Azure.\n\nThis is a huge milestone for our customers and for GitHub–a multi-year effort that required extensive time, effort, and dedication across the company. We’re excited to share a behind-the-scenes look at how we leveraged GitHub to develop the next evolution of Enterprise Cloud.\n\nDesigning the architecture for the next evolution of GitHub Enterprise\n\nThis effort started in summer of 2022 with a proof of concept (PoC) and involved teams across GitHub. We carefully considered which architecture would enable us to be successful. After iterating with different approaches, we decided to build the new offering as a feature set that extends Enterprise Cloud. This approach would allow us to be consistently in sync with features on github.com and provide the performance, reliability, and security that our customers expect. For hosting, we effectively leveraged Microsoft Azure’s scale, security, and regional footprint to produce a reliable and secured product with data residency built-in, without having to build new data centers ourselves.\n\nAs the home for all developers, developer experience is critically important for us. We recognized early on that consistency was important, so we sought to minimize differences in developing for Enterprise Cloud and Enterprise Cloud with data residency. To this end, the architecture across both is very similar, reducing complexity, risk, and development costs. The deployment model is familiar to our developers: it builds off of GitHub Actions. Also, changes to github.com and Enterprise Cloud with data residency are deployed minutes apart as part of a unified pipeline.\n\nTo accomplish this, we had to organize the work, modify our build and deployment systems, and validate the quality of the platform. We were able to do all three of these by using GitHub.\n\nOrganizing with GitHub Issues and Projects\n\nTo organize the project, we used GitHub Issues and Projects, taking advantage of multiple views to effectively drive work across multiple projects, more than 100 teams, and over 2,000 issues. Different stakeholders and teams could take advantage of these views to focus on the information most relevant to them. Our talented technical project management team helped coordinate updates and used the filtering and slicing capabilities of Projects to present continuously updated information for each milestone in an easily consumable way.\n\nWe also used upcoming features like issues hierarchy to help us understand relationships between issues, and issue types to help clearly classify issues across repositories. As part of using these features internally we were able to give feedback to the teams working on them and refine the final product. Keep an eye out for future announcements for issues hierarchy and issue types coming soon!\n\nAll of these powerful features helped us keep the initiative on track. We were able to clearly understand potential risk areas and partner across multiple teams to resolve blockers and complex dependencies, keeping the project effectively moving forward across multiple years.\n\nBuilding Enterprise Cloud with data residency using GitHub\n\nGitHub has always been built using GitHub. We wanted to continue this practice to set ourselves up for success with the new data residency feature. To this end, we continued leveraging GitHub Codespaces for development and GitHub Actions for continuous integration (CI). In addition, we added deployment targets for new regions. This produced a development, testing, and CI model that required no changes for our developers and a deployment process that was tightly integrated into the existing flow.\n\nWe have previously discussed our deploy then merge model, where we deploy branches before merging into the main branch. We expanded this approach to include successful deployments to Enterprise Cloud data residency targets before changes could be merged and considered complete, continuing to use the existing GitHub merge queue. A visualization of our monolithic deployment pipeline is shown in the figure below.\n\nWe start by deploying to environments used by GitHub employees in parallel. This includes the internal environment for Enterprise Cloud with data residency discussed more in the next section. As we use GitHub every day to build GitHub, this step helps us catch issues as employees use the product before it impacts our customers. After automated and manual testing, we proceed to roll out to “Canary.” Canary is the name for the stage where we configure our load balancers to gradually direct an increasing percentage of github.com traffic to the updated version of the code in a staged manner. Additional testing occurs in between each stage. Once we successfully deploy the updated version of github.com to all users, we then deploy and validate Enterprise Cloud with data residency in the EU before finishing the process and merging the pull request.\n\nEnsuring all deployments are successful before we merge means changes are deployed in sync across all Enterprise Cloud environments and monitored effectively. Note that in addition to deployments, we also use feature flags to gradually roll out changes to groups of customers to reduce risk. If a deployment to any target fails, we roll back the change completely. Once we have understood the failure and are ready to deploy again, the entire process starts from the beginning with the merge queue.\n\nFinally, to maintain consistency across all teams and services, we created automation to generate deployment pipelines for over 100 services so, as new targets are introduced, each service automatically deploys to the new environment in a consistent order.\n\nUsing Enterprise Cloud with data residency ourselves\n\nTo create the best possible product, we also prioritized using it ourselves and stood up an isolated environment for this purpose. Using our GitHub migration tooling, we moved the day-to-day development for the team working on GitHub Enterprise Importer to that environment, and invested in updating our build, deploy, and development environments to support working from the data resident environment. Since its creation, we have deployed to this environment over 8,000 times. This gave us invaluable feedback about the experience of working in the product with issues, pull requests, and actions that we were able to address early in the development process. We were also able to iterate on our status page tooling and internal Service Level Objective (SLO) process with the new environment in mind. The team is continuing to work in this environment today and runs over 1,000 actions jobs a month. This is a testament to the stability and quality we’ve been able to deliver and our commitment to this feature.\n\nWhat’s next\n\nWe are proud that we’ve been able to evolve Enterprise Cloud to offer data residency while using GitHub to organize, build, deploy, and test it. We’re excited to unlock GitHub for even more developers and for you to experience what we have built, starting on October 29, 2024 in the EU, with more regions on the way.\n\nIf you’re excited about Enterprise Cloud with data residency, please join us at GitHub Universe 2024 to learn more and hear from other companies how they’ve used this to accelerate software development and innovation.\n\nTags:", "label": "non_personal"}
{"title": "How to make Storybook Interactions respect user motion preferences", "url": "https://github.blog/engineering/user-experience/how-to-make-storybook-interactions-respect-user-motion-preferences/", "content": "Recently, while browsing my company’s Storybook, I came across something that seemed broken: a flickering component that appeared to be re-rendering repeatedly. The open source tool that helps designers, developers, and others build and use reusable components was behaving weirdly. As I dug in, I realized I was seeing the unintended effects of the Storybook Interactions addon, which allows developers to simulate user interactions within a story, in action.\n\nStorybook Interactions can be a powerful tool, enabling developers to simulate and test user behaviors quickly. But if you’re unfamiliar with Interactions—especially if you’re just looking to explore available components—the simulated tests jumping around on the screen can feel disorienting.\n\nThis can be especially jarring for users who have the prefers-reduced-motion setting enabled in their operating system. When these users encounter a story that includes an interaction, their preferences are ignored and they have no option to disable or enable it. Instead, the Storybook Interaction immediately plays on page load, regardless. These rapid screen movements can cause disorientation for users or in some cases can even trigger a seizure.\n\nKnowledge share Operating systems allow users to set a motion preference. Adhering to this setting can be critical for some users. For example, for users who have photosensitive epilepsy or vertigo, even a small animation can be life-threatening. This explicit preference for a reduced motion experience can be used by browsers, applications, and websites to reduce unnecessary animations and motions via the prefers-reduced-motion CSS media feature.\n\nAt this time, Storybook does not have built-in capabilities to toggle interactions on or off. Until this feature can be baked in I am hoping this blog will provide you with an alternative way to make your work environment more inclusive. Now, let’s get into building an addon that respects user’s motion preferences and allows users to toggle interactions on and off.\n\nGoals\n\nUsers with prefers-reduced-motion enabled MUST have interactions off by default. Users with prefers-reduced-motion enabled MUST have a way to toggle the feature on or off without altering their operating system user preferences. All users SHOULD have a way to toggle the feature on or off without altering their user preferences.\n\nLet’s get started\n\nStep 1: Build a Storybook addon\n\nStorybook allows developers to create custom addons. In this case, we will create one that will allow users to toggle Interactions on or off, while respecting the prefers-reduced-motion setting.\n\nAdd the following code to a file in your project’s .storybook folder:\n\nimport React, {useCallback, useEffect} from 'react' import {IconButton} from '@storybook/components' import {PlayIcon, StopIcon} from '@storybook/icons' export const ADDON_ID = 'toggle-interaction' export const TOOL_ID = `${ADDON_ID}/tool` export const INTERACTION_STORAGE_KEY = 'disableInteractions' export const InteractionToggle = () => { const [disableInteractions, setDisableInteractions] = React.useState( window?.localStorage.getItem(INTERACTION_STORAGE_KEY) === 'true', ) useEffect(() => { const reducedMotion = matchMedia('(prefers-reduced-motion)') if (window?.localStorage.getItem(INTERACTION_STORAGE_KEY) === null && reducedMotion.matches) { window?.localStorage?.setItem(INTERACTION_STORAGE_KEY, 'true') setDisableInteractions(true) } }, []) const toggleMyTool = useCallback(() => { window?.localStorage?.setItem(INTERACTION_STORAGE_KEY, `${!disableInteractions}`) setDisableInteractions(!disableInteractions) // Refreshes the page to cause the interaction to stop/start window.location.reload() }, [disableInteractions, setDisableInteractions]) return ( <IconButton key={TOOL_ID} aria-label=\"Disable Interactions\" onClick={toggleMyTool} defaultChecked={disableInteractions} aria-pressed={disableInteractions} > {disableInteractions ? <PlayIcon /> : <StopIcon />} Interactions </IconButton> ) }\n\nCode breakdown\n\nThis addon stores user preferences for Interactions using window.localStorage . When the addon first loads, it checks whether the preference is already set and, if so, it defaults to the user’s preference.\n\nconst [disableInteractions, setDisableInteractions] = React.useState( window?.localStorage.getItem(INTERACTION_STORAGE_KEY) === 'true', )\n\nThis useEffect hook checks if a user has their motion preferences set to prefers-reduced-motion and ensures that Interactions are turned off if the user hasn’t already set a preference in Storybook.\n\nuseEffect(() => { const reducedMotion = matchMedia('(prefers-reduced-motion)') if (window?.localStorage.getItem(INTERACTION_STORAGE_KEY) === null && reducedMotion.matches) { window?.localStorage?.setItem(INTERACTION_STORAGE_KEY, 'true') setDisableInteractions(true) } }, [])\n\nWhen a user clicks the toggle button, preferences are updated and the page is refreshed to reflect the changes.\n\nconst toggleMyTool = useCallback(() => { window?.localStorage?.setItem(INTERACTION_STORAGE_KEY, `${!disableInteractions}`) setDisableInteractions(!disableInteractions) // Refreshes the page to cause the interaction to stop/start window.location.reload() }, [disableInteractions, setDisableInteractions])\n\nStep 2: Register your new addon with Storybook\n\nIn your .storybook/manager file, register your new addon:\n\naddons.register(ADDON_ID, () => { addons.add(TOOL_ID, { title: 'toggle interaction', type: types.TOOL as any, match: ({ viewMode, tabId }) => viewMode === 'story' && !tabId, render: () => <InteractionToggle />, }) })\n\nThis adds the toggle button to the Storybook toolbar, which will allow users to change their Storybook Interaction preferences.\n\nStep 3: Add functionality to check user preferences\n\nFinally, create a function that checks whether Interactions should be played and add it to your interaction stories:\n\nimport {INTERACTION_STORAGE_KEY} from './.storybook/src/InteractionToggle' export const shouldInteractionPlay = () => { const disableInteractions = window?.localStorage?.getItem(INTERACTION_STORAGE_KEY) return disableInteractions === 'false' || disableInteractions === null } export const SomeComponentStory = { render: SomeComponent, play: async ({context}) => { if (shouldInteractionPlay()) { ... } }) }\n\nWith this custom addon, you can ensure your workplace remains accessible to users with motion sensitivities while benefiting from Storybook’s Interactions. For those with prefers-reduced-motion enabled, motion will be turned off by default and all users will be able to toggle interactions on or off.", "label": "non_personal"}
{"title": "Breaking down CPU speed: How utilization impacts performance", "url": "https://github.blog/engineering/architecture-optimization/breaking-down-cpu-speed-how-utilization-impacts-performance/", "content": "Introduction ⛵\n\nThe GitHub Performance Engineering team regularly conducts experiments to observe how our systems perform under varying load conditions. A consistent pattern in these experiments is the significant impact of CPU utilization on system performance. We’ve observed that as CPU utilization rises, it can lead to increased latency, which provides an opportunity to optimize system efficiency. Addressing this challenge allows us to maintain performance levels while reducing the need for additional machines, ultimately preventing inefficiencies.\n\nAlthough we recognized the correlation between higher CPU utilization and increased latency, we saw an opportunity to explore the specific thresholds and impacts at various stages in greater detail. With a diverse set of instance types powered by different CPU families, we focused on understanding the unique performance characteristics of each CPU model. This deeper insight empowered us to make smarter, data-driven decisions, enabling us to provision our infrastructure with greater efficiency and confidence.\n\nWith these goals in mind, we embarked on a new journey of exploration and experimentation to uncover these insights.\n\nExperiment setup 🧰\n\nCollecting accurate data for this type of experiment was no easy feat. We needed to gather data from workloads that were as close to our production as possible, while also capturing how the system behaves under different phases of load. Since CPU usage patterns vary across workloads, we focused primarily on our flagship workloads. However, increasing the load could introduce small performance discrepancies, so our goal was to minimize disruption for our users.\n\nFortunately, a year ago, the Performance Engineering team developed an environment designed to meet these requirements, codenamed Large Unicorn Collider (LUC). This environment operates within a small portion of our Kubernetes clusters, mirroring the same architecture and configuration as our flagship workloads. It also has the flexibility to be hosted on dedicated machines, preventing interference from or with other workloads. Typically, the LUC environment remains idle, but when needed, we can direct a small, adjustable amount of traffic towards it. Activating or deactivating this traffic takes only seconds, allowing us to react quickly if performance concerns arise.\n\nTo accurately assess the impact of CPU utilization, we first established a baseline by sending moderate production traffic to a LUC Kubernetes pod hosted on one of its dedicated machines. This provided us with a benchmark for comparison. Importantly, the number of requests handled by the LUC pods remained constant throughout the experiment, ensuring consistent CPU load over time.\n\nOnce the baseline was set, we gradually increased CPU utilization using a tool called “stress,” which artificially occupies a specified number of CPU cores by running random processing tasks. Each instance type has a different number of CPU cores, so we adjusted the steps accordingly. However, the common factor across all instances was the total CPU utilization.\n\nNote: It’s important to recognize that this is not a direct 1:1 comparison to the load generated by actual production workloads. The stress tool continuously runs mathematical operations, while our production workloads involve I/O operations and interrupts, which place different demands on system resources. Nevertheless, this approach still offers valuable insights into how our CPUs perform under load.\n\nWith the environment set up and our plan in place, we proceeded to collect as much data as possible to analyze the impact.\n\nResults 📃\n\nWith our experiment setup finalized, let’s examine the data we gathered. As previously mentioned, we repeated the process across different instance types. Each instance type showed unique behavior and varying thresholds where performance started to decline.\n\nAs anticipated, CPU time increased for all instance types as CPU utilization rose. The graph below illustrates the CPU time per request as CPU utilization increases.\n\nCPU time per request vs CPU utilization\n\nThe latency differences between instance types are expected due to the variations in CPU models. Focusing on the percentage increase in latency may provide more meaningful insights.\n\nLatency percentage increase vs CPU utilization\n\nIn both graphs, one line stands out by deviating more than the others. We’ll examine this case in detail shortly.\n\nTurbo Boost effect\n\nAn interesting observation is how CPU frequency changes as utilization increases, which can be attributed to Intel’s Turbo Boost Technology. Since all the instances we used are equipped with Intel CPUs, the impact of Turbo Boost is noticeable across all of them. In the graph below, you can see how the CPU frequency decreases as the CPU utilization increases. The red arrows are showing the CPU utilization level.\n\nCPU Cores Frequency\n\nWhen CPU utilization remains at lower levels (around 30% or below), we benefit from increased core frequencies, leading to faster CPU times and, consequently, lower overall latency. However, as the demand for more CPU cores rises and utilization increases, we are likely to reach the CPU’s thermal and power limits, causing frequencies to decrease. In essence, lower CPU utilization results in better performance, while higher utilization leads to a decline in performance. For instance, a workload running on a specific node with approximately 30% CPU utilization will report faster response times compared to the same workload on the same VM when CPU utilization exceeds 50%.\n\nVariations in CPU frequency are not the only factors influencing performance changes. All our nodes have Hyper-Threading enabled, an Intel technology that allows a single physical CPU core to operate as two virtual cores. Although there is only one physical core, the Linux kernel recognizes it as two virtual CPU cores. The kernel attempts to distribute the CPU load across these cores, aiming to keep only one hardware thread (virtual core) busy per physical core. This approach is effective until we reach a certain level of CPU utilization. Beyond this threshold, we cannot fully utilize both virtual CPU cores, resulting in reduced performance compared to normal operation.\n\nFinding the “Golden Ratio” of CPU utilization\n\nUnderutilized nodes lead to wasted resources, power, and space in our data centers, while nodes that are excessively utilized also create inefficiencies. As noted, higher CPU utilization results in decreased performance, which can give a misleading impression that additional resources are necessary, resulting in a cycle of over-provisioning. This issue is particularly pronounced with blocking workloads that do not follow an asynchronous model. As CPU performance deteriorates, each process can manage fewer tasks per second, making existing capacity inadequate. To achieve the optimal balance—the “Golden Ratio” of CPU utilization—we must identify a threshold where CPU utilization is sufficiently high to ensure efficiency without significantly impairing performance. Striving to keep our nodes near this threshold will enable us to utilize our current hardware more effectively alongside our existing software.\n\nSince we already have experimental data demonstrating how CPU time increases with rising utilization, we can develop a mathematical model to identify this threshold. First, we need to determine what percentage of CPU time degradation is acceptable for our specific use case. This may depend on user expectations or performance Service Level Agreements (SLAs). Once we establish this threshold, it will help us select a level of CPU utilization that remains within acceptable limits.\n\nWe can plot the CPU utilization vs. CPU time (latency) and find the point where:\n\nCPU utilization is high enough to avoid resource underutilization.\n\nCPU time degradation does not exceed your acceptable limit.\n\nA specific example derived from the data above can be illustrated in the following graph.\n\nPercentage Increase in P50 Latency vs CPU Utilization\n\nIn this example, we aim to achieve less than 40% CPU time degradation, which would correspond to a CPU utilization of 61% on the specific instance.\n\nOutlier case\n\nAs previously mentioned, there was a specific instance that displayed some outlying data points. Our experiment confirmed an already recognized issue where certain instances were not achieving their advertised maximum Turbo Boost CPU frequency. Instead, we observed steady CPU frequencies that fell below the maximum advertised value under low CPU utilization. In the example below, you can see an instance from a CPU family that advertises Turbo Boost frequencies above 3 GHz, but it is only reporting a maximum CPU frequency of 2.8 GHz.\n\nCPU cores frequency\n\nThis issue turned out to be caused by a disabled CPU C-state, which prevented the CPU cores from halting even when they were not in use. As a result, these cores were perceived as “busy” by the turbo driver, limiting our ability to take advantage of Turbo Boost benefits with higher CPU frequencies. By enabling the C-state and allowing for optimization and power reduction during idle mode, we observed the expected Turbo Boost behavior. This change had an immediate impact on the CPU time spent by our test workloads. The images below illustrate the prompt changes in CPU frequencies and latency reported following the C-state adjustment.\n\nCPU cores frequency\n\nP50 CPU time on a request\n\nUpon re-evaluating the percentage change in CPU time, we now observe similar behavior across all instances.\n\nPercentage Increase in P50 Latency vs CPU Utilization\n\nAs we anticipated many of these insights, our objective was to validate our theories using data from our complex system. While we confirmed that performance lowers as CPU utilization increases across different CPU families, by identifying optimal CPU utilization thresholds, we can achieve a better balance between performance and efficiency, ensuring that our infrastructure remains both cost-effective and high performing. Going forward, these insights will inform us of our resource provisioning strategies and help us maximize the effectiveness of our hardware investments.\n\nThank you for sticking with us until the end!! A special shout-out to @adrmike , @schlubbi , @terrorobe , the @github/compute-platform and finally the @github/performance-engineering team for their invaluable assistance throughout these experiments, data analysis, and for reviewing the content for accuracy and consistency. ❤️\n\nTags:", "label": "non_personal"}
{"title": "Considerations for making a tree view component accessible", "url": "https://github.blog/engineering/user-experience/considerations-for-making-a-tree-view-component-accessible/", "content": "Tree views are a core part of the GitHub experience. You’ve encountered one if you’ve ever navigated through a repository’s file structure or reviewed a pull request.\n\nOn GitHub, a tree view is the list of folders and the files they contain. It is analogous to the directory structure your operating system uses as a way of organizing things.\n\nTree views are notoriously difficult to implement in an accessible way. This post is a deep dive into some of the major considerations that went into how we made GitHub’s tree view component accessible. We hope that it can be used as a reference and help others.\n\nStart with Windows\n\nIt’s important to have components with complex interaction requirements map to something people are already familiar with using. This allows for responsiveness to the keypresses they will try to navigate and take action on our tree view instances.\n\nWe elected to adopt Windows File Explorer’s tree view implementation, given the prominence of Windows’ usage for desktop screen reader users.\n\nNavigating and taking actions on items in Windows’ tree view using NVDA and JAWS helped us get a better understanding of how things worked, including factors such as focus management, keyboard shortcuts, and expected assistive technology announcements.\n\nThen maybe reference the APG\n\nThe ARIA Authoring Practices Guide (APG) is a bit of an odd artifact. It looks official but is no longer recognized by the W3C as a formal document.\n\nThis is to say that the APG can serve as a helpful high-level resource for things to consider for your overall approach, but its suggestions for code necessitate deeper scrutiny.\n\nBuild from a solid, semantic foundation\n\nAt its core, a tree view is a list of lists. Because of this, we used ul and li elements for parent and child nodes:\n\n<ul> <li> <ul> <li>.github/</li> <li>source/</li> <li>test/</li> </ul> </li> <li>.gitignore</li> <li>README.md</li> </ul>\n\nThere are a few reasons for doing this, but the main considerations are:\n\nBetter assurance that a meaningful accessibility tree is generated,\n\nLessening the work we need for future maintenance, and consequential re-verification that our updates continue to work properly, and\n\nBetter guaranteed interoperability between different browsers, apps, and other technologies.\n\nNOTE: GitHub currently does not virtualize its file trees. We would need to revisit this architectural decision if this ever changes.\n\nBetter broad assistive technology support\n\nThe more complicated an interactive pattern is, the greater the risk that there are bugs or gaps with assistive technology support.\n\nGiven the size of the audience GitHub serves, it’s important that we consider more than just majority share assistive technology considerations.\n\nWe found that utilizing semantic HTML elements also performed better for some less-common assistive technologies. This was especially relevant with some lower-power devices, like an entry-level Android smartphone from 2021.\n\nBetter Forced Color Mode support\n\nSemantic HTML elements also map to native operating system UI patterns, meaning that Forced Color Mode’s heuristics will recognize them without any additional effort. This is helpful for people who rely on the mode to see screen content.\n\nThe heuristic mapping behavior does not occur if we used semantically neutral div or span elements, and would have to be manually recreated and maintained.\n\nUse a composite widget\n\nA composite widget allows a component that contains multiple interactive elements to only require one tab stop unless someone chooses to interact with it further.\n\nConsider a file tree for a repository that contains 500+ files in 20+ directories. Without a composite widget treatment, someone may have to press Tab far too many times to bypass the file tree component and get what they need.\n\nThink about wrapping it in a landmark\n\nLike using a composite widget, landmark regions help some people quickly and efficiently navigate through larger overall sections of the page. Because of this, we wrapped the entire file tree in a nav landmark element.\n\nThis does not mean every tree view component should be a landmark, however! We made this decision for the file tree because it is frequently interacted with as a way to navigate through a repository’s content.\n\nGo with a roving tabindex approach\n\nA roving tabindex is a technique that uses tabindex=\"-1\" applied to each element in a series, and then updates the tabindex value to use 0 instead in response to user keyboard input. This allows someone to traverse the series of elements, as focus “roves” to follow their keypresses.\n\n<li tabindex=\"-1\">File 1</li> <li tabindex=\"-1\">File 2</li> <li tabindex=\"0\">File 3</li> <li tabindex=\"-1\">File 4</li>\n\nThe roving tabindex approach performed better than utilizing aria-activedescendant , which had issues with VoiceOver on macOS and iOS.\n\nEnhance with ARIA\n\nWe use a considered set of ARIA declarations to build off our semantic foundation.\n\nNote that while we intentionally started with semantic HTML, there are certain ARIA declarations that are needed. The use of ARIA here is necessary and intentional, as it expands the capabilities of HTML to describe something that HTML alone cannot describe—a tree view construct.\n\nOur overall approach follows what the APG suggests, in that we use the following:\n\nrole=\"tree\" is placed on the parent ul element, to communicate that it is a tree view construct.\n\nis placed on the parent element, to communicate that it is a tree view construct. role=\"treeitem\" is placed on the child li elements, to communicate that they are tree view nodes.\n\nis placed on the child elements, to communicate that they are tree view nodes. role=\"group\" is declared on child ul elements, to communicate that they contain branch and leaf nodes.\n\nis declared on child elements, to communicate that they contain branch and leaf nodes. aria-expanded is declared on directories, with a value of true to communicate that the branch node is in an opened state and a value of false to communicate that it is in a collapsed state instead.\n\nis declared on directories, with a value of to communicate that the branch node is in an opened state and a value of to communicate that it is in a collapsed state instead. aria-selected is used to indicate if branch or leaf nodes have been chosen by user navigation, and can therefore have user actions applied to them.\n\nWe also made the following additions:\n\naria-hidden=\"true\" is applied to SVG icons (folders, files, etc.) to ensure its content is not announced.\n\nis applied to SVG icons (folders, files, etc.) to ensure its content is not announced. aria-current=\"true\" is placed on the selected node to better support when a node is deep linked to via URL.\n\nNOTE: We use “branch node” and “leaf node” as broad terms that can apply to all tree view components we use on GitHub. For the file tree, branch nodes would correspond to directories and subdirectories, and leaf nodes would correspond to files.\n\nSupport expected navigation techniques\n\nThe following behaviors are what people will try when operating a tree view construct, so we support them:\n\nKeyboard keypresses\n\nTab : Places focus on the entire tree view component, then moves focus to the next focusable item on the view.\n\n: Places focus on the entire tree view component, then moves focus to the next focusable item on the view. Enter : If a branch node is selected: Displays the directory’s contents. If a leaf node is selected: Displays the leaf node’s contents.\n\n: Down : Moves selection to the next node that can be selected without opening or closing a node.\n\n: Moves selection to the next node that can be selected without opening or closing a node. Up : Moves selection to the previous node that can be selected without opening or closing a node.\n\n: Moves selection to the previous node that can be selected without opening or closing a node. Right : If a branch node is selected and in a collapsed state: Expands the selected collapsed branch node and does not move selection. If a branch node is selected and in an expanded state: Moves selection to the directory’s first child node.\n\n: Left : If a branch node is selected and in an expanded state: Collapses the selected collapsed directory node and does not move selection. If a branch node is selected and in a collapsed state: Moves selection to the node’s parent directory. If a leaf node is selected: Moves selection to the leaf node’s parent directory.\n\n: End : Moves selection to the last node that can be selected.\n\n: Moves selection to the last node that can be selected. Home : Moves selection to the first node that can be selected.\n\nWe also support typeahead selection, as we are modeling Windows File Explorer’s tree view behaviors. Here, we move selection to the node closest to the currently selected node whose name matches what the user types.\n\nMiddle clicking\n\nNodes on tree view constructs are tree items, not links. Because of this, tree view nodes do not support the behaviors you get with using an anchor element, such as opening its URL in a new tab or window.\n\nWe use JavaScript to listen for middle clicks and Control + Enter keypresses to replicate this behavior.\n\nConsider states\n\nLoading\n\nTree views on GitHub can take time to retrieve their content, and we may not always know how much content a branch node contains.\n\nLive region announcements are tricky to get right, but integral to creating an equivalent experience. We use the following announcements:\n\nIf there is a known amount of nodes that load, we enumerate the incoming content with an announcement that reads, “Loading {x} items.”\n\nIf there is an unknown number of nodes that load, we instead use a more generic announcement of, “Loading…”\n\nIf there are no nodes that load we use an announcement message that reads, “{branch node name} is empty.”\n\nAdditionally, we manage focus for loading content:\n\nIf focus is placed on a placeholder loading node when the content loads in: Move focus from the placeholder node to the first child node in the branch node.\n\nIf focus is on a placeholder loading node but the branch node does not contain content: Move focus back to the branch node. Additionally, we remove the branch node’s aria-expanded declaration.\n\nErrors\n\nCircumstances can conspire to interfere with a tree view component’s intended behavior. Examples of this could be a branch node failing to retrieve content or a partial system outage.\n\nIn these scenarios, the tree view component will use a straightforward dialog component to communicate the error.\n\nFix interoperability issues\n\nAs previously touched on, complicated interaction patterns run the risk of compatibility issues. Because of this, it’s essential to test your efforts with actual assistive technology to ensure it actually works.\n\nWe made the following adjustments to provide better assistive technology support:\n\nUse aria-level\n\nScreen readers can report on the depth of a nested list item. For example, a li element placed inside of a ul element nested three levels deep can announce itself as such.\n\nWe found that we needed to explicitly declare the level on each li element to recreate this behavior for a tree view. For our example, we’d also need to set aria-level=\"3\" on the li element.\n\nThis fix addressed multiple forms of assistive technology we tested with.\n\nExplicitly set the node’s accessible name on the li element\n\nA node’s accessible name is typically set by the text string placed inside the li element:\n\n<li>README.md</li>\n\nHowever, we found that VoiceOver on macOS and iOS did not support this. This may be because of the relative complexity of each node’s inner DOM structure.\n\nWe used aria-labelledby to get around this problem, with a value that pointed to the id set on the text portion of each node:\n\n<li aria-labelledby=\"readme-md\"> <div> <!-- Icon --> </div> <div id=\"readme-md\"> README.md </div> </li>\n\nThis guarantees that:\n\nthe node’s accessible name is announced when focus is placed on the li element,\n\nelement, and that the announcement matches what is shown visually.\n\nWhere we’d like to go from here\n\nThere’s a couple areas we’re prototyping and iterating on to better serve our users:\n\nBrowsers apply a lot of behaviors to anchor elements, such as the ability to copy the URL.\n\nWe’d like to replace the JavaScript that listens for middle clicks with a more robust native solution, only without sacrificing interoperability and assistive technology support.\n\nSupporting multiple actions per node\n\nTree views constructs were designed assuming a user will only ever navigate to a node and activate it.\n\nGitHub has use cases that require actions other than activating the node, and we’re exploring how to accomplish that. This is exciting, as it represents an opportunity to evolve the tree view construct on the web.\n\nAlways learning\n\nAn accessible tree view is a complicated component to make, and it requires a lot of effort and testing to get right. However, this work helps to ensure that everyone can use a core part of GitHub, regardless of device, circumstance, or ability.\n\nWe hope that highlighting the considerations that went into our work can help you on your accessibility journey.\n\n\n\nShare your experience: We’d love to hear from you if you’ve run into issues using our tree view component with assistive technology. This feedback is invaluable to helping us continue to make GitHub more accessible.", "label": "non_personal"}
{"title": "How GitHub uses CodeQL to secure GitHub", "url": "https://github.blog/engineering/how-github-uses-codeql-to-secure-github/", "content": "GitHub’s Product Security Engineering team writes code and implements tools that help secure the code that powers GitHub. We use GitHub Advanced Security (GHAS) to discover, track, and remediate vulnerabilities and enforce secure coding standards at scale. One tool we rely heavily on to analyze our code at scale is CodeQL.\n\nCodeQL is GitHub’s static analysis engine that powers automated security analyses. You can use it to query code in much the same way you would query a database. It provides a much more robust way to analyze code and uncover problems than an old-fashioned text search through a codebase.\n\nThe following post will detail how we use CodeQL to keep GitHub secure and how you can apply these lessons to your own organization. You will learn why and how we use:\n\nCustom query packs (and how we create and manage them).\n\nCustom queries.\n\nVariant analysis to uncover potentially insecure programming practices.\n\nEnabling CodeQL at scale\n\nWe employ CodeQL in a variety of ways at GitHub.\n\nDefault setup with the default and security-extended query suites\n\nDefault setup with the default and security-extended query suites meets the needs of the vast majority of our over 10,000 repositories. With these settings, pull requests automatically get a security review from CodeQL. Advanced setup with a custom query pack\n\nA few repositories, like our large Ruby monolith, need extra special attention, so we use advanced setup with a query pack containing custom queries to really tailor to our needs. Multi-repository variant analysis (MRVA)\n\nTo conduct variant analysis and quick auditing, we use MRVA. We also write custom CodeQL queries to detect code patterns that are either specific to GitHub’s codebases or patterns we want a security engineer to manually review.\n\nThe specific custom Actions workflow step we use on our monolith is pretty simple. It looks like this:\n\n- name: Initialize CodeQL uses: github/codeql-action/init@v3 with: languages: ${{ matrix.language }} config-file: ./.github/codeql/${{ matrix.language }}/codeql-config.yml\n\nOur Ruby configuration is pretty standard, but advanced setup offers a variety of configuration options using custom configuration files. The interesting part is the packs option, which is how we enable our custom query pack as part of the CodeQL analysis. This pack contains a collection of CodeQL queries we have written for Ruby, specifically for the GitHub codebase.\n\nSo, let’s dive deeper into why we did that—and how!\n\nPublishing our CodeQL query pack\n\nInitially, we published CodeQL query files directly to the GitHub monolith repository, but we moved away from this approach for several reasons:\n\nIt required going through the production deployment process for each new or updated query.\n\nQueries not included in a query pack were not pre-compiled, which slowed down CodeQL analysis in CI.\n\nOur test suite for CodeQL queries ran as part of the monolith’s CI jobs. When a new version of the CodeQL CLI was released, it sometimes caused the query tests to fail because of changes in the query output, even when there were no changes to the code in the pull request. This often led to confusion and frustration among engineers, as the failure wasn’t related to their pull request changes.\n\nBy switching to publishing a query pack to GitHub Container Registry (GCR), we’ve simplified our process and eliminated many of these pain points, making it easier to ship and maintain our CodeQL queries. So while it’s possible to deploy custom CodeQL query files directly to a repository, we recommend publishing CodeQL queries as a query pack to the GCR for easier deployment and faster iteration.\n\nCreating our query pack\n\nWhen setting up our custom query pack, we faced several considerations, particularly around managing dependencies like the ruby-all package.\n\nTo ensure our custom queries remain maintainable and concise, we extend classes from the default query suite, such as the ruby-all library. This allows us to leverage existing functionality rather than reinventing the wheel, keeping our queries concise and maintainable. However, changes to the CodeQL library API can introduce breaking changes, potentially deprecating our queries or causing errors. Since CodeQL runs as part of our CI, we wanted to minimize the chance of this happening, as this can lead to frustration and loss of trust from developers.\n\nWe develop our queries against the latest version of the ruby-all package, ensuring we’re always working with the most up-to-date functionality. To mitigate the risk of breaking changes affecting CI, we pin the ruby-all version when we’re ready to release, locking it in the codeql-pack.lock.yml file. This guarantees that when our queries are deployed, they will run with the specific version of ruby-all we’ve tested, avoiding potential issues from unintentional updates.\n\nHere’s how we manage this setup:\n\nIn our qlpack.yml, we set the dependency to use the latest version of ruby-all\n\nDuring development, this configuration pulls in the latest version) of ruby-all when running codeql pack init , ensuring we’re always up to date. // Our custom query pack's qlpack.yml library: false name: github/internal-ruby-codeql version: 0.2.3 extractor: 'ruby' dependencies: codeql/ruby-all: \"*\" tests: 'test' description: \"Ruby CodeQL queries used internally at GitHub\"\n\nwhen running , ensuring we’re always up to date. Before releasing, we lock the version in the codeql-pack.lock.yml file, specifying the exact version to ensure stability and prevent issues in CI. // Our custom query pack's codeql-pack.lock.yml lockVersion: 1.0.0 dependencies: ... codeql/ruby-all: version: 1.0.6\n\nThis approach allows us to balance developing against the latest features of the ruby-all package while ensuring stability when we release.\n\nWe also have a set of CodeQL unit tests that exercise our queries against sample code snippets, which helps us quickly determine if any query will cause errors before we publish our pack. These tests are run as part of the CI process in our query pack repository, providing an early check for issues. We strongly recommend writing unit tests for your custom CodeQL queries to ensure stability and reliability.\n\nAltogether, the basic flow for releasing new CodeQL queries via our pack is as follows:\n\nOpen a pull request with the new query.\n\nWrite unit tests for the new query.\n\nMerge the pull request.\n\nIncrement the pack version in a new pull request.\n\nRun codeql pack init to resolve dependencies.\n\nto resolve dependencies. Correct unit tests as needed.\n\nPublish the query pack to the GitHub Container Registry (GCR).\n\nRepositories with the query pack in their config will start using the updated queries.\n\nWe have found this flow balances our team’s development experience while ensuring stability in our published query pack.\n\nConfiguring our repository to use our custom query pack\n\nWe won’t provide a general recommendation on configuration here, given that it ultimately depends on how your organization deploys code. We opted against locking our pack to a particular version in our CodeQL configuration file (see above). Instead, we chose to manage our versioning by publishing the CodeQL package in GCR. This results in the GitHub monolith retrieving the latest published version of the query pack. To roll back changes, we simply have to republish the package. In one instance, we released a query that had a high number of false positives and we were able to publish a new version of the pack that removed that query in less than 15 minutes. This is faster than the time it would have taken us to merge a pull request on the monolith repository to roll back the version in the CodeQL configuration file.\n\nOne of the problems we encountered with publishing the query pack in GCR was how to easily make the package available to multiple repositories within our enterprise. There are several approaches we explored.\n\nGrant access permissions for individual repositories. On the package management page, you can grant permissions for individual repositories to access your package. This was not a good solution for us since we have too many repositories for it to be feasible to do manually, yet there is not currently a way to configure programmatically using an API.\n\nOn the package management page, you can grant permissions for individual repositories to access your package. This was not a good solution for us since we have too many repositories for it to be feasible to do manually, yet there is not currently a way to configure programmatically using an API. Mint a personal access token for the CodeQL action runner. We could have minted a personal access token (PAT) that has access to read all packages for our organization and added that to the CodeQL action runner. However, this would have required managing a new token, and it seemed a bit more permissive than we wanted because it could read all of our private packages rather than ones we explicitly allow it to have access to.\n\nWe could have minted a personal access token (PAT) that has access to read all packages for our organization and added that to the CodeQL action runner. However, this would have required managing a new token, and it seemed a bit more permissive than we wanted because it could read all of our private packages rather than ones we explicitly allow it to have access to. Provide access permissions via a linked repository. We ended up implementing the third solution that we explored. We link a repository to the package and allow the package to inherit access permissions from the linked repository.\n\nCodeQL query pack queries\n\nWe write a variety of custom queries to be used in our custom query packs. These cover GitHub-specific patterns that aren’t included in the default CodeQL query pack. This allows us to tailor the analysis to patterns and preferences that are specific to our company and codebase. Some of the types of things we alert on using our custom query pack include:\n\nHigh-risk APIs specific to GitHub’s code that can be dangerous if they receive unsanitized user input.\n\nUse of specific built-in Rails methods for which we have safer, custom methods or functions.\n\nRequired authorization methods not being used in our REST API endpoint definitions and GraphQL object/mutation definitions.\n\nREST API endpoints and GraphQL mutations that require engineers to define access control methods to determine which actors can access them. (Specifically, the query detects the absence of this method definition to ensure that the actors’ permissions are being checked for these endpoints.)\n\nUse of signed tokens so we can nudge engineers to include Product Security as a reviewer when using them.\n\nCustom queries can be used more for educational purposes rather than being blockers to shipping code. For example, we want to alert engineers when they use the ActiveRecord::decrypt method. This method should generally not be used in production code, as it will cause an encrypted column to become decrypted. We use the recommendation severity in the query metadata so these alerts are treated as more of an informational alert. That means this may trigger an alert in a pull request, but it won’t cause the CodeQL CI job to fail. We use this lower severity level to allow engineers to assess the impact of new queries without immediate blocking. Additionally, this alert level isn’t tracked through our Fundamentals program, meaning it doesn’t require immediate action, reflecting the query’s maturity as we continue to refine its relevance and risk assessment.\n\n/** * @id rb/github/use-of-activerecord-decrypt * @description Do not use the .decrypt method on AR models, this will decrypt all encrypted attributes and save * them unencrypted, effectively undoing encryption and possibly making the attributes inaccessible. * If you need to access the unencrypted value of any attribute, you can do so by calling my_model.attribute_name. * @kind problem * @severity recommendation * @name Use of ActiveRecord decrypt method * @tags security * github-internal */ import ruby import DataFlow import codeql.ruby.DataFlow import codeql.ruby.frameworks.ActiveRecord /** Match against .decrypt method calls where the receiver may be an ActiveRecord object */ class ActiveRecordDecryptMethodCall extends ActiveRecordInstanceMethodCall { ActiveRecordDecryptMethodCall() { this.getMethodName() = \"decrypt\" } } from ActiveRecordDecryptMethodCall call select call, \"Do not use the .decrypt method on AR models, this will decrypt all encrypted attributes and save them unencrypted.\n\nAnother educational query is the one mentioned above in which we detect the absence of the `control_access` method in a class that defines a REST API endpoint. If a pull request introduces a new endpoint without `control_access`, a comment will appear on the pull request saying that the `control_access` method wasn’t found and it’s a requirement for REST API endpoints. This will notify the reviewer of a potential issue and prompt the developer to fix it.\n\n/** * @id rb/github/api-control-access * @name Rest API Without 'control_access' * @description All REST API endpoints must call the 'control_access' method, to ensure that only specified actor types are able to access the given endpoint. * @kind problem * @tags security * github-internal * @precision high * @problem.severity recommendation */ import codeql.ruby.AST import codeql.ruby.DataFlow import codeql.ruby.TaintTracking import codeql.ruby.ApiGraphs // Api::App REST API endpoints should generally call the control_access method private DataFlow::ModuleNode appModule() { result = API::getTopLevelMember(\"Api\").getMember(\"App\").getADescendentModule() and not result = protectedApiModule() and not result = staffAppApiModule() } // Api::Admin, Api::Staff, Api::Internal, and Api::ThirdParty REST API endpoints do not need to call the control_access method private DataFlow::ModuleNode protectedApiModule() { result = API::getTopLevelMember([\"Api\"]) .getMember([\"Admin\", \"Staff\", \"Internal\", \"ThirdParty\"]) .getADescendentModule() } // Api::Staff::App REST API endpoints do not need to call the control_access method private DataFlow::ModuleNode staffAppApiModule() { result = API::getTopLevelMember([\"Api\"]).getMember(\"Staff\").getMember(\"App\").getADescendentModule() } private class ApiRouteWithoutControlAccess extends DataFlow::CallNode { ApiRouteWithoutControlAccess() { this = appModule().getAModuleLevelCall([\"get\", \"post\", \"delete\", \"patch\", \"put\"]) and not performsAccessControl(this.getBlock()) } } predicate performsAccessControl(DataFlow::BlockNode blocknode) { accessControlCalled(blocknode.asExpr().getExpr()) } predicate accessControlCalled(Block block) { // the method `control_access` is called somewhere inside `block` block.getAStmt().getAChild*().(MethodCall).getMethodName() = \"control_access\" } from ApiRouteWithoutControlAccess api select api.getLocation(), \"The control_access method was not detected in this REST API endpoint. All REST API endpoints must call this method to ensure that the endpoint is only accessible to the specified actor types.\"\n\nVariant analysis\n\nVariant analysis (VA) refers to the process of searching for variants of security vulnerabilities. This is particularly useful when we’re responding to a bug bounty submission or a security incident. We use a combination of tools to do this, including GitHub’s code search functionality, custom scripts, and CodeQL. We will often start by using code search to find patterns similar to the one that caused a particular vulnerability across numerous repositories. This is sometimes not good enough, as code search is not semantically aware, meaning that it cannot determine whether a given variable is an Active Record object or whether it is being used in an `if` expression. To answer those types of questions we turn to CodeQL.\n\nWhen we write CodeQL queries for variant analysis we are much less concerned about false positives, since the goal is to provide results for security engineers to analyze. The quality of the code is also not quite as important, as these queries will only be used for the duration of the VA effort. Some of the types of things we use CodeQL for during VAs are:\n\nWhere are we using SHA1 hashes?\n\nOne of our internal API endpoints was vulnerable to SQLi according to a recent bug bounty report. Where are we passing user input to that API endpoint?\n\nThere is a problem with how some HTTP request libraries in Ruby handle the proxy setting. Can we look at places we are instantiating our HTTP request libraries with a proxy setting?\n\nOne recent example involved a subtle vulnerability in Rails. We wanted to detect when the following condition was present in our code:\n\nA parameter was used to look up an Active Record object.\n\nThat parameter is later reused after the Active Record object is looked up.\n\nThe concern with this condition is that it could lead to an insecure direct object reference (IDOR) vulnerability because Active Record finder methods can accept an array. If the code looks up an Active Record object in one call to determine if a given entity has access to a resource, but later uses a different element from that array to find an object reference, that can lead to an IDOR vulnerability. It would be difficult to write a query to detect all vulnerable instances of this pattern, but we were able to write a query that found potential vulnerabilities that gave us a list of code paths to manually analyze. We ran the query against a large number of our Ruby codebases using CodeQL’s MRVA.\n\nThe query, which is a bit hacky and not quite production grade, is below:\n\n/** * @name wip array query * @description an array is passed to an AR finder object */ import ruby import codeql.ruby.AST import codeql.ruby.ApiGraphs import codeql.ruby.frameworks.Rails import codeql.ruby.frameworks.ActiveRecord import codeql.ruby.frameworks.ActionController import codeql.ruby.DataFlow import codeql.ruby.Frameworks import codeql.ruby.TaintTracking // Gets the \"final\" receiver in a chain of method calls. // For example, in `Foo.bar`, this would give the `Foo` access, and in // `foo.bar.baz(\"arg\")` it would give the `foo` variable access private Expr getUltimateReceiver(MethodCall call) { exists(Expr recv | recv = call.getReceiver() and ( result = getUltimateReceiver(recv) or not recv instanceof MethodCall and result = recv ) ) } // Names of class methods on ActiveRecord models that may return one or more // instances of that model. This also includes the `initialize` method. // See https://api.rubyonrails.org/classes/ActiveRecord/FinderMethods.html private string staticFinderMethodName() { exists(string baseName | baseName = [\"find_by\", \"find_or_create_by\", \"find_or_initialize_by\", \"where\"] and result = baseName + [\"\", \"!\"] ) // or // result = [\"new\", \"create\"] } private class ActiveRecordModelFinderCall extends ActiveRecordModelInstantiation, DataFlow::CallNode { private ActiveRecordModelClass cls; ActiveRecordModelFinderCall() { exists(MethodCall call, Expr recv | call = this.asExpr().getExpr() and recv = getUltimateReceiver(call) and ( // The receiver refers to an `ActiveRecordModelClass` by name recv.(ConstantReadAccess).getAQualifiedName() = cls.getAQualifiedName() or // The receiver is self, and the call is within a singleton method of // the `ActiveRecordModelClass` recv instanceof SelfVariableAccess and exists(SingletonMethod callScope | callScope = call.getCfgScope() and callScope = cls.getAMethod() ) ) and ( call.getMethodName() = staticFinderMethodName() or // dynamically generated finder methods call.getMethodName().indexOf(\"find_by_\") = 0 ) ) } final override ActiveRecordModelClass getClass() { result = cls } } class FinderCallArgument extends DataFlow::Node { private ActiveRecordModelFinderCall finderCallNode; FinderCallArgument() { this = finderCallNode.getArgument(_) } } class ParamsHashReference extends DataFlow::CallNode { private Rails::ParamsCall params; // TODO: only direct element references against `params` calls are considered ParamsHashReference() { this.getReceiver().asExpr().getExpr() = params } string getArgString() { result = this.getArgument(0).asExpr().getConstantValue().getStringlikeValue() } } class ArrayPassedToActiveRecordFinder extends TaintTracking::Configuration { ArrayPassedToActiveRecordFinder() { this = \"ArrayPassedToActiveRecordFinder\" } override predicate isSource(DataFlow::Node source) { source instanceof ParamsHashReference } override predicate isSink(DataFlow::Node sink) { sink instanceof FinderCallArgument } string getParamsArg(DataFlow::CallNode paramsCall) { result = paramsCall.getArgument(0).asExpr().getConstantValue().getStringlikeValue() } // this doesn't check for anything fancy like whether it's reuse in a if/else // only intended for quick manual audit filtering of interesting candidates // so remains fairly broad to not induce false negatives predicate paramsUsedAfterLookups(DataFlow::Node source) { exists(DataFlow::CallNode y | y instanceof ParamsHashReference and source.getEnclosingMethod() = y.getEnclosingMethod() and source != y and getParamsArg(source) = getParamsArg(y) // we only care if it's used again AFTER an object lookup and y.getLocation().getStartLine() > source.getLocation().getStartLine()) } } from ArrayPassedToActiveRecordFinder config, DataFlow::Node source, DataFlow::Node sink where config.hasFlow(source, sink) and config.paramsUsedAfterLookups(source) select source, sink.getLocation()\n\nConclusion\n\nCodeQL can be very useful for product security engineering teams to detect and prevent vulnerabilities at scale. We use a combination of queries that run in CI using our query pack and one-off queries run through MRVA to find potential vulnerabilities and communicate them to engineers. CodeQL isn’t only useful for finding security vulnerabilities, though; it is also useful for detecting the presence or absence of security controls that are defined in code. This saves our security team time by surfacing certain security problems automatically, and saves our engineers time by detecting them earlier in the development process.\n\nWriting custom CodeQL queries\n\nTips for getting started\n\nWe have a large number of articles and resources for writing custom CodeQL queries. If you haven’t written custom CodeQL queries before, here are some resources to help get you started:\n\nImprove the security of your applications today by enabling CodeQL for free on your public repositories, or try GitHub Advanced Security for your organization.\n\nMichael Recachinas, GitHub Staff Security Engineer, also contributed to this blog post.", "label": "non_personal"}
{"title": "How to debug code with GitHub Copilot", "url": "https://github.blog/ai-and-ml/github-copilot/how-to-debug-code-with-github-copilot/", "content": "Debugging is an essential part of a developer’s workflow—but it’s also one of the most time consuming. What if AI could streamline the process, helping you analyze, fix, and document code faster? Enter GitHub Copilot, your AI-powered coding assistant.\n\nGitHub Copilot isn’t just for writing code—it’s also a powerful tool for debugging. Whether you’re troubleshooting in your IDE, using Copilot Chat’s slash commands like /fix , or reviewing pull requests (PR) on github.com, GitHub Copilot offers flexible, intelligent solutions to speed up your debugging process. And with the free version of GitHub Copilot, available to all personal GitHub accounts, you can start exploring these features today.\n\nIn this guide, we’ll explore how to debug code with GitHub Copilot, where to use it in your workflow, and best practices to get the most out of its capabilities. Whether you’re new to GitHub Copilot or looking to deepen your skills, this guide has something for you.\n\nStart using GitHub Copilot 🌟 GitHub Copilot Free includes 2,000 code completions, 50 Copilot Chat messages per month, multi-file edits, and model options like GPT-4o or Claude 3.5 Sonnet, with native support in VS Code and on GitHub.\n\nDebugging code with GitHub Copilot: surfaces and workflows\n\nDebugging code with GitHub Copilot can help you tackle issues faster while enhancing your understanding of the codebase. Whether you’re fixing syntax errors, refactoring inefficient code, or troubleshooting unexpected behavior, GitHub Copilot can provide valuable insights in your debugging journey.\n\nSo, how exactly does this work? “GitHub Copilot is recognizing patterns and suggesting solutions based on what it has learned,” says Christopher Harrison, Senior Developer Advocate. “Once you’ve identified the problem area, you can turn to GitHub Copilot and ask, ‘I’m giving this input but getting this output—what’s wrong?’ That’s where GitHub Copilot really shines.”\n\nLet’s explore how GitHub Copilot can help you debug your code across different surfaces, from your IDE to github.com and even pull requests.\n\n1. In Copilot Chat\n\nCopilot Chat acts as an interactive AI assistant, helping you debug issues with natural language queries. And with Copilot Free, you get 50 chat messages per month. With Copilot Chat, you can:\n\nGet real-time explanations: Ask “Why is this function throwing an error?” and Copilot Chat will analyze the code and provide insights.\n\nAsk “Why is this function throwing an error?” and Copilot Chat will analyze the code and provide insights. Use slash commands for debugging: Try /fix to generate a potential solution or /explain for a step-by-step breakdown of a complex function. (More on this later!)\n\nTry to generate a potential solution or for a step-by-step breakdown of a complex function. (More on this later!) Refactor code for efficiency: If your implementation is messy or inefficient, Copilot Chat can suggest cleaner alternatives. Christopher explains, “Refactoring improves the readability of code, making it easier for both developers and GitHub Copilot to understand. And if code is easier to understand, it’s easier to debug and spot problems.”\n\nIf your implementation is messy or inefficient, Copilot Chat can suggest cleaner alternatives. Christopher explains, “Refactoring improves the readability of code, making it easier for both developers and GitHub Copilot to understand. And if code is easier to understand, it’s easier to debug and spot problems.” Walk through errors interactively: Describe your issue in chat and get tailored guidance without ever having to leave your IDE.\n\n🔎 How to find GitHub Copilot Chat Look for the GitHub Copilot icon or prompt to start a conversation in your IDE, on github.com, or within pull requests. Simply click on the icon to get started with code suggestions, unit tests, suggested code fixes, and more!\n\n2. In your IDE\n\nWhen working in popular IDEs like VS Code or JetBrains, GitHub Copilot offers real-time suggestions as you type. It helps by:\n\nFlagging issues: For example, if you declare a variable but forget to initialize it, GitHub Copilot can suggest a correction.\n\nFor example, if you declare a variable but forget to initialize it, GitHub Copilot can suggest a correction. Code fixes: Encounter a syntax error? GitHub Copilot can suggest a fix in seconds, ensuring your code stays error-free.\n\nEncounter a syntax error? GitHub Copilot can suggest a fix in seconds, ensuring your code stays error-free. Contextual assistance: By analyzing your workspace, GitHub Copilot provides solutions tailored to your codebase and project structure.\n\n🔎 How to find GitHub Copilot in VS Code To open up the chat view, head over to the VS Code title bar and select, “Use AI features with Copilot for Free.”\n\nSign in with your GitHub account by clicking on “Sign in to use Copilot for Free.”\n\nDon’t have a GitHub Copilot subscription yet? Then follow the browser steps to sign up for your Copilot Free plan. Learn how to install GitHub Copilot in JetBrains, Azure Data Studio, and more >\n\n3. On github.com\n\nGitHub Copilot extends beyond your IDE, offering debugging assistance directly on github.com via Copilot Chat, particularly in repositories and discussions. With this feature, you can:\n\nTroubleshoot code in repositories: Open a file, highlight a problematic section, and use Copilot Chat to analyze it.\n\nOpen a file, highlight a problematic section, and use Copilot Chat to analyze it. Generate test cases: If you’re unsure how to verify a function, GitHub Copilot can suggest test cases based on existing code.\n\nIf you’re unsure how to verify a function, GitHub Copilot can suggest test cases based on existing code. Understand unfamiliar code: Reviewing an open-source project or a teammate’s PR? Ask GitHub Copilot to summarize a function or explain its logic.\n\n🔎 How to find GitHub Copilot on github.com\n\n4. For pull request assistance\n\nGitHub Copilot can also streamline debugging within PRs, ensuring code quality before merging.\n\nSuggest improvements in PR comments: GitHub Copilot can review PRs and propose fixes directly in the conversation.\n\nGitHub Copilot can review PRs and propose fixes directly in the conversation. Generate PR summaries: Struggling to describe your changes? Greg Larkin, Senior Service Delivery Engineer, says, “I use GitHub Copilot in the PR creation process to generate a summary of the changes in my feature branch compared to the branch I’m merging into. That can be really helpful when I’m struggling to figure out a good description, so that other people understand what I did.”\n\nStruggling to describe your changes? Greg Larkin, Senior Service Delivery Engineer, says, “I use GitHub Copilot in the PR creation process to generate a summary of the changes in my feature branch compared to the branch I’m merging into. That can be really helpful when I’m struggling to figure out a good description, so that other people understand what I did.” Explain diffs: Not sure why a change was made? Ask GitHub Copilot to summarize what’s different between commits.\n\nNot sure why a change was made? Ask GitHub Copilot to summarize what’s different between commits. Catch edge cases before merging: Use /analyze to identify potential issues and /tests to generate missing test cases.\n\nUse to identify potential issues and to generate missing test cases. Refactor on the fly: If a PR contains redundant or inefficient code, GitHub Copilot can suggest optimized alternatives.\n\nBy integrating Copilot into your PR workflow, you can speed up code reviews while maintaining high-quality standards. Just be sure to pair it with peer expertise for the best results.\n\n🔎 How to find GitHub Copilot in pull requests\n\n5 slash commands in GitHub Copilot for debugging code\n\nSlash commands turn GitHub Copilot into an on-demand debugging assistant, helping you solve issues faster, get more insights, and improve your code quality. Here are some of the most useful slash commands for debugging:\n\n1. Use /help to get guidance on using GitHub Copilot effectively\n\nThe /help slash command provides guidance on how to interact with GitHub Copilot effectively, offering tips on structuring prompts, using slash commands, and maximizing GitHub Copilot’s capabilities.\n\nHow it works : Type /help in Copilot Chat to receive suggestions on your current task, whether it’s debugging, explaining code, or generating test cases.\n\n: Type in Copilot Chat to receive suggestions on your current task, whether it’s debugging, explaining code, or generating test cases. Example: Need a refresher on what GitHub Copilot can do? Use /help to access a quick guide to slash commands like /fix and /explain .\n\n2. Use /fix to suggest and apply fixes\n\nThe /fix command is a go-to tool for resolving code issues by allowing you to highlight a block of problematic code or describe an error.\n\nHow it works: Select the code causing issues, type /fix , and let Copilot Chat generate suggestions.\n\nSelect the code causing issues, type , and let Copilot Chat generate suggestions. Example: If you have a broken API call, use /fix to get a corrected version with appropriate headers or parameters.\n\n3. Use /explain to understand code and errors\n\nThe /explain command breaks down complex code or cryptic error messages into simpler, more digestible terms.\n\nHow it works: Highlight the code or error message you want clarified, type /explain , and Copilot Chat will provide an explanation. It will explain the function’s purpose, how it processes the data, potential edge cases, and any possible bugs or issues.\n\nHighlight the code or error message you want clarified, type , and Copilot Chat will provide an explanation. It will explain the function’s purpose, how it processes the data, potential edge cases, and any possible bugs or issues. Example: Encounter a “NullPointerException”? Use /explain to understand why it occurred and how to prevent it.\n\n4. Use /tests to generate tests\n\nTesting is key to identifying bugs, and the /tests command helps by generating test cases based on your code.\n\nHow it works: Use /tests on a function or snippet, and Copilot Chat will generate relevant test cases.\n\nUse on a function or snippet, and Copilot Chat will generate relevant test cases. Example: Apply /tests to a sorting function, and Copilot Chat might generate unit tests for edge cases like empty arrays or null inputs.\n\n5. Use /doc to generate or improve documentation\n\nThere are long-term benefits to having good text documentation—for developers and GitHub Copilot, which can draw context from it—because it makes your codebase that much more searchable. By using the /doc command with Copilot Free, you can even ask GitHub Copilot to write a summary of specific code blocks within your IDE.\n\nThe /doc command helps you create or refine documentation for your code, which is critical when debugging or collaborating with others. Clear documentation provides context for troubleshooting, speeds up issue resolution, and helps fellow developers understand your code faster.\n\nHow it works: Highlight a function, class, or file, type /doc and right-click to see the context menu, and Copilot Chat will generate comprehensive comments or documentation.\n\nHighlight a function, class, or file, type and right-click to see the context menu, and Copilot Chat will generate comprehensive comments or documentation. Example: Apply /doc to a function, and Copilot Chat will generate inline comments detailing its purpose, parameters, and expected output.\n\nBy mastering these commands, you can streamline your debugging workflow and resolve issues faster without switching between tools or wasting time on manual tasks.\n\nUsing shortcut keys: Quickly activate GitHub Copilot’s debugging features in VS Code Start/continue debugging: F5\n\nStop debugging: Shift + F5\n\nStep over: F10\n\nStep into: F11\n\nStep out: Shift + F11\n\nToggle breakpoint: F9\n\nBest practices for debugging code with GitHub Copilot\n\nProvide clear context for better results\n\nProviding the right context helps GitHub Copilot generate even more relevant debugging suggestions. As Christopher explains, “The better that Copilot is able to understand what you’re trying to do and how you’re trying to do it, the better the responses are that it’s able to give to you.”\n\nSince GitHub Copilot analyzes your code within the surrounding scope, ensure your files are well structured and that relevant dependencies are included. If you’re using Copilot Chat, reference specific functions, error messages, or logs to get precise answers instead of generic suggestions.\n\n💡 Pro tip: Working across multiple files? Use the @workspace command to point GitHub Copilot in the right direction and give it more context for your prompt and intended goal.\n\nAsk, refine, and optimize in real time\n\nInstead of treating GitHub Copilot as a one-and-done solution, refine its suggestions by engaging in a back-and-forth process. Greg says, “I find it useful to ask GitHub Copilot for three or four different options on how to fix a problem or to analyze for performance. The more detail you provide about what you’re after—whether it’s speed, memory efficiency, or another constraint—the better the result.”\n\nThis iterative approach can help you explore alternative solutions you might not have considered, leading to more robust and efficient code.\n\nMaster the art of specific prompts\n\nThe more specific your prompt, the better GitHub Copilot’s response. Instead of asking “What’s wrong with this function?” try “Why is this function returning undefined when the input is valid?” GitHub Copilot performs best when given clear, detailed queries—this applies whether you’re requesting a fix, asking for an explanation, or looking for test cases to verify your changes.\n\nBy crafting precise prompts and testing edge cases, you can use GitHub Copilot to surface potential issues before they become production problems.\n\nTry a structured approach with progressive debugging\n\nNext, try a step-by-step approach to your debugging process! Instead of immediately applying fixes, use GitHub Copilot’s commands to first understand the issue, analyze potential causes, and then implement a solution. This structured workflow—known as progressive debugging—helps you gain deeper insights into your code while ensuring that fixes align with the root cause of the problem.\n\nFor example:\n\nStart with the slash command /explain on a problematic function to understand the issue. Use the slash command /startDebugging to help with configuring interactive debugging. Finally, apply the slash command /fix to generate possible corrections.\n\n📌 Use case: If a function in your React app isn’t rendering as expected, start by running /explain on the relevant JSX or state logic, then use /debug to identify mismanaged props, and finally, apply /fix for a corrected implementation.\n\nCombine commands for a smarter workflow\n\nSome issues require multiple levels of debugging and refinement. By combining commands, you can move from diagnosis to resolution even faster.\n\nFor example:\n\nUse /explain + /fix to understand and resolve issues quickly.\n\nto understand and resolve issues quickly. Apply /fixTestFailure + /tests to find failing tests and generate new ones.\n\n📌 Use case:\n\nFixing a broken function: Run the slash command /explain to understand why it fails, then use the slash command /fix to generate a corrected version.\n\nRun the slash command to understand why it fails, then use the slash command to generate a corrected version. Improving test coverage: Use the slash command /fixTestFailure to identify and fix failing tests, then use the slash command /tests to generate additional unit tests for the highlighted code.\n\nRemember, slash commands are most effective when they’re used in the appropriate context, combined with clear descriptions of the problem, are part of a systematic debugging approach, and followed up with verification and testing.\n\nGitHub Copilot is a powerful tool that enhances your workflow, but it doesn’t replace the need for human insight, critical thinking, and collaboration. As Greg points out, “GitHub Copilot can essentially act as another reviewer, analyzing changes and providing comments. Even so, it doesn’t replace human oversight. Having multiple perspectives on your code is crucial, as different reviewers will spot issues that others might miss.”\n\nBy combining GitHub Copilot’s suggestions with human expertise and rigorous testing, you can debug more efficiently while maintaining high-quality, reliable code.\n\nReady to try the free version of GitHub Copilot?\n\nStart using GitHub Copilot today >\n\nYou can keep the learning going with these resources:\n\n* Debug your app with GitHub Copilot in Visual Studio\n\n* Example prompts for GitHub Copilot Chat\n\n* GitHub Copilot and VS Code tutorials", "label": "non_personal"}
{"title": "Finding leaked passwords with AI: How we built Copilot secret scanning", "url": "https://github.blog/engineering/platform-security/finding-leaked-passwords-with-ai-how-we-built-copilot-secret-scanning/", "content": "In October 2024, we announced the general availability of Copilot secret scanning, leveraging AI to detect generic passwords in users’ codebases. This post describes how Copilot secret scanning works under the hood, the challenges we ran into when developing it, and the framework we use for testing and iteration.\n\nWhat is Copilot secret scanning?\n\nCopilot secret scanning is a feature of GitHub Secret Protection, which protects millions of repositories on GitHub by detecting hundreds of pattern types through our partner program. The precision of these detections is paramount for security teams and developers when dealing with security alerts. Historically, our detection approach has relied on regular expressions, which is an effective method for identifying secrets with strict, provider-minted formats. However, this method struggles with the nuanced and varied structures of generic passwords, often generating excessive noise for security teams and developers.\n\nWe now detect generic passwords with GitHub Copilot, using AI to analyze context—such as the usage and location of a potential secret—to limit noise and deliver relevant alerts that are critical to the health and security of your repositories.\n\nGetting to the point where we were confident in our password precision was a journey over many test cases, prompt iterations, and model changes. Let’s dive in to explore what we learned along the way and find out where we’re going.\n\nThe private preview highlighted a problem early on: unconventional file types and structures\n\nAt the core of Copilot secret scanning lies a request to a large language model (LLM), expressed through an LLM prompt consisting of:\n\nGeneral information about the type of vulnerability, in this case passwords.\n\nThe source code location and contents of the file where we believe the vulnerability may exist.\n\nA strict JSON format specification for the model output, to allow for automated processing.\n\nOur first iteration of the prompt used the few-shot prompting technique, which provides the LLM with example inputs and outputs to demonstrate how to perform the task. We wanted a resource-effective model to run the detections at scale and landed on GPT-3.5-Turbo. In parallel, we developed a basic offline evaluation framework, including manually curated test cases with both positive and negative findings, to help us validate that our approach was sound before deploying it to customers.\n\nWe deployed this first iteration to our private preview participants and immediately noticed a problem. While it worked reasonably well at identifying credentials in our offline evaluation, it would fail spectacularly in some customer repositories. The model had difficulty interpreting file types and structures not typically seen in the conventional coding languages and patterns that LLMs train on.\n\nThis experience revealed the complexity of the problem and the limiting nature of LLMs. We had to reevaluate our approach.\n\nThe road to public preview: Improving offline evaluation and prompting\n\nIn response to these initial results, we enhanced the offline evaluation framework in a few key ways. First, we added reports from private preview participants to increase the diversity of our test cases. Next, we enhanced the framework so that we could visually identify and analyze deviations resulting from model or prompt changes. This allowed us to better see the impact of customizing different steps in our prompting strategy. Finally, we leveraged the GitHub Code Security team’s evaluation processes to create a data collection pipeline, and used GPT-4 to create our own test cases based on learnings from existing secret scanning alerts in open source repositories.\n\nThis improved offline evaluation and gave us the breadth needed to measure both precision and recall. Precision is the ability to find secrets more accurately, with concerns to the false positive rate, while recall is the ability to find secrets more reliably, with concerns to the false negative rate.\n\nFrom here, we ran a series of experiments to evaluate detection quality:\n\nWhat if we tried a different model?\n\nWhat if we ran the prompt multiple times and somehow combined the responses?\n\nWhat if we ran two different prompts on two different models in sequence?\n\nHow do we better handle the nondeterministic nature of LLM responses?\n\nMore specifically, we started experimenting with a few different mechanisms to improve our detection with the LLM.\n\nWe tried voting (asking the model the same question many times), which allowed for more deterministic responses but had no material impact on our precision.\n\nWe also tried using a larger model (GPT-4) trained on a larger set of parameters as a confirming scanner, to validate the accuracy of candidates found by GPT-3.5-Turbo. This helped improve precision without reducing our recall, but was also more resource intensive.\n\nWe also tried a few different prompting strategies, such as Fill-in-the-Middle, Zero-Shot, and Chain-of-Thought. We ended up collaborating with our colleagues at Microsoft and used their MetaReflection technique, a novel offline reinforcement learning technique that allows experiential learnings from past trials to come up with a hybrid Chain of Thought (CoT) and few-shot prompt that improves precision with a small penalty in recall.\n\nWe ultimately ended up using a combination of all these techniques and moved Copilot secret scanning into public preview, opening it widely to all GitHub Secret Protection customers. This brings us to our next hurdle: scale.\n\nScaling out capacity for a public preview\n\nSecret scanning not only scans incoming Git pushes, but also your entire Git history on all branches. With each new customer, the necessary resources increase linearly. Rather than simply expanding LLM capacity, we focused on striking the most effective balance between value and cost to ensure optimal performance and efficiency. Before tackling how we managed the resources, we tried to find ways to reduce resource usage itself by:\n\nIdentifying and excluding a class of changes from scanning (such as media files or language files that contain “test,” “mock,” or “spec” in the filepath), because we expected they would never contain credentials or they would be incomprehensible to the model.\n\nExperimenting with newer models, such as GPT-4-Turbo and GPT-4o-mini, that were expected to be less resource intensive without compromising on performance and latency.\n\nExperimenting with different context windows to find one that reduced resources without significantly increasing latency for the LLM to respond to our queries.\n\nMaking improvements to how we tokenize the content we want to scan, including retaining some memory of previous tokenizations while processing new parts of a file.\n\nWhile some of these efforts proved fruitful, such as limiting the content we scanned, other efforts were less effective. For example, breaking down content into smaller pieces didn’t have much of an impact, while using a more powerful model did.\n\nUltimately, the most impactful change came from creating a workload-aware request management system that allowed us to maximize and equitably share LLM capacity against the variety of different workloads we run during scans.\n\nIn building the system, we noticed a fundamental problem that needed addressing in our capacity management: assigning specific rate limits to individual workloads (such as scanning incoming Git commits or scanning the full history) was suboptimal. As each workload was tied to specific traffic patterns—Git commits, for example, tend to correlate with working hours, while full history scanning correlates with discrete events like a security manager or administrator enabling the feature on a new organization—it was easy to land in a situation where an individual workload could run into rate limits within its operational context, leaving additional resources available elsewhere unused.\n\nWe drew significant inspiration from existing solutions in this space, such as Doorman, GitHub’s own Freno, and various other weighted, fair-priority, queue-related algorithms. We came up with an algorithm that allows us to set a range of limits for each workload, preventing the workload from completely overwhelming the LLM, while allowing it to tap into resources from other workloads going unused at the moment. This strategy was so effective at maximizing utilization that we ended up using it within Copilot Autofix and security campaigns as well.\n\nMirror testing our way to general availability\n\nAchieving confidence in detection quality was crucial for moving Copilot secret scanning to general availability. We implemented a mirror testing framework that ran our prompt and filtering changes against a subset of repositories that participated in our public preview. Rescanning these repositories with our latest improvements allowed us to assess the change in real alert volumes and false positive resolutions, without impacting users.\n\nWe found a huge drop in detections and false positives with very few missing real passwords. In some cases, we saw a 94% reduction in false positives across organizations! This before-and-after comparison indicated that all the different changes we made during private and public preview led to increased precision without sacrificing recall, and that we were ready to provide a reliable and efficient detection mechanism to all GitHub Secret Protection customers.\n\nLessons for the future\n\nCopilot secret scanning is now detecting passwords on nearly 35% of all GitHub Secret Protection repositories. We’re continuing to monitor performance and apply lessons learned as we leverage the tooling we created along the way:\n\nA focus on precision: Security and development teams need accurate and actionable alerts without the noise—this is always our primary goal.\n\nSecurity and development teams need accurate and actionable alerts without the noise—this is always our primary goal. Including diverse test cases: We continue to incorporate examples based on learnings from customer feedback into our test bed as we refine our detection capabilities.\n\nWe continue to incorporate examples based on learnings from customer feedback into our test bed as we refine our detection capabilities. Effective resource management: We always need to balance scalability with performance.\n\nWe always need to balance scalability with performance. Collaborative innovation: Partnering with other GitHub and Microsoft teams helps us push the boundaries of what Copilot can achieve.\n\nThese learnings are also shared across Copilot Autofix, which continues to expand coverage for code scanning alerts and helps development teams remediate code scanning alerts quickly.\n\nSince our general availability launch, enablement for Copilot secret scanning has been included in security configurations, allowing you to control which repositories are detecting secrets across your organizations or enterprise. We’re dedicated to continuous improvement through ongoing monitoring, mirror testing, and approach refinement based on customer feedback and detection trends. Copilot secret scanning serves as a critical component for robust application security and will evolve to meet the dynamic needs of our users.\n\nCopilot secret scanning is a feature of GitHub Secret Protection, which offers enterprise-ready solutions for preventing accidental secret exposure in your repositories. GitHub Secret Protection is available to purchase starting April 1, 2025.", "label": "non_personal"}
{"title": "Introducing sub-issues: Enhancing issue management on GitHub", "url": "https://github.blog/engineering/architecture-optimization/introducing-sub-issues-enhancing-issue-management-on-github/", "content": "Recently we launched sub-issues, a feature designed to tackle complex issue management scenarios. This blog post delves into the journey of building sub-issues, what we learned along the way, how we implemented sub-issues, and the benefits of being able to use sub-issues to build itself.\n\nWhat are sub-issues?\n\nSub-issues are a way to break a larger issue into smaller, more manageable tasks. With this feature, you can now create hierarchical lists within a single issue, making it easier to track progress and dependencies. By providing a clear structure, sub-issues help teams stay organized and focused on their goals.\n\nFor example, I often realize that a batch of work requires multiple steps, like implementing code in different repositories. Breaking this task into discrete sub-issues makes it easier to track progress and more clearly define the work I need to do. In practice we’ve noticed this helps keep linked PRs more concise and easier to review.\n\nA brief history\n\nIssues have long been at the heart of project management on GitHub. From tracking bugs to planning feature development, issues provide a flexible and collaborative way for teams to organize their work. Over time, we’ve enriched this foundation with tools like labels, milestones, and task lists, all to make project management even more intuitive and powerful.\n\nOne of the key challenges we set out to solve was how to better represent and manage hierarchical tasks within issues. As projects grow in complexity, breaking down work into smaller, actionable steps becomes essential. We want to empower users to seamlessly manage these nested relationships while maintaining the simplicity and clarity GitHub is known for.\n\nOur journey toward sub-issues began with a fundamental goal: to create a system that integrates deeply into the GitHub Issues experience, enabling users to visually and functionally organize their work without adding unnecessary complexity. Achieving this required careful design and technical innovation.\n\nBuilding sub-issues\n\nTo build sub-issues, we began by designing a new hierarchical structure for tasks rather than modifying the existing task list functionality. We introduced the ability to nest tasks within tasks, creating a hierarchical structure. This required updates to our data models and rendering logic to support nested sub-issues.\n\nFrom a data modeling perspective, the sub-issues table stores the relationships between parent and child issues. For example, if Issue X is a parent of Issue Y, the sub-issues table would store this link, ensuring the hierarchical relationship is maintained.\n\nIn addition, we roll up sub-issue completion information into a sub-issue list table. This allows us to performantly get progress without having to traverse through a list of sub-issues. For instance, when Issue Y is completed, the system automatically updates the progress of Issue X, eliminating the need to manually check the status of all sub-issues.\n\nWe wanted a straightforward representation of sub-issues as relationships in MySQL. This approach provided several benefits, including easier support for sub-issues in environments like GitHub Enterprise Server and GitHub Enterprise Cloud with data residency.\n\nWe exposed sub-issues through GraphQL endpoints, which let us build upon the new Issues experience and leverage newly crafted list-view components. This approach provided some benefits, including more efficient data fetching and enhanced flexibility in how issue data is queried and displayed. Overall, we could move faster because we reused existing components and leveraged new components that would be used in multiple features. This was all made possible by building sub-issues in the React ecosystem.\n\nWe also focused on providing intuitive controls for creating, editing, and managing sub-issues. To this end, we worked closely with accessibility designers and GitHub’s shared components team that built the list view that powers sub-issues.\n\nOur goal was to make it as easy as possible for users to break down their tasks without disrupting their workflow.\n\nUsing sub-issues in practice\n\nDogfooding is a best practice at GitHub and it’s how we build GitHub! We used sub-issues extensively within our own teams throughout the company to manage complex projects and track progress. Having a discrete area to manage our issue hierarchy resulted in a simpler, more performant experience. Through this hands-on experience, we identified areas for improvement and ensured that the feature met our high standards.\n\nOur teams found that sub-Issues significantly improved their ability to manage large projects. By breaking down tasks into smaller, actionable items, they maintained better visibility and control over their work. The hierarchical structure also made it easier to identify dependencies and ensure nothing fell through the cracks.\n\nGathering early feedback\n\nBuilding sub-issues was a team effort. Feedback from our beta testers was instrumental in shaping the final product and ensuring it met the needs of our community. For example, understanding how much metadata to display in the sub-issue list was crucial. We initially started with only issue titles, but eventually added the issue number and repository name, if the issue was from another repository.\n\nBuilding features at GitHub makes it really easy to improve our own features as we go. It was really cool to start breaking down the sub-issues work using sub-issues. This allowed us to experience the feature firsthand and identify any pain points or areas for improvement. For example, the has:sub-issues-progress and has:parent-issue filters evolved from early discussions around filtering syntax. This hands-on approach ensured that we delivered a polished and user-friendly product.\n\nThese lessons have been invaluable in not only improving sub-issues, but also in shaping our approach to future feature development. By involving users early and actively using our own features, we can continue to build products that truly meet the needs of our community. These practices will be important to our development process going forward, ensuring that we deliver high-quality, user-centric solutions.\n\nCall to action\n\nSub-issues are designed to help you break down complex tasks into manageable pieces, providing clarity and structure to your workflows. Whether you’re tracking dependencies, managing progress, or organizing cross-repository work, sub-issues offer a powerful way to stay on top of your projects.\n\nWe’d love for you to try sub-issues and see how they can improve your workflow. Your feedback is invaluable in helping us refine and enhance this feature. Join the conversation in our community discussion to share your thoughts, experiences, and suggestions.\n\nThank you for being an integral part of the GitHub community. Together, we’re shaping the future of collaborative development!\n\nTags:", "label": "non_personal"}
{"title": "How the GitHub CLI can now enable triangular workflows", "url": "https://github.blog/open-source/git/how-the-github-cli-can-now-enable-triangular-workflows/", "content": "Most developers are familiar with the standard Git workflow. You create a branch, make changes, and push those changes back to the same branch on the main repository. Git calls this a centralized workflow. It’s straightforward and works well for many projects.\n\nHowever, sometimes you might want to pull changes from a different branch directly into your feature branch to help you keep your branch updated without constantly needing to merge or rebase. However, you’ll still want to push local changes to your own branch. This is where triangular workflows come in.\n\nIt’s possible that some of you have already used triangular workflows, even without knowing it. When you fork a repo, contribute to your fork, then open a pull request back to the original repo, you’re working in a triangular workflow. While this can work seamlessly on github.com, the process hasn’t always been seamless with the GitHub CLI.\n\nThe GitHub CLI team has recently made improvements (released in v2.71.2) to better support these triangular workflows, ensuring that the gh pr commands work smoothly with your Git configurations. So, whether you’re working on a centralized workflow or a more complex triangular one, the GitHub CLI will be better equipped to handle your needs.\n\nIf you’re already familiar with how Git handles triangular workflows, feel free to skip ahead to learn about how to use gh pr commands with triangular workflows. Otherwise, let’s get into the details of how Git and the GitHub CLI have historically differed, and how four-and-a-half years after it was first requested, we have finally unlocked managing pull requests using triangular workflows in the GitHub CLI.\n\nFirst, a lesson in Git fundamentals\n\nTo provide a framework for what we set out to do, it’s important to first understand some Git basics. Git, at its core, is a way to store and catalog changes on a repository and communicate those changes between copies of that repository. This workflow typically looks like the diagram below:\n\nFigure 1: A typical git branch setup\n\nThe building blocks of this diagram illustrate two important Git concepts you likely use every day, a ref and push/pull.\n\nRefs\n\nA ref is a reference to a repository and branch. It has two parts: the remote, usually a name like origin or upstream, and the branch. If the remote is the local repository, it is blank. So, in the example above, origin/branch in the purple box is a remote ref, referring to a branch named branch on the repository name origin, while branch in the green box is a local ref, referring to a branch named branch on the local machine.\n\nWhile working with GitHub, the remote ref is usually the repository you are hosting on GitHub. In the diagram above, you can consider the purple box GitHub and the green box your local machine.\n\nPushing and pulling\n\nA push and a pull refer to the same action, but from two different perspectives. Whether you are pushing or pulling is determined by whether you are sending or receiving the changes. I can push a commit to your repo, or you can pull that commit from my repo, and the references to that action would be the same.\n\nTo disambiguate this, we will refer to different refs as the headRef or baseRef, where the headRef is sending the changes (pushing them) and the baseRef is receiving the changes (pulling them).\n\nFigure 2: Disambiguating headRef and baseRef for push/pull operations\n\nWhen dealing with a branch, we’ll often refer to the headRef of its pull operations as its pullRef and the baseRef of its push operations as its pushRef. That’s because, in these instances, the working branch is the pull’s baseRef and the push’s headRef, so they’re already disambiguated.\n\nThe @{push} revision syntax\n\nTurns out, Git has a handy built-in tool for referring to the pushRef for a branch: the @{push} revision syntax. You can usually determine a branch’s pushRef by running the following command:\n\ngit rev-parse --abbrev-ref @{push}\n\nThis will result in a human-readable ref, like origin/branch, if one can be determined.\n\nPull Requests\n\nOn GitHub, a pull request is a proposal to integrate changes from one ref to another. In particular, they act as a simple “pause” before performing the actual integration operation, often called a merge, when changes are being pushed from ref to another. This pause allows for humans (code reviews) and robots (GitHub Copilot reviews and GitHub Actions workflows) to check the code before the changes are integrated. The name pull request came from this language specifically: You are requesting that a ref pulls your changes into itself.\n\nFigure 3: Demonstrating how GitHub Pull Requests correspond to pushing and pulling\n\nCommon Git workflows\n\nNow that you understand the basics, let’s talk about the workflows we typically use with Git every day.\n\nA centralized workflow is how most folks interact with Git and GitHub. In this configuration, any given branch is pushing and pulling from a remote ref with the same branch name. For most of us, this type of configuration is set up by default when we clone a repo and push a branch. It is the situation shown in Figure 1.\n\nIn contrast, a triangular workflow pushes to and pulls from different refs. A common use case for this configuration is to pull directly from a remote repository’s default branch into your local feature branch, eliminating the need to run commands like git rebase <default> or git merge <default> on your feature branch to ensure the branch you’re working on is always up to date with the default branch. However, when pushing changes, this configuration will typically push to a remote ref with the same branch name as the feature branch.\n\nFigure 4: juxtaposing centralized workflows from triangular workflows.\n\nWe complete the triangle when considering pull requests: the headRef is the pushRef for the local ref and the baseRef is the pullRef for the local branch:\n\nFigure 5: a triangular workflow\n\nWe can go one step further and set up triangular workflows using different remotes as well. This most commonly occurs when you’re developing on a fork. In this situation, you usually give the fork and source remotes different names. I’ll use origin for the fork and upstream for the source, as these are common names used in these setups. This functions exactly the same as the triangular workflows above, but the remotes and branches on the pushRef and pullRef are different:\n\nFigure 6: juxtaposing triangular workflows and centralized workflows with different remotes such as with forks\n\nUsing a Git configuration file for triangular workflows\n\nThere are two primary ways that you can set up a triangular workflow using the Git configuration – typically defined in a `.git/config` or `.gitconfig` file. Before explaining these, let’s take a look at what the relevant bits of a typical configuration look like in a repo’s `.git/config` file for a centralized workflow:\n\n[remote “origin”] url = https://github.com/OWNER/REPO.git fetch = +refs/heads/*:refs/remotes/origin/* [branch “default”] remote = origin merge = refs/heads/default [branch “branch”] remote = origin merge = refs/heads/branch\n\nFigure 7: A typical Git configuration setup found in .git/config\n\nThe [remote “origin”] part is naming the Git repository located at github.com/OWNER/REPO.git to origin, so we can reference it elsewhere by that name. We can see that reference being used in the specific [branch] configurations for both the default and branch branches in their remote keys. This key, in conjunction with the branch name, typically makes up the branch’s pushRef: in this example, it is origin/branch.\n\nThe remote and merge keys are combined to make up the branch’s pullRef: in this example, it is origin/branch.\n\nSetting up a triangular branch workflow\n\nThe simplest way to assemble a triangular workflow is to set the branch’s merge key to a different branch name, like so:\n\n[branch “branch”] remote = origin merge = refs/heads/default\n\nFigure 8: a triangular branch’s Git configuration found in .git/config\n\nThis will result in the branch pullRef as origin/default, but pushRef as origin/branch, as shown in Figure 9.\n\nFigure 9: A triangular branch workflow\n\nSetting up a triangular fork workflow\n\nWorking with triangular forks requires a bit more customization than triangular branches because we are dealing with multiple remotes. Thus, our remotes in the Git config will look different than the one shown previously in Figure 7:\n\n[remote “upstream”] url = https://github.com/ORIGINALOWNER/REPO.git fetch = +refs/heads/*:refs/remotes/upstream/* [remote “origin”] url = https://github.com/FORKOWNER/REPO.git fetch = +refs/heads/*:refs/remotes/origin/*\n\nFigure 10: a Git configuration for a multi-remote Git setup found in .git/config\n\nUpstream and origin are the most common names used in this construction, so I’ve used them here, but they can be named anything you want.\n\nHowever, toggling a branch’s remote key between upstream and origin won’t actually set up a triangular fork workflow—it will just set up a centralized workflow with either of those remotes, like the centralized workflow shown in Figure 6. Luckily, there are two common Git configuration options to change this behavior.\n\nSetting a branch’s pushremote\n\nA branch’s configuration has a key called pushremote that does exactly what the name suggests: configures the remote that the branch will push to. A triangular fork workflow config using pushremote may look like this:\n\n[branch “branch”] remote = upstream merge = refs/heads/default pushremote = origin\n\nFigure 11: a triangular fork’s Git config using pushremote found in .git/config\n\nThis assembles the triangular fork repo we see in Figure 12. The pullRef is upstream/default, as determined by combining the remote and merge keys, while the pushRef is origin/branch, as determined by combining the pushremote key and the branch name.\n\nFigure 12: A triangular fork workflow\n\nSetting a repo’s remote.pushDefault\n\nTo configure all branches in a repository to have the same behavior as what you’re seeing in Figure 12, you can instead set the repository’s pushDefault . The config for this is below:\n\n[remote] pushDefault = origin [branch “branch”] remote = upstream merge = refs/heads/default\n\nFigure 13: a triangular fork’s Git config using remote.pushDefault found in .git/config\n\nThis assembles the same triangular fork repo as shown in Figure 12 above, however this time the pushRef is determined by combining the remote.pushDefault key and the branch name, resulting in origin/branch.\n\nWhen using the branch’s pushremote and the repo’s remote.pushDefault keys together, Git will preferentially resolve the branch’s configuration over the repo’s, so the remote set on pushremote supersedes the remote set on remote.pushDefault .\n\nUpdating the gh pr command set to reflect Git\n\nPreviously, the gh pr command set did not resolve pushRefs and pullRefs in the same way that Git does. This was due to technical design decisions that made this change both difficult and complex. Instead of discussing that complexity—a big enough topic for a whole article in itself—I’m going to focus here on what you can now do with the updated gh pr command set.\n\nIf you set up triangular Git workflows in the manner described above, we will automatically resolve gh pr commands in accordance with your Git configuration.\n\nTo be slightly more specific, when trying to resolve a pull request for a branch, the GitHub CLI will respect whatever @{push} resolves to first, if it resolves at all. Then it will fall back to respect a branch’s pushremote, and if that isn’t set, finally look for a repo’s remote.pushDefault config settings.\n\nWhat this means is that the CLI is assuming your branch’s pullRef is the pull request’s baseRef and the branch’s pushRef is the pull requests headRef. In other words, if you’ve configured git pull and git push to work, then gh pr commands should just work. The diagram below, a general version of Figure 5, demonstrates this nicely:\n\nFigure 14: the triangular workflow supported by the GitHub CLI with respect to a branch’s pullRef and pushRef. This is the generalized version of Figure 5\n\nConclusion\n\nWe’re constantly working to improve the GitHub CLI, and we’d like the behavior of the GitHub CLI to reasonably reflect the behavior of Git. This was a team effort—everyone contributed to understanding, reviewing, and testing the code to enable this enhanced gh pr command set functionality.\n\nIt also couldn’t have happened without the support of our contributors, so we extend our thanks to them:\n\nCLI native support for triangular workflows was 4.5 years in the making, and we’re proud to have been able to provide this update for the community.\n\nThe GitHub CLI Team\n\n@andyfeller , @babakks , @bagtoad , @jtmcg , @mxie , @RyanHecht , and @williammartin\n\nTags:", "label": "non_personal"}
{"title": "Design system annotations, part 2: Advanced methods of annotating components", "url": "https://github.blog/engineering/user-experience/design-system-annotations-part-2-advanced-methods-of-annotating-components/", "content": "In part one of our design system annotation series, we discussed the ways in which accessibility can get left out of design system components from one instance to another. Our solution? Using a set of “Preset annotations” for each component with Primer. This allows designers to include specific pre-set details that aren’t already built into the component and visually communicated in the design itself.\n\nThat being said, Preset annotations are unique to each design system — and while ours may be a helpful reference for how to build them — they’re not something other organizations can utilize if you’re not also using the Primer design system.\n\nLuckily, you can build your own. Here’s how.\n\nHow to make Preset annotations for your design system\n\nStart by assessing components to understand which ones would need Preset annotations—not all of them will. Prioritize components that would benefit most from having a Preset annotation, and build that key information into each one. Next, determine what properties should be included. Only include key information that isn’t conveyed visually, isn’t in the component properties, and isn’t already baked into a coded component.\n\nPrioritizing components\n\nWhen a design system has 60+ components, knowing where to start can be a challenge. Which components need these annotations the most? Which ones would have the highest impact for both design teams and our users?\n\nWhen we set out to create a new set of Preset annotations based on our proof of concept, we decided to use ten Primer components that would benefit the most. To help pick them, we used an internal tool called Primer Query that tracks all component implementations across the GitHub codebase as well as any audit issues connected to them. Here is a video breakdown of how it works, if you’re curious.\n\nWe then prioritized new Preset annotations based on the following criteria:\n\nComponents that align to organization priorities (i.e. high value products and/or those that receive a lot of traffic). Components that appear frequently in accessibility audit issues. Components with React implementations (as our preferred development framework). Most frequently implemented components.\n\nMapping out the properties\n\nFor each component, we cross-referenced multiple sources to figure out what component properties and attributes would need to be added in each Preset annotation. The things we were looking for may only exist in one or two of those places, and thus are less likely to be accounted for all the way through the design and development lifecycle. The sources include:\n\nComponent documentation on Primer.style\n\nDesign system docs should contain usage guidance for designers and developers, and accessibility requirements should be a part of this guidance as well. Some of the guidance and requirements get built into the component’s Figma asset, while some only end up in the coded component.\n\nLook for any accessibility requirements that are not built into either Figma or code. If it’s built in, putting the same info in the Preset annotation may be redundant or irrelevant.\n\nPresets can account for rare use cases While building a Preset annotation for the TextInput component, we found that implementations may use an icon alone or have a hidden input label. With GitHub’s global search or filter inputs, the magnifying glass icon alone can act as the visible label, but the fields still need an accessible label for assistive technology users.\n\nCoded demos in Storybook\n\nOur component sandbox helped us see how each component is built in React or Rails, as well as what the HTML output is. We looked for any code structure or accessibility attributes that are not included in the component documentation or the Figma asset itself—especially when they may vary from one implementation to another.\n\nCode attributes a designer may not see or set Storybook helped us craft our TextInput component’s Preset annotation by showing some important attributes that don’t get any mention elsewhere. The type attribute is to the value of text . by default. Depending on the purpose of the field, an input’s type could also be search , email , number , tel , date , or time . This should be set intentionally so that users are able to use the most appropriate virtual keyboard.\n\nComponent properties in the Figma asset library\n\nLibrary assets provide a lot of flexibility through text layers, image fills, variants, and elaborate sets of component properties. We paid close attention to these options to understand what designers can and can’t change. Worthwhile additions to a Preset Annotation are accessibility attributes, requirements, and usage guidance in other sources that aren’t built into the Figma component.\n\nWhat’s missing from the TextInput’s Figma component When a TextInput is added to a design, the Figma component comes with many customizable options. There is an inputTextType property, which is about visual design and typography, not the type of form input. It’s possible to set the value of the Label and input field in Figma’s sidebar, but because it’s hidden by default, there’s no option to set the text of an error validation message. We can’t assume that every design delivered in Figma will come with examples of a form showing all of its error states, so these error messages may not get the attention they require. If this message can’t be built into the component as a text property, it can be added to the Preset annotation.\n\nOther potential sources\n\nExperiences from team members: The designers, developers, and accessibility specialists you work with may have insight into things that the docs and design tools may have missed. If your team and design system have been around for a while, their insights may be more valuable than those you’ll find in the docs, component demos, or asset libraries. Take some time to ask which components have had challenging bugs and which get intentionally broken when implemented.\n\nThe designers, developers, and accessibility specialists you work with may have insight into things that the docs and design tools may have missed. If your team and design system have been around for a while, their insights may be more valuable than those you’ll find in the docs, component demos, or asset libraries. Take some time to ask which components have had challenging bugs and which get intentionally broken when implemented. Findings from recent audits: Design system components themselves may have unresolved audit issues and remediation recommendations. If that’s the case, those issues are likely present in Storybook demos and may be unaccounted for in the component documentation. Design system audit issues may have details that both help create a Preset annotation and offer insights about what should not be carried over from existing resources.\n\nPutting it all together Our new Preset annotation for the TextInput component included links to usage guidance and Storybook as well as an optional tutorial for how the component is best used in a design to avoid potential issues. There are two mandatory prompts for input type and error text, and an optional one for the occasional hidden form label.\n\nWhat we learned from creating Preset annotations\n\nPreset annotations may not be for every team or organization. However, they are especially well suited for younger design systems and those that aren’t well adopted.\n\nMature design systems like Primer have frequent updates. This means that without close monitoring, the design system components themselves may fall out of sync with how a Preset annotation is built. This can end up causing confusion and rework after development starts, so it may be wise to make sure there’s some capacity to maintain these annotations after they’ve been created.\n\nFor newer teams at GitHub, new members of existing teams, and team members who were less familiar with the design system, the built-in guidance and links to documentation and component demos proved very useful. Those who are more experienced are also able to fine-tune the Presets and how they’re used.\n\nIf you don’t already have extensive experience with the design system components (or peers to help build them), it can take a lot of time to assess and map out the properties needed to build a Preset. It can also be challenging to name a component property succinctly enough that it doesn’t get truncated in Figma’s properties panel. If the context is not self-evident, some training or additional documentation may help.\n\nIt’s not always clear that you need a Preset annotation\n\nThere may be enough overlap between the Preset annotation for a component and types of annotations that aren’t specific to the design system.\n\nFor example, the GitHub Annotation Toolkit has components to annotate basic <textarea> form elements in addition to a Preset annotation for our <TextArea> Primer component:\n\nIn many instances, this flexibility may be confusing because you could use either annotation. For example, the Primer <TextArea> Preset has built-in links to specific Primer docs, and while the non-Preset version doesn’t, you could always add the links manually. While there’s some overlap between the two, using either one is better than none.\n\nOne way around this confusion is to add Primer-specific properties to the default set of annotations. This would allow you to do things like toggle a boolean property on a normal Button annotation and have it show links and properties specific to your design system’s button component.\n\nOur Preset creation process may unlock automation\n\nThere are currently a number of existing Figma plugins that advertise the ability to scan a design file to help with annotations. That being said, the results are often mixed and contain an unmanageable amount of noise and false positives. One of the reasons these issues happen is that these public plugins are design system agnostic.\n\nCurrent automated annotation tools aren’t able to understand that any design system components are being used without bespoke programming or thorough training of AI models. For plugins like this to be able to label design elements accurately, they first need to understand how to identify the components on the canvas, the variants used, and the set properties.\n\nWith that in mind, perhaps the most exciting insight is that the process of mapping out component properties for a Preset annotation—the things that don’t get conveyed in the visual design or in the code—is also something that would need to be done in any attempt to automate more usable annotations.\n\nIn other words, if a team uses a design system and wants to automate adding annotations, the tool they use would need to understand their components. In order for it to understand their components well enough to automate accurately, these hidden component properties would need to be mapped out. The task of creating a set of Preset annotations may be a vital stepping stone to something even more streamlined.\n\nA promising new method: Figma’s Code Connect\n\nWhile building our new set of Preset annotations, we experimented with other ways to enhance Primer with annotations. Though not all of those experiments worked out, one of them did: adding accessibility attributes through Code Connect.\n\nPrimer was one of the early adopters of Figma’s new Code Connect feature in Dev Mode. Says Lukas Oppermann, our staff systems designer, “With Code Connect, we can actually move the design and the code a little bit further apart again. We can concentrate on creating the best UX for the designers working in Figma with design libraries and, on the code side, we can have the best developer experience.”\n\nTo that end, Code Connect allows us to bypass much of our Preset annotations, as well as the downsides of some of our other experiments. It does this by adding key accessibility details directly into the code that developers can export from Figma.\n\nGitHub’s Octicons are used in many of our Primer components. They are decorative by default, but they sometimes need alt text or aria-label attributes depending on how they’re used. In the IconButton component, that button uses an Octicon and needs an accessible name to describe its function.\n\nWhen using a basic annotation kit, this may mean adding stamps for a Button and Decorative Image as well as a note in the margins that specifies what the aria-label should be. When using Preset annotations, there are fewer things to add to the canvas and the annotation process takes less time.\n\nWith Code Connect set up, Lukas added a hidden layer in the IconButton Figma component. It has a text property for aria-label which lets designers add the value directly from the component properties panel. No annotations needed. The hidden layer doesn’t disrupt any of the visuals, and the aria-label property gets exported directly with the rest of the component’s code.\n\nIt takes time to set up Code Connect with each of your design system components. Here are a few tips to help:\n\nConsistency is key. Make sure that the properties you create and how you place hidden layers is consistent across components. This helps set clear expectations so your teams can understand how these hidden layers and properties function.\n\nMake sure that the properties you create and how you place hidden layers is consistent across components. This helps set clear expectations so your teams can understand how these hidden layers and properties function. Use a branch of your design system library to experiment. Hiding attributes like aria-label is quite simple compared to other complex information that Preset annotations are capable of handling.\n\nHiding attributes like aria-label is quite simple compared to other complex information that Preset annotations are capable of handling. Use visual regression testing (VRT). Adding complexity directly to a component comes with increased risk of things breaking in the future, especially for those with many variants. Figma’s merge conflict UI is helpful, but may not catch everything.\n\nAs we continue to innovate with annotations and make our components more accessible, we are aiming to release our GitHub Annotation Toolkit in the near future. Stay tuned!\n\nFurther reading\n\nAccessibility annotation kits are a great resource, provided they’re used responsibly. Eric Bailey, one of the contributors to our forthcoming GitHub Annotation Toolkit, has written extensively about how annotations can highlight and amplify deeply structural issues when you’re building digital products.\n\nTags:", "label": "non_personal"}
{"title": "Building a more accessible GitHub CLI", "url": "https://github.blog/engineering/user-experience/building-a-more-accessible-github-cli/", "content": "At GitHub, we’re committed to making our tools truly accessible for every developer, regardless of ability or toolset. The command line interface (CLI) is a vital part of the developer experience, and the GitHub CLI is our product that brings the power of GitHub to your terminal.\n\nWhen it comes to accessibility, the terminal is fundamentally different from a web browser or a graphical user interface, with a lineage that predates the web itself. While standards like the Web Content Accessibility Guidelines (WCAG) provide a clear path for making web and graphical applications accessible, there is no equivalent, comprehensive standard for the terminal and CLIs. The W3C offers some high-level guidance for non-web software, but it stops short of prescribing concrete techniques, leaving much open to interpretation and innovation.\n\nThis gap has challenged us to think creatively and purposefully about what accessibility should look like in the terminal. Our recent Public Preview is focused on addressing the needs of three key groups: users who rely on screen readers, users who need high contrast between background and text, and users who require customizable color options. Our work aims to make the GitHub CLI more inclusive for all, regardless of how you interact with your terminal. Run gh a11y in the latest version of the GitHub CLI to enable these features, or read on to learn about our path to designing and implementing them.\n\nUnderstanding the terminal landscape\n\nText-based and command-line applications differ fundamentally from graphical or web applications. On a web page, assistive technologies like screen readers make use of the document object model (DOM) to infer structure and context of the page. Web pages can be designed such that the DOM’s structure is friendly to these technologies without impacting the visual design of the page. By contrast, CLI’s primary output is plain text, without hidden markup. A terminal emulator acts as the “user agent” for text apps, rendering characters as directed by the server application. Assistive technologies access this matrix of characters, analyze its layout, and try to infer structure. As the WCAG2ICT guidance notes, accessibility in this space means ensuring that all text output is available to assistive technologies, and that structural information is conveyed in a way that’s programmatically determinable—even if no explicit markup is present.\n\nIn our quest to improve the GitHub CLI’s usability for blind, low-vision, and colorblind users, we found ourselves navigating a landscape with lots of guidance, but few concrete techniques for implementing accessible experiences. We studied how assistive technology interacts with terminals: how screen readers review output, how color and contrast can be customized, and how structural cues can be inferred from plain text. Our recent Public Preview contains explorations into various use cases in these spaces.\n\nRethinking prompts and progress for screen readers\n\nOne of the GitHub CLI’s strengths as a command-line application is its rich prompting experience, which gives our users an interactive interface to enter command options. However, this rich interactive experience poses a hurdle for speech synthesis screen readers: Non-alphanumeric visual cues and uses of constant screen redraws for visual or other effects can be tricky to correctly interpret as speech.\n\nA demo video with sound of screen reader reading legacy prompter.\n\nTo reduce confusion and make it easier for blind and low vision users to confidently answer questions and navigate choices, we’re introducing a prompting experience that allows speech synthesis screen readers to accurately convey prompts to users. Our new prompter is built using Charm’s open source charmbracelet/huh prompting library.\n\n\n\nA demo of a screenreader correctly reading a prompt.\n\nAnother use case where the terminal is redrawn for visual effect is when showing progress bars. Our existing implementation uses a “spinner” made by redrawing the screen to display different braille characters (yes, we appreciate the irony) to give the user the indication that their command is executing. Speech synthesis screen readers do not handle this well:\n\n\n\nA demo of a screenreader and an old spinner.\n\nThis has been replaced with a static text progress indicator (with a relevant message to the action being taken where possible, falling back to a general “Working…” message). We’re working on identifying other areas we can further improve the contextual text.\n\nA demo video of the new progress indicator experience.\n\nColor, contrast, and customization\n\nColor is more than decoration in the terminal: It’s a vital tool for highlighting information, signaling errors, and guiding workflows. But color can also be a barrier—if contrast between the color of the terminal background and the text displayed on it is too low, some users will have difficulty discerning the displayed information. Unlike in a web browser, a terminal’s background color is not set by the application. That task is handled by the user’s terminal emulator. In order to maintain contrast, it is important that a command line application takes into account this variable. Our legacy color palette used for rendering Markdown did not take the terminal’s background color into account, leading to low contrast in some cases.\n\nThe colors themselves also matter. Different terminal environments have varied color capabilities (some support 4-bit, some 8-bit, some 24-bit, etc). No matter the capability, terminals enable users to customize their color preferences, choosing how different hues are displayed. However, most terminals only support changing a limited subset of colors: namely, the sixteen colors in the ANSI 4-bit color table. The GitHub CLI has made extensive efforts to align our color palettes to 4-bit colors so our users can completely customize their experience using their terminal preferences. We built on top of the accessibility foundations pioneered by Primer when deciding which 4-bit colors to use.\n\nBuilding for the CLI community\n\nOur improvements aim to support a wide range of developer needs, from blind users who need screen readers, to low vision users who need high contrast, to colorblind users who require customizable color options. But this Public Preview does not mark the end of our team’s commitment to enabling all developers to use the GitHub CLI. We intend to make it easier for our extension authors to implement the same accessibility improvements that we’ve made to the core CLI. This will allow users to have a cohesive experience across all GitHub CLI commands, official or community-maintained, and so that more workflows can be made accessible by default. We’re also looking into experiences to customize the formatting of tables output by commands to be more easily read/interpreted by screen readers. We’re excited to continue our accessibility journey.\n\n\n\nWe couldn’t have come this far without collaboration with our friends at Charm and our colleagues on the GitHub Accessibility team.\n\nA call for feedback\n\nWe invite you to help us in our goal to make the GitHub CLI an experience for all developers:\n\nTry it out : Update the GitHub CLI to v2.72.0 and run gh a11y in your terminal to learn more about enabling these new accessible features.\n\n: Update the GitHub CLI to v2.72.0 and run in your terminal to learn more about enabling these new accessible features. Share your experience : Join our GitHub CLI accessibility discussion to provide feedback or suggestions.\n\n: Join our GitHub CLI accessibility discussion to provide feedback or suggestions. Connect with us: If you have a lived experience relevant to our accessibility personas, reach out to the accessibility team or get involved in our discussion panel.\n\nLooking forward\n\nAdapting accessibility standards for the command line is a challenge—and an opportunity. We’re committed to sharing our approach, learning from the community, and helping set a new standard for accessible CLI tools.\n\nThank you for building a more accessible GitHub with us.\n\nWant to help us make GitHub the home for all developers? Learn more about GitHub’s accessibility efforts.", "label": "non_personal"}
{"title": "Design system annotations, part 1: How accessibility gets left out of components", "url": "https://github.blog/engineering/user-experience/design-system-annotations-part-1-how-accessibility-gets-left-out-of-components/", "content": "When it comes to design systems, every organization tends to be at a different place in their accessibility journey. Some have put a great deal of work into making their design system accessible while others have a long way to go before getting there. To help on this journey, many organizations rely on accessibility annotations to make sure there are no access barriers when a design is ready to be built.\n\nHowever, it’s a common misconception (especially for organizations with mature design systems) that accessible components will result in accessible designs. While design systems are fantastic for scaling standards and consistency, they can’t prevent every issue with our designs or how we build them. Access barriers can still slip through the cracks and make it into production.\n\nThis is the root of the problem our Accessibility Design team set out to solve.\n\nIn this two-part series, we’ll show you exactly how accessible design system components can produce inaccessible designs. Then we’ll demonstrate our solution: integrating annotations with our Primer components. This allows us to spend less time annotating, increases design system adoption, and reaches teams who may not have accessibility support. And in our next post, we’ll walk you through how you can do the same for your own components.\n\nLet’s dig in.\n\nWhat are annotations and their benefits?\n\nAnnotations are notes included in design projects that help make the unseen explicit by conveying design intent that isn’t shown visually. They improve the usability of digital experiences by providing a holistic picture for developers of how an experience should function. Integrating annotations into our design process helps our teams work better together by closing communication gaps and preventing quality issues, accessibility audit issues, and expensive re-work.\n\nSome of the questions annotations help us answer include:\n\nHow is assistive technology meant to navigate a page from one element to another?\n\nWhat’s the alternative text for informative images and buttons without labels?\n\nHow does content shift depending on viewport size, screen orientation, or zoom level?\n\nWhich virtual keyboard should be used for a form input on mobile?\n\nHow should focus be managed for complex interactions?\n\nOur answers to questions like this—or the lack thereof—can make or break the experience of the web for a lot of people, especially users with disabilities. Some annotation tools are built specifically to help with this by guiding designers to include key details about web standards, platform functionality, and accessibility (a11y).\n\nMost public annotation kits are well suited for teams who are creating new design system components, teams who aren’t already using a design system, or teams who don’t have specialized accessibility knowledge. They usually help annotate things like:\n\nControls such as buttons and links\n\nStructural elements such as headings and landmarks\n\nDecorative images and informative descriptions\n\nForms and other elements that require labels and semantic roles\n\nFocus order for assistive technology and keyboard navigation\n\nGitHub’s annotation’s toolkit\n\nOne of our top priorities is to meet our colleagues where they’re at. We wanted all our designers to be able to use annotations out of the box because we believe they shouldn’t need to be a certified accessibility specialist in order to get things built in an accessible way.\n\nTo this end, last year we began creating an internal Figma library—the GitHub Annotation Toolkit (which we aim to release to the public soon). Our toolkit builds on the legacy of the former Inclusive Design team at CVS Health. Their two open source annotation kits help make documentation that’s easy to create and consume, and are among the most widely used annotation libraries in the Figma Community.\n\nWhile they add clarity, annotations can also add overhead. If teams are only relying on specialists to interpret designs and technical specifications for developers, the hand-off process can take longer than it needs to. To create our annotation toolkit, we rebuilt its predecessor from the ground up to avoid that overhead, making extensive improvements and adding inline documentation to make it more intuitive and helpful for all of our designers—not just accessibility specialists.\n\nDesign systems can also help reduce that overhead. When you audit your design systems for accessibility, there’s less need for specialist attention on every product feature, since you’re using annotations to add technical semantics and specialist knowledge into every component. This means that designers and developers only need to adhere to the usage guidelines consistently, right?\n\nThe problems with annotations and design system components\n\nUnfortunately, it’s not that simple.\n\nAccessibility is not binary\n\nWhile design systems can help drive more accessible design at scale, they are constantly evolving and the work on them is never done. The accessibility of any component isn’t binary. Some may have a few severe issues that create access barriers, such as being inoperable with a keyboard or missing alt text. Others may have a few trivial issues, such as generic control labels.\n\nMost of the time, it will be a misnomer to claim that your design system is “fully accessible.” There’s always more work to do—it’s just a question of how much. The Web Content Accessibility Guidelines (WCAG) are a great starting point, but their “Success Criteria” isn’t tailored for the unique context that is your website or product or audience.\n\nWhile the WCAG should be used as a foundation to build from, it’s important to understand that it can’t capture every nuance of disabled users’ needs because your users’ needs are not every user’s needs. It would be very easy to believe that your design system is “fully accessible” if you never look past WCAG to talk to your users. If Primer has accessible components, it’s because we feel that direct participation and input from daily assistive technology users is the most important aspect of our work. Testing plans with real users—with and without disabilities—is where you really find what matters most.\n\nAccessible components do not guarantee accessible designs\n\nArranging a series of accessible components on a page does not automatically create an accurate and informative heading hierarchy. There’s a good chance that without additional documentation, the heading structure won’t make sense visually—nor as a medium for navigating with assistive technology.\n\nIt’s great when accessible components are flexible and responsive, but what about when they’re placed in a layout that the component guidance doesn’t account for? Do they adapt to different zoom levels, viewport sizes, and screen orientations? Do they lose any functionality or context when any of those things change?\n\nComponent usage is contextual. You can add an image or icon to your design, but the design system docs can’t write descriptive text for you. You can use the same image in multiple places, but the image description may need to change depending on context.\n\nSimilarly, forms built using the same input components may do different things and require different error validation messages. It’s no wonder that adopting design system components doesn’t get rid of all audit issues.\n\nDesign system components in Figma don’t include all the details\n\nAnnotation kits don’t include components for specific design systems because almost every organization is using their own. When annotation kits are adopted, teams often add ways to label their design system components.\n\nThis labeling lets developers know they can use something that’s already been built, and that they don’t need to build something from scratch. It also helps identify any design system components that get ‘detached’ in Figma. And it reduces the number of things that need to be annotated.\n\nLet’s look at an example:\n\nIf we’re using this Primer Button component from the Primer Web Figma library, there are a few important things that we won’t know just by looking at the design or the component properties:\n\nFunctional differences when components are implemented. Is this a link that just looks visually like a button? If so, a developer would use the <LinkButton> React component instead of <Button> .\n\nIs this a link that just looks visually like a button? If so, a developer would use the React component instead of . Accessible labels for folks using assistive technology. The icon may need alt text. In some cases, the button text might need some visually-hidden text to differentiate it from similar buttons. How would we know what that text is? Without annotations, the Figma component doesn’t have a place to display this.\n\nThe icon may need alt text. In some cases, the button text might need some visually-hidden text to differentiate it from similar buttons. How would we know what that text is? Without annotations, the Figma component doesn’t have a place to display this. Whether user data is submitted. When a design doesn’t include an obvious form with input fields, how do we convey that the button needs specific attributes to submit data?\n\nIt’s risky to leave questions like this unanswered, hoping someone notices and guesses the correct answer.\n\nA solution that streamlines the annotation process while minimizing risk\n\nWhen creating new components, a set of detailed annotations can be a huge factor in how robust and accessible they are. Once the component is built, design teams can start to add instances of that component in their designs. When those designs are ready to be annotated, those new components shouldn’t need to be annotated again. In most cases, it would be redundant and unnecessary—but not in every case.\n\nThere are some important details in many Primer components that may change from one instance to another. If we use the CVS Health annotation kit out of the box, we should be able to capture those variations, but we wouldn’t be able to avoid those redundant and unnecessary annotations. As we built our own annotation toolkit, we built a set of annotations for each Primer component to do both of those things at once.\n\nThis accordion component has been thoroughly annotated so that an engineer has everything they need to build it the first time. These include heading levels, semantics for <detail> and <summary> elements, landmarks, and decorative icons. All of this is built into the component so we don’t need to annotate most of this when adding the accordion to our new designs.\n\nHowever, there are two important things we need to annotate, as they can change from one instance to another:\n\nThe optional title at the top. The heading level of each item within the accordion.\n\nIf we don’t specify these things, we’re leaving it to chance that the page’s heading structure will break or that the experience will be confusing for people to understand and navigate the page. The risks may be low for a single button or basic accordion, but they grow with pattern complexity, component nesting, interaction states, duplicated instances, and so on.\n\nInstead of annotating what’s already built into the component or leaving these details to chance, we can add two quick annotations. One Stamp to point to the component, and one Details annotation where we fill in some blanks to make the heading levels clear.\n\nBecause the prompts for specific component details are pre-set in the annotation, we call them Preset annotations.\n\nIntroducing our Primer A11y Preset annotations\n\nWith this proof of concept, we selected ten frequently used Primer components for the same treatment and built a new set of Preset annotations to document these easily missed accessibility details—our Primer A11y Presets.\n\nThose Primer components tend to contribute to more accessibility audit issues when key details are missing on implementation. Issues for these components relate to things like lack of proper labels, error validation messages, or missing HTML or ARIA attributes.\n\nEach of our Preset annotations is linked to component docs and Storybook demos. This will hopefully help developers get straight to the technical info they need without designers having to find and add links manually. We also included guidance for how to fill out each Preset, as well as how to use the component in an accessible way. This helps designers get support inline without leaving their Figma canvas.\n\nWant to create your own? Check out Design system annotations, part 2\n\nButton components in Google’s Material Design and Shopify’s Polaris, IBM’s Carbon, or our Primer design system are all very different from one another. Because Preset annotations are based on specific components, they only work if you’re also using the design system they’re made for.\n\nIn part 2 of this series, we’ll walk you through how you can build your own set of Preset annotations for your design system, as well as some different ways to document important accessibility details before development starts.\n\nYou may also like:\n\nIf you’re more of a visual learner, you can watch Alexis Lucio explore Preset annotations during GitHub’s Dev Community Event to kick off Figma’s Config 2024.\n\nTags:", "label": "non_personal"}
{"title": "GitHub Issues search now supports nested queries and boolean operators: Here’s how we (re)built it", "url": "https://github.blog/developer-skills/application-development/github-issues-search-now-supports-nested-queries-and-boolean-operators-heres-how-we-rebuilt-it/", "content": "Originally, Issues search was limited by a simple, flat structure of queries. But with advanced search syntax, you can now construct searches using logical AND/OR operators and nested parentheses, pinpointing the exact set of issues you care about.\n\nBuilding this feature presented significant challenges: ensuring backward compatibility with existing searches, maintaining performance under high query volume, and crafting a user-friendly experience for nested searches. We’re excited to take you behind the scenes to share how we took this long-requested feature from idea to production.\n\nHere’s what you can do with the new syntax and how it works behind the scenes\n\nIssues search now supports building queries with logical AND/OR operators across all fields, with the ability to nest query terms. For example is:issue state:open author:rileybroughten (type:Bug OR type:Epic) finds all issues that are open AND were authored by rileybroughten AND are either of type bug or epic.\n\nHow did we get here?\n\nPreviously, as mentioned, Issues search only supported a flat list of query fields and terms, which were implicitly joined by a logical AND. For example, the query assignee:@me label:support new-project translated to “give me all issues that are assigned to me AND have the label support AND contain the text new-project.”\n\nBut the developer community has been asking for more flexibility in issue search, repeatedly, for nearly a decade now. They wanted to be able to find all issues that had either the label support or the label question , using the query label:support OR label:question . So, we shipped an enhancement towards this request in 2021, when we enabled an OR style search using a comma-separated list of values.\n\nHowever, they still wanted the flexibility to search this way across all issue fields, and not just the labels field. So we got to work.\n\nTechnical architecture and implementation\n\nFrom an architectural perspective, we swapped out the existing search module for Issues (IssuesQuery), with a new search module (ConditionalIssuesQuery), that was capable of handling nested queries while continuing to support existing query formats.\n\nThis involved rewriting IssueQuery, the search module that parsed query strings and mapped them into Elasticsearch queries.\n\nTo build a new search module, we first needed to understand the existing search module, and how a single search query flowed through the system. At a high level, when a user performs a search, there are three stages in its execution:\n\nParse: Breaking the user input string into a structure that is easier to process (like a list or a tree) Query: Transforming the parsed structure into an Elasticsearch query document, and making a query against Elasticsearch. Normalize: Mapping the results obtained from Elasticsearch (JSON) into Ruby objects for easy access and pruning the results to remove records that had since been removed from the database.\n\nEach stage presented its own challenges, which we’ll explore in more detail below. The Normalize step remained unchanged during the re-write, so we won’t dive into that one.\n\nParse stage\n\nThe user input string (the search phrase) is first parsed into an intermediate structure. The search phrase could include:\n\nQuery terms: The relevant words the user is trying to find more information about (ex: “models”)\n\nThe relevant words the user is trying to find more information about (ex: “models”) Search filters: These restrict the set of returned search documents based on some criteria (ex: “assignee:Deborah-Digges”)\n\nExample search phrase:\n\nFind all issues assigned to me that contain the word “codespaces”: is:issue assignee:@me codespaces\n\nFind all issues with the label documentation that are assigned to me: assignee:@me label:documentation\n\n\n\nThe old parsing method: flat list\n\nWhen only flat, simple queries were supported, it was sufficient to parse the user’s search string into a list of search terms and filters, which would then be passed along to the next stage of the search process.\n\nThe new parsing method: abstract syntax tree\n\nAs nested queries may be recursive, parsing the search string into a list was no longer sufficient. We changed this component to parse the user’s search string into an Abstract Syntax Tree (AST) using the parsing library parslet.\n\nWe defined a grammar (a PEG or Parsing Expression Grammar) to represent the structure of a search string. The grammar supports both the existing query syntax and the new nested query syntax, to allow for backward compatibility.\n\nA simplified grammar for a boolean expression described by a PEG grammar for the parslet parser is shown below:\n\nclass Parser < Parslet::Parser rule(:space) { match[\" \"].repeat(1) } rule(:space?) { space.maybe } rule(:lparen) { str(\"(\") >> space? } rule(:rparen) { str(\")\") >> space? } rule(:and_operator) { str(\"and\") >> space? } rule(:or_operator) { str(\"or\") >> space? } rule(:var) { str(\"var\") >> match[\"0-9\"].repeat(1).as(:var) >> space? } # The primary rule deals with parentheses. rule(:primary) { lparen >> or_operation >> rparen | var } # Note that following rules are both right-recursive. rule(:and_operation) { (primary.as(:left) >> and_operator >> and_operation.as(:right)).as(:and) | primary } rule(:or_operation) { (and_operation.as(:left) >> or_operator >> or_operation.as(:right)).as(:or) | and_operation } # We start at the lowest precedence rule. root(:or_operation) end\n\nFor example, this user search string:\n\nis:issue AND (author:deborah-digges OR author:monalisa )\n\nwould be parsed into the following AST:\n\n{ \"root\": { \"and\": { \"left\": { \"filter_term\": { \"attribute\": \"is\", \"value\": [ { \"filter_value\": \"issue\" } ] } }, \"right\": { \"or\": { \"left\": { \"filter_term\": { \"attribute\": \"author\", \"value\": [ { \"filter_value\": \"deborah-digges\" } ] } }, \"right\": { \"filter_term\": { \"attribute\": \"author\", \"value\": [ { \"filter_value\": \"monalisa\" } ] } } } } } } }\n\nQuery\n\nOnce the query is parsed into an intermediate structure, the next steps are to:\n\nTransform this intermediate structure into a query document that Elasticsearch understands Execute the query against Elasticsearch to obtain results\n\nExecuting the query in step 2 remained the same between the old and new systems, so let’s only go over the differences in building the query document below.\n\nThe old query generation: linear mapping of filter terms using filter classes\n\nEach filter term (Ex: label:documentation ) has a class that knows how to convert it into a snippet of an Elasticsearch query document. During query document generation, the correct class for each filter term is invoked to construct the overall query document.\n\nThe new query generation: recursive AST traversal to generate Elasticsearch bool query\n\nWe recursively traversed the AST generated during parsing to build an equivalent Elasticsearch query document. The nested structure and boolean operators map nicely to Elasticsearch’s boolean query with the AND, OR, and NOT operators mapping to the must, should, and should_not clauses.\n\nWe re-used the building blocks for the smaller pieces of query generation to recursively construct a nested query document during the tree traversal.\n\nContinuing from the example in the parsing stage, the AST would be transformed into a query document that looked like this:\n\n{ \"query\": { \"bool\": { \"must\": [ { \"bool\": { \"must\": [ { \"bool\": { \"must\": { \"prefix\": { \"_index\": \"issues\" } } } }, { \"bool\": { \"should\": { \"terms\": { \"author_id\": [ \"<DEBORAH_DIGGES_AUTHOR_ID>\", \"<MONALISA_AUTHOR_ID>\" ] } } } } ] } } ] } // SOME TERMS OMITTED FOR BREVITY } }\n\nWith this new query document, we execute a search against Elasticsearch. This search now supports logical AND/OR operators and parentheses to search for issues in a more fine-grained manner.\n\nConsiderations\n\nIssues is one of the oldest and most heavily -used features on GitHub. Changing core functionality like Issues search, a feature with an average of nearly 2000 queries per second (QPS)—that’s almost 160M queries a day!—presented a number of challenges to overcome.\n\nEnsuring backward compatibility\n\nIssue searches are often bookmarked, shared among users, and linked in documents, making them important artifacts for developers and teams. Therefore, we wanted to introduce this new capability for nested search queries without breaking existing queries for users.\n\nWe validated the new search system before it even reached users by:\n\nTesting extensively : We ran our new search module against all unit and integration tests for the existing search module. To ensure that the GraphQL and REST API contracts remained unchanged, we ran the tests for the search endpoint both with the feature flag for the new search system enabled and disabled.\n\n: We ran our new search module against all unit and integration tests for the existing search module. To ensure that the GraphQL and REST API contracts remained unchanged, we ran the tests for the search endpoint both with the feature flag for the new search system enabled and disabled. Validating correctness in production with dark-shipping: For 1% of issue searches, we ran the user’s search against both the existing and new search systems in a background job, and logged differences in responses. By analyzing these differences we were able to fix bugs and missed edge cases before they reached our users. We weren’t sure at the outset how to define “differences,” but we settled on “number of results” for the first iteration. In general, it seemed that we could determine whether a user would be surprised by the results of their search against the new search capability if a search returned a different number of results when they were run within a second or less of each other.\n\nFor 1% of issue searches, we ran the user’s search against both the existing and new search systems in a background job, and logged differences in responses. By analyzing these differences we were able to fix bugs and missed edge cases before they reached our users.\n\nPreventing performance degradation\n\nWe expected more complex nested queries to use more resources on the backend than simpler queries, so we needed to establish a realistic baseline for nested queries, while ensuring no regression in the performance of existing, simpler ones.\n\nFor 1% of Issue searches, we ran equivalent queries against both the existing and the new search systems. We used scientist, GitHub’s open source Ruby library, for carefully refactoring critical paths, to compare the performance of equivalent queries to ensure that there was no regression.\n\nPreserving user experience\n\nWe didn’t want users to have a worse experience than before just because more complex searches were possible.\n\nWe collaborated closely with product and design teams to ensure usability didn’t decrease as we added this feature by:\n\nLimiting the number of nested levels in a query to five. From customer interviews, we found this to be a sweet spot for both utility and usability.\n\nin a query to five. From customer interviews, we found this to be a sweet spot for both utility and usability. Providing helpful UI/UX cues: We highlight the AND/OR keywords in search queries, and provide users with the same auto-complete feature for filter terms in the UI that they were accustomed to for simple flat queries.\n\nMinimizing risk to existing users\n\nFor a feature that is used by millions of users a day, we needed to be intentional about rolling it out in a way that minimized risk to users.\n\nWe built confidence in our system by:\n\nLimiting blast radius : To gradually build confidence, we only integrated the new system in the GraphQL API and the Issues tab for a repository in the UI to start. This gave us time to collect, respond to, and incorporate feedback without risking a degraded experience for all consumers. Once we were happy with its performance, we rolled it out to the Issues dashboard and the REST API.\n\n: To gradually build confidence, we only integrated the new system in the GraphQL API and the Issues tab for a repository in the UI to start. This gave us time to collect, respond to, and incorporate feedback without risking a degraded experience for all consumers. Once we were happy with its performance, we rolled it out to the Issues dashboard and the REST API. Testing internally and with trusted partners: As with every feature we build at GitHub, we tested this feature internally for the entire period of its development by shipping it to our own team during the early days, and then gradually rolling it out to all GitHub employees. We then shipped it to trusted partners to gather initial user feedback.\n\nAnd there you have it, that’s how we built, validated, and shipped the new and improved Issues search!\n\nFeedback\n\nWant to try out this exciting new functionality? Head to our docs to learn about how to use boolean operators and parentheses to search for the issues you care about!\n\nIf you have any feedback for this feature, please drop us a note on our community discussions.\n\nAcknowledgements\n\nSpecial thanks to AJ Schuster, Riley Broughten, Stephanie Goldstein, Eric Jorgensen Mike Melanson and Laura Lindeman for the feedback on several iterations of this blog post!\n\nTags:", "label": "non_personal"}
{"title": "How GitHub engineers tackle platform problems", "url": "https://github.blog/engineering/infrastructure/how-github-engineers-tackle-platform-problems/", "content": "In my spare time I enjoy building Gundam models, which are model kits to build iconic mechas from the Gundam universe. You might be wondering what this has to do with software engineering. Product engineers can be seen as the engineers who take these kits and build the Gundam itself. They are able to utilize all pieces and build a working product that is fun to collect or even play with!\n\nPlatform engineers, on the other hand, supply the tools needed to build these kits (like clippers and files) and maybe even build a cool display so everyone can see the final product. They ensure that whoever is constructing it has all the necessary tools, even if they don’t physically build the Gundam themselves.\n\nAbout a year ago, my team at GitHub moved to the infrastructure organization, inheriting new roles and Areas of Responsibility (AoRs). Previously, the team had tackled external customer problems, such as building the new deployment views across environments. This involved interacting with users who depend on GitHub to address challenges within their respective industries. Our new customers as a platform engineering team are internal, which makes our responsibilities different from the product-focused engineering work we were doing before.\n\nGoing back to my Gundam example, rather than constructing kits, we’re now responsible for building the components of the kits. Adapting to this change meant I had to rethink my approach to code testing and problem solving.\n\nWhether you’re working on product engineering or on the platform side, here are a few best practices to tackle platform problems.\n\nUnderstanding your domain\n\nOne of the most critical steps before tackling problems is understanding the domain. A “domain” is the business and technical subject area in which a team and platform organization operate. This requires gaining an understanding of technical terms and how these systems interact to provide fast and reliable solutions. Here’s how to get up to speed:\n\nTalk to your neighbors: Arrange a handover meeting with a team that has more knowledge and experience with the subject matter. This meeting provides an opportunity to ask questions about terminology and gain a deeper understanding of the problems the team will be addressing.\n\nArrange a handover meeting with a team that has more knowledge and experience with the subject matter. This meeting provides an opportunity to ask questions about terminology and gain a deeper understanding of the problems the team will be addressing. Investigate old issues: If there is a backlog of issues that are either stale or still persistent, they may give you a better understanding of the system’s current limitations and potential areas for improvement.\n\nIf there is a backlog of issues that are either stale or still persistent, they may give you a better understanding of the system’s current limitations and potential areas for improvement. Read the docs: Documentation is a goldmine of knowledge that can help you understand how the system works.\n\nBridging concepts to platform-specific skills\n\nWhile the preceding advice offers general guidance applicable to both product and platform teams, platform teams — serving as the foundational layer — necessitate a more in-depth understanding.\n\nNetworks : Understanding network fundamentals is crucial for all engineers, even those not directly involved in network operations. This includes concepts like TCP, UDP, and L4 load balancing, as well as debugging tools such as dig. A solid grasp of these areas is essential to comprehend how network traffic impacts your platform.\n\n: Understanding network fundamentals is crucial for all engineers, even those not directly involved in network operations. This includes concepts like TCP, UDP, and L4 load balancing, as well as debugging tools such as dig. A solid grasp of these areas is essential to comprehend how network traffic impacts your platform. Operating systems and hardware: Selecting appropriate virtual machines (VMs) or physical hardware is vital for both scalability and cost management. Making well-informed choices for particular applications requires a strong grasp of both. This is closely linked to choosing the right operating system for your machines, which is important to avoid systems with vulnerabilities or those nearing end of life.\n\nSelecting appropriate virtual machines (VMs) or physical hardware is vital for both scalability and cost management. Making well-informed choices for particular applications requires a strong grasp of both. This is closely linked to choosing the right operating system for your machines, which is important to avoid systems with vulnerabilities or those nearing end of life. Infrastructure as Code (IaC): Automation tools like Terraform, Ansible, and Consul are becoming increasingly essential. Proficiency in these tools is becoming a necessity as they significantly decrease human error during infrastructure provisioning and modifications.\n\nAutomation tools like Terraform, Ansible, and Consul are becoming increasingly essential. Proficiency in these tools is becoming a necessity as they significantly decrease human error during infrastructure provisioning and modifications. Distributed systems: Dealing with platform issues, particularly in distributed systems, necessitates a deep understanding that failures are inevitable. Consequently, employing proactive solutions like failover and recovery mechanisms is crucial for preserving system reliability and preventing adverse user experiences. The optimal approach for this depends entirely on the specific problem and the desired system behavior.\n\nKnowledge sharing\n\nBy sharing lessons and ideas, engineers can introduce new perspectives that lead to breakthroughs and innovations. Taking the time to understand why a project or solution did or didn’t work and sharing those findings provides new perspectives that we can use going forward.\n\nHere are three reasons why knowledge sharing is so important:\n\nTeamwork makes the dream work: Collaboration often results in quicker problem resolution and fosters new solution innovation, as engineers have the opportunity to learn from each other and expand upon existing ideas.\n\nCollaboration often results in quicker problem resolution and fosters new solution innovation, as engineers have the opportunity to learn from each other and expand upon existing ideas. Prevent lost knowledge : If we don’t share our lessons learned, we prevent the information from being disseminated across the team or organization. This becomes a problem if an engineer leaves the company or is simply unavailable.\n\n: If we don’t share our lessons learned, we prevent the information from being disseminated across the team or organization. This becomes a problem if an engineer leaves the company or is simply unavailable. Improve our customer success: As engineers, our solutions should effectively serve our customers. By sharing our knowledge and lessons learned, we can help the team build reliable, scalable, and secure platforms, which will enable us to create better products that meet customer needs and expectations!\n\nBut big differences start to appear between product engineering and infrastructure engineering when it comes to the impact radius and the testing process.\n\nImpact radius\n\nWith platforms being the fundamental building blocks of a system, any change (small or large) can affect a wide range of products. Our team is responsible for DNS, a foundational service that impacts numerous products. Even a minor alteration to this service can have extensive repercussions, potentially disrupting access to content across our site and affecting products ranging from GitHub Pages to GitHub Copilot.\n\nUnderstand the radius: Or understand the downstream dependencies. Direct communication with teams that depend on our service provides valuable insights into how proposed changes may affect other services.\n\nOr understand the downstream dependencies. Direct communication with teams that depend on our service provides valuable insights into how proposed changes may affect other services. Postmortems: By looking at past incidents related to our platform and asking “What is the impact of this incident?”, we can form more context around what change or failure was introduced, how our platform played a role in it, and how it was fixed.\n\nBy looking at past incidents related to our platform and asking “What is the impact of this incident?”, we can form more context around what change or failure was introduced, how our platform played a role in it, and how it was fixed. Monitoring and telemetry: Condense important monitoring and logging into a small and quickly digestible medium to give you the general health of the system. This could be a Single Availability Metric (SAM), for example. The ability to quickly glance at a single dashboard allows engineers to rapidly pinpoint the source of an issue and streamlines the debugging and incident mitigation process, as compared to searching through and interpreting detailed monitors or log messages.\n\nTesting changes\n\nTesting changes in a distributed environment can be challenging, especially for services like DNS. A crucial step in solving this issue is utilizing a test site as a “real” machine where you can implement and assess all your changes.\n\nInfrastructure as Code (IaC): When using tools like Terraform or Ansible, it’s crucial to test fundamental operations like provisioning and deprovisioning machines. There are circumstances where a machine will need to be re-provisioned. In these cases, we want to ensure the machine is not accidentally deleted and that we retain the ability to create a new one if needed.\n\nWhen using tools like Terraform or Ansible, it’s crucial to test fundamental operations like provisioning and deprovisioning machines. There are circumstances where a machine will need to be re-provisioned. In these cases, we want to ensure the machine is not accidentally deleted and that we retain the ability to create a new one if needed. End-to-End (E2E): Begin directing some network traffic to these servers. Then the team can observe host behavior by directly interacting with it, or we can evaluate functionality by diverting a small portion of traffic.\n\nBegin directing some network traffic to these servers. Then the team can observe host behavior by directly interacting with it, or we can evaluate functionality by diverting a small portion of traffic. Self-healing: We want to test the platform’s ability to recover from unexpected loads and identify bottlenecks before they impact our users. Early identification of bottlenecks or bugs is crucial for maintaining the health of our platform.\n\nIdeally changes will be implemented on a host-by-host basis once testing is complete. This approach allows for individual machine rollback and prevents changes from being applied to unaffected hosts.\n\nWhat to remember\n\nPlatform engineering can be difficult. The systems GitHub operates with are complex and there are a lot of services and moving parts. However, there’s nothing like seeing everything come together. All the hard work our engineering teams do behind the scenes really pays off when the platform is running smoothly and teams are able to ship faster and more reliably — which allows GitHub to be the home to all developers.\n\nWant to dive deeper? Check out our infrastructure related blog posts.", "label": "non_personal"}
{"title": "How enterprise engineering teams can successfully adopt AI", "url": "https://github.com/resources/whitepapers/how-enterprise-engineering-teams-can-successfully-adopt-ai", "content": "Afghanistan ( +93 ) Åland ( +358 ) Albania ( +355 ) Algeria ( +213 ) American Samoa ( +1 ) Andorra ( +376 ) Angola ( +244 ) Anguilla ( +1 ) Antigua and Barbuda ( +1 ) Argentina ( +54 ) Armenia ( +374 ) Aruba ( +297 ) Australia ( +61 ) Austria ( +43 ) Azerbaijan ( +994 ) Bahamas ( +1 ) Bahrain ( +973 ) Bangladesh ( +880 ) Barbados ( +1 ) Belarus ( +375 ) Belgium ( +32 ) Belize ( +501 ) Benin ( +229 ) Bermuda ( +1 ) Bhutan ( +975 ) Bolivia ( +591 ) Bonaire, Sint Eustatius and Saba ( +599 ) Bosnia and Herzegovina ( +387 ) Botswana ( +267 ) Brazil ( +55 ) British Indian Ocean Territory ( +246 ) Brunei Darussalam ( +673 ) Bulgaria ( +359 ) Burkina Faso ( +226 ) Burundi ( +257 ) Cambodia ( +855 ) Cameroon ( +237 ) Canada ( +1 ) Cape Verde ( +238 ) Cayman Islands ( +1 ) Central African Republic ( +236 ) Chad ( +235 ) Chile ( +56 ) China ( +86 ) Christmas Island ( +61 ) Cocos (Keeling) Islands ( +61 ) Colombia ( +57 ) Comoros ( +269 ) Congo (Brazzaville) ( +242 ) Congo (Kinshasa) ( +243 ) Cook Islands ( +682 ) Costa Rica ( +506 ) Côte d'Ivoire ( +225 ) Croatia ( +385 ) Curaçao ( +599 ) Cyprus ( +357 ) Czech Republic ( +420 ) Denmark ( +45 ) Djibouti ( +253 ) Dominica ( +1 ) Dominican Republic ( +1 ) Ecuador ( +593 ) Egypt ( +20 ) El Salvador ( +503 ) Equatorial Guinea ( +240 ) Eritrea ( +291 ) Estonia ( +372 ) Ethiopia ( +251 ) Falkland Islands ( +500 ) Faroe Islands ( +298 ) Fiji ( +679 ) Finland ( +358 ) France ( +33 ) French Guiana ( +594 ) French Polynesia ( +689 ) Gabon ( +241 ) Gambia ( +220 ) Georgia ( +995 ) Germany ( +49 ) Ghana ( +233 ) Gibraltar ( +350 ) Greece ( +30 ) Greenland ( +299 ) Grenada ( +1 ) Guadeloupe ( +590 ) Guam ( +1 ) Guatemala ( +502 ) Guernsey ( +44 ) Guinea ( +224 ) Guinea-Bissau ( +245 ) Guyana ( +592 ) Haiti ( +509 ) Honduras ( +504 ) Hong Kong ( +852 ) Hungary ( +36 ) Iceland ( +354 ) India ( +91 ) Indonesia ( +62 ) Iran ( +98 ) Iraq ( +964 ) Ireland ( +353 ) Isle of Man ( +44 ) Israel ( +972 ) Italy ( +39 ) Jamaica ( +1 ) Japan ( +81 ) Jersey ( +44 ) Jordan ( +962 ) Kazakhstan ( +7 ) Kenya ( +254 ) Kiribati ( +686 ) Korea, South ( +82 ) Kuwait ( +965 ) Kyrgyzstan ( +996 ) Laos ( +856 ) Latvia ( +371 ) Lebanon ( +961 ) Lesotho ( +266 ) Liberia ( +231 ) Libya ( +218 ) Liechtenstein ( +423 ) Lithuania ( +370 ) Luxembourg ( +352 ) Macau ( +853 ) Macedonia ( +389 ) Madagascar ( +261 ) Malawi ( +265 ) Malaysia ( +60 ) Maldives ( +960 ) Mali ( +223 ) Malta ( +356 ) Marshall Islands ( +692 ) Martinique ( +596 ) Mauritania ( +222 ) Mauritius ( +230 ) Mayotte ( +262 ) Mexico ( +52 ) Micronesia ( +691 ) Moldova ( +373 ) Monaco ( +377 ) Mongolia ( +976 ) Montenegro ( +382 ) Montserrat ( +1 ) Morocco ( +212 ) Mozambique ( +258 ) Myanmar ( +95 ) Namibia ( +264 ) Nauru ( +674 ) Nepal ( +977 ) Netherlands ( +31 ) New Caledonia ( +687 ) New Zealand ( +64 ) Nicaragua ( +505 ) Niger ( +227 ) Nigeria ( +234 ) Niue ( +683 ) Norfolk Island ( +672 ) Northern Mariana Islands ( +1 ) Norway ( +47 ) Oman ( +968 ) Pakistan ( +92 ) Palau ( +680 ) Palestine ( +970 ) Panama ( +507 ) Papua New Guinea ( +675 ) Paraguay ( +595 ) Peru ( +51 ) Philippines ( +63 ) Poland ( +48 ) Portugal ( +351 ) Puerto Rico ( +1 ) Qatar ( +974 ) Reunion ( +262 ) Romania ( +40 ) Rwanda ( +250 ) Saint Barthélemy ( +590 ) Saint Helena ( +290 ) Saint Kitts and Nevis ( +1 ) Saint Lucia ( +1 ) Saint Martin (French part) ( +590 ) Saint Pierre and Miquelon ( +508 ) Saint Vincent and the Grenadines ( +1 ) Samoa ( +685 ) San Marino ( +378 ) Sao Tome and Principe ( +239 ) Saudi Arabia ( +966 ) Senegal ( +221 ) Serbia ( +381 ) Seychelles ( +248 ) Sierra Leone ( +232 ) Singapore ( +65 ) Sint Maarten (Dutch part) ( +1 ) Slovakia ( +421 ) Slovenia ( +386 ) Solomon Islands ( +677 ) Somalia ( +252 ) South Africa ( +27 ) South Sudan ( +211 ) Spain ( +34 ) Sri Lanka ( +94 ) Sudan ( +249 ) Suriname ( +597 ) Svalbard and Jan Mayen Islands ( +47 ) Swaziland ( +268 ) Sweden ( +46 ) Switzerland ( +41 ) Taiwan ( +886 ) Tajikistan ( +992 ) Tanzania ( +255 ) Thailand ( +66 ) Timor-Leste ( +670 ) Togo ( +228 ) Tokelau ( +690 ) Tonga ( +676 ) Trinidad and Tobago ( +1 ) Tunisia ( +216 ) Türkiye ( +90 ) Turkmenistan ( +993 ) Turks and Caicos Islands ( +1 ) Tuvalu ( +688 ) Uganda ( +256 ) Ukraine ( +380 ) United Arab Emirates ( +971 ) United Kingdom ( +44 ) United States of America ( +1 ) Uruguay ( +598 ) Uzbekistan ( +998 ) Vanuatu ( +678 ) Vatican City ( +39 ) Venezuela ( +58 ) Vietnam ( +84 ) Virgin Islands, British ( +1 ) Virgin Islands, U.S. ( +1 ) Wallis and Futuna Islands ( +681 ) Yemen ( +967 ) Zambia ( +260 ) Zimbabwe ( +263 )\n\n+1", "label": "non_personal"}
{"title": "Unlocking the power of unstructured data with RAG", "url": "https://github.blog/ai-and-ml/llms/unlocking-the-power-of-unstructured-data-with-rag/", "content": "Whether they’re building a new product or improving a process or feature, developers and IT leaders need data and insights to make informed decisions.\n\nWhen it comes to software development, this data exists in two ways: unstructured and structured. While structured data follows a specific and predefined format, unstructured data—like email, an audio or visual file, code comment, or commit message—doesn’t. This makes unstructured data hard to organize and interpret, which means teams can miss out on potentially valuable insights.\n\nTo make the most of their unstructured data, development teams are turning to retrieval-augmented generation, or RAG, a method for customizing large language models (LLMs). They can use RAG to keep LLMs up to date with organizational knowledge and the latest information available on the web. They can also use RAG and LLMs to surface and extract insights from unstructured data.\n\nGitHub data scientists, Pam Moriarty and Jessica Guo, explain unstructured data’s unique value in software development, and how developers and organizations can use RAG to create greater efficiency and value in the development process.\n\nUnstructured data in software development\n\nWhen it comes to software development, unstructured data includes source code and the context surrounding it, as these sources of information don’t follow a predefined format.\n\nHere are some examples of unstructured data on GitHub:\n\nREADME files describe in text the purpose behind project source code, and include instructions for source code use, how to contribute, and other details that developers decide is important to include. While they’re usually written in Markdown, README files don’t follow a predefined structure.\n\ndescribe in text the purpose behind project source code, and include instructions for source code use, how to contribute, and other details that developers decide is important to include. While they’re usually written in Markdown, README files don’t follow a predefined structure. Code files are more orderly than README files in that they follow the syntax of a programming language. But not all code files have the exact same fields nor are they all written in the same format. Additionally, some parts of the file, like coding logic and variable names, are decided by individual developers.\n\nare more orderly than README files in that they follow the syntax of a programming language. But not all code files have the exact same fields nor are they all written in the same format. Additionally, some parts of the file, like coding logic and variable names, are decided by individual developers. Package documentation explains how the software works and how to use it. Documentation, written in natural language, can include installation instructions, troubleshooting tips, a description of the package’s API, and a list of any dependencies required to use the package. It can also include code snippets that highlight the package’s features.\n\nexplains how the software works and how to use it. Documentation, written in natural language, can include installation instructions, troubleshooting tips, a description of the package’s API, and a list of any dependencies required to use the package. It can also include code snippets that highlight the package’s features. Code comments explain the function behind certain code blocks in a code file. They’re text comments written in natural language and make the source code easier to understand by other developers.\n\nexplain the function behind certain code blocks in a code file. They’re text comments written in natural language and make the source code easier to understand by other developers. Wiki pages , while not limited to unstructured data, can contain helpful text documentation about installation instructions, API references, and other information.\n\n, while not limited to unstructured data, can contain helpful text documentation about installation instructions, API references, and other information. Commit messages describe in natural language text the changes a developer made to a codebase and why.\n\ndescribe in natural language text the changes a developer made to a codebase and why. Issue and pull request descriptions are written in natural language and in a text field. They can contain any kind of information a developer chooses to include about a bug, feature request, or general task in a project.\n\nare written in natural language and in a text field. They can contain any kind of information a developer chooses to include about a bug, feature request, or general task in a project. Discussions contain a wealth and variety of information, from developer and end- user feedback to open-ended conversations about a topic. As long as a repository enables discussions, anyone with a GitHub account can start a discussion.\n\ncontain a wealth and variety of information, from developer and end- user feedback to open-ended conversations about a topic. As long as a repository enables discussions, anyone with a GitHub account can start a discussion. Review comments are where developers can discuss changes before they’re merged into a codebase. Consequently, they contain information in natural language about code quality, context behind certain decisions, and concerns about potential bugs.\n\nThe value of unstructured data\n\nThe same features that make unstructured data valuable also make it hard to analyze.\n\nUnstructured data lacks inherent organization, as it often consists of free-form text, images, or multimedia content.\n\n“Without clear boundaries or predefined formats, extracting meaningful information from unstructured data becomes very challenging,” Guo says.\n\nBut LLMs can help to identify complex patterns in unstructured data—especially text. Though not all unstructured data is text, a lot of text is unstructured. And LLMs can help you to analyze it.\n\n“When dealing with ambiguous, semi-structured or unstructured data, LLMs dramatically excel at identifying patterns, sentiments, entities, and topics within text data and uncover valuable insights that might otherwise remain hidden,” Guo explains.\n\nNeed a refresher on LLMs? Check out our AI explainers, guides, and best practices >\n\nHere are a few reasons why developers and IT leaders might consider using RAG-powered LLMs to leverage unstructured data:\n\nSurface organizational best practices and establish consistency . Through RAG, an LLM can receive a prompt with additional context pulled from an organization’s repositories and documents. So, instead of sifting through and piece-mealing documents, developers can quickly receive answers from an LLM that align with their organization’s knowledge and best practices.\n\n. Through RAG, an LLM can receive a prompt with additional context pulled from an organization’s repositories and documents. So, instead of sifting through and piece-mealing documents, developers can quickly receive answers from an LLM that align with their organization’s knowledge and best practices. Accelerate and deepen understanding of an existing codebase—including its conventions, functions, common issues, and bugs. Understanding and familiarizing yourself with code written by another developer is a persisting challenge for several reasons, including but not limited to: code complexity, use of different coding styles, a lack of documentation, use of legacy code or deprecated libraries and APIs, and the buildup of technical debt from quick fixes and workarounds.\n\nRAG can help to mediate these pain points by enabling developers to ask and receive answers in natural language about a specific codebase. It can also guide developers to relevant documentation or existing solutions.\n\nAccelerated and deepened understanding of a codebase enables junior developers to contribute their first pull request with less onboarding time and senior developers to mitigate live site incidents, even when they’re unfamiliar with the service that’s failing. It also means that legacy code suffering from “code rot” and natural aging can be more quickly modernized and easily maintained.\n\nUnstructured data doesn’t just help to improve development processes. It can also improve product decisions by surfacing user pain points.\n\nMoriarty says, “Structured data might show a user’s decision to upgrade or renew a subscription, or how frequently they use a product or not. While those decisions represent the user’s attitude and feelings toward the product, it’s not a complete representation. Unstructured data allows for more nuanced and qualitative feedback, making for a more complete picture.”\n\nA lot of information and feedback is shared during informal discussions, whether those discussions happen on a call, over email, on social platforms, or in an instant message. From these discussions, decision makers and builders can find helpful feedback to improve a service or product, and understand general public and user sentiment.\n\nWhat about structured data?\n\nContrary to unstructured data, structured data—like relational databases, Protobuf files, and configuration files—follows a specific and predefined format.\n\nWe’re not saying unstructured data is more valuable than structured. But the processes for analyzing structured data are more straightforward: you can use SQL functions to modify the data and traditional statistical methods to understand the relationship between different variables.\n\nThat’s not to say AI isn’t used for structured data analysis. “There’s a reason that machine learning, given its predictive power, is and continues to be widespread across industries that use data,” according to Moriarty.\n\nHowever, “Structured data is often numeric, and numbers are simply easier to analyze for patterns than words are,” Moriarty says. Not to mention that methods for analyzing structured data have been around longer** **than those for analyzing unstructured data: “A longer history with more focus just means there are more established approaches, and more people are familiar with it,” she explains.\n\nThat’s why the demand to enhance structured data might seem less urgent, according to Guo. “The potential for transformative impact is significantly greater when applied to unstructured data,” she says.\n\nHow does RAG extract value from unstructured data?\n\nWith RAG, an LLM can use data sources beyond its training data to generate an output.\n\nRAG is a prompting method that uses retrieval—a process for searching for and accessing information—to add more context to a prompt that generates an LLM response.\n\nThis method is designed to improve the quality and relevance of an LLM’s outputs. Additional data sources include a vector database, traditional database, or search engine. So, developers who use an enterprise AI tool equipped with RAG can receive AI outputs customized to their organization’s best practices and knowledge, and proprietary data.\n\nWe break down these data sources in our RAG explainer, but here’s a quick summary:\n\nVector databases. While you code in your IDE, algorithms create embeddings for your code snippets, which are stored in a vector database. An AI coding tool can search that database to find snippets from across your codebase that are similar to the code you’re currently writing and generate a suggestion.\n\nAnd when you’re engaging with GitHub Copilot Chat on GitHub.com or in the IDE, your query or code is transformed into an embedding. Our retrieval service then fetches relevant embeddings from the vector database for the repository you’ve indexed. These embeddings are turned back into text and code when they’re added to the prompt as additional context for the LLM. This entire process leverages unstructured data, even though the retrieval system uses embeddings internally.\n\nGeneral text search. When developers engage with GitHub Copilot Chat under a GitHub Copilot Enterprise plan, they can index repositories—specifically code and documentation. So, when a developer on GitHub.com or in the IDE asks GitHub Copilot Chat a question about an indexed repository, the AI coding tool can retrieve data from all of those indexed, unstructured data sources. And on GitHub.com, GitHub Copilot Chat can tap into a collection of unstructured data in Markdown files from across repositories, which we call knowledge bases.\n\nLearn about GitHub Copilot Enterprise features >\n\nBut wait, why is Markdown considered unstructured data? Though you can use Markdown to format a file, the file itself can contain essentially any kind of data. Think about it this way: how would you put the contents of a Markdown file in a table?\n\nExternal or internal search engine. The retrieval method searches and pulls information from a wide range of sources from the public web or your internal platforms and websites. That information is used for RAG, which means the AI model now has data from additional files—like text, image, video, and audio—to answer your questions.\n\nRetrieval also taps into internal search engines. So, if a developer wants to ask a question about a specific repository, they can index the repository and then send their question to GitHub Copilot Chat on GitHub.com. Retrieval uses our internal search engine to find relevant code or text from the indexed files, which are then used by RAG to prompt the LLM for a contextually relevant response.\n\nStay smart: LLMs can do things they weren’t trained to do, so it’s important to always evaluate and verify their outputs.\n\nRAG and GitHub Copilot Enterprise Powered by RAG, GitHub Copilot Enterprise can help developers and leaders at all levels receive natural language answers to questions about specific repositories. GitHub Copilot can also use content in commits, issues, and discussions to provide contextually relevant responses. In fact, by asking GitHub Copilot questions, developers actually provide GitHub Copilot with more details about the context in which information is being used, which then helps the AI coding tool provide more accurate responses tailored to an organization’s unique codebase. Learn more about the use cases and benefits of GitHub Copilot Enterprise.\n\nUse RAG to unlock insights from unstructured data\n\nAs developers improve their productivity and write more code with AI tools like GitHub Copilot, there’ll be even more unstructured data. Not just in the code itself, but also the information used to build, contextualize, maintain, and improve that code.\n\nThat means even more data containing rich insights that organizations can surface and leverage, or let sink and disappear.\n\nDevelopers and IT leaders can use RAG as a tool to help improve their productivity, produce high-quality and consistent code at greater speed, preserve and share information, and increase their understanding of existing codebases, which can impact reduced onboarding time.\n\nWith a RAG-powered AI tool, developers and IT leaders can quickly discover, analyze, and evaluate a wealth of unstructured data—simply by asking a question.\n\nA RAG reading list 📚", "label": "non_personal"}
{"title": "How we use GitHub to be more productive, collaborative, and secure", "url": "https://github.blog/engineering/how-we-use-github-to-be-more-productive-collaborative-and-secure/", "content": "It’s that time of year where we’re all looking back at what we’ve accomplished and thinking ahead to goals and plans for the calendar year to come. As part of GitHub Universe, I shared some numbers that provided a window into the work our engineering and security teams drive each day on behalf of our community, customers, and Hubbers. As someone who loves data, it’s not just fun to see how we operate GitHub at scale, but it’s also rewarding to see how this work contributes to our vision to be the home for all developers–which includes our own engineering and security teams.\n\nOver the course of the past year, GitHub staff made millions of commits across all of our internal repositories. That’s a ton of branches, pull requests, Issues, and more. We processed billions of API requests daily. And we ran tens of thousands of production deployments across the internal apps that power GitHub’s services. If you do the math, that’s hundreds of deploys per day.\n\nGitHub is big. But the reality is, no matter your size, your scale, or your stage, we’re all dealing with the same questions. Those questions boil down to how to optimize for productivity, collaboration, and, of course, security.\n\nIt’s a running joke internally that you have to type “GitHub” three times to get to the monolith. So, let’s take a look at how we at GitHub (1) use GitHub (2) to build the GitHub (3) you rely on.\n\nProductivity\n\nGitHub’s cloud-powered experiences, namely Codespaces and GitHub Copilot, have been two of the biggest game changers for us in the past few years.\n\nCodespaces\n\nIt’s no secret that local development hasn’t evolved much in the past decade. The github/github repository, where much of what you experience on GitHub.com lives, is fairly large and took several minutes to clone even on a good network connection. Combine this with setting up dependencies and getting your environment the way you like it, spinning up a local environment used to take 45 minutes to go from checkout to a built local developer environment.\n\nBut now, with Codespaces, a few clicks and less than 60 seconds later, you’re in a working development environment that’s running on faster hardware than the MacBook I use daily.\n\nHeating my home office in the chilly Midwest with my laptop doing a local build was nice, but it’s a thing of the past. Moving to Codespaces last year has truly impacted our day-to-day developer experience, and we’re not looking back.\n\nGitHub Copilot\n\nWe’ve been using GitHub Copilot for more than a year internally, and it still feels like magic to me every day. We recently published a study that looked at GitHub Copilot performance across two groups of developers–one that used GitHub Copilot and one that didn’t. To no one’s surprise, the group that used GitHub Copilot was able to complete the same task 55% faster than the group that didn’t have GitHub Copilot.\n\nGetting the job done faster is great, but the data also provided incredible insight into developer satisfaction. Almost three-quarters of the developers surveyed said that GitHub Copilot helped them stay in the flow and spend more time focusing on the fun parts of their jobs. When was the last time you adopted an experience that made you love your job more? It’s an incredible example of putting developers first that has completely changed how we build here at GitHub.\n\nCollaboration\n\nAt GitHub, we’re remote-first and we have highly distributed teams, so we prioritize discoverability and how we keep teams up-to-date across our work. That’s where tools like Issues and projects come into play. They allow us to plan, track, and collaborate in a centralized place that’s right next to the code we’re working on.\n\nIncorporating projects across our security team has made it easier for us to not only track our work, but also to help people understand how their work fits into the company’s broader mission and supports our customers.\n\nProjects gives us a big picture view of our work, but what about the more tactical discovery of a file, function, or new feature another team is building? When you’re working on a massive 15-year-old codebase (looking at you, GitHub), sometimes you need to find code that was written well before you even joined the company, and that can feel like trying to find a needle in a haystack.\n\nSo, we’ve adopted the new code search and code view, which has helped our developers quickly find what they need without losing velocity. This improved discoverability, along with the enhanced organization offered by Issues and projects, has had huge implications for our teams in terms of how we’ve been able to collaborate across groups.\n\nShifting security left\n\nLike we saw when we looked at local development environments, the security industry still struggles with the same issues that have plagued us for more than a decade. Exposed credentials, as an example, are still the root cause for more than half of all data breaches today. Phishing is still the best, and cheapest, way for an adversary to get into organizations and wreak havoc. And we’re still pleading with organizations to implement multi-factor authentication to keep the most basic techniques from bad actors at bay.\n\nIt’s time to build security into everything we do across the developer lifecycle.\n\nThe software supply chain starts with the developer. Normalizing the use of strong authentication is one of the most important ways that we at GitHub, the home of open source, can help defend the entire ecosystem against supply chain attacks. We enforce multi-factor authentication with security keys for our internal developers, and we’re requiring that every developer who contributes software on GitHub.com enable 2FA by the end of next year. The closer we can bring our security and engineering teams together, the better the outcomes and security experiences we can create together.\n\nAnother way we do that is by scaling the knowledge of our security teams with tools like CodeQL to create checks that are deployed for all our developers, protecting all our users. And because the CodeQL queries are open source, the vulnerability patterns shared by security teams at GitHub or by our customers end up as CodeQL queries that are then available for everyone. This acts like a global force multiplier for security knowledge in the developer and security communities.\n\nSecurity shouldn’t be gatekeeping your teams from shipping. It should be the process that enables them to ship quickly–remember our hundreds of production deployments per day?–and with confidence.\n\nBig, small, or in-between\n\nAs you see, GitHub has the same priorities as any other development team out there.\n\nIt doesn’t matter if you’re processing billions of API requests a day, like we are, or if you’re just starting on that next idea that will be launched into the world.\n\nThese are just a few ways over the course of the last year that we’ve used GitHub to build our own platform securely and improve our own developer experiences, not only to be more productive, collaborative, and secure, but to be creative, to be happier, and to build the best work of our lives.\n\nTo learn more about how we use GitHub to build GitHub, and to see demos of the features highlighted here, take a look at this talk from GitHub Universe 2022.\n\nNotes\n\nTags:", "label": "non_personal"}
{"title": "How we brought multimedia search to Dropbox Dash", "url": "https://dropbox.tech/infrastructure/multimedia-search-dropbox-dash-evolution", "content": "Knowledge workers routinely lose valuable time trying to find that thing—the right images, videos, documents, or audio files—across their dozens of apps and essential work tools. When we started building Dropbox Dash, our universal search and knowledge management product, we knew it had to do more than just speed up search. It also needed to scale beyond text. Because often, the challenge isn’t just finding a file—it’s finding what’s inside that file. And that gets tricky when things aren’t labeled clearly, your team’s folder structure breaks down, or you just can’t remember where you saved what you need. Searching for multimedia content poses unique challenges. Images, for example, often come with cryptic names like IMG_6798 by default, and teams can quickly accumulate thousands of these unlabeled assets. Unlike documents, which usually contain metadata or readable content to help with discovery, media files frequently lack that context and require manual review. On top of that, they demand heavier compute resources and smarter ranking systems to deliver relevant results at speed. Supporting fast, accurate media search in Dash wasn’t a matter of layering features on top—it required fundamental changes across our infrastructure. We had to rethink how we indexed and ranked non-text files, how we rendered visual previews, and how we hydrated and surfaced metadata. We also had to reevaluate traditional document-search assumptions about relevance, latency, and even UI presentation. Our multimedia retrieval features were built to solve these exact problems, allowing users to find images, video, and audio just as easily as they find documents. What follows is a behind-the-scenes look at the engineering that made this possible: what we built, what we learned, and how we delivered a system that makes media as searchable as text.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images—plus generative AI capabilities across even more connected apps. See what's new →\n\nChallenges in supporting multimedia search\n\nSupporting search for multimedia—images, video, and audio—introduces a distinct set of technical hurdles. These files require significantly more processing power, have fewer textual cues for ranking, and often lack meaningful metadata. Delivering fast, relevant results means handling large file sizes efficiently, identifying new relevance signals, and optimizing how results are rendered for user review. That’s why our universal search solution had to support seamless browsing, filtering, and previewing of media—right inside Dash. Scaling search for this content meant facing higher storage and compute costs, tighter latency requirements, and adapting systems originally built for text-based retrieval. To understand what makes media search tricky, let’s break down some key considerations. Storage cost\n\nMedia files are significantly larger than typical documents. On average, image files are about 3X larger, and video files are roughly 13X larger than non-media files in our system. These size differences directly increase storage demands and costs. Compute cost\n\nMultimedia files, such as images and videos, require more intensive processing to extract features both due to their larger size and the complexity of the features. Unlike text documents, we also generate previews of different resolutions for the images and videos, thereby significantly increasing the compute demands in our system. Relevance\n\nDash operates a multi-phase retrieval and ranking scheme, which was previously trained and optimized for textual content. Retrieving and ranking multimedia content requires having indexed any new multimedia-specific signals, formulating a query plan that leverages these signals, and handling any corner cases to avoid poorly ranked results. Responsiveness\n\nServing multimedia content introduces new latency challenges that are not present with text-based documents. We need previews for the multimedia search results to be meaningful, and we need them in multiple resolutions, including high-res formats, for a rich product experience. The larger resolutions add to the storage and compute costs. Only a small fraction of the indexed files are actually viewed during search interactions. As a result, precomputing previews at multiple resolutions for all media files would be wasteful and unnecessary. To balance responsiveness with resource efficiency, we generate previews on demand during the read path rather than upfront. This minimizes upfront compute and storage costs but introduces new latency concerns during user interactions, since we want to generate previews quickly to have a snappy user experience. With these challenges in mind, we designed a solution that integrates scalable infrastructure, smarter content understanding, and a preview system optimized for speed and accuracy.\n\nBuilding a multimedia search solution\n\nTo deliver a responsive and scalable experience in Dash, we had to rebuild key parts of our infrastructure to support search that’s as smart and seamless for photos and videos as it is for documents. This work spans multiple layers of the stack, and it wasn’t pulled off successfully without trial and error. We began by indexing lightweight metadata—pulled from media blobs (the raw files like images, videos, or audio)—to keep compute costs low. We extended our relevance models to handle location-aware queries and fuzzy file naming, and we optimized our preview generation pipeline to balance latency with cost. Along the way, we made frontend and backend updates to ensure media renders quickly and consistently across devices. The result is a robust multimedia search experience powered by smart metadata, just-in-time previews, and a UI that helps users find the right visual asset fast. Let’s get into how we tackled it. Indexing media files by metadata To keep the compute costs low, we begin by indexing media files using available metadata, which is significantly cheaper to process than analyzing the full contents of media like images or videos. For example, we extract features such as file path, title, and EXIF. These metadata provide a lightweight foundation that enables basic search functionality with minimal processing overhead. As our capabilities evolve, we plan to build on this metadata-first approach by selectively incorporating deeper content analysis—such as semantic embedding and/or OCR—striking a balance between accuracy and cost.\n\nTo generate metadata features at scale, we leveraged Riviera, our internal compute framework that already powers Dropbox Search. Riviera processes tens of petabytes of data daily and includes mature business logic for metadata extraction. By reusing it, we benefited from proven scalability and consistency with existing Dropbox search infrastructure. Backfilling the index\n\nPrior to this initiative, we avoided downloading or storing raw media blobs in order to reduce storage and compute costs. As a result, our existing search index lacked the necessary features to support rich, media-specific search experiences. To address this gap, we added support for ingesting multimedia blob content to compute the required features. We retain the raw content for preview generation and to compute future features. Where possible, we download previews provided by third-party applications. These externally sourced previews are especially useful for design files like Canva, where we’re unable to generate our own. Using them also helps us reduce compute costs. Storage optimizations\n\nDash optimizes the file sizes and MIME types ingested to balance storage cost and file availability. We currently ingest about 97% of media files and are working to address the remaining gaps with smarter lifecycle management techniques. Retrieving media files by metadata When a user searches for media, we configure the query to match their input against the metadata features extracted during indexing. This includes fields like filenames, file paths, and location data. To enhance location-based search, we also apply custom query logic for interpreting geographic references. Internally, we index a GPS location as a chain of IDs corresponding to the geographical hierarchy. For instance, we can look up the GPS coordinates of a photo to be from San Francisco in a process known as “reverse geocoding.” Then, we would build a chain of IDs corresponding to San Francisco, California, and the United States, respectively, and place these IDs in the index for the photo. This allows us to retrieve the photo when the user wants to search for a photo taken in San Francisco, California or the entire United States, respectively.\n\nAt query time, we identify substrings of the query that may potentially be geographical locations, and then we determine whether they map to a valid location ID. In practice, because the number of known geographical locations has a manageably small cardinality, we retrieve the entire mapping upon the service startup and cache it. Lastly, in the course of building multimedia search, we realized that many multimedia files are named in particular ways. Many of them are files in the filesystem, e.g. PhotoShoot-Revised1234.jpg. To support better matching, we added logic to tokenize camel case, hyphenated strings, and numeric suffixes during both indexing and retrieval. Preview and metadata hydration at retrieval time Our system ingests data at a rate that’s approximately three orders of magnitude higher than the query rate. This disparity makes it prohibitively expensive to generate and store previews for all multimedia files during ingestion, both in terms of compute and storage. To address this, we adopted a just-in-time approach, where previews are generated at query time. This strategy significantly reduces upfront costs while still supporting a responsive user experience. As part of our storage optimization efforts, we considered precomputing previews during ingestion to enable deletion of the raw content afterward. However, we ultimately decided against this approach for two key reasons. First, managing the lifecycle of these additional preview artifacts would introduce significant code complexity. Second, retaining the raw content ensures future flexibility, allowing us to compute new features later without having to re-ingest the original files.\n\nTo power the just-in-time approach, we rely on an internal previews service built on top of Riviera, a framework originally developed for Dropbox Search. The previews service is designed to be fast, scalable, and efficient. It incorporates intelligent caching strategies, storing previews for up to 30 days. This allows us to serve previews quickly when needed without repeatedly generating them for every request. During a search, we generate preview URLs for the relevant results, which are then passed to the frontend. The frontend fetches these URLs and displays the corresponding previews directly to the user. By reusing both the Riviera framework and the previews service, we also create the opportunity to reuse frontend components across both Dropbox and Dash. This ensures a consistent product experience across both platforms. To improve latencies, we create the preview URLs in parallel with other search operations such as ranking the results, performing permission checks, and fetching additional metadata needed to render complete search results. By handling these tasks in parallel, we minimize the overall response time and ensure a responsive user experience.\n\nSometimes, a user may want to enlarge a preview and view additional metadata, such as camera information. However, this is a less common operation, and sending all the extra metadata with every search result would be inefficient. When users request more detail—such as camera metadata or timestamp—we fetch it on-demand via a separate endpoint. This keeps the initial search response lean while still supporting deeper inspection when needed. User experience Searching through images and videos is a different experience than searching documents, especially since media files often have names like “IMG_1234” that don’t tell you much. That’s why fast, visual previews are essential—they help users quickly decide which file is relevant without needing to open each one. We’ve designed our preview system to load quickly and adapt to different shapes and sizes of media, whether an image is tall, wide, or an unusual shape. The layout avoids awkward cropping and keeps things easy to browse. When a user wants a closer look, they can open a full-size preview that also shows helpful EXIF details like when the photo was taken, what kind of camera was used, and where it was captured. Everything is built to feel smooth and fast, whether you’re using Dash on a phone or computer. The interface stays out of the way and puts the focus on the content, making it easy to browse quickly or dive into a specific file when needed.\n\nLessons learned and future direction", "label": "non_personal"}
{"title": "Implementing end-to-end encryption for Dropbox teams", "url": "https://dropbox.tech/security/end-to-end-encryption-for-dropbox-teams", "content": "People trust Dropbox to keep their most important content secure. As more teams embrace remote and distributed work, ensuring the privacy and security of their data has never been more important. While customers already appreciate our simple, seamless access controls, those who work with more sensitive information have told us they want even more control over how their data is secured. One of the ways that Dropbox is meeting the needs of these customers is with the introduction of zero-knowledge, end-to-end encryption for team folders. While Dropbox already encrypts files at rest using 256-bit AES, customers are seeking end-to-end encryption where only they possess the decryption key, so not even Dropbox can access the contents of their files. For customers with especially sensitive or confidential data—for example, those working in finance or healthcare—end-to-end encryption offers an additional level of security. When enabled, files are encrypted directly on the customer’s device before being uploaded to our servers. Here we’ll discuss our implementation of end-to-end encryption for teams, the threat model of our design and encryption algorithms, and our commitment to minimizing the risk of data loss with a team-centric key management approach.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images—plus generative AI capabilities across even more connected apps. See what's new →\n\nBalancing security and usability\n\nOur commitment to simplicity and reliability is at the heart of our encryption design. In our view, a secure system must also be user-friendly. A security system serves no purpose if it’s too complicated to use. For a feature like end-to-end encryption, with its added layer of complexity, striking the right balance between security and usability is key. Our aim was to make this technology accessible without compromising security. At its core, our implementation of end-to-end encryption is designed so that neither Dropbox, unauthorized users, or malicious third parties can access a team’s encrypted files. Only the team holds the keys. Even if an attacker gains access to those keys, our implementation still ensures the confidentiality of new files or modifications, as long as the team’s keys have been rotated. Encryption also assures that a file has not been tampered with. In other words, if a file decrypts successfully, it is cryptographically guaranteed to be the exact same content as encrypted in the first place.\n\nAt the same time, because zero-knowledge encryption means customers manage their own keys, they also risk losing access to their data if those keys are lost. To address this, we’ve developed a key management system designed specifically for teams. It ensures that even if one member loses their keys, the data remains accessible and secure for the rest of the team.\n\nTeam-centric key management\n\nKey management in many end-to-end encryption systems has traditionally focused on individual users, mainly because they were the first to adopt and use these systems. In those systems, each user is responsible for managing their own set of keys and making sure they're always accessible. However, this can create complications that diminish the user experience and may even lead to data loss if keys are misplaced. To counteract potential data loss, some systems use a method called key escrow, which allows for data recovery by a trusted third party, e.g. a spouse or an administrator. But this adds complexity, both in terms of the coding required and in using the product itself. By focusing on our teams customers and drawing the cryptographic boundary around teams, we were able to re-think how the key management is done. With our approach, users don’t have any keys, but every team has a central team key. This key is accessible to all team members and controls access to the team’s encrypted data, providing protection against unauthorized third parties. The team-centric approach offers the following benefits: Reduced risk of data loss and implicit key escrow. By sharing the team key among all members, any member with access—such as a team admin with a recovery key or a member with a registered device—can restore access for everyone.\n\nBy sharing the team key among all members, any member with access—such as a team admin with a recovery key or a member with a registered device—can restore access for everyone. Reduced user responsibility. The burden of managing cryptographic keys shifts from individuals to the team, reducing the risk of a single person causing data loss.\n\nThe burden of managing cryptographic keys shifts from individuals to the team, reducing the risk of a single person causing data loss. Reduced complexity and improved user experience. The absence of user keys as well as an explicit key escrow significantly simplifies the implementation and improves the user experience. Team members can simply use end-to-end encryption without having to worry about keys at all. To preserve data confidentiality when team members change, admins can rotate keys for the entire team. Rotating keys upon the departure of a member ensures that any potentially leaked keys become obsolete for accessing new or modified encrypted data. This mechanism is critical in a scenario where a former member, now considered an untrusted outsider, attempts to misuse a previously acquired key. By instituting a new team key for encrypting subsequent data, the system effectively safeguards the confidentiality of new files or modifications made after key rotation, thereby aligning with the threat model's emphasis on protecting data integrity against insider threats turned external.\n\nAutomatic and manual device registration\n\nBefore a user can use end-to-end encryption on a new device, the required keys must first be made available. We offer admins a choice of two device registration modes: automatic device registration and manual device registration. Automatic device registration balances security with usability by distributing keys from our system to authorized team members through the Dropbox authentication and access control infrastructure—for example, when logging into a new device. Existing devices automatically authorize new devices by wrapping the team key with the new device's public key. If there are no devices available to do this, a team admin can use a recovery key to facilitate the new device's registration. The device then obtains and uses its version of the team key, ensuring quick and smooth setup without manual input. If a customer prefers more fine-grained control, they can opt for manual device registration. This process lets team admins personally approve new devices before they can access encrypted files. Team admins and members can check key authenticity by comparing the fingerprints, or security codes, of the device and team keys out-of-band. Only keys verified to belong to the correct devices and team will be used, ensuring that only legitimate team devices can access encrypted files. This process adds an additional safeguard against unauthorized access and man-in-the-middle attacks as admins can ensure that a key really belongs to a person or team, and not a malicious actor. Despite its security benefits, key verification can be cumbersome and impact usability, often leading to its limited real-world use—so we've made it an optional feature for those who need greater security.\n\nWhat end-to-end encryption doesn’t cover\n\nIt’s important to point out there are also some threats that fall beyond the scope of our implementation: Device security. Though end-to-end encryption keeps data safe during transmission and while stored on our servers, it doesn't address security at the device level. Since encrypted files decrypt automatically for access during sync or download, we still recommend customers adopt best practices such as full-disk encryption and secure access methods to protect their devices.\n\nThough end-to-end encryption keeps data safe during transmission and while stored on our servers, it doesn't address security at the device level. Since encrypted files decrypt automatically for access during sync or download, we still recommend customers adopt best practices such as full-disk encryption and secure access methods to protect their devices. Metadata visibility. Our encryption efforts concentrate on file contents rather than metadata. With this approach, customers can still search their Dropbox account based on metadata such as file name, file type, and creation date, ensuring end-to-end encryption is still practical in everyday use.\n\nOur encryption efforts concentrate on file contents rather than metadata. With this approach, customers can still search their Dropbox account based on metadata such as file name, file type, and creation date, ensuring end-to-end encryption is still practical in everyday use. Insider threats. Our implementation safeguards against external threats to a team but doesn't change internal permissions. Teams should continue using existing access controls to manage data access amongst members, ensuring sensitive information remains compartmentalized and secure.\n\nA closer look at our encryption techniques\n\nOur implementation uses a hybrid scheme, combining a symmetric algorithm for encrypting file content with an asymmetric algorithm for securing the keys. We aim for a balance of proven security, performance, and broad platform support in our choice of encryption algorithms. Symmetric file encryption\n\nPlaintext content is split into 4 MB blocks, where each block is authenticated using AES-256 encrypted in Galois/Counter Mode (GCM) with a random and unique 96-bit nonce. While AES-GCM guarantees authenticity and integrity for each block, the 128-bit authentication tags of all blocks are cryptographically hashed using HMAC-SHA-256 to expand these guarantees to the entirety of the file. This method supports partial encryption and decryption, offering seamless security without compromising the file's integrity or order. This method is especially effective for large files, as it aligns with our practice of chunking file content into 4 MB blocks for storage. It also avoids the limitations of in-memory processing required by some APIs, like WebCrypto. Asymmetric key wrapping\n\nTo encrypt secret keys, our approach to key management uses Hybrid Public Key Encryption (HPKE), a modern and flexible standard that combines asymmetric and symmetric encryption in a hybrid crypto system. We use HPKE in single shot, base mode using Elliptic-Curve Cryptography (ECC) with the P-256 curve, SHA-256, and AES-256-GCM (DHKEM(P-256, HKDF-SHA256), HKDF-SHA256, AES-256-GCM). When manual device registration is chosen, HPKE is used in auth mode to encrypt parts of the key chain with sender authentication required for effective key verification. NIST P-256 has been chosen over other curves like Curve25519 because it is widely adopted in the industry, is available in most cryptographic libraries (e.g. WebCrypto, CryptoKit, OpenSSL), and is specified in FIPS 186-4. Post-quantum cryptography\n\nThe algorithms mentioned above do not include any post-quantum cryptography (PQC). While there exist some products with early implementations of PQC, we're taking a more cautious approach, relying on proven and time-tested encryption algorithms for several reasons: PQC's reliability for long-term storage is still uncertain due to ongoing standardization efforts. For instance, the Kyber algorithm has seen several revisions throughout its NIST standardization process.\n\nPQC is relatively new in cryptographic terms and lacks the extensive scrutiny that more established algorithms have undergone. To counteract this, some PQC applications use a hybrid model, where traditional cryptography is also used. This ensures baseline security should the PQC component be compromised at the expense of greater complexity.\n\nPQC algorithms are not yet sufficiently included in common cryptographic libraries, requiring custom implementations across some codebases and increasing the risk of vulnerabilities, bugs, and other human error.\n\nThe threat posed by quantum computing—while significant—is still theoretical, with its practical impacts still unknown. Given these considerations, we’ve maintained flexibility around our ability to change our encryption protocols, while staying focused on trusted, well-known cryptographic implementations. This will enable us to integrate new encryption algorithms to our protocol at any time in the future. We are closely monitoring the development of Kyber and other PQC algorithms and will adapt our choice of encryption algorithms as they mature and standards evolve further.\n\nSecuring the future", "label": "non_personal"}
{"title": "Read This Will Be Fun – Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/21/read-this-will-be-fun/", "content": "The Princess Bride meets People We Meet on Vacation in this cozy quest romantasy about a group of friends who once defended their magical land together but haven’t spoken since, reuniting to attend a royal wedding, and ending up on a new adventure to save the realm—and hopefully themselves.\n\nTen years ago, they saved the realm. It ruined their lives.\n\nEveryone in Mythria knows the story of how best friends Beatrice and Elowen, handsome ex-bandit Clare, and valiant leader Galwell the Great defended the land from darkness. It’s a tale beloved by all—except the former heroes. They haven’t spoken in a decade, devastated by what their quest cost them.\n\nBut when they receive an invitation to the queen of Mythria’s wedding, it’s a summons they can’t refuse . . . and a reunion for the ages, with Clare secretly not over his long-ago fling with Beatrice, Beatrice fighting the guilt she feels over how everything ended, Elowen unprepared for the return of her former flame (the cunning Vandra), and all of them lost without Galwell’s presence. And if reuniting with old friends and lovers wasn’t perilous enough, dark forces from their past have returned, plotting a domination that only Mythria’s one-time defenders can stop. Maybe.\n\nDusting off old weapons and old instincts, they face undead nemeses, crystal caves, enchanted swords, coffee shops, games of magical Truth or Dare, and, hardest of all, their past—rife with wounds never healed and romances never forgotten.\n\nThis time around, will their story end in happily ever after?", "label": "non_personal"}
{"title": "Read The Arts and Crafts Movement – Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/24/read-the-arts-and-crafts-movement/", "content": "Read The Arts and Crafts Movement: A Study of Its Sources, Ideals and Influence on Design Theory by Gillian Naylor\n\nLots of original quotes are interspersed, from a variety of sources. The text itself sometimes got lost in facts, dates, and sequences; it was best where it narrativized and provided high level analysis of trends. Some of the quotes are 🔥\n\n“The movement… represents in some sense a revolt against the hard mechanical conventional life and its insensibility to beauty (quite another thing to ornament). It is a protest against that so-called industrial progress which provides shoddy wares, the cheapness of which is paid for by the lives of their producers and the degradation of their users.”\n\n— Walter Crane, The Revival of Design and Handicraft (orig. pg. 12)\n\nI found the layout of the text frustratingly uncomfortable to read. I read through page 145 (195 pages of content before endnotes).\n\nGraphics\n\nInteresting and wide-ranging collection of sample works included (chiefly black and white unfortunately) — though I was frustrated at least twice when a specific piece would be described in the text but not pictured.\n\nNotes and Quotes\n\nMartin Wiener – English Culture and the Decline of the Industrial Spirit\n\nDesign theory and categorization of ornament = related to industrialization of design\n\n“The Arts and Crafts movement was inspired by a crisis of conscience. Its motivations were social and moral, and its aesthetic values derived from the conviction that society produces the art and architecture it deserves.” (from 1989 preface by Gillian Naylor)\n\n“Averting mankind’s enslavement to the machine by saving the mass product and the home from mechanical anarchy and by restoring them to purpose, sense and life.”\n\n— Walter Gropius, The Scope of Total Architecture\n\ncontemporary critique from Thorstein Veblen 1899: idolizing the handmade –> conspicuous consumption — “propaganda of crudity” describing the “exaltation of the defective” aka imperfections in handwork (aestheticization!!!)\n\nthey don’t write insults like this anymore lol: “disencumber yourselves of the lymphatic ideology of your deplorable Ruskin” –Marinetti, 1912\n\n1835 committee to figure out how to give craftspeople a design sense and *taste* since consumers preferred imported aesthetics\n\nRuskin thought a craft and its society were inextricable\n\n“For it is not the material, but the absence of human labor, which makes the thing worthless, and a piece of terracotta, or plaster of paris, which has been wrought by the human hand, is worth all the stone and Carrara cut by machinery. It is, indeed, possible and even usual, for men to sink into machines themselves, so that even handwork has all the character of mechanization.”\n\n— Ruskin, ‘The Lamp of Truth’ from The Seven Lamps of Architecture\n\nThe Stones of Venice = key book to the movement — especially the essay “Nature of Gothic” (here as printed by William Morris)\n\n“It is not that men are ill-fed, but that they have no pleasure in the work by which they make their bread, and therefore look to wealth as the only means of pleasure.”\n\n— Ruskin, ‘Nature of Gothic’ from The Stones of Venice\n\nRuskin into destigmatizing manual labor\n\nsociety’s wealth measured in human happiness and its works of art\n\nthis is grouping Burne-Jones in with Arts and Crafts rather than Pre-Raphaelite\n\n“It was just a commonplace thing handled imaginatively, and it gave me as much pleasure as anything in the exhibition. It made me feel that it takes a big man to do a simple thing.”\n\n— architect John Sedding, about a piece of furniture designed by Ford Madox Brown\n\n“‘Art’ to them meant individuality and the search for ‘truth’, whether in painting, architecture or applied design — and truth, they felt, could be found both in the study of nature, and in the recreation of the spirit rather than the letter of mediaevalism.”\n\nduality of “straightforward, honest craftsmanship” and “mid-nineteenth-century ornamental conventions”\n\n“cardinal principle” = know your materials and learn the craft directly\n\nMorris thought pattern design should hold meaning:\n\n“do not introduce any lines or objects which cannot be explained by the structure of the pattern; it is just this logical sequence of form, this growth which looks as if, under the circumstances, it could not have been otherwise, which prevents the eye wearying of the repetition of the pattern.”\n\nconflict between Morris’ love of craftsmanship and the expense of producing quality goods blocking most people from accessing it led to him becoming a Socialist\n\n“[I]t is the allowing of machines to be our masters, and not our servants, that so injures the beauty of life nowadays.” — William Morris\n\nArts and Crafts accepted both “simple and luxurious” — “sprang from the ideal of the craftsman as artist, and from the belief in individualism and individual commitment”\n\nSee also: Read Liberty: British Colour Pattern\n\nRead In Harmony with Nature", "label": "non_personal"}
{"title": "tech industry – Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/tag/tech-industry/", "content": "This feels like a sister piece to Ed Zitron’s essay Era of the Business Idiots and Mandy Brown’s essay Toolmen. Fair warning, this is a 5000 word post; I’ve been working on this for weeks, pulling together what I’ve learned about generative AI and culture over the past two years, so I hope it is worth your time 😄 Bonus: it doubles as a playlist 🎶\n\n“‘Real power’ is achieved when a technology ‘[leaves] mythology and [enters] banality,'” Marion Fourcade and Kieran Healy quote Vincent Mosco in The Ordinal Society. We’ve had the mythology stage — the world tour with grandiose prophecies of imminent AGI — but now the race to normalize generative AI* is on: tech corporations are attempting to inure people to generative AI, an expression of the Business Borg aesthetic that currently carries a negative stigma outside of tech.\n\n*(My rule of thumb: if something is described as AI, it’s probably predatory and/or bullshit; if it’s described as machine learning, it probably does something useful. Not always true but a helpful predictor.)\n\nIn general, people like what we recognize better than what we don’t — we prefer cultural works we can categorize to the unfamiliar and undefinable — and we are facing an inescapable shock-and-awe barrage of genAI graphics across the web to inundate our synapses with uncanny synthetic renderings.\n\nCurrently, generative AI is shunned by many artists and writers, the traditional arbiters of good taste and culture, because it has been developed through the theft of their labor. But tech CEOs stand to make (even bigger) fortunes if they can convince people that genAI doesn’t signify bad taste, or make it seem like an irrevocable fact of life, like spam emails and text scammers. It’s being deployed upon us with the same lockstep corporate solidarity that forced us to pay fees for checked luggage on flights (younger folks, before 2008 your bag used to be included with your ticket! Stowing your carry-on wasn’t a competitive sport back in the day.).", "label": "non_personal"}
{"title": "Lies, damn lies, and business cases for AI hype", "url": "https://tommorris.org/posts/2024/lies-damn-lies-and-business-cases-for-ai-hype/", "content": "This week, to great fanfare, a report on how AI could “transform the state” has been published by the Tony Blair Institute of Global Change.\n\nThe hype around this report has been increased somewhat by the recent British general election, and whether the views are likely to get a better reception with Keir Starmer in Number 10 instead of Rishi Sunak.\n\nI’ll leave that kind of palace intrigue to others, and instead have a look at the report. It’s also available as a PDF, but the PDF version lacks some very exciting AI generated graphics, apparently.\n\nThe Executive Summary gives the pitch for why so-called AI is important.\n\nThe latest iterations of artificial-intelligence systems – generative AI such as large language models (LLMs) – are matching humans for quality and beating them on speed and cost. Knowledge workers using the GPT-4 model from OpenAI completed 12 per cent more tasks, 25 per cent quicker, with a bigger boost in productivity for less-skilled workers. Businesses using AI tools are 40 per cent more efficient and have 70 per cent higher customer and employee satisfaction than businesses that do not.\n\nOoh, statistics! Facts! Studies! The “25% quicker” number is from the Noy and Zhang study. Contrary to the excited MIT press release which claims it is “open access”, it isn’t. If you want to read it, you may have to rely on the working paper which has known mistakes.\n\nThere’s some interesting statistical issues with this paper pointed out in this Twitter thread, but I’m broadly of the view that it’s probably about right. If you look at the Supplementary Materials and Methods document that goes with the trial, you can see the sort of writing tasks that the study participants were given…\n\nmanagers and HR professionals were told that their employer had built a VR/metaverse-style “virtual office space” where employees working-from-home could hang out, but they weren’t using it because they quite like the solitude of not talking to weird cartoon avatars, and don’t want to wear annoying VR helmets. They were tasked with writing a company-wide email of around 400 words pleading with the employees to try to use the horrible metaverse nonsense more.\n\nmanagers were told to write an email explaining how the company was shifting towards a flatter organisational structure… but without any concrete details as to what that would be\n\ndata analysts were asked to write “code notebooks” setting out the steps they would take in analysing data for a bank to reduce customer churn, and for push notification marketing for an ecommerce service. A code notebook, it should be noted, is not actually something like a Jupyter notebook, but something akin to a diary explaining the steps the analyst would take, what tools they might use (Excel vs. Tableau, Python vs. R), and a summary of the kind of analysis they might look at: clustering, pivot tables, segmenting, A/B testing etc.\n\nmarketers were asked to write a press releases for self-driving e-bikes and for augmented-reality glasses\n\nfor consultants, they had to read passages from a number of reports and write a short benefits and risks summary for a business\n\nfor grant writers, they had to write cover letters for a grant application\n\nYou’ll note that these tasks are ones anyone could tell you that large language models are quite good at, because there is no link between the task and actual reality. In addition, I’m a little dubious about how much effort most people are going to put into an online study when compared to the amount of effort they are likely to put into their actual job, where the consequences of performing badly include loss of income, social embarassment, and lack of professional advancement–all of which are rather more significant than missing out on a couple of extra dollars in one’s beer money pot.\n\nAnyway, let’s move onto study number two. What did the Tony Blair Institute report say?\n\nBusinesses using AI tools are 40 per cent more efficient and have 70 per cent higher customer and employee satisfaction than businesses that do not.\n\nIt links to a blog post from Google Cloud, announcing a Harvard Business Review study sponsored by Google Cloud. That’d be the Google Cloud who attributed 28% of their revenue growth in Q1 2024 to AI. Totally unbiased research, in other words.\n\nIf you don’t want to give Google Cloud your personal details to read this report, here’s the link. Burner emails are useful.\n\nSo what’s the methodology used by the report? They did a survey of business executives, and compared the results of that survey with results from a set of executives defined as “leaders”. (What makes them leaders? Your guess is as good as mine.)\n\nThey asked them whether their companies had been doing data analytics AND/OR “AI/ML”, then asked them whether over the last year, their organization had improved in a number of categories, including the introduction of new products and services, operational efficiency, customer satisfaction, revenue/growth, customer loyalty/retention, profitability, market share, employee satisfaction, and the predictability of IT costs.\n\nThey also asked them whether they’d increased the usage of data analytics and/or AI/ML over the two years preceding the survey, and whether the use of those technologies had increased in importance over the same two years. It’s worth noting here that on these questions, AI/ML is bolted on almost as an afterthought. They get one question, the rest are on cloud services, data storage, analytics, use of APIs, open source etc.\n\nThe results? The business leaders in the specially selected category of super-duper leaders say they are doing more data analytics and AI than the normies back in economy class. They think it’s important for their business. And more of them say their businesses are doing significantly or slightly better on all those various metrics than they were previously.\n\nWhat does it not say? Well, that the interest or investment in data analytics and/or AI/ML technologies caused any improvement in those business metrics. Not that it could say that, because there is no verification of the results. Have the companies actually increased on those metrics? I mean, if they’re publicly traded, you can probably check on revenue, growth and profit, and you could use some independent survey data on customer satisfaction, and maybe you might be able to get some numbers from Glassdoor or whatever on employee retention which might stand in as a proxy for employee satisfaction.\n\nThe extremely scant methodology section also notes that 23% of respondents are in the technology sector (followed by 11% in financial services, 10% in healthcare, 9% in government/non-profits, 9% in manufacturing, and then an unreported long tail).\n\nPeople in the technology sector think investing in technology is important to the success of their business? I’m shocked, your honour, I genuinely had no idea.\n\n“It’s a paid-for report put out by a corporation with a target audience of business executives, not a Cochrane Review. Of course it’s bullshit, who cares?“, you might ask.\n\nWell, it’s somewhat important that the promises of brand new magic computers are considered with a little more skepticism than management consultant nonsense. The executive summary goes on to argue that “[a]dopting AI in the public sector is a question of strategic prioritisation that supersedes everything else”, and AI could bring public sector productivity improvements worth £40 billion a year. By golly, that’s over two Brexit’s worth of benefits!\n\nHow would we achieve this goal?\n\nInteroperable data, for one. The government needs to “secure upfront funding to rapidly link data across government that will make the implementation of AI at scale possible, maintaining privacy and anonymity”. Nobody’s tried that before, unless you ignore data.gov.uk, and the massive push across government to get more data, to use data science techniques etc.\n\nAlso on the agenda: buying a boatload of GPUs for the government’s data centers. This seems to rather put the cart before the horse. Google and Meta and OpenAI kind of need them because they’re training lots of machine learning models, but is the British government going to suddenly need to? And if they need to, why can’t they just make a sensible procurement decision between training their models on AWS/GCP/Azure/whatever and buying their own hardware at the time when that becomes a live issue?\n\nThe Civil Service could apparently be rejuvenated through hiring a bunch of AI experts, including a “graduate-entry route for AI experts through the Fast Stream”. The report envisions civil servants being “guided and supported in their day-to-day tasks by a Multidisciplinary AI Support Team (or MAST) platform”.\n\nThe benefits this would bring to citizens of the United Kingdom would apparently be immense. For instance, it could speed up responding to Freedom of Information Act requests.\n\nCurrently, responding to FOI requests requires a significant investment of time from officials to find and format information as well as make decisions about what can and cannot be shared, often inconsistently. Rather than deal with individual queries on an ad hoc basis, MAST allows departments to use open-data platforms for FOI requests, using the same mechanisms as in the previous examples.\n\nRight, so I send in an FOI request. It magically attempts to do a fancy JOIN command across a bunch of CSVs that may or may not be up-to-date and sends it back to me. If the data is already published, I can do that myself already. But if the data isn’t published, instead of the government department providing the data I asked for, I’ll get Clippy either making up data I didn’t ask for, or denying it on the basis of a clearly inappliable FOI exemption. All the joys of WhatsApp customer service chatbots but as applied to government transparency.\n\nIt also imagines that “the MAST platform” can help with public procurement. Once an area of interest only for the wonkiest of policy wonks, the last few years of controversy around VIP lanes for COVID PPE has certainly made it interesting again.\n\nWith AI analysis of large data sets on economic activity and past contracts, departments can reach out directly to organisations that meet different thresholds for risk, the vendor’s financial health and track record… Vendors, in turn, streamline the process of putting together a bid with AI- generated responses and receive an immediate assessment of their fit prior to its submission, demystifying the procurement process for SMEs.\n\nI have some questions. Quite boring ones, I am sure, but I fear they might be of some importance.\n\nImagine you run a small or medium sized company participating in the procurement process, and the government encourages you to use their magic AI “help vendors fill in forms” system. It screws up and makes a material misstatement to the government. The government relies on that statement, but you can’t deliver and so you breach the contract. Who will be liable? The vendor who trusted the crappy computer system the government told them to use, or the government for nudging them into using the crappy system.\n\nThe “immediate assessment” the system gives—is that something a vendor can rely on? What if it the system incorrectly tells a vendor that their bid is unlikely to succeed and they give up when they would otherwise have had a very good shot at getting the contract? A bold new frontier in the loss of a chance doctrine awaits!\n\nNext: Regulation 18 of the Public Contracts Regulations 2015 states that “contracting authorities” (read, the government or public sector body) “shall treat economic operators” (suppliers) “equally and without discrimination and shall act in a transparent and proportionate manner”.\n\nIs the magic AI procurement bot going to handle that? If and when it screws up, is the central government body who administers it going to cover the cost of the consequences, or will it come out of the departmental budget? Will the company who supplies the magic fix-everything technology take any responsibility? Or will we just say “it’s Agile, you’re holding it wrong” and move on.\n\n“What if it goes wrong? How long until it goes to court?” are totally valid questions to ask, especially given the considerable sums that’s just been handed out willy-nilly to party donors, chums and spivs one met down the pub to provide unusable PPE.\n\nAn important question that has to be considered in all attempts to use machine learning (and “AI”, for whatever that vague term is worth) in government is how it fits with the rule of law-type obligations that public bodies have to make decisions that are fair, unbiased, explainable, and compatible with human rights. Keep a beady eye out for the judge over your shoulder. How exactly one makes technology that sits well with these obligations is a matter on which a former Prime Minister could potentially impart some insight, and on which this report is remarkably quiet. The only real attempt to do so is framed around privacy. Privacy is important, but it’s only one of a number of policy concerns that really need a decent answer.\n\nAs with bold blockchain pronouncements and other tech hype, every experiment or prototype gets magically transformed by an army of consultants from “we’re kinda looking at it a bit” to “we’re trying it” to “we’re using it”, and then on to “it definitely works” and “it’s the greatest damn thing since the invention of the wheel”. One of the consequences of TED-style dilletantism and “naive wonderment”—of which there is a lot in the political and business leadership class of this country—is the perception that real problems can only be solved with the new and sexy and exciting.\n\nMeanwhile, the practicalities remain in short supply. Where are all those AI experts lurking in every Whitehall department to build citizen-facing chatbots and data platforms going to come from? Tech hiring is hard even when the UK university sector wasn’t facing a funding crisis that borders on existential. You can have an incredibly clever AI triage system in A&E, but if you don’t have the doctors, nurses and hospital beds to actually send them to, you’re spending a lot of money reshuffling the order of queues where hundreds of thousands of people are now waiting over 12 hours, rather than reducing the queue by actually treating them.\n\nBefore building the next magical mystery machine that will totally fix everything, ask yourself “yeah, but will it actually do that though?” If the case for it consists of crappy paid-for online surveys, prototypes/experiments that have been puffed up into dead-cert successes, and either silence or hand-waving on how one resolves the actual difficult practical problems, tread with considerable caution. And be aware that actual technologists are a hell of a lot more cautious in their claims and promises than the business and political leaders they work for.", "label": "non_personal"}
{"title": "AI transformations for sustainability", "url": "https://blogs.microsoft.com/on-the-issues/2025/01/16/ai-transformations-for-sustainability/", "content": "Today, Microsoft published a new paper, Accelerating Sustainability with AI: Innovations for a Better Future. You can read the foreword below and explore the paper in its entirety.\n\nThroughout history, societal transformations have been driven by the emergence of general-purpose technologies that reshaped entire economies, industries, and ways of life.\n\nThe steam engine, the printing press, electricity, and the internet have each marked pivotal social and economic shifts, leading to lasting changes in how we live and work. Today, AI stands as the latest—and potentially most powerful—general-purpose technology, offering an unprecedented opportunity to drive the societal transformations we urgently need to achieve the world’s sustainability goals.\n\nIn 2023, we published Accelerating Sustainability with AI: A Playbook, in which we highlighted that AI has three game-changing capabilities that make it an essential tool for accelerating sustainability. AI can enhance our ability to predict and optimize complex systems, accelerate the development and deployment of sustainable solutions, and empower the workforce to learn and achieve more—equipping society with the means to drive sustainability progress at a speed and scale previously beyond reach.\n\nOver the last year, we have seen the potential of AI for sustainability in action, empowering the world with new tools for tackling the climate crisis and sustainability challenges more broadly. For example, earlier this year, Microsoft collaborated with Pacific Northwest National Laboratory to use AI in discovering a new battery material requiring less lithium—a breakthrough achieved in weeks rather than the years that traditional research and development would have required. Reducing lithium dependence is crucial to decarbonization, as global demand for lithium is projected to outpace supply, potentially limiting the growth of the energy storage systems needed for the shift to electrification and renewable energy.\n\nAI’s transformative capabilities extend far beyond sustainability, the world has an opportunity to harness AI to enhance both productivity and prosperity. By enabling smarter resource use, optimizing systems for efficiency, and fostering innovations in carbon-free energy and conservation, the AI economy also has the potential to advance both economic growth and environmental stewardship.\n\n\n\nAt Microsoft, we believe the world needs AI that is broadly accessible and trustworthy. This includes addressing the sustainability challenges associated with this technology. The five plays outlined in our AI and sustainability playbook reflect the targeted actions needed to unlock the full potential of AI for accelerating sustainability progress globally.\n\nAcross our sustainability work, we regularly assess our progress and adjust our strategies for greater impact. One lesson from this last year is that minimizing the sustainability impact of AI operations requires more than minimizing resource use in datacenter operations; it also requires supporting the communities where datacenters are located and expanding access to zero-carbon electricity. Global electricity demand is growing rapidly, at an estimated average annual rate of 3–4%. While AI currently consumes less than 0.3% of global electricity demand—and, according to the International Energy Agency (IEA), is expected to remain a small portion in the decade ahead—rapid growth in certain regions can strain local grids.\n\nIn light of these realities, we have updated the third play of our playbook to include enhancing access to carbon-free energy on electricity grids and supporting local communities where we operate datacenters. In support of these expanded goals, we are expanding our effort to build and operate digital infrastructure that addresses societal challenges and creates benefits for communities.\n\nThis report highlights Microsoft’s innovations and actions to advance each of the five plays. Examples of our efforts across the five plays include:\n\nPlay 1: Invest in AI for sustainability\n\nMicrosoft is investing in building AI tools, such as MatterGen and MatterSim, which enable researchers to design and test materials with tenfold greater accuracy and significantly faster performance, while also predicting global weather and atmospheric processes with increased accuracy and at speeds up to 5,000 times greater than current forecasting systems. We are also building AI-enabled tools to empower stakeholders to more effectively and efficiently manage agriculture and water resources and to expedite the licensing process for carbon-free electricity.\n\nPlay 2: Develop digital and data infrastructure for the inclusive use of AI for sustainability\n\nWe are creating tools to fill critical data gaps, which can enhance AI models for better measuring and predicting complex systems such as biodiversity and climate. For instance, SPARROW captures images and acoustic recordings to gather data on biodiversity and ecosystem health in remote areas. Additionally, we are partnering with G42 on a $1 billion digital ecosystem initiative in Kenya.\n\nPlay 3: Minimize resource use, expand access to carbon-free electricity, and support local communities\n\nMicrosoft is innovating datacenter development with low-carbon materials like cross-laminated timber. Through an agreement with Brookfield, we aim to add 10.5 gigawatts (GW) of renewable energy to the grid.\n\nPlay 4: Advance AI policy principles and governance for sustainability\n\nWe advocated for policies that accelerate grid decarbonization, including Federal Energy Regulatory Commission (FERC) transmission rules and provisions in the Inflation Reduction Act in the United States. In addition, we continue to advance AI governance within Microsoft and globally.\n\nPlay 5: Build workforce capacity to use AI for sustainability\n\nMicrosoft Philanthropies’ Skills for Social Impact program trained over 14 million people in digital and AI skills to support a workforce ready to deploy AI for sustainability. As the window for achieving global sustainability goals narrows, the urgency for action intensifies. The world needs every tool at its disposal, and the potential of AI to accelerate sustainability is already being realized. Sustainability is not a journey that can be taken alone, and unlocking the full potential of AI for climate progress requires continued partnerships to combine expertise, technology, and innovation. As we continue to explore the ways AI can advance sustainability, we invite others to join us in this journey.\n\nRead the full report at https://aka.ms/AcceleratingSustainabilitywithAI2025\n\nTags: AI, Environmental Sustainability, Innovation, Responsible AI, sustainability, Workforce", "label": "non_personal"}
{"title": "TIL: Setting default browser on macOS using Nix", "url": "https://tommorris.org/posts/2024/til-setting-default-browser-on-macos-using-nix/", "content": "Let’s say you’ve just switched browser. If you’re a normal person, the new browser will probably ask you whether you want to change your default browser. You’ll click the little button and it’ll happen. Or you go into System Preferences and do it.\n\nNow you can stop reading. But if you’re weird and want to set it programatically on macOS, here’s a hacky way to do it using Nix-Darwin.\n\nIt uses defaultbrowser, which is a teeny little CLI tool that calls the relevant Objective-C functions.\n\nIf you run defaultbrowser without an argument, it shows you what browsers you have installed. You then pass one of those browser identifier strings to it as an argument.\n\nSome options for known browsers: safari , chrome , firefox , librewolf , torbrowser .\n\nNow let’s put it in a Nix-Darwin setup.\n\nlet vars = { # ... defaultbrowser = \"librewolf\"; }; in # ... your nix-darwin setup\n\ndefaultbrowser needs to be installed as a system package, so let’s add it to environment.systemPackages\n\nenvironment.systemPackages = import ./modules/packages.nix pkgs ++ (if (vars ? \"defaultbrowser\" && builtins.isString vars.defaultbrowser) then [ pkgs.defaultbrowser ] else [ ]);\n\nNow we need an activation script that’ll run it every time you run your Nix setup (which will be relatively often).\n\nsystem.activationScripts = (if (vars ? \"defaultbrowser\" && builtins.isString vars.defaultbrowser) then { postUserActivation.text = \"defaultbrowser ${vars.defaultbrowser}\"; } else { });\n\ndefaultbrowser is clever enough that if you tell it to set your default browser to one that already is your default browser, it’ll just print a nice message to the screen like:\n\nlibrewolf is already set as the default HTTP handler\n\nI thought about adding the variable to the list of packages (or Homebrew Casks, or Mac App Store apps) that need installing, but there’s a mismatch between the browser identifier string and the package name used by nixpkgs and/or Homebrew. Plus if you’re using Safari, there’s kind of no way to not install it. So you need to make sure you’ve installed the browser, and check the browser identifier string with defaultbrowser .", "label": "non_personal"}
{"title": "TIL: Wikidata SPARQL trick - getting item and subclasses", "url": "https://tommorris.org/posts/2024/til-wikidata-sparql-getting-item-and-subclasses/", "content": "If you are using the Wikidata Query Service to see how data is structured in Wikidata, one frequent query you might want to do is as follows.\n\nCount the number of items which are an instance of a subclass of X, or an instance of X itself.\n\nThis is useful as you can see roughly the structure of how objects are classified.\n\nThe following query answers half of the query above (replace THING with the item you’re interested in): count the number of items which are an instance of subclass of X.\n\nSELECT DISTINCT ?category ?categoryLabel (COUNT (DISTINCT ?item) AS ?count) WHERE { ?category wdt:P279 wd:THING . ?item wdt:P31 ?category . SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\". } } GROUP BY ?category ?categoryLabel ORDER BY DESC(?count) LIMIT 100\n\nBut we want to bind ?category to include the thing itself as well as the subclasses. Barber paradox? Who gives a damn?\n\nAn easy but hacky way of binding ?category to the thing itself? UNION it together with a sitelink.\n\nSELECT DISTINCT ?category ?categoryLabel (COUNT (DISTINCT ?item) AS ?count) WHERE { { ?category wdt:P279 wd:THING . } UNION { <https://en.wikipedia.org/wiki/ARTICLE_ABOUT_THING> schema:about ?category . } ?item wdt:P31 ?category . SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\". } } GROUP BY ?category ?categoryLabel ORDER BY DESC(?count) LIMIT 100\n\nSome helpful person might go and change the name of the Wikipedia article about the thing, so these kinds of queries might break. (But they might go edit Wikidata too. C’est la vie.) You could always find another statement where the object of the statement uniquely picks the thing out.", "label": "non_personal"}
{"title": "The value of AI: How Microsoft’s customers and partners are creating differentiated AI solutions to reinvent how they do business today", "url": "https://blogs.microsoft.com/blog/2025/01/28/the-value-of-ai-how-microsofts-customers-and-partners-are-creating-differentiated-ai-solutions-to-reinvent-how-they-do-business-today/", "content": "Organizational leaders in every industry around the world are evaluating ways AI can unlock opportunities, drive pragmatic innovation and yield value across their business. At Microsoft, we are dedicated to helping our customers accelerate AI Transformation by empowering human ambition with Copilots and agents, developing differentiated AI solutions and building scalable cybersecurity foundations. At Microsoft Ignite we made over 100 announcements that bring the latest innovation directly to our customers and partners, and shared how Microsoft is the only technology leader to offer three distinct AI platforms for them to build AI solutions:\n\nCopilot is your UI for AI, with Copilot Studio enabling low-code creation of agents and extensibility to your data. Azure AI Foundry is the only AI app server for building real-world, world-class, AI-native applications. Microsoft Fabric is the AI data platform that provides one common way to reason over your data —no matter where it lives.\n\nAll three of these platforms are open and work synchronously to enable the development of modern AI solutions; and each is surrounded by our world-class security offerings so leaders can move their AI-first strategies forward with confidence.\n\nAs we look ahead to what we can achieve together, I remain inspired by the work we are doing today. Below are a handful of the many stories from the past quarter highlighting the differentiated AI solutions our customers and partners are driving to move business forward across industries and realize pragmatic value. Their success clearly illustrates that real results can be harnessed from AI today, and it is changing the way organizations do business.\n\nTo power its industrial IoT and AI platform, ABB Group leveraged Microsoft Azure OpenAI Service to create Genix Copilot: a generative AI-powered analytics suite aimed at solving some of the most complex industrial problems. The solution helps customers analyze key functions in their operations —such as asset and process performance, energy optimization and emission monitoring — with real-time operational insights. As a result, customers are seeing up to 35% savings in operations and maintenance, and up to 20% improvement in energy and emission optimization. ABB also saw an 80% decrease in service calls with the self-service capabilities of Genix Copilot.\n\nServing government healthcare agencies across the US, Acentra Health turned to Microsoft to help introduce the latest AI capabilities that maximize talent and cut costs in a secure, HIPAA-compliant manner. Using Azure OpenAI Service, the company developed MedScribe — an AI-powered tool reducing the time specially trained nursing staff spend on appeal determination letters. This innovation saved 11,000 nursing hours and nearly $800,000, reducing time spent on each appeal determination letter by about 50%. MedScribe also significantly enhanced operational efficiency, enabling nurses to process 20 to 30 letters daily with a 99% approval rate.\n\nTo ease challenges for small farmers, Romanian agribusiness group Agricover revolutionized access to credit by developing MyAgricover. Built with help from partner Avaelgo, the scalable digital platform utilizes Microsoft Azure, Azure API Management and Microsoft Fabric to automate the loan process and enable faster approvals and disbursements. This has empowered small farmers to grow their businesses and receive faster access to financing by reducing loan approval time by 90 percent — from 10 working days to a maximum of 24 hours.\n\nBuilding on its status as a world-class airline with a strong Indian identity, Air India sought ways to enhance customer support while managing costs. By developing AI.g, one of the industry’s first generative AI virtual assistants built on Azure OpenAI Service, the airline upgraded the customer experience. Today, 97% of customer queries are handled with full automation, resulting in millions of dollars of support costs saved and improved customer satisfaction — further positioning the airline for continued growth.\n\nBMW Group aimed to enhance data delivery efficiency and improve vehicle development and prototyping cycles by implementing a Mobile Data Recorder (MDR) solution with Azure App Service, Azure AI and Azure Kubernetes Service (AKS). The solution achieved 10 times more efficient data delivery, significantly improved data accessibility and elevated overall development quality. The MDR monitors and records more than 10,000 signals twice per second in every vehicle of BMW’s fleet of 3,500 development cars and transmits data within seconds to a centralized cloud back end. Using Azure AI Foundry and Azure OpenAI Service, BMW Group created an MDR copilot fueled by GPT-4o. Engineers can now chat with the interface using natural language, and the MDR copilot converts the conversations into KQL queries, simplifying access to technical insights. Moving from on-premises tools to a cloud-based system with faster data management also helps engineers troubleshoot in real time. The vehicle data covered by the system has doubled, and data delivery and analysis happen 10 times faster.\n\nColes Group modernized its logistics and administrative applications using Microsoft Azure Stack HCI to scale its edge AI capabilities and improve efficiency and customer experience across its 1,800 stores. By expanding its Azure Stack HCI footprint from two stores to over 500, Coles achieved a six-fold increase in the pace of application deployment, significantly enhancing operational efficiency and enabling rapid innovation without disrupting workloads. The retailer is also using Azure Machine Learning to train and develop edge AI models, speeding up data annotation time for training models by 50%.\n\nMultinational advertising and media company Dentsu wanted to speed time to insights for its team of data scientists and media analysts to support its media planning and budget optimization. Using Microsoft Azure AI Foundry and Azure OpenAI Service, Dentsu developers built a predictive analytics copilot that uses conversational chat and draws on deep expertise in media forecasting, budgeting and optimization. This AI-driven tool has reduced time to media insights for employees and clients by 90% and cut analysis costs.\n\nTo overcome the limitations of its current systems, scale operations and automate processes across millions of workflows, Docusign created the Intelligent Agreement Management (IAM) platform on Azure. Using Azure AI, Azure Cosmos DB, Azure Logic Apps and AKS, the platform transforms agreement data into actionable insights to enhance productivity and accelerate contract review cycles. IAM also ensures better collaboration and unification across business systems to provide secure solutions tailored to diverse customer needs. For example, its customer KPC Private funds reported a 70% reduction in time and resources dedicated to agreement processes.\n\nEmirates Global Aluminium (EGA) transformed its manufacturing operations by leveraging a hybrid environment with Azure Arc, Azure Stack HCI and Azure Kubernetes Service. This digital manufacturing platform resulted in 86% cost savings for AI image and video analytics and a 13-fold improvement in AI response times. The seamless hybrid cloud architecture has enhanced EGA’s operational efficiency and agility, supporting its Industry 4.0 transformation strategy.\n\nEY collaborated with Microsoft to enhance the inclusivity of AI development using Azure AI Foundry. By involving neurodivergent technologists from EY’s Neuro-Diverse Centers of Excellence, they improved the accessibility and productivity of AI tools, resulting in more inclusive AI solutions, fostering innovation and ensuring that AI tools unlock the potential of all users. With an estimated 20% of the global workforce identifying as neurodivergent, inclusive AI solutions are crucial for maximizing creativity and productivity. Neurodivergent EY technologists also collaborated with Microsoft developers to make Azure AI Foundry more inclusive and help all users work productively to create innovative AI solutions.\n\nColombian household appliance manufacturer Haceb integrated AI to optimize processes, reduce costs and improve service quality. Using Microsoft Copilot Studio and Azure OpenAI Service, the company created a virtual technical support assistant, saving its 245 technicians 5 minutes per visit — a total of 5,000 minutes saved daily. This AI solution has enhanced efficiency and boosted customer satisfaction by allowing for faster issue resolution. Haceb’s AI adoption has also empowered employees, boosted productivity and positioned the company as a leader in AI innovation in Colombia.\n\nTo better serve its global patients, Operation Smile — in collaboration with partner Squadra — leveraged Azure AI, Machine Learning and Microsoft Fabric to develop an AI-powered solution to predict surgical outcomes and optimize resource allocation. This innovation resulted in a 30% increase in surgical efficiency, a 90% reduction in translation errors and improved patient outcomes. Additionally, report generation is now up to 95% quicker, and repeated medical events have decreased by 15%, enabling Operation Smile to provide better care to more children worldwide.\n\nOntada — a McKesson business dedicated to oncology data and evidence, clinical education and point-of-care technologies — needed a way to generate key insights across 150 million unstructured oncology documents. Using Microsoft Azure AI and Azure OpenAI Service, Ontada developed a data platform solution called ON.Genuity to provide AI-driven insights into the patient journey, enhance patient trial matching and identify care gaps. The company also implemented large language models to target nearly 100 critical oncology data elements across 39 cancer types, enabling the company to analyze an estimated 70% of previously inaccessible data, reduce processing time by 75% and accelerate product time-to-market from months to just one week.\n\nAs the UK’s largest pet care company, Pets at Home sought a way to combat fraud across its retail operations — particularly as its online business continued to grow. Working closely with its fraud team, it adopted Copilot Studio to develop an AI agent that quickly identifies suspicious transactions. The agent autonomously gathers relevant information, performs analysis and shares it with a fraud agent to enable a manual, data-intensive investigative process while ensuring a human remains in the loop. With this low-code agent extending and seamlessly integrating into existing systems, the company’s fraud department can act more quickly; what used to take 20 to 30 minutes is now handled by the AI agent within seconds. The company is identifying fraud 10 times faster and is processing 20 times more cases a day. Now, the company can operate at scale with speed, efficiency and accuracy — with savings expected to be in the seven figures as it continues to build more agents.\n\nRevenue Grid, a technology company specializing in sales engagement and revenue optimization solutions, partnered with Cloud Services to modernize its data infrastructure and develop a unified data warehouse capable of handling unstructured, semi-structured and structured data. By migrating to Microsoft Fabric, Revenue Grid can now deliver data-powered revenue intelligence, driven by a unified platform, elastic scalability, enhanced analytics capabilities and streamlined operations. Revenue Grid has reduced infrastructure costs by 60% while enhancing its analytical capabilities to improve real-time data processing, empowering sales teams with accurate and diverse data.\n\nTo better manage and integrate employee data across diverse regions and systems, UST built a comprehensive Employee Data platform on Microsoft Fabric. In under a year, UST migrated 20 years of employee data with all security measures to enhance data accessibility and employee productivity. The Meta Data Driven Integration (MDDI) framework in Fabric also helped the company cut data ingestion time by 50% so employees can focus more on analysis than preparation. As a result of this implementation, the company has seen an increase in collaboration and innovation from employees, helping put its values into action.\n\nThe Microsoft Commercial Marketplace offers millions of customers worldwide a convenient place to find, try and buy software and services across 140 countries. As a Marketplace partner, WeTransact is helping independent software vendors (ISVs) list and transact their software solutions — and find opportunities for co-selling and extending their reach to enterprise customers through development of the WeTransact platform. Powered by Azure OpenAI Service, the platform is changing the way partnerships are being built by using AI pairing to facilitate a “plug and play” reseller network. More than 300 ISVs worldwide have joined the Microsoft Commercial Marketplace using the WeTransact platform, cutting their time to publish by 75%.\n\nThe opportunity for AI to create value is no longer an ambition for the future — it is happening now, and organizational leaders across industries are investing in AI-first strategies to change the way they do business. We believe AI should empower human achievement and enrich the lives of employees; and we are uniquely differentiated to help you accelerate your AI Transformation responsibly and securely. Choosing the right technology provider comes down to trust, and I look forward to what we will achieve together as we partner with you on your AI journey.\n\nTags: AI, Azure, Azure AI, Azure AI Foundry, Azure Arc, Azure OpenAI Service, Azure Stack HCI, Copilot, Copilot Studio, Microsoft Fabric, Microsoft Ignite 2024", "label": "non_personal"}
{"title": "A new level unlocked", "url": "https://blogs.microsoft.com/blog/2025/02/19/a-new-level-unlocked/", "content": "Muse, the first World and Human Action Model, could facilitate interdisciplinary collaboration, for example, when exploring gameplay ideas.\n\nToday Microsoft released Muse, a first-of-its-kind generative AI model that we are applying to gaming. But it’s so much more than that. What we’re sharing today is a huge step forward for gameplay ideation. And what’s even more exciting is what this breakthrough represents in our journey of building and using generative AI, and what industries, developers and creators of all interests will be enabled to do next.\n\nThe impressive abilities we first witnessed with ChatGPT and GPT-4 to learn human language are now being matched by AI’s abilities to learn the mechanics of how things work, in effect developing a practical understanding of interactions in the world. As a computer scientist, this ability to understand and model a 3D world is something I and many other great researchers have pursued for over 10 years and, personally, I was not sure that it could be made possible with such speed and quality.\n\nIn the case of Muse, just from observing human gameplay, this model develops a deep understanding of the environment, including its dynamics and how it evolves over time in response to actions. This unlocks the ability to rapidly iterate, remix and create in video games so developers can eventually create immersive environments and unleash their full creativity.\n\nBeyond gaming, I’m excited by the potential of this capability to enable AI assistants that understand and help visualize things, from reconfiguring the kitchen in your home to redesigning a retail space to building a digital twin of a factory floor to test and explore different scenarios. All these things are just now becoming possible with AI. From the perspective of computer science research, it’s pretty amazing, and the future applications of this are likely to be transformative for creators.\n\n—\n\nAt Microsoft, we have a long history of collaboration between research and engineering. Today, as we release Muse, we are also announcing Azure AI Foundry Labs, where the AI community can explore the latest from Microsoft Research. Azure AI Foundry Labs will help accelerate the transition from research to solutions, bringing new ideas to the broader community to help shape the future of AI. Learn more.\n\nTags: AI, Azure AI Foundry Labs, ChatGPT, GPT-4", "label": "non_personal"}
{"title": "Microsoft Build 2025: The age of AI agents and building the open agentic web", "url": "https://blogs.microsoft.com/blog/2025/05/19/microsoft-build-2025-the-age-of-ai-agents-and-building-the-open-agentic-web/", "content": "TL;DR? Hear the news as an AI-generated audio overview made using Microsoft 365 Copilot. You can read the transcript here.\n\nWe’ve entered the era of AI agents. Thanks to groundbreaking advancements in reasoning and memory, AI models are now more capable and efficient, and we’re seeing how AI systems can help us all solve problems in new ways.\n\nFor example, 15 million developers are already using GitHub Copilot, and features like agent mode and code review are streamlining the way they code, check, deploy and troubleshoot.\n\nHundreds of thousands of customers are using Microsoft 365 Copilot to help research, brainstorm and develop solutions, and more than 230,000 organizations — including 90% of the Fortune 500 — have already used Copilot Studio to build AI agents and automations.\n\nCompanies like Fujitsu and NTT DATA are using Azure AI Foundry to build and manage AI apps and agents that help prioritize sales leads, speed proposal creation and surface client insights. Stanford Health Care is using Microsoft’s healthcare agent orchestrator to build and test AI agents that can help alleviate the administrative burden and speed up the workflow for tumor board preparation.\n\nDevelopers are at the center of it all. For 50 years Microsoft has been empowering developers with tools and platforms to turn their ideas into reality, accelerating innovation at every stage. From AI-driven automation to seamless cloud integration and more, it’s exciting to see how developers are fueling the next generation of digital transformation.\n\nSo, what’s next?\n\nWe envision a world in which agents operate across individual, organizational, team and end-to-end business contexts. This emerging vision of the internet is an open agentic web, where AI agents make decisions and perform tasks on behalf of users or organizations.\n\nAt Microsoft Build we’re showing the steps we’re taking to make this vision a reality through our platforms, products and infrastructure. We’re putting new models and coding agents in the hands of developers, introducing enterprise-grade agents, making our platforms like Azure AI Foundry, GitHub and Windows the best places to build, embracing open protocols and accelerating scientific discovery with AI, all so that developers and organizations can go invent the next big thing.\n\nHere’s a glimpse at just a few of the announcements today:\n\nReimagining the software development lifecycle with AI\n\nAI is fundamentally shifting how code is written, deployed and maintained. Developers are using AI to stay in the flow of their environment longer and to shift their focus to more strategic tasks. And as the software development lifecycle is being transformed, we’re providing new features across platforms including GitHub, Azure AI Foundry and Windows that enable developers to work faster, think bigger and build at scale.\n\nGitHub Copilot coding agent and new updates to GitHub Models: GitHub Copilot is evolving from an in-editor assistant to an agentic AI partner with a first-of-its-kind asynchronous coding agent integrated into the GitHub platform. We’re adding prompt management, lightweight evaluations and enterprise controls to GitHub Models so teams can experiment with best-in-class models, without leaving GitHub. Microsoft is also open-sourcing GitHub Copilot Chat in VS Code. The AI-powered capabilities from GitHub Copilot extensions will now be part of the same open-source repository that drives the world’s most popular development tool. As the home of over 150 million developers, this reinforces our commitment to open, collaborative, AI-powered software development. Learn more about GitHub Copilot updates.\n\nIntroducing Windows AI Foundry: For developers, Windows remains one of the most open and widely used platforms available, with scale, flexibility and growing opportunity. Windows AI Foundry offers a unified and reliable platform supporting the AI developer lifecycle across training and inference. With simple model APIs for vision and language tasks, developers can manage and run open source LLMs via Foundry Local or bring a proprietary model to convert, fine-tune and deploy across client and cloud. Windows AI Foundry is available to get started today. To learn more visit our Windows Developer Blog.\n\nAzure AI Foundry Models and new tools for model evaluation: Azure AI Foundry is a unified platform for developers to design, customize and manage AI applications and agents. With Azure AI Foundry Models, we’re bringing Grok 3 and Grok 3 mini models from xAI to our ecosystem, hosted and billed directly by Microsoft. Developers can now choose from more than 1,900 partner-hosted and Microsoft-hosted AI models, while managing secure data integration, model customization and enterprise-grade governance. We’re also introducing new tools like the Model Leaderboard, which ranks the top-performing AI models across different categories and tasks, and the Model Router, designed to select an optimal model for a specific query or task in real-time. Read more about Azure AI Foundry Models.\n\nMaking AI agents more capable and secure\n\nAI agents are not only changing how developers build, but how individuals, teams and companies get work done. At Build, we’re unveiling new pre-built agents, custom agent building blocks, multi-agent capabilities and new models to help developers and organizations build and deploy agents securely to help increase productivity in meaningful ways.\n\nWith the general availability of Azure AI Foundry Agent Service, Microsoft is bringing new capabilities to empower professional developers to orchestrate multiple specialized agents to handle complex tasks, including bringing Semantic Kernel and AutoGen into a single, developer-focused SDK and Agent-to-Agent (A2A) and Model Context Protocol (MCP) support. To help developers build trust and confidence in their AI agents, we’re announcing new features in Azure AI Foundry Observability for built-in observability into metrics for performance, quality, cost and safety, all incorporated alongside detailed tracing in a streamlined dashboard. Learn more about how to deploy enterprise-grade AI agents in Azure AI Foundry Service.\n\nDiscover, protect and govern in Azure AI Foundry: With Microsoft Entra Agent ID, now in preview, agents that developers create in Microsoft Copilot Studio or Azure AI Foundry are automatically assigned unique identities in an Entra directory, helping enterprises securely manage agents right from the start and avoid “agent sprawl” that could lead to blind spots. Apps and agents built with Foundry further benefit from Purview data security and compliance controls. Foundry also offers enhanced governance tools to set risk parameters, run automated evaluations and receive detailed reports. Learn more about Microsoft Entra Agent ID and Azure AI Foundry integrations with Microsoft Purview Compliance Manager.\n\nIntroducing Microsoft 365 Copilot Tuning and multi-agent orchestration: With Copilot Tuning, customers can use their own company data, workflows and processes to train models and create agents in a simple, low-code way. These agents perform highly accurate, domain-specific tasks securely from within the Microsoft 365 service boundary. For example, a law firm can create an agent that generates documents aligned with its organization’s expertise and style. Additionally, new multi-agent orchestration in Copilot Studio connects multiple agents, allowing them to combine skills and tackle broader, more complex tasks. Check out the Microsoft 365 blog to learn how to access these new tools as well as the Microsoft 365 Copilot Wave 2 spring release, which has moved to general availability and begins rolling out today.\n\nSupporting the open agentic web\n\nTo realize the future of AI agents, we’re advancing open standards and shared infrastructure to provide unique capabilities for customers.\n\nSupporting Model Context Protocol (MCP): Microsoft is delivering broad first-party support for Model Context Protocol (MCP) across its agent platform and frameworks, spanning GitHub, Copilot Studio, Dynamics 365, Azure AI Foundry, Semantic Kernel and Windows 11. In addition, Microsoft and GitHub have joined the MCP Steering Committee to help advance secure, at-scale adoption of the open protocol and announced two new contributions to the MCP ecosystem, an updated authorization specification, which enables people to use their existing trusted sign-in methods to give agents and LLM-powered apps access to data and services such as personal storage drives or subscription services, and the design of an MCP server registry service, which allows anyone to implement public or private, up-to-date, centralized repositories for MCP server entries. Check out the GitHub repository. As we expand our MCP capabilities, our top priority is to ensure we’re building upon a secure foundation. To learn more about this approach see: Securing the Model Context Protocol: Building a Safe Agentic Future on Windows.\n\nA new open project called NLWeb: Microsoft is introducing NLWeb, which we believe can play a similar role to HTML for the agentic web. NLWeb makes it easy for websites to provide a conversational interface for their users with the model of their choice and their own data, allowing users to interact directly with web content in a rich, semantic manner. Every NLWeb endpoint is also an MCP server, so websites can make their content easily discoverable and accessible to AI agents if they choose. Learn more here.\n\nAccelerating scientific discovery with AI\n\nScience may be one of the most important applications of AI, helping to tackle humanity’s most pressing challenges, from drug discovery to sustainability. At Build we’re introducing Microsoft Discovery, an extensible platform built to empower researchers to transform the entire discovery process with agentic AI, helping research and development departments across various industries accelerate the time to market for new products and accelerate and expand the end-to-end discovery process for all scientists. Learn more here.\n\nThis is only a small selection of the many exciting features and updates we will be announcing at Build. We’re looking forward to connecting with those who have registered to join us virtually and in-person, for keynote sessions, live code deep dives, hack sessions and more — much of which will be available on demand.\n\nPlus, you can get more on all these announcements by exploring the Book of News, the official compendium of all today’s news.\n\nTags: AI, Azure AI, Azure AI Foundry, Book of News, GitHub, GitHub Copilot, Microsoft 365 Copilot, Microsoft Copilot, Microsoft Purview", "label": "non_personal"}
{"title": "Introducing Azure AI Foundry Labs: A hub for the latest AI research and experiments at Microsoft", "url": "https://azure.microsoft.com/en-us/blog/introducing-azure-ai-foundry-labs-a-hub-for-the-latest-ai-research-and-experiments-at-microsoft/", "content": "We’re thrilled to announce the launch of Azure AI Foundry Labs, a hub for developers, startups, and enterprises to explore groundbreaking innovations from research at Microsoft.\n\nToday we’re launching Azure AI Foundry Labs, a hub for developers, startups, and enterprises to explore groundbreaking innovations from research at Microsoft. Foundry Labs unites cutting-edge research with real-world applications, to enable developers and creators across industries to discover new possibilities, solve complex problems, and share insights to shape the future of AI.\n\nMicrosoft’s newest AI breakthrough—Muse, a first-of-its-kind World and Human Action Model (WHAM), available today in Azure AI Foundry—is the latest example of bringing cutting-edge research innovation to our AI platform for customers to use.\n\nWith Azure AI Foundry Labs, we’re excited to unveil new assets for our latest research-driven projects that empower developers to explore, engage, and experiment. Projects across models and agentic frameworks include:\n\nAurora: A large-scale atmospheric model providing high-resolution weather forecasts and air pollution predictions, outperforming traditional tools.\n\nExACT: An open-source project enabling agents to learn from past interactions and improve search efficiency dynamically.\n\nMagentic-One: A multi-agent system solving complex problems by orchestrating multiple agents, built on the AutoGen framework.\n\nMatterSim: A deep learning model for atomistic simulations, predicting material properties with high precision.\n\nOmniParser v2: A vision-based module converting UI screenshots into structured elements, enhancing agents’ action generation.\n\nTamGen: A generative AI model for drug design, using a GPT-like chemical language model for target-aware molecule generation and refinement.\n\nThen versus now\n\nIn the early days of global positioning systems (GPS) technology, it took roughly a decade for GPS to make its way from specialized, military-grade instruments into everyday consumer use. What started as a niche innovation in the 1970’s didn’t become truly mainstream until the late 1990’s and early 2000’s, when GPS receivers became standard features in cars, cell phones, and handheld devices. Ten years might sound like a reasonable adoption curve—until you look at how quickly innovations are moving in AI today.\n\nIn recent years, the pace of AI advancement has accelerated dramatically. We’ve witnessed a shift from unveiling a new model every 4–6 months to releasing breakthroughs every 4–6 days. The amount of compute used for training AI models has grown 10 times every 12 months, turbocharging both research and commercialization. And time-to-product from foundational research to full-scale product deployment has gone from years to months.\n\nAt this velocity, ideas and prototypes need to be iterated upon, validated, and deployed faster than ever before. This rapid evolution demands new thinking in how we bridge research and application.\n\nAccelerating research to impact\n\nAzure AI Foundry Labs highlights the long-term collaboration between research and engineering teams at Microsoft and provides a single access point for developers and the broader AI community to experiment with new models, explore the latest frameworks, and be at the forefront of innovation. Developers can create prototypes using experimental research in Azure AI Foundry Labs, collaborate with researchers and engineering teams by sharing feedback, and help speed up the time to market for some of the most promising technologies.\n\nThe next chapter\n\nThe gap between breakthrough and impact has never been smaller. What once took years now takes weeks, and what was once confined to research labs now runs on devices in our pockets. Azure AI Foundry Labs exists to collapse this gap even further—to ensure that every breakthrough in AI research finds its way to the developers, creators, and innovators who can transform it into real-world impact.\n\nAzure AI Foundry Labs Bridging research and application. Discover more\n\nThis isn’t just about sharing research—it’s about accelerating the cycle of innovation itself. Whether you’re a developer, researcher, startup founder, or enterprise builder, Azure AI Foundry Labs gives you direct access to the bleeding edge of AI advancement. The tools and models available today are just the beginning.\n\nVisit Azure AI Foundry Labs to start building the future.", "label": "non_personal"}
{"title": "Video annotator: a framework for efficiently building video classifiers using vision-language models and active learning", "url": "https://netflixtechblog.com/video-annotator-building-video-classifiers-using-vision-language-models-and-active-learning-8ebdda0b2db4?source=collection_home---4------23-----------------------", "content": "Video annotator: a framework for efficiently building video classifiers using vision-language models and active learning Netflix Technology Blog 6 min read · Jun 19, 2024 -- 2 Listen Share\n\nAmir Ziai, Aneesh Vartakavi, Kelli Griggs, Eugene Lok, Yvonne Jukes, Alex Alonso, Vi Iyengar, Anna Pulido\n\nIntroduction\n\nProblem\n\nHigh-quality and consistent annotations are fundamental to the successful development of robust machine learning models. Conventional techniques for training machine learning classifiers are resource intensive. They involve a cycle where domain experts annotate a dataset, which is then transferred to data scientists to train models, review outcomes, and make changes. This labeling process tends to be time-consuming and inefficient, sometimes halting after a few annotation cycles.\n\nImplications\n\nConsequently, less effort is invested in annotating high-quality datasets compared to iterating on complex models and algorithmic methods to improve performance and fix edge cases. As a result, ML systems grow rapidly in complexity.\n\nFurthermore, constraints on time and resources often result in leveraging third-party annotators rather than domain experts. These annotators perform the labeling task without a deep understanding of the model’s intended deployment or usage, often making consistent labeling of borderline or hard examples, especially in more subjective tasks, a challenge.\n\nThis necessitates multiple review rounds with domain experts, leading to unexpected costs and delays. This lengthy cycle can also result in model drift, as it takes longer to fix edge cases and deploy new models, potentially hurting usefulness and stakeholder trust.\n\nSolution\n\nWe suggest that more direct involvement of domain experts, using a human-in-the-loop system, can resolve many of these practical challenges. We introduce a novel framework, Video Annotator (VA), which leverages active learning techniques and zero-shot capabilities of large vision-language models to guide users to focus their efforts on progressively harder examples, enhancing the model’s sample efficiency and keeping costs low.\n\nVA seamlessly integrates model building into the data annotation process, facilitating user validation of the model before deployment, therefore helping with building trust and fostering a sense of ownership. VA also supports a continuous annotation process, allowing users to rapidly deploy models, monitor their quality in production, and swiftly fix any edge cases by annotating a few more examples and deploying a new model version.\n\nThis self-service architecture empowers users to make improvements without active involvement of data scientists or third-party annotators, allowing for fast iteration.\n\nVideo understanding\n\nWe design VA to assist in granular video understanding which requires the identification of visuals, concepts, and events within video segments. Video understanding is fundamental for numerous applications such as search and discovery, personalization, and the creation of promotional assets. Our framework allows users to efficiently train machine learning models for video understanding by developing an extensible set of binary video classifiers, which power scalable scoring and retrieval of a vast catalog of content.\n\nVideo classification\n\nVideo classification is the task of assigning a label to an arbitrary-length video clip, often accompanied by a probability or prediction score, as illustrated in Fig 1.\n\nFig 1- Functional view of a binary video classifier. A few-second clip from ”Operation Varsity Blues: The College Admissions Scandal” is passed to a binary classifier for detecting the ”establishing shots” label. The classifier outputs a very high score (score is between 0 and 1), indicating that the video clip is very likely an establishing shot. In filmmaking, an establishing shot is a wide shot (i.e. video clip between two consecutive cuts) of a building or a landscape that is intended for establishing the time and location of the scene.\n\nVideo understanding via an extensible set of video classifiers\n\nBinary classification allows for independence and flexibility, allowing us to add or improve one model independent of the others. It also has the additional benefit of being easier to understand and build for our users. Combining the predictions of multiple models allows us a deeper understanding of the video content at various levels of granularity, illustrated in Fig 2.\n\nFig 2- Three video clips and the corresponding binary classifier scores for three video understanding labels. Note that these labels are not mutually exclusive. Video clips are from Operation Varsity Blues: The College Admissions Scandal, 6 Underground, and Leave The World Behind, respectively.\n\nVideo Annotator (VA)\n\nIn this section, we describe VA’s three-step process for building video classifiers.\n\nStep 1 — search\n\nUsers begin by finding an initial set of examples within a large, diverse corpus to bootstrap the annotation process. We leverage text-to-video search to enable this, powered by video and text encoders from a Vision-Language Model to extract embeddings. For example, an annotator working on the establishing shots model may start the process by searching for “wide shots of buildings”, illustrated in Fig 3.\n\nFig 3- Step 1 — Text-to-video search to bootstrap the annotation process.\n\nStep 2 — active learning\n\nThe next stage involves a classic Active Learning loop. VA then builds a lightweight binary classifier over the video embeddings, which is subsequently used to score all clips in the corpus, and presents some examples within feeds for further annotation and refinement, as illustrated in Fig 4.\n\nFig 4- Step 2 — Active Learning loop. The annotator clicks on build, which initiates classifier training and scoring of all clips in a video corpus. Scored clips are organized in four feeds.\n\nThe top-scoring positive and negative feeds display examples with the highest and lowest scores respectively. Our users reported that this provided a valuable indication as to whether the classifier has picked up the correct concepts in the early stages of training and spot cases of bias in the training data that they were able to subsequently fix. We also include a feed of “borderline” examples that the model is not confident about. This feed helps with discovering interesting edge cases and inspires the need for labeling additional concepts. Finally, the random feed consists of randomly selected clips and helps to annotate diverse examples which is important for generalization.\n\nThe annotator can label additional clips in any of the feeds and build a new classifier and repeat as many times as desired.\n\nStep 3 — review\n\nThe last step simply presents the user with all annotated clips. It’s a good opportunity to spot annotation mistakes and to identify ideas and concepts for further annotation via search in step 1. From this step, users often go back to step 1 or step 2 to refine their annotations.\n\nExperiments\n\nTo evaluate VA, we asked three video experts to annotate a diverse set of 56 labels across a video corpus of 500k shots. We compared VA to the performance of a few baseline methods, and observed that VA leads to the creation of higher quality video classifiers. Fig 5 compares VA’s performance to baselines as a function of the number of annotated clips.\n\nFig 5- Model quality (i.e. Average Precision) as a function of the number of annotated clips for the “establishing shots” label. We observe that all methods outperform the baseline, and that all methods benefit from additional annotated data, albeit to varying degrees.\n\nYou can find more details about VA and our experiments in this paper.\n\nConclusion\n\nWe presented Video Annotator (VA), an interactive framework that addresses many challenges associated with conventional techniques for training machine learning classifiers. VA leverages the zero-shot capabilities of large vision-language models and active learning techniques to enhance sample efficiency and reduce costs. It offers a unique approach to annotating, managing, and iterating on video classification datasets, emphasizing the direct involvement of domain experts in a human-in-the-loop system. By enabling these users to rapidly make informed decisions on hard samples during the annotation process, VA increases the system’s overall efficiency. Moreover, it allows for a continuous annotation process, allowing users to swiftly deploy models, monitor their quality in production, and rapidly fix any edge cases.\n\nThis self-service architecture empowers domain experts to make improvements without the active involvement of data scientists or third-party annotators, and fosters a sense of ownership, thereby building trust in the system.\n\nWe conducted experiments to study the performance of VA, and found that it yields a median 8.3 point improvement in Average Precision relative to the most competitive baseline across a wide-ranging assortment of video understanding tasks. We release a dataset with 153k labels across 56 video understanding tasks annotated by three professional video editors using VA, and also release code to replicate our experiments.", "label": "non_personal"}
{"title": "Enhancing Netflix Reliability with Service-Level Prioritized Load Shedding", "url": "https://netflixtechblog.com/enhancing-netflix-reliability-with-service-level-prioritized-load-shedding-e735e6ce8f7d?source=collection_home---4------21-----------------------", "content": "Without prioritized load-shedding, both user-initiated and prefetch availability drop when latency is injected. However, after adding prioritized load-shedding, user-initiated requests maintain a 100% availability and only prefetch requests are throttled.\n\nWe were ready to roll this out to production and see how it performed in the wild!\n\nReal-World Application and Results\n\nNetflix engineers work hard to keep our systems available, and it was a while before we had a production incident that tested the efficacy of our solution. A few months after deploying prioritized load shedding, we had an infrastructure outage at Netflix that impacted streaming for many of our users. Once the outage was fixed, we got a 12x spike in pre-fetch requests per second from Android devices, presumably because there was a backlog of queued requests built up.\n\nSpike in Android pre-fetch RPS\n\nThis could have resulted in a second outage as our systems weren’t scaled to handle this traffic spike. Did prioritized load-shedding in PlayAPI help us here?\n\nYes! While the availability for prefetch requests dropped as low as 20%, the availability for user-initiated requests was > 99.4% due to prioritized load-shedding.\n\nAvailability of pre-fetch and user-initiated requests\n\nAt one point we were throttling more than 50% of all requests but the availability of user-initiated requests continued to be > 99.4%.\n\nGeneric service work prioritization\n\nBased on the success of this approach, we have created an internal library to enable services to perform prioritized load shedding based on pluggable utilization measures, with multiple priority levels.\n\nUnlike API gateway, which needs to handle a large volume of requests with varying priorities, most microservices typically receive requests with only a few distinct priorities. To maintain consistency across different services, we have introduced four predefined priority buckets inspired by the Linux tc-prio levels:\n\nCRITICAL : Affect core functionality — These will never be shed if we are not in complete failure.\n\n: Affect core functionality — These will never be shed if we are not in complete failure. DEGRADED : Affect user experience — These will be progressively shed as the load increases.\n\n: Affect user experience — These will be progressively shed as the load increases. BEST_EFFORT : Do not affect the user — These will be responded to in a best effort fashion and may be shed progressively in normal operation.\n\n: Do not affect the user — These will be responded to in a best effort fashion and may be shed progressively in normal operation. BULK: Background work, expect these to be routinely shed.\n\nServices can either choose the upstream client’s priority or map incoming requests to one of these priority buckets by examining various request attributes, such as HTTP headers or the request body, for more precise control. Here is an example of how services can map requests to priority buckets:\n\nResourceLimiterRequestPriorityProvider requestPriorityProvider() {\n\nreturn contextProvider -> {\n\nif (contextProvider.getRequest().isCritical()) {\n\nreturn PriorityBucket.CRITICAL;\n\n} else if (contextProvider.getRequest().isHighPriority()) {\n\nreturn PriorityBucket.DEGRADED;\n\n} else if (contextProvider.getRequest().isMediumPriority()) {\n\nreturn PriorityBucket.BEST_EFFORT;\n\n} else {\n\nreturn PriorityBucket.BULK;\n\n}\n\n};\n\n}\n\nGeneric CPU based load-shedding\n\nMost services at Netflix autoscale on CPU utilization, so it is a natural measure of system load to tie into the prioritized load shedding framework. Once a request is mapped to a priority bucket, services can determine when to shed traffic from a particular bucket based on CPU utilization. In order to maintain the signal to autoscaling that scaling is needed, prioritized shedding only starts shedding load after hitting the target CPU utilization, and as system load increases, more critical traffic is progressively shed in an attempt to maintain user experience.\n\nFor example, if a cluster targets a 60% CPU utilization for auto-scaling, it can be configured to start shedding requests when the CPU utilization exceeds this threshold. When a traffic spike causes the cluster’s CPU utilization to significantly surpass this threshold, it will gradually shed low-priority traffic to conserve resources for high-priority traffic. This approach also allows more time for auto-scaling to add additional instances to the cluster. Once more instances are added, CPU utilization will decrease, and low-priority traffic will resume being served normally.\n\nPercentage of requests (Y-axis) being load-shed based on CPU utilization (X-axis) for different priority buckets\n\nExperiments with CPU based load-shedding\n\nWe ran a series of experiments sending a large request volume at a service which normally targets 45% CPU for auto scaling but which was prevented from scaling up for the purpose of monitoring CPU load shedding under extreme load conditions. The instances were configured to shed noncritical traffic after 60% CPU and critical traffic after 80%.\n\nAs RPS was dialed up past 6x the autoscale volume, the service was able to shed first noncritical and then critical requests. Latency remained within reasonable limits throughout, and successful RPS throughput remained stable.\n\nExperimental behavior of CPU based load-shedding using synthetic traffic.\n\nP99 latency stayed within a reasonable range throughout the experiment, even as RPS surpassed 6x the autoscale target.\n\nAnti-patterns with load-shedding\n\nAnti-pattern 1 — No shedding\n\nIn the above graphs, the limiter does a good job keeping latency low for the successful requests. If there was no shedding here, we’d see latency increase for all requests, instead of a fast failure in some requests that can be retried. Further, this can result in a death spiral where one instance becomes unhealthy, resulting in more load on other instances, resulting in all instances becoming unhealthy before auto-scaling can kick in.\n\nNo load-shedding: In the absence of load-shedding, increased latency can degrade all requests instead of rejecting some requests (that can be retried), and can make instances unhealthy\n\nAnti-pattern 2 — Congestive failure\n\nAnother anti-pattern to watch out for is congestive failure or shedding too aggressively. If the load-shedding is due to an increase in traffic, the successful RPS should not drop after load-shedding. Here is an example of what congestive failure looks like:\n\nCongestive failure: After 16:57, the service starts rejecting most requests and is not able to sustain a successful 240 RPS that it was before load-shedding kicked in. This can be seen in fixed concurrency limiters or when load-shedding consumes too much CPU preventing any other work from being done\n\nWe can see in the Experiments with CPU based load-shedding section above that our load-shedding implementation avoids both these anti-patterns by keeping latency low and sustaining as much successful RPS during load-shedding as before.\n\nGeneric IO based load-shedding\n\nSome services are not CPU-bound but instead are IO-bound by backing services or datastores that can apply back pressure via increased latency when they are overloaded either in compute or in storage capacity. For these services we re-use the prioritized load shedding techniques, but we introduce new utilization measures to feed into the shedding logic. Our initial implementation supports two forms of latency based shedding in addition to standard adaptive concurrency limiters (themselves a measure of average latency):\n\nThe service can specify per-endpoint target and maximum latencies, which allow the service to shed when the service is abnormally slow regardless of backend. The Netflix storage services running on the Data Gateway return observed storage target and max latency SLO utilization, allowing services to shed when they overload their allocated storage capacity.\n\nThese utilization measures provide early warning signs that a service is generating too much load to a backend, and allow it to shed low priority work before it overwhelms that backend. The main advantage of these techniques over concurrency limits alone is they require less tuning as our services already must maintain tight latency service-level-objectives (SLOs), for example a p50 < 10ms and p100 < 500ms. So, rephrasing these existing SLOs as utilizations allows us to shed low priority work early to prevent further latency impact to high priority work. At the same time, the system will accept as much work as it can while maintaining SLO’s.\n\nTo create these utilization measures, we count how many requests are processed slower than our target and maximum latency objectives, and emit the percentage of requests failing to meet those latency goals. For example, our KeyValue storage service offers a 10ms target with 500ms max latency for each namespace, and all clients receive utilization measures per data namespace to feed into their prioritized load shedding. These measures look like:\n\nutilization(namespace) = {\n\noverall = 12\n\nlatency = {\n\nslo_target = 12,\n\nslo_max = 0\n\n}\n\nsystem = {\n\nstorage = 17,\n\ncompute = 10,\n\n}\n\n}\n\nIn this case, 12% of requests are slower than the 10ms target, 0% are slower than the 500ms max latency (timeout), and 17% of allocated storage is utilized. Different use cases consult different utilizations in their prioritized shedding, for example batches that write data daily may get shed when system storage utilization is approaching capacity as writing more data would create further instability.\n\nAn example where the latency utilization is useful is for one of our critical file origin services which accepts writes of new files in the AWS cloud and acts as an origin (serves reads) for those files to our Open Connect CDN infrastructure. Writes are the most critical and should never be shed by the service, but when the backing datastore is getting overloaded, it is reasonable to progressively shed reads to files which are less critical to the CDN as it can retry those reads and they do not affect the product experience.\n\nTo achieve this goal, the origin service configured a KeyValue latency based limiter that starts shedding reads to files which are less critical to the CDN when the datastore reports a target latency utilization exceeding 40%. We then stress tested the system by generating over 50Gbps of read traffic, some of it to high priority files and some of it to low priority files:", "label": "non_personal"}
{"title": "Maestro: Data/ML Workflow Orchestrator at Netflix", "url": "https://netflixtechblog.com/maestro-netflixs-workflow-orchestrator-ee13a06f9c78?source=collection_home---4------20-----------------------", "content": "Maestro: Data/ML Workflow Orchestrator at Netflix Netflix Technology Blog 18 min read · Jul 22, 2024 -- 12 Listen Share\n\nBy Jun He, Natallia Dzenisenka, Praneeth Yenugutala, Yingyi Zhang, and Anjali Norwood\n\nTL;DR\n\nWe are thrilled to announce that the Maestro source code is now open to the public! Please visit the Maestro GitHub repository to get started. If you find it useful, please give us a star.\n\nWhat is Maestro\n\nMaestro is a horizontally scalable workflow orchestrator designed to manage large-scale Data/ML workflows such as data pipelines and machine learning model training pipelines. It oversees the entire lifecycle of a workflow, from start to finish, including retries, queuing, task distribution to compute engines, etc.. Users can package their business logic in various formats such as Docker images, notebooks, bash script, SQL, Python, and more. Unlike traditional workflow orchestrators that only support Directed Acyclic Graphs (DAGs), Maestro supports both acyclic and cyclic workflows and also includes multiple reusable patterns, including foreach loops, subworkflow, and conditional branch, etc.\n\nOur Journey with Maestro\n\nSince we first introduced Maestro in this blog post, we have successfully migrated hundreds of thousands of workflows to it on behalf of users with minimal interruption. The transition was seamless, and Maestro has met our design goals by handling our ever-growing workloads. Over the past year, we’ve seen a remarkable 87.5% increase in executed jobs. Maestro now launches thousands of workflow instances and runs half a million jobs daily on average, and has completed around 2 million jobs on particularly busy days.\n\nScalability and Versatility\n\nMaestro is a fully managed workflow orchestrator that provides Workflow-as-a-Service to thousands of end users, applications, and services at Netflix. It supports a wide range of workflow use cases, including ETL pipelines, ML workflows, AB test pipelines, pipelines to move data between different storages, etc. Maestro’s horizontal scalability ensures it can manage both a large number of workflows and a large number of jobs within a single workflow.\n\nAt Netflix, workflows are intricately connected. Splitting them into smaller groups and managing them across different clusters adds unnecessary complexity and degrades the user experience. This approach also requires additional mechanisms to coordinate these fragmented workflows. Since Netflix’s data tables are housed in a single data warehouse, we believe a single orchestrator should handle all workflows accessing it.\n\nJoin us on this exciting journey by exploring the Maestro GitHub repository and contributing to its ongoing development. Your support and feedback are invaluable as we continue to improve the Maestro project.\n\nIntroducing Maestro\n\nNetflix Maestro offers a comprehensive set of features designed to meet the diverse needs of both engineers and non-engineers. It includes the common functions and reusable patterns applicable to various use cases in a loosely coupled way.\n\nA workflow definition is defined in a JSON format. Maestro combines user-supplied fields with those managed by Maestro to form a flexible and powerful orchestration definition. An example can be found in the Maestro repository wiki.\n\nA Maestro workflow definition comprises two main sections: properties and versioned workflow including its metadata. Properties include author and owner information, and execution settings. Maestro preserves key properties across workflow versions, such as author and owner information, run strategy, and concurrency settings. This consistency simplifies management and aids in trouble-shootings. If the ownership of the current workflow changes, the new owner can claim the ownership of the workflows without creating a new workflow version. Users can also enable the triggering or alerting features for a given workflow over the properties.\n\nVersioned workflow includes attributes like a unique identifier, name, description, tags, timeout settings, and criticality levels (low, medium, high) for prioritization. Each workflow change creates a new version, enabling tracking and easy reversion, with the active or the latest version used by default. A workflow consists of steps, which are the nodes in the workflow graph defined by users. Steps can represent jobs, another workflow using subworkflow step, or a loop using foreach step. Steps consist of unique identifiers, step types, tags, input and output step parameters, step dependencies, retry policies, and failure mode, step outputs, etc. Maestro supports configurable retry policies based on error types to enhance step resilience.\n\nThis high-level overview of Netflix Maestro’s workflow definition and properties highlights its flexibility to define complex workflows. Next, we dive into some of the useful features in the following sections.\n\nWorkflow Run Strategy\n\nUsers want to automate data pipelines while retaining control over the execution order. This is crucial when workflows cannot run in parallel or must halt current executions when new ones occur. Maestro uses predefined run strategies to decide whether a workflow instance should run or not. Here is the list of predefined run strategies Maestro offers.\n\nSequential Run Strategy\n\nThis is the default strategy used by maestro, which runs workflows one at a time based on a First-In-First-Out (FIFO) order. With this run strategy, Maestro runs workflows in the order they are triggered. Note that an execution does not depend on the previous states. Once a workflow instance reaches one of the terminal states, whether succeeded or not, Maestro will start the next one in the queue.\n\nStrict Sequential Run Strategy\n\nWith this run strategy, Maestro will run workflows in the order they are triggered but block execution if there’s a blocking error in the workflow instance history. Newly triggered workflow instances are queued until the error is resolved by manually restarting the failed instances or marking the failed ones unblocked.\n\nIn the above example, run5 fails at 5AM, then later runs are queued but do not run. When someone manually marks run5 unblocked or restarts it, then the workflow execution will resume. This run strategy is useful for time insensitive but business critical workflows. This gives the workflow owners the option to review the failures at a later time and unblock the executions after verifying the correctness.\n\nFirst-only Run Strategy\n\nWith this run strategy, Maestro ensures that the running workflow is complete before queueing a new workflow instance. If a new workflow instance is queued while the current one is still running, Maestro will remove the queued instance. Maestro will execute a new workflow instance only if there is no workflow instance currently running, effectively turning off queuing with this run strategy. This approach helps to avoid idempotency issues by not queuing new workflow instances.\n\nLast-only Run Strategy\n\nWith this run strategy, Maestro ensures the running workflow is the latest triggered one and keeps only the last instance. If a new workflow instance is queued while there is an existing workflow instance already running, Maestro will stop the running instance and execute the newly triggered one. This is useful if a workflow is designed to always process the latest data, such as processing the latest snapshot of an entire table each time.\n\nParallel with Concurrency Limit Run Strategy\n\nWith this run strategy, Maestro will run multiple triggered workflow instances in parallel, constrained by a predefined concurrency limit. This helps to fan out and distribute the execution, enabling the processing of large amounts of data within the time limit. A common use case for this strategy is for backfilling the old data.\n\nParameters and Expression Language Support\n\nIn Maestro, parameters play an important role. Maestro supports dynamic parameters with code injection, which is super useful and powerful. This feature significantly enhances the flexibility and dynamism of workflows, allowing using parameters to control execution logic and enable state sharing between workflows and their steps, as well as between upstream and downstream steps. Together with other Maestro features, it makes the defining of workflows dynamic and enables users to define parameterized workflows for complex use cases.\n\nHowever, code injection introduces significant security and safety concerns. For example, users might unintentionally write an infinite loop that creates an array and appends items to it, eventually crashing the server with out-of-memory (OOM) issues. While one approach could be to ask users to embed the injected code within their business logic instead of the workflow definition, this would impose additional work on users and tightly couple their business logic with the workflow. In certain cases, this approach blocks users to design some complex parameterized workflows.\n\nTo mitigate these risks and assist users to build parameterized workflows, we developed our own customized expression language parser, a simple, secure, and safe expression language (SEL). SEL supports code injection while incorporating validations during syntax tree parsing to protect the system. It leverages the Java Security Manager to restrict access, ensuring a secure and controlled environment for code execution.\n\nSimple, Secure, and Safe Expression Language (SEL)\n\nSEL is a homemade simple, secure, and safe expression language (SEL) to address the risks associated with code injection within Maestro parameterized workflows. It is a simple expression language and the grammar and syntax follow JLS (Java Language Specifications). SEL supports a subset of JLS, focusing on Maestro use cases. For example, it supports data types for all Maestro parameter types, raising errors, datetime handling, and many predefined utility methods. SEL also includes additional runtime checks, such as loop iteration limits, array size checks, object memory size limits and so on, to enhance security and reliability. For more details about SEL, please refer to the Maestro GitHub documentation.\n\nOutput Parameters\n\nTo further enhance parameter support, Maestro allows for callable step execution, which returns output parameters from user execution back to the system. The output data is transmitted to Maestro via its REST API, ensuring that the step runtime does not have direct access to the Maestro database. This approach significantly reduces security concerns.\n\nParameterized Workflows\n\nThanks to the powerful parameter support, users can easily create parameterized workflows in addition to static ones. Users enjoy defining parameterized workflows because they are easy to manage and troubleshoot while being powerful enough to solve complex use cases.\n\nStatic workflows are simple and easy to use but come with limitations. Often, users have to duplicate the same workflow multiple times to accommodate minor changes. Additionally, workflow and jobs cannot share the states without using parameters.\n\nOn the other hand, completely dynamic workflows can be challenging to manage and support. They are difficult to debug or troubleshoot and hard to be reused by others.\n\nParameterized workflows strike a balance by being initialized step by step at runtime based on user defined parameters. This approach provides great flexibility for users to control the execution at runtime while remaining easy to manage and understand.\n\nAs we described in the previous Maestro blog post, parameter support enables the creation of complex parameterized workflows, such as backfill data pipelines.\n\nWorkflow Execution Patterns\n\nMaestro provides multiple useful building blocks that allow users to easily define dataflow patterns or other workflow patterns. It provides support for common patterns directly within the Maestro engine. Direct engine support not only enables us to optimize these patterns but also ensures a consistent approach to implementing them. Next, we will talk about the three major building blocks that Maestro provides.\n\nForeach Support\n\nIn Maestro, the foreach pattern is modeled as a dedicated step within the original workflow definition. Each iteration of the foreach loop is internally treated as a separate workflow instance, which scales similarly as any other Maestro workflow based on the step executions (i.e. a sub-graph) defined within the foreach definition block. The execution of sub-graph within a foreach step is delegated to a separate workflow instance. Foreach step then monitors and collects the status of these foreach workflow instances, each managing the execution of a single iteration. For more details, please refer to our previous Maestro blog post.\n\nThe foreach pattern is frequently used to repeatedly run the same jobs with different parameters, such as data backfilling or machine learning model tuning. It would be tedious and time consuming to request users to explicitly define each iteration in the workflow definition (potentially hundreds of thousands of iterations). Additionally, users would need to create new workflows if the foreach range changes, further complicating the process.\n\nConditional Branch Support\n\nThe conditional branch feature allows subsequent steps to run only if specific conditions in the upstream step are met. These conditions are defined using the SEL expression language, which is evaluated at runtime. Combined with other building blocks, users can build powerful workflows, e.g. doing some remediation if the audit check step fails and then run the job again.\n\nSubworkflow Support\n\nThe subworkflow feature allows a workflow step to run another workflow, enabling the sharing of common functions across multiple workflows. This effectively enables “workflow as a function” and allows users to build a graph of workflows. For example, we have observed complex workflows consisting of hundreds of subworkflows to process data across hundreds tables, where subworkflows are provided by multiple teams.\n\nThese patterns can be combined together to build composite patterns for complex workflow use cases. For instance, we can loop over a set of subworkflows or run nested foreach loops. One example that Maestro users developed is an auto-recovery workflow that utilizes both conditional branch and subworkflow features to handle errors and retry jobs automatically.\n\nIn this example, subworkflow `job1` runs another workflow consisting of extract-transform-load (ETL) and audit jobs. Next, a status check job leverages the Maestro parameter and SEL support to retrieve the status of the previous job. Based on this status, it can decide whether to complete the workflow or to run a recovery job to address any data issues. After resolving the issue, it then executes subworkflow `job2`, which runs the same workflow as subworkflow `job1`.\n\nStep Runtime and Step Parameter\n\nStep Runtime Interface\n\nIn Maestro, we use step runtime to describe a job at execution time. The step runtime interface defines two pieces of information:\n\nA set of basic APIs to control the behavior of a step instance at execution runtime. Some simple data structures to track step runtime state and execution result.\n\nMaestro offers a few step runtime implementations such as foreach step runtime, subworkflow step runtime (mentioned in previous section). Each implementation defines its own logic for start, execute and terminate operations. At runtime, these operations control the way to initialize a step instance, perform the business logic and terminate the execution under certain conditions (i.e. manual intervention by users).\n\nAlso, Maestro step runtime internally keeps track of runtime state as well as the execution result of the step. The runtime state is used to determine the next state transition of the step and tell if it has failed or terminated. The execution result hosts both step artifacts and the timeline of step execution history, which are accessible by subsequent steps.\n\nStep Parameter Merging\n\nTo control step behavior in a dynamic way, Maestro supports both runtime parameters and tags injection in step runtime. This makes a Maestro step more flexible to absorb runtime changes (i.e. overridden parameters) before actually being started. Maestro internally maintains a step parameter map that is initially empty and is updated by merging step parameters in the order below:\n\nDefault General Parameters : Parameters merging starts from default parameters that in general every step should have. For example, workflow_instance_id, step_instance_uuid, step_attempt_id and step_id are required parameters for each maestro step. They are internally reserved by maestro and cannot be passed by users.\n\n: Parameters merging starts from default parameters that in general every step should have. For example, workflow_instance_id, step_instance_uuid, step_attempt_id and step_id are required parameters for each maestro step. They are internally reserved by maestro and cannot be passed by users. Injected Parameters : Maestro then merges injected parameters (if present) into the parameter map. The injected parameters come from step runtime, which are dynamically generated based on step schema. Each type of step can have its own schema with specific parameters associated with this step. The step schema can evolve independently with no need to update Maestro code.\n\n: Maestro then merges injected parameters (if present) into the parameter map. The injected parameters come from step runtime, which are dynamically generated based on step schema. Each type of step can have its own schema with specific parameters associated with this step. The step schema can evolve independently with no need to update Maestro code. Default Typed Parameters : After injecting runtime parameters, Maestro tries to merge default parameters that are related to a specific type of step. For example, foreach step has loop_params and loop_index default parameters which are internally set by maestro and used for foreach step only.\n\n: After injecting runtime parameters, Maestro tries to merge default parameters that are related to a specific type of step. For example, foreach step has loop_params and loop_index default parameters which are internally set by maestro and used for foreach step only. Workflow and Step Info Parameters : These parameters contain information about step and the workflow it belongs to. This can be identity information, i.e. workflow_id and will be merged to step parameter map if present.\n\n: These parameters contain information about step and the workflow it belongs to. This can be identity information, i.e. workflow_id and will be merged to step parameter map if present. Undefined New Parameters : When starting or restarting a maestro workflow instance, users can specify new step parameters that are not present in initial step definition. ParamsManager merges these parameters to ensure they are available at execution time.\n\n: When starting or restarting a maestro workflow instance, users can specify new step parameters that are not present in initial step definition. ParamsManager merges these parameters to ensure they are available at execution time. Step Definition Parameters : These step parameters are defined by users at definition time and get merged if they are not empty.\n\n: These step parameters are defined by users at definition time and get merged if they are not empty. Run and Restart Parameters: When starting or restarting a maestro workflow instance, users can override defined parameters by providing run or restart parameters. These two types of parameters are merged at the end so that step runtime can see the most recent and accurate parameter space.\n\nThe parameters merging logic can be visualized in the diagram below.\n\nStep Dependencies and Signals\n\nSteps in the Maestro execution workflow graph can express execution dependencies using step dependencies. A step dependency specifies the data-related conditions required by a step to start execution. These conditions are usually defined based on signals, which are pieces of messages carrying information such as parameter values and can be published through step outputs or external systems like SNS or Kafka messages.\n\nSignals in Maestro serve both signal trigger pattern and signal dependencies (a publisher-subscriber) pattern. One step can publish an output signal (a sample example) that can unblock the execution of multiple other steps that depend on it. A signal definition includes a list of mapped parameters, allowing Maestro to perform “signal matching” on a subset of fields. Additionally, Maestro supports signal operators like <, >, etc., on signal parameter values.\n\nNetflix has built various abstractions on top of the concept of signals. For instance, a ETL workflow can update a table with data and send signals that unblock steps in downstream workflows dependent on that data. Maestro supports “signal lineage,” which allows users to navigate all historical instances of signals and the workflow steps that match (i.e. publishing or consuming) those signals. Signal triggering guarantees exactly-once execution for the workflow subscribing a signal or a set of joined signals. This approach is efficient, as it conserves resources by only executing the workflow or step when the specified conditions in the signals are met. A signal service is implemented for those advanced abstractions. Please refer to the Maestro blog for further details on it.\n\nBreakpoint\n\nMaestro allows users to set breakpoints on workflow steps, functioning similarly to code-level breakpoints in an IDE. When a workflow instance executes and reaches a step with a breakpoint, that step enters a “paused” state. This halts the workflow graph’s progression until a user manually resumes from the breakpoint. If multiple instances of a workflow step are paused at a breakpoint, resuming one instance will only affect that specific instance, leaving the others in a paused state. Deleting the breakpoint will cause all paused step instances to resume.\n\nThis feature is particularly useful during the initial development of a workflow, allowing users to inspect step executions and output data. It is also beneficial when running a step multiple times in a “foreach” pattern with various input parameters. Setting a single breakpoint on a step will cause all iterations of the foreach loop to pause at that step for debugging purposes. Additionally, the breakpoint feature allows human intervention during the workflow execution and can also be used for other purposes, e.g. supporting mutating step states while the workflow is running.\n\nTimeline\n\nMaestro includes a step execution timeline, capturing all significant events such as execution state machine changes and the reasoning behind them. This feature is useful for debugging, providing insights into the status of a step. For example, it logs transitions such as “Created” and “Evaluating params”, etc. An example of a timeline is included here for reference. The implemented step runtimes can add the timeline events into the timeline to surface the execution information to the end users.\n\nRetry Policies\n\nMaestro supports retry policies for steps that reach a terminal state due to failure. Users can specify the number of retries and configure retry policies, including delays between retries and exponential backoff strategies, in addition to fixed interval retries. Maestro distinguishes between two types of retries: “platform” and “user.” Platform retries address platform-level errors unrelated to user logic, while user retries are for user-defined conditions. Each type can have its own set of retry policies.\n\nAutomatic retries are beneficial for handling transient errors that can be resolved without user intervention. Maestro provides the flexibility to set retries to zero for non-idempotent steps to avoid retry. This feature ensures that users have control over how retries are managed based on their specific requirements.\n\nAggregated View\n\nBecause a workflow instance can have multiple runs, it is important for users to see an aggregated state of all steps in the workflow instance. Aggregated view is computed by merging base aggregated view with current runs instance step statuses. For example, as you can see on the figure below simulating a simple case, there is a first run, where step1 and step2 succeeded, step3 failed, and step4 and step5 have not started. When the user restarts the run, the run starts from step3 in run 2 with step1 and step2 skipped which succeeded in the previous run. After all steps succeed, the aggregated view shows the run states for all steps.\n\nRollup\n\nRollup provides a high-level summary of a workflow instance, detailing the status of each step and the count of steps in each status. It flattens steps across the current instance and any nested non-inline workflows like subworkflows or foreach steps. For instance, if a successful workflow has three steps, one of which is a subworkflow corresponding to a five-step workflow, the rollup will indicate that seven steps succeeded. Only leaf steps are counted in the rollup, as other steps serve merely as pointers to concrete workflows.\n\nRollup also retains references to any non-successful steps, offering a clear overview of step statuses and facilitating easy navigation to problematic steps, even within nested workflows. The aggregated rollup for a workflow instance is calculated by combining the current run’s runtime data with a base rollup. The current state is derived from the statuses of active steps, including aggregated rollups for foreach and subworkflow steps. The base rollup is established when the workflow instance begins and includes statuses of inline steps (excluding foreach and subworkflows) from the previous run that are not part of the current run.\n\nFor subworkflow steps, the rollup simply reflects the rollup of the subworkflow instance. For foreach steps, the rollup combines the base rollup of the foreach step with the current state rollup. The base is derived from the previous run’s aggregated rollup, excluding the iterations to be restarted in the new run. The current state is periodically updated by aggregating rollups of running iterations until all iterations reach a terminal state.\n\nDue to these processes, the rollup model is eventually consistent. While the figure below illustrates a straightforward example of rollup, the calculations can become complex and recursive, especially with multiple levels of nested foreaches and subworkflows.\n\nMaestro Event Publishing\n\nWhen workflow definition, workflow instance or step instance is changed, Maestro generates an event, processes it internally and publishes the processed event to external system(s). Maestro has both internal and external events. The internal event tracks changes within the life cycle of workflow, workflow instance or step instance. It is published to an internal queue and processed within Maestro. After internal events are processed, some of them will be transformed into external event and sent out to the external queue (i.e. SNS, Kafka). The external event carries maestro status change information for downstream services. The event publishing flow is illustrated in the diagram below:\n\nAs shown in the diagram, the Maestro event processor bridges the two aforementioned Maestro events. It listens on the internal queue to get the published internal events. Within the processor, the internal job event is processed based on its type and gets converted to an external event if needed. The notification publisher at the end emits the external event so that downstream services can consume.\n\nThe downstream services are mostly event-driven. The Maestro event carries the most useful message for downstream services to capture different changes in Maestro. In general, these changes can be classified into two categories: workflow change and instance status change. The workflow change event is associated with actions at workflow level, i.e definition or properties of a workflow has changed. Meanwhile, instance status change tracks status transition on workflow instance or step instance.\n\nGet Started with Maestro\n\nMaestro has been extensively used within Netflix, and today, we are excited to make the Maestro source code publicly available. We hope that the scalability and usability that Maestro offers can expedite workflow development outside Netflix. We invite you to try Maestro, use it within your organization, and contribute to its development.\n\nYou can find the Maestro code repository at github.com/Netflix/maestro. If you have any questions, thoughts, or comments about Maestro, please feel free to create a GitHub issue in the Maestro repository. We are eager to hear from you.\n\nWe are taking workflow orchestration to the next level and constantly solving new problems and challenges, please stay tuned for updates. If you are passionate about solving large scale orchestration problems, please join us.\n\nAcknowledgements\n\nThanks to other Maestro team members, Binbing Hou, Zhuoran Dong, Brittany Truong, Deepak Ramalingam, Moctar Ba, for their contributions to the Maestro project. Thanks to our Product Manager Ashim Pokharel for driving the strategy and requirements. We’d also like to thank Andrew Seier, Romain Cledat, Olek Gorajek, and other stunning colleagues at Netflix for their contributions to the Maestro project. We also thank Prashanth Ramdas, Eva Tse, David Noor, Charles Smith and other leaders of Netflix engineering organizations for their constructive feedback and suggestions on the Maestro project.", "label": "non_personal"}
{"title": "Java 21 Virtual Threads - Dude, Where’s My Lock?", "url": "https://netflixtechblog.com/java-21-virtual-threads-dude-wheres-my-lock-3052540e231d?source=collection_home---4------19-----------------------", "content": "Java 21 Virtual Threads - Dude, Where’s My Lock?\n\nGetting real with virtual threads Netflix Technology Blog 10 min read · Jul 29, 2024 -- 35 Listen Share\n\nBy Vadim Filanovsky, Mike Huang, Danny Thomas and Martin Chalupa\n\nIntro\n\nNetflix has an extensive history of using Java as our primary programming language across our vast fleet of microservices. As we pick up newer versions of Java, our JVM Ecosystem team seeks out new language features that can improve the ergonomics and performance of our systems. In a recent article, we detailed how our workloads benefited from switching to generational ZGC as our default garbage collector when we migrated to Java 21. Virtual threads is another feature we are excited to adopt as part of this migration.\n\nFor those new to virtual threads, they are described as “lightweight threads that dramatically reduce the effort of writing, maintaining, and observing high-throughput concurrent applications.” Their power comes from their ability to be suspended and resumed automatically via continuations when blocking operations occur, thus freeing the underlying operating system threads to be reused for other operations. Leveraging virtual threads can unlock higher performance when utilized in the appropriate context.\n\nIn this article we discuss one of the peculiar cases that we encountered along our path to deploying virtual threads on Java 21.\n\nThe problem\n\nNetflix engineers raised several independent reports of intermittent timeouts and hung instances to the Performance Engineering and JVM Ecosystem teams. Upon closer examination, we noticed a set of common traits and symptoms. In all cases, the apps affected ran on Java 21 with SpringBoot 3 and embedded Tomcat serving traffic on REST endpoints. The instances that experienced the issue simply stopped serving traffic even though the JVM on those instances remained up and running. One clear symptom characterizing the onset of this issue is a persistent increase in the number of sockets in closeWait state as illustrated by the graph below:\n\nCollected diagnostics\n\nSockets remaining in closeWait state indicate that the remote peer closed the socket, but it was never closed on the local instance, presumably because the application failed to do so. This can often indicate that the application is hanging in an abnormal state, in which case application thread dumps may reveal additional insight.\n\nIn order to troubleshoot this issue, we first leveraged our alerts system to catch an instance in this state. Since we periodically collect and persist thread dumps for all JVM workloads, we can often retroactively piece together the behavior by examining these thread dumps from an instance. However, we were surprised to find that all our thread dumps show a perfectly idle JVM with no clear activity. Reviewing recent changes revealed that these impacted services enabled virtual threads, and we knew that virtual thread call stacks do not show up in jstack -generated thread dumps. To obtain a more complete thread dump containing the state of the virtual threads, we used the “ jcmd Thread.dump_to_file ” command instead. As a last-ditch effort to introspect the state of JVM, we also collected a heap dump from the instance.\n\nAnalysis\n\nThread dumps revealed thousands of “blank” virtual threads:\n\n#119821 \"\" virtual\n\n\n\n#119820 \"\" virtual\n\n\n\n#119823 \"\" virtual\n\n\n\n#120847 \"\" virtual\n\n\n\n#119822 \"\" virtual\n\n...\n\nThese are the VTs (virtual threads) for which a thread object is created, but has not started running, and as such, has no stack trace. In fact, there were approximately the same number of blank VTs as the number of sockets in closeWait state. To make sense of what we were seeing, we need to first understand how VTs operate.\n\nA virtual thread is not mapped 1:1 to a dedicated OS-level thread. Rather, we can think of it as a task that is scheduled to a fork-join thread pool. When a virtual thread enters a blocking call, like waiting for a Future , it relinquishes the OS thread it occupies and simply remains in memory until it is ready to resume. In the meantime, the OS thread can be reassigned to execute other VTs in the same fork-join pool. This allows us to multiplex a lot of VTs to just a handful of underlying OS threads. In JVM terminology, the underlying OS thread is referred to as the “carrier thread” to which a virtual thread can be “mounted” while it executes and “unmounted” while it waits. A great in-depth description of virtual thread is available in JEP 444.\n\nIn our environment, we utilize a blocking model for Tomcat, which in effect holds a worker thread for the lifespan of a request. By enabling virtual threads, Tomcat switches to virtual execution. Each incoming request creates a new virtual thread that is simply scheduled as a task on a Virtual Thread Executor. We can see Tomcat creates a VirtualThreadExecutor here.\n\nTying this information back to our problem, the symptoms correspond to a state when Tomcat keeps creating a new web worker VT for each incoming request, but there are no available OS threads to mount them onto.\n\nWhy is Tomcat stuck?\n\nWhat happened to our OS threads and what are they busy with? As described here, a VT will be pinned to the underlying OS thread if it performs a blocking operation while inside a synchronized block or method. This is exactly what is happening here. Here is a relevant snippet from a thread dump obtained from the stuck instance:\n\n#119515 \"\" virtual\n\njava.base/jdk.internal.misc.Unsafe.park(Native Method)\n\njava.base/java.lang.VirtualThread.parkOnCarrierThread(VirtualThread.java:661)\n\njava.base/java.lang.VirtualThread.park(VirtualThread.java:593)\n\njava.base/java.lang.System$2.parkVirtualThread(System.java:2643)\n\njava.base/jdk.internal.misc.VirtualThreads.park(VirtualThreads.java:54)\n\njava.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:219)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:754)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:990)\n\njava.base/java.util.concurrent.locks.ReentrantLock$Sync.lock(ReentrantLock.java:153)\n\njava.base/java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:322)\n\nzipkin2.reporter.internal.CountBoundedQueue.offer(CountBoundedQueue.java:54)\n\nzipkin2.reporter.internal.AsyncReporter$BoundedAsyncReporter.report(AsyncReporter.java:230)\n\nzipkin2.reporter.brave.AsyncZipkinSpanHandler.end(AsyncZipkinSpanHandler.java:214)\n\nbrave.internal.handler.NoopAwareSpanHandler$CompositeSpanHandler.end(NoopAwareSpanHandler.java:98)\n\nbrave.internal.handler.NoopAwareSpanHandler.end(NoopAwareSpanHandler.java:48)\n\nbrave.internal.recorder.PendingSpans.finish(PendingSpans.java:116)\n\nbrave.RealSpan.finish(RealSpan.java:134)\n\nbrave.RealSpan.finish(RealSpan.java:129)\n\nio.micrometer.tracing.brave.bridge.BraveSpan.end(BraveSpan.java:117)\n\nio.micrometer.tracing.annotation.AbstractMethodInvocationProcessor.after(AbstractMethodInvocationProcessor.java:67)\n\nio.micrometer.tracing.annotation.ImperativeMethodInvocationProcessor.proceedUnderSynchronousSpan(ImperativeMethodInvocationProcessor.java:98)\n\nio.micrometer.tracing.annotation.ImperativeMethodInvocationProcessor.process(ImperativeMethodInvocationProcessor.java:73)\n\nio.micrometer.tracing.annotation.SpanAspect.newSpanMethod(SpanAspect.java:59)\n\njava.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n\njava.base/java.lang.reflect.Method.invoke(Method.java:580)\n\norg.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethodWithGivenArgs(AbstractAspectJAdvice.java:637)\n\n...\n\nIn this stack trace, we enter the synchronization in brave.RealSpan.finish(RealSpan.java:134) . This virtual thread is effectively pinned — it is mounted to an actual OS thread even while it waits to acquire a reentrant lock. There are 3 VTs in this exact state and another VT identified as “ <redacted> @DefaultExecutor - 46542 ” that also follows the same code path. These 4 virtual threads are pinned while waiting to acquire a lock. Because the app is deployed on an instance with 4 vCPUs, the fork-join pool that underpins VT execution also contains 4 OS threads. Now that we have exhausted all of them, no other virtual thread can make any progress. This explains why Tomcat stopped processing the requests and why the number of sockets in closeWait state keeps climbing. Indeed, Tomcat accepts a connection on a socket, creates a request along with a virtual thread, and passes this request/thread to the executor for processing. However, the newly created VT cannot be scheduled because all of the OS threads in the fork-join pool are pinned and never released. So these newly created VTs are stuck in the queue, while still holding the socket.\n\nWho has the lock?\n\nNow that we know VTs are waiting to acquire a lock, the next question is: Who holds the lock? Answering this question is key to understanding what triggered this condition in the first place. Usually a thread dump indicates who holds the lock with either “ - locked <0x…> (at …) ” or “ Locked ownable synchronizers ,” but neither of these show up in our thread dumps. As a matter of fact, no locking/parking/waiting information is included in the jcmd -generated thread dumps. This is a limitation in Java 21 and will be addressed in the future releases. Carefully combing through the thread dump reveals that there are a total of 6 threads contending for the same ReentrantLock and associated Condition . Four of these six threads are detailed in the previous section. Here is another thread:\n\n#119516 \"\" virtual\n\njava.base/java.lang.VirtualThread.park(VirtualThread.java:582)\n\njava.base/java.lang.System$2.parkVirtualThread(System.java:2643)\n\njava.base/jdk.internal.misc.VirtualThreads.park(VirtualThreads.java:54)\n\njava.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:219)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:754)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:990)\n\njava.base/java.util.concurrent.locks.ReentrantLock$Sync.lock(ReentrantLock.java:153)\n\njava.base/java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:322)\n\nzipkin2.reporter.internal.CountBoundedQueue.offer(CountBoundedQueue.java:54)\n\nzipkin2.reporter.internal.AsyncReporter$BoundedAsyncReporter.report(AsyncReporter.java:230)\n\nzipkin2.reporter.brave.AsyncZipkinSpanHandler.end(AsyncZipkinSpanHandler.java:214)\n\nbrave.internal.handler.NoopAwareSpanHandler$CompositeSpanHandler.end(NoopAwareSpanHandler.java:98)\n\nbrave.internal.handler.NoopAwareSpanHandler.end(NoopAwareSpanHandler.java:48)\n\nbrave.internal.recorder.PendingSpans.finish(PendingSpans.java:116)\n\nbrave.RealScopedSpan.finish(RealScopedSpan.java:64)\n\n...\n\nNote that while this thread seemingly goes through the same code path for finishing a span, it does not go through a synchronized block. Finally here is the 6th thread:\n\n#107 \"AsyncReporter <redacted>\"\n\njava.base/jdk.internal.misc.Unsafe.park(Native Method)\n\njava.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:221)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:754)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1761)\n\nzipkin2.reporter.internal.CountBoundedQueue.drainTo(CountBoundedQueue.java:81)\n\nzipkin2.reporter.internal.AsyncReporter$BoundedAsyncReporter.flush(AsyncReporter.java:241)\n\nzipkin2.reporter.internal.AsyncReporter$Flusher.run(AsyncReporter.java:352)\n\njava.base/java.lang.Thread.run(Thread.java:1583)\n\nThis is actually a normal platform thread, not a virtual thread. Paying particular attention to the line numbers in this stack trace, it is peculiar that the thread seems to be blocked within the internal acquire() method after completing the wait. In other words, this calling thread owned the lock upon entering awaitNanos() . We know the lock was explicitly acquired here. However, by the time the wait completed, it could not reacquire the lock. Summarizing our thread dump analysis:\n\nThere are 5 virtual threads and 1 regular thread waiting for the lock. Out of those 5 VTs, 4 of them are pinned to the OS threads in the fork-join pool. There’s still no information on who owns the lock. As there’s nothing more we can glean from the thread dump, our next logical step is to peek into the heap dump and introspect the state of the lock.\n\nInspecting the lock", "label": "non_personal"}
{"title": "Recommending for Long-Term Member Satisfaction at Netflix", "url": "https://netflixtechblog.com/recommending-for-long-term-member-satisfaction-at-netflix-ac15cada49ef?source=collection_home---4------17-----------------------", "content": "Recommending for Long-Term Member Satisfaction at Netflix Netflix Technology Blog 8 min read · Aug 29, 2024 -- 9 Listen Share\n\nBy Jiangwei Pan, Gary Tang, Henry Wang, and Justin Basilico\n\nIntroduction\n\nOur mission at Netflix is to entertain the world. Our personalization algorithms play a crucial role in delivering on this mission for all members by recommending the right shows, movies, and games at the right time. This goal extends beyond immediate engagement; we aim to create an experience that brings lasting enjoyment to our members. Traditional recommender systems often optimize for short-term metrics like clicks or engagement, which may not fully capture long-term satisfaction. We strive to recommend content that not only engages members in the moment but also enhances their long-term satisfaction, which increases the value they get from Netflix, and thus they’ll be more likely to continue to be a member.\n\nRecommendations as Contextual Bandit\n\nOne simple way we can view recommendations is as a contextual bandit problem. When a member visits, that becomes a context for our system and it selects an action of what recommendations to show, and then the member provides various types of feedback. These feedback signals can be immediate (skips, plays, thumbs up/down, or adding items to their playlist) or delayed (completing a show or renewing their subscription). We can define reward functions to reflect the quality of the recommendations from these feedback signals and then train a contextual bandit policy on historical data to maximize the expected reward.\n\nImproving Recommendations: Models and Objectives\n\nThere are many ways that a recommendation model can be improved. They may come from more informative input features, more data, different architectures, more parameters, and so forth. In this post, we focus on a less-discussed aspect about improving the recommender objective by defining a reward function that tries to better reflect long-term member satisfaction.\n\nRetention as Reward?\n\nMember retention might seem like an obvious reward for optimizing long-term satisfaction because members should stay if they’re satisfied, however it has several drawbacks:\n\nNoisy : Retention can be influenced by numerous external factors, such as seasonal trends, marketing campaigns, or personal circumstances unrelated to the service.\n\n: Retention can be influenced by numerous external factors, such as seasonal trends, marketing campaigns, or personal circumstances unrelated to the service. Low Sensitivity : Retention is only sensitive for members on the verge of canceling their subscription, not capturing the full spectrum of member satisfaction.\n\n: Retention is only sensitive for members on the verge of canceling their subscription, not capturing the full spectrum of member satisfaction. Hard to Attribute : Members might cancel only after a series of bad recommendations.\n\n: Members might cancel only after a series of bad recommendations. Slow to Measure: We only get one signal per account per month.\n\nDue to these challenges, optimizing for retention alone is impractical.\n\nProxy Rewards\n\nInstead, we can train our bandit policy to optimize a proxy reward function that is highly aligned with long-term member satisfaction while being sensitive to individual recommendations. The proxy reward r(user, item) is a function of user interaction with the recommended item. For example, if we recommend “One Piece” and a member plays then subsequently completes and gives it a thumbs-up, a simple proxy reward might be defined as r(user, item) = f(play, complete, thumb).\n\nClick-through rate (CTR)\n\nClick-through rate (CTR), or in our case play-through rate, can be viewed as a simple proxy reward where r(user, item) = 1 if the user clicks a recommendation and 0 otherwise. CTR is a common feedback signal that generally reflects user preference expectations. It is a simple yet strong baseline for many recommendation applications. In some cases, such as ads personalization where the click is the target action, CTR may even be a reasonable reward for production models. However, in most cases, over-optimizing CTR can lead to promoting clickbaity items, which may harm long-term satisfaction.\n\nBeyond CTR\n\nTo align the proxy reward function more closely with long-term satisfaction, we need to look beyond simple interactions, consider all types of user actions, and understand their true implications on user satisfaction.\n\nWe give a few examples in the Netflix context:\n\nFast season completion ✅: Completing a season of a recommended TV show in one day is a strong sign of enjoyment and long-term satisfaction.\n\n✅: Completing a season of a recommended TV show in one day is a strong sign of enjoyment and long-term satisfaction. Thumbs-down after completion ❌: Completing a TV show in several weeks followed by a thumbs-down indicates low satisfaction despite significant time spent.\n\n❌: Completing a TV show in several weeks followed by a thumbs-down indicates low satisfaction despite significant time spent. Playing a movie for just 10 minutes ❓: In this case, the user’s satisfaction is ambiguous. The brief engagement might indicate that the user decided to abandon the movie, or it could simply mean the user was interrupted and plans to finish the movie later, perhaps the next day.\n\n❓: In this case, the user’s satisfaction is ambiguous. The brief engagement might indicate that the user decided to abandon the movie, or it could simply mean the user was interrupted and plans to finish the movie later, perhaps the next day. Discovering new genres ✅ ✅: Watching more Korean or game shows after “Squid Game” suggests the user is discovering something new. This discovery was likely even more valuable since it led to a variety of engagements in a new area for a member.\n\nReward Engineering\n\nReward engineering is the iterative process of refining the proxy reward function to align with long-term member satisfaction. It is similar to feature engineering, except that it can be derived from data that isn’t available at serving time. Reward engineering involves four stages: hypothesis formation, defining a new proxy reward, training a new bandit policy, and A/B testing. Below is a simple example.\n\nChallenge: Delayed Feedback\n\nUser feedback used in the proxy reward function is often delayed or missing. For example, a member may decide to play a recommended show for just a few minutes on the first day and take several weeks to fully complete the show. This completion feedback is therefore delayed. Additionally, some user feedback may never occur; while we may wish otherwise, not all members provide a thumbs-up or thumbs-down after completing a show, leaving us uncertain about their level of enjoyment.\n\nWe could try and wait to give a longer window to observe feedback, but how long should we wait for delayed feedback before computing the proxy rewards? If we wait too long (e.g., weeks), we miss the opportunity to update the bandit policy with the latest data. In a highly dynamic environment like Netflix, a stale bandit policy can degrade the user experience and be particularly bad at recommending newer items.\n\nSolution: predict missing feedback\n\nWe aim to update the bandit policy shortly after making a recommendation while also defining the proxy reward function based on all user feedback, including delayed feedback. Since delayed feedback has not been observed at the time of policy training, we can predict it. This prediction occurs for each training example with delayed feedback, using already observed feedback and other relevant information up to the training time as input features. Thus, the prediction also gets better as time progresses.\n\nThe proxy reward is then calculated for each training example using both observed and predicted feedback. These training examples are used to update the bandit policy.\n\nBut aren’t we still only relying on observed feedback in the proxy reward function? Yes, because delayed feedback is predicted based on observed feedback. However, it is simpler to reason about rewards using all feedback directly. For instance, the delayed thumbs-up prediction model may be a complex neural network that takes into account all observed feedback (e.g., short-term play patterns). It’s more straightforward to define the proxy reward as a simple function of the thumbs-up feedback rather than a complex function of short-term interaction patterns. It can also be used to adjust for potential biases in how feedback is provided.\n\nThe reward engineering diagram is updated with an optional delayed feedback prediction step.\n\nTwo types of ML models\n\nIt’s worth noting that this approach employs two types of ML models:\n\nDelayed Feedback Prediction Models : These models predict p(final feedback | observed feedbacks). The predictions are used to define and compute proxy rewards for bandit policy training examples. As a result, these models are used offline during the bandit policy training.\n\n: These models predict p(final feedback | observed feedbacks). The predictions are used to define and compute proxy rewards for bandit policy training examples. As a result, these models are used offline during the bandit policy training. Bandit Policy Models: These models are used in the bandit policy π(item | user; r) to generate recommendations online and in real-time.\n\nChallenge: Online-Offline Metric Disparity\n\nImproved input features or neural network architectures often lead to better offline model metrics (e.g., AUC for classification models). However, when these improved models are subjected to A/B testing, we often observe flat or even negative online metrics, which can quantify long-term member satisfaction.\n\nThis online-offline metric disparity usually occurs when the proxy reward used in the recommendation policy is not fully aligned with long-term member satisfaction. In such cases, a model may achieve higher proxy rewards (offline metrics) but result in worse long-term member satisfaction (online metrics).\n\nNevertheless, the model improvement is genuine. One approach to resolve this is to further refine the proxy reward definition to align better with the improved model. When this tuning results in positive online metrics, the model improvement can be effectively productized. See [1] for more discussions on this challenge.\n\nSummary and Open Questions\n\nIn this post, we provided an overview of our reward engineering efforts to align Netflix recommendations with long-term member satisfaction. While retention remains our north star, it is not easy to optimize directly. Therefore, our efforts focus on defining a proxy reward that is aligned with long-term satisfaction and sensitive to individual recommendations. Finally, we discussed the unique challenge of delayed user feedback at Netflix and proposed an approach that has proven effective for us. Refer to [2] for an earlier overview of the reward innovation efforts at Netflix.\n\nAs we continue to improve our recommendations, several open questions remain:\n\nCan we learn a good proxy reward function automatically by correlating behavior with retention?\n\nHow long should we wait for delayed feedback before using its predicted value in policy training?\n\nHow can we leverage Reinforcement Learning to further align the policy with long-term satisfaction?\n\nReferences\n\n[1] Deep learning for recommender systems: A Netflix case study. AI Magazine 2021. Harald Steck, Linas Baltrunas, Ehtsham Elahi, Dawen Liang, Yves Raimond, Justin Basilico.\n\n[2] Reward innovation for long-term member satisfaction. RecSys 2023. Gary Tang, Jiangwei Pan, Henry Wang, Justin Basilico.", "label": "non_personal"}
{"title": "Noisy Neighbor Detection with eBPF", "url": "https://netflixtechblog.com/noisy-neighbor-detection-with-ebpf-64b1f4b3bbdd?source=collection_home---4------16-----------------------", "content": "The sched_wakeup and sched_wakeup_new hooks are invoked when a process changes state from 'sleeping' to 'runnable.' They let us identify when a process is ready to run and is waiting for CPU time. During this event, we generate a timestamp and store it in an eBPF hash map using the process ID as the key.\n\nstruct {\n\n__uint(type, BPF_MAP_TYPE_HASH);\n\n__uint(max_entries, MAX_TASK_ENTRIES);\n\n__uint(key_size, sizeof(u32));\n\n__uint(value_size, sizeof(u64));\n\n} runq_enqueued SEC(\".maps\");\n\n\n\nSEC(\"tp_btf/sched_wakeup\")\n\nint tp_sched_wakeup(u64 *ctx)\n\n{\n\nstruct task_struct *task = (void *)ctx[0];\n\nu32 pid = task->pid;\n\nu64 ts = bpf_ktime_get_ns();\n\n\n\nbpf_map_update_elem(&runq_enqueued, &pid, &ts, BPF_NOEXIST);\n\nreturn 0;\n\n}\n\nConversely, the sched_switch hook is triggered when the CPU switches between processes. This hook provides pointers to the process currently utilizing the CPU and the process about to take over. We use the upcoming task's process ID (PID) to fetch the timestamp from the eBPF map. This timestamp represents when the process entered the queue, which we had previously stored. We then calculate the run queue latency by simply subtracting the timestamps.\n\nSEC(\"tp_btf/sched_switch\")\n\nint tp_sched_switch(u64 *ctx)\n\n{\n\nstruct task_struct *prev = (struct task_struct *)ctx[1];\n\nstruct task_struct *next = (struct task_struct *)ctx[2];\n\nu32 prev_pid = prev->pid;\n\nu32 next_pid = next->pid;\n\n\n\n// fetch timestamp of when the next task was enqueued\n\nu64 *tsp = bpf_map_lookup_elem(&runq_enqueued, &next_pid);\n\nif (tsp == NULL) {\n\nreturn 0; // missed enqueue\n\n}\n\n\n\n// calculate runq latency before deleting the stored timestamp\n\nu64 now = bpf_ktime_get_ns();\n\nu64 runq_lat = now - *tsp;\n\n\n\n// delete pid from enqueued map\n\nbpf_map_delete_elem(&runq_enqueued, &next_pid);\n\n....\n\nOne of the advantages of eBPF is its ability to provide pointers to the actual kernel data structures representing processes or threads, also known as tasks in kernel terminology. This feature enables access to a wealth of information stored about a process. We required the process's cgroup ID to associate it with a container for our specific use case. However, the cgroup information in the process struct is safeguarded by an RCU (Read Copy Update) lock.\n\nTo safely access this RCU-protected information, we can leverage kfuncs in eBPF. kfuncs are kernel functions that can be called from eBPF programs. There are kfuncs available to lock and unlock RCU read-side critical sections. These functions ensure that our eBPF program remains safe and efficient while retrieving the cgroup ID from the task struct.\n\nvoid bpf_rcu_read_lock(void) __ksym;\n\nvoid bpf_rcu_read_unlock(void) __ksym;\n\n\n\nu64 get_task_cgroup_id(struct task_struct *task)\n\n{\n\nstruct css_set *cgroups;\n\nu64 cgroup_id;\n\nbpf_rcu_read_lock();\n\ncgroups = task->cgroups;\n\ncgroup_id = cgroups->dfl_cgrp->kn->id;\n\nbpf_rcu_read_unlock();\n\nreturn cgroup_id;\n\n}\n\nOnce the data is ready, we must package it and send it to userspace. For this purpose, we chose the eBPF ring buffer. It is efficient, high-performing, and user-friendly. It can handle variable-length data records and allows data reading without necessitating extra memory copying or syscalls. However, the sheer number of data points was causing the userspace program to use too much CPU, so we implemented a rate limiter in eBPF to sample the data.\n\nstruct {\n\n__uint(type, BPF_MAP_TYPE_RINGBUF);\n\n__uint(max_entries, RINGBUF_SIZE_BYTES);\n\n} events SEC(\".maps\");\n\n\n\nstruct {\n\n__uint(type, BPF_MAP_TYPE_PERCPU_HASH);\n\n__uint(max_entries, MAX_TASK_ENTRIES);\n\n__uint(key_size, sizeof(u64));\n\n__uint(value_size, sizeof(u64));\n\n} cgroup_id_to_last_event_ts SEC(\".maps\");\n\n\n\nstruct runq_event {\n\nu64 prev_cgroup_id;\n\nu64 cgroup_id;\n\nu64 runq_lat;\n\nu64 ts;\n\n};\n\n\n\nSEC(\"tp_btf/sched_switch\")\n\nint tp_sched_switch(u64 *ctx)\n\n{\n\n// ....\n\n// The previous code\n\n// ....\n\n\n\nu64 prev_cgroup_id = get_task_cgroup_id(prev);\n\nu64 cgroup_id = get_task_cgroup_id(next);\n\n\n\n// per-cgroup-id-per-CPU rate-limiting\n\n// to balance observability with performance overhead\n\nu64 *last_ts =\n\nbpf_map_lookup_elem(&cgroup_id_to_last_event_ts, &cgroup_id);\n\nu64 last_ts_val = last_ts == NULL ? 0 : *last_ts;\n\n\n\n// check the rate limit for the cgroup_id in consideration\n\n// before doing more work\n\nif (now - last_ts_val < RATE_LIMIT_NS) {\n\n// Rate limit exceeded, drop the event\n\nreturn 0;\n\n}\n\n\n\nstruct runq_event *event;\n\nevent = bpf_ringbuf_reserve(&events, sizeof(*event), 0);\n\n\n\nif (event) {\n\nevent->prev_cgroup_id = prev_cgroup_id;\n\nevent->cgroup_id = cgroup_id;\n\nevent->runq_lat = runq_lat;\n\nevent->ts = now;\n\nbpf_ringbuf_submit(event, 0);\n\n// Update the last event timestamp for the current cgroup_id\n\nbpf_map_update_elem(&cgroup_id_to_last_event_ts, &cgroup_id,\n\n&now, BPF_ANY);\n\n\n\n}\n\n\n\nreturn 0;\n\n}\n\nOur userspace application, developed in Go, processes events from the ring buffer to emit metrics to our metrics backend, Atlas. Each event includes a run queue latency sample with a cgroup ID, which we associate with containers running on the host. We categorize it as a system service if no such association is found. When a cgroup ID is associated with a container, we emit a percentile timer Atlas metric ( runq.latency ) for that container. We also increment a counter metric ( sched.switch.out ) to monitor preemptions occurring for the container's processes. Access to the prev_cgroup_id of the preempted process allows us to tag the metric with the cause of the preemption, whether it's due to a process within the same container (or cgroup), a process in another container, or a system service.\n\nIt's important to highlight that both the runq.latency metric and the sched.switch.out metrics are needed to determine if a container is affected by noisy neighbors, which is the goal we aim to achieve — relying solely on the runq.latency metric can lead to misconceptions. For example, if a container is at or over its cgroup CPU limit, the scheduler will throttle it, resulting in an apparent spike in run queue latency due to delays in the queue. If we were only to consider this metric, we might incorrectly attribute the performance degradation to noisy neighbors when it's actually because the container is hitting its CPU quota. However, simultaneous spikes in both metrics, mainly when the cause is a different container or system process, clearly indicate a noisy neighbor issue.\n\nA Noisy Neighbor Story", "label": "non_personal"}
{"title": "Improve Your Next Experiment by Learning Better Proxy Metrics From Past Experiments", "url": "https://netflixtechblog.com/improve-your-next-experiment-by-learning-better-proxy-metrics-from-past-experiments-64c786c2a3ac?source=collection_home---4------18-----------------------", "content": "We are excited to share our work on how to learn good proxy metrics from historical experiments at KDD 2024. This work addresses a fundamental question for technology companies and academic researchers alike: how do we establish that a treatment that improves short-term (statistically sensitive) outcomes also improves long-term (statistically insensitive) outcomes? Or, faced with multiple short-term outcomes, how do we optimally trade them off for long-term benefit?\n\nFor example, in an A/B test, you may observe that a product change improves the click-through rate. However, the test does not provide enough signal to measure a change in long-term retention, leaving you in the dark as to whether this treatment makes users more satisfied with your service. The click-through rate is a proxy metric (S, for surrogate, in our paper) while retention is a downstream business outcome or north star metric (Y). We may even have several proxy metrics, such as other types of clicks or the length of engagement after click. Taken together, these form a vector of proxy metrics.\n\nThe goal of our work is to understand the true relationship between the proxy metric(s) and the north star metric — so that we can assess a proxy’s ability to stand in for the north star metric, learn how to combine multiple metrics into a single best one, and better explore and compare different proxies.\n\nSeveral intuitive approaches to understanding this relationship have surprising pitfalls:\n\nLooking only at user-level correlations between the proxy S and north star Y. Continuing the example from above, you may find that users with a higher click-through rate also tend to have a higher retention. But this does not mean that a product change that improves the click-through rate will also improve retention (in fact, promoting clickbait may have the opposite effect). This is because, as any introductory causal inference class will tell you, there are many confounders between S and Y — many of which you can never reliably observe and control for.\n\nContinuing the example from above, you may find that users with a higher click-through rate also tend to have a higher retention. But this does not mean that a product change that improves the click-through rate will also improve retention (in fact, promoting clickbait may have the opposite effect). This is because, as any introductory causal inference class will tell you, there are many confounders between S and Y — many of which you can never reliably observe and control for. Looking naively at treatment effect correlations between S and Y. Suppose you are lucky enough to have many historical A/B tests. Further imagine the ordinary least squares (OLS) regression line through a scatter plot of Y on S in which each point represents the (S,Y)-treatment effect from a previous test. Even if you find that this line has a positive slope, you unfortunately cannot conclude that product changes that improve S will also improve Y. The reason for this is correlated measurement error — if S and Y are positively correlated in the population, then treatment arms that happen to have more users with high S will also have more users with high Y.\n\nBetween these naive approaches, we find that the second one is the easier trap to fall into. This is because the dangers of the first approach are well-known, whereas covariances between estimated treatment effects can appear misleadingly causal. In reality, these covariances can be severely biased compared to what we actually care about: covariances between true treatment effects. In the extreme — such as when the negative effects of clickbait are substantial but clickiness and retention are highly correlated at the user level — the true relationship between S and Y can be negative even if the OLS slope is positive. Only more data per experiment could diminish this bias — using more experiments as data points will only yield more precise estimates of the badly biased slope. At first glance, this would appear to imperil any hope of using existing experiments to detect the relationship.\n\nThis figure shows a hypothetical treatment effect covariance matrix between S and Y (white line; negative correlation), a unit-level sampling covariance matrix creating correlated measurement errors between these metrics (black line; positive correlation), and the covariance matrix of estimated treatment effects which is a weighted combination of the first two (orange line; no correlation).\n\nTo overcome this bias, we propose better ways to leverage historical experiments, inspired by techniques from the literature on weak instrumental variables. More specifically, we show that three estimators are consistent for the true proxy/north-star relationship under different constraints (the paper provides more details and should be helpful for practitioners interested in choosing the best estimator for their setting):\n\nA Total Covariance (TC) estimator allows us to estimate the OLS slope from a scatter plot of true treatment effects by subtracting the scaled measurement error covariance from the covariance of estimated treatment effects. Under the assumption that the correlated measurement error is the same across experiments (homogeneous covariances), the bias of this estimator is inversely proportional to the total number of units across all experiments, as opposed to the number of members per experiment.\n\nestimator allows us to estimate the OLS slope from a scatter plot of true treatment effects by subtracting the scaled measurement error covariance from the covariance of estimated treatment effects. Under the assumption that the correlated measurement error is the same across experiments (homogeneous covariances), the bias of this estimator is inversely proportional to the total number of units across all experiments, as opposed to the number of members per experiment. Jackknife Instrumental Variables Estimation (JIVE) converges to the same OLS slope as the TC estimator but does not require the assumption of homogeneous covariances. JIVE eliminates correlated measurement error by removing each observation’s data from the computation of its instrumented surrogate values.\n\nconverges to the same OLS slope as the TC estimator but does not require the assumption of homogeneous covariances. JIVE eliminates correlated measurement error by removing each observation’s data from the computation of its instrumented surrogate values. A Limited Information Maximum Likelihood (LIML) estimator is statistically efficient as long as there are no direct effects between the treatment and Y (that is, S fully mediates all treatment effects on Y). We find that LIML is highly sensitive to this assumption and recommend TC or JIVE for most applications.\n\nOur methods yield linear structural models of treatment effects that are easy to interpret. As such, they are well-suited to the decentralized and rapidly-evolving practice of experimentation at Netflix, which runs thousands of experiments per year on many diverse parts of the business. Each area of experimentation is staffed by independent Data Science and Engineering teams. While every team ultimately cares about the same north star metrics (e.g., long-term revenue), it is highly impractical for most teams to measure these in short-term A/B tests. Therefore, each has also developed proxies that are more sensitive and directly relevant to their work (e.g., user engagement or latency). To complicate matters more, teams are constantly innovating on these secondary metrics to find the right balance of sensitivity and long-term impact.\n\nIn this decentralized environment, linear models of treatment effects are a highly useful tool for coordinating efforts around proxy metrics and aligning them towards the north star:\n\nManaging metric tradeoffs. Because experiments in one area can affect metrics in another area, there is a need to measure all secondary metrics in all tests, but also to understand the relative impact of these metrics on the north star. This is so we can inform decision-making when one metric trades off against another metric. Informing metrics innovation. To minimize wasted effort on metric development, it is also important to understand how metrics correlate with the north star “net of” existing metrics. Enabling teams to work independently. Lastly, teams need simple tools in order to iterate on their own metrics. Teams may come up with dozens of variations of secondary metrics, and slow, complicated tools for evaluating these variations are unlikely to be adopted. Conversely, our models are easy and fast to fit, and are actively used to develop proxy metrics at Netflix.\n\nWe are thrilled about the research and implementation of these methods at Netflix — while also continuing to strive for great and always better, per our culture. For example, we still have some way to go to develop a more flexible data architecture to streamline the application of these methods within Netflix. Interested in helping us? See our open job postings!\n\nFor feedback on this blog post and for supporting and making this work better, we thank Apoorva Lal, Martin Tingley, Patric Glynn, Richard McDowell, Travis Brooks, and Ayal Chen-Zion.", "label": "non_personal"}
{"title": "Pushy to the Limit: Evolving Netflix’s WebSocket proxy for the future", "url": "https://netflixtechblog.com/pushy-to-the-limit-evolving-netflixs-websocket-proxy-for-the-future-b468bc0ff658?source=collection_home---4------15-----------------------", "content": "Pushy to the Limit: Evolving Netflix’s WebSocket proxy for the future Netflix Technology Blog 16 min read · Sep 10, 2024 -- 16 Listen Share\n\nBy Karthik Yagna, Baskar Odayarkoil, and Alex Ellis\n\nPushy is Netflix’s WebSocket server that maintains persistent WebSocket connections with devices running the Netflix application. This allows data to be sent to the device from backend services on demand, without the need for continually polling requests from the device. Over the last few years, Pushy has seen tremendous growth, evolving from its role as a best-effort message delivery service to be an integral part of the Netflix ecosystem. This post describes how we’ve grown and scaled Pushy to meet its new and future needs, as it handles hundreds of millions of concurrent WebSocket connections, delivers hundreds of thousands of messages per second, and maintains a steady 99.999% message delivery reliability rate.\n\nHistory & motivation\n\nThere were two main motivating use cases that drove Pushy’s initial development and usage. The first was voice control, where you can play a title or search using your virtual assistant with a voice command like “Show me Stranger Things on Netflix.” (See How to use voice controls with Netflix if you want to do this yourself!).\n\nIf we consider the Alexa use case, we can see how this partnership with Amazon enabled this to work. Once they receive the voice command, we allow them to make an authenticated call through apiproxy, our streaming edge proxy, to our internal voice service. This call includes metadata, such as the user’s information and details about the command, such as the specific show to play. The voice service then constructs a message for the device and places it on the message queue, which is then processed and sent to Pushy to deliver to the device. Finally, the device receives the message, and the action, such as “Show me Stranger Things on Netflix”, is performed. This initial functionality was built out for FireTVs and was expanded from there.\n\nSample system diagram for an Alexa voice command. Where aws ends and the internet begins is an exercise left to the reader.\n\nThe other main use case was RENO, the Rapid Event Notification System mentioned above. Before the integration with Pushy, the TV UI would continuously poll a backend service to see if there were any row updates to get the latest information. These requests would happen every few seconds, which ended up creating extraneous requests to the backend and were costly for devices, which are frequently resource constrained. The integration with WebSockets and Pushy alleviated both of these points, allowing the origin service to send row updates as they were ready, resulting in lower request rates and cost savings.\n\nFor more background on Pushy, you can see this InfoQ talk by Susheel Aroskar. Since that presentation, Pushy has grown in both size and scope, and this article will be discussing the investments we’ve made to evolve Pushy for the next generation of features.\n\nClient Reach\n\nThis integration was initially rolled out for Fire TVs, PS4s, Samsung TVs, and LG TVs, leading to a reach of about 30 million candidate devices. With these clear benefits, we continued to build out this functionality for more devices, enabling the same efficiency wins. As of today, we’ve expanded our list of candidate devices even further to nearly a billion devices, including mobile devices running the Netflix app and the website experience. We’ve even extended support to older devices that lack modern capabilities, like support for TLS and HTTPS requests. For those, we’ve enabled secure communication from client to Pushy via an encryption/decryption layer on each, allowing for confidential messages to flow between the device and server.\n\nScaling to handle that growth (and more)\n\nGrowth\n\nWith that extended reach, Pushy has gotten busier. Over the last five years, Pushy has gone from tens of millions of concurrent connections to hundreds of millions of concurrent connections, and it regularly reaches 300,000 messages sent per second. To support this growth, we’ve revisited Pushy’s past assumptions and design decisions with an eye towards both Pushy’s future role and future stability. Pushy had been relatively hands-free operationally over the last few years, and as we updated Pushy to fit its evolving role, our goal was also to get it into a stable state for the next few years. This is particularly important as we build out new functionality that relies on Pushy; a strong, stable infrastructure foundation allows our partners to continue to build on top of Pushy with confidence.\n\nThroughout this evolution, we’ve been able to maintain high availability and a consistent message delivery rate, with Pushy successfully maintaining 99.999% reliability for message delivery over the last few months. When our partners want to deliver a message to a device, it’s our job to make sure they can do so.\n\nHere are a few of the ways we’ve evolved Pushy to handle its growing scale.\n\nA few of the related services in Pushy’s immediate ecosystem and the changes we’ve made for them.\n\nMessage processor\n\nOne aspect that we invested in was the evolution of the asynchronous message processor. The previous version of the message processor was a Mantis stream-processing job that processed messages from the message queue. It was very efficient, but it had a set job size, requiring manual intervention if we wanted to horizontally scale it, and it required manual intervention when rolling out a new version.\n\nIt served Pushy’s needs well for many years. As the scale of the messages being processed increased and we were making more code changes in the message processor, we found ourselves looking for something more flexible. In particular, we were looking for some of the features we enjoy with our other services: automatic horizontal scaling, canaries, automated red/black rollouts, and more observability. With this in mind, we rewrote the message processor as a standalone Spring Boot service using Netflix paved-path components. Its job is the same, but it does so with easy rollouts, canary configuration that lets us roll changes safely, and autoscaling policies we’ve defined to let it handle varying volumes.\n\nRewriting always comes with a risk, and it’s never the first solution we reach for, particularly when working with a system that’s in place and working well. In this case, we found that the burden from maintaining and improving the custom stream processing job was increasing, and we made the judgment call to do the rewrite. Part of the reason we did so was the clear role that the message processor played — we weren’t rewriting a huge monolithic service, but instead a well-scoped component that had explicit goals, well-defined success criteria, and a clear path towards improvement. Since the rewrite was completed in mid-2023, the message processor component has been completely zero touch, happily automated and running reliably on its own.\n\nPush Registry\n\nFor most of its life, Pushy has used Dynomite for keeping track of device connection metadata in its Push Registry. Dynomite is a Netflix open source wrapper around Redis that provides a few additional features like auto-sharding and cross-region replication, and it provided Pushy with low latency and easy record expiry, both of which are critical for Pushy’s workload.\n\nAs Pushy’s portfolio grew, we experienced some pain points with Dynomite. Dynomite had great performance, but it required manual scaling as the system grew. The folks on the Cloud Data Engineering (CDE) team, the ones building the paved path for internal data at Netflix, graciously helped us scale it up and make adjustments, but it ended up being an involved process as we kept growing.\n\nThese pain points coincided with the introduction of KeyValue, which was a new offering from the CDE team that is roughly “HashMap as a service” for Netflix developers. KeyValue is an abstraction over the storage engine itself, which allows us to choose the best storage engine that meets our SLO needs. In our case, we value low latency — the faster we can read from KeyValue, the faster these messages can get delivered. With CDE’s help, we migrated our Push Registry to use KV instead, and we have been extremely satisfied with the result. After tuning our store for Pushy’s needs, it has been on autopilot since, appropriately scaling and serving our requests with very low latency.\n\nScaling Pushy horizontally and vertically\n\nMost of the other services our team runs, like apiproxy, the streaming edge proxy, are CPU bound, and we have autoscaling policies that scale them horizontally when we see an increase in CPU usage. This maps well to their workload — more HTTP requests means more CPU used, and we can scale up and down accordingly.\n\nPushy has slightly different performance characteristics, with each node maintaining many connections and delivering messages on demand. In Pushy’s case, CPU usage is consistently low, since most of the connections are parked and waiting for an occasional message. Instead of relying on CPU, we scale Pushy on the number of connections, with exponential scaling to scale faster after higher thresholds are reached. We load balance the initial HTTP requests to establish the connections and rely on a reconnect protocol where devices will reconnect every 30 minutes or so, with some staggering, that gives us a steady stream of reconnecting devices to balance connections across all available instances.\n\nFor a few years, our scaling policy had been that we would add new instances when the average number of connections reached 60,000 connections per instance. For a couple hundred million devices, this meant that we were regularly running thousands of Pushy instances. We can horizontally scale Pushy to our heart’s content, but we would be less content with our bill and would have to shard Pushy further to get around NLB connection limits. This evolution effort aligned well with an internal focus on cost efficiency, and we used this as an opportunity to revisit these earlier assumptions with an eye towards efficiency.\n\nBoth of these would be helped by increasing the number of connections that each Pushy node could handle, reducing the total number of Pushy instances and running more efficiently with the right balance between instance type, instance cost, and maximum concurrent connections. It would also allow us to have more breathing room with the NLB limits, reducing the toil of additional sharding as we continue to grow. That being said, increasing the number of connections per node is not without its own drawbacks. When a Pushy instance goes down, the devices that were connected to it will immediately try to reconnect. By increasing the number of connections per instance, it means that we would be increasing the number of devices that would be immediately trying to reconnect. We could have a million connections per instance, but a down node would lead to a thundering herd of a million devices reconnecting at the same time.\n\nThis delicate balance led to us doing a deep evaluation of many instance types and performance tuning options. Striking that balance, we ended up with instances that handle an average of 200,000 connections per node, with breathing room to go up to 400,000 connections if we had to. This makes for a nice balance between CPU usage, memory usage, and the thundering herd when a device connects. We’ve also enhanced our autoscaling policies to scale exponentially; the farther we are past our target average connection count, the more instances we’ll add. These improvements have enabled Pushy to be almost entirely hands off operationally, giving us plenty of flexibility as more devices come online in different patterns.\n\nReliability & building a stable foundation\n\nAlongside these efforts to scale Pushy for the future, we also took a close look at our reliability after finding some connectivity edge cases during recent feature development. We found a few areas for improvement around the connection between Pushy and the device, with failures due to Pushy attempting to send messages on a connection that had failed without notifying Pushy. Ideally something like a silent failure wouldn’t happen, but we frequently see odd client behavior, particularly on older devices.\n\nIn collaboration with the client teams, we were able to make some improvements. On the client side, better connection handling and improvements around the reconnect flow meant that they were more likely to reconnect appropriately. In Pushy, we added additional heartbeats, idle connection cleanup, and better connection tracking, which meant that we were keeping around fewer and fewer stale connections.\n\nWhile these improvements were mostly around those edge cases for the feature development, they had the side benefit of bumping our message delivery rates up even further. We already had a good message delivery rate, but this additional bump has enabled Pushy to regularly average 5 9s of message delivery reliability.\n\nPush message delivery success rate over a recent 2-week period.\n\nRecent developments\n\nWith this stable foundation and all of these connections, what can we now do with them? This question has been the driving force behind nearly all of the recent features built on top of Pushy, and it’s an exciting question to ask, particularly as an infrastructure team.\n\nShift towards direct push\n\nThe first change from Pushy’s traditional role is what we call direct push; instead of a backend service dropping the message on the asynchronous message queue, it can instead leverage the Push library to skip the asynchronous queue entirely. When called to deliver a message in the direct path, the Push library will look up the Pushy connected to the target device in the Push Registry, then send the message directly to that Pushy. Pushy will respond with a status code reflecting whether it was able to successfully deliver the message or it encountered an error, and the Push library will bubble that up to the calling code in the service.\n\nThe system diagram for the direct and indirect push paths.\n\nSusheel, the original author of Pushy, added this functionality as an optional path, but for years, nearly all backend services relied on the indirect path with its “best-effort” being good enough for their use cases. In recent years, we’ve seen usage of this direct path really take off as the needs of backend services have grown. In particular, rather than being just best effort, these direct messages allow the calling service to have immediate feedback about the delivery, letting them retry if a device they’re targeting has gone offline.\n\nThese days, messages sent via direct push make up the majority of messages sent through Pushy. For example, for a recent 24 hour period, direct messages averaged around 160,000 messages per second and indirect averaged at around 50,000 messages per second..\n\nGraph of direct vs indirect messages per second.\n\nDevice to device messaging\n\nAs we’ve thought through this evolving use case, our concept of a message sender has also evolved. What if we wanted to move past Pushy’s pattern of delivering server-side messages? What if we wanted to have a device send a message to a backend service, or maybe even to another device? Our messages had traditionally been unidirectional as we send messages from the server to the device, but we now leverage these bidirectional connections and direct device messaging to enable what we call device to device messaging. This device to device messaging supported early phone-to-TV communication in support of games like Triviaverse, and it’s the messaging foundation for our Companion Mode as TVs and phones communicate back and forth.\n\nA screenshot of one of the authors playing Triviaquest with a mobile device as the controller.\n\nThis requires higher level knowledge of the system, where we need to know not just information about a single device, but more broader information, like what devices are connected for an account that the phone can pair with. This also enables things like subscribing to device events to know when another device comes online and when they’re available to pair or send a message to. This has been built out with an additional service that receives device connection information from Pushy. These events, sent over a Kafka topic, let the service keep track of the device list for a given account. Devices can subscribe to these events, allowing them to receive a message from the service when another device for the same account comes online.\n\nPushy and its relationship with the Device List Service for discovering other devices.\n\nThis device list enables the discoverability aspect of these device to device messages. Once the devices have this knowledge of the other devices connected for the same account, they’re able to choose a target device from this list that they can then send messages to.\n\nOnce a device has that list, it can send a message to Pushy over its WebSocket connection with that device as the target in what we call a device to device message (1 in the diagram below). Pushy looks up the target device’s metadata in the Push registry (2) and sends the message to the second Pushy that the target device is connected to (3), as if it was the backend service in the direct push pattern above. That Pushy delivers the message to the target device (4), and the original Pushy will receive a status code in response, which it can pass back to the source device (5).\n\nA basic order of events for a device to device message.\n\nThe messaging protocol\n\nWe’ve defined a basic JSON-based message protocol for device to device messaging that lets these messages be passed from the source device to the target device. As a networking team, we naturally lean towards abstracting the communication layer with encapsulation wherever possible. This generalized message means that device teams are able to define their own protocols on top of these messages — Pushy would just be the transport layer, happily forwarding messages back and forth.\n\nThe client app protocol, built on top of the device to device protocol, built on top of Pushy.\n\nThis generalization paid off in terms of investment and operational support. We built the majority of this functionality in October 2022, and we’ve only needed small tweaks since then. We needed nearly no modifications as client teams built out the functionality on top of this layer, defining the higher level application-specific protocols that powered the features they were building. We really do enjoy working with our partner teams, but if we’re able to give them the freedom to build on top of our infrastructure layer without us getting involved, then we’re able to increase their velocity, make their lives easier, and play our infrastructure roles as message platform providers.\n\nWith early features in experimentation, Pushy sees an average of 1000 device to device messages per second, a number that will only continue to grow.\n\nGraph of device to device messages per second.\n\nThe Netty-gritty details\n\nIn Pushy, we handle incoming WebSocket messages in our PushClientProtocolHandler (code pointer to class in Zuul that we extend), which extends Netty’s ChannelInboundHandlerAdapter and is added to the Netty pipeline for each client connection. We listen for incoming WebSocket messages from the connected device in its channelRead method and parse the incoming message. If it’s a device to device message, we pass the message, the ChannelHandlerContext, and the PushUserAuth information about the connection’s identity to our DeviceToDeviceManager.\n\nA rough overview of the internal organization for these components.\n\nThe DeviceToDeviceManager is responsible for validating the message, doing some bookkeeping, and kicking off an async call that validates that the device is an authorized target, looks up the Pushy for the target device in the local cache (or makes a call to the data store if it’s not found), and forwards on the message. We run this asynchronously to avoid any event loop blocking due to these calls. The DeviceToDeviceManager is also responsible for observability, with metrics around cache hits, calls to the data store, message delivery rates, and latency percentile measurements. We’ve relied heavily on these metrics for alerts and optimizations — Pushy really is a metrics service that occasionally will deliver a message or two!\n\nSecurity\n\nAs the edge of the Netflix cloud, security considerations are always top of mind. With every connection over HTTPS, we’ve limited these messages to just authenticated WebSocket connections, added rate limiting, and added authorization checks to ensure that a device is able to target another device — you may have the best intentions in mind, but I’d strongly prefer it if you weren’t able to send arbitrary data to my personal TV from yours (and vice versa, I’m sure!).\n\nLatency and other considerations\n\nOne main consideration with the products built on top of this is latency, particularly when this feature is used for anything interactive within the Netflix app.\n\nWe’ve added caching to Pushy to reduce the number of lookups in the hotpath for things that are unlikely to change frequently, like a device’s allowed list of targets and the Pushy instance the target device is connected to. We have to do some lookups on the initial messages to know where to send them, but it enables us to send subsequent messages faster without any KeyValue lookups. For these requests where caching removed KeyValue from the hot path, we were able to greatly speed things up. From the incoming message arriving at Pushy to the response being sent back to the device, we reduced median latency to less than a millisecond, with the 99th percentile of latency at less than 4ms.\n\nOur KeyValue latency is usually very low, but we have seen brief periods of elevated read latencies due to underlying issues in our KeyValue datastore. Overall latencies increased for other parts of Pushy, like client registration, but we saw very little increase in device to device latency with this caching in place.\n\nCultural aspects that enable this work\n\nPushy’s scale and system design considerations make the work technically interesting, but we also deliberately focus on non-technical aspects that have helped to drive Pushy’s growth. We focus on iterative development that solves the hardest problem first, with projects frequently starting with quick hacks or prototypes to prove out a feature. As we do this initial version, we do our best to keep an eye towards the future, allowing us to move quickly from supporting a single, focused use case to a broad, generalized solution. For example, for our cross-device messaging, we were able to solve hard problems in the early work for Triviaverse that we later leveraged for the generic device to device solution.\n\nAs one can immediately see in the system diagrams above, Pushy does not exist in a vacuum, with projects frequently involving at least half a dozen teams. Trust, experience, communication, and strong relationships all enable this to work. Our team wouldn’t exist without our platform users, and we certainly wouldn’t be here writing this post without all of the work our product and client teams do. This has also emphasized the importance of building and sharing — if we’re able to get a prototype together with a device team, we’re able to then show it off to seed ideas from other teams. It’s one thing to mention that you can send these messages, but it’s another to show off the TV responding to the first click of the phone controller button!\n\nThe future of Pushy\n\nIf there’s anything certain in this world, it’s that Pushy will continue to grow and evolve. We have many new features in the works, like WebSocket message proxying, WebSocket message tracing, a global broadcast mechanism, and subscription functionality in support of Games and Live. With all of this investment, Pushy is a stable, reinforced foundation, ready for this next generation of features.\n\nWe’ll be writing about those new features as well — stay tuned for future posts.\n\nSpecial thanks to our stunning colleagues Jeremy Kelly and Justin Guerra who have both been invaluable to Pushy’s growth and the WebSocket ecosystem at large. We would also like to thank our larger teams and our numerous partners for their great work; it truly takes a village!", "label": "non_personal"}
{"title": "Introducing Netflix’s Key-Value Data Abstraction Layer", "url": "https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30?source=collection_home---4------14-----------------------", "content": "Introducing Netflix’s Key-Value Data Abstraction Layer Netflix Technology Blog 13 min read · Sep 18, 2024 -- 10 Listen Share\n\nVidhya Arvind, Rajasekhar Ummadisetty, Joey Lynch, Vinay Chella\n\nIntroduction\n\nAt Netflix our ability to deliver seamless, high-quality, streaming experiences to millions of users hinges on robust, global backend infrastructure. Central to this infrastructure is our use of multiple online distributed databases such as Apache Cassandra, a NoSQL database known for its high availability and scalability. Cassandra serves as the backbone for a diverse array of use cases within Netflix, ranging from user sign-ups and storing viewing histories to supporting real-time analytics and live streaming.\n\nOver time as new key-value databases were introduced and service owners launched new use cases, we encountered numerous challenges with datastore misuse. Firstly, developers struggled to reason about consistency, durability and performance in this complex global deployment across multiple stores. Second, developers had to constantly re-learn new data modeling practices and common yet critical data access patterns. These include challenges with tail latency and idempotency, managing “wide” partitions with many rows, handling single large “fat” columns, and slow response pagination. Additionally, the tight coupling with multiple native database APIs — APIs that continually evolve and sometimes introduce backward-incompatible changes — resulted in org-wide engineering efforts to maintain and optimize our microservice’s data access.\n\nTo overcome these challenges, we developed a holistic approach that builds upon our Data Gateway Platform. This approach led to the creation of several foundational abstraction services, the most mature of which is our Key-Value (KV) Data Abstraction Layer (DAL). This abstraction simplifies data access, enhances the reliability of our infrastructure, and enables us to support the broad spectrum of use cases that Netflix demands with minimal developer effort.\n\nIn this post, we dive deep into how Netflix’s KV abstraction works, the architectural principles guiding its design, the challenges we faced in scaling diverse use cases, and the technical innovations that have allowed us to achieve the performance and reliability required by Netflix’s global operations.\n\nThe Key-Value Service\n\nThe KV data abstraction service was introduced to solve the persistent challenges we faced with data access patterns in our distributed databases. Our goal was to build a versatile and efficient data storage solution that could handle a wide variety of use cases, ranging from the simplest hashmaps to more complex data structures, all while ensuring high availability, tunable consistency, and low latency.\n\nData Model\n\nAt its core, the KV abstraction is built around a two-level map architecture. The first level is a hashed string ID (the primary key), and the second level is a sorted map of a key-value pair of bytes. This model supports both simple and complex data models, balancing flexibility and efficiency.\n\nHashMap<String, SortedMap<Bytes, Bytes>>\n\nFor complex data models such as structured Records or time-ordered Events , this two-level approach handles hierarchical structures effectively, allowing related data to be retrieved together. For simpler use cases, it also represents flat key-value Maps (e.g. id → {\"\" → value} ) or named Sets (e.g. id → {key → \"\"} ). This adaptability allows the KV abstraction to be used in hundreds of diverse use cases, making it a versatile solution for managing both simple and complex data models in large-scale infrastructures like Netflix.\n\nThe KV data can be visualized at a high level, as shown in the diagram below, where three records are shown.\n\nmessage Item (\n\nBytes key,\n\nBytes value,\n\nMetadata metadata,\n\nInteger chunk\n\n)\n\nDatabase Agnostic Abstraction\n\nThe KV abstraction is designed to hide the implementation details of the underlying database, offering a consistent interface to application developers regardless of the optimal storage system for that use case. While Cassandra is one example, the abstraction works with multiple data stores like EVCache, DynamoDB, RocksDB, etc…\n\nFor example, when implemented with Cassandra, the abstraction leverages Cassandra’s partitioning and clustering capabilities. The record ID acts as the partition key, and the item key as the clustering column:\n\nThe corresponding Data Definition Language (DDL) for this structure in Cassandra is:\n\nCREATE TABLE IF NOT EXISTS <ns>.<table> (\n\nid text,\n\nkey blob,\n\nvalue blob,\n\nvalue_metadata blob,\n\n\n\nPRIMARY KEY (id, key))\n\nWITH CLUSTERING ORDER BY (key <ASC|DESC>)\n\nNamespace: Logical and Physical Configuration\n\nA namespace defines where and how data is stored, providing logical and physical separation while abstracting the underlying storage systems. It also serves as central configuration of access patterns such as consistency or latency targets. Each namespace may use different backends: Cassandra, EVCache, or combinations of multiple. This flexibility allows our Data Platform to route different use cases to the most suitable storage system based on performance, durability, and consistency needs. Developers just provide their data problem rather than a database solution!\n\nIn this example configuration, the ngsegment namespace is backed by both a Cassandra cluster and an EVCache caching layer, allowing for highly durable persistent storage and lower-latency point reads.\n\n\"persistence_configuration\":[\n\n{\n\n\"id\":\"PRIMARY_STORAGE\",\n\n\"physical_storage\": {\n\n\"type\":\"CASSANDRA\",\n\n\"cluster\":\"cassandra_kv_ngsegment\",\n\n\"dataset\":\"ngsegment\",\n\n\"table\":\"ngsegment\",\n\n\"regions\": [\"us-east-1\"],\n\n\"config\": {\n\n\"consistency_scope\": \"LOCAL\",\n\n\"consistency_target\": \"READ_YOUR_WRITES\"\n\n}\n\n}\n\n},\n\n{\n\n\"id\":\"CACHE\",\n\n\"physical_storage\": {\n\n\"type\":\"CACHE\",\n\n\"cluster\":\"evcache_kv_ngsegment\"\n\n},\n\n\"config\": {\n\n\"default_cache_ttl\": 180s\n\n}\n\n}\n\n]\n\n\n\nKey APIs of the KV Abstraction\n\nTo support diverse use-cases, the KV abstraction provides four basic CRUD APIs:\n\nPutItems — Write one or more Items to a Record\n\nThe PutItems API is an upsert operation, it can insert new data or update existing data in the two-level map structure.\n\nmessage PutItemRequest (\n\nIdempotencyToken idempotency_token,\n\nstring namespace,\n\nstring id,\n\nList<Item> items\n\n)\n\nAs you can see, the request includes the namespace, Record ID, one or more items, and an idempotency token to ensure retries of the same write are safe. Chunked data can be written by staging chunks and then committing them with appropriate metadata (e.g. number of chunks).\n\nGetItems — Read one or more Items from a Record\n\nThe GetItems API provides a structured and adaptive way to fetch data using ID, predicates, and selection mechanisms. This approach balances the need to retrieve large volumes of data while meeting stringent Service Level Objectives (SLOs) for performance and reliability.\n\nmessage GetItemsRequest (\n\nString namespace,\n\nString id,\n\nPredicate predicate,\n\nSelection selection,\n\nMap<String, Struct> signals\n\n)\n\nThe GetItemsRequest includes several key parameters:\n\nNamespace : Specifies the logical dataset or table\n\n: Specifies the logical dataset or table Id : Identifies the entry in the top-level HashMap\n\n: Identifies the entry in the top-level HashMap Predicate : Filters the matching items and can retrieve all items ( match_all ), specific items ( match_keys ), or a range ( match_range )\n\n: Filters the matching items and can retrieve all items ( ), specific items ( ), or a range ( ) Selection : Narrows returned responses for example page_size_bytes for pagination, item_limit for limiting the total number of items across pages and include / exclude to include or exclude large values from responses\n\n: Narrows returned responses for example for pagination, for limiting the total number of items across pages and / to include or exclude large values from responses Signals: Provides in-band signaling to indicate client capabilities, such as supporting client compression or chunking.\n\nThe GetItemResponse message contains the matching data:\n\nmessage GetItemResponse (\n\nList<Item> items,\n\nOptional<String> next_page_token\n\n)\n\nItems : A list of retrieved items based on the Predicate and Selection defined in the request.\n\n: A list of retrieved items based on the and defined in the request. Next Page Token: An optional token indicating the position for subsequent reads if needed, essential for handling large data sets across multiple requests. Pagination is a critical component for efficiently managing data retrieval, especially when dealing with large datasets that could exceed typical response size limits.\n\nDeleteItems — Delete one or more Items from a Record\n\nThe DeleteItems API provides flexible options for removing data, including record-level, item-level, and range deletes — all while supporting idempotency.\n\nmessage DeleteItemsRequest (\n\nIdempotencyToken idempotency_token,\n\nString namespace,\n\nString id,\n\nPredicate predicate\n\n)\n\n\n\nJust like in the GetItems API, the Predicate allows one or more Items to be addressed at once:\n\nRecord-Level Deletes (match_all) : Removes the entire record in constant latency regardless of the number of items in the record.\n\n: Removes the entire record in constant latency regardless of the number of items in the record. Item-Range Deletes (match_range) : This deletes a range of items within a Record. Useful for keeping “n-newest” or prefix path deletion.\n\n: This deletes a range of items within a Record. Useful for keeping “n-newest” or prefix path deletion. Item-Level Deletes (match_keys): Deletes one or more individual items.\n\nSome storage engines (any store which defers true deletion) such as Cassandra struggle with high volumes of deletes due to tombstone and compaction overhead. Key-Value optimizes both record and range deletes to generate a single tombstone for the operation — you can learn more about tombstones in About Deletes and Tombstones.\n\nItem-level deletes create many tombstones but KV hides that storage engine complexity via TTL-based deletes with jitter. Instead of immediate deletion, item metadata is updated as expired with randomly jittered TTL applied to stagger deletions. This technique maintains read pagination protections. While this doesn’t completely solve the problem it reduces load spikes and helps maintain consistent performance while compaction catches up. These strategies help maintain system performance, reduce read overhead, and meet SLOs by minimizing the impact of deletes.\n\nComplex Mutate and Scan APIs\n\nBeyond simple CRUD on single Records, KV also supports complex multi-item and multi-record mutations and scans via MutateItems and ScanItems APIs. PutItems also supports atomic writes of large blob data within a single Item via a chunked protocol. These complex APIs require careful consideration to ensure predictable linear low-latency and we will share details on their implementation in a future post.\n\nDesign Philosophies for reliable and predictable performance\n\nIdempotency to fight tail latencies\n\nTo ensure data integrity the PutItems and DeleteItems APIs use idempotency tokens, which uniquely identify each mutative operation and guarantee that operations are logically executed in order, even when hedged or retried for latency reasons. This is especially crucial in last-write-wins databases like Cassandra, where ensuring the correct order and de-duplication of requests is vital.\n\nIn the Key-Value abstraction, idempotency tokens contain a generation timestamp and random nonce token. Either or both may be required by backing storage engines to de-duplicate mutations.\n\nmessage IdempotencyToken (\n\nTimestamp generation_time,\n\nString token\n\n)\n\nAt Netflix, client-generated monotonic tokens are preferred due to their reliability, especially in environments where network delays could impact server-side token generation. This combines a client provided monotonic generation_time timestamp with a 128 bit random UUID token . Although clock-based token generation can suffer from clock skew, our tests on EC2 Nitro instances show drift is minimal (under 1 millisecond). In some cases that require stronger ordering, regionally unique tokens can be generated using tools like Zookeeper, or globally unique tokens such as a transaction IDs can be used.\n\nThe following graphs illustrate the observed clock skew on our Cassandra fleet, suggesting the safety of this technique on modern cloud VMs with direct access to high-quality clocks. To further maintain safety, KV servers reject writes bearing tokens with large drift both preventing silent write discard (write has timestamp far in past) and immutable doomstones (write has a timestamp far in future) in storage engines vulnerable to those.\n\nHandling Large Data through Chunking\n\nKey-Value is also designed to efficiently handle large blobs, a common challenge for traditional key-value stores. Databases often face limitations on the amount of data that can be stored per key or partition. To address these constraints, KV uses transparent chunking to manage large data efficiently.\n\nFor items smaller than 1 MiB, data is stored directly in the main backing storage (e.g. Cassandra), ensuring fast and efficient access. However, for larger items, only the id, key, and metadata are stored in the primary storage, while the actual data is split into smaller chunks and stored separately in chunk storage. This chunk storage can also be Cassandra but with a different partitioning scheme optimized for handling large values. The idempotency token ties all these writes together into one atomic operation.\n\nBy splitting large items into chunks, we ensure that latency scales linearly with the size of the data, making the system both predictable and efficient. A future blog post will describe the chunking architecture in more detail, including its intricacies and optimization strategies.\n\nClient-Side Compression\n\nThe KV abstraction leverages client-side payload compression to optimize performance, especially for large data transfers. While many databases offer server-side compression, handling compression on the client side reduces expensive server CPU usage, network bandwidth, and disk I/O. In one of our deployments, which helps power Netflix’s search, enabling client-side compression reduced payload sizes by 75%, significantly improving cost efficiency.\n\nSmarter Pagination\n\nWe chose payload size in bytes as the limit per response page rather than the number of items because it allows us to provide predictable operation SLOs. For instance, we can provide a single-digit millisecond SLO on a 2 MiB page read. Conversely, using the number of items per page as the limit would result in unpredictable latencies due to significant variations in item size. A request for 10 items per page could result in vastly different latencies if each item was 1 KiB versus 1 MiB.\n\nUsing bytes as a limit poses challenges as few backing stores support byte-based pagination; most data stores use the number of results e.g. DynamoDB and Cassandra limit by number of items or rows. To address this, we use a static limit for the initial queries to the backing store, query with this limit, and process the results. If more data is needed to meet the byte limit, additional queries are executed until the limit is met, the excess result is discarded and a page token is generated.\n\nThis static limit can lead to inefficiencies, one large item in the result may cause us to discard many results, while small items may require multiple iterations to fill a page, resulting in read amplification. To mitigate these issues, we implemented adaptive pagination which dynamically tunes the limits based on observed data.\n\nAdaptive Pagination\n\nWhen an initial request is made, a query is executed in the storage engine, and the results are retrieved. As the consumer processes these results, the system tracks the number of items consumed and the total size used. This data helps calculate an approximate item size, which is stored in the page token. For subsequent page requests, this stored information allows the server to apply the appropriate limits to the underlying storage, reducing unnecessary work and minimizing read amplification.\n\nWhile this method is effective for follow-up page requests, what happens with the initial request? In addition to storing item size information in the page token, the server also estimates the average item size for a given namespace and caches it locally. This cached estimate helps the server set a more optimal limit on the backing store for the initial request, improving efficiency. The server continuously adjusts this limit based on recent query patterns or other factors to keep it accurate. For subsequent pages, the server uses both the cached data and the information in the page token to fine-tune the limits.\n\nIn addition to adaptive pagination, a mechanism is in place to send a response early if the server detects that processing the request is at risk of exceeding the request’s latency SLO.\n\nFor example, let us assume a client submits a GetItems request with a per-page limit of 2 MiB and a maximum end-to-end latency limit of 500ms. While processing this request, the server retrieves data from the backing store. This particular record has thousands of small items so it would normally take longer than the 500ms SLO to gather the full page of data. If this happens, the client would receive an SLO violation error, causing the request to fail even though there is nothing exceptional. To prevent this, the server tracks the elapsed time while fetching data. If it determines that continuing to retrieve more data might breach the SLO, the server will stop processing further results and return a response with a pagination token.\n\nThis approach ensures that requests are processed within the SLO, even if the full page size isn’t met, giving clients predictable progress. Furthermore, if the client is a gRPC server with proper deadlines, the client is smart enough not to issue further requests, reducing useless work.\n\nIf you want to know more, the How Netflix Ensures Highly-Reliable Online Stateful Systems article talks in further detail about these and many other techniques.\n\nSignaling\n\nKV uses in-band messaging we call signaling that allows the dynamic configuration of the client and enables it to communicate its capabilities to the server. This ensures that configuration settings and tuning parameters can be exchanged seamlessly between the client and server. Without signaling, the client would need static configuration — requiring a redeployment for each change — or, with dynamic configuration, would require coordination with the client team.\n\nFor server-side signals, when the client is initialized, it sends a handshake to the server. The server responds back with signals, such as target or max latency SLOs, allowing the client to dynamically adjust timeouts and hedging policies. Handshakes are then made periodically in the background to keep the configuration current. For client-communicated signals, the client, along with each request, communicates its capabilities, such as whether it can handle compression, chunking, and other features.\n\nKV Usage @ Netflix\n\nThe KV abstraction powers several key Netflix use cases, including:\n\nStreaming Metadata : High-throughput, low-latency access to streaming metadata, ensuring personalized content delivery in real-time.\n\n: High-throughput, low-latency access to streaming metadata, ensuring personalized content delivery in real-time. User Profiles : Efficient storage and retrieval of user preferences and history, enabling seamless, personalized experiences across devices.\n\n: Efficient storage and retrieval of user preferences and history, enabling seamless, personalized experiences across devices. Messaging : Storage and retrieval of push registry for messaging needs, enabling the millions of requests to flow through.\n\n: Storage and retrieval of push registry for messaging needs, enabling the millions of requests to flow through. Real-Time Analytics: This persists large-scale impression and provides insights into user behavior and system performance, moving data from offline to online and vice versa.\n\nFuture Enhancements\n\nLooking forward, we plan to enhance the KV abstraction with:\n\nLifecycle Management : Fine-grained control over data retention and deletion.\n\n: Fine-grained control over data retention and deletion. Summarization : Techniques to improve retrieval efficiency by summarizing records with many items into fewer backing rows.\n\n: Techniques to improve retrieval efficiency by summarizing records with many items into fewer backing rows. New Storage Engines : Integration with more storage systems to support new use cases.\n\n: Integration with more storage systems to support new use cases. Dictionary Compression: Further reducing data size while maintaining performance.\n\nConclusion\n\nThe Key-Value service at Netflix is a flexible, cost-effective solution that supports a wide range of data patterns and use cases, from low to high traffic scenarios, including critical Netflix streaming use-cases. The simple yet robust design allows it to handle diverse data models like HashMaps, Sets, Event storage, Lists, and Graphs. It abstracts the complexity of the underlying databases from our developers, which enables our application engineers to focus on solving business problems instead of becoming experts in every storage engine and their distributed consistency models. As Netflix continues to innovate in online datastores, the KV abstraction remains a central component in managing data efficiently and reliably at scale, ensuring a solid foundation for future growth.\n\nAcknowledgments: Special thanks to our stunning colleagues who contributed to Key Value’s success: William Schor, Mengqing Wang, Chandrasekhar Thumuluru, Rajiv Shringi, John Lu, George Cambell, Ammar Khaku, Jordan West, Chris Lohfink, Matt Lehman, and the whole online datastores team (ODS, f.k.a CDE).", "label": "non_personal"}
{"title": "Introducing Netflix’s TimeSeries Data Abstraction Layer", "url": "https://netflixtechblog.com/introducing-netflix-timeseries-data-abstraction-layer-31552f6326f8?source=collection_home---4------13-----------------------", "content": "Introducing Netflix’s TimeSeries Data Abstraction Layer Netflix Technology Blog 18 min read · Oct 8, 2024 -- 16 Listen Share\n\nBy Rajiv Shringi, Vinay Chella, Kaidan Fullerton, Oleksii Tkachuk, Joey Lynch\n\nIntroduction\n\nAs Netflix continues to expand and diversify into various sectors like Video on Demand and Gaming, the ability to ingest and store vast amounts of temporal data — often reaching petabytes — with millisecond access latency has become increasingly vital. In previous blog posts, we introduced the Key-Value Data Abstraction Layer and the Data Gateway Platform, both of which are integral to Netflix’s data architecture. The Key-Value Abstraction offers a flexible, scalable solution for storing and accessing structured key-value data, while the Data Gateway Platform provides essential infrastructure for protecting, configuring, and deploying the data tier.\n\nBuilding on these foundational abstractions, we developed the TimeSeries Abstraction — a versatile and scalable solution designed to efficiently store and query large volumes of temporal event data with low millisecond latencies, all in a cost-effective manner across various use cases.\n\nIn this post, we will delve into the architecture, design principles, and real-world applications of the TimeSeries Abstraction, demonstrating how it enhances our platform’s ability to manage temporal data at scale.\n\nNote: Contrary to what the name may suggest, this system is not built as a general-purpose time series database. We do not use it for metrics, histograms, timers, or any such near-real time analytics use case. Those use cases are well served by the Netflix Atlas telemetry system. Instead, we focus on addressing the challenge of storing and accessing extremely high-throughput, immutable temporal event data in a low-latency and cost-efficient manner.\n\nChallenges\n\nAt Netflix, temporal data is continuously generated and utilized, whether from user interactions like video-play events, asset impressions, or complex micro-service network activities. Effectively managing this data at scale to extract valuable insights is crucial for ensuring optimal user experiences and system reliability.\n\nHowever, storing and querying such data presents a unique set of challenges:\n\nHigh Throughput : Managing up to 10 million writes per second while maintaining high availability.\n\n: Managing up to 10 million writes per second while maintaining high availability. Efficient Querying in Large Datasets : Storing petabytes of data while ensuring primary key reads return results within low double-digit milliseconds, and supporting searches and aggregations across multiple secondary attributes.\n\n: Storing petabytes of data while ensuring primary key reads return results within low double-digit milliseconds, and supporting searches and aggregations across multiple secondary attributes. Global Reads and Writes : Facilitating read and write operations from anywhere in the world with adjustable consistency models.\n\n: Facilitating read and write operations from anywhere in the world with adjustable consistency models. Tunable Configuration : Offering the ability to partition datasets in either a single-tenant or multi-tenant datastore, with options to adjust various dataset aspects such as retention and consistency.\n\n: Offering the ability to partition datasets in either a single-tenant or multi-tenant datastore, with options to adjust various dataset aspects such as retention and consistency. Handling Bursty Traffic : Managing significant traffic spikes during high-demand events, such as new content launches or regional failovers.\n\n: Managing significant traffic spikes during high-demand events, such as new content launches or regional failovers. Cost Efficiency: Reducing the cost per byte and per operation to optimize long-term retention while minimizing infrastructure expenses, which can amount to millions of dollars for Netflix.\n\nTimeSeries Abstraction\n\nThe TimeSeries Abstraction was developed to meet these requirements, built around the following core design principles:\n\nPartitioned Data : Data is partitioned using a unique temporal partitioning strategy combined with an event bucketing approach to efficiently manage bursty workloads and streamline queries.\n\n: Data is partitioned using a unique temporal partitioning strategy combined with an event bucketing approach to efficiently manage bursty workloads and streamline queries. Flexible Storage : The service is designed to integrate with various storage backends, including Apache Cassandra and Elasticsearch, allowing Netflix to customize storage solutions based on specific use case requirements.\n\n: The service is designed to integrate with various storage backends, including Apache Cassandra and Elasticsearch, allowing Netflix to customize storage solutions based on specific use case requirements. Configurability : TimeSeries offers a range of tunable options for each dataset, providing the flexibility needed to accommodate a wide array of use cases.\n\n: TimeSeries offers a range of tunable options for each dataset, providing the flexibility needed to accommodate a wide array of use cases. Scalability : The architecture supports both horizontal and vertical scaling, enabling the system to handle increasing throughput and data volumes as Netflix expands its user base and services.\n\n: The architecture supports both horizontal and vertical scaling, enabling the system to handle increasing throughput and data volumes as Netflix expands its user base and services. Sharded Infrastructure: Leveraging the Data Gateway Platform, we can deploy single-tenant and/or multi-tenant infrastructure with the necessary access and traffic isolation.\n\nLet’s dive into the various aspects of this abstraction.\n\nData Model\n\nWe follow a unique event data model that encapsulates all the data we want to capture for events, while allowing us to query them efficiently.\n\nLet’s start with the smallest unit of data in the abstraction and work our way up.\n\nEvent Item : An event item is a key-value pair that users use to store data for a given event. For example: {“device_type”: “ios”}.\n\n: An event item is a key-value pair that users use to store data for a given event. For example: {“device_type”: “ios”}. Event : An event is a structured collection of one or more such event items. An event occurs at a specific point in time and is identified by a client-generated timestamp and an event identifier (such as a UUID). This combination of event_time and event_id also forms part of the unique idempotency key for the event, enabling users to safely retry requests.\n\n: An event is a structured collection of one or more such event items. An event occurs at a specific point in time and is identified by a client-generated timestamp and an event identifier (such as a UUID). This combination of and also forms part of the unique idempotency key for the event, enabling users to safely retry requests. Time Series ID : A time_series_id is a collection of one or more such events over the dataset’s retention period. For instance, a device_id would store all events occurring for a given device over the retention period. All events are immutable, and the TimeSeries service only ever appends events to a given time series ID.\n\n: A is a collection of one or more such events over the dataset’s retention period. For instance, a would store all events occurring for a given device over the retention period. All events are immutable, and the TimeSeries service only ever appends events to a given time series ID. Namespace: A namespace is a collection of time series IDs and event data, representing the complete TimeSeries dataset. Users can create one or more namespaces for each of their use cases. The abstraction applies various tunable options at the namespace level, which we will discuss further when we explore the service’s control plane.\n\nAPI\n\nThe abstraction provides the following APIs to interact with the event data.\n\nWriteEventRecordsSync: This endpoint writes a batch of events and sends back a durability acknowledgement to the client. This is used in cases where users require a guarantee of durability.\n\nWriteEventRecords: This is the fire-and-forget version of the above endpoint. It enqueues a batch of events without the durability acknowledgement. This is used in cases like logging or tracing, where users care more about throughput and can tolerate a small amount of data loss.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"events\": [\n\n{\n\n\"timeSeriesId\": \"profile100\",\n\n\"eventTime\": \"2024-10-03T21:24:23.988Z\",\n\n\"eventId\": \"550e8400-e29b-41d4-a716-446655440000\",\n\n\"eventItems\": [\n\n{\n\n\"eventItemKey\": \"deviceType\",\n\n\"eventItemValue\": \"aW9z\"\n\n},\n\n{\n\n\"eventItemKey\": \"deviceMetadata\",\n\n\"eventItemValue\": \"c29tZSBtZXRhZGF0YQ==\"\n\n}\n\n]\n\n},\n\n{\n\n\"timeSeriesId\": \"profile100\",\n\n\"eventTime\": \"2024-10-03T21:23:30.000Z\",\n\n\"eventId\": \"123e4567-e89b-12d3-a456-426614174000\",\n\n\"eventItems\": [\n\n{\n\n\"eventItemKey\": \"deviceType\",\n\n\"eventItemValue\": \"YW5kcm9pZA==\"\n\n}\n\n]\n\n}\n\n]\n\n}\n\nReadEventRecords: Given a combination of a namespace, a timeSeriesId, a timeInterval, and optional eventFilters, this endpoint returns all the matching events, sorted descending by event_time, with low millisecond latency.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"timeSeriesId\": \"profile100\",\n\n\"timeInterval\": {\n\n\"start\": \"2024-10-02T21:00:00.000Z\",\n\n\"end\": \"2024-10-03T21:00:00.000Z\"\n\n},\n\n\"eventFilters\": [\n\n{\n\n\"matchEventItemKey\": \"deviceType\",\n\n\"matchEventItemValue\": \"aW9z\"\n\n}\n\n],\n\n\"pageSize\": 100,\n\n\"totalRecordLimit\": 1000\n\n}\n\nSearchEventRecords: Given a search criteria and a time interval, this endpoint returns all the matching events. These use cases are fine with eventually consistent reads.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"timeInterval\": {\n\n\"start\": \"2024-10-02T21:00:00.000Z\",\n\n\"end\": \"2024-10-03T21:00:00.000Z\"\n\n},\n\n\"searchQuery\": {\n\n\"booleanQuery\": {\n\n\"searchQuery\": [\n\n{\n\n\"equals\": {\n\n\"eventItemKey\": \"deviceType\",\n\n\"eventItemValue\": \"aW9z\"\n\n}\n\n},\n\n{\n\n\"range\": {\n\n\"eventItemKey\": \"deviceRegistrationTimestamp\",\n\n\"lowerBound\": {\n\n\"eventItemValue\": \"MjAyNC0xMC0wMlQwMDowMDowMC4wMDBa\",\n\n\"inclusive\": true\n\n},\n\n\"upperBound\": {\n\n\"eventItemValue\": \"MjAyNC0xMC0wM1QwMDowMDowMC4wMDBa\"\n\n}\n\n}\n\n}\n\n],\n\n\"operator\": \"AND\"\n\n}\n\n},\n\n\"pageSize\": 100,\n\n\"totalRecordLimit\": 1000\n\n}\n\nAggregateEventRecords: Given a search criteria and an aggregation mode (e.g. DistinctAggregation) , this endpoint performs the given aggregation within a given time interval. Similar to the Search endpoint, users can tolerate eventual consistency and a potentially higher latency (in seconds).\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"timeInterval\": {\n\n\"start\": \"2024-10-02T21:00:00.000Z\",\n\n\"end\": \"2024-10-03T21:00:00.000Z\"\n\n},\n\n\"searchQuery\": {...some search criteria...},\n\n\"aggregationQuery\": {\n\n\"distinct\": {\n\n\"eventItemKey\": \"deviceType\",\n\n\"pageSize\": 100\n\n}\n\n}\n\n}\n\nIn the subsequent sections, we will talk about how we interact with this data at the storage layer.\n\nStorage Layer\n\nThe storage layer for TimeSeries comprises a primary data store and an optional index data store. The primary data store ensures data durability during writes and is used for primary read operations, while the index data store is utilized for search and aggregate operations. At Netflix, Apache Cassandra is the preferred choice for storing durable data in high-throughput scenarios, while Elasticsearch is the preferred data store for indexing. However, similar to our approach with the API, the storage layer is not tightly coupled to these specific data stores. Instead, we define storage API contracts that must be fulfilled, allowing us the flexibility to replace the underlying data stores as needed.\n\nPrimary Datastore\n\nIn this section, we will talk about how we leverage Apache Cassandra for TimeSeries use cases.\n\nPartitioning Scheme\n\nAt Netflix’s scale, the continuous influx of event data can quickly overwhelm traditional databases. Temporal partitioning addresses this challenge by dividing the data into manageable chunks based on time intervals, such as hourly, daily, or monthly windows. This approach enables efficient querying of specific time ranges without the need to scan the entire dataset. It also allows Netflix to archive, compress, or delete older data efficiently, optimizing both storage and query performance. Additionally, this partitioning mitigates the performance issues typically associated with wide partitions in Cassandra. By employing this strategy, we can operate at much higher disk utilization, as it reduces the need to reserve large amounts of disk space for compactions, thereby saving costs.\n\nHere is what it looks like :\n\nTime Slice: A time slice is the unit of data retention and maps directly to a Cassandra table. We create multiple such time slices, each covering a specific interval of time. An event lands in one of these slices based on the event_time. These slices are joined with no time gaps in between, with operations being start-inclusive and end-exclusive, ensuring that all data lands in one of the slices. By utilizing these time slices, we can efficiently implement retention by dropping entire tables, which reduces storage space and saves on costs.\n\nWhy not use row-based Time-To-Live (TTL)?\n\nUsing TTL on individual events would generate a significant number of tombstones in Cassandra, degrading performance, especially during range scans. By employing discrete time slices and dropping them, we avoid the tombstone issue entirely. The tradeoff is that data may be retained slightly longer than necessary, as an entire table’s time range must fall outside the retention window before it can be dropped. Additionally, TTLs are difficult to adjust later, whereas TimeSeries can extend the dataset retention instantly with a single control plane operation.\n\nTime Buckets: Within a time slice, data is further partitioned into time buckets. This facilitates effective range scans by allowing us to target specific time buckets for a given query range. The tradeoff is that if a user wants to read the entire range of data over a large time period, we must scan many partitions. We mitigate potential latency by scanning these partitions in parallel and aggregating the data at the end. In most cases, the advantage of targeting smaller data subsets outweighs the read amplification from these scatter-gather operations. Typically, users read a smaller subset of data rather than the entire retention range.\n\nEvent Buckets: To manage extremely high-throughput write operations, which may result in a burst of writes for a given time series within a short period, we further divide the time bucket into event buckets. This prevents overloading the same partition for a given time range and also reduces partition sizes further, albeit with a slight increase in read amplification.\n\nNote: With Cassandra 4.x onwards, we notice a substantial improvement in the performance of scanning a range of data in a wide partition. See Future Enhancements at the end to see the Dynamic Event bucketing work that aims to take advantage of this.\n\nStorage Tables\n\nWe use two kinds of tables\n\nData tables : These are the time slices that store the actual event data.\n\n: These are the time slices that store the actual event data. Metadata table: This table stores information about how each time slice is configured per namespace.\n\nData tables\n\nThe partition key enables splitting events for a time_series_id over a range of time_bucket(s) and event_bucket(s), thus mitigating hot partitions, while the clustering key allows us to keep data sorted on disk in the order we almost always want to read it. The value_metadata column stores metadata for the event_item_value such as compression.\n\nWriting to the data table:\n\nUser writes will land in a given time slice, time bucket, and event bucket as a factor of the event_time attached to the event. This factor is dictated by the control plane configuration of a given namespace.\n\nFor example:\n\nDuring this process, the writer makes decisions on how to handle the data before writing, such as whether to compress it. The value_metadata column records any such post-processing actions, ensuring that the reader can accurately interpret the data.\n\nReading from the data table:\n\nThe below illustration depicts at a high-level on how we scatter-gather the reads from multiple partitions and join the result set at the end to return the final result.\n\nMetadata table\n\nThis table stores the configuration data about the time slices for a given namespace.\n\nNote the following:\n\nNo Time Gaps : The end_time of a given time slice overlaps with the start_time of the next time slice, ensuring all events find a home.\n\n: The end_time of a given time slice overlaps with the start_time of the next time slice, ensuring all events find a home. Retention : The status indicates which tables fall inside and outside of the retention window.\n\n: The status indicates which tables fall inside and outside of the retention window. Flexible: This metadata can be adjusted per time slice, allowing us to tune the partition settings of future time slices based on observed data patterns in the current time slice.\n\nThere is a lot more information that can be stored into the metadata column (e.g., compaction settings for the table), but we only show the partition settings here for brevity.\n\nIndex Datastore\n\nTo support secondary access patterns via non-primary key attributes, we index data into Elasticsearch. Users can configure a list of attributes per namespace that they wish to search and/or aggregate data on. The service extracts these fields from events as they stream in, indexing the resultant documents into Elasticsearch. Depending on the throughput, we may use Elasticsearch as a reverse index, retrieving the full data from Cassandra, or we may store the entire source data directly in Elasticsearch.\n\nNote: Again, users are never directly exposed to Elasticsearch, just like they are not directly exposed to Cassandra. Instead, they interact with the Search and Aggregate API endpoints that translate a given query to that needed for the underlying datastore.\n\nIn the next section, we will talk about how we configure these data stores for different datasets.\n\nControl Plane\n\nThe data plane is responsible for executing the read and write operations, while the control plane configures every aspect of a namespace’s behavior. The data plane communicates with the TimeSeries control stack, which manages this configuration information. In turn, the TimeSeries control stack interacts with a sharded Data Gateway Platform Control Plane that oversees control configurations for all abstractions and namespaces.\n\nSeparating the responsibilities of the data plane and control plane helps maintain the high availability of our data plane, as the control plane takes on tasks that may require some form of schema consensus from the underlying data stores.\n\nNamespace Configuration\n\nThe below configuration snippet demonstrates the immense flexibility of the service and how we can tune several things per namespace using our control plane.\n\n\"persistence_configuration\": [\n\n{\n\n\"id\": \"PRIMARY_STORAGE\",\n\n\"physical_storage\": {\n\n\"type\": \"CASSANDRA\", // type of primary storage\n\n\"cluster\": \"cass_dgw_ts_tracing\", // physical cluster name\n\n\"dataset\": \"tracing_default\" // maps to the keyspace\n\n},\n\n\"config\": {\n\n\"timePartition\": {\n\n\"secondsPerTimeSlice\": \"129600\", // width of a time slice\n\n\"secondPerTimeBucket\": \"3600\", // width of a time bucket\n\n\"eventBuckets\": 4 // how many event buckets within\n\n},\n\n\"queueBuffering\": {\n\n\"coalesce\": \"1s\", // how long to coalesce writes\n\n\"bufferCapacity\": 4194304 // queue capacity in bytes\n\n},\n\n\"consistencyScope\": \"LOCAL\", // single-region/multi-region\n\n\"consistencyTarget\": \"EVENTUAL\", // read/write consistency\n\n\"acceptLimit\": \"129600s\" // how far back writes are allowed\n\n},\n\n\"lifecycleConfigs\": {\n\n\"lifecycleConfig\": [ // Primary store data retention\n\n{\n\n\"type\": \"retention\",\n\n\"config\": {\n\n\"close_after\": \"1296000s\", // close for reads/writes\n\n\"delete_after\": \"1382400s\" // drop time slice\n\n}\n\n}\n\n]\n\n}\n\n},\n\n{\n\n\"id\": \"INDEX_STORAGE\",\n\n\"physicalStorage\": {\n\n\"type\": \"ELASTICSEARCH\", // type of index storage\n\n\"cluster\": \"es_dgw_ts_tracing\", // ES cluster name\n\n\"dataset\": \"tracing_default_useast1\" // base index name\n\n},\n\n\"config\": {\n\n\"timePartition\": {\n\n\"secondsPerSlice\": \"129600\" // width of the index slice\n\n},\n\n\"consistencyScope\": \"LOCAL\",\n\n\"consistencyTarget\": \"EVENTUAL\", // how should we read/write data\n\n\"acceptLimit\": \"129600s\", // how far back writes are allowed\n\n\"indexConfig\": {\n\n\"fieldMapping\": { // fields to extract to index\n\n\"tags.nf.app\": \"KEYWORD\",\n\n\"tags.duration\": \"INTEGER\",\n\n\"tags.enabled\": \"BOOLEAN\"\n\n},\n\n\"refreshInterval\": \"60s\" // Index related settings\n\n}\n\n},\n\n\"lifecycleConfigs\": {\n\n\"lifecycleConfig\": [\n\n{\n\n\"type\": \"retention\", // Index retention settings\n\n\"config\": {\n\n\"close_after\": \"1296000s\",\n\n\"delete_after\": \"1382400s\"\n\n}\n\n}\n\n]\n\n}\n\n}\n\n]\n\nProvisioning Infrastructure\n\nWith so many different parameters, we need automated provisioning workflows to deduce the best settings for a given workload. When users want to create their namespaces, they specify a list of workload desires, which the automation translates into concrete infrastructure and related control plane configuration. We highly encourage you to watch this ApacheCon talk, by one of our stunning colleagues Joey Lynch, on how we achieve this. We may go into detail on this subject in one of our future blog posts.\n\nOnce the system provisions the initial infrastructure, it then scales in response to the user workload. The next section describes how this is achieved.\n\nScalability\n\nOur users may operate with limited information at the time of provisioning their namespaces, resulting in best-effort provisioning estimates. Further, evolving use-cases may introduce new throughput requirements over time. Here’s how we manage this:\n\nHorizontal scaling : TimeSeries server instances can auto-scale up and down as per attached scaling policies to meet the traffic demand. The storage server capacity can be recomputed to accommodate changing requirements using our capacity planner.\n\n: TimeSeries server instances can auto-scale up and down as per attached scaling policies to meet the traffic demand. The storage server capacity can be recomputed to accommodate changing requirements using our capacity planner. Vertical scaling : We may also choose to vertically scale our TimeSeries server instances or our storage instances to get greater CPU, RAM and/or attached storage capacity.\n\n: We may also choose to vertically scale our TimeSeries server instances or our storage instances to get greater CPU, RAM and/or attached storage capacity. Scaling disk : We may attach EBS to store data if the capacity planner prefers infrastructure that offers larger storage at a lower cost rather than SSDs optimized for latency. In such cases, we deploy jobs to scale the EBS volume when the disk storage reaches a certain percentage threshold.\n\n: We may attach EBS to store data if the capacity planner prefers infrastructure that offers larger storage at a lower cost rather than SSDs optimized for latency. In such cases, we deploy jobs to scale the EBS volume when the disk storage reaches a certain percentage threshold. Re-partitioning data: Inaccurate workload estimates can lead to over or under-partitioning of our datasets. TimeSeries control-plane can adjust the partitioning configuration for upcoming time slices, once we realize the nature of data in the wild (via partition histograms). In the future we plan to support re-partitioning of older data and dynamic partitioning of current data.\n\nDesign Principles\n\nSo far, we have seen how TimeSeries stores, configures and interacts with event datasets. Let’s see how we apply different techniques to improve the performance of our operations and provide better guarantees.\n\nEvent Idempotency\n\nWe prefer to bake in idempotency in all mutation endpoints, so that users can retry or hedge their requests safely. Hedging is when the client sends an identical competing request to the server, if the original request does not come back with a response in an expected amount of time. The client then responds with whichever request completes first. This is done to keep the tail latencies for an application relatively low. This can only be done safely if the mutations are idempotent. For TimeSeries, the combination of event_time, event_id and event_item_key form the idempotency key for a given time_series_id event.\n\nSLO-based Hedging\n\nWe assign Service Level Objectives (SLO) targets for different endpoints within TimeSeries, as an indication of what we think the performance of those endpoints should be for a given namespace. We can then hedge a request if the response does not come back in that configured amount of time.\n\n\"slos\": {\n\n\"read\": { // SLOs per endpoint\n\n\"latency\": {\n\n\"target\": \"0.5s\", // hedge around this number\n\n\"max\": \"1s\" // time-out around this number\n\n}\n\n},\n\n\"write\": {\n\n\"latency\": {\n\n\"target\": \"0.01s\",\n\n\"max\": \"0.05s\"\n\n}\n\n}\n\n}\n\nPartial Return\n\nSometimes, a client may be sensitive to latency and willing to accept a partial result set. A real-world example of this is real-time frequency capping. Precision is not critical in this case, but if the response is delayed, it becomes practically useless to the upstream client. Therefore, the client prefers to work with whatever data has been collected so far rather than timing out while waiting for all the data. The TimeSeries client supports partial returns around SLOs for this purpose. Importantly, we still maintain the latest order of events in this partial fetch.\n\nAdaptive Pagination\n\nAll reads start with a default fanout factor, scanning 8 partition buckets in parallel. However, if the service layer determines that the time_series dataset is dense — i.e., most reads are satisfied by reading the first few partition buckets — then it dynamically adjusts the fanout factor of future reads in order to reduce the read amplification on the underlying datastore. Conversely, if the dataset is sparse, we may want to increase this limit with a reasonable upper bound.\n\nLimited Write Window\n\nIn most cases, the active range for writing data is smaller than the range for reading data — i.e., we want a range of time to become immutable as soon as possible so that we can apply optimizations on top of it. We control this by having a configurable “acceptLimit” parameter that prevents users from writing events older than this time limit. For example, an accept limit of 4 hours means that users cannot write events older than now() — 4 hours. We sometimes raise this limit for backfilling historical data, but it is tuned back down for regular write operations. Once a range of data becomes immutable, we can safely do things like caching, compressing, and compacting it for reads.\n\nBuffering Writes\n\nWe frequently leverage this service for handling bursty workloads. Rather than overwhelming the underlying datastore with this load all at once, we aim to distribute it more evenly by allowing events to coalesce over short durations (typically seconds). These events accumulate in in-memory queues running on each instance. Dedicated consumers then steadily drain these queues, grouping the events by their partition key, and batching the writes to the underlying datastore.\n\nThe queues are tailored to each datastore since their operational characteristics depend on the specific datastore being written to. For instance, the batch size for writing to Cassandra is significantly smaller than that for indexing into Elasticsearch, leading to different drain rates and batch sizes for the associated consumers.\n\nWhile using in-memory queues does increase JVM garbage collection, we have experienced substantial improvements by transitioning to JDK 21 with ZGC. To illustrate the impact, ZGC has reduced our tail latencies by an impressive 86%:\n\nBecause we use in-memory queues, we are prone to losing events in case of an instance crash. As such, these queues are only used for use cases that can tolerate some amount of data loss .e.g. tracing/logging. For use cases that need guaranteed durability and/or read-after-write consistency, these queues are effectively disabled and writes are flushed to the data store almost immediately.\n\nDynamic Compaction\n\nOnce a time slice exits the active write window, we can leverage the immutability of the data to optimize it for read performance. This process may involve re-compacting immutable data using optimal compaction strategies, dynamically shrinking and/or splitting shards to optimize system resources, and other similar techniques to ensure fast and reliable performance.\n\nThe following section provides a glimpse into the real-world performance of some of our TimeSeries datasets.\n\nReal-world Performance\n\nThe service can write data in the order of low single digit milliseconds\n\nwhile consistently maintaining stable point-read latencies:\n\nAt the time of writing this blog, the service was processing close to 15 million events/second across all the different datasets at peak globally.\n\nTime Series Usage @ Netflix\n\nThe TimeSeries Abstraction plays a vital role across key services at Netflix. Here are some impactful use cases:\n\nTracing and Insights: Logs traces across all apps and micro-services within Netflix, to understand service-to-service communication, aid in debugging of issues, and answer support requests.\n\nLogs traces across all apps and micro-services within Netflix, to understand service-to-service communication, aid in debugging of issues, and answer support requests. User Interaction Tracking : Tracks millions of user interactions — such as video playbacks, searches, and content engagement — providing insights that enhance Netflix’s recommendation algorithms in real-time and improve the overall user experience.\n\n: Tracks millions of user interactions — such as video playbacks, searches, and content engagement — providing insights that enhance Netflix’s recommendation algorithms in real-time and improve the overall user experience. Feature Rollout and Performance Analysis : Tracks the rollout and performance of new product features, enabling Netflix engineers to measure how users engage with features, which powers data-driven decisions about future improvements.\n\n: Tracks the rollout and performance of new product features, enabling Netflix engineers to measure how users engage with features, which powers data-driven decisions about future improvements. Asset Impression Tracking and Optimization : Tracks asset impressions ensuring content and assets are delivered efficiently while providing real-time feedback for optimizations.\n\n: Tracks asset impressions ensuring content and assets are delivered efficiently while providing real-time feedback for optimizations. Billing and Subscription Management: Stores historical data related to billing and subscription management, ensuring accuracy in transaction records and supporting customer service inquiries.\n\nand more…\n\nFuture Enhancements\n\nAs the use cases evolve, and the need to make the abstraction even more cost effective grows, we aim to make many improvements to the service in the upcoming months. Some of them are:\n\nTiered Storage for Cost Efficiency: Support moving older, lesser-accessed data into cheaper object storage that has higher time to first byte, potentially saving Netflix millions of dollars.\n\nSupport moving older, lesser-accessed data into cheaper object storage that has higher time to first byte, potentially saving Netflix millions of dollars. Dynamic Event Bucketing: Support real-time partitioning of keys into optimally-sized partitions as events stream in, rather than having a somewhat static configuration at the time of provisioning a namespace. This strategy has a huge advantage of not partitioning time_series_ids that don’t need it, thus saving the overall cost of read amplification. Also, with Cassandra 4.x, we have noted major improvements in reading a subset of data in a wide partition that could lead us to be less aggressive with partitioning the entire dataset ahead of time.\n\nSupport real-time partitioning of keys into optimally-sized partitions as events stream in, rather than having a somewhat static configuration at the time of provisioning a namespace. This strategy has a huge advantage of not partitioning time_series_ids that don’t need it, thus saving the overall cost of read amplification. Also, with Cassandra 4.x, we have noted major improvements in reading a subset of data in a wide partition that could lead us to be less aggressive with partitioning the entire dataset ahead of time. Caching: Take advantage of immutability of data and cache it intelligently for discrete time ranges.\n\nTake advantage of immutability of data and cache it intelligently for discrete time ranges. Count and other Aggregations: Some users are only interested in counting events in a given time interval rather than fetching all the event data for it.\n\nConclusion\n\nThe TimeSeries Abstraction is a vital component of Netflix’s online data infrastructure, playing a crucial role in supporting both real-time and long-term decision-making. Whether it’s monitoring system performance during high-traffic events or optimizing user engagement through behavior analytics, TimeSeries Abstraction ensures that Netflix operates seamlessly and efficiently on a global scale.\n\nAs Netflix continues to innovate and expand into new verticals, the TimeSeries Abstraction will remain a cornerstone of our platform, helping us push the boundaries of what’s possible in streaming and beyond.\n\nStay tuned for Part 2, where we’ll introduce our Distributed Counter Abstraction, a key element of Netflix’s Composite Abstractions, built on top of the TimeSeries Abstraction.\n\nAcknowledgments\n\nSpecial thanks to our stunning colleagues who contributed to TimeSeries Abstraction’s success: Tom DeVoe Mengqing Wang, Kartik Sathyanarayanan, Jordan West, Matt Lehman, Cheng Wang, Chris Lohfink .", "label": "non_personal"}
{"title": "Investigation of a Workbench UI Latency Issue", "url": "https://netflixtechblog.com/investigation-of-a-workbench-ui-latency-issue-faa017b4653d?source=collection_home---4------12-----------------------", "content": "Investigation of a Workbench UI Latency Issue Netflix Technology Blog 12 min read · Oct 14, 2024 -- 4 Listen Share\n\nBy: Hechao Li and Marcelo Mayworm\n\nWith special thanks to our stunning colleagues Amer Ather, Itay Dafna, Luca Pozzi, Matheus Leão, and Ye Ji.\n\nOverview\n\nAt Netflix, the Analytics and Developer Experience organization, part of the Data Platform, offers a product called Workbench. Workbench is a remote development workspace based on Titus that allows data practitioners to work with big data and machine learning use cases at scale. A common use case for Workbench is running JupyterLab Notebooks.\n\nRecently, several users reported that their JupyterLab UI becomes slow and unresponsive when running certain notebooks. This document details the intriguing process of debugging this issue, all the way from the UI down to the Linux kernel.\n\nSymptom\n\nMachine Learning engineer Luca Pozzi reported to our Data Platform team that their JupyterLab UI on their workbench becomes slow and unresponsive when running some of their Notebooks. Restarting the ipykernel process, which runs the Notebook, might temporarily alleviate the problem, but the frustration persists as more notebooks are run.\n\nQuantify the Slowness\n\nWhile we observed the issue firsthand, the term “UI being slow” is subjective and difficult to measure. To investigate this issue, we needed a quantitative analysis of the slowness.\n\nItay Dafna devised an effective and simple method to quantify the UI slowness. Specifically, we opened a terminal via JupyterLab and held down a key (e.g., “j”) for 15 seconds while running the user’s notebook. The input to stdin is sent to the backend (i.e., JupyterLab) via a WebSocket, and the output to stdout is sent back from the backend and displayed on the UI. We then exported the .har file recording all communications from the browser and loaded it into a Notebook for analysis.\n\nUsing this approach, we observed latencies ranging from 1 to 10 seconds, averaging 7.4 seconds.\n\nBlame The Notebook\n\nNow that we have an objective metric for the slowness, let’s officially start our investigation. If you have read the symptom carefully, you must have noticed that the slowness only occurs when the user runs certain notebooks but not others.\n\nTherefore, the first step is scrutinizing the specific Notebook experiencing the issue. Why does the UI always slow down after running this particular Notebook? Naturally, you would think that there must be something wrong with the code running in it.\n\nUpon closely examining the user’s Notebook, we noticed a library called pystan , which provides Python bindings to a native C++ library called stan, looked suspicious. Specifically, pystan uses asyncio. However, because there is already an existing asyncio event loop running in the Notebook process and asyncio cannot be nested by design, in order for pystan to work, the authors of pystan recommend injecting pystan into the existing event loop by using a package called nest_asyncio, a library that became unmaintained because the author unfortunately passed away.\n\nGiven this seemingly hacky usage, we naturally suspected that the events injected by pystan into the event loop were blocking the handling of the WebSocket messages used to communicate with the JupyterLab UI. This reasoning sounds very plausible. However, the user claimed that there were cases when a Notebook not using pystan runs, the UI also became slow.\n\nMoreover, after several rounds of discussion with ChatGPT, we learned more about the architecture and realized that, in theory, the usage of pystan and nest_asyncio should not cause the slowness in handling the UI WebSocket for the following reasons:\n\nEven though pystan uses nest_asyncio to inject itself into the main event loop, the Notebook runs on a child process (i.e., the ipykernel process) of the jupyter-lab server process, which means the main event loop being injected by pystan is that of the ipykernel process, not the jupyter-server process. Therefore, even if pystan blocks the event loop, it shouldn’t impact the jupyter-lab main event loop that is used for UI websocket communication. See the diagram below:\n\nIn other words, pystan events are injected to the event loop B in this diagram instead of event loop A. So, it shouldn’t block the UI WebSocket events.\n\nYou might also think that because event loop A handles both the WebSocket events from the UI and the ZeroMQ socket events from the ipykernel process, a high volume of ZeroMQ events generated by the notebook could block the WebSocket. However, when we captured packets on the ZeroMQ socket while reproducing the issue, we didn’t observe heavy traffic on this socket that could cause such blocking.\n\nA stronger piece of evidence to rule out pystan was that we were ultimately able to reproduce the issue even without it, which I’ll dive into later.\n\nBlame Noisy Neighbors\n\nThe Workbench instance runs as a Titus container. To efficiently utilize our compute resources, Titus employs a CPU oversubscription feature, meaning the combined virtual CPUs allocated to containers exceed the number of available physical CPUs on a Titus agent. If a container is unfortunate enough to be scheduled alongside other “noisy” containers — those that consume a lot of CPU resources — it could suffer from CPU deficiency.\n\nHowever, after examining the CPU utilization of neighboring containers on the same Titus agent as the Workbench instance, as well as the overall CPU utilization of the Titus agent, we quickly ruled out this hypothesis. Using the top command on the Workbench, we observed that when running the Notebook, the Workbench instance uses only 4 out of the 64 CPUs allocated to it. Simply put, this workload is not CPU-bound.\n\nBlame The Network\n\nThe next theory was that the network between the web browser UI (on the laptop) and the JupyterLab server was slow. To investigate, we captured all the packets between the laptop and the server while running the Notebook and continuously pressing ‘j’ in the terminal.\n\nWhen the UI experienced delays, we observed a 5-second pause in packet transmission from server port 8888 to the laptop. Meanwhile, traffic from other ports, such as port 22 for SSH, remained unaffected. This led us to conclude that the pause was caused by the application running on port 8888 (i.e., the JupyterLab process) rather than the network.\n\nThe Minimal Reproduction\n\nAs previously mentioned, another strong piece of evidence proving the innocence of pystan was that we could reproduce the issue without it. By gradually stripping down the “bad” Notebook, we eventually arrived at a minimal snippet of code that reproduces the issue without any third-party dependencies or complex logic:\n\nimport time\n\nimport os\n\nfrom multiprocessing import Process\n\n\n\nN = os.cpu_count()\n\n\n\ndef launch_worker(worker_id):\n\ntime.sleep(60)\n\n\n\nif __name__ == '__main__':\n\nwith open('/root/2GB_file', 'r') as file:\n\ndata = file.read()\n\nprocesses = []\n\nfor i in range(N):\n\np = Process(target=launch_worker, args=(i,))\n\nprocesses.append(p)\n\np.start()\n\n\n\nfor p in processes:\n\np.join()\n\nThe code does only two things:\n\nRead a 2GB file into memory (the Workbench instance has 480G memory in total so this memory usage is almost negligible). Start N processes where N is the number of CPUs. The N processes do nothing but sleep.\n\nThere is no doubt that this is the most silly piece of code I’ve ever written. It is neither CPU bound nor memory bound. Yet it can cause the JupyterLab UI to stall for as many as 10 seconds!\n\nQuestions\n\nThere are a couple of interesting observations that raise several questions:\n\nWe noticed that both steps are required in order to reproduce the issue . If you don’t read the 2GB file (that is not even used!), the issue is not reproducible. Why using 2GB out of 480GB memory could impact the performance?\n\n. If you don’t read the 2GB file (that is not even used!), the issue is not reproducible. When the UI delay occurs, the jupyter-lab process CPU utilization spikes to 100% , hinting at contention on the single-threaded event loop in this process (event loop A in the diagram before). What does the jupyter-lab process need the CPU for, given that it is not the process that runs the Notebook?\n\n, hinting at contention on the single-threaded event loop in this process (event loop A in the diagram before). The code runs in a Notebook, which means it runs in the ipykernel process, that is a child process of the jupyter-lab process. How can anything that happens in a child process cause the parent process to have CPU contention?\n\nThe workbench has 64CPUs. But when we printed os.cpu_count(), the output was 96. That means the code starts more processes than the number of CPUs. Why is that?\n\nLet’s answer the last question first. In fact, if you run lscpu and nproc commands inside a Titus container, you will also see different results — the former gives you 96, which is the number of physical CPUs on the Titus agent, whereas the latter gives you 64, which is the number of virtual CPUs allocated to the container. This discrepancy is due to the lack of a “CPU namespace” in the Linux kernel, causing the number of physical CPUs to be leaked to the container when calling certain functions to get the CPU count. The assumption here is that Python os.cpu_count() uses the same function as the lscpu command, causing it to get the CPU count of the host instead of the container. Python 3.13 has a new call that can be used to get the accurate CPU count, but it’s not GA’ed yet.\n\nIt will be proven later that this inaccurate number of CPUs can be a contributing factor to the slowness.\n\nMore Clues\n\nNext, we used py-spy to do a profiling of the jupyter-lab process. Note that we profiled the parent jupyter-lab process, not the ipykernel child process that runs the reproduction code. The profiling result is as follows:\n\nAs one can see, a lot of CPU time (89%!!) is spent on a function called __parse_smaps_rollup. In comparison, the terminal handler used only 0.47% CPU time. From the stack trace, we see that this function is inside the event loop A, so it can definitely cause the UI WebSocket events to be delayed.\n\nThe stack trace also shows that this function is ultimately called by a function used by a Jupyter lab extension called jupyter_resource_usage. We then disabled this extension and restarted the jupyter-lab process. As you may have guessed, we could no longer reproduce the slowness!\n\nBut our puzzle is not solved yet. Why does this extension cause the UI to slow down? Let’s keep digging.\n\nRoot Cause Analysis\n\nFrom the name of the extension and the names of the other functions it calls, we can infer that this extension is used to get resources such as CPU and memory usage information. Examining the code, we see that this function call stack is triggered when an API endpoint /metrics/v1 is called from the UI. The UI apparently calls this function periodically, according to the network traffic tab in Chrome’s Developer Tools.\n\nNow let’s look at the implementation starting from the call get(jupter_resource_usage/api.py:42) . The full code is here and the key lines are shown below:\n\ncur_process = psutil.Process()\n\nall_processes = [cur_process] + cur_process.children(recursive=True)\n\n\n\nfor p in all_processes:\n\ninfo = p.memory_full_info()\n\nBasically, it gets all children processes of the jupyter-lab process recursively, including both the ipykernel Notebook process and all processes created by the Notebook. Obviously, the cost of this function is linear to the number of all children processes. In the reproduction code, we create 96 processes. So here we will have at least 96 (sleep processes) + 1 (ipykernel process) + 1 (jupyter-lab process) = 98 processes when it should actually be 64 (allocated CPUs) + 1 (ipykernel process) + 1 (jupyter-lab process) = 66 processes, because the number of CPUs allocated to the container is, in fact, 64.\n\nThis is truly ironic. The more CPUs we have, the slower we are!\n\nAt this point, we have answered one question: Why does starting many grandchildren processes in the child process cause the parent process to be slow? Because the parent process runs a function that’s linear to the number all children process recursively.\n\nHowever, this solves only half of the puzzle. If you remember the previous analysis, starting many child processes ALONE doesn’t reproduce the issue. If we don’t read the 2GB file, even if we create 2x more processes, we can’t reproduce the slowness.\n\nSo now we must answer the next question: Why does reading a 2GB file in the child process affect the parent process performance, especially when the workbench has as much as 480GB memory in total?\n\nTo answer this question, let’s look closely at the function __parse_smaps_rollup. As the name implies, this function parses the file /proc/<pid>/smaps_rollup.\n\ndef _parse_smaps_rollup(self):\n\nuss = pss = swap = 0\n\nwith open_binary(\"{}/{}/smaps_rollup\".format(self._procfs_path, self.pid)) as f:\n\nfor line in f:\n\nif line.startswith(b”Private_”):\n\n# Private_Clean, Private_Dirty, Private_Hugetlb\n\ns uss += int(line.split()[1]) * 1024\n\nelif line.startswith(b”Pss:”):\n\npss = int(line.split()[1]) * 1024\n\nelif line.startswith(b”Swap:”):\n\nswap = int(line.split()[1]) * 1024\n\nreturn (uss, pss, swap)\n\nNaturally, you might think that when memory usage increases, this file becomes larger in size, causing the function to take longer to parse. Unfortunately, this is not the answer because:\n\nFirst, the number of lines in this file is constant for all processes .\n\n. Second, this is a special file in the /proc filesystem, which should be seen as a kernel interface instead of a regular file on disk. In other words, I/O operations of this file are handled by the kernel rather than disk.\n\nThis file was introduced in this commit in 2017, with the purpose of improving the performance of user programs that determine aggregate memory statistics. Let’s first focus on the handler of open syscall on this /proc/<pid>/smaps_rollup.\n\nFollowing through the single_open function, we will find that it uses the function show_smaps_rollup for the show operation, which can translate to the read system call on the file. Next, we look at the show_smaps_rollup implementation. You will notice a do-while loop that is linear to the virtual memory area.\n\nstatic int show_smaps_rollup(struct seq_file *m, void *v) {\n\n…\n\nvma_start = vma->vm_start;\n\ndo {\n\nsmap_gather_stats(vma, &mss, 0);\n\nlast_vma_end = vma->vm_end;\n\n…\n\n} for_each_vma(vmi, vma);\n\n…\n\n}\n\nThis perfectly explains why the function gets slower when a 2GB file is read into memory. Because the handler of reading the smaps_rollup file now takes longer to run the while loop. Basically, even though smaps_rollup already improved the performance of getting memory information compared to the old method of parsing the /proc/<pid>/smaps file, it is still linear to the virtual memory used.\n\nMore Quantitative Analysis\n\nEven though at this point the puzzle is solved, let’s conduct a more quantitative analysis. How much is the time difference when reading the smaps_rollup file with small versus large virtual memory utilization? Let’s write some simple benchmark code like below:\n\nimport os\n\n\n\ndef read_smaps_rollup(pid):\n\nwith open(\"/proc/{}/smaps_rollup\".format(pid), \"rb\") as f:\n\nfor line in f:\n\npass\n\n\n\nif __name__ == “__main__”:\n\npid = os.getpid()\n\n\n\nread_smaps_rollup(pid)\n\n\n\nwith open(“/root/2G_file”, “rb”) as f:\n\ndata = f.read()\n\n\n\nread_smaps_rollup(pid)\n\nThis program performs the following steps:\n\nReads the smaps_rollup file of the current process. Reads a 2GB file into memory. Repeats step 1.\n\nWe then use strace to find the accurate time of reading the smaps_rollup file.\n\n$ sudo strace -T -e trace=openat,read python3 benchmark.py 2>&1 | grep “smaps_rollup” -A 1\n\n\n\nopenat(AT_FDCWD, “/proc/3107492/smaps_rollup”, O_RDONLY|O_CLOEXEC) = 3 <0.000023>\n\nread(3, “560b42ed4000–7ffdadcef000 — -p 0”…, 1024) = 670 <0.000259>\n\n...\n\nopenat(AT_FDCWD, “/proc/3107492/smaps_rollup”, O_RDONLY|O_CLOEXEC) = 3 <0.000029>\n\nread(3, “560b42ed4000–7ffdadcef000 — -p 0”…, 1024) = 670 <0.027698>\n\nAs you can see, both times, the read syscall returned 670, meaning the file size remained the same at 670 bytes. However, the time it took the second time (i.e., 0.027698 seconds) is 100x the time it took the first time (i.e., 0.000259 seconds)! This means that if there are 98 processes, the time spent on reading this file alone will be 98 * 0.027698 = 2.7 seconds! Such a delay can significantly affect the UI experience.\n\nSolution\n\nThis extension is used to display the CPU and memory usage of the notebook process on the bar at the bottom of the Notebook:\n\nWe confirmed with the user that disabling the jupyter-resource-usage extension meets their requirements for UI responsiveness, and that this extension is not critical to their use case. Therefore, we provided a way for them to disable the extension.\n\nSummary\n\nThis was such a challenging issue that required debugging from the UI all the way down to the Linux kernel. It is fascinating that the problem is linear to both the number of CPUs and the virtual memory size — two dimensions that are generally viewed separately.\n\nOverall, we hope you enjoyed the irony of:\n\nThe extension used to monitor CPU usage causing CPU contention. An interesting case where the more CPUs you have, the slower you get!\n\nIf you’re excited by tackling such technical challenges and have the opportunity to solve complex technical challenges and drive innovation, consider joining our Data Platform teams. Be part of shaping the future of Data Security and Infrastructure, Data Developer Experience, Analytics Infrastructure and Enablement, and more. Explore the impact you can make with us!", "label": "non_personal"}
{"title": "Netflix’s Distributed Counter Abstraction", "url": "https://netflixtechblog.com/netflixs-distributed-counter-abstraction-8d0c45eb66b2?source=collection_home---4------11-----------------------", "content": "Netflix’s Distributed Counter Abstraction Netflix Technology Blog 19 min read · Nov 12, 2024 -- 22 Listen Share\n\nBy: Rajiv Shringi, Oleksii Tkachuk, Kartik Sathyanarayanan\n\nIntroduction\n\nIn our previous blog post, we introduced Netflix’s TimeSeries Abstraction, a distributed service designed to store and query large volumes of temporal event data with low millisecond latencies. Today, we’re excited to present the Distributed Counter Abstraction. This counting service, built on top of the TimeSeries Abstraction, enables distributed counting at scale while maintaining similar low latency performance. As with all our abstractions, we use our Data Gateway Control Plane to shard, configure, and deploy this service globally.\n\nDistributed counting is a challenging problem in computer science. In this blog post, we’ll explore the diverse counting requirements at Netflix, the challenges of achieving accurate counts in near real-time, and the rationale behind our chosen approach, including the necessary trade-offs.\n\nNote: When it comes to distributed counters, terms such as ‘accurate’ or ‘precise’ should be taken with a grain of salt. In this context, they refer to a count very close to accurate, presented with minimal delays.\n\nUse Cases and Requirements\n\nAt Netflix, our counting use cases include tracking millions of user interactions, monitoring how often specific features or experiences are shown to users, and counting multiple facets of data during A/B test experiments, among others.\n\nAt Netflix, these use cases can be classified into two broad categories:\n\nBest-Effort: For this category, the count doesn’t have to be very accurate or durable. However, this category requires near-immediate access to the current count at low latencies, all while keeping infrastructure costs to a minimum. Eventually Consistent: This category needs accurate and durable counts, and is willing to tolerate a slight delay in accuracy and a slightly higher infrastructure cost as a trade-off.\n\nBoth categories share common requirements, such as high throughput and high availability. The table below provides a detailed overview of the diverse requirements across these two categories.\n\nDistributed Counter Abstraction\n\nTo meet the outlined requirements, the Counter Abstraction was designed to be highly configurable. It allows users to choose between different counting modes, such as Best-Effort or Eventually Consistent, while considering the documented trade-offs of each option. After selecting a mode, users can interact with APIs without needing to worry about the underlying storage mechanisms and counting methods.\n\nLet’s take a closer look at the structure and functionality of the API.\n\nAPI\n\nCounters are organized into separate namespaces that users set up for each of their specific use cases. Each namespace can be configured with different parameters, such as Type of Counter, Time-To-Live (TTL), and Counter Cardinality, using the service’s Control Plane.\n\nThe Counter Abstraction API resembles Java’s AtomicInteger interface:\n\nAddCount/AddAndGetCount: Adjusts the count for the specified counter by the given delta value within a dataset. The delta value can be positive or negative. The AddAndGetCount counterpart also returns the count after performing the add operation.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"counter_name\": \"counter123\",\n\n\"delta\": 2,\n\n\"idempotency_token\": {\n\n\"token\": \"some_event_id\",\n\n\"generation_time\": \"2024-10-05T14:48:00Z\"\n\n}\n\n}\n\nThe idempotency token can be used for counter types that support them. Clients can use this token to safely retry or hedge their requests. Failures in a distributed system are a given, and having the ability to safely retry requests enhances the reliability of the service.\n\nGetCount: Retrieves the count value of the specified counter within a dataset.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"counter_name\": \"counter123\"\n\n}\n\nClearCount: Effectively resets the count to 0 for the specified counter within a dataset.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"counter_name\": \"counter456\",\n\n\"idempotency_token\": {...}\n\n}\n\nNow, let’s look at the different types of counters supported within the Abstraction.\n\nTypes of Counters\n\nThe service primarily supports two types of counters: Best-Effort and Eventually Consistent, along with a third experimental type: Accurate. In the following sections, we’ll describe the different approaches for these types of counters and the trade-offs associated with each.\n\nBest Effort Regional Counter\n\nThis type of counter is powered by EVCache, Netflix’s distributed caching solution built on the widely popular Memcached. It is suitable for use cases like A/B experiments, where many concurrent experiments are run for relatively short durations and an approximate count is sufficient. Setting aside the complexities of provisioning, resource allocation, and control plane management, the core of this solution is remarkably straightforward:\n\n// counter cache key\n\ncounterCacheKey = <namespace>:<counter_name>\n\n\n\n// add operation\n\nreturn delta > 0\n\n? cache.incr(counterCacheKey, delta, TTL)\n\n: cache.decr(counterCacheKey, Math.abs(delta), TTL);\n\n\n\n// get operation\n\ncache.get(counterCacheKey);\n\n\n\n// clear counts from all replicas\n\ncache.delete(counterCacheKey, ReplicaPolicy.ALL);\n\nEVCache delivers extremely high throughput at low millisecond latency or better within a single region, enabling a multi-tenant setup within a shared cluster, saving infrastructure costs. However, there are some trade-offs: it lacks cross-region replication for the increment operation and does not provide consistency guarantees, which may be necessary for an accurate count. Additionally, idempotency is not natively supported, making it unsafe to retry or hedge requests.\n\nEdit: A note on probabilistic data structures:\n\nProbabilistic data structures like HyperLogLog (HLL) can be useful for tracking an approximate number of distinct elements, like distinct views or visits to a website, but are not ideally suited for implementing distinct increments and decrements for a given key. Count-Min Sketch (CMS) is an alternative that can be used to adjust the values of keys by a given amount. Data stores like Redis support both HLL and CMS. However, we chose not to pursue this direction for several reasons:\n\nWe chose to build on top of data stores that we already operate at scale.\n\nProbabilistic data structures do not natively support several of our requirements, such as resetting the count for a given key or having TTLs for counts. Additional data structures, including more sketches, would be needed to support these requirements.\n\nOn the other hand, the EVCache solution is quite simple, requiring minimal lines of code and using natively supported elements. However, it comes at the trade-off of using a small amount of memory per counter key.\n\nEventually Consistent Global Counter\n\nWhile some users may accept the limitations of a Best-Effort counter, others opt for precise counts, durability and global availability. In the following sections, we’ll explore various strategies for achieving durable and accurate counts. Our objective is to highlight the challenges inherent in global distributed counting and explain the reasoning behind our chosen approach.\n\nApproach 1: Storing a Single Row per Counter\n\nLet’s start simple by using a single row per counter key within a table in a globally replicated datastore.\n\nLet’s examine some of the drawbacks of this approach:\n\nLack of Idempotency : There is no idempotency key baked into the storage data-model preventing users from safely retrying requests. Implementing idempotency would likely require using an external system for such keys, which can further degrade performance or cause race conditions.\n\n: There is no idempotency key baked into the storage data-model preventing users from safely retrying requests. Implementing idempotency would likely require using an external system for such keys, which can further degrade performance or cause race conditions. Heavy Contention: To update counts reliably, every writer must perform a Compare-And-Swap operation for a given counter using locks or transactions. Depending on the throughput and concurrency of operations, this can lead to significant contention, heavily impacting performance.\n\nSecondary Keys: One way to reduce contention in this approach would be to use a secondary key, such as a bucket_id, which allows for distributing writes by splitting a given counter into buckets, while enabling reads to aggregate across buckets. The challenge lies in determining the appropriate number of buckets. A static number may still lead to contention with hot keys, while dynamically assigning the number of buckets per counter across millions of counters presents a more complex problem.\n\nLet’s see if we can iterate on our solution to overcome these drawbacks.\n\nApproach 2: Per Instance Aggregation\n\nTo address issues of hot keys and contention from writing to the same row in real-time, we could implement a strategy where each instance aggregates the counts in memory and then flushes them to disk at regular intervals. Introducing sufficient jitter to the flush process can further reduce contention.\n\nHowever, this solution presents a new set of issues:\n\nVulnerability to Data Loss : The solution is vulnerable to data loss for all in-memory data during instance failures, restarts, or deployments.\n\n: The solution is vulnerable to data loss for all in-memory data during instance failures, restarts, or deployments. Inability to Reliably Reset Counts : Due to counting requests being distributed across multiple machines, it is challenging to establish consensus on the exact point in time when a counter reset occurred.\n\n: Due to counting requests being distributed across multiple machines, it is challenging to establish consensus on the exact point in time when a counter reset occurred. Lack of Idempotency: Similar to the previous approach, this method does not natively guarantee idempotency. One way to achieve idempotency is by consistently routing the same set of counters to the same instance. However, this approach may introduce additional complexities, such as leader election, and potential challenges with availability and latency in the write path.\n\nThat said, this approach may still be suitable in scenarios where these trade-offs are acceptable. However, let’s see if we can address some of these issues with a different event-based approach.\n\nApproach 3: Using Durable Queues\n\nIn this approach, we log counter events into a durable queuing system like Apache Kafka to prevent any potential data loss. By creating multiple topic partitions and hashing the counter key to a specific partition, we ensure that the same set of counters are processed by the same set of consumers. This setup simplifies facilitating idempotency checks and resetting counts. Furthermore, by leveraging additional stream processing frameworks such as Kafka Streams or Apache Flink, we can implement windowed aggregations.\n\nHowever, this approach comes with some challenges:\n\nPotential Delays : Having the same consumer process all the counts from a given partition can lead to backups and delays, resulting in stale counts.\n\n: Having the same consumer process all the counts from a given partition can lead to backups and delays, resulting in stale counts. Rebalancing Partitions: This approach requires auto-scaling and rebalancing of topic partitions as the cardinality of counters and throughput increases.\n\nFurthermore, all approaches that pre-aggregate counts make it challenging to support two of our requirements for accurate counters:\n\nAuditing of Counts : Auditing involves extracting data to an offline system for analysis to ensure that increments were applied correctly to reach the final value. This process can also be used to track the provenance of increments. However, auditing becomes infeasible when counts are aggregated without storing the individual increments.\n\n: Auditing involves extracting data to an offline system for analysis to ensure that increments were applied correctly to reach the final value. This process can also be used to track the provenance of increments. However, auditing becomes infeasible when counts are aggregated without storing the individual increments. Potential Recounting: Similar to auditing, if adjustments to increments are necessary and recounting of events within a time window is required, pre-aggregating counts makes this infeasible.\n\nBarring those few requirements, this approach can still be effective if we determine the right way to scale our queue partitions and consumers while maintaining idempotency. However, let’s explore how we can adjust this approach to meet the auditing and recounting requirements.\n\nApproach 4: Event Log of Individual Increments\n\nIn this approach, we log each individual counter increment along with its event_time and event_id. The event_id can include the source information of where the increment originated. The combination of event_time and event_id can also serve as the idempotency key for the write.\n\nHowever, in its simplest form, this approach has several drawbacks:\n\nRead Latency : Each read request requires scanning all increments for a given counter potentially degrading performance.\n\n: Each read request requires scanning all increments for a given counter potentially degrading performance. Duplicate Work : Multiple threads might duplicate the effort of aggregating the same set of counters during read operations, leading to wasted effort and subpar resource utilization.\n\n: Multiple threads might duplicate the effort of aggregating the same set of counters during read operations, leading to wasted effort and subpar resource utilization. Wide Partitions : If using a datastore like Apache Cassandra, storing many increments for the same counter could lead to a wide partition, affecting read performance.\n\n: If using a datastore like Apache Cassandra, storing many increments for the same counter could lead to a wide partition, affecting read performance. Large Data Footprint: Storing each increment individually could also result in a substantial data footprint over time. Without an efficient data retention strategy, this approach may struggle to scale effectively.\n\nThe combined impact of these issues can lead to increased infrastructure costs that may be difficult to justify. However, adopting an event-driven approach seems to be a significant step forward in addressing some of the challenges we’ve encountered and meeting our requirements.\n\nHow can we improve this solution further?\n\nNetflix’s Approach\n\nWe use a combination of the previous approaches, where we log each counting activity as an event, and continuously aggregate these events in the background using queues and a sliding time window. Additionally, we employ a bucketing strategy to prevent wide partitions. In the following sections, we’ll explore how this approach addresses the previously mentioned drawbacks and meets all our requirements.\n\nNote: From here on, we will use the words “rollup” and “aggregate” interchangeably. They essentially mean the same thing, i.e., collecting individual counter increments/decrements and arriving at the final value.\n\nTimeSeries Event Store:\n\nWe chose the TimeSeries Data Abstraction as our event store, where counter mutations are ingested as event records. Some of the benefits of storing events in TimeSeries include:\n\nHigh-Performance: The TimeSeries abstraction already addresses many of our requirements, including high availability and throughput, reliable and fast performance, and more.\n\nReducing Code Complexity: We reduce a lot of code complexity in Counter Abstraction by delegating a major portion of the functionality to an existing service.\n\nTimeSeries Abstraction uses Cassandra as the underlying event store, but it can be configured to work with any persistent store. Here is what it looks like:\n\nHandling Wide Partitions: The time_bucket and event_bucket columns play a crucial role in breaking up a wide partition, preventing high-throughput counter events from overwhelming a given partition. For more information regarding this, refer to our previous blog.\n\nNo Over-Counting: The event_time, event_id and event_item_key columns form the idempotency key for the events for a given counter, enabling clients to retry safely without the risk of over-counting.\n\nEvent Ordering: TimeSeries orders all events in descending order of time allowing us to leverage this property for events like count resets.\n\nEvent Retention: The TimeSeries Abstraction includes retention policies to ensure that events are not stored indefinitely, saving disk space and reducing infrastructure costs. Once events have been aggregated and moved to a more cost-effective store for audits, there’s no need to retain them in the primary storage.\n\nNow, let’s see how these events are aggregated for a given counter.\n\nAggregating Count Events:\n\nAs mentioned earlier, collecting all individual increments for every read request would be cost-prohibitive in terms of read performance. Therefore, a background aggregation process is necessary to continually converge counts and ensure optimal read performance.\n\nBut how can we safely aggregate count events amidst ongoing write operations?\n\nThis is where the concept of Eventually Consistent counts becomes crucial. By intentionally lagging behind the current time by a safe margin, we ensure that aggregation always occurs within an immutable window.\n\nLets see what that looks like:\n\nLet’s break this down:\n\nlastRollupTs : This represents the most recent time when the counter value was last aggregated. For a counter being operated for the first time, this timestamp defaults to a reasonable time in the past.\n\n: This represents the most recent time when the counter value was last aggregated. For a counter being operated for the first time, this timestamp defaults to a reasonable time in the past. Immutable Window and Lag: Aggregation can only occur safely within an immutable window that is no longer receiving counter events. The “acceptLimit” parameter of the TimeSeries Abstraction plays a crucial role here, as it rejects incoming events with timestamps beyond this limit. During aggregations, this window is pushed slightly further back to account for clock skews.\n\nThis does mean that the counter value will lag behind its most recent update by some margin (typically in the order of seconds). This approach does leave the door open for missed events due to cross-region replication issues. See “Future Work” section at the end.\n\nAggregation Process: The rollup process aggregates all events in the aggregation window since the last rollup to arrive at the new value.\n\nRollup Store:\n\nWe save the results of this aggregation in a persistent store. The next aggregation will simply continue from this checkpoint.\n\nWe create one such Rollup table per dataset and use Cassandra as our persistent store. However, as you will soon see in the Control Plane section, the Counter service can be configured to work with any persistent store.\n\nLastWriteTs: Every time a given counter receives a write, we also log a last-write-timestamp as a columnar update in this table. This is done using Cassandra’s USING TIMESTAMP feature to predictably apply the Last-Write-Win (LWW) semantics. This timestamp is the same as the event_time for the event. In the subsequent sections, we’ll see how this timestamp is used to keep some counters in active rollup circulation until they have caught up to their latest value.\n\nRollup Cache\n\nTo optimize read performance, these values are cached in EVCache for each counter. We combine the lastRollupCount and lastRollupTs into a single cached value per counter to prevent potential mismatches between the count and its corresponding checkpoint timestamp.\n\nBut, how do we know which counters to trigger rollups for? Let’s explore our Write and Read path to understand this better.\n\nAdd/Clear Count:\n\nAn add or clear count request writes durably to the TimeSeries Abstraction and updates the last-write-timestamp in the Rollup store. If the durability acknowledgement fails, clients can retry their requests with the same idempotency token without the risk of overcounting. Upon durability, we send a fire-and-forget request to trigger the rollup for the request counter.\n\nGetCount:\n\nWe return the last rolled-up count as a quick point-read operation, accepting the trade-off of potentially delivering a slightly stale count. We also trigger a rollup during the read operation to advance the last-rollup-timestamp, enhancing the performance of subsequent aggregations. This process also self-remediates a stale count if any previous rollups had failed.\n\nWith this approach, the counts continually converge to their latest value. Now, let’s see how we scale this approach to millions of counters and thousands of concurrent operations using our Rollup Pipeline.\n\nRollup Pipeline:\n\nEach Counter-Rollup server operates a rollup pipeline to efficiently aggregate counts across millions of counters. This is where most of the complexity in Counter Abstraction comes in. In the following sections, we will share key details on how efficient aggregations are achieved.\n\nLight-Weight Roll-Up Event: As seen in our Write and Read paths above, every operation on a counter sends a light-weight event to the Rollup server:\n\nrollupEvent: {\n\n\"namespace\": \"my_dataset\",\n\n\"counter\": \"counter123\"\n\n}\n\nNote that this event does not include the increment. This is only an indication to the Rollup server that this counter has been accessed and now needs to be aggregated. Knowing exactly which specific counters need to be aggregated prevents scanning the entire event dataset for the purpose of aggregations.\n\nIn-Memory Rollup Queues: A given Rollup server instance runs a set of in-memory queues to receive rollup events and parallelize aggregations. In the first version of this service, we settled on using in-memory queues to reduce provisioning complexity, save on infrastructure costs, and make rebalancing the number of queues fairly straightforward. However, this comes with the trade-off of potentially missing rollup events in case of an instance crash. For more details, see the “Stale Counts” section in “Future Work.”\n\nMinimize Duplicate Effort: We use a fast non-cryptographic hash like XXHash to ensure that the same set of counters end up on the same queue. Further, we try to minimize the amount of duplicate aggregation work by having a separate rollup stack that chooses to run fewer beefier instances.\n\nAvailability and Race Conditions: Having a single Rollup server instance can minimize duplicate aggregation work but may create availability challenges for triggering rollups. If we choose to horizontally scale the Rollup servers, we allow threads to overwrite rollup values while avoiding any form of distributed locking mechanisms to maintain high availability and performance. This approach remains safe because aggregation occurs within an immutable window. Although the concept of now() may differ between threads, causing rollup values to sometimes fluctuate, the counts will eventually converge to an accurate value within each immutable aggregation window.\n\nRebalancing Queues: If we need to scale the number of queues, a simple Control Plane configuration update followed by a re-deploy is enough to rebalance the number of queues.\n\n\"eventual_counter_config\": {\n\n\"queue_config\": {\n\n\"num_queues\" : 8, // change to 16 and re-deploy\n\n...\n\nHandling Deployments: During deployments, these queues shut down gracefully, draining all existing events first, while the new Rollup server instance starts up with potentially new queue configurations. There may be a brief period when both the old and new Rollup servers are active, but as mentioned before, this race condition is managed since aggregations occur within immutable windows.\n\nMinimize Rollup Effort: Receiving multiple events for the same counter doesn’t mean rolling it up multiple times. We drain these rollup events into a Set, ensuring a given counter is rolled up only once during a rollup window.\n\nEfficient Aggregation: Each rollup consumer processes a batch of counters simultaneously. Within each batch, it queries the underlying TimeSeries abstraction in parallel to aggregate events within specified time boundaries. The TimeSeries abstraction optimizes these range scans to achieve low millisecond latencies.\n\nDynamic Batching: The Rollup server dynamically adjusts the number of time partitions that need to be scanned based on cardinality of counters in order to prevent overwhelming the underlying store with many parallel read requests.\n\nAdaptive Back-Pressure: Each consumer waits for one batch to complete before issuing the rollups for the next batch. It adjusts the wait time between batches based on the performance of the previous batch. This approach provides back-pressure during rollups to prevent overwhelming the underlying TimeSeries store.\n\nHandling Convergence:\n\nIn order to prevent low-cardinality counters from lagging behind too much and subsequently scanning too many time partitions, they are kept in constant rollup circulation. For high-cardinality counters, continuously circulating them would consume excessive memory in our Rollup queues. This is where the last-write-timestamp mentioned previously plays a crucial role. The Rollup server inspects this timestamp to determine if a given counter needs to be re-queued, ensuring that we continue aggregating until it has fully caught up with the writes.\n\nNow, let’s see how we leverage this counter type to provide an up-to-date current count in near-realtime.\n\nExperimental: Accurate Global Counter\n\nWe are experimenting with a slightly modified version of the Eventually Consistent counter. Again, take the term ‘Accurate’ with a grain of salt. The key difference between this type of counter and its counterpart is that the delta, representing the counts since the last-rolled-up timestamp, is computed in real-time.\n\nAnd then, currentAccurateCount = lastRollupCount + delta\n\nAggregating this delta in real-time can impact the performance of this operation, depending on the number of events and partitions that need to be scanned to retrieve this delta. The same principle of rolling up in batches applies here to prevent scanning too many partitions in parallel. Conversely, if the counters in this dataset are accessed frequently, the time gap for the delta remains narrow, making this approach of fetching current counts quite effective.\n\nNow, let’s see how all this complexity is managed by having a unified Control Plane configuration.\n\nControl Plane\n\nThe Data Gateway Platform Control Plane manages control settings for all abstractions and namespaces, including the Counter Abstraction. Below, is an example of a control plane configuration for a namespace that supports eventually consistent counters with low cardinality:\n\n\"persistence_configuration\": [\n\n{\n\n\"id\": \"CACHE\", // Counter cache config\n\n\"scope\": \"dal=counter\",\n\n\"physical_storage\": {\n\n\"type\": \"EVCACHE\", // type of cache storage\n\n\"cluster\": \"evcache_dgw_counter_tier1\" // Shared EVCache cluster\n\n}\n\n},\n\n{\n\n\"id\": \"COUNTER_ROLLUP\",\n\n\"scope\": \"dal=counter\", // Counter abstraction config\n\n\"physical_storage\": {\n\n\"type\": \"CASSANDRA\", // type of Rollup store\n\n\"cluster\": \"cass_dgw_counter_uc1\", // physical cluster name\n\n\"dataset\": \"my_dataset_1\" // namespace/dataset\n\n},\n\n\"counter_cardinality\": \"LOW\", // supported counter cardinality\n\n\"config\": {\n\n\"counter_type\": \"EVENTUAL\", // Type of counter\n\n\"eventual_counter_config\": { // eventual counter type\n\n\"internal_config\": {\n\n\"queue_config\": { // adjust w.r.t cardinality\n\n\"num_queues\" : 8, // Rollup queues per instance\n\n\"coalesce_ms\": 10000, // coalesce duration for rollups\n\n\"capacity_bytes\": 16777216 // allocated memory per queue\n\n},\n\n\"rollup_batch_count\": 32 // parallelization factor\n\n}\n\n}\n\n}\n\n},\n\n{\n\n\"id\": \"EVENT_STORAGE\",\n\n\"scope\": \"dal=ts\", // TimeSeries Event store\n\n\"physical_storage\": {\n\n\"type\": \"CASSANDRA\", // persistent store type\n\n\"cluster\": \"cass_dgw_counter_uc1\", // physical cluster name\n\n\"dataset\": \"my_dataset_1\", // keyspace name\n\n},\n\n\"config\": {\n\n\"time_partition\": { // time-partitioning for events\n\n\"buckets_per_id\": 4, // event buckets within\n\n\"seconds_per_bucket\": \"600\", // smaller width for LOW card\n\n\"seconds_per_slice\": \"86400\", // width of a time slice table\n\n},\n\n\"accept_limit\": \"5s\", // boundary for immutability\n\n},\n\n\"lifecycleConfigs\": {\n\n\"lifecycleConfig\": [\n\n{\n\n\"type\": \"retention\", // Event retention\n\n\"config\": {\n\n\"close_after\": \"518400s\",\n\n\"delete_after\": \"604800s\" // 7 day count event retention\n\n}\n\n}\n\n]\n\n}\n\n}\n\n]\n\nUsing such a control plane configuration, we compose multiple abstraction layers using containers deployed on the same host, with each container fetching configuration specific to its scope.\n\nProvisioning\n\nAs with the TimeSeries abstraction, our automation uses a bunch of user inputs regarding their workload and cardinalities to arrive at the right set of infrastructure and related control plane configuration. You can learn more about this process in a talk given by one of our stunning colleagues, Joey Lynch : How Netflix optimally provisions infrastructure in the cloud.\n\nPerformance\n\nAt the time of writing this blog, this service was processing close to 75K count requests/second globally across the different API endpoints and datasets:\n\nwhile providing single-digit millisecond latencies for all its endpoints:\n\nFuture Work\n\nWhile our system is robust, we still have work to do in making it more reliable and enhancing its features. Some of that work includes:\n\nRegional Rollups: Cross-region replication issues can result in missed events from other regions. An alternate strategy involves establishing a rollup table for each region, and then tallying them in a global rollup table. A key challenge in this design would be effectively communicating the clearing of the counter across regions.\n\nCross-region replication issues can result in missed events from other regions. An alternate strategy involves establishing a rollup table for each region, and then tallying them in a global rollup table. A key challenge in this design would be effectively communicating the clearing of the counter across regions. Error Detection and Stale Counts: Excessively stale counts can occur if rollup events are lost or if a rollup fails and isn’t retried. This isn’t an issue for frequently accessed counters, as they remain in rollup circulation. This issue is more pronounced for counters that aren’t accessed frequently. Typically, the initial read for such a counter will trigger a rollup, self-remediating the issue. However, for use cases that cannot accept potentially stale initial reads, we plan to implement improved error detection, rollup handoffs, and durable queues for resilient retries.\n\nConclusion\n\nDistributed counting remains a challenging problem in computer science. In this blog, we explored multiple approaches to implement and deploy a Counting service at scale. While there may be other methods for distributed counting, our goal has been to deliver blazing fast performance at low infrastructure costs while maintaining high availability and providing idempotency guarantees. Along the way, we make various trade-offs to meet the diverse counting requirements at Netflix. We hope you found this blog post insightful.\n\nStay tuned for Part 3 of Composite Abstractions at Netflix, where we’ll introduce our Graph Abstraction, a new service being built on top of the Key-Value Abstraction and the TimeSeries Abstraction to handle high-throughput, low-latency graphs.\n\nAcknowledgments\n\nSpecial thanks to our stunning colleagues who contributed to the Counter Abstraction’s success: Joey Lynch, Vinay Chella, Kaidan Fullerton, Tom DeVoe, Mengqing Wang, Varun Khaitan", "label": "non_personal"}
{"title": "Title Launch Observability at Netflix Scale", "url": "https://netflixtechblog.com/title-launch-observability-at-netflix-scale-c88c586629eb?source=collection_home---4------10-----------------------", "content": "Title Launch Observability at Netflix Scale\n\nPart 1: Understanding The Challenges Netflix Technology Blog 5 min read · Dec 17, 2024 -- 7 Listen Share\n\nBy: Varun Khaitan\n\nWith special thanks to my stunning colleagues: Mallika Rao, Esmir Mesic, Hugo Marques\n\nIntroduction\n\nAt Netflix, we manage over a thousand global content launches each month, backed by billions of dollars in annual investment. Ensuring the success and discoverability of each title across our platform is a top priority, as we aim to connect every story with the right audience to delight our members. To achieve this, we are committed to building robust systems that deliver comprehensive observability, enabling us to take full accountability for every title on our service.\n\nThe Challenge of Title Launch Observability\n\nAs engineers, we’re wired to track system metrics like error rates, latencies, and CPU utilization — but what about metrics that matter to a title’s success?\n\nConsider the following example of two different Netflix Homepages:\n\nSample Homepage A\n\nSample Homepage B\n\nTo a basic recommendation system, the two sample pages might appear equivalent as long as the viewer watches the top title. Yet, these pages couldn’t be more different. Each title represents countless hours of effort and creativity, and our systems need to honor that uniqueness.\n\nHow do we bridge this gap? How can we design systems that recognize these nuances and empower every title to shine and bring joy to our members?\n\nThe Operational Needs of a Personalization System\n\nIn the early days of Netflix Originals, our launch team would huddle together at midnight, manually verifying that titles appeared in all the right places. While this hands-on approach worked for a handful of titles, it quickly became clear that it couldn’t scale. As Netflix expanded globally and the volume of title launches skyrocketed, the operational challenges of maintaining this manual process became undeniable.\n\nOperating a personalization system for a global streaming service involves addressing numerous inquiries about why certain titles appear or fail to appear at specific times and places.\n\nSome examples:\n\nWhy is title X not showing on the Coming Soon row for a particular member?\n\nWhy is title Y missing from the search page in Brazil?\n\nIs title Z being displayed correctly in all product experiences as intended?\n\nAs Netflix scaled, we faced the mounting challenge of providing accurate, timely answers to increasingly complex queries about title performance and discoverability. This led to a suite of fragmented scripts, runbooks, and ad hoc solutions scattered across teams — an approach that was neither sustainable nor efficient.\n\nThe stakes are even higher when ensuring every title launches flawlessly. Metadata and assets must be correctly configured, data must flow seamlessly, microservices must process titles without error, and algorithms must function as intended. The complexity of these operational demands underscored the urgent need for a scalable solution.\n\nAutomating the Operations\n\nIt becomes evident over time that we need to automate our operations to scale with the business. As we thought more about this problem and possible solutions, two clear options emerged.\n\nOption 1: Log Processing\n\nLog processing offers a straightforward solution for monitoring and analyzing title launches. By logging all titles as they are displayed, we can process these logs to identify anomalies and gain insights into system performance. This approach provides a few advantages:\n\nLow burden on existing systems: Log processing imposes minimal changes to existing infrastructure. By leveraging logs, which are already generated during regular operations, we can scale observability without significant system modifications. This allows us to focus on data analysis and problem-solving rather than managing complex system changes. Using the source of truth: Logs serve as a reliable “source of truth” by providing a comprehensive record of system events. They allow us to verify whether titles are presented as intended and investigate any discrepancies. This capability is crucial for ensuring our recommendation systems and user interfaces function correctly, supporting successful title launches.\n\nHowever, taking this approach also presents several challenges:\n\nCatching Issues Ahead of Time: Logging primarily addresses post-launch scenarios, as logs are generated only after titles are shown to members. To detect issues proactively, we need to simulate traffic and predict system behavior in advance. Once artificial traffic is generated, discarding the response object and relying solely on logs becomes inefficient. Appropriate Accuracy: Comprehensive logging requires services to log both included and excluded titles, along with reasons for exclusion. This could lead to an exponential increase in logged data. Utilizing probabilistic logging methods could compromise accuracy, making it difficult to ascertain whether a title’s absence in logs is due to exclusion or random chance. SLA and Cost Considerations: Our existing online logging systems do not natively support logging at the title granularity level. While reengineering these systems to accommodate this additional axis is possible, it would entail increased costs. Additionally, the time-sensitive nature of these investigations precludes the use of cold storage, which cannot meet the stringent SLAs required.\n\nOption 2: Observability Endpoints in Our Personalization Systems\n\nTo prioritize title launch observability, we could adopt a centralized approach. By introducing observability endpoints across all systems, we can enable real-time data flow into a dedicated microservice for title launch observability. This approach embeds observability directly into the very fabric of services managing title launches and personalization, ensuring seamless monitoring and insights. Key benefits and strategies include:\n\nReal-Time Monitoring: Observability endpoints enable real-time monitoring of system performance and title placements, allowing us to detect and address issues as they arise. Proactive Issue Detection: By simulating future traffic(an aspect we call “time travel”) and capturing system responses ahead of time, we can preemptively identify potential issues before they impact our members or the business. Enhanced Accuracy: Observability endpoints provide precise data on title inclusions and exclusions, allowing us to make accurate assertions about system behavior and title visibility. It also provides us with advanced debugability information needed to fix identified issues. Scalability and Cost Efficiency: While initial implementation required some investment, this approach ultimately offers a scalable and cost-effective solution to managing title launches at Netflix scale.\n\nChoosing this option also comes with some tradeoffs:\n\nSignificant Initial Investment: Several systems would need to create new endpoints and refactor their codebases to adopt this new method of prioritizing launches. Synchronization Risk: There would be a potential risk that these new endpoints may not accurately represent production behavior, thus necessitating conscious efforts to ensure all endpoints remain synchronized.\n\nUp Next\n\nBy adopting a comprehensive observability strategy that includes real-time monitoring, proactive issue detection, and source of truth reconciliation, we’ve significantly enhanced our ability to ensure the successful launch and discovery of titles across Netflix, enriching the global viewing experience for our members. In the next part of this series, we’ll dive into how we achieved this, sharing key technical insights and details.\n\nStay tuned for a closer look at the innovation behind the scenes in Part 2!", "label": "non_personal"}
{"title": "Cloud Efficiency at Netflix", "url": "https://netflixtechblog.com/cloud-efficiency-at-netflix-f2a142955f83?source=collection_home---4------9-----------------------", "content": "Cloud Efficiency at Netflix Netflix Technology Blog 5 min read · Dec 17, 2024 -- 10 Listen Share\n\nBy J Han, Pallavi Phadnis\n\nContext\n\nAt Netflix, we use Amazon Web Services (AWS) for our cloud infrastructure needs, such as compute, storage, and networking to build and run the streaming platform that we love. Our ecosystem enables engineering teams to run applications and services at scale, utilizing a mix of open-source and proprietary solutions. In turn, our self-serve platforms allow teams to create and deploy, sometimes custom, workloads more efficiently. This diverse technological landscape generates extensive and rich data from various infrastructure entities, from which, data engineers and analysts collaborate to provide actionable insights to the engineering organization in a continuous feedback loop that ultimately enhances the business.\n\nOne crucial way in which we do this is through the democratization of highly curated data sources that sunshine usage and cost patterns across Netflix’s services and teams. The Data & Insights organization partners closely with our engineering teams to share key efficiency metrics, empowering internal stakeholders to make informed business decisions.\n\nData is Key\n\nThis is where our team, Platform DSE (Data Science Engineering), comes in to enable our engineering partners to understand what resources they’re using, how effectively and efficiently they use those resources, and the cost associated with their resource usage. We want our downstream consumers to make cost conscious decisions using our datasets.\n\nTo address these numerous analytic needs in a scalable way, we’ve developed a two-component solution:\n\nFoundational Platform Data (FPD): This component provides a centralized data layer for all platform data, featuring a consistent data model and standardized data processing methodology. Cloud Efficiency Analytics (CEA): Built on top of FPD, this component offers an analytics data layer that provides time series efficiency metrics across various business use cases.\n\nFoundational Platform Data (FPD)\n\nWe work with different platform data providers to get inventory, ownership, and usage data for the respective platforms they own. Below is an example of how this framework applies to the Spark platform. FPD establishes data contracts with producers to ensure data quality and reliability; these contracts allow the team to leverage a common data model for ownership. The standardized data model and processing promotes scalability and consistency.\n\nCloud Efficiency Analytics (CEA Data)\n\nOnce the foundational data is ready, CEA consumes inventory, ownership, and usage data and applies the appropriate business logic to produce cost and ownership attribution at various granularities. The data model approach in CEA is to compartmentalize and be transparent; we want downstream consumers to understand why they’re seeing resources show up under their name/org and how those costs are calculated. Another benefit to this approach is the ability to pivot quickly as new or changes in business logic is/are introduced.\n\n* For cost accounting purposes, we resolve assets to a single owner, or distribute costs when assets are multi-tenant. However, we do also provide usage and cost at different aggregations for different consumers.\n\nData Principles\n\nAs the source of truth for efficiency metrics, our team’s tenants are to provide accurate, reliable, and accessible data, comprehensive documentation to navigate the complexity of the efficiency space, and well-defined Service Level Agreements (SLAs) to set expectations with downstream consumers during delays, outages or changes.\n\nWhile ownership and cost may seem straightforward, the complexity of the datasets is considerably high due to the breadth and scope of the business infrastructure and platform specific features. Services can have multiple owners, cost heuristics are unique to each platform, and the scale of infra data is large. As we work on expanding infrastructure coverage to all verticals of the business, we face a unique set of challenges:\n\nA Few Sizes to Fit the Majority\n\nDespite data contracts and a standardized data model on transforming upstream platform data into FPD and CEA, there is usually some degree of customization that is unique to that particular platform. As the centralized source of truth, we feel the constant tension of where to place the processing burden. Decision-making involves ongoing transparent conversations with both our data producers and consumers, frequent prioritization checks, and alignment with business needs as informed captains in this space.\n\nData Guarantees\n\nFor data correctness and trust, it’s crucial that we have audits and visibility into health metrics at each layer in the pipeline in order to investigate issues and root cause anomalies quickly. Maintaining data completeness while ensuring correctness becomes challenging due to upstream latency and required transformations to have the data ready for consumption. We continuously iterate our audits and incorporate feedback to refine and meet our SLAs.\n\nAbstraction Layers\n\nWe value people over process, and it is not uncommon for engineering teams to build custom SaaS solutions for other parts of the organization. Although this fosters innovation and improves development velocity, it can create a bit of a conundrum when it comes to understanding and interpreting usage patterns and attributing cost in a way that makes sense to the business and end consumer. With clear inventory, ownership, and usage data from FPD, and precise attribution in the analytical layer, we aim to provide metrics to downstream users regardless of whether they utilize and build on top of internal platforms or on AWS resources directly.\n\nFuture Forward\n\nLooking ahead, we aim to continue onboarding platforms to FPD and CEA, striving for nearly complete cost insight coverage in the upcoming year. Longer term, we plan to extend FPD to other areas of the business such as security and availability. We aim to move towards proactive approaches via predictive analytics and ML for optimizing usage and detecting anomalies in cost.\n\nUltimately, our goal is to enable our engineering organization to make efficiency-conscious decisions when building and maintaining the myriad of services that allow us to enjoy Netflix as a streaming service.\n\nAcknowledgments\n\nThe FPD and CEA work would not have been possible without the cross functional input of many outstanding colleagues and our dedicated team building these important data assets.\n\n—\n\nA bit about the authors:\n\nJHan enjoys nature, reading fantasy, and finding the best chocolate chip cookies and cinnamon rolls. She is adamant about writing the SQL select statement with leading commas.\n\nPallavi enjoys music, travel and watching astrophysics documentaries. With 15+ years working with data, she knows everything’s better with a dash of analytics and a cup of coffee!", "label": "non_personal"}
{"title": "Part 1: A Survey of Analytics Engineering Work at Netflix", "url": "https://netflixtechblog.com/part-1-a-survey-of-analytics-engineering-work-at-netflix-d761cfd551ee?source=collection_home---4------8-----------------------", "content": "Part 1: A Survey of Analytics Engineering Work at Netflix Netflix Technology Blog 7 min read · Dec 17, 2024 -- 2 Listen Share\n\nThis article is the first in a multi-part series sharing a breadth of Analytics Engineering work at Netflix, recently presented as part of our annual internal Analytics Engineering conference. We kick off with a few topics focused on how we’re empowering Netflix to efficiently produce and effectively deliver high quality, actionable analytic insights across the company. Subsequent posts will detail examples of exciting analytic engineering domain applications and aspects of the technical craft.\n\nAt Netflix, we seek to entertain the world by ensuring our members find the shows and movies that will thrill them. Analytics at Netflix powers everything from understanding what content will excite and bring members back for more to how we should produce and distribute a content slate that maximizes member joy. Analytics Engineers deliver these insights by establishing deep business and product partnerships; translating business challenges into solutions that unblock critical decisions; and designing, building, and maintaining end-to-end analytical systems.\n\nEach year, we bring the Analytics Engineering community together for an Analytics Summit — a 3-day internal conference to share analytical deliverables across Netflix, discuss analytic practice, and build relationships within the community. We covered a broad array of exciting topics and wanted to spotlight a few to give you a taste of what we’re working on across Analytics Engineering at Netflix!\n\nDataJunction: Unifying Experimentation and Analytics\n\nYian Shang, Anh Le\n\nAt Netflix, like in many organizations, creating and using metrics is often more complex than it should be. Metric definitions are often scattered across various databases, documentation sites, and code repositories, making it difficult for analysts and data scientists to find reliable information quickly. This fragmentation leads to inconsistencies and wastes valuable time as teams end up reinventing metrics or seeking clarification on definitions that should be standardized and readily accessible.\n\nEnter DataJunction (DJ). DJ acts as a central store where metric definitions can live and evolve. Once a metric owner has registered a metric into DJ, metric consumers throughout the organization can apply that same metric definition to a set of filtered records and aggregate to any dimensional grain.\n\nAs an example, imagine an analyst wanting to create a “Total Streaming Hours” metric. To add this metric to DJ, they need to provide two pieces of information:\n\nThe fact table that the metric comes from:\n\nSELECT\n\naccount_id, country_iso_code, streaming_hours\n\nFROM streaming_fact_table\n\nThe metric expression:\n\n`SUM(streaming_hours)`\n\nThen metric consumers throughout the organization can call DJ to request either the SQL or the resulting data. For example,\n\ntotal_streaming_hours of each account:\n\ndj.sql(metrics=[“total_streaming_hours”], dimensions=[“account_id”]))\n\ntotal_streaming_hours of each country:\n\ndj.sql(metrics=[“total_streaming_hours”], dimensions=[“country_iso_code”]))\n\ntotal_streaming_hours of each account in the US:\n\ndj.sql(metrics=[“total_streaming_hours”], dimensions=[“country_iso_code”], filters=[“country_iso_code = ‘US’”]))\n\nThe key here is that DJ can perform the dimensional join on users’ behalf. If country_iso_code doesn’t already exist in the fact table, the metric owner only needs to tell DJ that account_id is the foreign key to an `users_dimension_table` (we call this process “dimension linking”). DJ then can perform the joins to bring in any requested dimensions from `users_dimension_table`.\n\nThe Netflix Experimentation Platform heavily leverages this feature today by treating cell assignment as just another dimension that it asks DJ to bring in. For example, to compare the average streaming hours in cell A vs cell B, the Experimentation Platform relies on DJ to bring in “cell_assignment” as a user’s dimension (no different from country_iso_code). A metric can therefore be defined once in DJ and be made available across analytics dashboards and experimentation analysis.\n\nDJ has a strong pedigree–there are several prior semantic layers in the industry (e.g. Minerva at Airbnb; dbt Transform, Looker, and AtScale as paid solutions). DJ stands out as an open source solution that is actively developed and stress-tested at Netflix. We’d love to see DJ easing your metric creation and consumption pain points!\n\nLORE: How we’re democratizing analytics at Netflix\n\nApurva Kansara\n\nAt Netflix, we rely on data and analytics to inform critical business decisions. Over time, this has resulted in large numbers of dashboard products. While such analytics products are tremendously useful, we noticed a few trends:\n\nA large portion of such products have less than 5 MAU (monthly active users) We spend a tremendous amount of time building and maintaining business metrics and dimensions We see inconsistencies in how a particular metric is calculated, presented, and maintained across the Data & Insights organization. It is challenging to scale such bespoke solutions to ever-changing and increasingly complex business needs.\n\nAnalytics Enablement is a collection of initiatives across Data & Insights all focused on empowering Netflix analytic practitioners to efficiently produce and effectively deliver high-quality, actionable insights.\n\nSpecifically, these initiatives are focused on enabling analytics rather than on the activities that produce analytics (e.g., dashboarding, analysis, research, etc.).\n\nAs part of broad analytics enablement across all business domains, we invested in a chatbot to provide real insights to our end users using the power of LLM. One reason LLMs are well suited for such problems is that they tie the versatility of natural language with the power of data query to enable our business users to query data that would otherwise require sophisticated knowledge of underlying data models.\n\nBesides providing the end user with an instant answer in a preferred data visualization, LORE instantly learns from the user’s feedback. This allows us to teach LLM a context-rich understanding of internal business metrics that were previously locked in custom code for each of the dashboard products.\n\nSome of the challenges we run into:\n\nGaining user trust: To gain our end users’ trust, we focused on our model’s explainability. For example, LORE provides human-readable reasoning on how it arrived at the answer that users can cross-verify. LORE also provides a confidence score to our end users based on its grounding in the domain space.\n\nTraining: We created easy-to-provide feedback using 👍 and 👎 with a fully integrated fine-tuning loop to allow end-users to teach new domains and questions around it effectively. This allowed us to bootstrap LORE across several domains within Netflix.\n\nDemocratizing analytics can unlock the tremendous potential of data for everyone within the company. With Analytics enablement and LORE, we’ve enabled our business users to truly have a conversation with the data.\n\nLeveraging Foundational Platform Data to enable Cloud Efficiency Analytics\n\nJ Han, Pallavi Phadnis\n\nAt Netflix, we use Amazon Web Services (AWS) for our cloud infrastructure needs, such as compute, storage, and networking to build and run the streaming platform that we love. Our ecosystem enables engineering teams to run applications and services at scale, utilizing a mix of open-source and proprietary solutions. In order to understand how efficiently we operate in this diverse technological landscape, the Data & Insights organization partners closely with our engineering teams to share key efficiency metrics, empowering internal stakeholders to make informed business decisions.\n\nThis is where our team, Platform DSE (Data Science Engineering), comes in to enable our engineering partners to understand what resources they’re using, how effectively they utilize those resources, and the cost associated with their resource usage. By creating curated datasets and democratizing access via a custom insights app and various integration points, downstream users can gain granular insights essential for making data-driven, cost-effective decisions for the business.\n\nTo address the numerous analytic needs in a scalable way, we’ve developed a two-component solution:\n\nFoundational Platform Data (FPD): This component provides a centralized data layer for all platform data, featuring a consistent data model and standardized data processing methodology. We work with different platform data providers to get inventory, ownership, and usage data for the respective platforms they own. Cloud Efficiency Analytics (CEA): Built on top of FPD, this component offers an analytics data layer that provides time series efficiency metrics across various business use cases. Once the foundational data is ready, CEA consumes inventory, ownership, and usage data and applies the appropriate business logic to produce cost and ownership attribution at various granularities.\n\nAs the source of truth for efficiency metrics, our team’s tenants are to provide accurate, reliable, and accessible data, comprehensive documentation to navigate the complexity of the efficiency space, and well-defined Service Level Agreements (SLAs) to set expectations with downstream consumers during delays, outages, or changes.\n\nLooking ahead, we aim to continue onboarding platforms, striving for nearly complete cost insight coverage. We’re also exploring new use cases, such as tailored reports for platforms, predictive analytics for optimizing usage and detecting anomalies in cost, and a root cause analysis tool using LLMs.\n\nUltimately, our goal is to enable our engineering organization to make efficiency-conscious decisions when building and maintaining the myriad of services that allows us to enjoy Netflix as a streaming service. For more detail on our modeling approach and principles, check out this post!", "label": "non_personal"}
{"title": "Introducing Configurable Metaflow", "url": "https://netflixtechblog.com/introducing-configurable-metaflow-d2fb8e9ba1c6?source=collection_home---4------7-----------------------", "content": "Introducing Configurable Metaflow Netflix Technology Blog 13 min read · Dec 20, 2024 -- 4 Listen Share\n\nDavid J. Berg*, David Casler^, Romain Cledat*, Qian Huang*, Rui Lin*, Nissan Pow*, Nurcan Sonmez*, Shashank Srikanth*, Chaoying Wang*, Regina Wang*, Darin Yu*\n\n*: Model Development Team, Machine Learning Platform\n\n^: Content Demand Modeling Team\n\nA month ago at QConSF, we showcased how Netflix utilizes Metaflow to power a diverse set of ML and AI use cases, managing thousands of unique Metaflow flows. This followed a previous blog on the same topic. Many of these projects are under constant development by dedicated teams with their own business goals and development best practices, such as the system that supports our content decision makers, or the system that ranks which language subtitles are most valuable for a specific piece of content.\n\nAs a central ML and AI platform team, our role is to empower our partner teams with tools that maximize their productivity and effectiveness, while adapting to their specific needs (not the other way around). This has been a guiding design principle with Metaflow since its inception.\n\nMetaflow infrastructure stack\n\nStanding on the shoulders of our extensive cloud infrastructure, Metaflow facilitates easy access to data, compute, and production-grade workflow orchestration, as well as built-in best practices for common concerns such as collaboration, versioning, dependency management, and observability, which teams use to setup ML/AI experiments and systems that work for them. As a result, Metaflow users at Netflix have been able to run millions of experiments over the past few years without wasting time on low-level concerns.\n\nA long standing FAQ: configurable flows\n\nWhile Metaflow aims to be un-opinionated about some of the upper levels of the stack, some teams within Netflix have developed their own opinionated tooling. As part of Metaflow’s adaptation to their specific needs, we constantly try to understand what has been developed and, more importantly, what gaps these solutions are filling.\n\nIn some cases, we determine that the gap being addressed is very team specific, or too opinionated at too high a level in the stack, and we therefore decide to not develop it within Metaflow. In other cases, however, we realize that we can develop an underlying construct that aids in filling that gap. Note that even in that case, we do not always aim to completely fill the gap and instead focus on extracting a more general lower level concept that can be leveraged by that particular user but also by others. One such recurring pattern we noticed at Netflix is the need to deploy sets of closely related flows, often as part of a larger pipeline involving table creations, ETLs, and deployment jobs. Frequently, practitioners want to experiment with variants of these flows, testing new data, new parameterizations, or new algorithms, while keeping the overall structure of the flow or flows intact.\n\nA natural solution is to make flows configurable using configuration files, so variants can be defined without changing the code. Thus far, there hasn’t been a built-in solution for configuring flows, so teams have built their bespoke solutions leveraging Metaflow’s JSON-typed Parameters, IncludeFile, and deploy-time Parameters or deploying their own home-grown solution (often with great pain). However, none of these solutions make it easy to configure all aspects of the flow’s behavior, decorators in particular.\n\nRequests for a feature like Metaflow Config\n\nOutside Netflix, we have seen similar frequently asked questions on the Metaflow community Slack as shown in the user quotes above:\n\nhow can I adjust the @resource requirements, such as CPU or memory, without having to hardcode the values in my flows?\n\nhow to adjust the triggering @schedule programmatically, so our production and staging deployments can run at different cadences?\n\nNew in Metaflow: Configs!\n\nToday, to answer the FAQ, we introduce a new — small but mighty — feature in Metaflow: a Config object. Configs complement the existing Metaflow constructs of artifacts and Parameters, by allowing you to configure all aspects of the flow, decorators in particular, prior to any run starting. At the end of the day, artifacts, Parameters and Configs are all stored as artifacts by Metaflow but they differ in when they are persisted as shown in the diagram below:\n\nDifferent data artifacts in Metaflow\n\nSaid another way:\n\nAn artifact is resolved and persisted to the datastore at the end of each task.\n\nis resolved and persisted to the datastore at the end of each task. A parameter is resolved and persisted at the start of a run; it can therefore be modified up to that point. One common use case is to use triggers to pass values to a run right before executing. Parameters can only be used within your step code.\n\nis resolved and persisted at the start of a run; it can therefore be modified up to that point. One common use case is to use triggers to pass values to a run right before executing. Parameters can only be used within your step code. A config is resolved and persisted when the flow is deployed. When using a scheduler such as Argo Workflows, deployment happens when create’ing the flow. In the case of a local run, “deployment” happens just prior to the execution of the run — think of “deployment” as gathering all that is needed to run the flow. Unlike parameters, configs can be used more widely in your flow code, particularly, they can be used in step or flow level decorators as well as to set defaults for parameters. Configs can of course also be used within your flow.\n\nAs an example, you can specify a Config that reads a pleasantly human-readable configuration file, formatted as TOML. The Config specifies a triggering ‘@schedule’ and ‘@resource’ requirements, as well as application-specific parameters for this specific deployment:\n\n[schedule]\n\ncron = \"0 * * * *\"\n\n\n\n[model]\n\noptimizer = \"adam\"\n\nlearning_rate = 0.5\n\n\n\n[resources]\n\ncpu = 1\n\nUsing the newly released Metaflow 2.13, you can configure a flow with a Config like above, as demonstrated by this flow:\n\nimport pprint\n\nfrom metaflow import FlowSpec, step, Config, resources, config_expr, schedule\n\n\n\n@schedule(cron=config_expr(\"config.schedule.cron\"))\n\nclass ConfigurableFlow(FlowSpec):\n\nconfig = Config(\"config\", default=\"myconfig.toml\", parser=\"tomllib.loads\")\n\n\n\n@resources(cpu=config.resources.cpu)\n\n@step\n\ndef start(self):\n\nprint(\"Config loaded:\")\n\npprint.pp(self.config)\n\nself.next(self.end)\n\n\n\n@step\n\ndef end(self):\n\npass\n\n\n\nif __name__ == \"__main__\":\n\nConfigurableFlow()\n\nThere is a lot going on in the code above, a few highlights:\n\nyou can refer to configs before they have been defined using ‘config_expr’.\n\nyou can define arbitrary parsers — using a string means the parser doesn’t even have to be present remotely!\n\nFrom the developer’s point of view, Configs behave like dictionary-like artifacts. For convenience, they support the dot-syntax (when possible) for accessing keys, making it easy to access values in a nested configuration. You can also unpack the whole Config (or a subtree of it) with Python’s standard dictionary unpacking syntax, ‘**config’. The standard dictionary subscript notation is also available.\n\nSince Configs turn into dictionary artifacts, they get versioned and persisted automatically as artifacts. You can access Configs of any past runs easily through the Client API. As a result, your data, models, code, Parameters, Configs, and execution environments are all stored as a consistent bundle — neatly organized in Metaflow namespaces — paving the way for easily reproducible, consistent, low-boilerplate, and now easily configurable experiments and robust production deployments.\n\nMore than a humble config file\n\nWhile you can get far by accompanying your flow with a simple config file (stored in your favorite format, thanks to user-definable parsers), Configs unlock a number of advanced use cases. Consider these examples from the updated documentation:\n\nA major benefit of Config over previous more hacky solutions for configuring flows is that they work seamlessly with other features of Metaflow: you can run steps remotely and deploy flows to production, even when relying on custom parsers, without having to worry about packaging Configs or parsers manually or keeping Configs consistent across tasks. Configs also work with the Runner and Deployer.\n\nThe Hollywood principle: don’t call us, we’ll call you\n\nWhen used in conjunction with a configuration manager like Hydra, Configs enable a pattern that is highly relevant for ML and AI use cases: orchestrating experiments over multiple configurations or sweeping over parameter spaces. While Metaflow has always supported sweeping over parameter grids easily using foreaches, it hasn’t been easily possible to alter the flow itself, e.g. to change @resources or @pypi/@conda dependencies for every experiment.\n\nIn a typical case, you trigger a Metaflow flow that consumes a configuration file, changing how a run behaves. With Hydra, you can invert the control: it is Hydra that decides what gets run based on a configuration file. Thanks to Metaflow’s new Runner and Deployer APIs, you can create a Hydra app that operates Metaflow programmatically — for instance, to deploy and execute hundreds of variants of a flow in a large-scale experiment.\n\nTake a look at two interesting examples of this pattern in the documentation. As a teaser, this video shows Hydra orchestrating deployment of tens of Metaflow flows, each of which benchmarks PyTorch using a varying number of CPU cores and tensor sizes, updating a visualization of the results in real-time as the experiment progresses:\n\nExample using Hydra with Metaflow\n\nMetaboosting Metaflow — based on a true story\n\nTo give a motivating example of what configurations look like at Netflix in practice, let’s consider Metaboost, an internal Netflix CLI tool that helps ML practitioners manage, develop and execute their cross-platform projects, somewhat similar to the open-source Hydra discussed above but with specific integrations to the Netflix ecosystem. Metaboost is an example of an opinionated framework developed by a team already using Metaflow. In fact, a part of the inspiration for introducing Configs in Metaflow came from this very use case.\n\nMetaboost serves as a single interface to three different internal platforms at Netflix that manage ETL/Workflows (Maestro), Machine Learning Pipelines (Metaflow) and Data Warehouse Tables (Kragle). In this context, having a single configuration system to manage a ML project holistically gives users increased project coherence and decreased project risk.\n\nConfiguration in Metaboost\n\nEase of configuration and templatizing are core values of Metaboost. Templatizing in Metaboost is achieved through the concept of bindings, wherein we can bind a Metaflow pipeline to an arbitrary label, and then create a corresponding bespoke configuration for that label. The binding-connected configuration is then merged into a global set of configurations containing such information as GIT repository, branch, etc. Binding a Metaflow, will also signal to Metaboost that it should instantiate the Metaflow flow once per binding into our orchestration cluster.\n\nImagine a ML practitioner on the Netflix Content ML team, sourcing features from hundreds of columns in our data warehouse, and creating a multitude of models against a growing suite of metrics. When a brand new content metric comes along, with Metaboost, the first version of the metric’s predictive model can easily be created by simply swapping the target column against which the model is trained.\n\nSubsequent versions of the model will result from experimenting with hyper parameters, tweaking feature engineering, or conducting feature diets. Metaboost’s bindings, and their integration with Metaflow Configs, can be leveraged to scale the number of experiments as fast as a scientist can create experiment based configurations.\n\nScaling experiments with Metaboost bindings — backed by Metaflow Config\n\nConsider a Metaboost ML project named `demo` that creates and loads data to custom tables (ETL managed by Maestro), and then trains a simple model on this data (ML Pipeline managed by Metaflow). The project structure of this repository might look like the following:\n\n├── metaflows\n\n│ ├── custom -> custom python code, used by\n\n| | | Metaflow\n\n│ │ ├── data.py\n\n│ │ └── model.py\n\n│ └── training.py -> defines our Metaflow pipeline\n\n├── schemas\n\n│ ├── demo_features_f.tbl.yaml -> table DDL, stores our ETL\n\n| | output, Metaflow input\n\n│ └── demo_predictions_f.tbl.yaml -> table DDL,\n\n| stores our Metaflow output\n\n├── settings\n\n│ ├── settings.configuration.EXP_01.yaml -> defines the additive\n\n| | config for Experiment 1\n\n│ ├── settings.configuration.EXP_02.yaml -> defines the additive\n\n| | config for Experiment 2\n\n│ ├── settings.configuration.yaml -> defines our global\n\n| | configuration\n\n│ └── settings.environment.yaml -> defines parameters based on\n\n| git branch (e.g. READ_DB)\n\n├── tests\n\n├── workflows\n\n│ ├── sql\n\n│ ├── demo.demo_features_f.sch.yaml -> Maestro workflow, defines ETL\n\n│ └── demo.main.sch.yaml -> Maestro workflow, orchestrates\n\n| ETLs and Metaflow\n\n└── metaboost.yaml -> defines our project for\n\nMetaboost\n\nThe configuration files in the settings directory above contain the following YAML files:\n\n# settings.configuration.yaml (global configuration)\n\nmodel:\n\nfit_intercept: True\n\nconda:\n\nnumpy: '1.22.4'\n\n\"scikit-learn\": '1.4.0'\n\n# settings.configuration.EXP_01.yaml\n\ntarget_column: metricA\n\nfeatures:\n\n- runtime\n\n- content_type\n\n- top_billed_talent\n\n# settings.configuration.EXP_02.yaml\n\ntarget_column: metricA\n\nfeatures:\n\n- runtime\n\n- director\n\n- box_office\n\nMetaboost will merge each experiment configuration (*.EXP*.yaml) into the global configuration (settings.configuration.yaml) individually at Metaboost command initialization. Let’s take a look at how Metaboost combines these configurations with a Metaboost command:\n\n(venv-demo) ~/projects/metaboost-demo [branch=demoX]\n\n$ metaboost metaflow settings show --yaml-path=configuration\n\n\n\nbinding=EXP_01:\n\nmodel: -> defined in setting.configuration.yaml (global)\n\nfit_intercept: true\n\nconda: -> defined in setting.configuration.yaml (global)\n\nnumpy: 1.22.4\n\n\"scikit-learn\": 1.4.0\n\ntarget_column: metricA -> defined in setting.configuration.EXP_01.yaml\n\nfeatures: -> defined in setting.configuration.EXP_01.yaml\n\n- runtime\n\n- content_type\n\n- top_billed_talent\n\n\n\nbinding=EXP_02:\n\nmodel: -> defined in setting.configuration.yaml (global)\n\nfit_intercept: true\n\nconda: -> defined in setting.configuration.yaml (global)\n\nnumpy: 1.22.4\n\n\"scikit-learn\": 1.4.0\n\ntarget_column: metricA -> defined in setting.configuration.EXP_02.yaml\n\nfeatures: -> defined in setting.configuration.EXP_02.yaml\n\n- runtime\n\n- director\n\n- box_office\n\nMetaboost understands it should deploy/run two independent instances of training.py — one for the EXP_01 binding and one for the EXP_02 binding. You can also see that Metaboost is aware that the tables and ETL workflows are not bound, and should only be deployed once. These details of which artifacts to bind and which to leave unbound are encoded in the project’s top-level metaboost.yaml file.\n\n(venv-demo) ~/projects/metaboost-demo [branch=demoX]\n\n$ metaboost project list\n\n\n\nTables (metaboost table list):\n\nschemas/demo_predictions_f.tbl.yaml (binding=default):\n\ntable_path=prodhive/demo_db/demo_predictions_f\n\nschemas/demo_features_f.tbl.yaml (binding=default):\n\ntable_path=prodhive/demo_db/demo_features_f\n\n\n\nWorkflows (metaboost workflow list):\n\nworkflows/demo.demo_features_f.sch.yaml (binding=default):\n\ncluster=sandbox, workflow.id=demo.branch_demox.demo_features_f\n\nworkflows/demo.main.sch.yaml (binding=default):\n\ncluster=sandbox, workflow.id=demo.branch_demox.main\n\n\n\nMetaflows (metaboost metaflow list):\n\nmetaflows/training.py (binding=EXP_01): -> EXP_01 instance of training.py\n\ncluster=sandbox, workflow.id=demo.branch_demox.EXP_01.training\n\nmetaflows/training.py (binding=EXP_02): -> EXP_02 instance of training.py\n\ncluster=sandbox, workflow.id=demo.branch_demox.EXP_02.training\n\nBelow is a simple Metaflow pipeline that fetches data, executes feature engineering, and trains a LinearRegression model. The work to integrate Metaboost Settings into a user’s Metaflow pipeline (implemented using Metaflow Configs) is as easy as adding a single mix-in to the FlowSpec definition:\n\nfrom metaflow import FlowSpec, Parameter, conda_base, step\n\nfrom custom.data import feature_engineer, get_data\n\nfrom metaflow.metaboost import MetaboostSettings\n\n\n\n@conda_base(\n\nlibraries=MetaboostSettings.get_deploy_time_settings(\"configuration.conda\")\n\n)\n\nclass DemoTraining(FlowSpec, MetaboostSettings):\n\nprediction_date = Parameter(\"prediction_date\", type=int, default=-1)\n\n\n\n@step\n\ndef start(self):\n\n# get show_settings() for free with the mixin\n\n# and get convenient debugging info\n\nself.show_settings(exclude_patterns=[\"artifact*\", \"system*\"])\n\n\n\nself.next(self.get_features)\n\n\n\n@step\n\ndef get_features(self):\n\n# feature engineers on our extracted data\n\nself.fe_df = feature_engineer(\n\n# loads data from our ETL pipeline\n\ndata=get_data(prediction_date=self.prediction_date),\n\nfeatures=self.settings.configuration.features +\n\n[self.settings.configuration.target_column]\n\n)\n\n\n\nself.next(self.train)\n\n\n\n@step\n\ndef train(self):\n\nfrom sklearn.linear_model import LinearRegression\n\n\n\n# trains our model\n\nself.model = LinearRegression(\n\nfit_intercept=self.settings.configuration.model.fit_intercept\n\n).fit(\n\nX=self.fe_df[self.settings.configuration.features],\n\ny=self.fe_df[self.settings.configuration.target_column]\n\n)\n\nprint(f\"Fit slope: {self.model.coef_[0]}\")\n\nprint(f\"Fit intercept: {self.model.intercept_}\")\n\n\n\nself.next(self.end)\n\n\n\n@step\n\ndef end(self):\n\npass\n\n\n\n\n\nif __name__ == \"__main__\":\n\nDemoTraining()\n\nThe Metaflow Config is added to the FlowSpec by mixing in the MetaboostSettings class. Referencing a configuration value is as easy as using the dot syntax to drill into whichever parameter you’d like.\n\nFinally let’s take a look at the output from our sample Metaflow above. We execute experiment EXP_01 with\n\nmetaboost metaflow run --binding=EXP_01\n\nwhich upon execution will merge the configurations into a single settings file (shown previously) and serialize it as a yaml file to the .metaboost/settings/compiled/ directory.\n\nYou can see the actual command and args that were sub-processed in the Metaboost Execution section below. Please note the –config argument pointing to the serialized yaml file, and then subsequently accessible via self.settings. Also note the convenient printing of configuration values to stdout during the start step using a mixed in function named show_settings().\n\n(venv-demo) ~/projects/metaboost-demo [branch=demoX]\n\n$ metaboost metaflow run --binding=EXP_01\n\n\n\nMetaboost Execution:\n\n- python3.10 /root/repos/cdm-metaboost-irl/metaflows/training.py\n\n--no-pylint --package-suffixes=.py --environment=conda\n\n--config settings\n\n.metaboost/settings/compiled/settings.branch_demox.EXP_01.training.mP4eIStG.yaml\n\nrun --prediction_date20241006\n\n\n\nMetaflow 2.12.39+nflxfastdata(2.13.5);nflx(2.13.5);metaboost(0.0.27)\n\nexecuting DemoTraining for user:dcasler\n\nValidating your flow...\n\nThe graph looks good!\n\nBootstrapping Conda environment... (this could take a few minutes)\n\nAll packages already cached in s3.\n\nAll environments already cached in s3.\n\n\n\nWorkflow starting (run-id 50), see it in the UI at\n\nhttps://metaflowui.prod.netflix.net/DemoTraining/50\n\n\n\n[50/start/251640833] Task is starting.\n\n[50/start/251640833] Configuration Values:\n\n[50/start/251640833] settings.configuration.conda.numpy = 1.22.4\n\n[50/start/251640833] settings.configuration.features.0 = runtime\n\n[50/start/251640833] settings.configuration.features.1 = content_type\n\n[50/start/251640833] settings.configuration.features.2 = top_billed_talent\n\n[50/start/251640833] settings.configuration.model.fit_intercept = True\n\n[50/start/251640833] settings.configuration.target_column = metricA\n\n[50/start/251640833] settings.environment.READ_DATABASE = data_warehouse_prod\n\n[50/start/251640833] settings.environment.TARGET_DATABASE = demo_dev\n\n[50/start/251640833] Task finished successfully.\n\n\n\n[50/get_features/251640840] Task is starting.\n\n[50/get_features/251640840] Task finished successfully.\n\n\n\n[50/train/251640854] Task is starting.\n\n[50/train/251640854] Fit slope: 0.4702672504331096\n\n[50/train/251640854] Fit intercept: -6.247919678070083\n\n[50/train/251640854] Task finished successfully.\n\n\n\n[50/end/251640868] Task is starting.\n\n[50/end/251640868] Task finished successfully.\n\n\n\nDone! See the run in the UI at\n\nhttps://metaflowui.prod.netflix.net/DemoTraining/50\n\nTakeaways\n\nMetaboost is an integration tool that aims to ease the project development, management and execution burden of ML projects at Netflix. It employs a configuration system that combines git based parameters, global configurations and arbitrarily bound configuration files for use during execution against internal Netflix platforms.\n\nIntegrating this configuration system with the new Config in Metaflow is incredibly simple (by design), only requiring users to add a mix-in class to their FlowSpec — similar to this example in Metaflow documentation — and then reference the configuration values in steps or decorators. The example above templatizes a training Metaflow for the sake of experimentation, but users could just as easily use bindings/configs to templatize their flows across target metrics, business initiatives or any other arbitrary lines of work.\n\nTry it at home\n\nIt couldn’t be easier to get started with Configs! Just\n\npip install -U metaflow\n\nto get the latest version and head to the updated documentation for examples. If you are impatient, you can find and execute all config-related examples in this repository as well.\n\nIf you have any questions or feedback about Config (or other Metaflow features), you can reach out to us at the Metaflow community Slack.\n\nAcknowledgments\n\nWe would like to thank Outerbounds for their collaboration on this feature; for rigorously testing it and developing a repository of examples to showcase some of the possibilities offered by this feature.", "label": "non_personal"}
{"title": "Part 2: A Survey of Analytics Engineering Work at Netflix", "url": "https://netflixtechblog.com/part-2-a-survey-of-analytics-engineering-work-at-netflix-4f1f53b4ab0f?source=collection_home---4------6-----------------------", "content": "Part 2: A Survey of Analytics Engineering Work at Netflix Netflix Technology Blog 9 min read · Jan 2, 2025 -- 9 Listen Share\n\nThis article is the second in a multi-part series sharing a breadth of Analytics Engineering work at Netflix, recently presented as part of our annual internal Analytics Engineering conference. Need to catch up? Check out Part 1. In this article, we highlight a few exciting analytic business applications, and in our final article we’ll go into aspects of the technical craft.\n\nGame Analytics\n\nYimeng Tang, Claire Willeck, Sagar Palao\n\nUser Acquisition Incrementality for Netflix Games\n\nNetflix has been launching games for the past three years, during which it has initiated various marketing efforts, including User Acquisition (UA) campaigns, to promote these games across different countries. These UA campaigns typically feature static creatives, launch trailers, and game review videos on platforms like Google, Meta, and TikTok. The primary goals of these campaigns are to encourage more people to install and play the games, making incremental installs and engagement crucial metrics for evaluating their effectiveness.\n\nMost UA campaigns are conducted at the country level, meaning that everyone in the targeted countries can see the ads. However, due to the absence of a control group in these countries, we adopt a synthetic control framework (blog post) to estimate the counterfactual scenario. This involves creating a weighted combination of countries not exposed to the UA campaign to serve as a counterfactual for the treated countries. To facilitate easier access to incrementality results, we have developed an interactive tool powered by this framework. This tool allows users to directly obtain the lift in game installs and engagement, view plots for both the treated country and the synthetic control unit, and assess the p-value from placebo tests.\n\nTo better guide the design and budgeting of future campaigns, we are developing an Incremental Return on Investment model. This model incorporates factors such as the incremental impact, the value of the incremental engagement and incremental signups, and the cost of running the campaign. In addition to using the causal inference framework mentioned earlier to estimate incrementality, we also leverage other frameworks, such as Incremental Account Lifetime Valuation (blog post), to assign value to the incremental engagement and signups resulting from the campaigns.\n\nMeasuring and Validating Incremental Signups for Netflix Games\n\nNetflix is a subscription service meaning members buy subscriptions which include games but not the individual games themselves. This makes it difficult to measure the impact of different game launches on acquisition. We only observe signups, not why members signed up.\n\nThis means we need to estimate incremental signups. We adopt an approach developed at Netflix to estimate incremental acquisition (technical paper). This approach uses simple assumptions to estimate a counterfactual for the rate that new members start playing the game.\n\nBecause games differ from series/films, it’s crucial to validate this estimation method for games. Ideally, we would have causal estimates from an A/B test to use for validation, but since that is not available, we use another causal inference design as one of our ensemble of validation approaches. This causal inference design involves a systematic framework we designed to measure game events that relies on synthetic control (blog post).\n\nAs we mentioned above, we have been launching User Acquisition (UA) campaigns in select countries to boost game engagement and new memberships. We can use this cross-country variation to form a synthetic control and measure the incremental signups due to the UA campaign. The incremental signups from UA campaigns differ from those attributed to a game, but they should be similar. When our estimated incremental acquisition numbers over a campaign period are similar to the incremental acquisition numbers calculated using synthetic control, we feel more confident in our approach to measuring incremental signups for games.\n\nNetflix Games Players’ Adventure: Modeled using State Machine\n\nAt Netflix Games, we aim to have a high number of members engaging with games each month, referred to as Monthly Active Accounts (MAA). To evaluate our progress toward this objective and to find areas to boost our MAA, we modeled the Netflix players’ journey as a state machine.\n\nWe track a daily state machine showing the probability of account transitions between states.\n\nNetflix Players’ Journey as State machine\n\nModeling the players’ journey as a state machine allows us to simulate future states and assess progress toward engagement goals. The most basic operation involves multiplying the daily state-transition matrix with the current state values to determine the next day’s state values.\n\nThis basic operation allows us to explore various scenarios:\n\nConstant Trends: If transition rates stay constant, we can predict future states by repeatedly multiplying the daily state-transition matrix to new state values, helping us assess progress towards annual goals under unchanged conditions.\n\nDynamic Scenarios: By modifying transition rates, we can simulate complex scenarios. For instance, mimicking past changes in transition rates from a game launch allows us to predict the impact of similar future launches by altering the transition rate for a specific period.\n\nSteady State: We can calculate the steady state of the state-transition matrix (excluding new players) to estimate the MAA once all accounts have tried Netflix games and understand long-term retention and reactivation effects.\n\nBeyond predicting future states, we use the state machine for sensitivity analysis to find which transition rates most impact MAA. By making small changes to each transition rate we calculate the resulting MAA and measure its impact. This guides us in prioritizing efforts on top-of-funnel improvements, member retention, or reactivation.\n\nContent Cash Modeling\n\nAlex Diamond\n\nAt Netflix we produce a variety of entertainment: movies, series, documentaries, stand-up specials, and more. Each format has a different production process and different patterns of cash spend, called our “Content Forecast”. Looking into the future, Netflix keeps a plan of how many titles we intend to produce, what kinds, and when. Because we don’t yet know what specific titles that content will eventually become, these generic placeholders are called “TBD Slots.” A sizable portion of our Content Forecast is represented by TBD Slots.\n\nAlmost all businesses have a cash forecasting process informing how much cash they need in a given time period to continue executing on their plans. As plans change, the cash forecast will change. Netflix has a cash forecast that projects our cash needs to produce the titles we plan to make. This presents the question: how can we optimally forecast cash needs for TBD Slots, given we don’t have details on what real titles they will become?\n\nThe large majority of our titles are funded throughout the production process — starting from when we begin developing the title to shooting the actual shows and movies to launch on our Netflix service.\n\nSince cash spend is driven by what is happening on a production, we model it by breaking down into these three steps:\n\nDetermine estimated production phase durations using historical actuals Determine estimated percent of cash spent in each production phase Model the shape of cash spend within each phase\n\nPutting these three pieces together allows us to generate a generic estimation of cash spend per day leading up to and beyond a title’s launch date (a proxy for “completion”). We could distribute this spend linearly across each phase, but this approach allows us to capture nuance around patterns of spend that ramp up slowly, or are concentrated at the start and taper off throughout.\n\nBefore starting any math, we need to ensure a high quality historical dataset. Data quality plays a huge role in this work. For example, if we see 80% of our cash spent before production even started, it might be safe to say that either the production dates (which are manually captured) are incorrect or that title had a unique spending pattern that we don’t want to anticipate our future titles will follow.\n\nFor the first two steps, finding the estimated phase durations and cash percent per phase, we’ve found that simple math works best, for interpretability and consistency. We use a weighted average across our “clean” historical actuals to produce these estimated assumptions.\n\nFor modeling the shape of spend throughout each phase, we perform constrained optimization to fit a 3rd degree polynomial function. The constraints include:\n\nMust pass through the points (0,0) and (1,1). This ensures that 0% through the phase, 0% of that phase’s cash has been spent. Similarly, 100% through the phase, 100% of that phase’s cash has been spent. The derivative must be non-negative. This ensures that the function is monotonically increasing, avoiding counterintuitively forecasting any negative spend.\n\nThe optimization’s objective function minimizes the sum of squared residuals and returns the coefficients of the polynomial that will guide the shape of cash spend through each phase.\n\nOnce we have these coefficients, we can evaluate this polynomial at each day of the expected phase duration, and then multiply the result by the expected cash per phase. With some additional data processing, this yields an expected percent of cash spend each day leading up to and beyond the launch date, which we can base our forecasts on.\n\nAssistive Speech Recognition in Dubbing Workflows at Netflix\n\nTanguy Cornau\n\nGreat stories can come from anywhere and be loved everywhere. At Netflix, we strive to make our titles accessible to a global audience, transcending language barriers to connect with viewers worldwide. One of the key ways we achieve this is through creating dubs in many languages.\n\nFrom the transcription of the original titles all the way to the delivery of the dub audio, we blend innovation with human expertise to preserve the original creative intent.\n\nLeveraging technologies like Assistive Speech Recognition (ASR), we seek to make the transcription part of the process more efficient for our linguists. Transcription, in our context, involves creating a verbatim script of the spoken dialogue, along with precise timing information to perfectly align the text with the original video. With ASR, instead of starting the transcription from scratch, linguists get a pre-generated starting point which they can use and edit for complete accuracy.\n\nThis efficiency enables linguists to focus more on other creative tasks, such as adding cultural annotations and references, which are crucial for downstream dubbing.\n\nWith ASR, and other new and enhanced technologies we introduce, rigorous analytics and measurement are essential to their success. To effectively evaluate our ASR system, we’ve established a multi-layered measurement framework that provides comprehensive insights into its performance across many dimensions (for example, the accuracy of the text and timing predictions), offline and online.\n\nASR is expected to perform differently for various languages; therefore, at a high level, we track metrics by original language of the show, allowing us to assess overall ASR effectiveness and identify trends across different linguistic contexts. We further break down performance by various dimensions, e.g. content type, genre, etc… to help us pinpoint specific areas where the ASR system may encounter difficulties. Furthermore, our framework allows us to conduct in-depth analyses of individual titles’ transcription, focusing on critical quality dimensions around text and timing accuracy of ASR suggestions. By zooming in on where the system falls short, we gain valuable insights into specific challenges, enabling us to further refine our understanding of ASR performance.\n\nThese measurement layers collectively empower us to continuously monitor, identify improvement areas, and implement targeted enhancements, ensuring that our ASR technology gets more and more accurate, effective, and helpful to linguists across diverse content types and languages. By refining our dubbing workflows through these innovations, we aim to keep improving the quality of our dubs to help great stories travel across the globe and bring joy to our members.", "label": "non_personal"}
{"title": "Part 3: A Survey of Analytics Engineering Work at Netflix", "url": "https://netflixtechblog.com/part-3-a-survey-of-analytics-engineering-work-at-netflix-e67f0aa82183?source=collection_home---4------5-----------------------", "content": "Part 3: A Survey of Analytics Engineering Work at Netflix Netflix Technology Blog 9 min read · Jan 6, 2025 -- 3 Listen Share\n\nThis article is the last in a multi-part series sharing a breadth of Analytics Engineering work at Netflix, recently presented as part of our annual internal Analytics Engineering conference. Need to catch up? Check out Part 1, which detailed how we’re empowering Netflix to efficiently produce and effectively deliver high quality, actionable analytic insights across the company and Part 2, which stepped through a few exciting business applications for Analytics Engineering. This post will go into aspects of technical craft.\n\nDashboard Design Tips\n\nRina Chang, Susie Lu\n\nWhat is design, and why does it matter? Often people think design is about how things look, but design is actually about how things work. Everything is designed, because we’re all making choices about how things work, but not everything is designed well. Good design doesn’t waste time or mental energy; instead, it helps the user achieve their goals.\n\nWhen applying this to a dashboard application, the easiest way to use design effectively is to leverage existing patterns. (For example, people have learned that blue underlined text on a website means it’s a clickable link.) So knowing the arsenal of available patterns and what they imply is useful when making the choice of when to use which pattern.\n\nFirst, to design a dashboard well, you need to understand your user.\n\nTalk to your users throughout the entire product lifecycle. Talk to them early and often, through whatever means you can.\n\nUnderstand their needs, ask why, then ask why again. Separate symptoms from problems from solutions.\n\nPrioritize and clarify — less is more! Distill what you can build that’s differentiated and provides the most value to your user.\n\nHere is a framework for thinking about what your users are trying to achieve. Where do your users fall on these axes? Don’t solve for multiple positions across these axes in a given view; if that exists, then create different views or potentially different dashboards.\n\nSecond, understanding your users’ mental models will allow you to choose how to structure your app to match. A few questions to ask yourself when considering the information architecture of your app include:\n\nDo you have different user groups trying to accomplish different things? Split them into different apps or different views.\n\nWhat should go together on a single page? All the information needed for a single user type to accomplish their “job.” If there are multiple jobs to be done, split each out onto its own page.\n\nWhat should go together within a single section on a page? All the information needed to answer a single question.\n\nDoes your dashboard feel too difficult to use? You probably have too much information! When in doubt, keep it simple. If needed, hide complexity under an “Advanced” section.\n\nHere are some general guidelines for page layouts:\n\nChoose infinite scrolling vs. clicking through multiple pages depending on which option suits your users’ expectations better\n\nLead with the most-used information first, above the fold\n\nCreate signposts that cue the user to where they are by labeling pages, sections, and links\n\nUse cards or borders to visually group related items together\n\nLeverage nesting to create well-understood “scopes of control.” Specifically, users expect a controller object to affect children either: Below it (if horizontal) or To the right of it (if vertical)\n\nThird, some tips and tricks can help you more easily tackle the unique design challenges that come with making interactive charts.\n\nTitles: Make sure filters are represented in the title or subtitle of the chart for easy scannability and screenshot-ability.\n\nTooltips: Core details should be on the page, while the context in the tooltip is for deeper information. Annotate multiple points when there are only a handful of lines.\n\nAnnotations: Provide annotations on charts to explain shifts in values so all users can access that context.\n\nColor: Limit the number of colors you use. Be consistent in how you use colors. Otherwise, colors lose meaning.\n\nOnboarding: Separate out onboarding to your dashboard from routine usage.\n\nFinally, it is important to note that these are general guidelines, but there is always room for interpretation and/or the use of good judgment to adapt them to suit your own product and use cases. At the end of the day, the most important thing is that a user can leverage the data insights provided by your dashboard to perform their work, and good design is a means to that end.\n\nLearnings from Deploying an Analytics API at Netflix\n\nDevin Carullo\n\nAt Netflix Studio, we operate at the intersection of art and science. Data is a tool that enhances decision-making, complementing the deep expertise and industry knowledge of our creative professionals.\n\nOne example is in production budgeting — namely, determining how much we should spend to produce a given show or movie. Although there was already a process for creating and comparing budgets for new productions against similar past projects, it was highly manual. We developed a tool that automatically selects and compares similar Netflix productions, flagging any anomalies for Production Finance to review.\n\nTo ensure success, it was essential that results be delivered in real-time and integrated seamlessly into existing tools. This required close collaboration among product teams, DSE, and front-end and back-end developers. We developed a GraphQL endpoint using Metaflow, integrating it into the existing budgeting product. This solution enabled data to be used more effectively for real-time decision-making.\n\nWe recently launched our MVP and continue to iterate on the product. Reflecting on our journey, the path to launch was complex and filled with unexpected challenges. As an analytics engineer accustomed to crafting quick solutions, I underestimated the effort required to deploy a production-grade analytics API.\n\nFig 1. My vague idea of how my API would work\n\nFig 2: Our actual solution\n\nWith hindsight, below are my key learnings.\n\nMeasure Impact and Necessity of Real-Time Results\n\nBefore implementing real-time analytics, assess whether real-time results are truly necessary for your use case. This can significantly impact the complexity and cost of your solution. Batch processing data may provide a similar impact and take significantly less time. It’s easier to develop and maintain, and tends to be more familiar for analytics engineers, data scientists, and data engineers.\n\nAdditionally, if you are developing a proof of concept, the upfront investment may not be worth it. Scrappy solutions can often be the best choice for analytics work.\n\nExplore All Available Solutions\n\nAt Netflix, there were multiple established methods for creating an API, but none perfectly suited our specific use case. Metaflow, a tool developed at Netflix for data science projects, already supported REST APIs. However, this approach did not align with the preferred workflow of our engineering partners. Although they could integrate with REST endpoints, this solution presented inherent limitations. Large response sizes rendered the API/front-end integration unreliable, necessitating the addition of filter parameters to reduce the response size.\n\nAdditionally, the product we were integrating into was using GraphQL, and deviating from this established engineering approach was not ideal. Lastly, given our goal to overlay results throughout the product, GraphQL features, such as federation, proved to be particularly advantageous.\n\nAfter realizing there wasn’t an existing solution at Netflix for deploying python endpoints with GraphQL, we worked with the Metaflow team to build this feature. This allowed us to continue developing via Metaflow and allowed our engineering partners to stay on their paved path.\n\nAlign on Performance Expectations\n\nA major challenge during development was managing API latency. Much of this could have been mitigated by aligning on performance expectations from the outset. Initially, we operated under our assumptions of what constituted an acceptable response time, which differed greatly from the actual needs of our users and our engineering partners.\n\nUnderstanding user expectations is key to designing an effective solution. Our methodology resulted in a full budget analysis taking, on average, 7 seconds. Users were willing to wait for an analysis when they modified a budget, but not every time they accessed one. To address this, we implemented caching using Metaflow, reducing the API response time to approximately 1 second for cached results. Additionally, we set up a nightly batch job to pre-cache results.\n\nWhile users were generally okay with waiting for analysis during changes, we had to be mindful of GraphQL’s 30-second limit. This highlighted the importance of continuously monitoring the impact of changes on response times, leading us to our next key learning: rigorous testing.\n\nReal-Time Analysis Requires Rigorous Testing\n\nLoad Testing: We leveraged Locust to measure the response time of our endpoint and assess how the endpoint responded to reasonable and elevated loads. We were able to use FullStory, which was already being used in the product, to estimate expected calls per minute.\n\nFig 3. Locust allows us to simulate concurrent calls and measure response time\n\nUnit Tests & Integration Tests: Code testing is always a good idea, but it can often be overlooked in analytics. It is especially important when you are delivering live analysis to circumvent end users from being the first to see an error or incorrect information. We implemented unit testing and full integration tests, ensuring that our analysis would return correct results.\n\nThe Importance of Aligning Workflows and Collaboration\n\nThis project marked the first time our team collaborated directly with our engineering partners to integrate a DSE API into their product. Throughout the process, we discovered significant gaps in our understanding of each other’s workflows. Assumptions about each other’s knowledge and processes led to misunderstandings and delays.\n\nDeployment Paths: Our engineering partners followed a strict deployment path, whereas our approach on the DSE side was more flexible. We typically tested our work on feature branches using Metaflow projects and then pushed results to production. However, this lack of control led to issues, such as inadvertently deploying changes to production before the corresponding product updates were ready and difficulties in managing a test endpoint. Ultimately, we deferred to our engineering partners to establish a deployment path and collaborated with the Metaflow team and data engineers to implement it effectively.\n\nFig 4. Our current deployment path\n\nWork Planning: While the engineering team operated on sprints, our DSE team planned by quarters. This misalignment in planning cycles is an ongoing challenge that we are actively working to resolve.\n\nLooking ahead, our team is committed to continuing this partnership with our engineering colleagues. Both teams have invested significant time in building this relationship, and we are optimistic that it will yield substantial benefits in future projects.\n\nExternal Speaker: Benn Stancil\n\nIn addition to the above presentations, we kicked off our Analytics Summit with a keynote talk from Benn Stancil, Founder of Mode Analytics. Benn stepped through a history of the modern data stack, and the group discussed ideas on the future of analytics.", "label": "non_personal"}
{"title": "Title Launch Observability at Netflix Scale", "url": "https://netflixtechblog.com/title-launch-observability-at-netflix-scale-19ea916be1ed?source=collection_home---4------4-----------------------", "content": "Title Launch Observability at Netflix Scale\n\nPart 2: Navigating Ambiguity Netflix Technology Blog 6 min read · Jan 7, 2025 -- 8 Listen Share\n\nBy: Varun Khaitan\n\nWith special thanks to my stunning colleagues: Mallika Rao, Esmir Mesic, Hugo Marques\n\nBuilding on the foundation laid in Part 1, where we explored the “what” behind the challenges of title launch observability at Netflix, this post shifts focus to the “how.” How do we ensure every title launches seamlessly and remains discoverable by the right audience?\n\nIn the dynamic world of technology, it’s tempting to leap into problem-solving mode. But the key to lasting success lies in taking a step back — understanding the broader context before diving into solutions. This thoughtful approach doesn’t just address immediate hurdles; it builds the resilience and scalability needed for the future. Let’s explore how this mindset drives results.\n\nUnderstanding the Bigger Picture\n\nLet’s take a comprehensive look at all the elements involved and how they interconnect. We should aim to address questions such as: What is vital to the business? Which aspects of the problem are essential to resolve? And how did we arrive at this point?\n\nThis process involves:\n\nIdentifying Stakeholders: Determine who is impacted by the issue and whose input is crucial for a successful resolution. In this case, the main stakeholders are:\n\n\n\n- Title Launch Operators\n\nRole: Responsible for setting up the title and its metadata into our systems.\n\nChallenge: Don’t understand the cascading effects of their setup on these perceived black box personalization systems\n\n\n\n- Personalization System Engineers\n\nRole: Develop and operate the personalization systems.\n\nChallenge: End up spending unplanned cycles on title launch and personalization investigations.\n\n\n\n- Product Managers\n\nRole: Ensure we put forward the best experience for our members.\n\nChallenge: Members may not connect with the most relevant title.\n\n\n\n- Creative Representatives\n\nRole: Mediator between the content creators and Netflix.\n\nChallenge: Build trust in the Netflix brand with content creators. Mapping the Current Landscape: By charting the existing landscape, we can pinpoint areas ripe for improvement and steer clear of redundant efforts. Beyond the scattered solutions and makeshift scripts, it became evident that there was no established solution for title launch observability. This suggests that this area has been neglected for quite some time and likely requires significant investment. This situation presents both challenges and opportunities; while it may be more difficult to make initial progress, there are plenty of easy wins to capitalize on. Clarifying the Core Problem: By clearly defining the problem, we can ensure that our solutions address the root cause rather than just the symptoms. While there were many issues and problems we could address, the core problem here was to make sure every title was treated fairly by our personalization stack. If we can ensure fair treatment with confidence and bring that visibility to all our stakeholders, we can address all their challenges. Assessing Business Priorities: Understanding what is most important to the organization helps prioritize actions and resources effectively. In this context, we’re focused on developing systems that ensure successful title launches, build trust between content creators and our brand, and reduce engineering operational overhead. While this is a critical business need and we definitely should solve it, it’s essential to evaluate how it stacks up against other priorities across different areas of the organization.\n\nDefining Title Health\n\nNavigating such an ambiguous space required a shared understanding to foster clarity and collaboration. To address this, we introduced the term “Title Health,” a concept designed to help us communicate effectively and capture the nuances of maintaining each title’s visibility and performance. This shared language became a foundation for discussing the complexities of this domain.\n\n“Title Health” encompasses various metrics and indicators that reflect how well a title is performing, in terms of discoverability and member engagement. The three main questions we try to answer are:\n\nIs this title visible at all to any member? Is this title visible to an appropriate audience size? Is this title reaching all the appropriate audiences?\n\nDefining Title Health provided a framework to monitor and optimize each title’s lifecycle. It allowed us to align with partners on principles and requirements before building solutions, ensuring every title reaches its intended audience seamlessly. This common language not only introduced the problem space effectively but also accelerated collaboration and decision-making across teams.\n\nCategories of issues\n\nTo build a robust plan for title launch observability, we first needed to categorize the types of issues we encounter. This structured approach allows us to address all aspects of title health comprehensively.\n\nCurrently, these issues are grouped into three primary categories:\n\n1. Title Setup\n\nA title’s setup includes essential attributes like metadata (e.g., launch dates, audio and subtitle languages, editorial tags) and assets (e.g., artwork, trailers, supplemental messages). These elements are critical for a title’s eligibility in a row, accurate personalization, and an engaging presentation. Since these attributes feed directly into algorithms, any delays or inaccuracies can ripple through the system.\n\nThe observability system must ensure that title setup is complete and validated in a timely manner, identify potential bottlenecks and ensure a smooth launch process.\n\n2. Personalization Systems\n\nTitles are eligible to be recommended across multiple canvases on product — HomePage, Coming Soon, Messaging, Search and more. Personalization systems handle the recommendation and serving of titles on these canvases, leveraging a vast ecosystem of microservices, caches, databases, code, and configurations to build these product canvases.\n\nWe aim to validate that titles are eligible in all appropriate product canvases across the end to end personalization stack during all of the title’s launch phases.\n\n3. Algorithms\n\nComplex algorithms drive each personalized product experience, recommending titles tailored to individual members. Observability here means validating the accuracy of algorithmic recommendations for all titles.\n\nAlgorithmic performance can be affected by various factors, such as model shortcomings, incomplete or inaccurate input signals, feature anomalies, or interactions between titles. Identifying and addressing these issues ensures that recommendations remain precise and effective.\n\nBy categorizing issues into these areas, we can systematically address challenges and deliver a reliable, personalized experience for every title on our platform.\n\nIssue Analysis\n\nLet’s also learn more about how often we see each of these types of issues and how much effort it takes to fix them once they come up.\n\nFrom the above chart, we see that setup issues are the most common but they are also easy to fix since it’s relatively straightforward to go back and rectify a title’s metadata. System issues, which mostly manifest as bugs in our personalization microservices are not uncommon, and they take moderate effort to address. Algorithm issues, while rare, are really difficult to address since these often involve interpreting and retraining complex machine learning models.\n\nEvaluating Our Options\n\nNow that we understand more deeply about the problems we want to address and how we should go about prioritizing our resources. Lets go back to the two options we discussed in Part 1, and make an informed decision.\n\nUltimately, we realized this space demands the full spectrum of features we’ve discussed. But the question remained: Where do we start?\n\nAfter careful consideration, we chose to focus on proactive issue detection first. Catching problems before launch offered the greatest potential for business impact, ensuring smoother launches, better member experiences, and stronger system reliability.\n\nThis decision wasn’t just about solving today’s challenges — it was about laying the foundation for a scalable, robust system that can grow with the complexities of our ever-evolving platform.\n\nUp next\n\nIn the next iteration we will talk about how to design an observability endpoint that works for all personalization systems. What are the main things to keep in mind while creating a microservice API endpoint? How do we ensure standardization? What is the architecture of the systems involved?\n\nKeep an eye out for our next binge-worthy episode!", "label": "non_personal"}
{"title": "Title Launch Observability at Netflix Scale", "url": "https://netflixtechblog.com/title-launch-observability-at-netflix-scale-8efe69ebd653?source=collection_home---4------2-----------------------", "content": "Title Launch Observability at Netflix Scale\n\nPart 3: System Strategies and Architecture Netflix Technology Blog 7 min read · Mar 5, 2025 -- 6 Listen Share\n\nBy: Varun Khaitan\n\nWith special thanks to my stunning colleagues: Mallika Rao, Esmir Mesic, Hugo Marques\n\nThis blog post is a continuation of Part 2, where we cleared the ambiguity around title launch observability at Netflix. In this installment, we will explore the strategies, tools, and methodologies that were employed to achieve comprehensive title observability at scale.\n\nDefining the observability endpoint\n\nTo create a comprehensive solution, we decided to introduce observability endpoints first. Each microservice involved in our Personalization stack that integrated with our observability solution had to introduce a new “Title Health” endpoint. Our goal was for each new endpoint to adhere to a few principles:\n\nAccurate reflection of production behavior Standardization across all endpoints Answering the Insight Triad: “Healthy” or not, why not and how to fix it.\n\nAccurately Reflecting Production Behavior\n\nA key part of our solution is insights into production behavior, which necessitates our requests to the endpoint result in traffic to the real service functions that mimics the same pathways the traffic would take if it came from the usual callers.\n\nIn order to allow for this mimicking, many systems implement an “event” handling, where they convert our request into a call to the real service with properties enabled to log when titles are filtered out of their response and why. Building services that adhere to software best practices, such as Object-Oriented Programming (OOP), the SOLID principles, and modularization, is crucial to have success at this stage. Without these practices, service endpoints may become tightly coupled to business logic, making it challenging and costly to add a new endpoint that seamlessly integrates with the observability solution while following the same production logic.\n\nA service with modular business logic facilitates the seamless addition of an observability endpoint.\n\nStandardization\n\nTo standardize communication between our observability service and the personalization stack’s observability endpoints, we’ve developed a stable proto request/response format. This centralized format, defined and maintained by our team, ensures all endpoints adhere to a consistent protocol. As a result, requests are uniformly handled, and responses are processed cohesively. This standardization enhances adoption within the personalization stack, simplifies the system, and improves understanding and debuggability for engineers.\n\nThe request schema for the observability endpoint.\n\nThe Insight Triad API\n\nTo efficiently understand the health of a title and triage issues quickly, all implementations of the observability endpoint must answer: is the title eligible for this phase of promotion, if not — why is it not eligible, and what can be done to fix any problems.\n\nThe end-users of this observability system are Launch Managers, whose job it is to ensure smooth title launches. As such, they must be able to quickly see whether there is a problem, what the problem is, and how to solve it. Teams implementing the endpoint must provide as much information as possible so that a non-engineer (Launch Manager) can understand the root cause of the issue and fix any title setup issues as they arise. They must also provide enough information for partner engineers to identify the problem with the underlying service in cases of system-level issues.\n\nThese requirements are captured in the following protobuf object that defines the endpoint response.\n\nThe response schema for the observability endpoint.\n\nHigh level architecture\n\nWe’ve distilled our comprehensive solution into the following key steps, capturing the essence of our approach:\n\nEstablish observability endpoints across all services within our Personalization and Discovery Stack. Implement proactive monitoring for each of these endpoints. Track real-time title impressions from the Netflix UI. Store the data in an optimized, highly distributed datastore. Offer easy-to-integrate APIs for our dashboard, enabling stakeholders to track specific titles effectively. “Time Travel” to validate ahead of time.\n\nObservability stack high level architecture diagram\n\nIn the following sections, we will explore each of these concepts and components as illustrated in the diagram above.\n\nKey Features\n\nProactive monitoring through scheduled collectors jobs\n\nOur Title Health microservice runs a scheduled collector job every 30 minutes for most of our personalization stack.\n\nFor each Netflix row we support (such as Trending Now, Coming Soon, etc.), there is a dedicated collector. These collectors retrieve the relevant list of titles from our catalog that qualify for a specific row by interfacing with our catalog services. These services are informed about the expected subset of titles for each row, for which we are assessing title health.\n\nOnce a collector retrieves its list of candidate titles, it orchestrates batched calls to assigned row services using the above standardized schema to retrieve all the relevant health information of the titles. Additionally, some collectors will instead poll our kafka queue for impressions data.\n\nReal-time Title Impressions and Kafka Queue\n\nIn addition to evaluating title health via our personalization stack services, we also keep an eye on how our recommendation algorithms treat titles by reviewing impressions data. It’s essential that our algorithms treat all titles equitably, for each one has limitless potential.\n\nThis data is processed from a real-time impressions stream into a Kafka queue, which our title health system regularly polls. Specialized collectors access the Kafka queue every two minutes to retrieve impressions data. This data is then aggregated in minute(s) intervals, calculating the number of impressions titles receive in near-real-time, and presented as an additional health status indicator for stakeholders.\n\nData storage and distribution through Hollow Feeds\n\nNetflix Hollow is an Open Source java library and toolset for disseminating in-memory datasets from a single producer to many consumers for high performance read-only access. Given the shape of our data, hollow feeds are an excellent strategy to distribute the data across our service boxes.\n\nOnce collectors gather health data from partner services in the personalization stack or from our impressions stream, this data is stored in a dedicated Hollow feed for each collector. Hollow offers numerous features that help us monitor the overall health of a Netflix row, including ensuring there are no large-scale issues across a feed publish. It also allows us to track the history of each title by maintaining a per-title data history, calculate differences between previous and current data versions, and roll back to earlier versions if a problematic data change is detected.\n\nObservability Dashboard using Health Check Engine\n\nWe maintain several dashboards that utilize our title health service to present the status of titles to stakeholders. These user interfaces access an endpoint in our service, enabling them to request the current status of a title across all supported rows. This endpoint efficiently reads from all available Hollow Feeds to obtain the current status, thanks to Hollow’s in-memory capabilities. The results are returned in a standardized format, ensuring easy support for future UIs.\n\nAdditionally, we have other endpoints that can summarize the health of a title across subsets of sections to highlight specific member experiences.\n\nMessage depicting a dashboard request.\n\nTime Traveling: Catching before launch\n\nTitles launching at Netflix go through several phases of pre-promotion before ultimately launching on our platform. For each of these phases, the first several hours of promotion are critical for the reach and effective personalization of a title, especially once the title has launched. Thus, to prevent issues as titles go through the launch lifecycle, our observability system needs to be capable of simulating traffic ahead of time so that relevant teams can catch and fix issues before they impact members. We call this capability “Time Travel”.\n\nMany of the metadata and assets involved in title setup have specific timelines for when they become available to members. To determine if a title will be viewable at the start of an experience, we must simulate a request to a partner service as if it were from a future time when those specific metadata or assets are available. This is achieved by including a future timestamp in our request to the observability endpoint, corresponding to when the title is expected to appear for a given experience. The endpoint then communicates with any further downstream services using the context of that future timestamp.\n\nAn example request with a future timestamp.\n\nConclusion\n\nThroughout this series, we’ve explored the journey of enhancing title launch observability at Netflix. In Part 1, we identified the challenges of managing vast content launches and the need for scalable solutions to ensure each title’s success. Part 2 highlighted the strategic approach to navigating ambiguity, introducing “Title Health” as a framework to align teams and prioritize core issues. In this final part, we detailed the sophisticated system strategies and architecture, including observability endpoints, proactive monitoring, and “Time Travel” capabilities; all designed to ensure a thrilling viewing experience.\n\nBy investing in these innovative solutions, we enhance the discoverability and success of each title, fostering trust with content creators and partners. This journey not only bolsters our operational capabilities but also lays the groundwork for future innovations, ensuring that every story reaches its intended audience and that every member enjoys their favorite titles on Netflix.\n\nThank you for joining us on this exploration, and stay tuned for more insights and innovations as we continue to entertain the world.", "label": "non_personal"}
{"title": "Foundation Model for Personalized Recommendation", "url": "https://netflixtechblog.com/foundation-model-for-personalized-recommendation-1a0bd8e02d39?source=collection_home---4------1-----------------------", "content": "Foundation Model for Personalized Recommendation Netflix Technology Blog 11 min read · Mar 21, 2025 -- 36 Listen Share\n\nBy Ko-Jen Hsiao, Yesu Feng and Sudarshan Lamkhede\n\nMotivation\n\nNetflix’s personalized recommender system is a complex system, boasting a variety of specialized machine learned models each catering to distinct needs including “Continue Watching” and “Today’s Top Picks for You.” (Refer to our recent overview for more details). However, as we expanded our set of personalization algorithms to meet increasing business needs, maintenance of the recommender system became quite costly. Furthermore, it was difficult to transfer innovations from one model to another, given that most are independently trained despite using common data sources. This scenario underscored the need for a new recommender system architecture where member preference learning is centralized, enhancing accessibility and utility across different models.\n\nParticularly, these models predominantly extract features from members’ recent interaction histories on the platform. Yet, many are confined to a brief temporal window due to constraints in serving latency or training costs. This limitation has inspired us to develop a foundation model for recommendation. This model aims to assimilate information both from members’ comprehensive interaction histories and our content at a very large scale. It facilitates the distribution of these learnings to other models, either through shared model weights for fine tuning or directly through embeddings.\n\nThe impetus for constructing a foundational recommendation model is based on the paradigm shift in natural language processing (NLP) to large language models (LLMs). In NLP, the trend is moving away from numerous small, specialized models towards a single, large language model that can perform a variety of tasks either directly or with minimal fine-tuning. Key insights from this shift include:\n\nA Data-Centric Approach: Shifting focus from model-centric strategies, which heavily rely on feature engineering, to a data-centric one. This approach prioritizes the accumulation of large-scale, high-quality data and, where feasible, aims for end-to-end learning. Leveraging Semi-Supervised Learning: The next-token prediction objective in LLMs has proven remarkably effective. It enables large-scale semi-supervised learning using unlabeled data while also equipping the model with a surprisingly deep understanding of world knowledge.\n\nThese insights have shaped the design of our foundation model, enabling a transition from maintaining numerous small, specialized models to building a scalable, efficient system. By scaling up semi-supervised training data and model parameters, we aim to develop a model that not only meets current needs but also adapts dynamically to evolving demands, ensuring sustainable innovation and resource efficiency.\n\nData\n\nAt Netflix, user engagement spans a wide spectrum, from casual browsing to committed movie watching. With over 300 million users at the end of 2024, this translates into hundreds of billions of interactions — an immense dataset comparable in scale to the token volume of large language models (LLMs). However, as in LLMs, the quality of data often outweighs its sheer volume. To harness this data effectively, we employ a process of interaction tokenization, ensuring meaningful events are identified and redundancies are minimized.\n\nTokenizing User Interactions: Not all raw user actions contribute equally to understanding preferences. Tokenization helps define what constitutes a meaningful “token” in a sequence. Drawing an analogy to Byte Pair Encoding (BPE) in NLP, we can think of tokenization as merging adjacent actions to form new, higher-level tokens. However, unlike language tokenization, creating these new tokens requires careful consideration of what information to retain. For instance, the total watch duration might need to be summed or engagement types aggregated to preserve critical details.\n\nFigure 1.Tokenization of user interaction history by merging actions on the same title, preserving important information.\n\nThis tradeoff between granular data and sequence compression is akin to the balance in LLMs between vocabulary size and context window. In our case, the goal is to balance the length of interaction history against the level of detail retained in individual tokens. Overly lossy tokenization risks losing valuable signals, while too granular a sequence can exceed practical limits on processing time and memory.\n\nEven with such strategies, interaction histories from active users can span thousands of events, exceeding the capacity of transformer models with standard self attention layers. In recommendation systems, context windows during inference are often limited to hundreds of events — not due to model capability but because these services typically require millisecond-level latency. This constraint is more stringent than what is typical in LLM applications, where longer inference times (seconds) are more tolerable.\n\nTo address this during training, we implement two key solutions:\n\nSparse Attention Mechanisms: By leveraging sparse attention techniques such as low-rank compression, the model can extend its context window to several hundred events while maintaining computational efficiency. This enables it to process more extensive interaction histories and derive richer insights into long-term preferences. Sliding Window Sampling: During training, we sample overlapping windows of interactions from the full sequence. This ensures the model is exposed to different segments of the user’s history over multiple epochs, allowing it to learn from the entire sequence without requiring an impractically large context window.\n\nAt inference time, when multi-step decoding is needed, we can deploy KV caching to efficiently reuse past computations and maintain low latency.\n\nThese approaches collectively allow us to balance the need for detailed, long-term interaction modeling with the practical constraints of model training and inference, enhancing both the precision and scalability of our recommendation system.\n\nInformation in Each ‘Token’: While the first part of our tokenization process focuses on structuring sequences of interactions, the next critical step is defining the rich information contained within each token. Unlike LLMs, which typically rely on a single embedding space to represent input tokens, our interaction events are packed with heterogeneous details. These include attributes of the action itself (such as locale, time, duration, and device type) as well as information about the content (such as item ID and metadata like genre and release country). Most of these features, especially categorical ones, are directly embedded within the model, embracing an end-to-end learning approach. However, certain features require special attention. For example, timestamps need additional processing to capture both absolute and relative notions of time, with absolute time being particularly important for understanding time-sensitive behaviors.\n\nTo enhance prediction accuracy in sequential recommendation systems, we organize token features into two categories:\n\nRequest-Time Features: These are features available at the moment of prediction, such as log-in time, device, or location. Post-Action Features: These are details available after an interaction has occurred, such as the specific show interacted with or the duration of the interaction.\n\nTo predict the next interaction, we combine request-time features from the current step with post-action features from the previous step. This blending of contextual and historical information ensures each token in the sequence carries a comprehensive representation, capturing both the immediate context and user behavior patterns over time.\n\nConsiderations for Model Objective and Architecture\n\nAs previously mentioned, our default approach employs the autoregressive next-token prediction objective, similar to GPT. This strategy effectively leverages the vast scale of unlabeled user interaction data. The adoption of this objective in recommendation systems has shown multiple successes [1–3]. However, given the distinct differences between language tasks and recommendation tasks, we have made several critical modifications to the objective.\n\nFirstly, during the pretraining phase of typical LLMs, such as GPT, every target token is generally treated with equal weight. In contrast, in our model, not all user interactions are of equal importance. For instance, a 5-minute trailer play should not carry the same weight as a 2-hour full movie watch. A greater challenge arises when trying to align long-term user satisfaction with specific interactions and recommendations. To address this, we can adopt a multi-token prediction objective during training, where the model predicts the next n tokens at each step instead of a single token[4]. This approach encourages the model to capture longer-term dependencies and avoid myopic predictions focused solely on immediate next events.\n\nSecondly, we can use multiple fields in our input data as auxiliary prediction objectives in addition to predicting the next item ID, which remains the primary target. For example, we can derive genres from the items in the original sequence and use this genre sequence as an auxiliary target. This approach serves several purposes: it acts as a regularizer to reduce overfitting on noisy item ID predictions, provides additional insights into user intentions or long-term genre preferences, and, when structured hierarchically, can improve the accuracy of predicting the target item ID. By first predicting auxiliary targets, such as genre or original language, the model effectively narrows down the candidate list, simplifying subsequent item ID prediction.\n\nUnique Challenges for Recommendation FM\n\nIn addition to the infrastructure challenges posed by training bigger models with substantial amounts of user interaction data that are common when trying to build foundation models, there are several unique hurdles specific to recommendations to make them viable. One of unique challenges is entity cold-starting.\n\nAt Netflix, our mission is to entertain the world. New titles are added to the catalog frequently. Therefore the recommendation foundation models require a cold start capability, which means the models need to estimate members’ preferences for newly launched titles before anyone has engaged with them. To enable this, our foundation model training framework is built with the following two capabilities: Incremental training and being able to do inference with unseen entities.\n\nIncremental training : Foundation models are trained on extensive datasets, including every member’s history of plays and actions, making frequent retraining impractical. However, our catalog and member preferences continually evolve. Unlike large language models, which can be incrementally trained with stable token vocabularies, our recommendation models require new embeddings for new titles, necessitating expanded embedding layers and output components. To address this, we warm-start new models by reusing parameters from previous models and initializing new parameters for new titles. For example, new title embeddings can be initialized by adding slight random noise to existing average embeddings or by using a weighted combination of similar titles’ embeddings based on metadata. This approach allows new titles to start with relevant embeddings, facilitating faster fine-tuning. In practice, the initialization method becomes less critical when more member interaction data is used for fine-tuning. Dealing with unseen entities : Even with incremental training, it’s not always guaranteed to learn efficiently on new entities (ex: newly launched titles). It’s also possible that there will be some new entities that are not included/seen in the training data even if we fine-tune foundation models on a frequent basis. Therefore, it’s also important to let foundation models use metadata information of entities and inputs, not just member interaction data. Thus, our foundation model combines both learnable item id embeddings and learnable embeddings from metadata. The following diagram demonstrates this idea.\n\nFigure 2. Titles are associated with various metadata, such as genres, storylines, and tones. Each type of metadata could be represented by averaging its respective embeddings, which are then concatenated to form the overall metadata-based embedding for the title.\n\nTo create the final title embedding, we combine this metadata-based embedding with a fully-learnable ID-based embedding using a mixing layer. Instead of simply summing these embeddings, we use an attention mechanism based on the “age” of the entity. This approach allows new titles with limited interaction data to rely more on metadata, while established titles can depend more on ID-based embeddings. Since titles with similar metadata can have different user engagement, their embeddings should reflect these differences. Introducing some randomness during training encourages the model to learn from metadata rather than relying solely on ID embeddings. This method ensures that newly-launched or pre-launch titles have reasonable embeddings even with no user interaction data.\n\nDownstream Applications and Challenges\n\nOur recommendation foundation model is designed to understand long-term member preferences and can be utilized in various ways by downstream applications:\n\nDirect Use as a Predictive Model The model is primarily trained to predict the next entity a user will interact with. It includes multiple predictor heads for different tasks, such as forecasting member preferences for various genres. These can be directly applied to meet diverse business needs.. Utilizing embeddings The model generates valuable embeddings for members and entities like videos, games, and genres. These embeddings are calculated in batch jobs and stored for use in both offline and online applications. They can serve as features in other models or be used for candidate generation, such as retrieving appealing titles for a user. High-quality title embeddings also support title-to-title recommendations. However, one important consideration is that the embedding space has arbitrary, uninterpretable dimensions and is incompatible across different model training runs. This poses challenges for downstream consumers, who must adapt to each retraining and redeployment, risking bugs due to invalidated assumptions about the embedding structure. To address this, we apply an orthogonal low-rank transformation to stabilize the user/item embedding space, ensuring consistent meaning of embedding dimensions, even as the base foundation model is retrained and redeployed. Fine-Tuning with Specific Data The model’s adaptability allows for fine-tuning with application-specific data. Users can integrate the full model or subgraphs into their own models, fine-tuning them with less data and computational power. This approach achieves performance comparable to previous models, despite the initial foundation model requiring significant resources.\n\nScaling Foundation Models for Netflix Recommendations\n\nIn scaling up our foundation model for Netflix recommendations, we draw inspiration from the success of large language models (LLMs). Just as LLMs have demonstrated the power of scaling in improving performance, we find that scaling is crucial for enhancing generative recommendation tasks. Successful scaling demands robust evaluation, efficient training algorithms, and substantial computing resources. Evaluation must effectively differentiate model performance and identify areas for improvement. Scaling involves data, model, and context scaling, incorporating user engagement, external reviews, multimedia assets, and high-quality embeddings. Our experiments confirm that the scaling law also applies to our foundation model, with consistent improvements observed as we increase data and model size.\n\nFigure 3. The relationship between model parameter size and relative performance improvement. The plot demonstrates the scaling law in recommendation modeling, showing a trend of increased performance with larger model sizes. The x-axis is logarithmically scaled to highlight growth across different magnitudes.\n\nConclusion\n\nIn conclusion, our Foundation Model for Personalized Recommendation represents a significant step towards creating a unified, data-centric system that leverages large-scale data to increase the quality of recommendations for our members. This approach borrows insights from Large Language Models (LLMs), particularly the principles of semi-supervised learning and end-to-end training, aiming to harness the vast scale of unlabeled user interaction data. Addressing unique challenges, like cold start and presentation bias, the model also acknowledges the distinct differences between language tasks and recommendation. The Foundation Model allows various downstream applications, from direct use as a predictive model to generate user and entity embeddings for other applications, and can be fine-tuned for specific canvases. We see promising results from downstream integrations. This move from multiple specialized models to a more comprehensive system marks an exciting development in the field of personalized recommendation systems.\n\nAcknowledgements\n\nContributors to this work (name in alphabetical order): Ai-Lei Sun Aish Fenton Anne Cocos Anuj Shah Arash Aghevli Baolin Li Bowei Yan Dan Zheng Dawen Liang Ding Tong Divya Gadde Emma Kong Gary Yeh Inbar Naor Jin Wang Justin Basilico Kabir Nagrecha Kevin Zielnicki Linas Baltrunas Lingyi Liu Luke Wang Matan Appelbaum Michael Tu Moumita Bhattacharya Pablo Delgado Qiuling Xu Rakesh Komuravelli Raveesh Bhalla Rob Story Roger Menezes Sejoon Oh Shahrzad Naseri Swanand Joshi Trung Nguyen Vito Ostuni Wei Wang Zhe Zhang\n\nReference", "label": "non_personal"}
{"title": "HDR10+ Now Streaming on Netflix", "url": "https://netflixtechblog.com/hdr10-now-streaming-on-netflix-c9ab1f4bd72b?source=collection_home---4------0-----------------------", "content": "HDR10+ Now Streaming on Netflix Netflix Technology Blog 5 min read · Mar 24, 2025 -- 13 Listen Share\n\nRoger Quero, Liwei Guo, Jeff Watts, Joseph McCormick, Agata Opalach, Anush Moorthy\n\nWe are excited to announce that we are now streaming HDR10+ content on our service for AV1-enabled devices, enhancing the viewing experience for certified HDR10+ devices, which previously only received HDR10 content. The dynamic metadata included in our HDR10+ content improves the quality and accuracy of the picture when viewed on these devices.\n\nDelighting Members with Even Better Picture Quality\n\nNearly a decade ago, we made a bold move to be a pioneering adopter of High Dynamic Range (HDR) technology. HDR enables images to have more details, vivid colors, and improved realism. We began producing our shows and movies in HDR, encoding them in HDR, and streaming them in HDR for our members. We were confident that it would greatly enhance our members’ viewing experience, and unlock new creative visions — and we were right! In the last five years, HDR streaming has increased by more than 300%, while the number of HDR-configured devices watching Netflix has more than doubled. Since launching HDR with season one of Marco Polo, Netflix now has over 11,000 hours of HDR titles for members to immerse themselves in.\n\nWe continue to enhance member joy while maintaining creative vision by adding support for HDR10+. This will further augment Netflix’s growing HDR ecosystem, preserve creative intent on even more devices, and provide a more immersive viewing experience.\n\nWe enabled HDR10+ on Netflix using the AV1 video codec that was standardized by the Alliance for Open Media (AOM) in 2018. AV1 is one of the most efficient codecs available today. We previously enabled AV1 encoding for SDR content, and saw tremendous value for our members, including higher and more consistent visual quality, lower play delay and increased streaming at the highest resolution. AV1-SDR is already the second most streamed codec at Netflix, behind H.264/AVC, which has been around for over 20 years! With the addition of HDR10+ streams to AV1, we expect the day is not far when AV1 will be the most streamed codec at Netflix.\n\nTo enhance our offering, we have been adding HDR10+ streams to both new releases and existing popular HDR titles. AV1-HDR10+ now accounts for 50% of all eligible viewing hours. We will continue expanding our HDR10+ offerings with the goal of providing an HDR10+ experience for all HDR titles by the end of this year¹.\n\nIndustry Adopted Formats\n\nToday, the industry recognizes three prevalent HDR formats: Dolby Vision, HDR10, and HDR10+. For all three HDR Formats, metadata is embedded in the content, serving as instructions to guide the playback device — whether it’s a TV, mobile device, or computer — on how to display the image.\n\nHDR10 is the most widely adopted HDR format, supported by all HDR devices. HDR10 uses static metadata that is defined once for the entire content detailing aspects such as the maximum content light level (MaxCLL), maximum frame average light level (MaxFALL), as well as characteristics of the mastering display used for color grading. This metadata only allows for a one-size-fits-all tone mapping of the content for display devices. It cannot account for dynamic contrast across scenes, which most content contains.\n\nHDR10+ and Dolby Vision improve on this with dynamic metadata that provides content image statistics on a per-frame basis, enabling optimized tone mapping adjustments for each scene. This achieves greater perceptual fidelity to the original, preserving creative intent.\n\nHDR10 vs. HDR10+\n\nThe figure below shows screen grabs of two AV1-encoded frames of the same content displayed using HDR10 (top) and HDR10+ (bottom).\n\nPhotographs of devices displaying the same frame with HDR10 metadata (top) and HDR10+ metadata (bottom). Notice the preservation of the flashlight detail in the HDR10+ capture, and the over-exposure of the region under the flashlight in the HDR10 one².\n\nAs seen in the flashlight on the table, the highlight details are clipped in the HDR10 content, but are recovered in HDR10+. Further, the region under the flashlight is overexposed in the HDR10 content, while HDR10+ renders that region with greater fidelity to the source. The reason HDR10+, with its dynamic metadata, shines in this example is that the scenes preceding and following the scene with this frame have markedly different luminance statistics. The static HDR10 metadata is unable to account for the change in the content. While this is a simple example, the dynamic metadata in HDR10+ demonstrates such value across any set of scenes. This consistency allows our members to stay immersed in the content, and better preserves creative intent.\n\nReceiving HDR10+\n\nAt the time of launch, these requirements must be satisfied to receive HDR10+:\n\n1.Member must have a Netflix Premium plan subscription\n\n2. Title must be available in HDR10+ format\n\n3. Member device must support AV1 & HDR10+. Here are some examples of compatible devices:\n\nSmartTVs, mobile phones, and tablets that meet Netflix certification for HDR10+\n\nSource device (such as set-top boxes, streaming devices, MVPDs, etc.) that meets Netflix certification for HDR10+, connected to an HDR10+ compliant display via HDMI\n\n4. For TV or streaming devices, ensure that the HDR toggle is enabled in our Netflix application settings: https://help.netflix.com/en/node/100220\n\nAdditional guidance: https://help.netflix.com/en/node/13444\n\nSummary\n\nMore HDR content is watched every day on Netflix. Expanding the Netflix HDR ecosystem to include HDR10+ increases the accessibility of HDR content with dynamic metadata to more members, improves the viewing experience, and preserves the creative intent of our content creators. The commitment to innovation and quality underscores our dedication to delivering an immersive and authentic viewing experience for all our members.\n\nAcknowledgements\n\nLaunching HDR10+ was a collaborative effort involving multiple teams at Netflix, and we are grateful to everyone who contributed to making this idea a reality. We would like to extend our thanks to the following teams for their crucial roles in this launch:\n\nFootnotes", "label": "non_personal"}
{"title": "Globalizing Productions with Netflix’s Media Production Suite", "url": "https://netflixtechblog.com/globalizing-productions-with-netflixs-media-production-suite-fc3c108c0a22?source=collection_home---4------2-----------------------", "content": "Globalizing Productions with Netflix’s Media Production Suite Netflix Technology Blog 12 min read · Mar 31, 2025 -- 8 Listen Share\n\nJesse Korosi, Thijs van de Kamp, Mayra Vega, Laura Futuro, Anton Margoline\n\nThe journey from script to screen is full of challenges in the ever-evolving world of film and television. The industry has always innovated, and over the last decade, it started moving towards cloud-based workflows. However, unlocking cloud innovation and all its benefits on a global scale has proven to be difficult. The opportunity is clear: streamline complex media management logistics, eliminate tedious, non-creative task-based work and enable productions to focus on what matters most–creative storytelling. With these challenges in mind, Netflix has developed a suite of tools by filmmakers for filmmakers: the Media Production Suite (MPS).\n\nWhat are we solving for?\n\nSignificant time and resources are devoted to managing media logistics throughout the production lifecycle. An average Netflix title produces around ~200 Terabytes of Original Camera Files (OCF), with outliers up to 700 Terabytes, not including any work-in-progress files, VFX assets, 3D assets, etc. The data produced on set is traditionally copied to physical tape stock like LTO. This workflow has been considered the industry norm for a long time and may be cost-effective, but comes with trade-offs. Aside from needing to physically ship and track all movement of the tape stock, storing media on a physical tape makes it harder to search, play and share media assets; slowing down accessibility to production media when needed, especially when titles need to collaborate with talent and vendors all over the world.\n\nEven when workflows are fully digital, the distribution of media between multiple departments and vendors can still be challenging. A lack of automation and standardization often results in a labour-intensive process across post-production and VFX with a lot of dependencies that introduce potential human errors and security risks. Many productions utilize a large variety of vendors, making this collaboration a large technical puzzle. As file sizes grow and workflows become more complex, these issues are magnified, leading to inefficiencies that slow down post-production and reduce the available time spent on creative work.\n\nMoving media into the cloud introduces new challenges for production and post ramping up to meet the operational and technological hurdles this poses. For some post-production facilities, it’s not uncommon to see a wall of portable hard drives at their facility, with media being hand-carried between vendors because alternatives are not available. The need for a centralized, cloud-based solution that transcends these barriers is more pressing than ever. This results in a willingness to embrace new and innovative ideas, even if exploratory, and introduce drastic workflow changes to productions in pursuit of creative evolution.\n\nAt Netflix, we believe that great stories can come from anywhere, but we have seen that technical limitations in traditional workflows reduce access to media and restrict filmmakers’ access to talent. Besides the need for robust cloud storage for their media, artists need access to powerful workstations and real-time playback. Depending on the market, or production budget, cutting-edge technology might not be available or affordable.\n\nWhat if we started charting a course to break free from many of these technical limitations and found ways to enhance creativity? Industry trade shows like the International Broadcast Convention (IBC) and the National Association of Broadcasters Show (NAB) highlight a strong global trend: instead of bringing media to the artist/applications (traditional workflow) we see the shift towards bringing people and applications to the media (cloud workflows and remote workstations). The concept of cloud-based workflows is not new, as many technology leaders in our industry have been experimenting in this space for more than a decade. However, executing this vision at a Netflix scale with hundreds of titles a year has not been done before…\n\nThe challenge of building a global technology to solve this\n\nBuilding solutions at a global scale poses significant challenges. The art of making movies and series lacks equal access to technology, best practices, and global standardization. Different countries worldwide are at different phases of innovation based on local needs and nuances. While some regions boast over a century of cinematic history and have a strong industry, others are just beginning to carve their niche. This vast gap presents a unique challenge: developing global technology that caters to both established and emerging markets, each with distinct languages and workflows.\n\nThe large diversity of needs by talent and vendors globally creates a standardization challenge and can be seen when productions use a global talent pool. Many mature post-production and VFX facilities have built scripts and automation that flow between various artists and personnel within their facility; allowing a more streamlined workflow, even though the customization is time-consuming. E.g., Transcoding, or transcriptions that automatically run when files are dropped in a hot folder, with the expectation that certain sidecar metadata files will accompany them with a specific organizational structure. Embracing and integrating new workflows introduces the fear of disrupting a well-established process, increasing additional pressure on the profit margins of vendors. Small workflow changes that may seem arbitrary may actually have a large impact on vendors. Therefore, innovation should provide meaningful benefits to a title in order to get adopted at scale. Reliability, a proven track record, strong support, and an incredibly low tolerance for bugs, or issues are top of mind in well-established markets.\n\nIn developing this suite, we recognized the necessity of addressing the vast array of titles that flow through Netflix without the luxury of expanding into a massive operational entity. Consequently, automation became imperative. The intricacies of color and framing management, along with deliverables, must be seamlessly controlled and effortlessly managed by the user, without the need for manual intervention. Therefore, we cannot lean into humans configuring JSON files behind the scenes to map camera formats into deliverables. By embracing open standards, we not only streamline these processes but also facilitate smoother collaboration across diverse markets and countries, ensuring that our global productions can operate with unparalleled efficiency and cohesion. To ensure this, we’ve decided to lean heavily into standards like ACES, AMF, ASC MHL, ASC FDL, and OTIO. ACES and AMF for color pipeline management. ASC MHL for any file management/verifications. ASC FDL will serve as our framing interoperability and OTIO for any timeline interchange. Leaning into standards like this means that many things can be automated at scale and more importantly, high-complexity workflows can be offered to markets or shows that don’t normally have access to them. As an example, if a show is shot on various camera formats all framed and recorded at different resolutions, with different lenses and different safeties on each frame. The task of normalizing all of these for a VFX vendor into one common container with a normalized center extracted frame is often only offered to very high-end titles, considering it takes a human behind the curtain to create all of these mappings. But by leaning into a standard like the FDL, it means this can now easily be automated, and the control for these mappings, put directly in the hands of users.\n\nOur Answer — Content Hub’s Media Production Suite (MPS)\n\nIntroducing Content Hub Media Production Suite video\n\nBuilding a global scalable solution that could be utilized in a diversity of markets has been an exciting challenge. We set out to provide customizable and feature-rich tooling for advanced users while remaining intuitive and streamlined enough for less experienced filmmakers. With collaboration from Netflix teams, vendors, and talent across the globe, we’ve taken a bold step forward in enabling a suite of tools inside Netflix Content Hub that democratizes technology: the Media Production Suite. While leveraging our scale economies and access to resources, we can now unlock global talent pools for our productions, drastically reduce non-creative task-based work, streamline workflows, and level the playing field between our markets, ultimately maximizing the time available for what matters most; creative work!\n\nSo what is it?\n\n1. Netflix Hybrid Infrastructure: Netflix has invested in a hybrid infrastructure, a mix of cloud-based and physically distributed capabilities operating in multiple locations across the world and close to our productions to optimize user performance. This infrastructure is available for Netflix shows and is foundational under Content Hub’s Media Production Suite tooling. Local storage and compute services are connected through the Netflix Open Connect network (Netflix Content Delivery Network) to the infrastructure of Amazon Web Services (AWS). The system facilitates large volumes of camera and sound media and is built for speed. In order to ensure that productions have sufficient upload speeds to get their media into the cloud, Netflix has started to roll out Content Hub Ingest Centers globally to provide high-speed internet connectivity where required. With all media centralized, MPS eliminates the need for physical media transport and reduces the risk of human error. This approach not only streamlines operations but also enhances security and accessibility.\n\n2. Automation and Tooling: In addition to the Netflix Hybrid infrastructure layer, MPS consists of a suite of tools that tap into the media in the Netflix ecosystem.\n\nFootage Ingest — An application that allows users to upload media/files into Content Hub.\n\nMedia Library — A central library that allows users to search, preview, share and download media.\n\nDailies — A workflow, backed by an operational team, offering automated Quality Control of your footage, sound sync, application of color, rendering, and delivering dailies directly to editorial.\n\nRemote Workstations — Offering access to remote editorial workstations and storage for post-production needs.\n\nVFX Pulls — An automated method for converting and delivering visual effects plates, associated color, and framing files to VFX vendors.\n\nConform Pulls — An automated method for consolidating, trimming, and delivering all OCF to picture-finishing vendors.\n\nMedia Downloader — An automated download tool that initiates a download once media has been made available in the Netflix cloud.\n\nWhile each of the individual tools within MPS is at different states of maturity, over 350 titles have made use of at least one of the tools noted above. Input has been taken from all over the world while developing, with users ranging from UCAN (United States/Canada), EMEA (Europe, Middle East, and Africa), SEA (South East Asia), LATAM (Latin America), and APAC (Asia Pacific).\n\nSenna: Early Adoption and Insightful Feedback Driving MPS Evolution\n\nMedia from the Brazilian-produced series ‘Senna’ being reviewed in MPS\n\nThe Brazilian-produced series Senna, which follows the life of legendary Formula 1 driver Ayrton Senna, utilized MPS to reshape their content creation workflow, overcome geographical barriers, and unlock innovation to support world-class storytelling for a global audience. Senna is a groundbreaking series, not just for its storytelling but for its production journey across Argentina, Uruguay, Brazil, and the United Kingdom. With editorial teams spread across Porto Alegre and Spain, and VFX studios collaborating across locations in Brazil, Canada, the United States, and India, all orchestrated by our subsidiary Scanline VFX. The series exemplifies the global nature of modern filmmaking and was the perfect fit for Netflix’s new Content Hub Media Production Suite (MPS).\n\nAt the heart of Senna’s workflow orchestration is MPS. While each of the tools within MPS is based on an opt-in model, in order to use many of the downstream services, the first step is ensuring that the original camera files (OCF) and original sound files (OSF) are uploaded. “We knew we were going to shoot in different places,” said Post Supervisor Gabriel Queiroz,“to have all this material cloud-based, it’s definitely one of the most important things for us. It would be hard to bring all this media physically from Argentina or wherever to Brazil. It will take us a lot of time.” With Senna shooting across locations, allowing production the capability of uploading their OCF and OSF resulted in no longer requiring shuttling hard drives on airplanes, creating LTO tapes, & managing physical shipments for their negative. And yes, you read that correctly; when utilizing MPS, we don’t require LTO tapes to be written unless there are title-specific needs.\n\nWith Senna beginning production back in June of 2023, our investment in MPS was still very early stages, and the tooling was considered beta. However, with the help, feedback, and partnership from this production, it was quickly realized that the investment was worth doubling down on. Since the early version used on Senna, Netflix has been spinning up ingest centers around the world, where drives can be dropped off, and within a matter of hours, all original camera files are uploaded into the Netflix ecosystem. While creating the ability to upload is not a novel concept, behind the scenes, it’s far from simple. Once a drive has been plugged in and our Netflix Footage Ingest application is opened, a validation is run, ensuring all expected media from set is on the drive. After media has been uploaded and checksums are run validating media integrity, all media is inspected, metadata is extracted, and assets are created for viewing/sharing/downloading with playable proxies. All media is then automatically backed up to a second tier of cloud-based storage for the final archive.\n\nTraditionally, if you wanted to check in with your post vendor on how things are going for each of these media management steps noted above, or whether or not you can clear on set camera cards if you haven’t gotten a completion notification, you would have to pick up the phone and call your vendor. For Senna, anyone who wanted visibility on progress, simply logged in to Content Hub and could see any activity in the Footage Ingest dashboard, as well as look up any information needed on past uploads.\n\nRemote monitoring media being uploaded and archived using the MPS Footage Ingest workflow\n\nWhile many services in MPS are available once media has been uploaded, Senna’s use of MPS focused on VFX. With Senna shooting a high volume of footage and the show having a high volume of VFX shots, according to Post Supervisor Gabriel Queiroz “Using MPS was basically a no-brainer, [having] used the tool before, I knew what it could bring to the project. And to be honest, with the amount of footage that we have, it was just so much material and with the amount of vendors we have, knowing that we would have to deliver all this footage to all these kinds of vendors, including outside of Brazil and to different parts of the world.”\n\nWith a traditional workflow, utilizing available resources in Latin America, VFX Pulls would have been done manually. This process is prone to human error and more importantly, for a show like Senna, too slow and would have resulted in different I/O methods for every vendor.\n\nIllustrating a traditional VFX Editor having to manage various I/O methods\n\nBy utilizing MPS, the Assistant Editor was able to log into Content Hub, upload an EDL, and have their VFX Pulls automatically transcoded, color files consolidated and all media placed into a Google Drive style folder built directly in Content Hub (called Workspaces). The VFX Editor was able to make any additional tweaks they wanted to the directory before farming out each of the shots to whichever vendor they were meant for. When it came time for the VFX vendors to then send shots back to editorial or DI, this was also done through MPS. Having one standard method for I/O for all VFX file sharing meant that Editorial and DI did not have to manage a different file transfer/workflow for every single vendor that was onboarded.\n\nIllustrating a more streamlined workflow for VFX vendors when using MPS\n\nAfter picture was locked and it was time for Senna to do their Online, the DI facility Quanta was able to utilize the Conform Pull service within MPS. The Conform Pull service allowed their team to upload an EDL, which ran a QC on all of the media from within their edit to ensure a smooth conform and then consolidated, trimmed, and packaged up all of the media they needed for the online. Since this early beta and thanks to learnings from many shows like Senna, advancements have been made in the system’s ability to match back to source media for both Conform and VFX Pulls. Rather than requiring an exact match between EDL and source OCF, there are several variations of fuzzy matching that can take place, as well as a current investigation in using one of our perceptual matching algorithms, allowing for a perceptual conform using computer vision, instead of solely relying on metadata.\n\nInside Senna with Content Hub Media Production Suite video\n\nConclusion\n\nThe Media Production Suite (MPS) represents a transformative leap in how we approach media production at Netflix. By embracing open standards, we have crafted a scalable solution that not only makes economic sense but also democratizes access to advanced production tools across the globe. This approach allows us to eliminate tedious tasks, enabling our teams to focus on what truly matters: creative storytelling. By fostering global collaboration and leveraging the power of cloud-based workflows, we’re not just enhancing efficiency but also elevating the quality of our productions. As we continue to innovate and refine our processes, we remain committed to breaking down barriers and unlocking the full potential of creative talent worldwide. The future of filmmaking is here, and with MPS, we are leading the charge toward a more connected and creatively empowered industry.", "label": "non_personal"}
{"title": "Leveraging BigQuery JSON for Optimized MongoDB Dataflow Pipelines", "url": "https://developers.googleblog.com/en/leveraging-bigquery-json-for-optimized-mongodb-dataflow-pipelines/", "content": "This streamlined approach saves time and resources, empowering users to unlock the full potential of their data through advanced data analytics and machine learning.\n\nWe're delighted to introduce a major enhancement to our Google Cloud Dataflow templates for MongoDB Atlas. By enabling direct support for JSON data types, users can now seamlessly integrate their MongoDB Atlas data into BigQuery, eliminating the need for complex data transformations.\n\nLimitations without JSON support\n\nTraditionally, Dataflow pipelines designed to handle MongoDB Atlas data often necessitate the transformation of data into JSON strings or flattening complex structures to a single level of nesting before loading into BigQuery. Although this approach is viable, it can result in several drawbacks:\n\nIncreased latency: The multiple data conversions required can lead to increased latency and can significantly slow down the overall pipeline execution time.\n\nHigher operational costs: The extra data transformations and storage requirements associated with this approach can lead to increased operational costs.\n\nReduced query performance: Flattening complex document structures in JSON String format can impact query performance and make it difficult to analyze nested data.\n\n\n\nSo, what’s new?\n\nBigQuery's Native JSON format addresses these challenges by enabling users to directly load nested JSON data from MongoDB Atlas into BigQuery without any intermediate conversions.\n\nThis approach offers numerous benefits:\n\nReduced operating costs: By eliminating the need for additional data transformations, users can significantly reduce operational expenses, including those associated with infrastructure, storage, and compute resources.\n\nEnhanced query performance: BigQuery's optimized storage and query engine is designed to efficiently process data in Native JSON format, resulting in significantly faster query execution times and improved overall query performance.\n\nImproved data flexibility: users can easily query and analyze complex data structures, including nested and hierarchical data, without the need for time-consuming and error-prone flattening or normalization processes.\n\nA significant advantage of this pipeline lies in its ability to directly leverage BigQuery's powerful JSON functions on the MongoDB data loaded into BigQuery. This eliminates the need for a complex and time-consuming data transformation process. The JSON data within BigQuery can be queried and analyzed using standard BQML queries.\n\nWhether you prefer a streamlined cloud-based approach or a hands-on, customizable solution, the Dataflow pipeline can be deployed either through the Google Cloud console or by running the code from github repository.\n\n\n\nEnabling data-driven decision-making\n\nTo summarize, Google’s Dataflow template provides a flexible solution for transferring data from MongoDB to BigQuery. It can process entire collections or capture incremental changes using MongoDB's Change Stream functionality. The pipeline's output format can be customized to suit your specific needs. Whether you prefer a raw JSON representation or a flattened schema with individual fields, you can easily configure it through the userOption parameter. Additionally, data transformation can be performed during template execution using User-Defined Functions (UDFs).\n\nBy adopting BigQuery Native JSON format in your Dataflow pipelines, you can significantly enhance the efficiency, performance, and cost-effectiveness of your data processing workflows. This powerful combination empowers you to extract valuable insights from your data and make data-driven decisions.\n\nFollow the Google Documentation to learn how to set up the Dataflow templates for MongoDB Atlas and BigQuery.", "label": "non_personal"}
{"title": "Simplified Dataflow Connectors with Managed I/O", "url": "https://developers.googleblog.com/en/simplified-dataflow-connectors-with-managed-io/", "content": "Google Cloud Dataflow offers a fully managed data processing system for running Apache Beam pipelines on Google Cloud in a highly scalable manner. Due to being a fully managed service, Dataflow users do not have to worry about any service side regressions and versioning. The promise is that you only concern yourself with your pipeline logic while Google takes care of the service infrastructure. While this is certainly true, Apache Beam itself is a very full featured SDK that provides many simple to highly complex transforms for you to use in their pipelines. For example, Apache Beam provides a number of I/O connectors. Many of these connectors are Apache Beam composite transforms from 10s to 100s of steps. Historically, these have been considered \"user code\" from the service's perspective, despite being not authored or maintained by the user. There are several common complications customers run into complex Beam transforms such as I/O connectors. You are on the hook for upgrading Beam to adopt any fixes and improvements to connectors. Connector APIs vary widely and moving from one connector to another usually requires a lot of exploration and learning. While connectors offer a complete API, the API might not be optimized for the Dataflow runner. To alleviate all three of these issues, Dataflow recently introduced a new offering named Managed I/O. With Managed I/O the service itself is able to manage these complexities on your behalf. Hence you can truly focus on their pipelines business logic instead of focussing on the minutiae related to using and configuring a specific connector to suit their needs. Below we detail how each of the above mentioned complexities are addressed via Managed I/O.\n\nAutomatic SDK upgrades Apache Beam is a fully fledged SDK with many transforms, features, and optimization. Like many large pieces of software, upgrading Beam to a new version can be a significant process. Usually upgrading Beam involves upgrading all parts of a pipeline including all I/O connectors. But sometimes, you just need to obtain access to a critical bug fix or an improvement available in the latest version of one or more I/O connectors used in your pipeline. Managed I/O with Dataflow simplifies this by completely taking over the management of the Beam I/O connector version. With Managed I/O, Dataflow will make sure that I/O connectors used by pipelines are always up to date. Dataflow performs this by always upgrading I/O connectors to the latest vetted version during job submission and streaming update via replacement. For example, assume that you use a Beam pipeline that uses Beam 2.x.0 and assume that you use the Managed Apache Iceberg I/O source in your pipeline. Also, assume that the latest vetted version of the Iceberg I/O source supported by Dataflow is 2.y.0. During job submission, Dataflow will replace this specific connector with version 2.y.0 and will keep the rest of the Beam pipeline including any standard (non-managed) I/O connectors at version 2.x.0.\n\nAfter replacement, Dataflow optimizes the updated pipeline and executes it in GCE. To achieve isolation between connectors from different Beam versions, Dataflow deploys an additional Beam SDK container in GCE VMs. So in this case, Beam SDK containers from both versions 2.x.0 and 2.y.0 will be running in each GCE VM used by the Dataflow job. So with Managed I/O you can be assured that I/O connectors used in your pipeline are always up to date. This allows you to focus on improving the business logic of your pipeline without worrying about upgrading the Beam version to simply obtain I/O connector updates.\n\nSimplified IO API APIs differences across Beam I/O connectors vary greatly. This means that, whenever you try to use a new Beam I/O connector, you would have to learn an API specific to that connector. Some of the APIs can be quite large and non-intuitive. This can be due to: Support for various and in some cases redundant features offered by the underlying system. Maintaining backwards compatibility for legacy (or archaic) features or defaults. Support for customizing the I/O connector to support edge cases and implementation details that may only apply to few customers. Above points result in very large API surfaces for some connectors that are not intuitive for a new customer to use efficiently.\n\nManaged I/O offers standardized Java and Python APIs for supported I/O connectors. For example, with Beam Java SDK an I/O connector source can be instantiated in the following standardized form.\n\nManaged.read(SOURCE).withConfig(sourceConfig) Java Copied\n\nAn I/O connector sink can be instantiated in the following form.\n\nManaged.write(SINK).withConfig(sinkConfig) Java Copied\n\nHere SOURCE and SINK are keys specifically identifying the connector while sourceConfig and sinkConfig are maps of configurations used to instantiate the connector source or sink. The map of configurations may also be provided as YAML files available locally or in Google Cloud Storage. Please see the Managed I/O website for more complete examples for supported sources and sinks. Beam Python SDK offers a similarly simplified API. This means that various Beam I/O connectors with different APIs can be instantiated in a very standard way. For example,\n\n// Create a Java BigQuery I/O source Map<String, Object> bqReadConfig = ImmutableMap.of(\"query\", \"<query>\", ...); Managed.read(Managed.BIGQUERY).withConfig(bqReadConfig) // Create a Java Kafka I/O source. Map<String, Object> kafkaReadConfig = ImmutableMap.of(\"bootstrap_servers\", \"<server>\", \"topic\", \"<topic>\", ...); Managed.read(Managed.KAFKA).withConfig(kafkaReadConfig) // Create a Java Kafka I/O source but with a YAML based config available in Google Cloud Storage. String kafkaReadYAMLConfig = \"gs://path/to/config.yaml\" Managed.read(Managed.KAFKA).withConfigUrl(kafkaReadYAMLConfig) // Create a Python Iceberg I/O source. iceberg_config = {\"table\": \"<table>\", ...} managed.Read(managed.ICEBERG, config=iceberg_config) Java Copied\n\nAutomatically optimized for Dataflow Many Beam connectors offer a comprehensive API for configuring and optimizing the connector to suit a given pipeline and a given Beam runner. One downside of this is that if you specifically want to run on Dataflow, you may have to learn the specific configurations that best suit Dataflow and apply them when setting up your pipeline. Connector related documentation can be long and detailed and specific changes needed might not be intuitive. This might result in connectors used in Dataflow pipelines performing in a sub-optimal way. Manage I/O connectors alleviates this by automatically re-configuring the connectors to incorporate best practices and configure them to best suit Dataflow. Such re-configuration may occur during job submission or streaming update via replacement. For example, Dataflow streaming pipelines offer two modes, exactly-once and at-least-once while BigQuery I/O sink with Storage Write API offer two analogous delivery semantics, exactly-once and at-least-once. BigQuery sink with at-least-once delivery semantics is usually less expensive and results in lower latencies. With standard BigQuery I/O connectors, you are responsible for making sure that you use the appropriate mode when using the BigQuery I/O. With Managed BigQuery I/O sink this is automatically configured for you. Which means that if your streaming pipeline is operating at the at-least-once mode, your Managed I/O BigQuery sink will be automatically configured to use the at-least-once delivery semantics.\n\nReal-world pipelines We ran several pipelines that wrote data using the Managed Iceberg I/O sink backed by a Hadoop catalog deployed in GCS (please see here for the other supported catalogs). Pipelines were submitted using Beam 2.61.0 and the Managed I/O sink was automatically upgraded by Dataflow to the latest supported version. All benchmarks used n1-standard-4 VMs and the number of VMs used by the pipeline was fixed to 100. Please note that execution time here does not include the startup and shutdown time.\n\nAs the benchmarks show, Managed Iceberg I/O scaled up nicely and both metrics grew linearly with the data size. We also ran a streaming pipeline that read from Google Pub/Sub and used the Managed I/O Kafka sink to push messages to a Kafka cluster hosted in GCP. The pipeline used Beam 2.61.0 and Dataflow upgraded the Managed Kafka sink to the latest supported version. During the steady state, the pipeline used 10 n1-standard-4 VMs (max 20 VMs). The pipeline was consistently processing messages at a throughput of 250k msgs/sec across all steps and was run for 2 hours.\n\nThe following graph shows the data throughputs of various steps of the pipeline. Note that throughputs are different here since the element size changes between steps. The pipeline read from Pub/Sub at a rate of 75 MiB/sec (red line) and wrote to Kafka at a rate of 40 MiB/sec (green line).\n\nBoth latency and backlog was low for the duration of the pipeline execution.\n\nThe pipeline used VM CPU and memory efficiently.", "label": "non_personal"}
{"title": "Unlock your potential: Discover the enhanced Google Developer Program", "url": "https://developers.googleblog.com/en/google-developer-program-latest-enhancements/", "content": "The Google Developer Program is evolving. We're introducing AI-powered tools and expanded resources designed to help you build faster, smarter and more effectively with Google’s technologies.\n\nIn today’s fast-paced development landscape, having the right toolkit is crucial. We’ve listened to your feedback and are delivering updates that will enable you to focus on what matters most: creating exceptional AI applications.\n\n\n\nUnlock your full potential with Google Developer Program Premium: Now infused with AI\n\nThe Google Developer Program premium membership ($299/year) is an enhanced set of resources designed to provide developers with more advanced capabilities, including access to Google’s latest AI tools. Think of it as your catalyst for growth with advanced tools and dedicated support at every phase of your development journey.\n\nToday, we are adding some new ways you can experience the power of Google’s AI:\n\nEnhanced Coding Assistance with Gemini Code Assist Standard: Access paid Gemini in Firebase features and Gemini Code Assist to instantly boost your coding efficiency. Write high-quality code, faster, and with greater confidence.\n\nIncreased Capacity with Firebase Studio Workspaces: Get more room to build! Project IDX is now part of Firebase Studio. With premium, you get 30 Firebase Studio workspaces, providing greater flexibility to handle complex projects and scale your applications.\n\nExperiment with the Latest Models: Dive into API-driven AI with a $50 GenAI developer credit for Google AI Studio and Google Cloud Vertex AI. Experiment with cutting-edge Gemini, Imagen, and Veo models and integrate powerful AI capabilities into your application.\n\nAccess to Premium Google AI Features: Premium members receive a 3-month free trial of Google One AI Premium. Enjoy Gemini Advanced, NotebookLM Plus, increased storage, and much more.\n\nWe've also improved the premium benefits dashboard. Log in to see all your benefits and easily activate them in one place.", "label": "non_personal"}
{"title": "Behind the Scenes: Building a Robust Ads Event Processing Pipeline", "url": "https://netflixtechblog.com/behind-the-scenes-building-a-robust-ads-event-processing-pipeline-e4e86caf9249?source=collection_home---4------1-----------------------", "content": "Behind the Scenes: Building a Robust Ads Event Processing Pipeline Netflix Technology Blog 8 min read · May 9, 2025 -- 12 Listen Share\n\nKinesh Satiya\n\nIntroduction\n\nIn a digital advertising platform, a robust feedback system is essential for the lifecycle and success of an ad campaign. This system comprises of diverse sub-systems designed to monitor, measure, and optimize ad campaigns. At Netflix, we embarked on a journey to build a robust event processing platform that not only meets the current demands but also scales for future needs. This blog post delves into the architectural evolution and technical decisions that underpin our Ads event processing pipeline.\n\nAd serving acts like the “brain” — making decisions, optimizing delivery and ensuring right Ad is shown to the right member at the right time. Meanwhile, ad events, after an Ad is rendered, function like “heartbeats”, continuously providing real-time feedback (oxygen/nutrients) that fuels better decision-making, optimizations, reporting, measurement, and billing. Expanding on this analogy:\n\nJust as the brain relies on continuous blood flow, ad serving depends on a steady stream of ad events to adjust next ad serving decision, frequency capping, pacing, and personalization.\n\nIf the nervous system stops sending signals (ad events stop flowing), the brain (ad serving) lacks critical insights and starts making poor decisions or even fails.\n\nThe healthier and more accurate the event stream (just like strong heart function), the better the ad serving system can adapt, optimize, and drive business outcomes.\n\nLet’s dive into the journey of building this pipeline.\n\nThe Pilot\n\nIn November 2022, we launched a brand new basic ads plan, in partnership with Microsoft. The software systems extended the existing Netflix playback systems to play ads. Initially, the system was designed to be simple, secure, and efficient, with an underlying ethos of device-originated and server-proxied operations. The system consisted of three main components: the Microsoft Ad Server, Netflix Ads Manager, and Ad Event Handler. Each ad served required tracking to ensure the feedback loop functioned effectively, providing the external ad server with insights on impressions, frequency capping (advertiser policy that limits the number of times a user sees a specific ad), and monetization processes.\n\nKey features of this system include:", "label": "non_personal"}
{"title": "FM-Intent: Predicting User Session Intent with Hierarchical Multi-Task Learning", "url": "https://netflixtechblog.com/fm-intent-predicting-user-session-intent-with-hierarchical-multi-task-learning-94c75e18f4b8?source=collection_home---4------0-----------------------", "content": "FM-Intent: Predicting User Session Intent with Hierarchical Multi-Task Learning Netflix Technology Blog 7 min read · May 21, 2025 -- 4 Listen Share\n\nAuthors: Sejoon Oh, Moumita Bhattacharya, Yesu Feng, Sudarshan Lamkhede, Ko-Jen Hsiao, and Justin Basilico\n\nMotivation\n\nRecommender systems have become essential components of digital services across e-commerce, streaming media, and social networks [1, 2]. At Netflix, these systems drive significant product and business impact by connecting members with relevant content at the right time [3, 4]. While our recommendation foundation model (FM) has made substantial progress in understanding user preferences through large-scale learning from interaction histories (please refer to this article about FM @ Netflix), there is an opportunity to further enhance its capabilities. By extending FM to incorporate the prediction of underlying user intents, we aim to enrich its understanding of user sessions beyond next-item prediction, thereby offering a more comprehensive and nuanced recommendation experience.\n\nRecent research has highlighted the importance of understanding user intent in online platforms [5, 6, 7, 8]. As Xia et al. [8] demonstrated at Pinterest, predicting a user’s future intent can lead to more accurate and personalized recommendations. However, existing intent prediction approaches typically employ simple multi-task learning that adds intent prediction heads to next-item prediction models without establishing a hierarchical relationship between these tasks.\n\nTo address these limitations, we introduce FM-Intent, a novel recommendation model that enhances our foundation model through hierarchical multi-task learning. FM-Intent captures a user’s latent session intent using both short-term and long-term implicit signals as proxies, then leverages this intent prediction to improve next-item recommendations. Unlike conventional approaches, FM-Intent establishes a clear hierarchy where intent predictions directly inform item recommendations, creating a more coherent and effective recommendation pipeline.\n\nFM-Intent makes three key contributions:\n\nA novel recommendation model that captures user intent on the Netflix platform and enhances next-item prediction using this intent information. A hierarchical multi-task learning approach that effectively models both short-term and long-term user interests. Comprehensive experimental validation showing significant performance improvements over state-of-the-art models, including our foundation model.\n\nUnderstanding User Intent in Netflix\n\nIn the Netflix ecosystem, user intent manifests through various interaction metadata, as illustrated in Figure 1. FM-Intent leverages these implicit signals to predict both user intent and next-item recommendations.\n\nFigure 1: Overview of user engagement data in Netflix. User intent can be associated with several interaction metadata. We leverage various implicit signals to predict user intent and next-item.\n\nIn Netflix, there can be multiple types of user intents. For instance,\n\nAction Type: Categories reflecting what users intend to do on Netflix, such as discovering new content versus continuing previously started content. For example, when a member plays a follow-up episode of something they were already watching, this can be categorized as “continue watching” intent. Genre Preference: The pre-defined genre labels (e.g., Action, Thriller, Comedy) that indicate a user’s content preferences during a session. These preferences can shift significantly between sessions, even for the same user. Movie/Show Type: Whether a user is looking for a movie (typically a single, longer viewing experience) or a TV show (potentially multiple episodes of shorter duration). Time-since-release: Whether the user prefers newly released content, recent content (e.g., between a week and a month), or evergreen catalog titles.\n\nThese dimensions serve as proxies for the latent user intent, which is often not directly observable but crucial for providing relevant recommendations.\n\nFM-Intent Model Architecture\n\nFM-Intent employs a hierarchical multi-task learning approach with three major components, as illustrated in Figure 2.\n\nFigure 2: An architectural illustration of our hierarchical multi-task learning model FM-Intent for user intent and item predictions. We use ground-truth intent and item-ID labels to optimize predictions.\n\n1. Input Feature Sequence Formation\n\nThe first component constructs rich input features by combining interaction metadata. The input feature for each interaction combines categorical embeddings and numerical features, creating a comprehensive representation of user behavior.\n\n2. User Intent Prediction\n\nThe intent prediction component processes the input feature sequence through a Transformer encoder and generates predictions for multiple intent signals.\n\nThe Transformer encoder effectively models the long-term interest of users through multi-head attention mechanisms. For each prediction task, the intent encoding is transformed into prediction scores via fully-connected layers.\n\nA key innovation in FM-Intent is the attention-based aggregation of individual intent predictions. This approach generates a comprehensive intent embedding that captures the relative importance of different intent signals for each user, providing valuable insights for personalization and explanation.\n\n3. Next-Item Prediction with Hierarchical Multi-Task Learning\n\nThe final component combines the input features with the user intent embedding to make more accurate next-item recommendations.\n\nFM-Intent employs hierarchical multi-task learning where intent predictions are conducted first, and their results are used as input features for the next-item prediction task. This hierarchical relationship ensures that the next-item recommendations are informed by the predicted user intent, creating a more coherent and effective recommendation model.\n\nOffline Results\n\nWe conducted comprehensive offline experiments on sampled Netflix user engagement data to evaluate FM-Intent’s performance. Note that FM-Intent uses a much smaller dataset for training compared to the FM production model due to its complex hierarchical prediction architecture.\n\nNext-Item and Next-Intent Prediction Accuracy\n\nTable 1 compares FM-Intent with several state-of-the-art sequential recommendation models, including our production model (FM-Intent-V0).\n\nTable 1: Next-item and next-intent prediction results of baselines and our proposed method FM-Intent on the Netflix user engagement dataset.\n\nAll metrics are represented as relative % improvements compared to the SOTA baseline: TransAct. N/A indicates that a model is not capable of predicting a certain intent. Note that we added additional fully-connected layers to LSTM, GRU, and Transformer baselines in order to predict user intent, while we used original implementations for other baselines. FM-Intent demonstrates statistically significant improvement of 7.4% in next-item prediction accuracy compared to the best baseline (TransAct).\n\nMost baseline models show limited performance as they either cannot predict user intent or cannot incorporate intent predictions into next-item recommendations. Our production model (FM-Intent-V0) performs well but lacks the ability to predict and leverage user intent. Note that FM-Intent-V0 is trained with a smaller dataset for a fair comparison with other models; the actual production model is trained with a much larger dataset.\n\nQualitative Analysis: User Clustering\n\nFigure 3: K-means++ (K=10) clustering of user intent embeddings found by FM-Intent; FM-Intent finds unique clusters of users that share the similar intent.\n\nFM-Intent generates meaningful user intent embeddings that can be used for clustering users with similar intents. Figure 3 visualizes 10 distinct clusters identified through K-means++ clustering. These clusters reveal meaningful user segments with distinct viewing patterns:\n\nUsers who primarily discover new content versus those who continue watching recent/favorite content.\n\nGenre enthusiasts (e.g., anime/kids content viewers).\n\nUsers with specific viewing patterns (e.g., Rewatchers versus casual viewers).\n\nPotential Applications of FM-Intent\n\nFM-Intent has been successfully integrated into Netflix’s recommendation ecosystem, can be leveraged for several downstream applications:\n\nPersonalized UI Optimization: The predicted user intent could inform the layout and content selection on the Netflix homepage, emphasizing different rows based on whether users are in discovery mode, continue-watching mode, or exploring specific genres. Analytics and User Understanding: Intent embeddings and clusters provide valuable insights into viewing patterns and preferences, informing content acquisition and production decisions. Enhanced Recommendation Signals: Intent predictions serve as features for other recommendation models, improving their accuracy and relevance. Search Optimization: Real-time intent predictions help prioritize search results based on the user’s current session intent.\n\nConclusion\n\nFM-Intent represents an advancement in Netflix’s recommendation capabilities by enhancing them with hierarchical multi-task learning for user intent prediction. Our comprehensive experiments demonstrate that FM-Intent significantly outperforms state-of-the-art models, including our prior foundation model that focused solely on next-item prediction. By understanding not just what users might watch next but what underlying intents users have, we can provide more personalized, relevant, and satisfying recommendations.\n\nAcknowledgements\n\nWe thank our stunning colleagues in the Foundation Model team & AIMS org. for their valuable feedback and discussions. We also thank our partner teams for getting this up and running in production.\n\nReferences\n\n[1] Amatriain, X., & Basilico, J. (2015). Recommender systems in industry: A netflix case study. In Recommender systems handbook (pp. 385–419). Springer.\n\n[2] Gomez-Uribe, C. A., & Hunt, N. (2015). The netflix recommender system: Algorithms, business value, and innovation. ACM Transactions on Management Information Systems (TMIS), 6(4), 1–19.\n\n[3] Jannach, D., & Jugovac, M. (2019). Measuring the business value of recommender systems. ACM Transactions on Management Information Systems (TMIS), 10(4), 1–23.\n\n[4] Bhattacharya, M., & Lamkhede, S. (2022). Augmenting Netflix Search with In-Session Adapted Recommendations. In Proceedings of the 16th ACM Conference on Recommender Systems (pp. 542–545).\n\n[5] Chen, Y., Liu, Z., Li, J., McAuley, J., & Xiong, C. (2022). Intent contrastive learning for sequential recommendation. In Proceedings of the ACM Web Conference 2022 (pp. 2172–2182).\n\n[6] Ding, Y., Ma, Y., Wong, W. K., & Chua, T. S. (2021). Modeling instant user intent and content-level transition for sequential fashion recommendation. IEEE Transactions on Multimedia, 24, 2687–2700.\n\n[7] Liu, Z., Chen, H., Sun, F., Xie, X., Gao, J., Ding, B., & Shen, Y. (2021). Intent preference decoupling for user representation on online recommender system. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence (pp. 2575–2582).\n\n[8] Xia, X., Eksombatchai, P., Pancha, N., Badani, D. D., Wang, P. W., Gu, N., Joshi, S. V., Farahpour, N., Zhang, Z., & Zhai, A. (2023). TransAct: Transformer-based Realtime User Action Model for Recommendation at Pinterest. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (pp. 5249–5259).", "label": "non_personal"}
{"title": "Announcing the Agent2Agent Protocol (A2A)", "url": "https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/", "content": "A new era of Agent Interoperability\n\n\n\nAI agents offer a unique opportunity to help people be more productive by autonomously handling many daily recurring or complex tasks. Today, enterprises are increasingly building and deploying autonomous agents to help scale, automate and enhance processes throughout the workplace–from ordering new laptops, to aiding customer service representatives, to assisting in supply chain planning.\n\nTo maximize the benefits from agentic AI, it is critical for these agents to be able to collaborate in a dynamic, multi-agent ecosystem across siloed data systems and applications. Enabling agents to interoperate with each other, even if they were built by different vendors or in a different framework, will increase autonomy and multiply productivity gains, while lowering long-term costs.\n\n\n\nToday, we’re launching a new, open protocol called Agent2Agent (A2A), with support and contributions from more than 50 technology partners like Atlassian, Box, Cohere, Intuit, Langchain, MongoDB, PayPal, Salesforce, SAP, ServiceNow, UKG and Workday; and leading service providers including Accenture, BCG, Capgemini, Cognizant, Deloitte, HCLTech, Infosys, KPMG, McKinsey, PwC, TCS, and Wipro. The A2A protocol will allow AI agents to communicate with each other, securely exchange information, and coordinate actions on top of various enterprise platforms or applications. We believe the A2A framework will add significant value for customers, whose AI agents will now be able to work across their entire enterprise application estates.\n\nThis collaborative effort signifies a shared vision of a future when AI agents, regardless of their underlying technologies, can seamlessly collaborate to automate complex enterprise workflows and drive unprecedented levels of efficiency and innovation.\n\nA2A is an open protocol that complements Anthropic's Model Context Protocol (MCP), which provides helpful tools and context to agents. Drawing on Google's internal expertise in scaling agentic systems, we designed the A2A protocol to address the challenges we identified in deploying large-scale, multi-agent systems for our customers. A2A empowers developers to build agents capable of connecting with any other agent built using the protocol and offers users the flexibility to combine agents from various providers. Critically, businesses benefit from a standardized method for managing their agents across diverse platforms and cloud environments. We believe this universal interoperability is essential for fully realizing the potential of collaborative AI agents.", "label": "non_personal"}
{"title": "Model Once, Represent Everywhere: UDA (Unified Data Architecture) at Netflix", "url": "https://netflixtechblog.com/uda-unified-data-architecture-6a6aee261d8d?source=collection_home---4------0-----------------------", "content": "Model Once, Represent Everywhere: UDA (Unified Data Architecture) at Netflix Netflix Technology Blog 15 min read · Jun 12, 2025 -- 23 Listen Share\n\nBy Alex Hutter, Alexandre Bertails, Claire Wang, Haoyuan He, Kishore Banala, Peter Royal, Shervin Afshar\n\nAs Netflix’s offerings grow — across films, series, games, live events, and ads — so does the complexity of the systems that support it. Core business concepts like ‘actor’ or ‘movie’ are modeled in many places: in our Enterprise GraphQL Gateway powering internal apps, in our asset management platform storing media assets, in our media computing platform that powers encoding pipelines, to name a few. Each system models these concepts differently and in isolation, with little coordination or shared understanding. While they often operate on the same concepts, these systems remain largely unaware of that fact, and of each other.\n\nAs a result, several challenges emerge:\n\nDuplicated and Inconsistent Models — Teams re-model the same business entities in different systems, leading to conflicting definitions that are hard to reconcile.\n\n— Teams re-model the same business entities in different systems, leading to conflicting definitions that are hard to reconcile. Inconsistent Terminology — Even within a single system, teams may use different terms for the same concept, or the same term for different concepts, making collaboration harder.\n\n— Even within a single system, teams may use different terms for the same concept, or the same term for different concepts, making collaboration harder. Data Quality Issues — Discrepancies and broken references are hard to detect across our many microservices. While identifiers and foreign keys exist, they are inconsistently modeled and poorly documented, requiring manual work from domain experts to find and fix any data issues.\n\n— Discrepancies and broken references are hard to detect across our many microservices. While identifiers and foreign keys exist, they are inconsistently modeled and poorly documented, requiring manual work from domain experts to find and fix any data issues. Limited Connectivity — Within systems, relationships between data are constrained by what each system supports. Across systems, they are effectively non-existent.\n\nTo address these challenges, we need new foundations that allow us to define a model once, at the conceptual level, and reuse those definitions everywhere. But it isn’t enough to just document concepts; we need to connect them to real systems and data. And more than just connect, we have to project those definitions outward, generating schemas and enforcing consistency across systems. The conceptual model must become part of the control plane.\n\nThese were the core ideas that led us to build UDA.\n\nIntroducing UDA\n\nUDA (Unified Data Architecture) is the foundation for connected data in Content Engineering. It enables teams to model domains once and represent them consistently across systems — powering automation, discoverability, and semantic interoperability.\n\nUsing UDA, users and systems can:\n\nRegister and connect domain models — formal conceptualizations of federated business domains expressed as data.\n\nWhy? So everyone uses the same official definitions for business concepts, which avoids confusion and stops different teams from rebuilding similar models in conflicting ways.\n\nCatalog and map domain models to data containers, such as GraphQL type resolvers served by a Domain Graph Service, Data Mesh sources, or Iceberg tables, through their representation as a graph.\n\nWhy? To make it easy to find where the actual data for these business concepts lives (e.g., in which specific database, table, or service) and understand how it’s structured there.\n\nTranspile domain models into schema definition languages like GraphQL, Avro, SQL, RDF, and Java, while preserving semantics.\n\nWhy? To automatically create consistent technical data structures (schemas) for various systems directly from the domain models, saving developers manual effort and reducing errors caused by out-of-sync definitions.\n\nMove data faithfully between data containers, such as from federated GraphQL entities to Data Mesh (a general purpose data movement and processing platform for moving data between Netflix systems at scale), Change Data Capture (CDC) sources to joinable Iceberg Data Products.\n\nWhy? To save developer time by automatically handling how data is moved and correctly transformed between different systems. This means less manual work to configure data movement, ensuring data shows up consistently and accurately wherever it’s needed.\n\nDiscover and explore domain concepts via search and graph traversal.\n\nWhy? So anyone can more easily find the specific business information they’re looking for, understand how different concepts and data are related, and be confident they are accessing the correct information.\n\nProgrammatically introspect the knowledge graph using Java, GraphQL, or SPARQL.\n\nWhy? So developers can build smarter applications that leverage this connected business information, automate more complex data-dependent workflows, and help uncover new insights from the relationships in the data.\n\nThis post introduces the foundations of UDA as a knowledge graph, connecting domain models to data containers through mappings, and grounded in an in-house metamodel, or model of models, called Upper. Upper defines the language for domain modeling in UDA and enables projections that automatically generate schemas and pipelines across systems.\n\nThe same domain model can be connected to semantically equivalent data containers in the UDA knowledge graph.\n\nThis post also highlights two systems that leverage UDA in production:\n\nPrimary Data Management (PDM) is our platform for managing authoritative reference data and taxonomies. PDM turns domain models into flat or hierarchical taxonomies that drive a generated UI for business users. These taxonomy models are projected into Avro and GraphQL schemas, automatically provisioning data products in the Warehouse and GraphQL APIs in the Enterprise Gateway.\n\nSphere is our self-service operational reporting tool for business users. Sphere uses UDA to catalog and relate business concepts across systems, enabling discovery through familiar terms like ‘actor’ or ‘movie.’ Once concepts are selected, Sphere walks the knowledge graph and generates SQL queries to retrieve data from the warehouse, no manual joins or technical mediation required.\n\nUDA is a Knowledge Graph\n\nUDA needs to solve the data integration problem. We needed a data catalog unified with a schema registry, but with a hard requirement for semantic integration. Connecting business concepts to schemas and data containers in a graph-like structure, grounded in strong semantic foundations, naturally led us to consider a knowledge graph approach.\n\nWe chose RDF and SHACL as the foundation for UDA’s knowledge graph. But operationalizing them at enterprise scale surfaced several challenges:\n\nRDF lacked a usable information model. While RDF offers a flexible graph structure, it provides little guidance on how to organize data into named graphs, manage ontology ownership, or define governance boundaries. Standard follow-your-nose mechanisms like owl:imports apply only to ontologies and don’t extend to named graphs; we needed a generalized mechanism to express and resolve dependencies between them.\n\nWhile RDF offers a flexible graph structure, it provides little guidance on how to organize data into named graphs, manage ontology ownership, or define governance boundaries. Standard follow-your-nose mechanisms like owl:imports apply only to ontologies and don’t extend to named graphs; we needed a generalized mechanism to express and resolve dependencies between them. SHACL is not a modeling language for enterprise data. Designed to validate native RDF, SHACL assumes globally unique URIs and a single data graph. But enterprise data is structured around local schemas and typed keys, as in GraphQL, Avro, or SQL. SHACL could not express these patterns, making it difficult to model and validate real-world data across heterogeneous systems.\n\nDesigned to validate native RDF, SHACL assumes globally unique URIs and a single data graph. But enterprise data is structured around local schemas and typed keys, as in GraphQL, Avro, or SQL. SHACL could not express these patterns, making it difficult to model and validate real-world data across heterogeneous systems. Teams lacked shared authoring practices. Without strong guidelines, teams modeled their ontologies inconsistently breaking semantic interoperability. Even subtle differences in style, structure, or naming led to divergent interpretations and made transpilation harder to define consistently across schemas.\n\nWithout strong guidelines, teams modeled their ontologies inconsistently breaking semantic interoperability. Even subtle differences in style, structure, or naming led to divergent interpretations and made transpilation harder to define consistently across schemas. Ontology tooling lacked support for collaborative modeling. Unlike GraphQL Federation, ontology frameworks had no built-in support for modular contributions, team ownership, or safe federation. Most engineers found the tools and concepts unfamiliar, and available authoring environments lacked the structure needed for coordinated contributions.\n\nTo address these challenges, UDA adopts a named-graph-first information model. Each named graph conforms to a governing model, itself a named graph in the knowledge graph. This systematic approach ensures resolution, modularity, and enables governance across the entire graph. While a full description of UDA’s information infrastructure is beyond the scope of this post, the next sections explain how UDA bootstraps the knowledge graph with its metamodel and uses it to model data container representations and mappings.\n\nUpper is Domain Modeling\n\nUpper is a language for formally describing domains — business or system — and their concepts. These concepts are organized into domain models: controlled vocabularies that define classes of keyed entities, their attributes, and their relationships to other entities, which may be keyed or nested, within the same domain or across domains. Keyed concepts within a domain model can be organized in taxonomies of types, which can be as complex as the business or the data system needs them to be. Keyed concepts can also be extended from other domain models — that is, new attributes and relationships can be contributed monotonically. Finally, Upper ships with a rich set of datatypes for attribute values, which can also be customized per domain.\n\nThe graph representation of the onepiece: domain model from our UI. Depicted here you can see how Characters are related to Devil Fruit, and that each Devil Fruit has a type.\n\nUpper domain models are data. They are expressed as conceptual RDF and organized into named graphs, making them introspectable, queryable, and versionable within the UDA knowledge graph. This graph unifies not just the domain models themselves, but also the schemas they transpile to — GraphQL, Avro, Iceberg, Java — and the mappings that connect domain concepts to concrete data containers, such as GraphQL type resolvers served by a Domain Graph Service, Data Mesh sources, or Iceberg tables, through their representations. Upper raises the level of abstraction above traditional ontology languages: it defines a strict subset of semantic technologies from the W3C tailored and generalized for domain modeling. It builds on ontology frameworks like RDFS, OWL, and SHACL so domain authors can model effectively without even needing to learn what an ontology is.\n\nUDA domain model for One Piece. Link to full definition.\n\nUpper is the metamodel for Connected Data in UDA — the model for all models. It is designed as a bootstrapping upper ontology, which means that Upper is self-referencing, because it models itself as a domain model; self-describing, because it defines the very concept of a domain model; and self-validating, because it conforms to its own model. This approach enables UDA to bootstrap its own infrastructure: Upper itself is projected into a generated Jena-based Java API and GraphQL schema used in GraphQL service federated into Netflix’s Enterprise GraphQL gateway. These same generated APIs are then used by the projections and the UI. Because all domain models are conservative extensions of Upper, other system domain models — including those for GraphQL, Avro, Data Mesh, and Mappings — integrate seamlessly into the same runtime, enabling consistent data semantics and interoperability across schemas.\n\nTraversing a domain model programmatically using the Java API generated from the Upper metamodel.\n\nData Container Representations\n\nData containers are repositories of information. They contain instance data that conform to their own schema languages or type systems: federated entities from GraphQL services, Avro records from Data Mesh sources, rows from Iceberg tables, or objects from Java APIs. Each container operates within the context of a system that imposes its own structural and operational constraints.\n\nA Data Mesh source is a data container.\n\nData container representations are data. They are faithful interpretations of the members of data systems as graph data. UDA captures the definition of these systems as their own domain models, the system domains. These models encode both the information architecture of the systems and the schemas of the data containers within. They provide a blueprint for translating the systems into graph representations.", "label": "non_personal"}
{"title": "Google Cloud announces general availability of APIM Operator for Apigee", "url": "https://developers.googleblog.com/en/google-cloud-announces-apim-operator-for-apigee-general-availability/", "content": "We're excited to announce the general availability of the Apigee APIM Operator, a new feature that brings lightweight API Management and API Gateway capabilities to your GKE environment. This release marks a significant step in our strategy to enable Apigee for API management on any gateway, anywhere.\n\n\n\nWhat does this mean for you?\n\nDeveloper-Native Tooling: For the many cloud-native businesses using CNCF-standardized tooling, you can now configure API management using Kubernetes-like YAML, eliminating the need to switch between different tools.\n\nReduced Friction: By supporting APIM with Kubernetes and CNCF toolchains, we're reducing the conceptual and operational friction for service developers and platform administrators.\n\nPolicy Management: Admins can create APIM template rules with RBAC, allowing different groups to use different sets of policies based on their needs. Users and admins can also add various Apigee policies to APIM templates, achieving a comparable level of functionality to Apigee Hybrid.\n\n\n\nKey Features and Capabilities\n\nThe GA release enables customers to configure a GKE cluster and GKE Gateway to use an Apigee Hybrid instance for API management through a traffic extension (ext-proc callout). It also provides API lifecycle management using YAML-based policies operated through the Kubernetes/CNCF toolchain and offers factory-built-in starter defaults for Day-Zero with workload tailoring.\n\n\n\nAddressing Customer Needs\n\nThis feature addresses the growing need for developer-friendly tooling that simplifies API management. Apigee, with its perceived complexity and the requirement to switch from kubectl to other tooling, was seen as less agile. The APIM Operator is our response to this feedback, providing a more streamlined and efficient way to manage APIs.\n\n\n\nLooking Ahead\n\nBuilding on the strong foundation of this GA release, we are exploring potential future enhancements such as gRPC and GraphQL support to accommodate a wider range of API types. We are also evaluating ways to address current limitations concerning the number of Gateway resources and policy attachments, and will update the community as new features and support become available.\n\nWe believe that the APIM Operator will significantly improve the developer experience and streamline API management for our customers. We are excited to see the innovative ways you will use this feature to build and deploy your applications.", "label": "non_personal"}
{"title": "What's new with Agents: ADK, Agent Engine, and A2A Enhancements", "url": "https://developers.googleblog.com/en/agents-adk-agent-engine-a2a-enhancements-google-io/", "content": "At Google, we envision a future where intelligent agents are not just tools, but collaborative partners in solving complex challenges, streamlining workflows, and unlocking new possibilities. We believe that empowering developers with a platform that offers flexibility, trust, and comprehensive capabilities is key to realizing this potential. Today, we're thrilled to share a series of significant updates across our product portfolio that reflect this vision, designed to help you build and manage your intelligent agents with unprecedented ease and power. These enhancements focus on providing robust development tools, intuitive management interfaces, and seamless agent-to-agent communications, delivering a stronger foundation for the next generation of AI powered solutions.\n\nBuilding with confidence and flexibility: Agent Development Kit (ADK) To empower you to create sophisticated agents with stability and adaptability, we've added significant innovations with our Agent Development Kit (ADK). Python ADK v1.0.0: Stability for Production-Ready Agents We're excited to announce the v1.0.0 stable release of our Python Agent Development Kit. This milestone signifies that the Python ADK is now production-ready, offering a reliable and robust platform for developers to confidently build and deploy their agents in live environments. We've heard incredible feedback from customers using Agent Development Kit already, including Renault Group, Box, and Revionics. Java ADK v0.1.0: Extending Agent Capabilities to the Java Ecosystem Expanding our reach, we're also launching the initial release of the Java ADK v0.1.0. This development brings the power and flexibility of the ADK to Java developers, enabling them to leverage its capabilities for their agent development needs.\n\nTo get started with the Java ADK, you can add the following dependency to your Maven project:\n\n<dependency> <groupId>com.google.adk</groupId> <artifactId>google-adk</artifactId> <version>0.1.0</version> </dependency> XML Copied\n\nIntuitive control and management: The Agent Engine UI The Vertex AI Agent Engine helps developers deploy, manage, and scale agents in production. We’re excited to now offer an Agent Engine UI to simplify the agent lifecycle in a more straightforward and centralized way. This user-friendly interface, accessible within the Google Cloud console, provides a comprehensive dashboard to view and manage your deployed agents, list sessions, trace and debug actions, and monitor your agents. This streamlined approach significantly enhances the development and management process, offering you greater control and deeper insights into your agent's behavior and performance.", "label": "non_personal"}
{"title": "Data-driven marketing starts with developers", "url": "https://developers.googleblog.com/en/data-driven-marketing-starts-with-developers/", "content": "To build a great marketing campaign in today’s landscape, data needs to be steering your strategy, not just measuring success. Developers play a key role in implementing the tools that analyze and process this data, turning it into insights, smarter strategies, and better results.\n\nUnlock the power in your marketing data with these three developer-friendly MarTech solutions. From gathering data with unparalleled transparency and control, to transforming raw data into structured insights, or using automated A/B testing for optimal performance, here’s how developers can transform what marketing data can do.\n\n\n\nsGTM Pantheon\n\nGain more control and transparency over your marketing data\n\nFrom buttons clicked to pages scrolled, knowing how people interact with your website or app is crucial to optimizing performance. Server-side Google Tag Manager (sGTM) makes this process easier by measuring traffic and managing data flow—while opening the doors to better privacy, performance, control, and productivity.\n\nsGTM Pantheon is a toolbox of easy-to-deploy solutions that complement the existing capabilities of sGTM in different ways:\n\nImprove reporting, bidding, audience management, and data pipeline processes.\n\nReceive unparalleled transparency and control over website and app data.\n\nAccess data from external APIs and cloud-based customer, product, and business data in real time.\n\nOffer real-time website personalization and conversion rate optimization.\n\nAccess advanced analytics and reporting using cloud databases.\n\n\n\nDevelopers have the flexibility to mix and match solutions to create a single pipeline that can be integrated with both Google and non-Google platforms. And because sGTM Pantheon uses a server environment, the solutions run in a private, first-party cloud-secure environment.\n\n\n\nWhat will you find in the sGTM Pantheon toolbox?\n\nTo gather data:\n\nSoteria: Calculates bid to profit for online transactions without exposing data.\n\nPhoebe: Calls Vertex AI in real time for Lifetime Value (LTV) bidding and lead scoring.\n\nArtemis: Gets customer data from Firestore for audience segmentation.\n\nApollo: Retrieves data from a Google Sheet to generate lead gen value for lead scoring.\n\nCerberus: Integrates reCAPTCHA to filter bot-generated events and suspicious activity.\n\nDioscuri: Offers personalization with quick access to Gemini.\n\n\n\nTo send data:\n\nHephaestus: Advances bidding, audience, analytics, and marketing data pipeline automation.\n\nDeipeus: Sends first-party data back to the website for personalization.\n\nChaos: Drives advanced analytics, data recovery, and audience creation.\n\nHermes: Simplifies the sending of data in data pipelines.\n\n\n\nTo manage data:\n\nArgos: Monitors critical gTag settings.\n\n\n\nsGTM Pantheon is a living solution and is continually growing. Want to see more tools? Explore the full sGTM Pantheon on GitHub.\n\n\n\nGA4 Dataform\n\nTransform BigQuery data into accessible insights with GA4 Dataform\n\nYour Google Analytics 4 (GA4) marketing data holds untold stories, powerful insights, and new ways to connect with your audience—but deciphering it isn’t always easy.\n\nGA4 Dataform is a data transformation tool that organizes raw BigQuery data into clear, modular tables, such as events, items, sessions, transactions, and more—so users of all technical skill levels can analyze data and steer data-driven campaigns. Offering both depth and simplicity, GA4 Dataform gives you the power to go beyond default settings, build your own data models, and find new ways to engage with customers.\n\n\n\nHow do I integrate GA4 Dataform with BigQuery?\n\nGA4 Dataform is a Google Cloud Dataform project that provides SQL data models for transforming raw GA4 BigQuery exports. The code is essentially a starter pack to help you build models on top of the GA4 raw data exports for data-driven marketing insights.", "label": "non_personal"}
{"title": "Google Cloud donates A2A to Linux Foundation", "url": "https://developers.googleblog.com/en/google-cloud-donates-a2a-to-linux-foundation/", "content": "Today at Open Source Summit North America, the Linux Foundation announced the formation of the Agent2Agent project with Amazon Web Services, Cisco, Google, Microsoft, Salesforce, SAP, and ServiceNow. With the formation of this new, independent entity, the companies will collaborate closely on fostering an open and interoperable ecosystem for AI agents with the Agent2Agent (A2A) protocol and other interoperability technology. The project will be hosted by the Linux Foundation and will be seeded with Google’s transfer of the groundbreaking Agent2Agent (A2A) protocol specification, accompanying SDKs, and developer tooling.\n\nThe A2A protocol, an open standard for communication and collaboration between distinct AI agents, aims to break down the silos that currently limit the potential of artificial intelligence. More than 100 companies now support the protocol, with AWS and Cisco as its newest validators. By providing a common language for AI agents to discover each other’s capabilities, securely exchange information, and coordinate complex tasks, the A2A protocol is paving the way for a new era of more powerful, collaborative, and innovative AI applications.\n\nThe formation of the Agent2Agent project under the neutral governance of the Linux Foundation will ensure that this critical component remains vendor-agnostic and community-driven. This move is designed to accelerate the adoption and development of the A2A protocol by providing a robust framework for open collaboration, intellectual property management, and long-term stewardship.", "label": "non_personal"}
{"title": "Unlocking the Potential of Quantum Computing", "url": "https://developers.googleblog.com/en/unlocking-the-potential-of-quantum-computing-a-developers-guide-to-error-correction/", "content": "A Developer’s Guide to Error Correction\n\nManipulating quantum states on a superconducting chip cooled to a fraction of a degree above absolute zero at the bottom of a cryogenic fridge is incredibly challenging. And things don't always go to plan. Errors happen. Lots of errors. Sophisticated strategies are required to extract reliable computation from what would otherwise be just so many random output bits. Success is only possible if errors are not too overwhelmingly common. Today, Google Quantum AI announced the construction of a quantum chip where errors are indeed not overwhelmingly common. It makes use of the surface code, essentially a square patch of nearest-neighbor coupled physical qubits that work together to form a single more reliable logical qubit. As the square patch gets bigger, a logical qubit should get more reliable, and that is exactly what we demonstrated: a chip with logical qubits getting over a factor of two more reliable with each increase in patch size. But why is this needed—what are quantum errors and how are they corrected? We'll give a short version of the answers here, and if you want more than a brief overview, you can head straight to the hands-on quantum error correction course now available for free on Coursera where you will find a series of videos and exercises to develop your understanding. “This new course opens a door to a complex domain, making quantum error correction not just accessible, but tangible for millions of learners worldwide,” says Marni Baker Stein, Chief Content Officer at Coursera. “Our collaboration with an industry pioneer like Google Quantum AI signifies another step towards a future where knowledge is the key to harnessing the power of quantum computing.”\n\nWhat is a quantum error? Let's start with classical errors. You have a bit. Suppose it should be 0. But a cosmic ray hits it and it becomes 1. That's a bit-flip error, the only type of error in a classical computer. In comparison, a quantum chip doesn't consist of bits but rather multilevel quantum systems so cold they have well-defined discrete ground and excited states. We write these states as |0>, |1>, |2>, etc. When a cosmic ray hits a qubit that should be |0>, it can produce a superposition a|0> + b|1> + c|2> + ... of many states. Careful engineering of our device has made our qubits robust to all but the most energetic impacts, allowing us to compute results and simply discard the output when we are hit. When computing, we try hard to only use states |0> and |1>, but control and measurement errors inevitably lead to the occasional |2+> state, known as leakage errors. Resetting a qubit gets rid of this error, but also gets rid of any data on that qubit, so we have a special gate that moves higher states off data qubits and onto a qubit about to be reset. This prevents the accumulation of |2+> states in the computer. Then there is the problem that quantum data just doesn't like hanging around very long. Qubits like to relax. If you use an excited state to represent |1>, after a short time it will relax to |0>. We also want to be able to store superpositions like a|0> + b|1>, and that ‘plus’ can spontaneously become a ‘minus’, or a phase-flip error. The various ways that qubits lose data are collectively called decoherence. In general, decoherence can produce a completely different state to the one we want, but fortunately this difference can be broken into a mix of bit flips and phase flips; for brevity we call them X and Z errors.\n\nHow do we detect X and Z errors? Let's start with a picture of our quantum chip.\n\nThis is a 2D array of qubits with nearest neighbor interactions only. Data qubits store our precious quantum state. Measure qubits are used to detect X and Z errors. A good way to understand this is to imagine each light blue region detecting Z errors on the data qubits it touches, and each dark blue region detecting X errors. A Z error on a data qubit activates the neighboring light blue regions, locating it and allowing us to compensate for its presence in software. Provided the density of X and Z errors is low enough, the pattern of lit up regions will give us clear information allowing us to find and compensate for these errors. If a measure qubit suffers an error, that can give you a falsely activated region. To cope with this, the search for errors is repeated as often as possible, and the next time the check is performed there is a good chance it will be resolved. This creates an identifiable signature for a measurement error, enabling these to also be handled in software. It is an area of ongoing research to devise more sophisticated algorithms to handle the output of measure qubits to better identify the location and type of errors, all while keeping pace with the quantum computer.\n\nLearning more What we’ve covered above is scraping the surface of quantum error correction and the critical role it plays in advancing quantum computing. For a step-by-step explanation and labs starting from the very basics, through quantum states and circuits, to some of the latest error correction tools used today, go to Coursera for our hands-on quantum error correction course. If you are a software engineer who has always wanted to work on a quantum problem, after taking the course head over to Quantum AI open source tools, where we build open-source software like Cirq, Stim and Crumble to simulate quantum circuits and develop error correction techniques. Learn how to design quantum algorithms and contribute to the development of tools that will enable the realization of practical quantum computing.\n\nQuantum computing: extra credit Here is an exciting area of research that combines theoretical computer science, software engineering, and quantum physics:", "label": "non_personal"}
{"title": "Building a better smart home", "url": "https://developers.googleblog.com/en/building-a-better-smart-home-expanding-access-for-developers-and-users/", "content": "Expanding access for app and device developers\n\nThe smart home industry is constantly evolving, and we're committed to staying at the forefront of innovation. At I/O 2024 we announced how we reimagined Google Home as a platform for all developers. Since then we have reached major milestones and our first partners have begun launching apps built on our platform.\n\nOur goal has remained the same - to make it easier for developers to create amazing experiences for users, and for those users to enjoy seamless connectivity and interoperability across all their devices. To that end, we are making a series of investments designed to enable all developers to build for the home.\n\n\n\nEmpowering developers with Home APIs\n\nWe believe that open platforms foster greater innovation, and it's clear that developers do too. We’ve had nearly 2,000 developers sign up to learn more about the Home APIs since I/O 2024 and that's why we're thrilled to announce the public developer beta launch of Home APIs - today for Android, and in the coming months for iOS. These APIs provide developers with the tools they need to build richer and more integrated smart home experiences.\n\nThis isn't just theoretical, our partners are already making headway. Early access partners Eve, Nanoleaf, LG, ADT and Tuya Smart have launched new apps and features built with the Google Home Platform, with even more partners like Cync, GE Appliances, Yale, and Aqara releasing in the coming months. At CES 2025, our partners are showcasing device control and automation experiences in their apps built using these APIs. Hisense and Aqara will demo how the Home APIs have helped them to create and surface automations for different areas of their users’ lives. SDMC will demonstrate how the Device & Structure API enables their users to control devices connected to Google Home directly in their apps.\n\n\n\nInvesting in connectivity and interoperability\n\nDelivering a truly smart home requires seamless connectivity made effortless via our platform. Our investments in Matter are a foundational layer to the smart home experience, and we continue to increase our investments in this area across multiple fronts:\n\nExpanding Matter support: A hub for Google Home is critical to unlocking Matter's fast, secure and reliable experience - it enables remote access and fully local control of Matter devices. We've significantly expanded the reach of hubs for Google Home by integrating the Google Home runtime into over 40 million devices, including Google Nest devices, Chromecasts, Google TV devices on Android 14 and eligible LG TVs. This means more users can enjoy the benefits of Matter connectivity, with less work. You can learn more details on our expanded Matter support here.\n\nIncreasing our investment to improve Matter quality: We firmly believe in Matter's potential to unify the smart home, and we are committed to its growth. That's why we, along with Apple and Samsung, are going beyond our existing commitments to Matter to further accelerate the improvement of quality for Matter. At Google, we're investing in Matter's growth in a number of new ways, including increasing development resources to enhance certification automation, interoperability scripting, and SDK bug fixes & maintenance.\n\nStreamlining certification with Connectivity Standards Alliance: In an effort to reduce time and costs for developers looking to certify software and products across multiple ecosystems, Google is excited to join Apple and Samsung in accepting Connectivity Standards Alliance Interop Lab test results. With this, you can now get certified with Works With Google Home for Matter devices without an additional certification process with Google.\n\nMaking Thread more universal: One key customer challenge with Matter today is that Thread devices require a Thread Border Router (TBR) in the home for control. To address this challenge at the ecosystem level and expand TBR availability, we’ve partnered with MediaTek on the new Trinity chip (MT7903) that includes Wi-Fi, Bluetooth LE and Thread on a single system-on-a-chip (SoC). This makes it easier and more affordable for device OEMs to build Thread into all their new products.\n\n\n\nThe Future of the smart home\n\nWe're incredibly excited about the future of the smart home, and we believe these investments will pave the way for a new era of innovation and interoperability. By empowering developers and fostering a robust ecosystem, we're making the dream of a truly connected and intuitive smart home a reality.\n\nTo make sure you are staying up to date with the latest news, announcements, and resources from Google Home, be sure to subscribe to the Google Home Developer Newsletter.", "label": "non_personal"}
{"title": "Usability and safety updates to Google Auth Platform", "url": "https://developers.googleblog.com/en/usability-and-safety-updates-to-google-auth-platform/", "content": "Millions of developers rely on Google’s identity platform for user authentication and the ability to authorize access to hundreds of APIs. Underpinning the platform is one of the world’s largest implementations of the OAuth 2.0 protocol and related OpenID Connect standard, which provide a seamless, safe, and reliable way for developers to integrate with Google. We’re excited to share some updates that will make the platform even more secure and easy to use.\n\n\n\nSimplified OAuth configuration in the Google Cloud Console\n\nDevelopers that use Sign in with Google for authentication or to obtain user authorization to call Google APIs need to register their apps and websites to create client credentials. For developers that use the Google Cloud Console, OAuth configuration pages previously lived in the APIs & Services section. Now, these pages have their own dedicated navigation section called Google Auth Platform. As part of this change, we’ve made it easier to register new projects, reduced the time it takes to update app configurations, and added more helpful guidance for developers. Stay tuned for more improvements in the coming months, including a better onboarding wizard, simplified OAuth scope management, and changes to make app verification faster and more transparent.\n\nFor developers who use OAuth capabilities through other consoles like Firebase or Apps Script, your experience on those products remains unchanged.\n\n\n\nChange to how OAuth client secrets are displayed\n\nSome OAuth clients are required to use a “secret” when making authentication and authorization requests. The client secret is like a password for a website or application, so it’s critical to protect these strings to ensure the security and privacy of user accounts and data.\n\nHistorically, developers have been able to view and download their own client secrets in the Google Cloud Console, Firebase Console, and other places across Google developer products. Starting in June, we’ll start masking OAuth secrets in the client management pages of the Google Cloud Console. As an aid to help identify them, developer consoles will show the last few characters.", "label": "non_personal"}
{"title": "How It’s Made: Little Language Lessons uses Gemini’s multilingual capabilities to personalize language learning", "url": "https://developers.googleblog.com/en/how-its-made-little-language-lessons-to-personalize-learning/", "content": "As an engineer, I’ve always been fascinated by languages—both the kind we code in and the kind we speak. Learning a new programming language typically begins by building something tangible, instantly putting theory into practice. Learning a new spoken language, on the other hand, often happens in a vacuum—through textbooks or exercises that feel strangely disconnected from the situations where language actually matters. As is the case with programming, language is best learned through meaningful contexts: the conversations we have, the objects around us, the moments we find ourselves in. Unlike traditional learning tools, AI can adapt to a learner’s context, making it uniquely suited to help us practice languages in ways that feel more natural and personal. This led me, along with a small group of colleagues, to experiment with the Gemini API, which enables developers to access the latest generative models from Google. The result is Little Language Lessons: a collection of three bite-sized learning experiments, all powered by Google’s Gemini models.\n\nExperiment 1, Tiny Lesson: Learning what you need, when you need it One of the most frustrating parts about learning a language is finding yourself in a situation where you need a specific word or phrase—and it’s one that you haven’t learned yet. That’s the idea behind Tiny Lesson. You describe a situation—maybe it’s “asking for directions” or “finding a lost passport”—and receive useful vocabulary, phrases, and grammar tips tailored to that context.\n\nSorry, your browser doesn't support playback for this video\n\nWe were able to accomplish this using a simple prompt recipe. The prompt begins with a persona-setting preamble that looks like this:\n\nYou are a(n) {target language} tutor who is bilingual in {target language} and {source language} and an expert at crafting educational content that is custom-tailored to students' language usage goals. Markdown Copied\n\nIn this prompt and in all of the prompts to come, we took advantage of Gemini’s ability to provide outputs as structured JSON, defining desired result as a list of keys in an object:\n\nFor the given usage context, provide a JSON object containing two keys: \"vocabulary\" and \"phrases\". The value of \"vocabulary\" should be an array of objects, each containing three keys: \"term\", “transliteration”, and \"translation\". The value of \"term\" should be a {target language} word that is highly relevant and useful in the given context. If the language of interest is ordinarily written in the Latin script, the value of “transliteration” should be an empty string. Otherwise, the value of “transliteration” should be a transliteration of the term. The value of \"translation\" should be the {source language} translation of the term. ... Markdown Copied\n\nIn total, each lesson is the result of two calls to the Gemini API. One prompt handles generating all of the vocabulary and phrases, and the other deals with generating relevant grammar topics. And the end of each prompt, we interpolate the user’s desired usage context as follows:\n\nINPUT (usage context): {user input} Markdown Copied\n\nExperiment 2, Slang Hang: Learning to sound less like a textbook There’s a moment in the journey of learning a language when you start feeling comfortable. You can hold conversations, express yourself, and mostly get by. But then you realize, you still sound… off. Too formal. Stiff. We built Slang Hang to help address this. The idea is simple: generate a realistic conversation between native speakers and let users learn from it. You can watch the dialogue unfold, revealing one message at a time and unpacking unfamiliar terms as they appear.\n\nSorry, your browser doesn't support playback for this video\n\nThe preamble for the Slang Hang prompt looks like this:\n\nYou are a screenwriter who is bilingual in {source language} and {target language} and an expert and crafting captivating dialogues. You are also a linguist and highly attuned to the cultural nuances that shape natural speech. Markdown Copied\n\nAlthough users can only reveal messages one at a time, everything—the setting, the conversation, the explanations for highlighted terms—is generated from a single call to the Gemini API. We define the structure of the JSON output as follows:\n\nGenerate a short scene that contains two interlocutors speaking authentic {target language}. Give the result as a JSON object that contains two keys: \"context\" and \"dialogue\". The value of \"context\" should be a short paragraph in {SOURCE LANGUAGE} that describes the setting of the scene, what is happening, who the speakers are, and speakers' relationship to each other. The value of \"dialogue\" should be an array of objects, where each object contains information about a single conversational turn. Each object in the \"dialogue\" array should contain four keys: \"speaker\", \"gender\", \"message\", and \"notes\". ... Markdown Copied\n\nThe dialogue is generated in the user’s target language, but users can also translate messages into their native language (a functionality powered by the Cloud Translation API). One of the more interesting aspects of this experiment is the element of emergent storytelling. Each scene is unique and generated on the fly—it could be a street vendor chatting with a customer, two coworkers meeting on the subway, or even a pair of long-lost friends unexpectedly reuniting at an exotic pet show. That said, we found that this experiment is somewhat susceptible to accuracy errors: it occasionally misuses certain expressions and slang, or even makes them up. LLMs still aren’t perfect, and for that reason it’s important to cross-reference with reliable sources.\n\nExperiment 3, Word Cam: Learning from your surroundings Sometimes, you just need words for the things in front of you. It can be extremely humbling to realize just how much you don’t know how to say in your target language. You know the word for “window”, but how do you say “windowsill”? Or “blinds”? Word Cam turns your camera into an instant vocabulary helper. Snap a photo, and Gemini will detect objects, label them in your target language, and give you additional words that you can use to describe them.\n\nSorry, your browser doesn't support playback for this video\n\nThis experiment leverages Gemini’s vision capabilities for object detection. We send the model an image and ask it for the bounding box coordinates of the different objects in that image:\n\nProvide insights about the objects that are present in the given image. Give the result as a JSON object that contains a single key called \"objects\". The value of \"objects\" should be an array of objects whose length is no more than the number of distinct objects present in the image. Each object in the array should contain four keys: \"name\", \"transliteration\", \"translation\", and \"coordinates\". ... The value of \"coordinates\" should be an integer array representing the coordinates of the bounding box for the object. Give the coordinates as [ymin, xmin, ymax, xmax]. Markdown Copied\n\nOnce the user selects an object, we send the cropped image to Gemini in a separate prompt and ask it to generate descriptors for that object in the user’s target language:\n\nFor the object represented in the given image, provide descriptors that describe the object. Give the result as a JSON object that contains a single key called \"descriptors\". The value of \"descriptors\" should be an array of objects, where each object contains five keys: \"descriptor\", \"transliteration\", \"translation\", \"exampleSentence\", \"exampleSentenceTransliteration\", and \"exampleSentenceTranslation\". ... Markdown Copied", "label": "non_personal"}
{"title": "Explore the latest updates on Google Wallet", "url": "https://developers.googleblog.com/en/explore-the-latest-updates-google-wallet-io-25/", "content": "Last year, we were thrilled to expand access to Google Wallet for users in more than 90 countries and territories. Recently we expanded Google Wallet adding +50 more countries, allowing users to view and use digital passes in the app and on the web. And we've worked hard to make Google Wallet even more robust, flexible, and ultimately, more feature-rich for you, our developer community.\n\nLet’s dive into the exciting new capabilities we’ve announced at our I/O session this year.\n\n\n\nDigital IDs: A foundation of trust, ease, and interoperability\n\nDigital IDs in Google Wallet are live today, built with trust, ease of use, and interoperability as our top priorities. We're committed to bringing more digital IDs to Google Wallet to support our users across the globe. Now, it’s easier to prove age and identity with Google Wallet.\n\nExpanding Availability: Residents in Arkansas, Montana, Puerto Rico and West Virginia will soon be able to save their government-issued digital IDs to Google Wallet. And in Arizona, Georgia, Maryland and New Mexico, users will also be able to use their mobile IDs at the DMV for improved and streamlined customer experiences.\n\nU.K. Passport Support: U.K. passport holders will soon be able to create digital ID passes with their U.K. passports and securely and conveniently store them in Google Wallet. At launch, we’re partnering with Rail Delivery Group, which will offer train travellers the opportunity to use their digital ID to verify that they meet the eligibility criteria for select Railcards on its Railcard retailing platform railcard.co.uk.\n\nNew use cases: New use cases are on the way in collaboration with our strong partner ecosystem. Soon, you'll be able to use your digital ID to recover Amazon accounts, access online health services with CVS Health and MyChart by Epic, verify profiles on platforms like Uber and more.\n\nIntroducing the Digital Credentials API: To empower you to leverage these digital IDs, we collaborated with ecosystem partners in the W3C to develop our Digital Credentials API. This unified and secure framework allows apps and websites to request verifiable proof of age or identity directly from any digital wallet on a user's device.\n\n\n\nConnecting with families: Google Wallet enabled for kids\n\nWe're very excited to provide parents and guardians a way to allow their children to access Google Wallet with appropriate supervision.\n\nParents and guardians in select countries can now allow their kids to tap and pay in stores, plus keep supported passes like event tickets, library cards, and gift cards, all in one place, on their Android devices. Safety is key, so parents have controls with Family Link. They will get notified by email about every transaction, and can easily track recent purchases, remove payment cards, and turn off passes access in Family Link.\n\nBest of all, there are no changes in the Google Wallet API to support this enhancement.\n\n\n\nElevating user engagement: more granular notifications\n\nLast year, we expanded mobile push notifications for the Google Wallet API, empowering you to deliver timely updates to your users. This year, we're taking it a step further with the introduction of field update notifications.\n\nImagine being able to trigger a push notification not just on a general update, but specifically when a particular field within a pass changes. For example, when a user's points balance crosses a threshold, or when their tier status is upgraded, a push notification can be triggered which will notify them that the data on their pass has changed. This granularity allows for a much more engaging and user-centric experience, driving higher engagement and utility on the pass.\n\n\n\nProximity power: introducing Nearby Passes Notifications\n\nSpeaking of notifications, we’re really excited to announce that we will support Nearby Passes notifications for the Google Wallet API. If enabled by the user, this feature is designed to provide timely and relevant information by alerting users about pertinent passes, such as loyalty cards, offers, boarding passes, or event tickets, when they approach a designated point of interest.\n\nThe Google Wallet app sends a contextual notification when the user is near a specific location. This notification serves as a direct gateway, allowing users to seamlessly access the associated pass with a single tap. This direct access promotes a more fluid and intuitive interaction with the stored passes, encouraging users to leverage the wallet's capabilities more effectively.\n\nTo ensure user control and flexibility, we’ve introduced two new toggles to help users control their notification experience. The first is on the pass details screen, that allows users to turn on or off notifications from that specific pass. This applies to all notifications related to your pass, including field updates, and nearby passes notifications. The second is through the Nearby Passes notifications channel that allows users to control whether they receive nearby passes notifications. This empowers users to tailor their notification settings based on their specific needs and preferences.\n\n\n\nBeyond transactions: unlocking Value Added Opportunities\n\nEngaging users goes beyond just notifications. To address this, we’re introducing Value Added Opportunities which empowers you to integrate personalized modules directly into your passes, showcasing relevant deals, promotions, and additional services.\n\nThis is a significant step towards transforming passes into a more dynamic engagement surface for you and your users. By highlighting these value-added benefits, such as exclusive offers or upgrade options, you can guide users back to your app or website, creating a dynamic gateway for ongoing user interaction.\n\n\n\nBridging the gap: introducing the Pass Upgrade experience\n\nUsers have saved millions of passes by manually adding their loyalty cards to Google Wallet. These passes are unlinked from their merchant accounts, and, as a developer, you can’t update, or engage users on these passes. We’ve now introduced a Pass Upgrade experience that prompts users to sign into their merchant account and save a linked version of the pass. All you need to do is integrate with the Wallet API’s User Loyalty Program Enrollment feature, and ensure that users can save the linked pass once they successfully sign in. We’ll be expanding on this feature in the future to enable users who have added a pass using our “everything else” feature to link to the merchant account as well.\n\n\n\nSeamless journeys: enhancements for travel\n\nAt last year's I/O, we announced Auto Linked Passes, allowing you to add an additional related pass automatically to your users’ Google Wallet provided they already have an existing pass issued by you. Now, we're happy to announce an expansion of this feature for airlines. Developers from airlines that integrate loyalty cards for their frequent flyer programs with the Google Wallet API can automatically push boarding passes to their users’ wallets once they check in for a flight.\n\nGoogle Wallet users already benefit from streamlined travel on open-loop EMV transit systems, using tokenized payment methods for seamless fare transactions. Our solution further enhances this experience by providing riders with detailed journey and fare construction details. The tokenized open loop payment card acts as a bridge between user payments and transit systems, facilitating this communication.\n\nBuilding on this foundation, we're excited to announce the upcoming expansion of Google Transit Insights to support specific pass types sales, such as season passes, directly linked to the users' tokenized open loop payment card. With this new capability, developers will soon be able to leverage the Google Wallet APIs to implement seamless pass purchasing and management, eliminating the need for separate transit cards.\n\nUsers of Google Wallet will now see real-time transit pass updates, such as on-time or delayed train status, directly on their passes. This enhanced experience is powered by the Google Wallet API's new live status support and a seamless integration with Google Maps. Train operators can enable this feature by ensuring their tickets contain the required fields and providing a real-time GTFS feed, making it accessible to major operators worldwide.\n\n\n\nPersonalization and security: Secure Private Images\n\nFor some use cases, it’s important for you to create a pass that includes a profile photo of the user you’re distributing the pass to. To achieve this, we’re enabling Secure Private Images on passes.\n\nWith this feature, you’re able to define passes that include images that are only accessible to the holder of the pass. For example, this allows you to create digital business cards, membership passes, or event tickets with the user profile image on it.\n\nPlease note that this feature can’t be used for official identity verification.\n\n\n\nLearn more and build with Google Wallet\n\nGoogle Wallet continues to evolve, offering developers powerful new capabilities to create richer, more engaging user experiences. From expanding digital identity features like the Digital Credentials API and secure private images, to enhancing user engagement with granular notifications and Value Added Opportunities, and streamlining travel with expanded Auto Linked Passes and real-time transit updates, these features are designed to help you build innovative solutions that connect more deeply with your users and unlock new value within the Google Wallet platform.\n\nPlease take a look at the following resources to learn more:\n\nLearn everything about Google Wallet in the developer documentation website.\n\nStay tuned with upcoming and past events at our events page.\n\nTry one of our codelabs to have a hands-on code experience.\n\nUse one of our client libraries for your favorite language/platform.", "label": "non_personal"}
{"title": "Bringing Gemini intelligence to Google Home APIs", "url": "https://developers.googleblog.com/en/bringing-gemini-intelligence-to-google-home-apis/", "content": "The smart home is rapidly evolving into an intuitive ecosystem to make life easier, and its next era will be powered by Gemini and the Home APIs. This isn't just about connected devices; it's about creating effortless experiences. With the Home APIs, our goal has been to empower all developers to build innovative devices and experiences for the home. Now, with Gemini in the Home APIs, we're taking the next step: bringing the best of Google's AI directly to you. We're moving beyond simple device control to create an effortless smart home that truly understands, adapts, and responds to your users' needs. At Google I/O 2024, we announced the Home APIs, providing app developers with access to over 600M devices. We are excited to share that our ecosystem has grown even more to over 750M devices that developers now have access to along with Google's hubs and Matter infrastructure, and an automation engine powered by Google intelligence. We’ve spent time rolling it out to a few early access partners, our Android and iOS SDKs are in public developer beta, and some developers have already leveraged the Home APIs to release new apps on Android.\n\nPartner experiences built with Home APIs Last year, we shared the innovative new ways partners like ADT, LG, and Eve built on Google Home, and now there are even more partners showcasing how Home APIs are making their customer experience even better:\n\nFirst Alert Control your smoke alarm from the First Alert app or the Google Home app and seamlessly interconnect with your existing Nest Protects.\n\nYale Yale’s upcoming Matter lock, the successor to the Nest x Yale lock, takes advantage of the best-in-class lock features in the Google Home app, built using the Home APIs.\n\nCync Imagine your home automatically adjusting lighting and fan settings to ensure your pet's comfort when you're away. Cync is making this a reality.\n\niRobot Select iRobot Roomba® robots can create automations using Google Home presence sensing, so they can automatically clean your home when you leave the house, ensuring a spotless return.\n\nMotorola Moto Tag You can create custom smart home routines triggered by simple tag interactions, offering unparalleled personalization.\n\nTuya Smart Tuya Smart is enhancing seamless interoperability. Now, users can easily set up a Matter device and control devices connected to Google directly in Tuya Smart app.\n\nBringing your cameras to life with Gemini-powered Home APIs Last fall, we introduced Gemini-powered camera features in public preview in the Google Home app, allowing users to ask natural questions like, “Did the kids leave their bikes in the driveway?” and instantly get relevant video clips. Now we are bringing those camera experiences directly to developers too.\n\nWe're including the standard camera features you'd expect – like live streaming, event history access, two-way talk capabilities, and camera settings. But we're going further by integrating the Gemini-powered intelligence that our users love, such as AI descriptions and the ability to search camera history, making it easier to quickly identify what you are looking for in your camera history, keeping you and your family safer.\n\nMaking automation effortless with Gemini Figuring out the perfect automation to help improve your home experience and implementing it can be a daunting task many users don’t want to undertake. So, we’re introducing new Gemini-powered features to the Automations API designed to make creating powerful routines easier than ever: Suggested Automations: Gemini intelligently analyzes the devices in a user's home and proactively suggests potentially useful automations they might not have thought of.\n\nHelp me create: Building automations becomes as simple as a conversation. Users can tell Gemini what they want to achieve using natural language, and the automation is drafted for them.\n\nNew Automation Starters: We're adding more sophisticated triggers based on dates and weather conditions, allowing automations to respond more dynamically to the complexities of real life.\n\nThese new features will enable you to offer unprecedented Gemini-powered intelligent capabilities to your users more quickly than ever before.\n\nGemini across the Google Home surfaces The benefits don't stop within your app. When you integrate your devices using the Google Home APIs, they can participate in Gemini-powered experiences across Google's surfaces.\n\nFor instance, Google Home users, while in the Gemini app, can control and inquire about their smart home devices using natural language. We've also previewed Gemini enhancing the voice experience on smart speakers, smart displays, and Google TV, enabling more natural interaction, deeper exploration of topics, device control, and even voice-based automation creation. And we are testing a Home Summary Widget on Pixel with a select set of users, providing insights about your home without having to open an app!", "label": "non_personal"}
{"title": "Google Pay inside sandboxed iframe for PCI DSS v4 compliance", "url": "https://developers.googleblog.com/en/google-pay-inside-sandboxed-iframe-for-pci-dss-v4-compliance/", "content": "Using a sandboxed iframe satisfies any concerns with compliance since scripts within the iFrame will not have access to the parent DOM. See the following illustration for an example:\n\nOne way to comply with this requirement is to use a technique like Subresource Integrity (SRI) . However, the Google Pay JavaScript (pay.js) build and release process does not allow for a long-lived, stable hash required by techniques like SRI.\n\nIf you are developing or maintaining a checkout page you might come across PCI DSS v4 which includes the following requirement under 6.4.3:\n\nIn this case the domain “cdn.somewhereelse.com” would load Google Pay’s pay.js JavaScript file. After a successful transaction, the inner iframe can communicate with the parent page through mechanisms like window.postMessage() if needed.\n\nIn order for Google Pay to work in all browsers we need the following 4 sandbox attribute values in addition to allow=”payment” :\n\nallow-scripts\n\nTo allow the iframe to execute scripts (pay.js as an example)\n\nallow-popups\n\nAllows the embedded page to create 'child browsing contexts'. In practice, this flag enables the embedded iframe to open new tabs and windows when the user clicks a link.\n\nallow-same-origin\n\nIf not set, fails on various occasions for browsers. If set, the iframe has access to the parents storage and cookies.\n\nallow-forms\n\nAllows forms such as the Google Pay login to submit the data.\n\nSee this test page to see the various iframe sandbox values in action.\n\n\n\nShopify successfully certified for PCI DSS v4\n\nGoogle Pay partnered with Shopify to implement the above solution. Shopify was able to successfully pass the PCI DSS v4 audit by using a sandboxed iframe to display the Google Pay button. Here is what Shopify has to say:\n\nWe’ve built Shopify Checkout in such a way that Google Pay code executes in a secure sandboxed environment, allowing us to maintain the integrity of our checkout and comply with PCI DSS V4 requirements.\n\n\n\n– Ilya Grigorik, Distinguished Engineer at Shopify\n\nFor more information on how Shopify built their checkout solution using sandboxed iframes, their “Powering Shopify’s High-Performance, PCI DSS v4 Compliant Checkout with Sandboxing” blog post has the insights.\n\n\n\nConclusion\n\nWrapping your Google Pay integration in a sandboxed iframe can help you to comply with PCI DSS v4 requirements. For more assistance with your implementation, sign in to the Google Pay & Wallet Console to create a support ticket. In addition, you can join the developer community in the #payments channel on Discord.\n\nFollow @GooglePayDevs on X for future updates. If you have questions, tag @GooglePayDevs and include #AskGooglePayDevs in your tweets.", "label": "non_personal"}
{"title": "Streamlining LLM Inference at the Edge with TFLite", "url": "https://developers.googleblog.com/en/streamlining-llm-inference-at-the-edge-with-tflite/", "content": "Optimizing Time to First Token and Peak Memory Usage with a Smarter Cache for XNNPack\n\nXNNPack is the default TensorFlow Lite CPU inference engine for all models. It delivers game changing speedups across mobile, desktop, and Web platforms. One of the optimizations employed in XNNPack is repacking the static weights of the Convolution, Depthwise Convolution, Transposed Convolution, and Fully Connected operators into an internal layout optimized for inference computations. During inference, the repacked weights are accessed in a sequential pattern that is friendly to the processors’ pipelines. The inference latency reduction comes at a cost: repacking essentially creates an extra copy of the weights inside XNNPack. Previous efforts have been made to reduce that cost by adding an in-memorycache to XNNPack. This cache allows sharing the packed weights between independent TFLite interpreters that would run the same model independently. TFLite XNNPack delegate implementation has been improved to address some of the shortcomings of the existing cache.\n\n1. The cache lives in anonymous memory, which incurs swapping to disk in case of memory pressure, leading to poor performance. 2. It requires repacking the initial weights every time a process is started. 3. Because repacking reads the original TFLite weights and writes to a new buffer, this leads to a high peak memory usage during the packing. 4. It requires tedious steps and careful lifecycle management to properly enable caching through XNNPack delegate. 5. It doesn’t allow sharing the weights across processes.\n\n.\n\nThe New XNNPack Cache Provider Interface XNNPack has been updated and provides an interface that lets you implement a weight cache provider. A weight cache provider behaves as a dictionary that XNNPack will fill and query in order to access packed buffers. Here are its main functions. look_up looks up a packed buffer key and returns a unique identifier (or a special identifier reserved for NotFound) that may be later used to retrieve the buffer address. reserve_space reserves a buffer that may be used to store information of a given size. That buffer then needs to be committed using look_up_or_insert . look_up_or_insert checks if a buffer matching the given key exists in the cache provider. If not, the given data is committed to the cache provider. This function also returns the identifier that may be used to retrieve the buffer address. offset_to_addr returns the buffer address from the identifier returned by look_up and look_up_or_insert . The interactions between XNNPack and the weight cache provider are illustrated in the following diagram.\n\n.\n\nLoading the Cache From Disk with MMAP in the TFLite Delegate The TFLite Delegate now uses this new interface and has its own weight cache provider. This provider is capable of saving and loading the packed weights directly to / from disk. TFLite has been leveraging flatbuffer and file-backed memory mapping for a long time. We are filling the gap here by leveraging the same technique, for the following advantages.\n\nIt eliminates the repacking overhead. Persisting packed weights on disk bypasses the costly repacking process each time a model is loaded. This translates to a significant reduction in both startup latency and peak memory usage. Even for the initial building, this offers packed data deduplication and further improves packing performance by avoiding repacking the same data again.\n\nIt improves memory management. mmap leverages the operating system's virtual memory management allowing it to optimize overall system memory usage and performance. In our case, this is especially advantageous for random access bulky read-only file access, like a neural network’s operation’s constant weights for instance. With packed data stored on disk, the XNNPack cache no longer relies on anonymous memory which can be prone to performance issues under memory pressure. Instead, it leverages the operating system's virtual memory management for smoother operation. By eliminating the need to copy data between the file system and memory, mmap significantly reduces overhead and speeds up access times. You can find more information about file mappings and memory usage directly from mmap’s man page and other interesting reads.\n\nIt allows cross-process collaboration. mmap -based file loading opens the door for seamless weight sharing between multiple processes as each process’ virtual address space maps to the same physical memory pages. This not only reduces the overall memory footprint as multiple processes share the same memory but also accelerates model loading across the board.\n\n.\n\nIt simplifies the user facing API. Instead of requiring the user to setup and manage the cache object throughout the application lifetime, they can simply provide a path to the cache file.\n\nstd::unique_ptr<tflite::Interpreter> interpreter; // Setup the options for the XNNPack delegate. TfLiteXNNPackDelegateOptions xnnpack_options = TfLiteXNNPackDelegateOptionsDefault(); xnnpack_options.weight_cache_file_path = \"/tmp/cache_file.xnn_cache\"; // Create and apply the XNNPack delegate to a TFLite interpreter. // Static weights will be packed and written into weights_cache on the first run. // They will be automatically loaded for all other runs. TfLiteDelegate* delegate = TfLiteXNNPackDelegateCreate(&xnnpack_options); interpreter->ModifyGraphWithDelegate(delegate); C++ Copied\n\nMaintaining Cache Integrity To guarantee accurate and efficient inference, it's crucial to invalidate the XNNPack cache under specific conditions: Model Evolution: if your model's weights or structure change, the cached data becomes outdated and must be invalidated. This means removing the file at the provided cache path. XNNPack Upgrades: updates to XNNPack's internal packing algorithm may result in incompatible cached weights, requiring the cache to be recomputed. Fortunately XNNPack is capable of detecting this and will replace the existing cache automatically. In essence, any modification that could impact the way weights are packed or utilized by XNNPack should trigger a cache invalidation.\n\nBenchmarks The session initialisation is dominated by the weight packing. For LLMs several subgraphs are reusing the same weights. Building the cache is faster because the deduplication functionality avoids packing those same weights multiple times. For more standard models, like stable diffusion, there is no deduplication and the slightly higher initialisation time is due to saving the cache to disk. Reloading the cache (from the 2nd run on) brings the initialisation down to a fraction of the previous time in all the cases. The session initialisation improvement naturally affects the time to the first token for LLMs, roughly dividing it by 2 in the benchmarks. The memory gains brought by the cache implementation can also be seen. The peak Resident Set Size is lowered for LLMs thanks to the deduplication. For other models that don’t benefit from the deduplication, there is no change. Reloading the cache brings the peak RSS even further down because the TFLite original models aren’t read anymore and therefore never get pulled into memory.\n\nGemma 2B on a Pixel 8 Pro\n\n.\n\nPhi2 on a Pixel 8 Pro\n\n.\n\nStable Diffusion on a Pixel 8 Pro\n\n.", "label": "non_personal"}
{"title": "TensorFlow Lite is now LiteRT", "url": "https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/", "content": "LiteRT, part of the Google AI Edge suite of tools, is the runtime that lets you seamlessly deploy ML and AI models on Android, iOS, and embedded devices. With AI Edge's robust model conversion and optimization tools, you can ready both open-source and custom models for on-device development.\n\nSince its debut in 2017, TFLite has enabled developers to bring ML-powered experiences to over 100K apps running on 2.7B devices. More recently, TFLite has grown beyond its TensorFlow roots to support models authored in PyTorch , JAX , and Keras with the same leading performance. The name LiteRT captures this multi-framework vision for the future: enabling developers to start with any popular framework and run their model on-device with exceptional performance.\n\nLiteRT (short for Lite Runtime) is the new name for TensorFlow Lite (TFLite). While the name is new, it's still the same trusted, high-performance runtime for on-device AI, now with an expanded vision.\n\nThis change will roll out progressively. Starting today, you’ll see the LiteRT name reflected in the developer documentation, which is moving to ai.google.dev/edge/litert, and in other references across the AI Edge website. The documentation at tensorflow.org/lite now redirects to corresponding pages at ai.google.dev/edge/litert.\n\nThe main TensorFlow brand will not be affected, nor will apps already using TensorFlow Lite.\n\n\n\nHow to access LiteRT\n\nOur goal is that this change is minimally disruptive, requiring as few code changes from developers as possible.\n\nIf you currently use TensorFlow Lite via packages, you’ll need to update any dependencies to use the new LiteRT from Maven, PyPi, Cocoapods.\n\nIf you currently use TensorFlow Lite via Google Play Services, no change is necessary at this time.\n\nIf you currently build TensorFlow Lite from source, please continue building from the TensorFlow repo until code has been fully moved to the new LiteRT repo later this year.\n\n\n\nFrequently asked questions\n\n\n\n1. What is changing beyond the new name, LiteRT?\n\nFor now, the only change is the new name, LiteRT. Your production apps will not be affected. With a new name and refreshed vision, look out for more updates coming to LiteRT, improving how you deploy classic ML models, LLMs, and diffusion models with GPU and NPU acceleration across platforms.\n\n\n\n2. What’s happening to the TensorFlow Lite Support Library (including TensorFlow Lite Tasks)?\n\nThe TensorFlow Lite support library and TensorFlow Lite Tasks will remain in the /tensorflow repository at this time. We encourage you to use MediaPipe Tasks for future development.\n\n\n\n3. What’s happening to TensorFlow Lite Model Maker?\n\nYou can continue to access TFLite Model Maker via https://pypi.org/project/tflite-model-maker/\n\n\n\n4. What if I want to contribute code?\n\nFor now, please contribute code to the existing TensorFlow Lite repository. We’ll make a separate announcement when we’re ready for contributions to the LiteRT repository.\n\n\n\n5. What’s happening to the .tflite file extension and file format?\n\nNo changes are being made to the .tflite file extension or format. Conversion tools will continue to output .tflite flatbuffer files, and .tflite files will be readable by LiteRT.\n\n\n\n6. How do I convert models to .tflite format?\n\nFor Tensorflow, Keras and Jax you can continue to use the same flows. For PyTorch support check out ai-edge-torch.\n\n\n\n7. Will there be any changes to classes and methods?\n\nNo. Aside from package names, you won’t have to change any code you’ve written for now.\n\n\n\n8. Will there be any changes to TensorFlow.js?\n\nNo, TensorFlow.js will continue to function independently as part of the Tensorflow codebase.\n\n\n\n9. My production app uses TensorFlow Lite. Will it be affected?\n\nApps that have already deployed TensorFlow Lite will not be affected. This includes apps that access TensorFlow Lite via Google Play Services. (TFLite is compiled into the apps at build time, so once they’re deployed, apps have no dependency.)\n\n\n\n10. Why “LiteRT”?\n\n“LiteRT” (short for Lite Runtime) reflects the legacy of TensorFlow Lite, a pioneering “lite”, on-device runtime, plus Google’s commitment to supporting today’s thriving multi-framework ecosystem.\n\n\n\n11. Is TensorFlow Lite still being actively developed?\n\nYes, but under the name LiteRT. Active development will continue on the runtime (now called LiteRT), as well as the conversion and optimization tools. To ensure you're using the most up-to-date version of the runtime, please use LiteRT.\n\n\n\n12. Where can I see examples of LiteRT in practice?\n\nYou can find examples for Python, Android, and iOS in the official LiteRT samples repo.\n\n\n\nWe’re excited for the future of on-device ML, and are committed to our vision of making LiteRT the easiest to use, highest performance runtime for a wide range of models.", "label": "non_personal"}
{"title": "Learn to build and run AI powered apps at Firebase Demo Day ‘24", "url": "https://developers.googleblog.com/en/firebase-demo-day-24/", "content": "Welcome to Firebase Demo Day 2024\n\nWe just released 8 bite sized demo videos to showcase how Firebase helps you build and run AI-powered apps. We’ll show you how to use new Firebase products and features like Firebase Genkit, Vertex AI in Firebase, Gemini in Firebase and Firebase App Hosting, to build AI features into your existing applications, monitor their performance, and create great experiences for your users.\n\nTo bring these concepts to life, we'll take you on an app dev journey through Compass, our sample travel app. We’ll demonstrate how you can use Firebase to create features like personalized recommendations, smart itineraries, AI-powered chatbots, and more. Follow along as we highlight how you can leverage Firebase tools to add the same cutting-edge functionality to your own apps.\n\n\n\nWatch Firebase Demo Day 2024 from anywhere at any time, at your own pace.\n\n\n\n\n\n\n\nDemos to build AI-powered features\n\nWatch as we transform our travel app with the power of Firebase and AI. Our build demos show you how to build and deploy AI features with new Firebase products like Vertex AI, Genkit and Firebase Hosting, all while leveraging Firebase's fully managed infrastructure to get to market quickly and securely.\n\n\n\nCall Gemini from your Android app\n\nIntegrate the power of Gemini directly into your Android app using the native Vertex AI in Firebase SDK for Android to make calls to Gemini.", "label": "non_personal"}
{"title": "Celebrating Flutter’s “Production Era”", "url": "https://developers.googleblog.com/en/celebrating-flutters-production-era/", "content": "This article is cross posted on Flutter\n\nJust over six years ago, we unveiled Flutter 1.0. Today, at #FlutterInProduction, we’re celebrating how far we’ve come — from the immense support we’ve received from thousands of contributors in the community, to the widespread adoption of Flutter as a production-grade app framework for building multi-platform app experiences. If you haven’t experienced Flutter yet, we invite you to try it! As we shared today, you’d be joining a big group: Flutter has over 1 million monthly active developers across the globe, and powers nearly 30% of all new iOS apps. More than 90 thousand developers actively participate in Flutter Meetups across more than sixty countries. And if you want input on designing or building a new successful Flutter app, we have a large and growing list of Flutter Consultants ready to help you. “Apptopia tracks millions of apps in the Apple AppStore and Google Play Store, and analyzes and detects which developer SDKs were used to create the apps. Flutter is one of the most popular SDKs we track: In the Apple AppStore it has grown steadily in usage from around 10% of all tracked free apps in 2021 to nearly 30% of all tracked free apps in 2024!”\n\n— Apptopia Inc. A decade of innovation to reach the production era It’s been an incredible journey, starting in 2014 (in what we now call our experimental era) as a Google experiment codenamed “Sky.” Before Flutter, compromises were inevitable. Many developers have become skeptical that any framework can truly deliver a premium experience across multiple platforms. With the launch of Flutter 1.0 in 2018 we had a clear mission to resolve that technology dilemma: We aimed to empower developers with the ultimate app framework for crafting beautiful, high-performance user interfaces across all platforms. Also, to enable developers to reach all customers with high-quality apps on all the platforms that customers care about, but with lower cost and in less time. Our focus has remained constant through Flutter’s growth era, even as we’ve added support for the six major platforms across mobile, web, and desktop — and continue to push beyond, with work like Toyota’s use of Flutter for infotainment systems.\n\nWe’re now in the “production era,” and we’re celebrating that with #FlutterInProduction! This event spotlights the achievements of developers using Flutter in real-world applications.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nBuilding in partnership with the community None of this would be possible without our amazing community of over 1,400 contributors, more than 10,000 package publishers authoring over 50,000 packages, and passionate Flutter content creators and enthusiasts. Together, we’ve built a top-5 GitHub open-source project by contributions!\n\nAmazing user experiences It all starts with a focus on enabling amazing user experiences. Free from typical platform constraints, Flutter supports a broad set of design languages — support for Material Design and our Apple-inspired Cupertino widgets comes with the SDK. The ecosystem also provides a broad selection of design libraries like Windows-inspired fluent_ui , macOS-inspired macos_ui , and the Ubuntu-inspired yaru widgets.\n\nScandinavian Airlines design awards With Flutter, you have the flexibility and power to realize any design your design team envisions. This is exemplified by Scandinavian Airlines, who after creating their new mobile app with Flutter have filled their trophy case with prestigious design awards such as the Red Dot Design Award, the Webby People’s Voice Award, and the iF Design Gold Award. Charlotte Svensson, EVP & CIO at SAS explains: “I’m extremely proud over this award, which is not just an industry award, but a global recognition. It’s a testament to what we can do, when we go above-and-beyond in focusing on improving the customer experience, and when we interact and develop together with our customers. SAS has always been at the forefront of innovation in the aviation industry, and this award serves as a validation of its dedication to providing exceptional digital solutions for our customers.”\n\nGreat performance & reliability Performance and reliability are crucial for a positive user experience and brand perception. Slow or crash-prone apps not only frustrate users in the short term but can also damage your brand reputation in the long run through negative reviews and word-of-mouth. Flutter has prioritized performance and reliability from the outset. By choosing the Dart programming language, we ensure fast startup times through ahead-of-time compilation to native machine code or web assembly. Dart’s rich, null-safety type system helps catch errors during development, further enhancing reliability. Additionally, Flutter’s custom Impeller rendering engine, designed specifically for multi-platform UI, delivers smooth animations and gives us full control over the rendering stack, top to bottom, from the UI source code to the GPU.\n\nUniversal Studios performance and reliability For example, Universal Destinations and Experiences recently reported that by adopting Flutter, they not only decreased their app size — a significant benefit for users with unreliable internet connections — but also dramatically reduced app crashes to near zero, thus lowering their total cost of ownership.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nLG Electronics performance LG Electronics has traditionally relied on web apps for their webOS-powered smart TVs due to concerns about the high development cost of traditional native apps. However, they found that web apps launch slower and consume more memory than native apps. With Flutter, LG Electronics has a solution that combines fast development speed and excellent performance. As a result, they plan to use Flutter for key applications in webOS TVs globally starting in 2025.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nFirst-class developer experience and thriving ecosystem Flutter’s success is deeply rooted in its focus on developer experience. We pioneered instant developer workflows with Stateful Hot Reload, and during our growth era added Flutter DevTools to significantly accelerate diagnostics and debugging workflows. Flutter’s community provides a thriving and open ecosystem of over 50,000 packages published by over 10,000 publishers, combined with robust third-party services & technologies. Also, if you want input on designing or building a new successful Flutter app, we have a large list of Flutter Consultants ready to help you.\n\nMGM and developer productivity App agency Superformula has built with Flutter since August 2020. They found that Flutter is easy to learn and well documented, enabling them to get new team members up to speed quickly and contribute effectively. Superformula also used Flutter to revitalize the digital dining experience for MGM Resorts’ 400+ restaurants. The new Flutter-based MGM Rewards app was rebuilt in just 4 months, cutting the total amount of code in half, and improving delivery speed by a factor of 4. One core enabler of productivity for Superformula is the ability to share code across mobile, tablet-based kiosks, and web-based tools.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nGEICO user interface elements shared across web, iOS, and Android.", "label": "non_personal"}
{"title": "New Google Pay features to enhance your payment flows", "url": "https://developers.googleblog.com/en/new-google-pay-features-to-enhance-your-payment-flows/", "content": "At Google I/O 2025, we unveiled updates across the Google Pay API designed to help you create smoother, safer, and more successful checkout experiences for your users. Whether you're looking to boost conversions, enable new payment scenarios, enhance security, or simplify your integration, there's something new for you. Let's dive into the key announcements developers need to know.\n\nEnhancing the checkout experience and conversion Google Pay in Android WebViews Big news! Starting with Chrome v137, users can seamlessly use Google Pay within Android WebViews, accessing an Android native experience and device tokens from their Google Wallet. Simply enable PaymentRequest in your app manifest, and tap into the opportunities of in-app browser purchases with a high-quality, secure form of payment. Take a look at the integration guide to learn more.\n\nSorry, your browser doesn't support playback for this video Figure 1: A sample checkout flow via a WebView on Android that uses Google Pay to complete the payment\n\nA more versatile API to power modern checkout flows We are introducing improvements to the Google Pay API to help you adapt to a payment ecosystem that is in continuous evolution. Here are some of our favorite updates: The Google Pay payment sheet now features richer card art and names, helping users select their preferred card faster. The payment sheet also supports dark mode for a more integrated feel within your application.\n\nFigure 2: Screenshots showcasing the Google Pay payment sheet in their dark and light versions.\n\nBuilding on last year's success, the createButton API for Web now offers more customization options (show/hide border, more button text options) to better match your UI, and continue to help boost sales by showing card details upfront. Need to show card-identifying information without using a payment button? We're introducing a new API in the coming months to enable this use case.\n\nfigure 3: An example offering Google Pay using a list selector through the Payment Metadata API\n\nWe're adding support for Merchant-Initiated Transactions (MITs) (subscriptions, auto-reloads, deferred charges) to the Google Pay Online API. This includes details in the payment sheet to inform users, device-independent tokens for payment continuity (even if users change devices), and lifecycle notifications for underlying card changes.\n\nStreamlining the developer experience We are dedicated to making the Google Pay API easier to integrate, test and maintain. Here are some updates that improve the integration experience: Testing just got easier. We have improved the test card suite, so you can now see relevant test cards (regular, tokenized, debit) for your specific PSP directly in the payments sheet when using the TEST environment. Debug your integrations faster on Android with more fine-grained build time error logs to amend your logic more easily, and detailed exceptions/error codes at runtime. Check out the troubleshooting guide if you are seeing errors in your integration.\n\n\n\nFigure 4: More detailed error messages are now surfaced via the Logcat and the debugger\n\nWe've launched new codelabs, Firebase Studio templates (one-click development environments), and a learning pathway for web developers. We are planning to add similar resources for native Android, Flutter, React JS, React Native, and Angular. Stay informed about the status of the Google Pay API with the new Google Pay API Status Dashboard. The dashboard monitors key APIs like the CreateButton, IsReadyToPay, or LoadPaymentData APIs in real-time. Check the availability of the API (99.99% uptime last year!) and get incident updates instantly.\n\nFigure 5: The Google Pay API Status Dashboard includes service uptime and health information.", "label": "non_personal"}
{"title": "Exploring the Magic Mirror: an interactive experience powered by the Gemini models", "url": "https://developers.googleblog.com/en/magic-mirror-interactive-experience-powered-by-gemini-models/", "content": "Imagine gazing into a mirror and seeing not just your reflection, but a gateway to information, creativity, and a touch of enchantment. This is precisely what the Gemini backed Magic Mirror project brings to life. Moving beyond a simple display, this project showcases the incredible interactive capabilities of the Gemini API and JavaScript GenAI SDK, transforming a familiar object into a new chat interface.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nThis project creates its interactive experience using several features of the Gemini API:\n\n1: Fluid, Real-Time Conversations with the Live API The foundation of the magic mirror's interactivity is the Live API. This allows for continuous, real-time voice interactions. You speak, and the mirror doesn't just listen for a single command, it engages in a flowing conversation by processing your speech as you talk, allowing for a more natural back-and-forth dialogue in either text or audio. On top of this, the Live API is able to understand when you’re speaking during playback and interpret that interruption to pivot the narrative and conversation based on your inputs, allowing for dynamic audible conversations alongside text.\n\nLink to Youtube Video (visible only when JS is disabled)\n\n2: The enchanted storyteller On top of being able to have a conversation through the Live API, the magic mirror can also be customized to weave tales, all thanks to the Gemini model's advanced generation capabilities by providing specific system instructions and updating speech configurations during initialization to include different dialects or accents, voices, and a variety of other attributes.\n\nLink to Youtube Video (visible only when JS is disabled)\n\n3: Instant information: grounding with Google Search While conversations and stories are great, sometimes you want to be able to know about the world around you as it’s happening. This magic mirror project leverages the model’s ability to integrate with Grounding with Google Search, providing grounded, up-to-date information.\n\nLink to Youtube Video (visible only when JS is disabled)\n\n4: Visual alchemy: image generation on command Using Function Calling with the Gemini API, the magic mirror is able to generate visuals based on your descriptions, adding depth to stories and deepening the experience of interacting with the Gemini model. The Gemini model determines that your request requires image generation and calls a predefined function based on stated characteristics, passing along the detailed prompt it derives from your spoken words.\n\nLink to Youtube Video (visible only when JS is disabled)", "label": "non_personal"}
{"title": "Gemini 2.5: Updates to our family of thinking models", "url": "https://developers.googleblog.com/en/gemini-2-5-thinking-model-updates/", "content": "Today we are excited to share updates across the board to our Gemini 2.5 model family: Gemini 2.5 Pro is generally available and stable (no changes from the 06-05 preview) Gemini 2.5 Flash is generally available and stable (no changes from the 05-20 preview, see pricing updates below) Gemini 2.5 Flash-Lite is now available in preview Gemini 2.5 models are thinking models, capable of reasoning through their thoughts before responding, resulting in enhanced performance and improved accuracy. Each model has control over the thinking budget, giving developers the ability to choose when and how much the model “thinks” before generating a response.\n\nOverview of our family of Gemini 2.5 thinking models\n\nIntroducing Gemini 2.5 Flash-Lite Today, we’re introducing 2.5 Flash-Lite in preview with the lowest latency and cost in the 2.5 model family. It’s designed as a cost-effective upgrade from our previous 1.5 and 2.0 Flash models. It also offers better performance across most evals, and lower time to first token while also achieving higher tokens per second decode. This model is great for high throughput tasks like classification or summarization at scale. Gemini 2.5 Flash-Lite is a reasoning model, which allows for dynamic control of the thinking budget with an API parameter. Because Flash-Lite is optimized for cost and speed, “thinking” is off by default, unlike our other models. 2.5 Flash-Lite also supports all of our native tools like Grounding with Google Search, Code Execution, and URL Context in addition to function calling.\n\nBenchmarks for Gemini 2.5 Flash-Lite\n\nOver the last year, our research teams have continued to push the pareto frontier with our Flash model series. When 2.5 Flash was initially announced, we had not yet finalized the capabilities for 2.5 Flash-Lite. We also launched with a “thinking” and “non-thinking price”, which led to developer confusion.\n\n\n\nWith the stable version of Gemini 2.5 Flash rolling out (which is the same 05-20 model preview we made available at Google I/O), and the incredible performance of 2.5 Flash, we are updating the pricing for 2.5 Flash: $0.30 / 1M input tokens (*up from $0.15 input) $2.50 / 1M output tokens (*down from $3.50 output) We removed the thinking vs. non-thinking price difference We kept a single price tier regardless of input token size While we strive to maintain consistent pricing between preview and stable releases to minimize disruption, this is a specific adjustment reflecting Flash’s exceptional value, still offering the best cost-per-intelligence available. And with Gemini 2.5 Flash-Lite, we now have an even lower cost option (with or without thinking) for cost and latency sensitive use cases that require less model intelligence.\n\nPricing updates for our Gemini Flash family\n\nIf you are using the Gemini 2.5 Flash Preview 04-17 , the existing preview pricing will remain in effect until its planned deprecation on July 15, 2025, at which point that model endpoint will be turned off. You can transition to the generally available model “gemini-2.5-flash”, or switch to 2.5 Flash-Lite Preview as a lower cost option.\n\nContinued growth of Gemini 2.5 Pro The growth and demand for Gemini 2.5 Pro continues to be the steepest of any of our models we have ever seen. To allow more customers to build on this model in production, we are making the 06-05 version of the model stable, with the same pareto frontier price point as before. We expect that cases where you need the highest intelligence and most capabilities are where you will see Pro shine, like coding and agentic tasks. Gemini 2.5 Pro is at the heart of many of the most loved developer tools.\n\nTop developer tools using Gemini 2.5 Pro", "label": "non_personal"}
{"title": "Multilingual innovation in LLMs: How open models help unlock global communication", "url": "https://developers.googleblog.com/en/unlock-global-communication-gemma-projects/", "content": "We are thrilled to celebrate the incredible contributions of the community to the Unlock Global Communication with Gemma competition on Kaggle! Developers tackled the critical challenge in AI of adapting state-of-the-art large language models (LLMs) for diverse cultural and linguistic contexts.\n\nModels often exhibit a bias towards high-resource languages due to the predominant language of their training and evaluation datasets. This can lead to a performance gap, where the latest AI advancements may not be realized in lower-resourced languages. Additionally, these models may not only lack understanding of the language, but also culturally-relevant context that would make these models helpful for the communities.\n\nWe were incredibly impressed by the community's creative solutions for translation of languages, lyrics, old texts, and more.\n\n\n\nHonoring the innovators\n\nThrough hundreds of submissions, developers demonstrated how to bring the transformative power of LLMs to languages everywhere. Projects leveraged custom datasets and efficient post-training methods to adapt Gemma for instruction following, translation, and specific domains. We encourage you to explore the notebooks on Kaggle to see these techniques in action and apply them to your own multilingual projects.\n\nThe first place project adapted Gemma for Swahili understanding, opening up new possibilities to reach 200+ million language speakers. Gemma models were fine-tuned using parameter-efficient fine-tuning techniques for the 2B, 9B, and 27B parameter sizes.\n\nA key aspect of their tuning was Gemma’s “remarkable flexibility in instruction-response formatting,” which allowed the models to parse instructions with minimal structural constraints and generate coherent responses across different input formats.\n\nKnowledge Yielding Adaptive Retrieval Augmentation (Kyara) explored retrieval processes for LLM fine-tuning, demonstrating how to enhance Gemma’s ability to generate informed responses in Traditional Chinese.\n\nThe project focused on building high-quality question & answer (Q&A) datasets using a graph-based approach to knowledge retrieval, inspired on how humans learn by connecting concepts.\n\nThe project fine-tuned Gemma for Arabic language tasks, including translation, summarization, storytelling, and dialogue generation.\n\nAs a language with a rich historical past, the project also aimed to enhance comprehension of older forms of Arabic used in literary texts and art, employing multiple techniques to bridge tasks between Modern Standard Arabic and Classical Arabic.\n\nThis project focused on improving Italian language understanding for Gemma using a cost-effective post-training approach that addresses pitfalls such as hallucinations and catastrophic forgetting.\n\nThe 2B and 9B model sizes were fine-tuned on a mix of data, including a new instruction tuning dataset created using LLM-as-a-judge to ensure the quality of translations.\n\nThis project developed an “Ancient Chinese Expert” using Gemma to understand and generate translations for ancient Chinese texts, highlighting the potential of LLMs for historical cultural preservation.\n\nThe model was fine-tuned on a comprehensive dataset to improve linguistic understanding, and post-training included techniques to improve instruction following.\n\nThis project tackled nuanced challenges specific to AI-driven lyric translation, enhancing Gemma’s sensitivity to cultural references and symbolic language, while also ensuring rhythmic fidelity to the original song.\n\nA multilingual dataset contained lyric translations annotated to capture crucial cultural context, emotional tone, and rhythmic features, enabling the model to grasp and replicate the artistic depth of lyrical content.\n\nThis project adapted Gemma 2 JPN to generate Yomigana/Furigana, a reading aid for Japanese text and assist language learners or readers encountering complex Kanji.\n\nWhile other rule-based tools currently exist, LLMs can recognize rare Kanji better and “interpret the context of a sentence, enabling accurate disambiguation of polyphonic Kanji”. The notebook also noted that conversational capabilities had degraded due to training on the singular translation task.\n\nThis project enhances Gemma’s mathematical and logical understanding in Hindi numeric words, which presents a challenge for models to interpret given complex word formations, for example “दो सौ” for “200” or “ढाई” for “2.5”.\n\nThe 9B model was fine-tuned on a curated and human expert-verified dataset featuring a wide array of question types, unlocking uses for AI-driven educational tools, automated tutoring, and localized content\n\nThis project fine-tuned the Gemma 2 9B model for translation tasks in Kazakh. A language written in three distinct scripts (Cyrillic, Latin, and Arabic), the Cyrillic version requires approximately twice as many tokens as English, presenting a challenge for training with limited resources.\n\nModel performance showed better benchmarks than the 27B Gemma variant and Google Translate, demonstrating how to adapt LLMs for underrepresented languages using a cost-effective approach.\n\nThis project enables Gemma to understand and translate Old English, the earliest recorded form of the English language. A custom dataset with Old English-Modern English language pairs was created to help tackle the challenge of working with historical languages and limited publicly available data.\n\nThe notebook also features a bonus audio generation component, based on an open-source Icelandic text-to-speech model, offering an approximation of how speech might have sounded.\n\n\n\n10 more awesome projects\n\nGemma 2 Reasoning for Japanese Math: This project created reasoning variants to perform chain-of-thought processes and handle complex problems.\n\nMultitask Gemma2 Agents - Summarise & Translate: This project focused on developing agents capable of multiple tasks.\n\nKorean AI Doctor Gemma2: This project adapted Gemma for medical applications in Korean.\n\nGemma Fine-Tuning for Ru-En Medical Translations: This project enhanced Gemma translation accuracy in ophthalmology.\n\nGemma PT: This project fine-tuned the ShieldGemma content classifier to detect prejudice and disinformation in Portuguese.\n\nHow to Fine-tune Gemma 2 for Advanced Reasoning: This project enhanced Gemma reasoning capabilities by implementing the Coconut (Chain of Continuous Thought) paradigm.\n\nFinetune Gemma Turkish Chat: This project fine-tuned on Gemma on a Q&A dataset to improve accuracy and conversational ability.\n\nFinetuning Gemma2 Customized Dataset: This project fine-tuned Gemma for English-Arabic translation and medical understanding.\n\nGemma-2 Finetuning on Telugu News Dataset: This project adapted Gemma to generate Telugu headlines from news articles.\n\nFinetuned Gemma2 9B Math Reasoning Model Russian: This project enhanced Gemma performance for math problems in Russian.\n\n\n\nLooking ahead with Gemma 3\n\nWith over 7,000 languages spoken worldwide, the potential for AI to bridge communication gaps is immense. The Gemma open model family provides a powerful foundation for developers to adapt high-performing models to low-resource languages.\n\nThe innovation and dedication demonstrated by the Kaggle community in adapting Gemma 2 for various languages are truly inspiring. As we continue to build a future where AI empowers global communication for everyone, we're excited for Gemma 3, which brings pretrained support for over 140 languages, making it a great foundation to build on.\n\nWe encourage developers to explore the possibilities of Gemma, to share their datasets and models with others, and continue to advance multilingual AI together.", "label": "non_personal"}
{"title": "Supercharge your notebooks: The new AI-first Google Colab is now available to everyone", "url": "https://developers.googleblog.com/en/new-ai-first-google-colab-now-available-to-everyone/", "content": "Last month at Google I/O 2025, we shared our vision for a reimagined, AI-first Colab, a true coding partner in your notebook designed to help you tackle your most challenging problems faster than ever. We’ve started by rolling these features out to a small group of users, and the response has been incredible. Today, we are thrilled to make AI-first Colab available to everyone!\n\n\n\nEarly users have been embracing Colab's new agentic capabilities to accelerate their projects, learn new skills, and unlock insights from their data in ways that have delighted and inspired us.\n\n\n\nFrom early access to everyday productivity\n\nOur goal was to build an AI collaborator that understands your intentions and integrates seamlessly into your workflow. Based on user feedback, this new experience is already making a significant impact.\n\nHere are a few of the top ways people are using the new Colab AI:\n\n\n\n1: Accelerating End-to-End Machine Learning Projects\n\nUsers are leveraging Colab AI for the entire machine learning lifecycle. From taking a raw dataset and asking the agent to autonomously perform cleaning and preparation, to generating feature analysis, training models, and evaluating the results. This turns hours of work into a guided, conversational experience.\n\n\n\n2: Smarter Debugging\n\nCoding is an iterative process. Colab AI acts as a pair programmer to help you prototype ideas, generate boilerplate code, and understand new libraries. When you hit an error, the AI doesn't just help find the bug, it can suggest a fix in a clear diff view, helping you learn and keep going with your project. The result is a massive boost in productivity.\n\n\n\n3: Creating Stunning Visualizations with Zero Hassle\n\nData exploration is incomplete without visualization. Users are simply asking Colab AI to graph their data, and it generates high-quality, clearly labeled charts without the need for manual tweaking of plotting libraries.\n\n\n\nA quick look at the features powering your workflow\n\nThese use cases are powered by a suite of new, deeply integrated features:\n\nIterative Querying: A conversational experience where you can ask for code, get explanations about libraries, and intelligently fix errors.\n\nNext-Generation Data Science Agent (DSA): Trigger autonomous analytical workflows. The agent creates a plan, executes code, reasons about the results, and presents its findings, all while allowing you to provide feedback and stay in control.\n\nEffortless Code Transformation: Simply describe a change in natural language, and Colab will identify and refactor the relevant code for you.\n\n\n\nGet started with Colab AI today!\n\nWe are incredibly excited to put these powerful new capabilities into the hands of the entire Colab community. This is a major step in our journey to create a more powerful and intuitive AI-first Colab, and we’re just getting started.\n\nReady to try it out? It’s easy to get started:\n\n1: Open any new or existing notebook in Google Colab.\n\n2: Look for the Gemini spark icon in the bottom toolbar.", "label": "non_personal"}
{"title": "Using KerasHub for easy end-to-end machine learning workflows with Hugging Face", "url": "https://developers.googleblog.com/en/load-model-weights-from-safetensors-into-kerashub-multi-framework-machine-learning/", "content": "How to load SafeTensors checkpoints across different frameworks\n\nAs the AI ecosystem continues to evolve, there are more and more ways to define machine learning models, and even more ways to save the model weights that result from training and fine-tuning. In this growing set of choices, KerasHub allows you to mix and match popular model architectures and their weights across different ML frameworks. For example, a popular place to load checkpoints from is the Hugging Face Hub. Many of those model checkpoints were created with the Hugging Face transformers library in the SafeTensors format. Regardless of what ML framework was used to create the model checkpoint, those weights can be loaded into a KerasHub model, which allows you to use your choice of framework (JAX, PyTorch, or TensorFlow) to run the model. Yes, that means you can run a checkpoint from Mistral or Llama on JAX, or even load Gemma with PyTorch – it doesn't get any more flexible than that. Let's take a look at some of these terms in more detail, and talk about how this works in practice.\n\nModel architecture vs. model weights When loading models, there are two distinct parts that we need: the model architecture and the model weights (often called \"checkpoints\"). Let's define each of these in more detail. When we say \"model architecture\", we are referring to how the layers of the model are arranged, and the operations that happen within them. Another way to describe this might be to call it the \"structure\" of the model. We use Python frameworks like PyTorch, JAX, or Keras to express model architectures. When we talk about \"model weights\", we are referring to the \"parameters\" of a model, or numbers in a model that are changed over the course of training. The particular values of these weights are what give a trained model its characteristics. \"Checkpoints\" are a snapshot of the values of the model weights at a particular point in the training. The typical checkpoint files that are shared and widely used are the ones where the model has reached a particularly good training outcome. As the same model architecture is further refined with fine-tuning and other techniques, additional new checkpoint files are created. For example, many developers have taken Google's gemma-2-2b-it model and fine-tuned it with their own datasets, and you can see over 600 examples. All of these fine-tuned models use the same architecture as the original gemma-2-2b-it model, but their checkpoints have differing weights. So there we have it: the model architecture is described with code, while model weights are trained parameters, saved as checkpoint files. When we have a model architecture together with a set of model weights (in the form of a checkpoint file), we create a functioning model that produces useful outputs.\n\nSorry, your browser doesn't support playback for this video Different model weights can be loaded into the same model architecture. These different sets of weights are saved as checkpoints.\n\nTools like Hugging Face's transformers library and Google's KerasHub library provide model architectures and the APIs you need to experiment with them. Examples of checkpoint repositories include Hugging Face Hub and Kaggle Models. You can mix and match model architecture libraries with your choice of checkpoint repositories. For example, you can load a checkpoint from Hugging Face Hub into a JAX model architecture and fine-tune it with KerasHub. For a different task, you might find a checkpoint on Kaggle Models that's suitable for your needs. This flexibility and separation means you are not boxed into one ecosystem.\n\nWhat is KerasHub? So we’ve mentioned KerasHub a few times– let’s go into it in more detail. KerasHub is a Python library that helps make defining model architectures easier. It contains many of the most popular and commonly used machine learning models today, and more are being added all the time. Because it's based on Keras, KerasHub supports all three major Python machine learning libraries used today: PyTorch, JAX, and TensorFlow. This means you can have model architectures defined in whichever library you'd like. Furthermore, since KerasHub supports the most common checkpoint formats, you can easily load checkpoints from many checkpoint repositories. For example, you can find hundreds of thousands of checkpoints on Hugging Face and Kaggle to load into these model architectures.\n\nComparisons to the Hugging Face transformers library A common workflow by developers is to use the Hugging Face transformers library to fine-tune a model and upload it to the Hugging Face Hub. And if you’re a user of transformers , you’ll also find many familiar API patterns in KerasHub. Check out the KerasHub API documentation to learn more. An interesting aspect of KerasHub is that many of the checkpoints found on Hugging Face Hub are compatible with not only the transformers library, but also KerasHub. Let's take a look at how that works.\n\nKerasHub is compatible with Hugging Face Hub Hugging Face has a model checkpoint repository, called Hugging Face Hub. It's one of the many places where the machine learning community uploads their model checkpoints to share with the world. Especially popular on Hugging Face is the SafeTensors format, which is compatible with KerasHub. You can load these checkpoints from Hugging Face Hub directly into your KerasHub model, as long as the model architecture is available. Wondering if your favorite model is available? You can check https://keras.io/keras_hub/presets/ for a list of supported model architectures. And don't forget, all the community created fine-tuned checkpoints of these model architectures are also compatible! We recently created a new guide to help explain the process in more detail. How does this all work? KerasHub has built-in converters that simplify the use of Hugging Face transformers models. These converters automatically handle the process of translating Hugging Face model checkpoints into a format that's compatible with the KerasHub. This means you can seamlessly load a wide variety of pretrained Hugging Face transformer models from the Hugging Face Hub directly into KerasHub with just a few lines of code. If you notice a missing model architecture, you can add it by filing a pull request on GitHub.\n\nHow to load a Hugging Face Hub checkpoint into KerasHub So how do we get checkpoints from Hugging Face Hub loaded into KerasHub? Let's take a look at some concrete examples. We'll start by first choosing our machine learning library as our Keras \"backend\". We'll use JAX in the examples shown, but you can choose between JAX, PyTorch, or TensorFlow for any of them. All the examples below work regardless of which one you choose. Then we can proceed by importing keras , keras_hub , and huggingface_hub , and then login with our Hugging Face User Access token so we can access the model checkpoints.\n\nimport os os.environ[\"KERAS_BACKEND\"] = \"jax\" # or \"torch\" or \"tensorflow\" import keras from keras_hub import models from huggingface_hub import login login('HUGGINGFACE_TOKEN') Python Copied\n\nPut a Mistral model on JAX First up, perhaps we want to run a checkpoint from Mistral on JAX? Over on KerasHub, there are a handful of Mistral models available on KerasHub's list of available model architectures, let's try out mistral_0.2_instruct_7b_en . Clicking into it, we see that we should use the MistralCausalLM class to call from_preset . On the Hugging Face Hub side of things, we see that the corresponding model checkpoint is stored here, with over 900 fine-tuned versions. Browsing that list, there's a popular cybersecurity-focused fine-tuned model called Lily, with the pathname of segolilylabs/Lily-Cybersecurity-7B-v0.2 . We'll also need to add \" hf:// \" before that path to specify that KerasHub should look at Hugging Face Hub.\n\nSorry, your browser doesn't support playback for this video\n\nPutting it all together, we get the following code:\n\n# Model checkpoint from Hugging Face Hub gemma_lm = models.MistralCausalLM.from_preset(\"hf://segolilylabs/Lily-Cybersecurity-7B-v0.2\") gemma_lm.generate(\"Lily, how do evil twin wireless attacks work?\", max_length=30) Python Copied\n\nRunning Llama 3.1 on JAX Llama 3.1-8B-Instruct is a popular model, with over 5 million downloads last month. Let's put a fine-tuned version on JAX. With over 1400 fine-tuned checkpoints, there's no lack of choice. The xVerify fine-tuned checkpoint looks interesting, let's load that into JAX on KerasHub. We'll use the Llama3CausalLM class to reflect the model architecture that we are using. As before, we'll need the appropriate path from Hugging Face Hub, prefixed with \" hf:// \". It's pretty amazing that we can load and call a model with just two lines of code, right?\n\n# Model checkpoint from Hugging Face Hub gemma_lm = models.Llama3CausalLM.from_preset(\"hf://IAAR-Shanghai/xVerify-8B-I\") gemma_lm.generate(\"What is the tallest building in NYC?\", max_length=100) Python Copied\n\nLoad Gemma on JAX Finally, let's load a fine-tuned Gemma-3-4b-it checkpoint into JAX. We'll use the Gemma3CausalLM class, and select one of the fine-tuned checkpoints. How about EraX, a multilingual translator? As before, we'll use the pathname with the Hugging Face Hub prefix to create the full path of \" hf://erax-ai/EraX-Translator-V1.0 \".\n\n# Model checkpoint from Hugging Face Hub gemma_lm = models.Gemma3CausalLM.from_preset(\"hf://erax-ai/EraX-Translator-V1.0\") gemma_lm.generate(\"Translate to German: \", max_length=30) Python Copied", "label": "non_personal"}
{"title": "Gemini 2.5 for robotics and embodied intelligence", "url": "https://developers.googleblog.com/en/gemini-25-for-robotics-and-embodied-intelligence/", "content": "The latest generation of Gemini models, 2.5 Pro and Flash, are unlocking new frontiers in robotics. Their advanced coding, reasoning, and multimodal capabilities, now combined with spatial understanding, provide the foundation for the next generation of interactive and intelligent robots. This post explores how developers can leverage Gemini 2.5 to build sophisticated robotics applications. We'll provide practical examples with prompts to show using Gemini 2.5 and the Live API for: Semantic scene understanding for complex queries: Identify and label objects from robot camera feeds. Understand complex queries through multimodal reasoning. Combine spatial reasoning with code generation to control robots: Use the robot's API to call functions and bring task plans to life. Build interactive robotics applications with the Live API: Convert voice commands into executable robot plans. In March, we launched our Gemini Robotics models, including Gemini Robotics-ER, our advanced embodied reasoning model optimized for the unique demands of robotics applications. We’re also excited to share how our Gemini Robotics trusted testers are already demonstrating the power of Gemini in robotics applications. We are including examples from Agile Robots, Agility Robotics, Boston Dynamics, and Enchanted Tools. Join the Gemini Robotics-ER trusted tester program waitlist.\n\nSemantic scene understanding for complex queries Reasoning about the physical world is at the core of general and robust control. Gemini 2.5 represents a step in this direction with its improved ability to reason multimodally. Below we share two examples, utilizing Gemini’s pointing and object detection capabilities. Pointing allows a model to refer to entities or parts of entities precisely, and locate them in space. Gemini 2.5 Pro is able to reason about the entities it is pointing to, opening new opportunities for interacting with images. For example, Gemini 2.5 Pro is able to reason about empty space in the context of a supermarket display, knowing that this indicates restocking may be needed. In the example below, Gemini identifies the baby eggplant needs restocking. Gemini 2.5 Pro also shows a nascent ability to locate and read information from that location, as illustrated in the gauge example. Example 1: Gemini 2.5 can locate objects in the scene based on fine-grained language descriptions, for example, find a shelf that needs restocking. Prompt: Point to one bin on the shelf that needs restocking. The answer should follow the json format: [{\"point\": <point>, \"label\": <label1>}, ...]. The points are in [y, x] format normalized to 0-1000. Input image with response overlay:\n\nExample 2: Gemini 2.5 can locate small objects in the scene and estimate states of those objects. For example, it can read gauges. Prompt: Point to all the round gauges. The answer should follow the json format: [{\"point\": <point>, \"label\": <label1>}, ...]. The points are in [y, x] format normalized to 0-1000. Input image with response overlay:\n\nPrompt: What is the reading on the middle gauge? Response: Based on the close-up view, the round gauge in the center-left of the image appears to be reading 0. The needle is pointing directly at the \"0\" mark on the dial.\n\nGoing beyond object-centric perception Gemini 2.5 is able to accurately track multiple objects across time and detect open ended concepts like ‘a spill’. Gemini 2.5 can be prompted into trajectory prediction in the form of a sequence of points. Example 1: Gemini 2.5 can generate bounding boxes for each frame in a video and be visualized like below. Prompt: Detect green bowl, crab, wallet, pink bowl, phone, return a json array with keys box_2d and label. (executed per frame). Input image with response overlay:\n\nSorry, your browser doesn't support playback for this video\n\nExample 2: Gemini 2.5 can detect open-ended concepts relevant to robotics, requiring commonsense knowledge and context specific reasoning. For example, a helpful robot needs to understand the concept of a “spill”. Prompt: 1) Show me the bounding box of spill. Return in a json array with keys box_2d and label. 2) Give the segmentation masks for the spill. Output a JSON list of segmentation masks where each entry contains the 2D bounding box in the key \"box_2d\", the segmentation mask in key \"mask\", and the text label in the key \"label\". Input image with response overlay:\n\nExample 3: Gemini 2.5 can be prompted into trajectory prediction in the form of a sequence of points. Prompt: Generate a robot arm trajectory of 10 points to move the cloth to the spill. The answer should follow the json format: [{\"point\": <point>, \"label\": <label1>}, ...]. The points are in [y, x] format normalized to 0-1000. Input image with response overlay:\n\nUsing spatial understanding and code generation to control robots Gemini 2.5 can utilize its underlying spatial understanding to control robots through code generation. By providing Gemini 2.5 with a robot control API, it can apply advanced capabilities in scene understanding, object manipulation, and code writing together to perform tasks zero-shot, with no additional training. Example 1 below showcases code-generation for “Put the banana in the bowl”. It gives Gemini access to a robot control API and shows how the model leverages its spatial understanding, thinking, and code generation capabilities to select the appropriate API calls and arguments given the task. Gemini 2.5 generates 2 different feasible plans for putting the banana in the bowl. The first solution is to simply pick up the banana, move it above the bowl, and drop it. The second solution lifts the banana, moves the bowl below the banana, and then drops the banana. Example 1: Gemini 2.5 Generate high-level planning code for pick-and-place tasks with different strategies. Prompt: You are given a robot control API and example code below:\n\n# Provided API class RobotAPI: def detect_object(self, obj: str) -> Detection \"\"\"Detects the given object's XYZ location\"\"\" def get_grasp_position_and_euler_orientation(self, gripper: str, object_name: str) -> tuple[numpy.ndarray, numpy.ndarray]: \"\"\"Returns the grasp position and orientation for the given object and gripper from the Gemini Robotics ER model or other grasping primitives. def move_gripper_to(self, gripper: str, position: numpy.ndarray, orientation: numpy.ndarray): \"\"\"Moves the gripper to the given position and orientation.\"\"\" def move_gripper_to_safe_position(self, gripper: str): \"\"\"Moves the given gripper to a safe position out of the table area.\"\"\" def open_gripper(self, gripper: str): \"\"\"Opens the given gripper.\"\"\" def close_gripper(self, gripper: str): \"\"\"Closes the given gripper.\"\"\" robot=RobotAPI() # Example code robot.open_gripper(gripper='RIGHT') banana_detection = robot.detect_object(obj='banana') banana_position = banana_detection.position print(f\"Detected banana position: {banana_position}\") # Gemini uses this to determine which gripper to use. grasp_position, grasp_orientation = robot.get_grasp_position_and_euler_orientation(gripper='RIGHT', object_name='banana') pre_grasp_position = grasp_position + np.array([0, 0, 0.1]) # Add 10cm to the Z-coordinate for pre-grasp robot.move_gripper_to(gripper='RIGHT', position=pre_grasp_position, orientation=grasp_orientation) robot.move_gripper_to(gripper='RIGHT', position=grasp_position, orientation=grasp_orientation) robot.close_gripper(gripper='RIGHT') Python Copied\n\nNow generate code for the task “put the banana in the bowl”. Response 1 with results of code executed: Reasoning (abbreviated) 1: The current position of the right gripper is [-0.04, -0.028, 0.176] . 2: The bowl's detected position is [-0.03839983, -0.02811164, 0.0073104] . 3: To place the banana inside the bowl, the gripper's Z-coordinate needs to be lowered. I will use the bowl's detected X and Y coordinates for the target position. For the Z-coordinate, I will use the bowl's detected Z-coordinate ( 0.0073104 ) and add a small offset (e.g., 0.03m) to ensure the banana is placed inside the bowl and not through the table. This would result in a target Z of approximately 0.007 + 0.03 = 0.037m . This Z-coordinate is higher than the bowl's base but lower than the typical height of a banana when standing on the table, ensuring it's placed *into* the bowl. Code Snippet:\n\n# Detect the bowl's position (re-detect to ensure fresh data, though it's unlikely to have moved) bowl_detection = robot.detect_object('bowl') if bowl_detection.position is None: raise ValueError(\"Bowl not detected.\") # Define the target Z-coordinate for dropping the banana into the bowl # Use the bowl's Z-position and add an offset to place the banana inside. # The banana's original Z was ~0.032, bowl's Z is ~0.007. # A target Z of 0.037 (0.007 + 0.03) should place it inside the bowl. drop_z = bowl_detection.position[2] + 0.03 Python Copied\n\nSorry, your browser doesn't support playback for this video\n\nResponse 2 with results of code executed: Reasoning (abbreviated) # Re-planning due to reachability constraints: The RIGHT arm cannot reach the bowl's current position. # The plan will now involve the LEFT arm moving the bowl to the RIGHT arm's reachable area. # The RIGHT arm can reach x-coordinates from 0.04m to 0.40m. I will move the bowl to an x-coordinate of 0.10m, which is well within the RIGHT arm's reach and also allows for some clearance from the center.\n\nCode Snippet:\n\ntarget_bowl_position = [-0.07274145, -0.08317622, 0.00720479] # Original bowl position # Move the bowl to an x-coordinate reachable by the RIGHT arm, keeping the same y and z (lifted) # Let's choose x = 0.10m for the bowl, which is within the right arm's reach (0.04 to 0.40) # Keep the lifted height at 0.20m move_bowl_position = [0.10, bowl_detection.position[1], 0.20] robot.move_gripper_to('LEFT', move_bowl_position, current_left_gripper_orientation) Python Copied\n\nSorry, your browser doesn't support playback for this video\n\nGemini 2.5 can also effectively utilize a small number of in-context examples to perform more dexterous robot control tasks. In the two examples shown below of an ALOHA robot packing boxes and folding a dress, 10 demonstrations consisting of interleaved reasoning and robot actions for each task were added to Gemini’s context. We’ve created open-source code showing how to do this using Gemini, including examples of the input demonstrations. This enables robots to be taught and deployed on the spot. See the Colab. Example 2: Gemini 2.5 (Flash) utilizes a small number of in-context examples to perform more dexterous robot control tasks. Prompt: see colab. Response with results of code executed:\n\nSorry, your browser doesn't support playback for this video\n\nSorry, your browser doesn't support playback for this video\n\nBuilding interactive robotics applications with the Live API The Live API for realtime streaming was recently introduced and can be used to build interactive applications that let people control robots using their voice. Intuitive human-robot-interaction is an important aspect of making robots that are easy and safe to use. We recently showcased an interactive Gemini Robotics demo at I/O 2025, which was built around Live API for voice interaction and function calling. Live API supports both audio and video as input modalities, and audio / text as output modalities. This allows you to send both voice input and the robot camera feed to the Live API. This is even more powerful when combined with tool use. Tool use allows Live API to go beyond just conversation by enabling it to perform actions in the real-world while maintaining a real time connection. For example, the robot APIs defined above can be defined as function calls including robot.open_gripper() , robot.close_gripper() and robot.move_gripper_to() . After they are defined as tool calls, they can be integrated into the workflow where people can interact with the robot using voice in real time. Developers can get started on GitHub, and refer to API documentation for function calling features.\n\nSorry, your browser doesn't support playback for this video 🔊 Demonstration of a realtime web console for robotics built with Live API, right click to open in a new tab for audio.\n\nSafety The 2.5 Pro and 2.5 Flash models demonstrate robust performance on the ASIMOV Multimodal and Physical Injury benchmarks released along with the Gemini Robotics tech report, exhibiting accuracy comparable to that of 2.0 models. Beyond the ASIMOV benchmarks, the 2.5 Pro and 2.5 Flash models also exhibit excellent performance in rejecting prompts that attempt to leverage embodied reasoning capabilities while violating safety policies such as promoting harmful stereotypes, discrimination, or endangerment of minors. Following rigorous evaluation against such synthetically generated adversarial prompts, 2.5 Pro and Flash demonstrated near-zero violation rates.\n\nHow Gemini is being used today for Robotics In March we released the Gemini Robotics-ER model and we’re already inspired by how the community is using it for robotics applications. Check out these examples of interactivity, perception, planning, and function calling from our trusted testers: Agile Robots, Agility Robotics, Boston Dynamics, and Enchanted Tools.\n\nSorry, your browser doesn't support playback for this video\n\nSorry, your browser doesn't support playback for this video\n\nSorry, your browser doesn't support playback for this video\n\nSorry, your browser doesn't support playback for this video", "label": "non_personal"}
{"title": "Simulating a neural operating system with Gemini 2.5 Flash-Lite", "url": "https://developers.googleblog.com/en/simulating-a-neural-operating-system-with-gemini-2-5-flash-lite/", "content": "In traditional computing, user interfaces are pre-defined. Every button, menu, and window is meticulously coded by developers. But what if an interface could be generated in real time, adapting to a user's context with each interaction? We explored this question by building a research prototype (view demo app in Google AI Studio) for a generative, infinite computer experience. Our prototype simulates an operating system where each screen is generated on the fly by a large language model. It uses Gemini 2.5 Flash-Lite, a model whose low latency is critical for creating a responsive interaction that feels instantaneous. Instead of navigating a static file system, the user interacts with an environment that the model builds and rebuilds with every click. This post outlines the core technical concepts behind this prototype.\n\nSorry, your browser doesn't support playback for this video\n\nConditioning the model for on-the-fly UI generation To generate a UI on-the-fly, we need to provide the model with a clear structure and context for each request. We engineered our prompt by dividing the model's input into two parts: a \"UI constitution\" and a \"UI interaction\". The UI constitution is a system prompt that contains a fixed set of rules for UI generation. These rules define consistent elements like the OS-level styling, the home screen format, and logic for embedding elements like maps. The UI interaction is a JSON object that captures the user's most recent action, such as a mouse click on an icon. This object serves as the specific query that prompts the model to generate the next screen. For example, clicking on a “Save Note” icon within the Notepad app may generate an object as the following:\n\n{ // `id`: The unique ID from the button's `data-interaction-id` attribute. id: 'save_note_action', // `type`: The interaction type from `data-interaction-type`. type: 'button_press', // `value`: Because the button has a `data-value-from` attribute, the system // retrieves the content from the textarea with the ID 'notepad_main_textarea'. value: 'Meeting notes\n\n- Discuss Q3 roadmap\n\n- Finalize budget', // `elementType`: The HTML tag of the element that was clicked. elementType: 'button', // `elementText`: The visible text inside the button. elementText: 'Save Note', // `appContext`: The ID of the application the user is currently in. // This comes from the `activeApp` state in `App.tsx`. appContext: 'notepad_app' } JSON Copied", "label": "non_personal"}
{"title": "Unlock deeper insights with the new Python client library for Data Commons", "url": "https://developers.googleblog.com/en/pythondatacommons/", "content": "Data is the bedrock of progress across nearly every field. It serves as the raw material from which profound insights are forged, enabling us to precisely measure current realities, identify critical trends, and possibly predict future outcomes.\n\nAt Google, our mission with Data Commons is to organize the world's publicly available statistical data, making it more accessible and useful for everyone. It's an open-source knowledge graph that unifies a vast array of public data from diverse sources, simplifying access and comprehension for developers, researchers, and data analysts alike. Along with the datacommons.org website, Google Search uses Data Commons to answer queries like What is the population of San Francisco?, with the top graph generated by Data Commons.\n\nToday, we're announcing the general availability of the new Python client library for the Data Commons based on the V2 REST API. This new Python library dramatically enhances how data developers can leverage Data Commons.\n\n\n\nReal-world impact: partnering with ONE.org\n\nThis milestone was significantly shaped by the vision and substantial contributions of our partner The ONE Campaign, a global organization working to create the investments needed for economic opportunities and healthier lives in Africa. We built Data Commons as an open-source platform precisely to encourage community contributions and enable innovative uses, and this partnership with The ONE Campaign perfectly exemplifies that goal. ONE advocated for, proposed the design and coded the client library to make Data Commons' rich insights available to data scientists and analysts who want to leverage the rich ecosystem of Python analytical tools and libraries.\n\n\n\nSupport for custom Data Commons instances\n\nThe Data Commons platform also allows organizations, like the United Nations or ONE, to host their own Data Commons instances. These custom instances enable the seamless integration of proprietary datasets with the foundational Data Commons knowledge graph. Organizations leverage the Data Commons data framework and tools while maintaining full control over their data and resources.\n\nOne of the most impactful additions in the V2 library is robust support for custom instances. This means you can now use the Python library to programmatically query any public or private instance—whether hosted locally, within your organization or on the Google Cloud Platform.\n\n\n\nPowerful new features\n\nThe Python library makes it very easy to perform common queries against Data Commons data, such as:\n\nExploring the structure of the knowledge graph\n\nRetrieving data for any of the 200,000+ statistical variables from over 200 datasets in domains such as demographics, economy, education, energy, environment, health, and housing\n\nEasily mapping entities from other datasets to entities in Data Commons\n\n\n\nV2 of the client library offers many technical improvements over the V1 library, including:\n\nPandas dataframe APIs are supported as an integral module, with a single installation package, allowing seamless use with other API endpoints in the same client\n\nSeveral new convenience methods for common data queries\n\nAPI key management and other stateful operations built in to the client class\n\nIntegration with the Pydantic libraries for improved type safety, validation and serialization", "label": "non_personal"}
{"title": "What you should know from the Google I/O 2025 Developer keynote", "url": "https://developers.googleblog.com/en/google-io-2025-developer-keynote-recap/", "content": "This year at Google I/O we’re showing how you can build across Google’s different platforms, and innovate using our best AI models from Google DeepMind. Here are the top announcements from the Developer keynote.\n\nBuilding with Gemini\n\nGoogle AI Studio is the fastest way to evaluate models and start building with the Gemini API.\n\nGoogle AI Studio makes it easy to build with the Gemini API: We’ve integrated Gemini 2.5 Pro into the native code editor, enabling you to prototype faster. It’s tightly optimized with the GenAI SDK so you can instantly generate web apps from text, image, or video prompts. Start from a simple prompt, or get inspired by starter apps in the showcase.\n\nBuild agentic experiences with the Gemini API: Build agents with Gemini 2.5 advanced reasoning capabilities via the Gemini API and new tools, like URL Context. It enables the model to pull context from web pages with just a link. We also announced the Gemini SDKs will support Model Context Protocol (MCP) definitions, making it easier to leverage open source tools.\n\nGemini 2.5 Flash Native Audio in the Live API: Build agentic applications that hear and speak, with full control over the model’s voice, tone, speed, and overall style, in 24 languages. Gemini 2.5 Flash Native Audio is much better at understanding conversational flow and ignoring stray sounds or voices, leading to smoother, more natural back-and-forth.\n\nGenerate high-quality UI designs with Stitch: A new AI-powered tool to generate user interface designs and corresponding frontend code for web applications. Iterate on your designs conversationally using chat, adjust themes, and easily export your creations to CSS/HTML or Figma to keep working. Try Stitch for UI design.\n\nOur async code agent, Jules, is now in public beta: Jules is a parallel, asynchronous coding agent that works directly with your GitHub repositories. You can ask Jules to take on tasks such as version upgrades, writing tests, updating features, and bug fixes, to name a few. It spins up a Cloud VM, makes coordinated edits across your codebase, runs tests, and you can open a pull request from its branch when you're happy with the code.\n\n\n\nAndroid\n\nLearn how we’re making it easier for you to build great experiences across devices.\n\nBuilding experiences with generative AI: Generative AI enhances apps by making them intelligent, personalized, and agentic. We announced new ML Kit GenAI APIs using Gemini Nano for common on-device tasks. We showcased an AI sample app, Androidify, which lets you create an Android robot of yourself using a selfie. Discover how Androidify is built, and read the developer documentation to get started.\n\nBuilding excellent apps adaptively across 500 million devices: Mobile Android apps form the foundation across phones, foldables, tablets, and ChromeOS, and this year we’re helping you bring them to cars and Android XR. You can also take advantage of Material 3 Expressive to help make your apps shine.\n\nGemini in Android Studio - AI agents to help you work: Gemini in Android Studio is the AI-powered coding companion that makes developers more productive at every stage of the dev lifecycle. We previewed Journeys, an agentic experience that helps with writing and executing end-to-end tests. We also previewed the Version Upgrade Agent which helps update dependencies. Learn more about how these agentic experiences in Gemini in Android Studio can help you build better apps, faster.\n\n\n\nWeb\n\nWe’re making it easier to create powerful web experiences, from building better UI and faster debugging, to creating new AI-powered features.\n\nCarousels are now easier than ever to build with a few lines of CSS and HTML: Build beautiful carousels with CSS that are interactive at first paint. With Chrome 135, we've combined a few new CSS primitives to make building carousels, and other types of off-screen UI, dramatically easier. Use familiar CSS concepts to create rich, interactive, smooth, and more accessible carousels, in a fraction of the time.\n\nIntroducing the new experimental Interest Invoker API: Declaratively toggle popovers when visitor interest is active for a small duration. Combine with the Anchor Positioning API and Popover API to build complex, responsive, layered UI elements like tooltips and hover cards, without JavaScript. Interest Invoker API is available as an origin trial.\n\nBaseline features availability is now in your familiar tools: VS Code now displays the Baseline status of features as you build, with support coming soon to other VS Code-based IDEs and WebStorm by JetBrains. Baseline is now also supported in ESLint for CSS, HTML ESLint, and Stylelint. RUMvision combines Baseline information with real-user data, letting you strategically select the optimal Baseline target for your audience. Plus, with the web-features data set now 100% mapped, you can now access the Baseline status of every feature on every major browser.\n\nAI in Chrome DevTools supports your debugging workflow: Boost your development workflow with Gemini integrated directly into Chrome DevTools. With AI assistance, you can now directly apply suggested changes to the files in your workspace in the Elements panel. Plus, the reimagined Performance Panel now features a powerful ‘Ask AI’ integration that provides contextual performance insights to help optimize your web application’s Core Web Vitals.\n\nNew built-in AI APIs using Gemini Nano are now available, including multimodal capabilities: Gemini Nano brings enhanced privacy, reduced latency, and lower cost. Starting from Chrome 138, the Summarizer API, Language Detector API, Translator API, and Prompt API for Chrome Extensions are available in Stable. The Writer and Rewriter APIs are available in origin trials, and the Proofreader API and Prompt API with multimodal capabilities are in Canary. Join our early preview program to help shape the future of AI on the web.\n\n\n\nFirebase\n\nPrototype, build, and run modern, AI-powered, full-stack apps users love with Firebase. Use Firebase Studio, a cloud-based, AI workspace powered by Gemini 2.5, to turn your ideas into a full-stack app in minutes, from prompt to publish.\n\nFigma designs can be brought to life in Firebase Studio: Import a Figma design directly into Firebase Studio using the builder.io plugin, then add features and functionality using Gemini in Firebase without having to write any code.\n\nFirebase Studio will now suggest a backend: Rolling out over the next several weeks, when you use the App Prototyping agent, Firebase Studio can detect the need for a backend. Firebase Studio will now recommend Firebase Auth and Cloud Firestore, and when you're ready to publish the app to Firebase App Hosting, Firebase Studio will provision those services for you.\n\nFirebase AI Logic: Integrate Google’s gen AI models directly through your client apps, or through Genkit for server-side implementation. As part of the evolution from Vertex AI in Firebase to Firebase AI Logic, we’re also releasing new features such as client side integrations for the Gemini Developer API, hybrid inference, enhanced observability, and deeper integrations with Firebase products such as App Check and Remote Config.\n\n\n\nBuilding with open models\n\nThere's so much you can do when building with Gemini, but sometimes it's better to train and tune your own model. That’s why we released Gemma, our family of open models designed to be state of the art, and fit on devices.\n\nGemma 3n is in early preview: This model can run on as little as 2GB of RAM thanks to research innovations. It is the first model built on the new, advanced mobile-first architecture that will also power the next generation of Gemini Nano, and is engineered for unmatched AI performance directly on portable devices.\n\nMedGemma is our most capable open model for multimodal medical text and image comprehension: A variant of Gemma 3, MedGemma is a great starting point for developers to fine tune and adapt to build their own healthcare-based AI applications. Its small size makes it efficient for inference, and because it’s open, it enables developers with the flexibility to fine-tune the model and run it in their preferred environments. MedGemma is available for use now as part of Health AI Developer Foundations.\n\nColab is launching an agent first experience that transforms coding: Powered by Gemini 2.5 Flash, Colab helps you navigate complex tasks, such as fine-tuning a model. We showcased how the new AI-first Colab can build UI, saving you lots of coding time.\n\nSignGemma is a sign language understanding model coming later this year to the Gemma family: It is the most capable model for translating sign languages into spoken language text to date (best at American Sign Language to English), enabling you to develop new ways for Deaf/Hard of Hearing users to access technology. Share your input at goo.gle/SignGemma.\n\nDolphinGemma is the world’s first large language model for dolphins: Working with researchers at Georgia Tech and the Wild Dolphin Project, DolphinGemma was fine-tuned on data from decades of field research, to help scientists better understand patterns in how dolphins communicate.\n\n\n\nGoogle Developer Program\n\nWe expanded AI benefits for the Google Developer Program, including Gemini Code Assist Standard, a new gen AI developer annual credit, and 3 months of Google One AI Premium. We also announced a new Google Cloud & NVIDIA community where you can connect with experts from both companies in a dedicated forum, and soon gain access to exclusive learning content and credits.\n\n\n\nTune into all of the developer news\n\nFollowing the keynotes, we’ll be livestreaming sessions across AI, Android, web, and cloud May 20-21. Then, check out all of the Google I/O announcements and updates with 100+ sessions, codelabs, and more available on demand starting May 22.\n\nMake sure to connect with our thriving global community of developers, and follow along on LinkedIn and Instagram as we bring I/O Connect events to developers around the world.", "label": "non_personal"}
{"title": "On-device small language models with multimodality, RAG, and Function Calling", "url": "https://developers.googleblog.com/en/google-ai-edge-small-language-models-multimodality-rag-function-calling/", "content": "Last year Google AI Edge introduced support for on-device small language models (SLMs) with four initial models on Android, iOS, and Web. Today, we are excited to expand support to over a dozen models including the new Gemma 3 and Gemma 3n models, hosted on our new LiteRT Hugging Face community. Gemma 3n, available via Google AI Edge as an early preview, is Gemma’s first multimodal on-device small language model supporting text, image, video, and audio inputs. Paired with our new Retrieval Augmented Generation (RAG) and Function Calling libraries, you have everything you need to prototype and build transformative AI features fully on the edge.\n\nSorry, your browser doesn't support playback for this video Let users control apps with on-device SLMs and our new function calling library\n\nBroader model support You can find our growing list of models to choose from in the LiteRT Hugging Face Community. Download any of these models and easily run them on-device with just a few lines of code. The models are fully optimized and converted for mobile and web. Full instructions on how to run these models can be found in our documentation and on each model card on Hugging Face. To customize any of these models, you finetune the base model and then convert and quantize the model using the appropriate AI Edge libraries. We have a Colab showing every step you need to fine-tune and then convert Gemma 3 1B. With the latest release of our quantization tools, we have new quantization schemes that allow for much higher quality int4 post training quantization. Compared to bf16, the default data type for many models, int4 quantization can reduce the size of language models by a factor of 2.5-4X while significantly decreasing latency and peak memory consumption.\n\nGemma 3 1B & Gemma 3n Earlier this year, we introduced Gemma 3 1B. At only 529MB, this model can run up to 2,585 tokens per second pre-fill on the mobile GPU, allowing it to process up to a page of content in under a second. Gemma 3 1B’s small footprint allows it to support a wide range of devices and limits the size of files an end user would need to download in their application. Today, we are thrilled to add an early preview of Gemma 3n to our collection of supported models. The 2B and 4B parameter variants will both support native text, image, video, and audio inputs. The text and image modalities are available on Hugging Face with audio to follow shortly.\n\nSorry, your browser doesn't support playback for this video Gemma 3n analyzing images fully on-device", "label": "non_personal"}
{"title": "Imagen 4 is now available in the Gemini API and Google AI Studio", "url": "https://developers.googleblog.com/en/imagen-4-now-available-in-the-gemini-api-and-google-ai-studio/", "content": "We're thrilled to bring Imagen 4, our best text-to-image model yet, to paid preview in the Gemini API and for limited free testing in Google AI Studio. Imagen 4 offers significantly improved text rendering over our prior image models and pushes the boundaries of text-to-image generation quality.\n\n\n\nThe Imagen 4 Family: Imagen 4 and Imagen 4 Ultra\n\nWe’re introducing two models within the Imagen 4 family, built to serve a variety of creative needs:\n\n\n\nImagen 4: Your go-to for most tasks\n\nThis is our flagship text-to-image model designed to handle a wide range of image generation tasks with significant improvements in quality, particularly for text generation, over Imagen 3. Imagen 4 is priced at $0.04 per output image.\n\n\n\nImagen 4 Ultra: Precision for your prompts\n\nWhen you need your images to precisely follow instructions, Imagen 4 Ultra is the model for you. It's designed to produce outputs that are more highly aligned with your text prompts, achieving strong results compared to other leading image generation models. Imagen 4 Ultra is priced at $0.06 per output image.\n\nWe will introduce additional billing tiers in the coming weeks. In the meantime, you can request higher rate limits for Imagen 4 and 4 Ultra.\n\n\n\nSee Imagen 4 in action\n\nTo give you a glimpse of Imagen 4's capabilities, here are some examples of what you can create. Created using Imagen 4 Ultra, the prompts below showcase the model's versatility across various styles and content.\n\nPrompt: A 3-panel cosmic epic comic. Panel 1: Tiny 'Stardust' in nebula; radar shows anomaly (text 'ANOMALY DETECTED'), hull text 'stardust'. Pilot whispers. Panel 2: Bioluminescent leviathan emerges; console red text 'WARNING!. Panel 3: Leviathan chases ship through asteroids; console re text 'SHIELD CRITICAL!', screen text 'EVADE!'. Pilot screams, SFX 'CRUNCH!', 'ROOOOAAARR!'.", "label": "non_personal"}
{"title": "From idea to app: Introducing Stitch, a new way to design UIs", "url": "https://developers.googleblog.com/en/stitch-a-new-way-to-design-uis/", "content": "That's precisely the problem Stitch aims to solve – Stitch is a new experiment from Google Labs that allows you to turn simple prompt and image inputs into complex UI designs and frontend code in minutes.\n\nBuilding great applications always comes down to a powerful partnership between design and development. Designers envision the user experience, crafting intuitive and engaging interfaces. Developers then bring those designs to life with functional code. Traditionally, connecting design ideas to working code took a lot of manual effort and back-and-forth.\n\nStitch was born of an idea between a designer and an engineer, both looking to build a product that optimized their respective workflows. It leverages the multimodal capabilities of Gemini 2.5 Pro to create a more fluid and integrated workflow between design and development. And, with an option to refine your design with image inputs, an interactive chat, theme selectors, and a paste to Figma function, Stitch lets you truly hone in on your creative designs and development needs.\n\n\n\nHere’s what Stitch offers today to enhance your design and development process:\n\n\n\nGenerate UI from natural language\n\nDescribe the application you want to build in plain English, including details like color palettes or desired user experience. Stitch can generate a visual interface tailored to your description.\n\n\n\nGenerate UI from images or wireframes\n\nHave a design sketch on a whiteboard, a screenshot of a compelling UI, or a rough wireframe? Upload it to Stitch. Stitch processes the image to produce a corresponding digital UI, bridging your initial visual ideas to a functional design.\n\n\n\nRapid iteration and design exploration\n\nDesign is an iterative process, and Stitch facilitates this by allowing you to generate multiple variants of your interface. Experiment with different layouts, components, and styles to achieve the desired look and feel.\n\n\n\nSeamless transition to development\n\nOnce you're satisfied with your design, Stitch provides crucial bridges to the development workflow:\n\nPaste to Figma: Your generated design can be seamlessly pasted to Figma for easy further refinement, collaboration with design teams, and integration into existing design systems.\n\nExport front-end code: Stitch generates clean, functional front-end code based on your design, so you have a fully functional UI ready to go.\n\n\n\nStitch is about unlocking the magic of app creation for everyone. We're thrilled to bring this experiment to you and can't wait to see what you'll build with it.\n\nTry out Stitch at stitch.withgoogle.com and let us know what you think!", "label": "non_personal"}
{"title": "Google Play's billing system", "url": "https://developer.android.com/google/play/billing/", "content": "By Aug 31, 2025, all new apps and updates to existing apps must use Billing Library version 7 or newer. If you need more time to update your app, you can request an extension until Nov 1, 2025. Learn about Play Billing Library version deprecation\n\nGoogle Play's billing system is a service that enables you to sell digital products and content in your Android app, whether you want to monetize through one-time purchases or offer subscriptions to your services. Google Play offers a full set of APIs for integration with both your Android app and your server backend that unlock the familiarity and safety of Google Play purchases for your users.\n\nNote: Google Play's billing system is only for digital items. For physical goods and services, or other non-digital content, see the Google Pay SDK\n\nIntegration architecture\n\nThis section introduces the different functional modules that you can build and the APIs and libraries available to simplify the process.\n\nFigure 1. Diagram of a typical Google Play billing integration.\n\nYou can integrate Google Play's billing system with your Android app using the Play Billing Library. This library enables communication with the Google Play Services layer that provides the localized product offering available to each user in your app, as well as methods to handle other necessary user operations, like launching the purchase flow and handling its outcome.\n\nYou should also integrate Google Play's billing system with your server backend to create the necessary developer flows. This is essential to guarantee that your purchase management and cross-platform entitlements are efficient and secure. You can create this integration with the Subscriptions and in-app purchases API provided by the Google Play Developer API. The backend integration also leverages some Google Cloud platform tools.\n\nFigure 2. APIs and services provided by the Google Play Developer API.\n\nTerminology\n\nThis section lists and describes the high-level technologies and concepts that you might encounter when integrating Google Play's billing system into your app. Reference this list as you proceed through the integration guidance.\n\nTechnologies\n\nConcepts\n\nFlow . A flow shows the typical steps involved in a billing-related task. For example, a purchase flow outlines the steps involved when a user purchases your product. A subscription flow might show how a subscription transitions between states.\n\n. A flow shows the typical steps involved in a billing-related task. For example, a purchase flow outlines the steps involved when a user purchases your product. A subscription flow might show how a subscription transitions between states. Entitlement . When a user purchases an in-app product, they are then entitled to that product within your app. For one-time products, this means that the user should now have permanent access to the product. For subscriptions, this means that the user should have access while the subscription is active.\n\n. When a user purchases an in-app product, they are then entitled to that product within your app. For one-time products, this means that the user should now have permanent access to the product. For subscriptions, this means that the user should have access while the subscription is active. Product ID . The ID of a specific product type.\n\n. The ID of a specific product type. Purchase token . A string that represents a buyer's entitlement to a product on Google Play. It indicates that a Google user has paid for a specific product.\n\n. A string that represents a buyer's entitlement to a product on Google Play. It indicates that a Google user has paid for a specific product. Order ID. A string that represents a financial transaction on Google Play. An order ID is created every time a financial transaction occurs. This string is included in a receipt that is emailed to the buyer. You can use the order ID to manage refunds in the Order Management section of the Google Play Console. Order IDs are also used in sales and payout reports.\n\nNext steps\n\nTo begin integrating Google Play's billing system with your app and server backend, see the setup guide.", "label": "non_personal"}
{"title": "Sharing our Engineering Career Framework with the world", "url": "https://dropbox.tech/culture/sharing-our-engineering-career-framework-with-the-world", "content": "Looking for our latest Engineering Career Framework? We made some updates in 2023. Learn more about the newest version.\n\n~ ~ ~\n\nAt Dropbox, we strive to be a great place for all engineers to grow and be recognized for that growth. Our Engineering Career Framework helps keep us accountable there and is viewable by anyone within the company. Today, we are also making it viewable by anyone outside the company. Our goal in doing so is to be transparent externally on how we think about leveling and career progression on the Engineering team at Dropbox. It also enables others to adapt and apply it to their own organizations.\n\nIn early 2020, we completed a major revision to the framework. Our focus was to be more explicit about the “what” (i.e. business impact made) and create better representation for all the different crafts within engineering that make Dropbox successful. After a year of running this current version through several promotion cycles, we are satisfied that it is moving our culture in the right direction.\n\nDropbox’s Engineering Career Framework describes what’s expected for our engineers at each of our career levels. Along with helping managers set expectations and hold teams accountable for their work, this resource empowers engineers to achieve greater impact in their role and grow in their careers.\n\nThat said, the framework is not meant to be a promotion checklist; rather, it’s designed to help our engineers know what their impact could look like at the next level. This framework is also not an exhaustive list of examples and behaviors; each responsibility includes three to four key behaviors that serve as a guide for how to think about one’s work. Consequently, it’s worth noting that the framework should not be seen as a replacement for regular, active conversations between engineers and their managers about short and long-term career growth and development.\n\nTeams across our Engineering and People orgs worked together thoughtfully and closely to build this latest version of the Engineering Career Framework. As Dropbox grows and changes, expect it to be a living document that reflects Dropbox’s evolution as an Engineering team.", "label": "non_personal"}
{"title": "Introducing Gemma 3n: The developer guide", "url": "https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/", "content": "The first Gemma model launched early last year and has since grown into a thriving Gemmaverse of over 160 million collective downloads. This ecosystem includes our family of over a dozen specialized models for everything from safeguarding to medical applications and, most inspiringly, the countless innovations from the community. From innovators like Roboflow building enterprise computer vision to the Institute of Science Tokyo creating highly-capable Japanese Gemma variants, your work has shown us the path forward. Building on this incredible momentum, we're excited to announce the full release of Gemma 3n. While last month's preview offered a glimpse, today unlocks the full power of this mobile-first architecture. Gemma 3n is designed for the developer community that helped shape Gemma. It’s supported by your favorite tools including Hugging Face Transformers, llama.cpp, Google AI Edge, Ollama, MLX, and many others, enabling you to fine-tune and deploy for your specific on-device applications with ease. This post is the developer deep dive: we'll explore some of the innovations behind Gemma 3n, share new benchmark results, and show you how to start building today.\n\nWhat’s new in Gemma 3n? Gemma 3n represents a major advancement for on-device AI, bringing powerful multimodal capabilities to edge devices with performance previously only seen in last year's cloud-based frontier models.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nMultimodal by design: Gemma 3n natively supports image, audio, video, and text inputs and text outputs. Optimized for on-device: Engineered with a focus on efficiency, Gemma 3n models are available in two sizes based on effective parameters: E2B and E4B. While their raw parameter count is 5B and 8B respectively, architectural innovations allow them to run with a memory footprint comparable to traditional 2B and 4B models, operating with as little as 2GB (E2B) and 3GB (E4B) of memory. Groundbreaking architecture: At its core, Gemma 3n features novel components like the MatFormer architecture for compute flexibility, Per Layer Embeddings (PLE) for memory efficiency, LAuReL and AltUp for architectural efficiency, and new audio and MobileNet-v5 based vision encoders optimized for on-device use cases. Enhanced quality: Gemma 3n delivers quality improvements across multilinguality (supporting 140 languages for text and multimodal understanding of 35 languages), math, coding, and reasoning. The E4B version achieves an LMArena score over 1300, making it the first model under 10 billion parameters to reach this benchmark.\n\nAchieving this leap in on-device performance required rethinking the model from the ground up. The foundation is Gemma 3n’s unique mobile-first architecture, and it all starts with MatFormer.\n\nMatFormer: One model, many sizes At the core of Gemma 3n is the MatFormer (🪆Matryoshka Transformer) architecture, a novel nested transformer built for elastic inference. Think of it like Matryoshka dolls: a larger model contains smaller, fully functional versions of itself. This approach extends the concept of Matryoshka Representation Learning from just embeddings to all transformer components.\n\nDuring the MatFormer training of the 4B effective parameter (E4B) model, a 2B effective parameter (E2B) sub-model is simultaneously optimized within it, as shown in the figure above. This provides developers two powerful capabilities and use cases today: 1: Pre-extracted models: You can directly download and use either the main E4B model for the highest capabilities, or the standalone E2B sub-model which we have already extracted for you, offering up to 2x faster inference. 2: Custom sizes with Mix-n-Match: For more granular control tailored to specific hardware constraints, you can create a spectrum of custom-sized models between E2B and E4B using a method we call Mix-n-Match. This technique allows you to precisely slice the E4B model's parameters, primarily by adjusting the feed forward network hidden dimension per layer (from 8192 to 16384) and selectively skipping some layers. We are releasing the MatFormer Lab, a tool that shows how to retrieve these optimal models, which were identified by evaluating various settings on benchmarks like MMLU.\n\nMMLU scores for the pre-trained Gemma 3n checkpoints at different model sizes (using Mix-n-Match)\n\nLooking ahead, the MatFormer architecture also paves the way for elastic execution. While not part of today’s launched implementations, this capability allows a single deployed E4B model to dynamically switch between E4B and E2B inference paths on the fly, enabling real-time optimization of performance and memory usage based on the current task and device load.\n\nPer-Layer Embeddings (PLE): Unlocking more memory efficiency Gemma 3n models incorporate Per-Layer Embeddings (PLE). This innovation is tailored for on-device deployment as it dramatically improves model quality without increasing the high-speed memory footprint required on your device's accelerator (GPU/TPU). While the Gemma 3n E2B and E4B models have a total parameter count of 5B and 8B respectively, PLE allows a significant portion of these parameters (the embeddings associated with each layer) to be loaded and computed efficiently on the CPU. This means only the core transformer weights (approximately 2B for E2B and 4B for E4B) need to sit in the typically more constrained accelerator memory (VRAM).\n\nWith Per-Layer Embeddings, you can use Gemma 3n E2B while only having ~2B parameters loaded in your accelerator.\n\nKV Cache sharing: Faster long-context processing Processing long inputs, such as the sequences derived from audio and video streams, is essential for many advanced on-device multimodal applications. Gemma 3n introduces KV Cache Sharing, a feature designed to significantly accelerate time-to-first-token for streaming response applications. KV Cache Sharing optimizes how the model handles the initial input processing stage (often called the \"prefill\" phase). The keys and values of the middle layer from local and global attention are directly shared with all the top layers, delivering a notable 2x improvement on prefill performance compared to Gemma 3 4B. This means the model can ingest and understand lengthy prompt sequences much faster than before.\n\nAudio understanding: Introducing speech to text and translation Gemma 3n uses an advanced audio encoder based on the Universal Speech Model (USM). The encoder generates a token for every 160ms of audio (about 6 tokens per second), which are then integrated as input to the language model, providing a granular representation of the sound context. This integrated audio capability unlocks key features for on-device development, including: Automatic Speech Recognition (ASR): Enable high-quality speech-to-text transcription directly on the device. Automatic Speech Translation (AST): Translate spoken language into text in another language. We've observed particularly strong AST results for translation between English and Spanish, French, Italian, and Portuguese, offering great potential for developers targeting applications in these languages. For tasks like speech translation, leveraging Chain-of-Thought prompting can significantly enhance results. Here’s an example:", "label": "non_personal"}
{"title": "Here's the latest version of our Engineering Career Framework", "url": "https://dropbox.tech/culture/our-updated-engineering-career-framework", "content": "Two years ago, we shared the Engineering Career Framework we use at Dropbox. The framework is intended to help our engineers have greater impact in their roles and on their teams. For each role, we outline Core Responsibilities, or CRs—key behaviors that define what impactful work looks like and how to deliver it. We hoped that by providing consistent expectations for each level, we could better help Dropboxers grow in their engineering careers. The framework is now widely used as a reference during interviewing and hiring, performance reviews and calibrations, and the rating and promotion process. We recently refreshed this framework to address some specific needs and reduce ambiguity. Along with sharing the updates, we wanted to go into a little more detail about how we manage this framework and what the update process is like. We used these updates for the 2022 review and calibration cycle, and feedback indicates that the changes did improve the clarity and accuracy of the framework descriptions—though some issues remain. Our work here is not finished. We have updated the originally published framework in place, but to summarize: We added both technical craft expectations for engineering managers, and business acumen expectations for managers and many higher-level roles.\n\nWe clarified and expanded on expectations for decision-making, collaboration, and contributions to organizational health.\n\nWe strengthened and clarified the expectations for ownership over code, processes, and operational systems. We hope that sharing some of our thought process here may prove useful to others undertaking a similar project. We’ll talk about why we decided to make the update and what changes we focused on for this round of changes. Then we’ll explain how we deployed the framework operationally, and our new technical approach for making updates, now and in the future. Finally, we’ll look in more detail at the feedback we received and the path ahead.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images—plus generative AI capabilities across even more connected apps. See what's new →\n\nIn late 2021, Dropbox staff engineers gathered for our annual summit where we assessed the state of engineering at Dropbox. Our goal was to identify concrete things we could improve. Two relevant initiatives came out of that effort: Update the CRs to promote engineering efficiency\n\nProvide clearer paths and CRs to help senior engineers grow These initiatives were based on the perception that reviews and promotions were too heavily biased towards big, complex, or high-profile achievements. Taking ownership, making tough decisions, or doing so-called keeping-the-lights-on (KTLO) work is no less important, but was often deemed less impactful. We heard this resulted in a tendency to build the wrong things or build them in the wrong way. We also received repeated feedback that the CRs were not as helpful for senior engineers looking to understand their performance relative to expectations. This was especially the case for more experienced engineers looking for paths to promotion. Several high-level engineers and engineering managers (EMs) formed a working group to update our framework accordingly. This was done in coordination with a broader Dropbox-wide initiative to refresh our CRs to take into account Virtual First, as well as the critical role that managers play in building and sustaining successful cultures. Because the CRs explicitly state the expectations of engineers at all levels, they can be a powerful lever for affecting cultural change. We had the explicit goal of better rewarding the kinds of engineering behaviors that we wanted to see, and more clearly defining the different roles or archetypes more senior engineers can function in and how those map to the CRs. Specifically, we wanted promotion evaluations to favor a variety of possible contributions, rather than over-indexing on the building of large, complex things.\n\nWhat changes did we make?\n\nWe started with a survey of engineers on how well the CRs mapped to the work they were doing and how they were evaluated. Of 106 respondents, more than a quarter reported they didn’t feel the CRs reflected the work they were doing on a daily basis. More than a fifth reported that they didn’t clearly understand what was expected to get to the next level. While the career framework explicitly states that it is not intended to be a checklist for promotion, the fact remains that because it is the canonical source of level expectations, some people will use it in this way. Much of the feedback we received centered on how the framework didn’t effectively capture how different CRs were weighted at review time, and how recognition for chores like on-call toil, KLTO, glue work, and documentation were under-valued. We also heard a desire for increased clarity around how more senior engineers (IC4+) could fulfill various roles or archetypes. For instance, how might a specific IC4 who is functioning more as a tech lead be evaluated fairly in comparison with an IC4 who is functioning more as an architect? To address this feedback, we made numerous minor updates to the CRs and descriptions, particularly at higher levels, in order to encourage the behaviors we want engineers to exemplify. For example: Before, IC3 software engineers didn’t have clear expectations of what ownership looked like at that level, so we added the following craft CR: “I look for ways to reduce future toil and tech debt for existing components my team owns.”\n\nTo ensure security-by-design, we often ask IC4 security engineers to collaborate with other teams during the early stages of an effort—but this wasn’t expressed in the CRs and wasn’t being assessed consistently in calibrations. We added the following culture CR: “I am effective at working with cross-functional stakeholders to identify technical blindspots and clarify ambiguity in their ideas.”\n\nIC3+ software engineers didn’t have guidance about expectations for their role within the larger organization beyond their team, so we added a business acumen CR, tailored per level. This is an example of what it looks like for an IC4: “I have a working knowledge of Dropbox’s org/team structure and how teams work together across Dropbox, and am able to help my team effectively collaborate effectively with other teams across our org.”\n\nA sample of some of the updates to the IC4 Software Engineer CRs that shows the level of thought and work that goes into modifying each line\n\nWe also added two special sections to the appendix of the framework. One explains and highlights different Engineering Archetype behaviors—heavily derived from Will Larson’s definitions—and how they map to levels and CRs. The other provides context and clarification around the CRs more generally and dispels myths about how they are to be used. The goal of these sections is to add archetypes and related behaviors to the language of growth and evaluation here at Dropbox, and to be a single source for greater clarity on how evaluations actually work.\n\nOperational approach\n\nWe wanted to deploy the updated framework quickly so that we could see the benefit from the changes. However, we saw early on that suddenly changing the evaluation criteria for anyone would feel like moving the goalposts mid-cycle, and therefore not align with one of our company values, Make Work Human. So our working group coordinated with the People team and established a plan for previewing changes early, before the mid-year lightweight review cycle, in order to get feedback and encourage engineering ICs and EMs to start aligning with the new descriptions. After the mid-year cycle, we incorporated the feedback, then heavily promoted and circulated the updates—via Slack announcements, emails, engineering all-hands meetings, etc. This was to ensure everyone was aware of the changes and had plenty of lead time to incorporate them into their models of how they and others would be evaluated. We wanted to make sure that nobody felt like we were moving the goalposts on them, even while we worked out the kinks and clarified things in response to feedback. Here’s our plan, right out of our working group coordination doc: Establish working group and work streams\n\nCollect feedback via survey\n\nUpdate and iterate on CRs\n\nShare with VPs and HR to get early feedback and update\n\nCollect feedback from engineering leadership team and update\n\nShare internally with all of engineering\n\nIncorporate any feedback and publish final updates\n\nPublish externally\n\nTechnical approach\n\nWith these changes in mind—and looking ahead to future iterations of this framework—we also went looking for a better way to manage updates to the underlying documents. We wanted a system that would, among other things, let us: facilitate the review and approval process preserve a history of changes over time and allow comparison between versions and related roles easily (re)generate content based on controlled source files have multiple, simultaneous, independent updates exist in various stages of the process Naturally, if you put a bunch of engineers and EMs in a room and give them requirements like this, their solutions will inevitably involve bespoke programming and source control—and we are no exception. Early proposals got fairly sophisticated; at one point, we considered modeling each role as a class hierarchy and then generating comparisons programmatically. But in the end, we took our own advice—impactful work doesn’t always have to be big or complex—and settled for something simple. We set up an internal git repository to contain the text content of the framework, and a bare-bones Django site for simple internal hosting. The underlying documents are stored in a folder hierarchy of markdown files, organized by role. Each file is a complete, self-contained description of the CRs for a single role and level (for example, “IC5 Staff Security Engineer”).\n\nUsing a comparison tool to assess changes to the framework in the process of writing this article\n\nStoring the CRs as markdown files in a git repository comes with some big advantages. They are easily rendered, but the source is also human readable and diff-able. No external database, CRM, or other system is necessary to maintain the contents outside of the source tree; updates can be controlled according to normal source code review processes. And when someone decides we need to change how we present the framework, the markdown files are easily portable to any number of static site generation systems. Here’s our guidance for any Dropboxer who wants to update the framework, directly from the README.md file: The Engineering Career Framework is a living document that will evolve over time, and anyone within engineering at Dropbox should feel empowered to suggest changes. For lightweight changes such as syntax or formatting changes, a simple diff should be reviewable by the members of the Engineering Career Framework working group and result in changes being visible by the next deployment [of our the internal tool]. Larger-scale changes such as significant rewording, adding/removing copy, or level/role changes will involve multiple levels of approvals. While these cannot be merged through a single diff approval, the diffs themselves can be used as the first step in surfacing where the engineering community at large feels there is a gap between the CRs and their day-to-day responsibilities. With this system in place, each individual update to the CRs can be made with the following process: Create a diff with the desired changes and circulate to stakeholders. Review the changes with stakeholders and respond to feedback. Update the changelog with a description of the changes, then merge the diff. Deploy the updated source to the internal mini-site.\n\nExample of a diff correcting a typographical error\n\nWe implemented the changes described above as we worked the kinks out of the process. We found that minor, non-material changes (like correcting typographical errors, removing duplication, etc.) could easily be applied with a relatively light review, while more substantial updates could be reviewed with increased rigor, as appropriate. When it came time to publish, we were able to export a static version of the framework and update our externally-facing repo and site. The externally published version is for transparency and the benefit of the community at large. Naturally, we don’t take pull requests on it.\n\nResults and path forward\n\nAfter launching this framework internally and using it for an entire full-year review cycle, our working group again ran a survey asking engineers who used the framework about the changes. We didn’t have as many respondents as the first survey, but nine out of ten agreed that the updated framework better reflects the work they are doing at their level, which is confirmation that this round of updates is a step in the right direction. We also received a lot of valuable comments. Some suggested the new archetypes are valuable, but could use more clarity around how they tend to be expressed in lower levels. Others felt the CRs are still too wordy and vague (most of this update cycle was spent adding clarification, not trimming and streamlining). And engineers still wanted a clearer explanation of the differences between levels (while the files are structured for easy comparison using a comparison tool, it would be better to make this a first-class capability of the presentation system). After going through this whole process, we found some downsides to using markdown, too. The markdown table syntax, while easily human-readable, is not at all conducive to tracking changes across large tables full of text—as we discovered the hard way. Reformatting the pages to avoid the use of tables mitigated this, but we had plenty of feedback that removing the tables made the text harder to ingest and compare. We also discovered the joys of maintaining large blocks of text in source control, and trying to decide whether to keep things in single lines, or apply line breaks. Late in the game we discovered SemBr, an approach for breaking up text with line breaks on semantic boundaries for easier comparison. We are looking to adopt this moving forward as the framework continues to evolve. Finally, we found that because so many of the CRs across roles have similar language, updating wording can be cumbersome, both for implementation and review. Storing each individual role as separate text allows for easy tailoring, but makes maintaining consistency more costly. We’re still exploring ways to do both, but we haven’t landed on a solution for this yet.\n\nConclusion", "label": "non_personal"}
{"title": "Lessons learned: Using a cybersecurity vendor to check for malicious links", "url": "https://dropbox.tech/security/changing-how-we-identify-malicious-urls-in-shared-documents", "content": "Dropbox employs numerous industry-standard measures to prevent our services from being used for malicious purposes. This includes working with trusted third-party vendors to help us identify viruses, malware, and phishing attempts. One of these trusted vendors* previously helped us identify malicious URLs embedded within documents shared using Dropbox. However, we recently discovered that the URLs we submitted were made visible to our vendor’s other paid subscribers and partners. As soon as we became aware of the situation, we immediately stopped submitting URLs to the vendor and worked with them to successfully remove the URLs from their database. To be clear: no files were ever submitted. Our investigation found 0.5% of registered Dropbox users and 10% of registered DocSend users were affected. We have no evidence that these URLs were ever exploited by malicious actors.\n\nWhat happened\n\nOn February 28, 2023, based on a report submitted to our bug bounty program, we became aware that URLs originating from Dropbox and DocSend were present in a database used to check for potential malware by the vendor’s paid subscribers and partners. In response, we immediately stopped submitting URLs and began to investigate. We soon found that, due to an implementation error on our part, URLs—and only the URLs—embedded within a document shared using Dropbox or uploaded to DocSend were visible to the vendor’s paid subscribers and partners. Neither the document itself, or any other information within it, were ever submitted.\n\nIn addition, any access controls on the embedded URLs—such as password protection, authentication measures, or other restrictions—remain intact. Out of an abundance of caution, we worked with our vendor to successfully remove the URLs from their database.\n\nOur tools enable collaboration—but unfortunately, malicious actors often try to use the same tools to trick Dropbox customers and the community into downloading malicious content or redirecting them to malicious sites to steal their data. To help keep everyone safe online, we have safeguards in place when people use Dropbox to share documents that contain embedded URLs. Checking URLs for malware and phishing is a standard practice across the industry, and using this vendor to check whether URLs in shared Dropbox documents are safe was one of our techniques.\n\nWhat we’re doing next", "label": "non_personal"}
{"title": "Detecting memory leaks in Android applications", "url": "https://dropbox.tech/mobile/detecting-memory-leaks-in-android-applications", "content": "Memory leaks occur when an application allocates memory for an object, but then fails to release the memory when the object is no longer being used. Over time, leaked memory accumulates and results in poor app performance and even crashes. Leaks can happen in any program and on any platform, but they’re especially prevalent in Android apps due to complications with activity lifecycles. Recent Android patterns such as ViewModel and LifecycleObserver can help avoid memory leaks, but if you’re following older patterns or don’t know what to look out for, it’s easy to let mistakes slip through.\n\nCommon examples\n\nReference to a long-lived service\n\nA fragment references an activity which references a long-lived service.\n\nIn this case, we have a standard setup with an activity that holds a reference to some long-living service, then a fragment and its view that hold references to the activity. For example, say that the activity somehow creates a reference to its child fragment. Then, for as long as the activity sticks around, the fragment will continue living too. This causes a leak for the duration between the fragment’s onDestroy and the activity’s onDestroy.\n\nThe fragment will never be used again, yet it persists in memory.\n\nLong-lived service which references a fragment’s view What if, in the other direction, the service obtained a reference to the fragment’s view? First, the view would now stay alive for the entire duration of the service. Furthermore, because the view holds a reference to its parent activity, the activity now leaks as well.\n\nAs long as the Service lives, the FragmentView and Activity will squander memory.\n\nDetecting memory leaks\n\nNow that we know how memory leaks happen, let’s discuss what we can do to detect them. An obvious first step is to check if your app ever crashes due to OutOfMemoryError. Unless there’s a single screen that eats more memory than your phone has available, you have a memory leak somewhere.\n\nThis approach only tells you the existence of the problem—not the root cause. The memory leak could have happened anywhere, and the crash that’s logged doesn’t point to the leak, only to the screen that finally tipped memory usage over the limit. You could inspect all the breadcrumbs to see if there’s some similarity, but chances are the culprit won’t be easy to discern. Let’s explore other options. LeakCanary One of the best tools out there is LeakCanary, a memory leak detection library for Android. We simply add a dependency on our build.gradle file. The next time we install and run our app, LeakCanary will be running alongside it. As we navigate through our app, LeakCanary will pause occasionally to dump the memory and provide leak traces of detected leaks. This one step is vastly better than what we had before. But the process is still manual, and each developer will only have a local copy of the memory leaks they’ve personally encountered. We can do better! LeakCanary and Bugsnag LeakCanary provides a very handy code recipe for uploading found leaks to Bugsnag. We’re then able to track memory leaks just as we do any other warning or crash in the app. We can even take this one step further and use Bugsnag’s integrations to hook it up to project management software such as Jira for even more visibility and accountability.\n\nBugsnag connected to Jira\n\nLeakCanary and integration tests Another way to improve automation is to hook up LeakCanary to CI tests. Again, we are given a code recipe to start with. From the official documentation: LeakCanary provides an artifact dedicated to detecting leaks in UI tests which provides a run listener that waits for the end of a test, and if the test succeeds then it looks for retained objects, trigger a heap dump if needed and perform an analysis. Be aware that LeakCanary will slow down testing, as it dumps the heap after each test to which it listens. In our case, because of our selective testing and sharding set up, the extra time added is negligible. Our end result is that memory leaks are surfaced just as any other build or test failure on CI, with the leak trace at the time of the leak recorded. Running LeakCanary on CI has helped us learn better coding patterns, especially when it comes to new libraries, before any code hits production. For example, it caught this leak when we were working with MvRx mocks:\n\nCopy <failure>Test failed because application memory leaks were detected: ==================================== HEAP ANALYSIS RESULT ==================================== 4 APPLICATION LEAKS References underlined with \"~~~\" are likely causes. Learn more at https://squ.re/leaks. 198449 bytes retained by leaking objects Signature: 6bf2ba80511dcb6ab9697257143e3071fca4 ┬─── │ GC Root: System class │ ├─ com.airbnb.mvrx.mocking.MockableMavericks class │ Leaking: NO (a class is never leaking) │ ↓ static MockableMavericks.mockStateHolder │ ~~~~~~~~~~~~~~~ ├─ com.airbnb.mvrx.mocking.MockStateHolder instance │ Leaking: UNKNOWN │ ↓ MockStateHolder.delegateInfoMap │ ~~~~~~~~~~~~~~~ ├─ java.util.LinkedHashMap instance │ Leaking: UNKNOWN │ ↓ LinkedHashMap.header │ ~~~~~~ ├─ java.util.LinkedHashMap$LinkedEntry instance │ Leaking: UNKNOWN │ ↓ LinkedHashMap$LinkedEntry.prv │ ~~~ ├─ java.util.LinkedHashMap$LinkedEntry instance │ Leaking: UNKNOWN │ ↓ LinkedHashMap$LinkedEntry.key │ ~~~ ╰→ com.dropbox.product.android.dbapp.photos.ui.view.PhotosFragment instance Leaking: YES (ObjectWatcher was watching this because com.dropbox.product.android.dbapp.photos.ui.view.PhotosFragment received Fragment#onDestroy() callback and Fragment#mFragmentManager is null) key = 391c9051-ad2c-4282-9279-d7df13d205c3 watchDurationMillis = 7304 retainedDurationMillis = 2304 198427 bytes retained by leaking objects Signature: d1c9f9707034dd15604d8f2e63ff3bf3ecb61f8\n\nIt turned out that we hadn’t properly cleaned up the mocks when writing the test. Adding a few lines of code avoids the leak:\n\nCopy @After fun teardown() { scenario.close() val holder = MockableMavericks.mockStateHolder holder.clearAllMocks() }\n\nYou may be wondering: Since this memory leak only happens in tests, is it really that important to fix? Well, that’s up to you! Like linters, leak detection can tell you when there’s code smell or bad coding patterns. It can help teach engineers to write more robust code—in this case, we learned about the existence of clearAllMocks(). The severity of a leak and whether or not it’s imperative to fix are decisions an engineer can make. For tests on which we don’t want to run leak detection, we wrote a simple annotation:\n\nCopy @Retention(RetentionPolicy.RUNTIME) @Target({ElementType.METHOD, ElementType.TYPE}) public @interface SkipLeakDetection { /** * The reason why the test should skip leak detection. */ String value(); }\n\nand in our class which overrides LeakCanary’s FailOnLeakRunListener():\n\nCopy override fun skipLeakDetectionReason(description: Description): String? { return when { description.getAnnotation(SkipLeakDetection::class.java) != null -> \"is annotated with @SkipLeakDetection\" description.testClass.isAnnotationPresent(SkipLeakDetection::class.java) -> \"class is annotated with @SkipLeakDetection\" else -> null } }\n\nIndividual tests or entire test classes can use this annotation to skip leak detection.\n\nFixing memory leaks\n\nNow that we’ve gone over various ways to find and surface memory leaks, let’s talk about how to actually understand and fix them. The leak trace provided by LeakCanary will be the single most useful tool for diagnosing a leak. Essentially, the leak trace prints out a chain of references associated with the leaked object, and provides an explanation of why it’s considered a leak. LeakCanary already has great documentation on how to read and use its leak trace, so there’s no need to repeat it here. Instead, let’s go over two categories of memory leaks that I mostly found myself dealing with. Views It’s common to see views declared as class level variables in fragments: private TextView myTextView; or, now that more Android code is being written in Kotlin: private lateinit var myTextView: TextView—common enough for us not to realize that these can all cause memory leaks. Unless these fields are nulled out in the fragment’s onDestroyView, (which you can’t do for a lateinit variable), the references to the views now live for the duration of the fragment’s lifecycle, and not the fragment’s view lifecycle as they should. The simplest scenario of how this causes a leak: We are on FragmentA. We navigate to FragmentB, and now FragmentA is on the back stack. FragmentA is not destroyed, but FragmentA’s view is destroyed. Any views that are tied to FragmentA’s lifecycle are now held in memory when they don’t need to be. For the most part, these leaks are small enough to not cause any performance issues or crashes. But for views that hold objects and data, images, view/data binding and the like, we are more likely to run into trouble. So when possible, avoid storing views in class-level variables, or be sure to clean them up properly in onDestroyView. Speaking of view/data binding, Android’s view binding documentation tells us exactly that: the field must be cleared to prevent leaks. Their code snippet recommends we do the following:\n\nCopy private var _binding: ResultProfileBinding? = null // This property is only valid between onCreateView and // onDestroyView. private val binding get() = _binding!! override fun onCreateView(inflater: LayoutInflater, container: ViewGroup?, savedInstanceState: Bundle? ): View? { _binding = ResultProfileBinding.inflate(inflater, container, false) val view = binding.root return view } override fun onDestroyView() { super.onDestroyView() _binding = null }\n\nThis is lot of boilerplate to put in every fragment (also, avoid using !! which will throw a KotlinNullPointerException if the variable is null. Use explicit null handling instead.) We addressed this issue is by creating a ViewBindingHolder (and DataBindingHolder) that fragments can then implement:\n\nCopy interface ViewBindingHolder<B : ViewBinding> { var binding: B? // Only valid between onCreateView and onDestroyView. fun requireBinding() = checkNotNull(binding) fun requireBinding(lambda: (B) -> Unit) { binding?.let { lambda(it) }} /** * Make sure to use this with Fragment.viewLifecycleOwner */ fun registerBinding(binding: B, lifecycleOwner: LifecycleOwner) { this.binding = binding lifecycleOwner.lifecycle.addObserver(object : DefaultLifecycleObserver { override fun onDestroy(owner: LifecycleOwner) { owner.lifecycle.removeObserver(this) this@ViewBindingHolder.binding = null } }) } } interface DataBindingHolder<B : ViewDataBinding> : ViewBindingHolder<B>\n\nThis provides an easy and clean way for fragments to: Ensure binding is present when it’s required\n\nOnly execute certain code if the binding is available\n\nClean up binding on onDestroyView automatically Temporal leaks These are leaks that only stick around for a short duration of time. In particular, one that we ran into was caused by an EditTextView's async task. The async task lasted just longer than LeakCanary’s default wait time, so a leak was reported even though the memory was cleaned up properly soon afterward. If you suspect you are running into a temporal leak, a good way to check is to use Android Studio’s memory profiler. Once you start a session within the profiler, take the steps to reproduce the leak, but wait for a longer period of time before dumping the heap and inspecting. The leak may be gone after the extra time.\n\nAndroid Studio’s memory profiler shows the effect of temporal leaks that get cleaned up.\n\nTest often, fix early", "label": "non_personal"}
{"title": "Making camera uploads for Android faster and more reliable", "url": "https://dropbox.tech/mobile/making-camera-uploads-for-android-faster-and-more-reliable", "content": "Camera uploads is a feature in our Android and iOS apps that automatically backs up a user’s photos and videos from their mobile device to Dropbox. The feature was first introduced in 2012, and uploads millions of photos and videos for hundreds of thousands of users every day. People who use camera uploads are some of our most dedicated and engaged users. They care deeply about their photo libraries, and expect their backups to be quick and dependable every time. It’s important that we offer a service they can trust.\n\nUntil recently, camera uploads was built on a C++ library shared between the Android and iOS Dropbox apps. This library served us well for a long time, uploading billions of images over many years. However, it had numerous problems. The shared code had grown polluted with complex platform-specific hacks that made it difficult to understand and risky to change. This risk was compounded by a lack of tooling support, and a shortage of in-house C++ expertise. Plus, after more than five years in production, the C++ implementation was beginning to show its age. It was unaware of platform-specific restrictions on background processes, had bugs that could delay uploads for long periods of time, and made outage recovery difficult and time-consuming. In 2019, we decided that rewriting the feature was the best way to offer a reliable, trustworthy user experience for years to come. This time, Android and iOS implementations would be separate and use platform-native languages (Kotlin and Swift respectively) and libraries (such as WorkManager and Room for Android). The implementations could then be optimized for each platform and evolve independently, without being constrained by design decisions from the other. This post is about some of the design, validation, and release decisions we made while building the new camera uploads feature for Android, which we released to all users during the summer of 2021. The project shipped successfully, with no outages or major issues; error rates went down, and upload performance greatly improved. If you haven’t already enabled camera uploads, you should try it out for yourself.\n\n\n\nDesigning for background reliability\n\nThe main value proposition of camera uploads is that it works silently in the background. For users who don’t open the app for weeks or even months at a time, new photos should still upload promptly.\n\nHow does this work? When someone takes a new photo or modifies an existing photo, the OS notifies the Dropbox mobile app. A background worker we call the scanner carefully identifies all the photos (or videos) that haven’t yet been uploaded to Dropbox and queues them for upload. Then another background worker, the uploader, batch uploads all the photos in the queue. Uploading is a two step process. First, like many Dropbox systems, we break the file into 4 MB blocks, compute the hash of each block, and upload each block to the server. Once all the file blocks are uploaded, we make a final commit request to the server with a list of all block hashes in the file. This creates a new file consisting of those blocks in the user’s Camera Uploads folder. Photos and videos uploaded to this folder can then be accessed from any linked device.\n\nOne of our biggest challenges is that Android places strong constraints on how often apps can run in the background and what capabilities they have. For example, App Standby limits our background network access if the Dropbox app hasn’t recently been foregrounded. This means we might only be allowed to access the network for a 10-minute interval once every 24 hours. These restrictions have grown more strict in recent versions of Android, and the cross-platform C++ version of camera uploads was not well-equipped to handle them. It would sometimes try to perform uploads that were doomed to fail because of a lack of network access, or fail to restart uploads during the system-provided window when network access became available. Our rewrite does not escape these background restrictions; they still apply unless the user chooses to disable them in Android’s system settings. However, we reduce delays as much as possible by taking maximum advantage of the network access we do receive. We use WorkManager to handle these background constraints for us, guaranteeing that uploads are attempted if, and only if, network access becomes available. Unlike our C++ implementation, we also do as much work as possible while offline—for example, by performing rudimentary checks on new photos for duplicates—before asking WorkManager to schedule us for network access.\n\nMeasuring interactions with our status banners helps us identify emerging issues in our apps, and is a helpful signal in our efforts to eliminate errors. After the rewrite was released, we saw users interacting with more “all done” statuses than usual, while the number of “waiting” or error status interactions went down. (This data reflects only paid users, but non-paying users show similar results.)\n\nTo further optimize use of our limited network access, we also refined our handling of failed uploads. C++ camera uploads aggressively retried failed uploads an unlimited number of times. In the rewrite we added backoff intervals between retry attempts, and also tuned our retry behavior for different error categories. If an error is likely to be transient, we retry multiple times. If it’s likely to be permanent, we don’t bother retrying at all. As a result, we make fewer overall retry attempts—which limits network and battery usage—and users see fewer errors.\n\n\n\nDesigning for performance\n\nOur users don’t just expect camera uploads to work reliably. They also expect their photos to upload quickly, and without wasting system resources. We were able to make some big improvements here. For instance, first-time uploads of large photo libraries now finish up to four times faster. There are a few ways our new implementation achieves this.\n\nParallel uploads\n\nFirst, we substantially improved performance by adding support for parallel uploads. The C++ version uploaded only one file at a time. Early in the rewrite, we collaborated with our iOS and backend infrastructure colleagues to design an updated commit endpoint with support for parallel uploads. Once the server constraint was gone, Kotlin coroutines made it easy to run uploads concurrently. Although Kotlin Flows are typically processed sequentially, the available operators are flexible enough to serve as building blocks for powerful custom operators that support concurrent processing. These operators can be chained declaratively to produce code that’s much simpler, and has less overhead, than the manual thread management that would’ve been necessary in C++.\n\n\n\nCopy val uploadResults = mediaUploadStore .getPendingUploads() .unorderedConcurrentMap(concurrentUploadCount) { mediaUploader.upload(it) } .takeUntil { it != UploadTaskResult.SUCCESS } .toList()\n\nA simple example of a concurrent upload pipeline. unorderedConcurrentMap is a custom operator that combines the built-in flatMapMerge and transform operators.\n\nOptimizing memory use\n\nAfter adding support for parallel uploads, we saw a big uptick in out-of-memory crashes from our early testers. A number of improvements were required to make parallel uploads stable enough for production. First, we modified our uploader to dynamically vary the number of simultaneous uploads based on the amount of available system memory. This way, devices with lots of memory could enjoy the fastest possible uploads, while older devices would not be overwhelmed. However, we were still seeing much higher memory usage than we expected, so we used the memory profiler to take a closer look. The first thing we noticed was that memory consumption wasn’t returning to its pre-upload baseline after all uploads were done. It turned out this was due to an unfortunate behavior of the Java NIO API. It created an in-memory cache on every thread where we read a file, and once created, the cache could never be destroyed. Since we read files with the threadpool-backed IO dispatcher, we typically ended up with many of these caches, one for each dispatcher thread we used. We resolved this by switching to direct byte buffers, which don’t allocate this cache. The next thing we noticed were large spikes in memory usage when uploading, especially with larger files. During each upload, we read the file in blocks, copying each block into a ByteArray for further processing. We never created a new byte array until the previous one had gone out of scope, so we expected only one to be in-memory at a time. However, it turned out that when we allocated a large number of byte arrays in a short time, the garbage collector could not free them quickly enough, causing a transient memory spike. We resolved this issue by re-using the same buffer for all block reads. Parallel scanning and uploading\n\nIn the C++ implementation of camera uploads, uploading could not start until we finished scanning a user’s photo library for changes. To avoid upload delays, each scan only looked at changes that were newer than what was seen in the previous scan. This approach had downsides. There were some edge cases where photos with misleading timestamps could be skipped completely. If we ever missed photos due to a bug or OS change, shipping a fix wasn’t enough to recover; we also had to clear affected users’ saved scan timestamps to force a full re-scan. Plus, when camera uploads was first enabled, we still had to check everything before uploading anything. This wasn’t a great first impression for new users.\n\nIn the rewrite, we ensured correctness by re-scanning the whole library after every change. We also parallelized uploading and scanning, so new photos can start uploading while we’re still scanning older ones. This means that although re-scanning can take longer, the uploads themselves still start and finish promptly.\n\nValidation\n\nA rewrite of this magnitude is risky to ship. It has dangerous failure modes that might only show up at scale, such as corrupting one out of every million uploads. Plus, as with most rewrites, we could not avoid introducing new bugs because we did not understand—or even know about—every edge case handled by the old system. We were reminded of this at the start of the project when we tried to remove some ancient camera uploads code that we thought was dead, and instead ended up DDOSing Dropbox’s crash reporting service. 🙃\n\nHash validation in production\n\nDuring early development, we validated many low-level components by running them in production alongside their C++ counterparts and then comparing the outputs. This let us confirm that the new components were working correctly before we started relying on their results. One of those components was a Kotlin implementation of the hashing algorithms that we use to identify photos. Because these hashes are used for de-duplication, unexpected things could happen if the hashes change for even a tiny percentage of photos. For instance, we might re-upload old photos believing they are new. When we ran our Kotlin code alongside the C++ implementation, both implementations almost always returned matching hashes, but they differed about 0.005% of the time. Which implementation was wrong? To answer this, we added some additional logging. In cases where Kotlin and C++ disagreed, we checked if the server subsequently rejected the upload because of a hash mismatch, and if so, what hash it was expecting. We saw that the server was expecting the Kotlin hashes, giving us high confidence the C++ hashes were wrong. This was great news, since it meant we had fixed a rare bug we didn’t even know we had. Validating state transitions\n\nCamera uploads uses a database to track each photo’s upload state. Typically, the scanner adds photos in state NEW and then moves them to PENDING (or DONE if they don’t need to be uploaded). The uploader tries to upload PENDING photos and then moves them to DONE or ERROR. Since we parallelize so much work, it’s normal for multiple parts of the system to read and write this state database simultaneously. Individual reads and writes are guaranteed to happen sequentially, but we’re still vulnerable to subtle bugs where multiple workers try to change the state in redundant or contradictory ways. Since unit tests only cover single components in isolation, they won’t catch these bugs. Even an integration test might miss rare race conditions. In the rewritten version of camera uploads, we guard against this by validating every state update against a set of allowed state transitions. For instance, we stipulate that a photo can never move from ERROR to DONE without passing back through PENDING. Unexpected state transitions could indicate a serious bug, so if we see one, we stop uploading and report an exception.\n\nThese checks helped us detect a nasty bug early in our rollout. We started to see a high volume of exceptions in our logs that were caused when camera uploads tried to transition photos from DONE to DONE. This made us realize we were uploading some photos multiple times! The root cause was a surprising behavior in WorkManager where unique workers can restart before the previous instance is fully cancelled. No duplicate files were being created because the server rejects them, but the redundant uploads were wasting bandwidth and time. Once we fixed the issue, upload throughput dramatically improved.\n\nRolling it out\n\nEven after all this validation, we still had to be cautious during the rollout. The fully-integrated system was more complex than its parts, and we’d also need to contend with a long tail of rare device types that are not represented in our internal user testing pool. We also needed to continue to meet or surpass the high expectations of all our users who rely on camera uploads.\n\nTo reduce this risk preemptively, we made sure to support rollbacks from the new version to the C++ version. For instance, we ensured that all user preference changes made in the new version would apply to the old version as well. In the end we never ended up needing to roll back, but it was still worth the effort to have the option available in case of disaster. We started our rollout with an opt-in pool of beta (Play Store early access) users who receive a new version of the Dropbox Android app every week. This pool of users was large enough to surface rare errors and collect key performance metrics such as upload success rate. We monitored these key metrics in this population for a number of months to gain confidence it was ready to ship widely. We discovered many problems during this time period, but the fast beta release cadence allowed us to iterate and fix them quickly. We also monitored many metrics that could hint at future problems. To make sure our uploader wasn’t falling behind over time, we watched for signs of ever-growing backlogs of photos waiting to upload. We tracked retry success rates by error type, and used this to fine-tune our retry algorithm. Last but not least, we also paid close attention to feedback and support tickets we received from users, which helped surface bugs that our metrics had missed. When we finally released the new version of camera uploads to all users, it was clear our months spent in beta had paid off. Our metrics held steady through the rollout and we had no major surprises, with improved reliability and low error rates right out of the gate. In fact, we ended up finishing the rollout ahead of schedule. Since we’d front-loaded so much quality improvement work into the beta period (with its weekly releases), we didn’t have any multi-week delays waiting for critical bug fixes to roll out in the stable releases.\n\nSo, was it worth it?\n\nRewriting a big legacy feature isn’t always the right decision. Rewrites are extremely time-consuming—the Android version alone took two people working for two full years—and can easily cause major regressions or outages. In order to be worthwhile, a rewrite needs to deliver tangible value by improving the user experience, saving engineering time and effort in the long term, or both.\n\nWhat advice do we have for others who are beginning a project like this? Define your goals and how you will measure them. At the start, this is important to make sure that the benefits will justify the effort. At the end, it will help you determine whether you got the results you wanted. Some goals (for example, future resilience against OS changes) may not be quantifiable—and that’s OK—but it’s good to spell out which ones are and aren’t.\n\nAt the start, this is important to make sure that the benefits will justify the effort. At the end, it will help you determine whether you got the results you wanted. Some goals (for example, future resilience against OS changes) may not be quantifiable—and that’s OK—but it’s good to spell out which ones are and aren’t. De-risk it. Identify the components (or system-wide interactions) that would cause the biggest problems if they failed, and guard against those failures from the very start. Build critical components first, and try to test them in production without waiting for the whole system to be finished. It’s also worth doing extra work up-front in order to be able to roll back if something goes wrong.\n\nIdentify the components (or system-wide interactions) that would cause the biggest problems if they failed, and guard against those failures from the very start. Build critical components first, and try to test them in production without waiting for the whole system to be finished. It’s also worth doing extra work up-front in order to be able to roll back if something goes wrong. Don’t rush. Shipping a rewrite is arguably riskier than shipping a new feature, since your audience is already relying on things to work as expected. Start by releasing to an audience that’s just large enough to give you the data you need to evaluate success. Then, watch and wait (and fix stuff) until your data give you confidence to continue. Dealing with problems when the user-base is small is much faster and less stressful in the long run.\n\nShipping a rewrite is arguably riskier than shipping a new feature, since your audience is already relying on things to work as expected. Start by releasing to an audience that’s just large enough to give you the data you need to evaluate success. Then, watch and wait (and fix stuff) until your data give you confidence to continue. Dealing with problems when the user-base is small is much faster and less stressful in the long run. Limit your scope. When doing a rewrite, it’s tempting to tackle new feature requests, UI cleanup, and other backlog work at the same time. Consider whether this will actually be faster or easier than shipping the rewrite first and fast-following with the rest. During this rewrite we addressed issues linked to the core architecture (such as crashes intrinsic to the underlying data model) and deferred all other improvements. If you change the feature too much, not only does it take longer to implement, but it’s also harder to notice regressions or roll back. In this case, we feel good about the decision to rewrite. We were able to improve reliability right away, and more importantly, we set ourselves up to stay reliable in the future. As the iOS and Android operating systems continue to evolve in separate directions, it was only a matter of time before the C++ library broke badly enough to require fundamental systemic changes. Now that the rewrite is complete, we’re able to build and iterate on camera uploads much faster—and offer a better experience for our users, too.\n\nAlso: We're hiring!", "label": "non_personal"}
{"title": "Introducing Focus, a new open source Gradle plugin", "url": "https://dropbox.tech/mobile/introducing-focus-a-new-open-source-gradle-plugin", "content": "Working with large projects in Gradle can be a pain. As a project grows larger over time, configuration and build times tend to grow with it. Breaking up large projects into lots of modules can help mitigate this, but unless you meticulously follow the seemingly ever-changing advice about project structure and build script configuration, it’s easy to end up with a build system that’s doing far more than it needs to. And while Gradle is doing that extra work, you have to sit and wait. At Dropbox, like many companies, we use Gradle to build our Android apps. Our monorepo contains almost 600 projects, most of which aren’t used by our engineers on a day-to-day basis. This means that Android Studio has to load many more modules than required, and we waste time waiting for our tools. When we’re working on features within large projects, our time is generally spent within a specific feature module. By using reasonable abstractions within our modules we are able to develop code and write tests without depending on most of the other projects in our monorepo. But wouldn’t it be great if we could easily tell Gradle exactly where we’re working and what dependencies we need, and have it ignore the rest? To answer that question, we recently set out to find an easy way for our engineers to specify the module in which they’re working, and let Gradle and Android Studio ignore the rest.\n\nThe manual way\n\nIn the past, we’ve been able to trim the number of projects that Gradle loads by manually adjusting the projects that are included in the settings.gradle.kts file. The quick and dirty way is to simply comment out the projects that you don’t need:\n\nCopy include(\":sample:app1\") //include(\":sample:app2\") include(\":sample:lib1a\") include(\":sample:lib1b\") //include(\":sample:lib2a\") //include(\":sample:lib2b\") //include(\":sample:lib2c\") include(\":sample:lib-shared\") include(\":sample:moved\") project(\":sample:moved\").projectDir = File(\"sample/lib-moved\")\n\nHowever, this approach quickly becomes unwieldy as a project grows, and the interdependency of your modules grows more complex. Another approach is to extract all of those include(\"...\") statements into a separate file. This way, you can more easily switch between modules by simply choosing which files you want to apply:\n\nCopy //apply(from = File(\"project-1.settings.gradle.kts\") //apply(from = File(\"project-2.settings.gradle.kts\") //apply(from = File(\"project-3.settings.gradle.kts\") apply(from = File(\"settings-all.gradle.kts\"))\n\nBut with all of the possible permutations, it’s easy for this to become unmanageable too. On top of that, making sure those extra settings files stay in sync can be a nightmare. Since Gradle already knows about the dependency tree of your project, wouldn’t it be great if there was an easy way to have Gradle create these files for you?\n\nEnter Focus\n\nFocus allows you to do just that. Our Focus Gradle Plugin evaluates your project configuration and creates a unique settings.gradle file for the module you want to focus on. This file only includes the project dependencies that a specific module requires, allowing you to easily ignore the rest. The plugin also creates a .focus file which identifies which module should currently be focused. With these files in place, Gradle will only configure the modules you need when you sync your project. Deleting the .focus file—which can be done manually, or by using the clearFocus task—will revert to including all of your modules. For example, while working on design systems at Dropbox, the Focus plugin allows us to easily focus on our UI Components Playground project (a sample app in our monorepo for working on design system components). This reduces the IDE sync time from 1 minute to 15 seconds. To start, we simply run the following command:\n\nCopy ./gradlew :applications:uicomponents_playground:focus\n\nThis creates a focus.settings.gradle file in the module’s build directory—which only includes the projects that are required to build the uicomponents_playground module—and a .focus file at the root of the project which points to the newly created Gradle settings file. Clicking the Sync Elephant icon in Android Studio will only load the required modules, improving the performance of both the IDE and Gradle. If we want to spend some time in a different module—perhaps a dependency that needs updating—we can simply focus on that other module and sync.\n\nCopy ./gradlew :dsys:components:focus\n\nThis way, we’re easily able to trim the number of modules loaded in the IDE and configured by Gradle. When we’re ready to go back to building the entire project, we can simply remove the .focus file in the root directory of the project, or run the following Gradle task:\n\nCopy ./gradlew clearFocus\n\nWith Focus, our engineers are able to iterate faster throughout the day, allowing them to spend more time focused on the quality of the code they’re developing, and less time waiting for their tools to sync.\n\nAdding the Focus plugin\n\nFocus is a settings plugin, so it should be applied in your settings.gradle(.kts) file. Since we currently publish the plugin to Maven Central, you’ll have to add that as a repository in your pluginsManagement block:\n\nCopy // settings.gradle(.kts) pluginsManagement { repositories { mavenCentral() gradlePluginPortal() } } plugins { id(\"com.dropbox.focus\") version \"0.4.0\" }\n\nNext, move all of your include statements into a settings-all.gradle file:\n\nCopy // settings-all.gradle(.kts) include ':sample:app2' include ':sample:lib2c' include ':sample:lib-shared' // ... include ':sample:moved' project(':sample:moved').projectDir = new File(\"sample/lib-moved\")\n\nOptionally, you can configure the plugin within your settings.gradle.kts file:\n\nCopy // settings.gradle(.kts) focus { // The name of the settings file allSettingsFileName = \"settings-all.gradle\" // Default // The name of the pointer file that tells Focus which module to focus on // This should be added to your .gitignore file. focusFileName = \".focus\" // Default }\n\nLastly, don’t forget to add your Focus file—.focus by default—to your .gitignore file. Once those steps are complete, you can use the focus Gradle tasks in each subproject to reduce the amount of time you have to sit around and wait.\n\nOpen source at Dropbox", "label": "non_personal"}
{"title": "Bringing AI-powered answers and summaries to file previews on the web", "url": "https://dropbox.tech/machine-learning/bringing-ai-powered-answers-and-summaries-to-file-previews-on-the-web", "content": "Dropbox offers a handful of features that use machine learning to understand content much like a human would. For example, Dropbox can generate summaries and answer questions about files when those files are previewed on the web. Instead of asking a coworker for the gist of last week’s all-hands meetings, Dropbox can provide a summary of the video and a user can ask questions about its contents—all from the file preview. We recently expanded AI-powered summarization and Q&A to handle multiple files simultaneously, too. (As part of our commitment to responsibly using AI, Dropbox abides by a set of AI principles; you can visit our Help Center to learn more. These features are still in early access, and not yet available to all users. These features are also optional, and can be turned on or off for you or your team.) Both our summarization and Q&A features leverage large language models (LLMs) to find, compare, and consolidate the content of the file. An LLM works by ingesting content as text, transforming the ideas contained within it into a numerical representation, and comparing those numerical representations against both the input query and an internal corpus of knowledge to answer the question. This effectively enables a computer to consume and compare information semantically, rather than lexically. For knowledge workers suffering from information overload, we can use machine learning to get them the answers they need—without them having to remember exactly how a piece of information was worded, or where it might be contained within a file. This is what we’ve done with file previews on the web.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images—plus generative AI capabilities across even more connected apps. See what's new →\n\nExtracting text and embeddings with Riviera\n\nThe first part of this process is, of course, retrieving the text. Luckily, we already have a framework for transforming basically any file type to text. Dropbox is capable of turning complex file types like CAD drawings into formats that are easily consumable by web browsers, such as PDF. Historically we have used this system for file previews, but we also use it to power features like transcription and Dropbox Replay. Internally, we call this system Riviera. At a high level, Riviera consists of a frontend which routes requests through one or more plugins that convert a file from one type to another. Each plugin runs in a jail, which is a container designed to run third party code and tools safely and in an isolated manner. The framework maintains a graph of possible conversions, and is capable of chaining together multiple plugins into a multi-step pipeline to perform even more complex transformations. We currently support conversions between about 300 file types, and the system crunches through about 2.5 billion requests—totalling nearly an exabyte, or one billion gigabytes of data—per day. In order to apply machine learning to file previews, the conversions we are interested are those that convert any file type to raw text. In the case of video, text extraction might looks something like:\n\nCopy Video (.mp4) -> Audio (.aac) -> Transcript (.txt)\n\nSome of the conversions in Riviera can be quite expensive to compute on-the-fly, so it also includes a sophisticated caching layer that allows us to reuse conversions between plugins. Each state of the pipeline is cached, so intermediate states can be reused. In the world of LLMs, the mathematical representation of the semantic meaning of text is called the embedding. Riviera treats embeddings like any other file conversion, so in the pipeline above, we can append:\n\n\n\nCopy Video (.mp4) -> Audio (.aac) -> Transcript (.txt) -> AIEmbedding\n\nBy separating the embedding generation from the actual summary generation we can reuse the cache features built into Riviera. If a user wants to summarize a video, then ask some follow-up questions, we only have to generate the transcript and embeddings once.\n\nCopy Video (.mp4) -> Audio (.aac) -> Transcript (.txt) -> AIEmbedding --> Summary | --> Q&A\n\nThe input for the embeddings plugin typically consists of text data extracted from various file types such as documents, videos, or audio files. In the case of video content, for example, the plugin may receive the transcript generated from the audio track of the video. The process of converting text into embeddings involves using advanced language models that take the text input and produce a vector representing that text. In our implementation, we split the text into paragraph-sized chunks and calculate an embedding for each chunk. By doing this, we effectively increase the granularity of the information stored within a file. Instead of having just a single embedding for an entire file, we have multiple embeddings for different sections of the text. This higher granularity, or bit depth, allows us to capture more detailed and nuanced information. We apply the same chunking and embedding method for both summaries and Q&A, ensuring they share the same embedding cache within Riviera. While the actual LLM processing happens inside the summary and Q&A plugins, treating embeddings, summaries, and queries as file conversions inside Riviera allows us to operate these features at Dropbox scale.\n\nThe high level architecture of our file previews surface, with new machine learning components highlighted\n\nThe summarization plugin\n\nA common use case for LLMs is to concisely summarize large amounts of text. When negotiating a contract, for example, a person might copy the text from a contract PDF, paste it into an LLM-powered chat prompt, and ask the LLM to summarize it in understandable terms. While adding this feature into Dropbox may have been useful as-is, we decided that we didn’t want to stop there. Dropbox users store long documents, videos, and other rich media files in Dropbox, and a summarization feature only gets more useful as the length of the file increases. We wanted to unlock this feature for all of our users' files, no matter the format or length. First, we needed to define the qualities of a good summary. A contract to purchase a home might include many different concepts. There might be a long description of payment terms, wire instructions, good-faith deposits, and escrow accounts. It also might have a long description of contingencies and when they can be triggered. A good summary might simply say “this document is an agreement to purchase a home for X amount, and it has finance and inspection contingencies.” In other words, we defined a good summary as one that can identify all the different ideas or concepts in a document and give the reader the gist of each. Language models enable this to be implemented algorithmically using embeddings, which allow passages to be compared semantically. King and Queen might be relatively close together (they are both regal), King and Dog might be somewhere in the middle (they are both living beings, perhaps), while King and Toaster have very little in common. Language model embeddings allow us to compare passages on thousands of dimensions that are all learned through a long training process. These embeddings in turn enable efficient summarization of large files of essentially unlimited length. Our summarization plugin takes the chunks and associated embeddings from the embeddings plugin and uses k-means clustering to group the text chunks from the file into clusters in this multi-dimensional embedding space. With this method, we can organize data into distinct groups, or clusters, based on their characteristics, so that chunks with similar content are placed in the same cluster. We then identify major clusters (the main ideas of the file) and concatenate a representative chunk from the corresponding text from each cluster into one blob—the context. Finally, we generate a summary of that context via an LLM. We found k-means clustering was better than alternatives such as a summary-of-summaries approach in a couple of ways: Higher diversity of topics. Many individual summaries before reaching the final summary of summaries often repeat similar information. Combining these summaries results in a significant loss of overall file content. Using k-means clustering, we discovered that the summaries covered a broader range of topics—approximately 50% more than with map-reduce, since we search for chunks that are semantically dissimilar to one another.\n\nMany individual summaries before reaching the final summary of summaries often repeat similar information. Combining these summaries results in a significant loss of overall file content. Using k-means clustering, we discovered that the summaries covered a broader range of topics—approximately 50% more than with map-reduce, since we search for chunks that are semantically dissimilar to one another. Lower chance of hallucinations. When the LLM receives the entire file in one go, its likelihood of hallucinating decreases significantly. Conversely, each call made to the LLM presents a chance for hallucination, making the problem exponentially worse when summarizing summaries. The map-reduce approach lacks the context provided by other chunks, compounding the issue. Using the k-means technique, pinpointing errors in the final output—especially when comparing between LLMs or models—becomes much easier because there is just a single LLM call to evaluate.\n\nThe Q&A plugin\n\nOur Q&A plugin works in a similar manner to the summarization plugin, but in a somewhat opposite way. The Q&A plugin takes in the embeddings and text chunks from the embedding plugin and generates a new embedding for the user question. Then for each chunk of file text it computes the distance to the query text embedding. By calculating the closeness of each chunk to the query in vector space, the most relevant chunks are selected. In the summarization plugin the text chunks were selected for dissimilarity, while in the Q&A plugin they were selected for similarity to the query text. These text chunks, along with the query, are sent to the language model for generating the answer. A language model uses the context to generate answers by analyzing the provided text chunks and the query to understand their meaning and relationships. When a query is received, the model first interprets the question and identifies key elements and intents. It then examines the text chunks, which serve as additional context, to extract relevant information. The model employs sophisticated algorithms to detect patterns, correlations, and nuances within the text, allowing it to discern how different pieces of information fit together. By integrating the context from the text chunks with the specifics of the query, the language model can produce a coherent and accurate response that is tailored to the query's requirements. This process involves leveraging large-scale language patterns learned during training, enabling the model to generate answers that are both contextually appropriate and informative. The relevant chunk locations are then returned to the user as sources, allowing them to reference the specific parts of the file that contributed to the answer. As an add on feature to both the summarization and Q&A plugins, we also request context-relevant follow-up questions from the LLM. In testing we found that follow-up questions allow the user to more naturally learn about a file and the topic they are interested in. To gather these follow-up questions, we utilize function calling and structured outputs to request follow-up questions from the LLM at the same time we request the summary or an answer to the initial question.\n\nExpanding to multiple files\n\nThe first iteration of intelligent summaries and Q&A was limited to one file at a time—but we knew we could do better for our customers. We wanted to make LLM-powered understanding possible across collections of files, and not just individual documents. Expanding our understanding capabilities to multiple files within Dropbox involved a significant evolution of our capabilities, infrastructure, UI, and algorithms. Building on our existing file processing pipeline, we expanded Riviera’s capabilities to handle multiple files simultaneously inside of a single plugin. The embeddings plugin would still be separate for every file, but the final summarization or Q&A plugin call would need to take in multiple files. The question was: which subset of files selected by the user would we extract the relevant information from? When testing this feature, we found that some questions were quite direct (“What is Dropbox?”) while some were quite broad (“Can you explain this contract like I’m 5?”). For best results we found that we needed to tailor our response accordingly. Direct questions can often be answered in a single sentence, while the answers to broader questions are typically longer and potentially sourced from a wider context. The challenge was determining which type of answer was required. Put another way: How should we determine the number of relevant chunks or files to use when answering a request or query? We eventually came to the conclusion that this was a trick question. You cannot determine if a question is direct or broad based just on the question itself. You also need the context the answer is pulled from. The question “What is Dropbox?” could both direct if asked about a list of tech companies, but also broad if asked about the Dropbox Wikipedia page. To solve this question, we took advantage of power law dynamics to determine the number of relevant chunks to send to the LLM.\n\nLine A is a direct question, while line B is a broad question\n\nOur solutions takes the max and min relevance scores from the top 50 text chunks related to the user query, as calculated through the embeddings, and cuts off the bottom 20% of that spread. This works well because, as shown in the diagram above, direct questions have a steeper power law curve than broad questions. For Example A above, lets say that the most relevant chunk has a score of 0.9 and the least is 0.2. In this case, everything below a 0.34 score is discarded (the 20th percentile score). Since the slope is steep, over half of the chunks will be discarded, leaving about 15 left. For Example B—a more broad question—let’s say that the most relevant chunk has a score of 0.5, and the least is 0.2. In this case, everything below a 0.26 would be discarded, leaving about 40 left. Since the slope is flatter, more chunks are chosen to send to the LLM. Another example could be a quarterly earnings report. For the question “what were the financial results?” a lot of the chunks would have medium relevance, resulting in something similar to the B curve above, since it is a broad question. For a question like “how much was spent on real estate?” there would be some very relevant chunks and a lot of non-relevant chunks—like the A curve above, since it is a more direct question. The first question would require more chunks to answer the question fully versus the second question. This algorithm allowed us to strategically determine which files and chunks within those files were relevant, and thus, which context to send to the LLM. Direct questions get less but more relevant context, while broad questions are given more context to expand on the topic.\n\nWhat we learned", "label": "non_personal"}
{"title": "How Edison is helping us build a faster, more powerful Dropbox on the web", "url": "https://dropbox.tech/frontend/edison-webserver-a-faster-more-powerful-dropbox-on-the-web", "content": "How Dropbox re-wrote its core web serving stack for the next decade—sunsetting technical debt accrued over 13 years, and migrating high-traffic surfaces to a future-proofed platform ready for the company’s multi-product evolution. ~ ~ ~ In the Fall of 2021, Dropbox quietly launched an internal alpha version of our core web-based file browser. This internal alpha marked a critical milestone in Dropbox web architecture: our file browser was now a Single Page Application, or SPA. On its own, this wouldn’t have been very remarkable. SPAs have been around in concept for almost two decades, and have been widely used on the web for at least half that time. In fact, to most people managing a website, SPA functionality is table stakes and barely worth writing a blog post about. That would have been true for us too if the web property wasn’t Dropbox.com—a very large surface spanning hundreds of back end services with deep and multi-layered feature sets owned by dozens of teams. We weren’t just launching a SPA, but a whole new platform to enable it. This platform would not only have to work correctly and outperform the existing stack, but—critically—do so without requiring a tough rewrite for teams who wanted to migrate. Anyone who has worked with a system as large as ours—with all its attendant tech debt—knows that rewriting even limited segments can be a challenging endeavor. Our debut of the SPA architecture represented a complete rebuild of web foundations at Dropbox—an entirely new web framework, stack, and platform called Edison. Developed over the last three years, Edison gives us the keys to unlock a host of performance and functional improvements. It enables sub-second developer iteration cycles and isomorphic rendering—meaning JavaScript that runs on both the client and server—and unlocks the unification of our web surface into a dynamic SPA. It is a full rewrite of our core web systems that sunsets some 13 years of technical debt. We built Edison to be our web serving stack for the next decade—a flexible platform fit for our highest-traffic surfaces and future products and features alike.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images—plus generative AI capabilities across even more connected apps. See what's new →\n\nWhat is the web to Dropbox?\n\n“Things can change so fast on the internet” — Tim Berners-Lee Dropbox launched its website in 2008, almost two decades after the web was invented. At launch, Dropbox was still just a desktop app, and the website a delivery mechanism and marketing surface. For much of the early years, the real action was in the app which ran on your computer and quietly kept your digital life in sync. Our engineering talent specialized in infrastructure—syncing, networking, chunking, and storage—rather than the web. Over time, Dropbox built a web platform piece by piece, moving from a brochure site to an account manager and file viewer—and finally, to today’s platform, where we can provide advanced features like PDF editing and video collaboration. Many more users are able to benefit from these advanced features thanks to the reach and ubiquity of the web. Today, the web is central to all Dropbox products—from enabling new file-based workflows to organizing cloud-only data—and increasingly where the battle for user utility will be fought. It’s critical to our future that we have a web platform which gives our product engineers the most reach and speed that it possibly can. As we move to meet the challenges of the future, Edison is just one example of how seriously Dropbox now takes the web, and the shifting internal and external culture around it.\n\nThe history of Dropbox on the web\n\nLike any surface with a storied history, our web code has undergone a number of significant iterations over the years. Back in 2012, our web architecture was based on Pyxl, a Python-with-inline-HTML engine—familiar ground to folks who grew up writing PHP! We built much of our client front end in CoffeeScript, until that turned into a dead-end and pushed us to migrate to TypeScript. We used and then deprecated jQuery. We adopted React. In short: much of our trajectory echoes the evolution of the web ecosystem itself over the last decade-and-a-half, and should be familiar to many who built web properties during that time. More interesting is how the integration between our back end and front end evolved. In broad strokes, we moved in step with the wider web ecosystem—adopting open source technologies, server rendering, and interactive front ends. But over time we also built out custom infrastructure to solve our own set of business challenges. A critical piece of this was our custom webserver, and the optimizations built into it. This (legacy) webserver is called DWS. (If you’re wondering what that acronym stands for, please take a guess; congratulations, you’re right!) DWS served us well for most of a decade, and allowed us to successfully iterate on our website during that time. But, inevitably, two things happened next: the web continued to evolve, and we wanted to demand more of DWS than it was designed to do. As we moved to expand the capabilities of our web products, the limitations of DWS presented somewhat intractable issues for both our application feature set and developer productivity. There were two main classes of problems we were up against.\n\n\n\nProblem 1: Pagelets DWS was designed so that feature teams could iterate independently on the web. For example, one team might want to work on navigation while another worked on the sidebar. In particular, those teams would want to iterate and release without continuous cross-team coordination. DWS enabled this. DWS not only allowed engineers to develop independently, but also enabled their code to execute independently on both the back and front end. These properties ensured that one feature team could not hold up or slow down another. To meet these requirements DWS settled on a pagelet architecture. For those unfamiliar with the term, a pagelet is a subsection of a page, such as the navigation bar or the footer. In our case, one pagelet encompasses both the back end (data fetching, controller) as well as the front end (view) code. Each pagelet has a controller in Python which retrieves the back end data it needs, specifies page metadata, and provides a JavaScript (JS) entry point with initialization data. Pagelets execute in parallel and stream the data they retrieve into the browser response, keeping the HTTP connection open for as long as needed until the streaming is complete. Under DWS, a collection of pagelets is defined as a route. In the browser, each pagelet’s entry point module is executed as an individual React app, which initializes as soon as ready and renders into a specific HTML slot in the page’s appshell (the basic HTML which initializes the page and provides root elements to hold incoming content). As each pagelet completes, the page is assembled into its complete form. Upsides to this architecture: Complex, multi-featured pages could be vertically sliced into manageable parts.\n\nThere was clear separation and ownership of product code.\n\nEach team’s product could be rendered independent of other pagelets.\n\nIt made early data retrieval possible. DWS issued parallel back end requests which streamed into the HTML, so that by the time the JS began executing, the data fetches it needed were already complete.1 Downsides to this architecture: Data fetch instructions were defined in the Python controller, but fetched data was consumed in the JS layer in the browser. These layers needed to be kept in sync manually, which was an obvious source of bugs. This also meant that it was hard to statically verify that a piece of data was actually still in use, making it easy to over-fetch data or forget to eliminate a data fetch as the front end changed. Writing functionality that crossed pagelet boundaries was nontrivial. The difficulty of crossing pagelet boundaries was an especially serious limitation. It meant we couldn’t evolve our site into anything which you’d define as a holistic application, let alone a SPA. If you wanted your pagelet’s JS code to interact with another pagelet’s JS code, you’d need to meet with the owning team and agree to design a mutual JS messaging API (yikes!). This closed off a whole world of possibilities and feature sets.\n\nProblem 2: Commingled layers Any engineer who has designed a distributed system knows a few principles to building robust, composable, flexible interfaces. Separation of concerns. Loose coupling. The basic problem we faced as we tried to stretch our wings on the web was predictable: because DWS had evolved from a system of HTML-in-Python, it still retained a lot of hard-to-shift coupling between the server and the JS client. What this meant was that if an engineer developing a feature for the web wanted to run their code—which might only be a few lines of JS and CSS—they also needed to run the whole webserver stack. The system wasn’t designed to be modular, and the JS application layer had many direct and indirect dependencies on server-rendered data. Executing JS without the webserver was rarely feasible, and even if some segments were, it wasn’t possible to build a universal way to do it. Running a webserver is easy if you only have a handful of routes, and maybe a few thousand lines of code. Many projects will use webpack-dev-server and start their site up in, say, 10-20 seconds. However, our scale made this approach prohibitive. At Dropbox, it meant running ~100 back end services, all of which needed to be compiled and started up in the virtualized server environment we call devbox. This could take 20 minutes—and sometimes took twice that! An experienced front end engineer who joined the company might see their workflow change from “I can load the dev site on my laptop in 20 seconds” to “I need a provisioned dedicated remote server and 20-30 minutes.” Again, yikes! We needed to get to a place where engineers could run their JS code quickly and easily—a place that didn’t require expensive builds of the back end and round-trips to a remote box. After all, it’s one thing to be able to run a single module without the webserver; there are systems that allow you to test some subset of your functionality (for example, Storybook). But these were patches on our problem, not full solutions. It’s no good if engineers need to work out how to run their code every time they need to test something. We needed a general solution.\n\n\n\nIntroducing Edison\n\nFinding an an answer to our challenges with web client development at Dropbox wasn’t straightforward. Our first incremental rewrite began in 2016, before I joined Dropbox, and before the initial release of Next.js—the open-source project which Edison is frequently compared to. We had several false starts and backward steps as we iterated on incremental versus foundational changes. In retrospect, the key functional requirements we had to nail with Edison were: The browser runtime is a single, unified JS application.\n\nThe JS client can execute without the server. The key non-functional requirements were:\n\nAn easy migration. Rewriting all our code for a new system was a non-starter. Any particular team rewriting their code was a non-starter. We had nearly two million lines of TypeScript code, and around 100,000 lines of back end code handling pagelets. This was partly a technical problem, but also a political one, since expensive rewrites would have the potential to heavily delay or even render a new system DOA.\n\nRewriting all our code for a new system was a non-starter. Any particular rewriting their code was a non-starter. We had nearly two million lines of TypeScript code, and around 100,000 lines of back end code handling pagelets. This was partly a technical problem, but also a political one, since expensive rewrites would have the potential to heavily delay or even render a new system DOA. Equal or better performance. The existing system had clear optimization lines that we had to match. We had to ensure that parallel products couldn’t impact each other, and that data fetches happened well in advance of the JS initialization. Again, this was both a technical and a political problem, because although limited performance regressions could hypothetically be the right technical outcome in some scenarios, they’d cause political pushback from impacted teams. By 2020, we had a clear sense of the necessary outcome: a serverless client architecture, re-imagined from the ground-up, alongside a server which could do Node-based rendering. In other words, we needed an isomorphic JS application stack tightly integrated with our existing services and code architecture.\n\nStructure Edison consists of three major parts: Edison Server written in Go\n\naccepts direct web traffic via our Envoy proxy layer\n\ndelegates server-side rendering (SSR 2 ) to Streaming Reactserver\n\n) to Streaming Reactserver performs data fetches via Courier, our gRPC framework\n\nhandles ahead-of-time fetching and communicates with Edison Client in the browser Streaming Reactserver Go-wrapped Node service 3 tasked specifically with performing React tree renders\n\ntasked specifically with performing React tree renders able to pass messages back to Edison Server asynchronously during render Edison Client, our browser-based runtime main browser entry point\n\ninterfaces with Edison Server to fulfill ahead-of-time data fetch guarantees\n\nhandles data exchange directly with Courier services for late fetches\n\nimplements a single React app for the whole page Running serverless is a key part of Edison’s design. In principle, you could build Edison Client and our product code and have them served from a static CDN. There’s no reason for us to do that in production, but it’s an enabling feature (we’ll come back to this later). But if we can run serverless, why have a server component at all? Edison Server is a critical part of our stack. It complements the client by acting as a runtime accelerator, and is responsible for meeting the performance targets set by DWS. Let’s dive into how that’s accomplished.\n\nApplication acceleration with Edison Earlier I discussed pagelets and the optimizations we’ve gained by processing their data requirements in parallel, early in the request lifecycle. This is one of the key optimizations the server needs to perform. But how does it provide the speed boost? Suppose we’re looking at a file preview, which is a sub-function of our file-browsing application. We want to bring up a page which has a file listing and an overlaid preview of a PDF. Here’s how a simplified version of that might look: The server naively responds to the initial request for /browse with the JS application entry point bundle for that page. Once it loads, the browse app requests the data for the user. Once it has that data, the browse app determines that it needs the preview JS app to proceed. Once that data loads, the page can complete rendering and the user can see the content. Here’s that process represented on a timeline:\n\nAn example of a naive cascade for a pure-client JS application\n\nWhen chains of requirements execute linearly like this, they can rapidly add up to unacceptable delays in serving an experience to a user. This cascade is also greatly simplified; an actual production application can have hundreds of modules which may interact in many different ways. This is where server acceleration shines. Since Edison Server can pre-analyze the whole application tree, it can anticipate many of these requests:\n\nThe same sequence, with modules and data preloaded\n\nThis kind of acceleration via application analysis is possible because of the cross-service communication between Edison Server and Streaming Reactserver during the server-side render of the React tree. When a request comes in, the following sequence results: Edison resolves the route to a JS entry point module, packages up all the data required to render that entry point, and sends it to Streaming Reactserver. Streaming Reactserver then begins a server-side render4. During the render, the code path encounters data fetch calls (specifically, calls to Edison.prefetch ). As those calls are encountered, Streaming Reactserver sends an asynchronous message to Edison Server, which immediately kicks off the data fetch while the React render continues. When that data fetch completes, Edison Server can stream the results into the response HTML, regardless of whether the React render is complete or not, so that the data needed is ready before the application requires it. This is how we unlock the same performance wins as Edison’s predecessor DWS. Even better, we end up with an architecture where data fetch requests are co-located with the code that needs them, and all the application logic now lives in a single layer of the system.\n\nMigrating to Edison Prior to Edison, we undertook a phased set of steps to gradually migrate the web page APIs away from Python and Pyxl towards JS. This provided manageable steps for stakeholders, allowing them to upgrade incrementally with benefits along the way. Edison was meant to be a seamless extension of that trend. By launch, teams that were up to date on our old stack had the following: a Python pagelet executor to handle data loads and provide initialization data\n\na set of JS application code to accept that data and render the product Migrating to Edison meant teams had only to make manageable adjustments:\n\na data servicer (could be Python, Go, or any other Dropbox-supported language) to handle data loads and provide initialization data back over gRPC\n\na set of JS application code which implemented Edison.prefetch to call out to the servicer In most cases, this work amounted to a refactor of a single Python file, repackaged with the Servicer API instead of the Pagelet API, and a new JS entry point module to wrap the JS logic and make the prefetches. Critically, this was a change that teams could undertake: incrementally, with the Edison version gated to internal traffic only\n\nwithout code duplication, since both back end and front end core code could be shared\n\nwith strong expectations that feature parity would be available to them out of the gate Asking teams to handle multiple incremental refactors took time and capital, but was critical. By the time Edison was ready for launch, the migration effort for major web services had been dramatically reduced. These refactors got us to a place where even major pages could be run simultaneously on DWS and Edison with the same JS code, making gradual rollouts easy to achieve.\n\nSolved problems To summarize: Edison successfully broke up our commingled application layers by moving all the product code into the TypeScript layer.\n\nWe preserved our ability to do acceleration with early data fetches by relying on asynchronous gRPC calls initiated during the server-side React render.\n\nWe collapsed two sources of truth for data fetches into one, making maintenance easier and the code more readable for product engineers.\n\nWe retained the ability for separate product modules—what we previously called pagelets in DWS — to own their back end data providers, and to execute their JS code as soon as those providers returned the initialization data they needed.\n\nto own their back end data providers, and to execute their JS code as soon as those providers returned the initialization data they needed. Application engineers could now trivially write functionality which crossed the old pagelet boundaries, since the application is now a single, coherent React tree.\n\nRoutes that had been refactored into pure React could run simultaneously on DWS and Edison, and migrated incrementally for safety. What’s left? The last piece is developer productivity. Remember that, with DWS, engineers needed to run the whole stack (100+ services) to execute any JS. At this stage in Edison’s rollout, that was still the case. So let’s talk about what Edison enabled us to do next, and how we’re solving the next set of problems while building for the future.\n\nBuilding on Edison\n\nShifting engineers to an Edison-based surface let us do two things. First, we could immediately work on a single, coherent React tree—bringing us back to the start of this journey, where I talked about moving the first parts of the Dropbox.com web surface to a SPA architecture. Second, splitting the layers between client and server properly now meant the client no longer needed to be served from the webserver. This allowed us to streamline much of our developers’ workflow.\n\nThe rapid rise of the SPA Single Page Applications aren’t an immediate panacea or necessarily a final endpoint of our architecture. For many sites—especially mostly-static sites—a thinner client could solve the same pain points. But for a site like ours, where actions such as file uploads, moves, and manipulation are center to its functionality, Dropbox on the web needs to function, in essence, as a full, graphical file manager. Being able to decouple user actions and page state from navigation around the surface is essential. Some specific examples of how a SPA architecture benefits our web surface are: It enables intuitive navigation around the file tree, allowing us to do something like drag a file into the sidebar. Those were entirely separate React apps before!\n\nWe can provide visual transitions between surfaces which used to be entirely separate, such as folder navigations. These give users a better sense of what’s changing and reduce cognitive overhead.\n\nWe can deliver better performance by powering page navigations with a single API call, instead of a full page load with all its attendant costs. As this part of our product continues to mature, I am sure the SPA will be a topic worthy of its own story. Rather than dig deeper on it now, let’s talk about client-server splitting and our developer productivity enhancement, which we call Edison Localmode.\n\n\n\nEdison Localmode Since the client no longer needs the webserver to function, we now have the ability to do things like: bundle our sources and deploy to a CDN, eliminating infrastructure\n\ndeploy an app to Electron—a unified, cross-platform code pipeline\n\nserve the whole web client from our engineers’ laptops, enabling rapid iteration All of these are awesome abilities—but for the purposes of this post, it‘s the third we care about the most, since it solves our problem with developer productivity! The basic goal of Edison Localmode is simple: web client developers should not need to run anything but the web client. We should be able to decouple them from the webserver. In the new Edison world, engineers who are creating and iterating on JS and CSS code should no longer need to repeatedly touch the webserver—or any back end services for that matter. They should be working on a pure client codebase which utilizes APIs to interface with the remainder of the Dropbox world. Edison Localmode fulfills this promise. Edison Localmode is an opt-in system which enables serving of all static assets (JS and CSS) directly from an engineer’s development laptop. An engineer loads a page as normal (from a Dropbox webserver, whether that be a dev server they’ve spun up, or our staging environment), boots a lightweight local Node-based asset server, and opts into Localmode. From then until they opt out, all static assets will be served from their local laptop. The engineer is then free of the need to touch the server. As they continue to iterate, the lightweight local server will watch the filesystem and re-transpile5 code on the fly, patching it directly into the running page so that modifications are applied instantly to the working surface. Let’s recap, because it’s deceptively easy to state, but quite transformative. Before: An engineer modifies and saves a source file. They manually issue a command to reload devbox, which syncs changed files, rebuilds, and restarts services. Once complete (15-30 seconds), the engineer switches back to the browser and hits refresh. If their code runs deep in a flow—like the third stage of an interactive form—the engineer needs to manually reconstruct that state each and every time they refresh. Once the page is reloaded, the engineer checks to see if their code is working correctly. Repeat as needed to complete the feature. After: An engineer modifies and saves a source file.\n\nThe code is transpiled and running in the page, with all states intact and requiring no manual intervention, before they’ve even had the time to switch back to the browser. For an engineer who might have previously reloaded their devbox every few minutes, the raw time savings can add up considerably. If you’re reloading your code every 15 minutes, and looking at the conservative side of devbox reload speeds, you’re potentially saving one minute every hour. At six hours a day, that scales up to around four working days per year you’re no longer wasting. Less quantifiably, whenever an engineer is delayed for 30 seconds, they will inevitably try to check messages on Slack while they wait, get bumped out of flow, and lose more time. Having instant feedback allows engineers to remain in flow more continuously, and increase the speed of their changes. These are big wins for developer productivity and overall satisfaction. Front end development on devbox was a notoriously frustrating experience; with the release of Edison Localmode, work that used to be a chore can now return to its natural state of being satisfying and quick!\n\nThe future\n\nEdison is the culmination of many things for Dropbox. As the web gained prominence, our workforce diversified; we grew from mostly back end engineers to include a strong cohort of full-stack web engineers. Edison would have never been possible without this culture shift. Developing Edison also led to a better understanding of the constraints of our own systems. DWS made many implicit assumptions that Edison illuminated, allowing them to be better understood and isolated. Getting to this point required incremental adjustments to our APIs and any code that used them. This increased our understanding of what future alignment of our systems with open source should look like, and what it would take to get there. Alignment with open-source is something we continue to work towards, and is a de facto good; it reduces our reliance on bespoke tools and can further increase developer productivity. In the meantime, Edison is a huge accelerator for our product development. Tasks and features which would have been inaccessible or lengthy projects under DWS are now easy, and development work which was riven with interruptions and delays has become smooth and continuous. Edison lets us think much more freely about what our web products ought to be and envision a Dropbox.com closer to the cutting edge. If building innovative products, experiences, and infrastructure excites you, come build the future with us! Visit dropbox.com/jobs to see our open roles, and follow @LifeInsideDropbox on Instagram and Facebook to see what it's like to create a more enlightened way of working. ~ ~ ~", "label": "non_personal"}
{"title": "Investigating the impact of HTTP3 on network latency for search", "url": "https://dropbox.tech/frontend/investigating-the-impact-of-http3-on-network-latency-for-search", "content": "Dropbox is well known for storing users’ files—but it’s equally important we can retrieve content quickly when our users need it most. For the Retrieval Experiences team, that means building a search experience that is as fast, simple, and powerful as possible. But when we conducted a research study in July 2022, one of the most common complaints was that search was still too slow. If search was faster, these users said, they would be more likely to use Dropbox on a regular basis. At that time, we found it took ~400-450ms (p75) for the search webpage to submit a query and receive a response from the server—far too slow for our users who expected quicker results. It sent us looking for ways that search latency could be improved. In our early analysis, we learned that of the time it took to fetch search query results, roughly half of that time was spent in transit to and from Dropbox servers (a.k.a. network latency) while the other half was spent on determining which search results to return (a.k.a. server latency). We decided to tackle both sides of the equation simultaneously. While some of our colleagues explored ways to reduce server latency, we investigated network latency.\n\nSearch’s total latency is comprised of server time and network time\n\nNetwork latency is significantly more variable than server latency. It depends on local network conditions, the user’s distance from a Dropbox datacenter, and even the time of day. During business hours, many users work at offices with strong internet connections, but at night, they are at homes with weaker internet connections. Compared to North America—where the majority of Dropbox data centers are located—latencies can be up to twice as high in Europe and three times as high in Asia. Considering 25% of search requests originate from Europe and 15% originate from Asia, a significant portion of Dropbox users would benefit from lower network latencies. At this point, we realized that we couldn’t tackle our network latency issues alone. In collaboration with the Traffic team, we considered our options and decided to test a possible solution: HTTP3.\n\nRegional differences in network latency\n\nA hypothetical speed boost\n\nDropbox.com currently uses HTTP2, a protocol based on TCP. The latest version, HTTP3, uses UDP. This speeds up the time to establish connections and serve parallel requests by: Introducing Zero Round Trip Time (0RTT) at the beginning of connections. Compared to HTTP2, HTTP3 makes one fewer round trip because it avoids the three-way handshake mandatory for TCP-based protocols. Furthermore, with 0RTT, subsequent HTTP3 connections establish a secure connection and make the actual request in the same packet, whereas in HTTP2, these pieces of data must be sent separately.\n\nCompared to HTTP2, HTTP3 makes one fewer round trip because it avoids the three-way handshake mandatory for TCP-based protocols. Furthermore, with 0RTT, subsequent HTTP3 connections establish a secure connection and make the actual request in the same packet, whereas in HTTP2, these pieces of data must be sent separately. Eliminating head-of-line blocking. TCP is stream-oriented and thus requires packets to be processed in a strict order. If a packet in one stream is lost, packets in subsequent streams could be delayed in the client’s TCP stack, even if the streams are unrelated to each other. But with UDP, if one stream is blocked, other streams can still deliver data to the application.\n\nHead-of-line blocking: In HTTP2, a blocked stream also delays subsequent streams, whereas in HTTP3, a blocked stream only affects that stream\n\nHTTP3 sounded promising. In theory, it could not only speed up search requests but also operations across all of Dropbox—from file uploads to content suggestions. However, it was unclear what the real world impact would be. It was entirely possible—albeit unlikely—for HTTP3 to be slower than HTTP2. We needed to be sure that Dropbox would benefit from a migration to HTTP3. Rather than take an unknown leap, we decided to test HTTP3 on a portion of Dropbox traffic first.\n\nSetting up the experiment\n\nTo evaluate the performance of HTTP3 on Dropbox servers, the Traffic team created a test subdomain that served our main website with HTTP3. The test site was specifically designed so that we could safely make specific API requests over HTTP3 without negatively impacting users of the main website. As part of this test site, we built a no-op API endpoint that could successfully leverage HTTP3. Because the server doesn’t perform any operations, server latency would be near zero—meaning any remaining latency would be network latency. With this endpoint in place, we then devised our HTTP3 test involving a series of actions meant to simulate typical request traffic on our website—including when a user performs a search. The simulation had three phases: Setup. First, we pre-warmed the cache by firing off two sequential HTTP3 requests, ignoring any timing data. This was done purely to warm up any networking caches related to the HTTP2 and HTTP3 servers equally, ensuring that subsequent HTTP2 vs. HTTP3 testing was a fair comparison. This is specifically necessary for our test because the first connection is always HTTP2; that’s when the client receives information required to support HTTP3. All subsequent connections would then try to use HTTP3. Running the HTTP2 control. We then ran five parallel HTTP2 requests to the no-op API endpoint and logged the network time for each request. This simulated how users currently get data from our servers, and thus was our control. Running the HTTP3 experiment. Finally, we ran another five parallel requests to the no-op API endpoint, but this time via HTTP3. We logged the elapsed network time for each request to compare against HTTP2. The most important aspect of this test was that the requests were made in parallel. This would simulate real-world scenarios at Dropbox, where many parallel requests are fired with each interaction with Dropbox web. But more importantly, it would help us determine whether eliminating head-of-line blocking would actually speed up parallel requests; if these requests were not faster, it was unlikely HTTP3 would help us in practice. To prevent any impact to user-facing performance, we only allowed our HTTP3 tests to be conducted once per page load, and only after the user completed a search. We ran the experiment for roughly two weeks between December 2022 and January 2023. Traffic regularly exceeded 1,500 queries per second (QPS) at peak times, and we successfully collected data from a wide sample of users around the world.\n\nComparing the results\n\nOver the course of our two-week experiment, 300,000 HTTP3 requests were fired per day. For the majority of our global users, HTTP3 reduced network latencies by 5-15ms (or 5%). While this is an improvement, these wins would appear negligible to the average user. At p90, however, HTTP3 demonstrated massive improvements, with a latency reduction of 48ms (or 13%)—and at p95, a reduction of 146ms (21%). This could be explained by the fact that HTTP3 is better at handling packet drops in parallel connections by eliminating head-of-line blocking; because packet drops are more likely to occur in networks with suboptimal connection quality, the benefits of HTTP3 are more visible at the higher percentiles.\n\nHTTP3 vs. HTTP2 p25 -4.23ms / -4.73% p50 -5.55ms / -4.15% p75 -13.1ms / -5.78% p90 -47.6ms / -12.5% p95 -146ms / -20.9%\n\nThe results are even more prominent when split by region at the higher percentiles. HTTP3 significantly reduced network latencies for Asia by around 77ms at p90 and by 200ms at p95. Other high-traffic regions like Europe and North and Central America experienced smaller absolute improvements, though the relative improvements are similar across the board (22% at p95).\n\nHTTP3 vs. HTTP2 North and Central America Europe Asia p25 -3.20ms / -6% -2.34ms / -2% -3.73ms / -2% p50 -4.21ms / -5% -3.84ms / -3% -5.12ms / -2% p75 -9.03ms / -8% -11.1ms / -6% -15.0ms / -4% p90 -44.9ms / -17% -47.3ms / -13% -77.3ms / -14% p95 -118ms / -22% -141ms / -21% -200ms / -22%\n\nWhat’s next for HTTP3", "label": "non_personal"}
{"title": "How we reduced the size of our JavaScript bundles by 33%", "url": "https://dropbox.tech/frontend/how-we-reduced-the-size-of-our-javascript-bundles-by-33-percent", "content": "When was the last time you were about to click a button on a website, only to have the page shift—causing you to click the wrong button instead? Or the last time you rage-quit a page that took too long to load? These problems are only amplified in applications as rich and interactive as ours. The more front-end code is written to support more complex features, the more bytes are sent to the browser to be parsed and executed, and the worse performance can get. At Dropbox, we understand how incredibly annoying such experiences can be. Over the past year, our web performance engineering team narrowed some of our performance problems down to an oft-overlooked culprit: the module bundler. Miller’s Law states that the human brain can only hold so much information at any given time—which is partially why most modern codebases (including ours) are broken up into smaller modules. A module bundler takes the various components of an application—such as JavaScript and CSS—and amalgamates them into bundles, which are then downloaded by the browser when a page is loaded. Most commonly, this takes the form of a minified JavaScript file that contains most of the logic for a web app. The first iteration of our module bundler was conceived way back in 2014—around the time that performance-first approaches to module bundling were becoming more popular (most notably by Webpack and Rollup in 2012 and 2015, respectively). For this reason, it was quite barebones relative to more modern options; our module bundler didn’t incorporate many performance optimizations and was onerous to work with, hampering our user experience and slowing down development velocity. As it became clear our existing bundler was showing its age, we decided the best way to optimize performance going forward would be to replace it. That was also the perfect time to do so since we were in the middle of migrating our pages to Edison—our new web serving stack—which presented an opportunity to piggyback on an existing migration plan and also provided an architecture that made it simpler to integrate a modern bundler into our static asset pipeline.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images—plus generative AI capabilities across even more connected apps. See what's new →\n\nExisting architecture\n\nWhile our existing bundler was relatively build-time efficient, it resulted in massive bundle sizes and proved to be a burden for engineers to maintain. We relied on engineers to manually define which scripts to bundle with a package, and we simply shipped all packages involved in rendering a page with few optimizations. Over time, the problems with this approach became clear: Problem #1: Multiple versions of bundled code\n\nUntil recently we used a custom web architecture called Dropbox Web Server (DWS). In short, each page consisted of multiple pagelets (i.e. subsections of pages), resulting in multiple JS entry points per page, with each servlet being served by its own controller on the backend. While this sped-up deployment in cases where a page was being worked on by multiple teams, it sometimes resulted in pagelets being on different backend code versions. This required DWS to support delivering separate versions of packaged code on the same page, which could potentially result in consistency issues (e.g. multiple instances of a singleton being loaded on the same page). Our migration to Edison would eliminate this pagelet architecture, giving us the flexibility to adopt a more industry-standard bundling scheme. Problem #2: Manual code-splitting\n\nCode splitting is the process of splitting a JavaScript bundle into smaller chunks, so that the browser only loads the parts of the codebase that are necessary for the current page. For example, assume a user visits dropbox.com/home, then dropbox.com/recents. Without code-splitting, the entire bundle.js is downloaded, which can significantly slow down the initial navigation to a page.\n\nAll code for all pages is served via a single file\n\nAfter code-splitting, however, only the chunks needed by the page are downloaded. This speeds up the initial navigation to dropbox.com/home, since less code is downloaded by the browser—and has several additional benefits too. Critical scripts are loaded first, after which non-critical scripts are loaded, parsed, and executed asynchronously. Shared pieces of code are also cached by the browser, further reducing the amount of JavaScript downloaded when moving between pages. All of the above can greatly reduce the load time of web apps.\n\nOnly the new chunks that are needed for the page are downloaded\n\nSince our existing bundler didn’t have any built-in code-splitting, engineers had to manually define packages. More specifically, our packaging map was a massive 6,000+ line dictionary that specified which modules were included in which package. As you can imagine, this became incredibly complex to maintain over time. To avoid sub-optimal packaging, we enforced a rigorous set of tests—the packager tests—which became dreaded by engineers since they would often require a manual reshuffling of modules with each change. This also resulted in a lot more code than what was needed by certain pages. For instance, assume we have the following package map:\n\nCopy { \"pkg-a\": [\"a\", \"b\"], \"pkg-c\": [\"c\", \"d\"], }\n\nIf a page depends on modules a, b, and c, the browser would only need to make two HTTP calls (i.e. to fetch pkg-a and pkg-b) instead of three separate calls, once per module. While this would reduce the HTTP call overhead, it would often result in having to load unnecessary modules—in this case, module d. Not only were we loading unnecessary code due to a lack of tree shaking, but we were also loading entire modules that weren’t necessary for a page, resulting in an overall slower user experience. Problem #3: No tree shaking\n\nTree shaking is a bundle-optimization technique to reduce bundle sizes by eliminating unused code. Let’s assume your app imports a third-party library that contains several modules. Without tree shaking, much of the bundled code is unused.\n\nAll code is bundled, regardless of whether or not it’s used\n\nWith tree shaking, the static structure of the code is analyzed and any code that is not directly referenced by other code is removed. This results in a final bundle that is much leaner.\n\nOnly used code is bundled\n\nSince our existing bundler was barebones, there wasn’t any tree shaking functionality either. The resulting packages would often contain large swaths of unused code, especially from third-party libraries, which translated to unnecessarily longer wait times for page loads. Also, since we used Protobuf definitions for efficient data transfer from the front-end to the back-end, instrumenting certain observability metrics would often end up introducing several additional megabytes of unused code!\n\nWhy Rollup\n\nAlthough we considered many solutions over the years, we realized that our primary requirement was having certain features like automatic code-splitting, tree shaking, and, optionally, some plugins for further optimizing the bundling pipeline. Rollup was the most mature at the time and most flexible to incorporate into our existing build pipeline, which is mainly why we settled on it. Another reason: less engineering overhead. Since we were already using Rollup for bundling our NPM modules (albeit without many of its useful features), expanding our adoption of Rollup would require less engineering overhead than integrating an entirely foreign tool in our build process. Additionally, this meant that we had more engineering expertise with Rollup’s quirks in our codebase versus that of other bundlers, reducing the the likelihood of so-called unknown unknowns. Also, replicating Rollup’s features within our existing module bundler would require significantly more engineering time than if we just integrated Rollup more deeply in our build process.\n\nRollup rollout\n\nWe knew that rolling out a module bundler safely and gradually would be no easy feat, especially since we’d need to reliably support two module bundlers (and consequently, two different sets of generated bundles) at the same time. Our primary concerns included ensuring stable and bug-free bundled code, the increased load on our build systems and CI, and how we would incentivize teams to opt-in to using Rollup bundles for the pages they owned. With reliability and scalability in mind, we divided the rollout process to four stages: The developer preview stage allowed engineers to opt-in to Rollup bundles in their dev environment. This allowed us to effectively crowdsource QA testing by having developers surface any unexpected application behavior introduced by Rollup bundles early on, giving us plenty of time to address bugs and scope changes.\n\nstage allowed engineers to opt-in to Rollup bundles in their dev environment. This allowed us to effectively crowdsource QA testing by having developers surface any unexpected application behavior introduced by Rollup bundles early on, giving us plenty of time to address bugs and scope changes. The Dropboxer preview stage involved serving Rollup bundles to all internal Dropbox employees, which allowed us to gather early performance data and further gather feedback on any application behavioral changes.\n\nstage involved serving Rollup bundles to all internal Dropbox employees, which allowed us to gather early performance data and further gather feedback on any application behavioral changes. The general availability stage involved gradually rolling out to all Dropbox users, both internal and external. This only happened once our Rollup packaging was thoroughly tested and deemed stable enough for users.\n\nstage involved gradually rolling out to all Dropbox users, both internal and external. This only happened once our Rollup packaging was thoroughly tested and deemed stable enough for users. The maintenance stage involved addressing any tech debt left over in the project and iterating on our use of Rollup to further optimize performance and the developer experience. We realized that projects of such a massive scale will inevitably end up accumulating some tech debt, and we should proactively plan to address it at some stage instead of sweeping it under the rug. To support each of these stages, we used a mix of cookie-based gating and our in-house feature-gating system. Historically, most rollouts at Dropbox are exclusively done using our in-house feature gating system; however, we decided to allow cookie-based gating to quickly toggle between Rollup and legacy packages, which sped up debugging. Nested within each of these rollout stages were gradual rollouts, which involved ramping up from 1%, 10%, 25%, 50%, to 100%. This gave us the flexibility to collect early performance and stability results—and to seamlessly roll-back any breaking changes if they occurred—while minimizing impact to both internal and external users. Because of the large number of pages we had to migrate, we not only needed a strategy to switch pages over to Rollup safely, but also to incentivize page owners to switch in the first place. Since our web stack was about to undergo a major renovation with Edison, we realized that piggybacking on Edison’s rollout could solve both our problems. If Rollup was an Edison-only feature, developer teams would have greater incentive to migrate to both Rollup and Edison, and we could tightly couple our migration strategy with Edison’s too. Edison was also expected to have its own performance and development velocity improvements. We figured that coupling Edison and Rollup together would have a transformational synergy strongly felt throughout the company.\n\nChallenges and roadblocks\n\nWhile we did expect to run into some unexpected challenges, we realized that daisy-chaining one build system (Rollup) with another (our existing Bazel-based infrastructure) proved to be more challenging than anticipated. Firstly, running two different module bundlers at the same time proved to be more resource-intensive than we estimated. Rollup’s tree-shaking algorithm, while quite mature, still had to load all modules into memory and generate the abstract syntax trees needed to analyze relationships and shake code out. Also, our integration of Rollup into Bazel limited us in being able to cache intermediary build results, requiring our CI to rebuild and re-minify all Rollup chunks on each build. This caused our CI builds to time-out due to memory exhaustion, and delayed the rollout significantly. We also found several bugs with Rollup’s tree-shaking algorithm which resulted in overly aggressive tree shaking. Thankfully, this only resulted in minor bugs that were caught and fixed during the developer preview phase without ever impacting our users. Additionally, we found that our legacy bundler was serving some code from third-party libraries that was incompatible with JavaScript’s strict mode. Serving this same code via the new bundler with strict mode enabled resulted in fail-hard runtime errors in the browser. This required us to conduct a one-time audit of our entire codebase and patch code that was incompatible with strict mode. Finally, during the Dropboxer preview phase, we found that our A/B telemetry metrics between Rollup and the legacy bundler weren’t showing as much of a TTVC improvement as we expected. We eventually narrowed this down to Rollup producing a lot more chunks than what our legacy packager produced. Although we initially hypothesized that HTTP2’s multiplexing would negate any performance degradations from a greater number of chunks, we found that too many chunks would result in the browser spending significantly more time in discovering all the modules needed for the page. Increasing the number of chunks also resulted in lower compression efficiency, since compression algorithms such as Zlib use a sliding-window approach to compression, which results in greater compression efficiency for one large file vs. many smaller files.\n\n\n\nResults", "label": "non_personal"}
{"title": "Why we built a custom Rust library for Capture", "url": "https://dropbox.tech/application/why-we-built-a-custom-rust-library-for-capture", "content": "In some respects, it was an easy decision. Dropbox has a thriving community of developers building Rust into our products. Rust is at the heart of our desktop client’s recently re-written sync engine , which is what makes the Dropbox folder on your computer work like magic. We also use it for file compression , in our crash reporting infrastructure , and in Magic Pocket—our exabyte scale custom storage infrastructure—to optimize the storage of file data .\n\nThere were a lot of ways we could have solved these problems—perhaps more TypeScript, or C++—but in the end we decided to go with Rust.\n\nIdeally, we wanted streamlined a codebase that could target multiple platforms painlessly, consistently, and that was easy for our developers to build. We also wanted more control over our ability to take screen captures and recordings, better error handling, and faster performance behind the scenes. In fact, we were looking for something that would give us more control at every layer—that didn’t require us to jump through quite so many hoops to call native code—and would better support the new features we wanted to build.\n\nOne of our team’s guiding principles is “be a Margherita pizza.” Just as a Margherita pizza is perfect in its simplicity—what more do you need than tomato sauce, mozzarella, and basil?—we’ve tried to keep Capture’s ingredients as simple and straightforward as possible. We knew early on that Electron and Node would make it easy to build a cross-platform TypeScript app for both macOS and Windows. But finding the right third ingredient that would enable us to quickly, simply, and reliably call native OS-level code took a bit more experimentation.\n\nDropbox Capture is a new visual communication tool designed to make it easy for teams to asynchronously share their work using screen recordings, video messages, screenshots, or GIFs. There's no formal onboarding required, and you can start sharing your ideas in seconds. In fact, simplicity is key to the Capture experience, and it's a value that also extends down to the development of Capture’s underlying code.\n\nFind, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images—plus generative AI capabilities across even more connected apps.\n\nIt turned out that Rust was perfect for our needs, too. Building a custom Rust library helped unlock higher-quality screen recording from 720p through 4K, made screenshots and screen recordings available to share more quickly, and dramatically improved our error handling capabilities, allowing us to offer a more reliable experience for our users.\n\nFrom Hack Week to here\n\nCapture began as an internal Hack Week project where rapid iteration was key. In early versions, we used a handful of third-party libraries to do things like take screenshots and process GIFs. Cobbling together bits of preexisting code helped us quickly develop a prototype, test out our initial assumptions, and experiment with new features. But when we considered the long-term health of Capture’s codebase—and all the complexity these third party libraries introduced—we knew we would eventually have to pay our early technical debt down.\n\nThe third-party libraries we used were usually shell-based applications; Capture would send commands to each shell app and receive stderr and/or stdout responses in return. This meant spinning up an application each time we wanted to complete certain tasks, or in some cases having an application running continuously and awaiting input—not exactly ideal.\n\nMore importantly, it also meant there was some inherent brittleness in the way Capture communicated with native code. Each line of output from the shell application had to be parsed. If a line failed to parse, we assumed it was an error, and if it was an error, the issue was likely masked and we wouldn’t know exactly what broke in the native code. As you might expect, this made monitoring and handling errors difficult!\n\nFrom a developer standpoint, the libraries posed other challenges. We found APIs could be quite different between macOS and Windows—even within the same cross-platform library—which added complexity when developing for the two platforms. And while some libraries were well maintained but missing features that we needed, other libraries had everything we wanted but were not as well maintained. Each presented tradeoffs we had to work around, some more easily than others.\n\nFor example, if we wanted to make any changes to the individual libraries we’d have to have the institutional knowledge of how to build each one and then build them into Capture. Case in point: It took one of our engineers hours of valuable development time to learn how to build the Windows screen recording library just to fix a single parsing bug!\n\n\n\nRust to the rescue\n\nBecause Rust was new to the Capture team, our early efforts were extremely incremental. Initially we focused on re-writing simple functions that otherwise required a third-party library. For example, activate-windows was previously a macOS-only library that let us bring a window to the forefront and only record that window. We were quickly able to port the feature to Rust on macOS, and then bring the feature to Windows where it didn’t previously exist.\n\nThese early successes gave us the confidence to try more ambitious things. The more we learned, the more features we moved to our custom Rust library, which benefitted Capture in a handful of ways:\n\nNo overhead. With Neon-bindings we could now easily make calls to native OS code from TypeScript without any overhead (and more reliably, too). In other words, we no longer had to spin up separate shell applications to complete certain tasks. Taking screenshots, for example, which was once asynchronous—requiring us to wait for a response from the shell application—was now immediate and fast.\n\nBetter error handling. Rust also dramatically improved our ability to handle errors. Once most of Capture’s code was running within our own library, and with a consistent API across both macOS and Windows, we were able to add more robust logging and monitoring. No more trying to interpret the output from shell apps! Having all of our code in one place gave us more insight into how our app actually was actually behaving.\n\nMore control. Ownership of the library meant fixes and enhancements could be made more quickly. Having all our code in one place also made it easier to tackle more nebulous issues—like the instability we kept encountering when taking captures or recordings with more than three screens. It resulted in a simpler build pipeline across platforms, too.\n\nA smaller footprint. Not having to include third-party libraries also reduced the overall size of our app. Around 17MB of Swift libraries were no longer needed on macOS, for example, after re-writing those capabilities in Rust. And now that we could simply call functions as needed—instead of having shell applications running in the background at all times—we also needed less memory than before.\n\n\n\nNew features. As we found early on with activate-windows, moving to Rust also allowed us to do things we just couldn’t do before. We were able to bring functionality to Windows that previously only existed on macOS. We were also able to introduce a new crop tool, new recording controls, and add new recording types like audio or camera only, as well as increase recording quality to 720p/1080p/4K. It’s not so much that we couldn’t have built these things with another language, but rather, Rust allowed us to build them faster and with less effort than before.\n\n\n\nWhat’s next for Capture and Rust\n\nCapture is available now in Beta, and if you haven’t already you should try it out.\n\n\n\nOne of our biggest surprises developing Capture was how easy it was to get started with Rust. In just a few weeks we were pushing Rust code to production, and we benefitted greatly from Dropbox’s supportive community of Rust-savvy engineers who offered help and guidance whenever we got stuck. And as our team grows, there’ll be less of a learning curve for new developers. Instead of wrestling with multiple libraries the way we used to, now we have a really simple document that basically says “here’s how to build everything using this single command.”\n\nWith Rust, our developers know exactly what to expect—just like our beloved Margherita pizza.\n\nIn time, we expect to move more features to our in-house library. The macOS screen recorder has already been re-written in Rust, and a similar re-write of the Windows recorder is on the way. We’ve been moving features like GIF creation and other OS-level integrations into our Rust library, too. And we’re especially excited for the future of Rust on Windows, which just recently reached v0.21.0.\n\nBut it’s also not an all-or-nothing approach. We’ve configured Rust so that we can still call third-party libraries using the old shell process approach if needed. This means we can be intentional about which features we choose to re-write and when. Of course, if we had struggled with Rust it would have been easy to turn back. But our enthusiasm and excitement turned out to be justified given the benefits Rust has unlocked.\n\n\n\nWe’re hiring!\n\nDo you love Rust? Do you want to grow as an engineer? Dropbox is hiring!\n\nMany of our teams are solving problems big and small with Rust and other new technologies—and Capture is no different. We're always on the lookout for clever, curious front-end engineers who want to learn new things, take on bigger challenges, and build products our customers love. If you're a front-end engineer with a talent, passion, and enthusiasm for Rust, we'd love to have you at Dropbox. Visit our careers page to apply.\n\nSpecial thanks to the Capture team (Youcef Es-skouri, Noga Raviv, Joey Diab, Kyle Shay, Andy Liu, Will Hall, Alex Pelan, Alan Chu, Mike Boht, Karan Khanna, Lien Chung, and Lily Lee) as well as Parker Timmerman and the rest of Dropbox’s internal Rust developer community.", "label": "non_personal"}
{"title": "How Dropbox Replay keeps everyone in sync", "url": "https://dropbox.tech/application/how-dropbox-replay-keeps-everyone-in-sync", "content": "How do you recreate the experience of an in-person screening room with a remote, distributed team? This is one of the reasons we built Dropbox Replay, our new video collaboration tool. Dropbox customers told us the shift to virtual work had turned their once-straightforward review sessions into lengthy, inefficient video calls. It was clear their previous in-person workflows hadn’t made an elegant transition online.\n\nWhen we looked at the market for virtual screening tools, we mostly found expensive Hollywood-scale solutions or clunky DIY experiences run over video conferencing tools. Nothing quite offered the kind of collaborative, live, and synchronous playback experience we envisioned—something that would approximate the feeling of being together in a screening room, with shared feedback, cursors, and playback controls. We knew there was an opportunity for an accessible yet high-quality online screening experience where people could collaborate in realtime as they would in person, but virtually—and where everyone could see the same thing and seamlessly annotate what they were seeing in real time. We created Dropbox Replay’s Live Review feature in response.\n\n\n\nBut keeping a virtual screening room with multiple collaborators in sync is a harder problem to solve than you might think. Anyone in a Live Review session can pause, adjust the playback speed, or scrub to a different frame at anytime. This is great for keeping screenings open and collaborative, but it also means that everyone might be sending conflicting commands at once. What if two people try to change the position of the video at the same time?\n\n\n\nFor Live Review to work, we need to make sure that everyone converges on the same playback state in a timely manner. When a person is ready to discuss a frame, they must be able to trust that everyone is seeing the same frame too.\n\nChanging states\n\nIn a Live Review session, we care about two types of state—single client state and shared client state. The position of someone’s mouse cursor or the drawings they make on a video frame are examples of single client state. These can’t conflict with other clients, so they’re relatively easy to handle: changes in single client state are sent to the server, which echoes them to the other clients, and that’s that.\n\nThe shared client state, on the other hand, is what keeps local playback synchronized between all of the clients in a Live Review session. When someone joins a Live Review session, their client opens a WebSocket connection with a Dropbox Replay server (we use an open source Go library called Gorilla, which is already used elsewhere within Dropbox). Anytime someone presses play, pause, or changes the position of the video, the client sends a message conveying this change to the server, which updates the shared client state and then sends it to everyone else in the session.\n\nThe playback state is encoded using Protocol Buffers, which offer an extensible, schematized, and space-efficient way to package the data. We like the combination of Protocol Buffers and WebSockets; Protocol Buffers don’t concern themselves with message framing and leave the format on the wire up to the developer. WebSockets, meanwhile, are largely content-agnostic and offer text or binary messages that are framed, delivered reliably and in order, and even take care of ping/pong heartbeats, which can keep sessions alive through proxies or firewalls that might otherwise terminate an idle connection.\n\nIn a perfect world, only one person would interact with the video at a time. But that’s not how most screenings work. Everyone has something to say, or something they want the group to see—often at the same time! It’s precisely because anyone can influence playback at any time that our protocol must be resilient enough to handle multiple concurrent interactions and still produce a consistent outcome for all participants.\n\nEstablishing an order of events\n\nOne approach might be to simply send all state changes—“video paused at frame” or “cursor is now at position”—to all other clients. However, when the state is shared between clients you can quickly see how this approach will lead to inconsistent states if more than one client changes the state at the same time—for example, if Patty makes a change, sends it to Steven, but Steven makes and sends a change before Patty’s change has arrived.", "label": "non_personal"}
{"title": "Fighting the forces of clock skew when syncing password payloads", "url": "https://dropbox.tech/application/dropbox-passwords-clock-skew-payload-sync-merge", "content": "A good password manager should be able to securely store, sync, and even autofill your username and password when logging into websites and apps. A password manager like…Dropbox Passwords! When we released Dropbox Passwords in the Summer of 2020, it was important we ensured that a user’s logins would always be available—and up to date—on any device they used. Luckily, Dropbox has some experience here, and we were able to leverage our existing syncing infrastructure to copy a user’s encrypted password info, known as a payload, from one device to another. However, while implementing this crucial component, we encountered an unexpected syncing issue where, sometimes, out-of-date login items would overwrite newer, more recent changes. Eventually we found a solution that built on prior Dropbox syncing work. But it also involved contemplating the very nature of time itself.\n\nThe two-way merge\n\nDropbox Passwords uses zero-knowledge encryption. This means the private encryption key for a user’s passwords is only stored on their local devices, and the server is unable to decrypt them. While this is good for security purposes, it means we need to rely on the client to manage syncing and merging, and thus cannot rely on the server as a source of truth for recency as we typically would. Initially, our sync algorithm used a two-way merge. If a user edited an existing login item—thus, modifying the client’s local payload—the client performed the following steps: Update the local payload with the new details and current time.\n\nDownload the equivalent payload from the server.\n\nCompare the timestamp of the local login item with the timestamp of the remote login item.\n\nUse the newer login item as the winner. Here’s a diagram illustrating what the flow might look like when a user updates their password from hunter1 to hunter2.\n\nA successful merge and sync.\n\nIn theory, this worked nicely. Old items were replaced with their most recent versions. In practice, however, we noticed that some items were syncing incorrectly. One of our engineers discovered a phenomenon in which an older item would somehow trump a newer one. After some thorough investigation, we found our culprit: clock skew.\n\n\n\nConflicting clocks\n\nLet’s consider the following scenario. You are a happy Dropbox Passwords user with a computer and phone synced to the same account. You edit the notes field on one of your login items from your phone so that you can remember those pesky security questions. A few minutes later, you realize that you made a mistake when typing one of the questions—but your phone is all the way across the room. So you open up your computer to fix your mistake. After updating the notes field, you save the item, but something strange has happened. You are shocked to see your initial change—the one you made from your phone—has overwritten the newer change you just made on your laptop. How could this be? Recall that the client is responsible for setting the timestamps on each item. We generally assume that our electronic devices have an accurate representation of the current time, but this is not always the case. It is actually possible for a device’s time to be out of sync with true time. This could be the result of a dead CMOS battery, an incorrect time zone setting, or perhaps even malware. In our example, the phone’s login item trumps the computer’s login item because of this clock skew. If the computer’s clock is behind true time—by ten minutes, ten seconds, or even ten years—it’s possible that a more recent change will actually be marked as older. Here is a diagram illustrating what a failed sync might look like with offset clocks.\n\nAn example of a failed merge. Note that the timestamp on the newer change is older than the stable version.\n\nThe three-way merge\n\nIn order to tackle this problem, we borrowed the idea of a three-way merge from Git, the popular source control tool. Git uses a three-way merge when a user wants to merge two branches but there isn’t a linear path to merge from one branch to another branch. In these cases, there is a common ancestor of both branches, but no branch is an ancestor of the other. In a three-way merge, the common ancestor and the tips of both branches are used to generate a merged commit. In fact, we already use the same idea in the Dropbox desktop client when syncing files between devices. Although this client has a more advanced version of conflict handling, the basic principle is the same. We realized a three-way merge might help us here, too. In our case, the main problem with doing a two-way merge is that, because we can’t use the server as source of truth, we don’t have a revision history to keep track of which revision is earlier. As a result, we always rely on timestamps to determine which update is newer. In order to solve this problem, we introduced a third payload copy known as base. Now… We have a local payload that represents what the user sees on their client.\n\npayload that represents what the user sees on their client. We have a remote payload that represents what is stored on the Dropbox server.\n\npayload that represents what is stored on the Dropbox server. We have a base payload that represents the most recently synced payload on that client. Instead of a simple time comparison of local and remote, we can now generate a diff from local <> base and a diff from remote <> base. If there are any differences between base and the payload to which we are comparing it, we know that the most recent change must come from the non-base payload, since there is no way to edit base directly. Once we obtain a list of local diffs and remote diffs, we can attempt to consolidate changes, with a goal of bringing the three payloads back in sync. There are two possible outcomes when comparing the diffs: Entries in either diff but not both. This means that the diffs do not conflict with each other. Both changes can be safely applied.\n\nThis means that the diffs do not conflict with each other. Both changes can be safely applied. Entries in both diffs. Changes in this category are considered conflicted, because both the local and remote payloads have updates to the same item. In this case, we will use their timestamp to resolve conflicts. The winner of the change will be the one that has a more recent timestamp. A keen reader will point out that this merging process is still susceptible to clock skew—but under normal usage, this is unlikely to ever occur. A user would not only need two devices with out-of-sync clocks, but would have to edit the same item on both devices at the same time. With the three-way merge in place, it was clear the addition of a base payload solved our merge issue. In a typical clock skew scenario, we no longer need to compare timestamps as there should be no conflicts during our merge.\n\nA successful merge and sync with skewed clocks.\n\nA solid foundation for the future\n\nAfter a year in general availability, the three-way merge has proven to be a much more robust solution than what we had before. We were especially thankful for the extra stability as we added new features throughout 2021—including support for password sharing and payment card items. Our two-way merge issue could have been much more frustrating for users trying to modify a shared item, so we were happy to solidify our foundation before building atop it. But why read about it when you can try it for yourself? Install Dropbox Passwords today and let your device finally experience the thrill of a three-way merge.\n\nOne more thing…", "label": "non_personal"}
{"title": "Listing the contents of all team-accessible namespaces", "url": "https://dropbox.tech/developers/listing-team-contents", "content": "A Dropbox team can contain multiple members, and each member may have many of their own files and folders, as well as access to other files and folders, such as via shared folders or team folders. That can add up to a lot of content, and sometimes, it‘s useful to be able to programmatically list all of those items across all of the members of the team, such as for organizational or auditing purposes.\n\n\n\nThe Dropbox API does offer the ability to list the contents of any particular folder, and there are multiple ways an app could go about finding and listing all of the contents across all team members. One good way to do so is by first getting the list of all of the namespaces that the team has access to, and then listing the contents of each of those namespaces. This way helps avoid reading the same listing twice (for example, in the case where multiple team members are in the same shared folder). This works for teams with any type of team folder or team space configuration.\n\n\n\nCheck out the new code sample here for an example of how to implement that strategy to list all of the files and folders that a team can access.", "label": "non_personal"}
{"title": "Dropbox API server root certificate changes coming in 2026", "url": "https://dropbox.tech/developers/api-server-certificate-changes", "content": "The Dropbox API servers will be updated with certificates issued from a new root starting on or after January 1, 2026.\n\nSome of the official Dropbox SDKs implemented certificate pinning based on a list of root certificates included in the SDKs. These certificates will stop working in many web browsers and devices in 2026, so Dropbox needs to change which root is used to issue its server certificates. Apps using these SDKs need to be updated to the latest versions of the Dropbox SDKs to ensure uninterrupted access to the API.\n\nThe versions listed below have been updated to maintain compatibility with the Dropbox API servers going forward:\n\nJava SDK v7.0.0 or greater\n\n.NET SDK v7.0.0 or greater Note that apps using older versions of the .NET SDK are only affected if they call DropboxCertHelper.InitializeCertPinning() .\n\nPython SDK v12.0.2 or greater\n\nIf you are using any of the above SDKs, please update your app(s) to use the latest versions of these SDKs as soon as possible to ensure continued access to the Dropbox API.\n\nThe following official Dropbox SDKs are not affected:\n\nThird party libraries may or may not be affected by these changes. Please consult their documentation for further information.\n\nIf you have any questions, you can always reach us on our forum or via our contact form.", "label": "non_personal"}
{"title": "Customizing scopes in the OAuth app authorization flow", "url": "https://dropbox.tech/developers/customizing-scopes-in-oauth-flow", "content": "As you may know, the Dropbox API authorization system uses \"scopes\" for granular control over what functionality an app can access. This allows app developers to select what API functionality their apps can use, so that users can feel more comfortable granting apps access to their accounts. This can help give users peace of mind that the apps will only be able to perform the operations that the apps actually need. It may not be obvious though that you can further customize exactly which scopes your app requests and when. Let's look at the options for configuring and customizing scopes in more detail.\n\nFirst, it's important to note that the scopes you enable on the Permissions tab of the app's page on the App Console define the maximum, as well as the default, set of scopes that the app can request. For example, let's look at a user-linked app. By default, it has the account_info.read scope, which is required to be registered for user-linked apps. We'll also enable files.content.read and files.metadata.read scopes for this example.\n\nA screenshot showing the app’s scopes configuration.\n\nWhen we send a user to the app authorization page, by default, they'll be prompted to authorize the app with all of those scopes: https://www.dropbox.com/oauth2/authorize?client_id=<APP_KEY>&response_type=code\n\nA screenshot of the app authorization page defaulting to the scopes registered to the app.\n\nHowever, if you don't need all of the scopes that are currently enabled on the app, you can instead set the scope parameter on the /oauth2/authorize URL you construct. In that parameter, you can put a space-delimited list of scopes to specify just a sub-set of scopes to request for that authorization. This can be useful in scenarios where the app doesn't need all of the app's potential access, or as a way to more gradually gain the user's trust.\n\n\n\nFor example, say we just want the app to be able to read the metadata of the files and folders in the user’s account; we would construct the URL like this: https://www.dropbox.com/oauth2/authorize?client_id=<APP_KEY>&response_type=code&scope=files.metadata.read\n\nA screenshot of the app authorization page requesting a sub-set of the scopes registered to the app.\n\nTip: Note how even though account_info.read is required to be enabled on the app itself, you don't have to request it during authorization. For more privacy-oriented scenarios where the app doesn't need access to the user's account information, you can set the scope parameter without the account_info.read scope as above.\n\nIf a user authorizes the app using that /oauth2/authorize URL, the app will then receive a payload like the following when it subsequently makes the corresponding call to /oauth2/token using the resulting authorization code:\n\nCopy { \"access_token\": \"<ACCESS_TOKEN>\", \"token_type\": \"bearer\", \"expires_in\": 14400, \"scope\": \"files.metadata.read\", \"uid\": \"<USER_ID>\", \"account_id\": \"<ACCOUNT_ID>\" }\n\nIf the app needs additional scopes later, it can prompt the user to authorize the app again, with the scope parameter configured with more scopes, or without the scope parameter set at all, to request all of the app’s scopes.\n\nYou can also use the include_granted_scopes parameter to make it easier to request additional scopes without explicitly listing the previously granted scopes again. For example, if we then additionally want the app to be able to read the content of files in that same user’s account, we would construct another URL like this:\n\nhttps://www.dropbox.com/oauth2/authorize?client_id=<APP_KEY>&response_type=code&scope=files.content.read&include_granted_scopes=user\n\nA screenshot of the app authorization page requesting additional scopes registered to the app.\n\nIf the user authorizes the app using that /oauth2/authorize URL, the app will then receive a payload like the following when it subsequently makes the corresponding call to /oauth2/token using the resulting authorization code:\n\nCopy { \"access_token\": \"<ACCESS_TOKEN>\", \"token_type\": \"bearer\", \"expires_in\": 14400, \"scope\": \"files.content.read files.metadata.read\", \"uid\": \"<USER_ID>\", \"account_id\": \"<ACCOUNT_ID>\" }", "label": "non_personal"}
{"title": "How we use Lakera Guard to secure our LLMs", "url": "https://dropbox.tech/security/how-we-use-lakera-guard-to-secure-our-llms", "content": "From search to organization, rapid advancements in artificial intelligence (AI) have made it easier for Dropbox users to discover and interact with their files. However, these advancements can also introduce new security challenges. Large Language Models (LLMs), integral to some of our most recent intelligent features, are also susceptible to various threats—from data breaches and adversarial attacks to exploitation by malicious actors. While hundreds of millions of users already trust Dropbox to protect their content, ensuring the security and integrity of these models is essential for maintaining that trust. Last year we evaluated several security solutions to help safeguard our LLM-powered applications and ultimately chose Lakera Guard. With its robust capabilities, Lakera Guard helps us secure and protect user data, and—as outlined in our AI principles—uphold the reliability and trustworthiness of our intelligent features. Addressing these challenges requires a multifaceted approach, incorporating stringent security protocols, continuous monitoring, and proactive risk management strategies. In this story, we’ll share insights into our approach to securing our LLMs, the criteria we used to evaluate potential solutions, and the key benefits of implementing Lakera's technology.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images—plus generative AI capabilities across even more connected apps. See what's new →\n\nWhat we were looking for\n\nLLM security is comprised of many parts. Common problems include reliability, consistency, alignment, and adversarial attacks. However, the scope of the problem we were trying to solve was more customer-centric—specifically, using LLMs to chat about, summarize, transcribe, and retrieve information, in addition to agent/assistant use cases. These kinds of untrusted user inputs could result in moderation issues or prompt injection—a method sometimes used to manipulate models—which creates a lot of headaches, including undesirable model outputs. We considered a variety of open source, in-house, and proprietary options before narrowing our criteria to either open source or commercial tools. Whatever we chose, we decided the following requirements were mandatory: We couldn’t call out to a third party. The solution had to be deployable in-house on our existing infrastructure.\n\nThe solution had to be deployable in-house on our existing infrastructure. Low latency. Dropbox is committed to maximizing performance for users across all of its products. We couldn’t add latency to LLM-powered features any more than absolutely necessary, so we determined upper latency numbers with the product teams. Latency for a given context length is also an important sub-problem here. Many options perform well on context lengths of <800 tokens, but drop off significantly at >4,000. Excellent support for long context lengths—the ability for models to process greater amounts of information—was critical, as many customer use cases routinely exceed this number.\n\nDropbox is committed to maximizing performance for users across all of its products. We couldn’t add latency to LLM-powered features any more than absolutely necessary, so we determined upper latency numbers with the product teams. Confidence scores. API integrations that not only allowed extensive control over the categories of blocking, but also the sensitivity, were key (eg., separating the danger classification jailbreak based on confidence scores in order to ensure we could meet the diverse needs of our product teams).\n\nAPI integrations that not only allowed extensive control over the categories of blocking, but also the sensitivity, were key (eg., separating the danger classification based on confidence scores in order to ensure we could meet the diverse needs of our product teams). Future intelligence and continuous improvement. Since LLM security is a fast evolving space, we wanted a solution that could also give us actionable insights into attacks and payloads in a rapidly shifting environment. In fact, given the rapidly shifting environment, our top priority was selecting a solution that gave us enough of a foothold to observe and reorient as needed.\n\nHow we tested\n\nOnce we had a short list of open-source and commercial tools that met our criteria, we set up each tool internally for evaluation. For our test suite, we used Garak, an open-source LLM vulnerability scanner customized to run Dropbox-specific security tests. With Garak, we could evaluate the security coverage of each of the potential solutions. This allowed us to conduct a range of tests involving prompt injection, jailbreak, and other security assessments developed by Dropbox. We then tested each solution directly against a range of LLMs already in use or under evaluation by our product teams. This enabled us to establish a baseline of each model’s vulnerability. For example, if a security tool blocked 90% of malicious prompts, but the LLM had already mitigated 85% of these vulnerabilities, we measured a net improvement of only 5%. Finally, we needed a tool that did not add excessive latency to LLM calls and respected the privacy of customer data (e.g., did not store prompt content or send it outside the Dropbox network). For this, we measured the response time of each security test and also monitored network requests and file changes to detect any potential breaches of user data. After extensive testing, Lakera Guard emerged as the product meeting all our requirements, offering both the lowest latency and highest security coverage.\n\nHow we integrated Lakera Guard\n\nLakera provides a Docker container that we run as an internal service at Dropbox. This means Lakera Guard is just an RPC call away from any LLM pipeline. Conceptually, the LLM security architecture at Dropbox is designed using LangChain as shown in the figure below.\n\nA high-level representation of the internal AI/ML security infrastructure at Dropbox\n\nHere, a textual LLM prompt is directed through one or more prompt security chains before hitting the model. We have a security chain that makes Lakera Guard security API endpoint requests to our internally-hosted Docker container, which responds with confidence scores for prompt injection and jailbreak attacks. Dropbox services can then action on the returned Lakera Guard prompt security categories as appropriate for the application. Prompts that are deemed to be safe are then passed to the LLM—either a third-party model, like GPT-4, or an internally hosted open-source model, like LLaMA 3, depending on the use case—which produces a textual response. The LLM’s response is then passed through our content moderation chains, which analyze the text for potentially harmful topics. The moderation chain calls out to Lakera’s content moderation API endpoint to identify harassing or explicit content that the Dropbox feature or service can withhold from the user as configured. Integrating Lakera Guard into our Dropbox infrastructure was a gradual process. We started with one product directly calling the Lakera-provided Docker container. Eventually, we created a custom Dropbox service that can automatically scale up more containers as needed—and can be called via the LLM security layer we built as part of Dropbox’s central machine learning libraries.\n\nWhat we learned, and what’s next", "label": "non_personal"}
{"title": "What’s new with Robinhood, our in-house load balancing service", "url": "https://dropbox.tech/infrastructure/robinhood-in-house-load-balancing-service", "content": "Robinhood is the internal Dropbox load balancing service we deployed in 2020. It is responsible for routing all our internal traffic between servers to balance service load. Before we built Robinhood, most Dropbox services suffered from uneven load distribution among backends. Hardware differences throughout our fleet and the limitations of our prior load balancing algorithms led to reliability issues due to overloaded instances. In order to solve this problem, we often had to over-provision a service’s fleet, which inevitably increased our hardware spend—a pricey and avoidable headache. Robinhood solves these long-standing load balancing problems at Dropbox scale, across our entire global data center footprint. Last year, we introduced the latest iteration to Robinhood: By leveraging proportional–integral–derivative (PID) controllers, Robinhood can now more quickly and effectively manage load imbalances. This has not only improved the reliability of our infrastructure, but yielded significant hardware cost savings. And with an increase in AI workloads that power our latest intelligent features, effectively managing demands on our GPU resources is more critical to the business than ever.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images—plus generative AI capabilities across even more connected apps. See what's new →\n\nThe challenge of load balancing at Dropbox\n\nOur in-house service discovery system can scale to hundreds of thousands of hosts across multiple data centers around the globe. Some Dropbox services have millions of clients; however, we cannot allow each client to create connections to every server instance. This approach puts too much memory pressure on servers, and TLS handshakes during server restarts can overwhelm servers. Instead, we can use a service discovery system, which gives each client a subset of servers to connect to. Without other information, the best load balancing strategy a client can use is a round-robin of the list of addresses given by our service discovery system. However, the load on each server instance can be quite imbalanced with this method1. While increasing the subset size is an easy mitigation, it won’t completely eliminate the imbalance and would just give service owners another parameter to deal with. And there’s another, deeper issue. Even if we send the same number of requests to each server, the underlying hardware might differ from one server to the next. In other words, a request would consume a different amount of resources on different hardware classes.\n\nAt its core, the issue is that clients do not have visibility into server load. Years ago, we attempted to solve the problem by having servers attach the load in the response headers. Clients could then perform load balancing themselves by picking the least-loaded endpoint in the subset of addresses. The results were promising, but there were still several downsides. The approach required code changes to servers and clients for the special load header, which was hard to adopt globally. More importantly, the results were good, but they weren’t good enough. In 2019, we officially decided to build Robinhood. This new service, built on top of our existing in-house service discovery system, collects load information from servers and attaches it to the routing information. Robinhood leverages Envoy’s Endpoint Discovery Service, which incorporates the load information into endpoint weights so that clients can perform weighted round-robin. Now that the gRPC community is adopting the Envoy xDS protocol, Robinhood is compatible with both our Envoy and gRPC clients2. Another reason to build a new service was that, to our knowledge, there was no existing load balancing solution that met our needs at the time. After a few years in production, the results have been promising. We successfully reduced fleet size by 25% for some of our largest services, resulting in substantial hardware cost savings each year. Reliability has also improved, thanks to fewer over-utilized processes. Below, we break down the architecture of Robinhood to illustrate how we created a far superior system for balancing service load.\n\nThe architecture of Robinhood\n\nA deployment of Robinhood within a single datacenter\n\nAs shown in the illustration above, a Robinhood instance is deployed to each of our data centers and consists of three parts: the load balancing service, a proxy, and routing database. Load balancing service (LBS) This is the heart of Robinhood. The LBS is responsible for collecting load information and generating routing information with endpoint weights. Since we have multiple instances updating the routing info for a service concurrently, we use our in house shard manager to assign the primary worker for each service. In addition, each service is independent, so we can shard the LBS by service and scale it horizontally. Proxy The proxy is responsible for routing a service’s load information to the corresponding LBS partition within the data center. A nice side effect of this setup is that the proxy also reduces the number of connections directly to LBS processes. Without a proxy, every LBS process would have to be connected to all nodes within our infrastructure. Instead, LBS processes are only connected to the proxy, which greatly reduces the memory pressure on the LBS. Also, because the proxy is only connected by nodes within the same data center, it can be scaled horizontally. This pattern is used in many parts of our infrastructure to protect services from receiving too many TLS connections. Routing database The routing database is a ZooKeeper/etcd-based database that stores routing information for services, such as hostname, IP address, weights generated by the LBS, etc. ZooKeeper and etcd can notify all watchers in real time of any node/key changes, and it scales pretty well for our read-heavy service discovery use case. The eventual consistency guaranteed by ZooKeeper/etcd is good enough for service discovery as well.\n\nA closer look at the load balancing service\n\nThe goal of load balancing is to ensure that the utilization of every node is equal to the average utilization. We use a PID controller to keep the utilization of each node almost the same as the average utilization. The LBS creates a PID controller for each node and uses the average utilization as the setpoint. The LBS then uses the output of the PID controller as the delta to the endpoint weight and normalizes the weight among all endpoints of the service. While it takes a couple of adjustments for a new node to converge on the average utilization, the PID controller works quite well for load balancing. The LBS is designed to handle a variety of scenarios that can affect load balancing, from node restarts to missing load reports. To maintain optimal performance, the LBS has implemented several strategies to handle these edge cases, which are detailed below. LBS start up. The LBS keeps load information and PID controller states in memory. During an LBS restart (which can occur due to a normal push, node rotation, hardware failure, etc.), the LBS does not immediately start updating weights but rather waits a short period of time for load reports to come in. For PID controller weights, LBS restores them by reading the endpoint weights from the routing database.\n\nThe LBS keeps load information and PID controller states in memory. During an LBS restart (which can occur due to a normal push, node rotation, hardware failure, etc.), the LBS does not immediately start updating weights but rather waits a short period of time for load reports to come in. For PID controller weights, LBS restores them by reading the endpoint weights from the routing database. Cold start node. New nodes frequently join the service fleet, and so it’s important we prevent thundering herd issues. Since a new node typically has an initial utilization of 0, LBS sets the weight of the new node to a low endpoint weight and lets the PID controller ramp up the node to the average utilization.\n\nNew nodes frequently join the service fleet, and so it’s important we prevent thundering herd issues. Since a new node typically has an initial utilization of 0, LBS sets the weight of the new node to a low endpoint weight and lets the PID controller ramp up the node to the average utilization. Missing load reports. Failures are common in distributed system environments. For example, load reports of some nodes might be delayed or never actually arrive because of network congestion or hardware failures. LBS skips these nodes during weight updates, meaning endpoint weights stay the same for those nodes since it doesn’t know whether to increase or decrease the weight of those nodes. However, if a large portion of load reports are missing—currently configured at 15%—the average utilization calculation can be off, so it might not have an accurate setpoint to adjust node weights. For safety, LBS skips the weight update phase entirely in this case.\n\nFailures are common in distributed system environments. For example, load reports of some nodes might be delayed or never actually arrive because of network congestion or hardware failures. LBS skips these nodes during weight updates, meaning endpoint weights stay the same for those nodes since it doesn’t know whether to increase or decrease the weight of those nodes. However, if a large portion of load reports are missing—currently configured at 15%—the average utilization calculation can be off, so it might not have an accurate setpoint to adjust node weights. For safety, LBS skips the weight update phase entirely in this case. Utilization metric. CPU utilization is the most popular metric for load balancing at Dropbox. For services not bottlenecked by CPU, the number of in-flight requests is a good alternate measurement. Therefore, we implemented LBS to support load balancing based on CPU and/or in-flight requests. 3\n\nCPU utilization is the most popular metric for load balancing at Dropbox. For services not bottlenecked by CPU, the number of in-flight requests is a good alternate measurement. Therefore, we implemented LBS to support load balancing based on CPU and/or in-flight requests. Limitations. The PID controller constructs a feedback loop to keep the utilization of the node close to the target value (the average utilization, in our case). This means that if there is little feedback—for example, in the case of a very low traffic service, or very high-latency requests measured in minutes—the load balancing won’t be as effective. We argue that services with high latency requests should be asynchronous.\n\nRouting across data centers\n\nAn LBS instance handles load balancing within the data center. For cross-data center routing, there are different considerations. For example, we want to route requests to the closest data center to reduce the round trip time for the requests. Therefore, we've introduced a locality config for defining traffic splits between destination data centers:\n\nCopy { # client data center -> traffic split between destination data centers zone_1: { \"zone_1\": 100, } zone_2: { \"zone_2\": 50, \"zone_1\": 50, } }\n\nThis example config indicates that for clients in zone_1, 100% of requests are sent to zone_1, and for clients in zone_2, requests are evenly split between zone_1 and zone_2. The service discovery system utilizes this config to build Endpoint Discovery Service responses for clients. gRPC clients and Envoy perform two layers of weighted round-robin. The load balancer first selects the zone and then selects endpoints within that zone. Additionally, we support hot reload for changes to the locality config, allowing service owners to perform real-time failovers between data centers.\n\nEvaluating the performance of our load balancer\n\nWe measure load balancing performance with a max/avg ratio. For example, if the service owner chooses to load balance based on CPU, we use maxCPU/avgCPU as the indicator of performance. The reason is that service owners usually provision their service based on the maximum utilization among nodes, and the main purpose of load balancing is to reduce the size of the fleet. The PID controller load balancing strategy can achieve a max/avg ratio close to 1.\n\nThis graph shows the max/avg CPU and p95/avg CPU of one of our biggest Envoy proxy clusters. After enabling PID controller-based load balancing, the two metrics dropped close to 1. The max/avg ratio dropped from 1.26 to 1.01, showing a 20% (1.01/1.26 ~ 0.8) improvement.\n\nThe graph above shows the quantile breakdown of CPU utilization per node. After enabling PID controller-based load balancing, the max, p95, avg, and p5 almost consolidated into a single line. Let’s look at another good example:\n\nNow, this graph shows the max/avg CPU and p95/avg CPU of another one of our biggest database frontend clusters. After enabling PID controller-based load balancing, the two metrics dropped close to 1. The max/avg ratio dropped from 1.4 to 1.05, showing a 25% improvement.\n\nFinally, this graph shows the quantile breakdown of CPU utilization per node. After enabling PID controller-based load balancing, the max, p95, avg, and p5 almost consolidated into a single line once again.\n\nWhy we built a config aggregator\n\nRobinhood exposes several options for service owners to choose from, and can even apply changes dynamically. Service owners create and update the Robinhood config for their services from within their service directory in the codebase. We then store these settings in our config management service, a convenient library that receives any changes to Robinhood’s config in real-time. However, we cannot simply build and push Robinhood’s mega config regularly from the codebase due to several problems: If a breaking change is introduced by a config push, it's risky to press the rollback button because we don’t know how many other services have also made changes since the last push.\n\nThe team that owns Robinhood is also responsible for each mega config push. This means that the Robinhood team would have to get involved in every breaking config push—which is a waste of engineering time, since most incidents can be resolved by the service owner.\n\nEach push takes hours to deploy to multiple data centers in order to minimize potential risks. To address these problems, we build another small service: the config aggregator.\n\nInstead of storing one Robinhood mega config in the config management service, we break the config into per-service configs. Each per-service config only contains the configuration for that particular service. This way, service owners can update and deploy their changes at any time without worrying about being affected by changes in other services. In the event of a breaking change, service owners can also roll back config pushes or apply fixes during incidents without having to involve the Robinhood team. To simplify the LBS and keep it focused on its primary task, we built another service—the config aggregator—which collects all the per-service configs and construct the mega config for LBS to consume. The config aggregator watches per-service configs and propagates the changes to the mega config in real-time. The config aggregator also provides a tombstone feature to prevent accidental deletion of a service's Robinhood config. When a service owner pushes a change to delete a service from the Robinhood config, the config aggregator puts a tombstone mark on the entry of the service instead of removing it immediately. The actual removal happens after several days. This feature also solves a race condition that could result from the different push cadences between the Robinhood config and other routing configs (e.g., Envoy config). One downside of our config management service is that it's not currently versioned. We periodically backup the mega config in case we need to revert the LBS config back to a known good state.\n\nA quick note on our migration strategy\n\nIt can be risky to switch load balancing strategies all at once. This is why we enable service owners to configure multiple load balancing strategies for a service in Robinhood. The LBS writes the weighted endpoints list generated by different strategies into different entries in the routing database. At Dropbox, we have a percentage-based feature gate, so we implement a mixed strategy where clients use the weighted sum of the weights generated by two load balancing strategies as the endpoint weight. For example, endpoint A might be weighted at 100 based on PID-based load balancing and 200 based on simple round-robin strategy. If we set the feature gate to 30% for PID-based load balancing, the weight of endpoint A becomes 100*0.3 + 200*0.7 = 170. This way, we can ensure that every client sees the same weight assignment for endpoints while gradually migrating to the new load balancing strategy.\n\nWhat we learned\n\nIn designing and implementing Robinhood, we learned several key lessons about what works and what doesn't. By prioritizing simplicity, minimizing client changes, and planning for migration from the outset, we were able to streamline the LBS's development and deployment, and avoid costly pitfalls. Configuration should be as simple as possible. Robinhood introduces many options for services owner to configure. However, for most cases what they need is a provided default setting. A good, simple default config—or even better, zero config—can save tons of engineering time.\n\nRobinhood introduces many options for services owner to configure. However, for most cases what they need is a provided default setting. A good, simple default config—or even better, zero config—can save tons of engineering time. Keep client changes simple, too. It can take several months to roll out changes to internal clients; although most deployments are pushed weekly, many are deployed only once a month, or not at all, for years. We learned that the more changes we could shift to the LBS, the better. For example, we decided early on to use weighted round robin for our client design, and we have not changed it since—which has significantly accelerated our progress. Limiting most of our changes to the LBS also reduces reliability risks. This is because we can roll back changes in the LBS within minutes if necessary.\n\nIt can take several months to roll out changes to internal clients; although most deployments are pushed weekly, many are deployed only once a month, or not at all, for years. We learned that the more changes we could shift to the LBS, the better. For example, we decided early on to use weighted round robin for our client design, and we have not changed it since—which has significantly accelerated our progress. Limiting most of our changes to the LBS also reduces reliability risks. This is because we can roll back changes in the LBS within minutes if necessary. Migration should be planned at project design phase. A migration takes a huge amount of engineering time. There are also reliability risks to consider. It’s not fun, but it’s important work. When designing a new service, think about how to smoothly migrate existing use cases onto the new service as early as possible. The more you ask of service owners, the more migration becomes a nightmare—especially for fundamental infrastructure components. The migration process for Robinhood was not well-designed from the very beginning, so we ended up spending much more time than expected reimplementing the process and redesigning the configuration. The amount of engineering time required for a migration should be a key metric for success. After roughly a year in production, it’s safe to say that the latest iteration of Robinhood effectively addresses our long-standing load balancing challenges. The PID controller algorithm at its core has yielded promising results—showcasing significant performance improvements in our largest services—and we’ve gained valuable insights into the design and operation of load balancing services at Dropbox-scale. Special thanks to Mikhail Dolinin, Sai Teja Duthuluri, Nikitha Girish, Jared Hance, Itsik Hefez, Pardhu Konakanchi, Andrew Lazarev, Miguel Michel, Ruslan Nigmatullin, Pranesh Pandurangan, Yi-Shu Tai, Jialin Xu, and all past and present members of the runtime team and services platform teams for their contributions.", "label": "non_personal"}
{"title": "Selecting a model for semantic search at Dropbox scale", "url": "https://dropbox.tech/machine-learning/selecting-model-semantic-search-dropbox-ai", "content": "Nautilus is our search engine for finding documents and other files in Dropbox. Introduced in 2018, Nautilus uses a conventional keyword-based approach that, while functional, has some inherent shortcomings. Because Nautilus has limited contextual understanding of what someone may be looking for, users are required to precisely recall a file’s exact name or the specific keywords within. For instance, a search for “employment contract” may overlook relevant “job agreement” or “offer letter” documents, as Nautilus did not grab their contextual similarity. And for multilingual users, Nautilus expects queries and documents to be in the same language, hindering efficient retrieval when dealing with content in different languages. To mitigate these limitations, we considered techniques such as stemming, spelling correction, and query expansion for improved flexibility. However, we wondered if we could elevate the Dropbox search experience further. Could it be possible to help users find their content without needing to know the exact search term? Enter semantic search. Rather than rely on exact keyword matches, semantic search aims to better understand the relationship between user queries and document content. This functionality ultimately enables Dropbox users to locate crucial information more quickly, so they can spend less time searching and more time focusing on the task at hand. For multilingual users, semantic search also unlocks another capability: cross-lingual search. This advanced feature allows users to search in one language and receive relevant results in other languages, further enhancing accessibility and usability. We’re excited to share that Dropbox now supports semantic search (powered by Nautilus), adding the aforementioned capabilities. We rolled it out for Dropbox users internally in early 2024 and then externally as an experiment for a subset of Pro and Essential users in May 2024. And with this release, we observed a nearly 17% reduction in empty search sessions (measured by ZRR, or zero-results rate), and a 2% lift in search session success (measured by qCTR, or qualified click-through rate). Based on these positive results, we decided to make semantic search generally available to all Pro and Essential users in August 2024, and coming soon in early 2025 for Business users. Staying true to our AI Principles, we performed the evaluation of different embedding models in-house with pre-trained models, and we did not train on any user data. Below, we’ll introduce the key concepts behind semantic search, and walk you through our approach for selecting one essential component: the text embedding model.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images—plus generative AI capabilities across even more connected apps. See what's new →\n\nUnderstanding semantic search: A new paradigm\n\nSemantic search is designed to retrieve information based on meaning and intent, going beyond the limitations of simple keyword matching. While the terms \"semantic search\" and \"vector search\" are sometimes used interchangeably, within the context of this post we define \"semantic search\" as the entire flow—from user query input to search results output—and \"vector search\" as the specific step of retrieving items based on vector similarity. We plan to introduce the concept of semantic search for multiple file types, but in this first iteration of semantic search at Dropbox, we focused on supporting only text files. At its core, semantic search relies on vector search as the underlying technology to interpret and process unstructured data such as text, images, audio, and video. This process involves transforming content into numeric representations called embeddings, which capture the rich features of the data in a high-dimensional space. These embeddings, or vectors, are dense lists of numbers encoding features across hundreds or thousands of dimensions. During training, machine learning models refine these dimensions to capture nuanced patterns—whether visual details, document structure, or associations among words. While individual dimensions are often abstract, certain aspects may implicitly capture features like sentiment, syntax, or relationships between concepts. This dense representation enables a context-aware understanding that drives more meaningful retrieval. Vector search provides the flexibility to store different types of embeddings, for different file types, based on the specific needs of the application. By selecting or training models that capture the most relevant features for the task at hand—such as visual characteristics for images or semantic relationships for text—we can tailor the embeddings to support a wide range of content types and use cases. Building on this foundation, semantic search operates by transforming user queries into embeddings and then performing vector search to retrieve results that align with the query's intent, rather than its literal terms. The search begins when a user enters a query, which is converted into an embedding and compared against stored embeddings using vector search. Related items naturally cluster together in the vector space, even when the formats differ—whether text, images, or diagrams. Using nearest-neighbor algorithms, vector search identifies results based on meaning, ensuring that the retrieved content aligns with the user's intent. For example, a search for “guides and resources” could surface instructional PDFs, training videos, and visual diagrams—regardless of the exact labels for each item. This similarity-based retrieval approach unlocks a wide range of possibilities, from linking related documents and multimedia to uncovering insights across diverse content. Vector search enhances our interaction with information by establishing meaningful connections between items, supporting a flexible and integrated search experience. Through its ability to understand meaning and context, vector search provides the technical foundation for a search system that can adapt to varied applications, delivering more relevant and context-aware results.\n\nIdentifying the right model for our purposes\n\nCentral to semantic search—and for our use case, for text documents—is the document embedding model, which maps each document and query to the respective embeddings. Implementing semantic search at Dropbox scale would require mapping both new and existing documents to their embeddings and storing them in our indices. For a search platform as large as Dropbox, indexing more than a trillion documents and exabytes of data, could get very expensive computationally! Overall performance—in terms of both latency and quality—would hinge on the model we used. Since computing the embedding for a user’s search query would now be in the critical path for retrieving their documents, speed would be paramount. Also, a faster model would mean we could index our users’ documents faster, leading to time and cost savings down the line. At the same time, the quality of our embeddings needed to be high enough to benefit from the vector search paradigm. Whatever document embedding model we chose would need to generate embeddings that were similar enough to match documents with their relevant queries, but still discriminative enough to filter out irrelevant documents. We decided to perform a thorough evaluation of available models to find the one most suitable for our purposes, measuring each model with speed and search quality in mind.\n\nIntegrating the Massive Text Embedding Benchmark\n\nFortunately, assessing document embedding models is an undertaking that has already been embraced by the research community. The Massive Text Embedding Benchmark (MTEB) is an open-source benchmark that assesses document embedding models across eight evaluation tasks and 56 datasets, some of them multilingual. MTEB has published over 2,000 results to its leaderboard since its release in 2022. We adapted the MTEB benchmark and leveraged the reranking task to fit our needs as follows: Customizations for Dropbox infrastructure. We added adapters to enable the evaluation of models running in our in-house inference services, in addition to models executed inline. We also added adapters to allow streaming datasets that reside in our infrastructure.\n\nWe added adapters to enable the evaluation of models running in our in-house inference services, in addition to models executed inline. We also added adapters to allow streaming datasets that reside in our infrastructure. Multiple embeddings per document. MTEB by default presumes that a single embedding is generated per document, as the size of documents in the public datasets are fairly consistent. However, in production, our user’s document can range from very tiny to very large, leading to the hypothesis that we should try generating multiple embeddings per document. We implemented various nuanced strategies for separating, or chunking, a given document into individual chunks. Chunking is further configurable with a parameter for specifying overlap between consecutive chunks. We also explored summarization, in order to ensure that we respect the model’s internal threshold on input size. The resulting embeddings can then be used directly or in aggregate.\n\nMTEB by default presumes that a single embedding is generated per document, as the size of documents in the public datasets are fairly consistent. However, in production, our user’s document can range from very tiny to very large, leading to the hypothesis that we should try generating multiple embeddings per document. We implemented various nuanced strategies for separating, or chunking, a given document into individual chunks. Chunking is further configurable with a parameter for specifying overlap between consecutive chunks. We also explored summarization, in order to ensure that we respect the model’s internal threshold on input size. The resulting embeddings can then be used directly or in aggregate. Optimizations for storage and precision. To optimize storage costs, we reduced the precision (full 32-bit floating point, half float, quarter float, fixed point of various bit depth) and dimensionality (via Gaussian random projections) of MTEB's full-sized embeddings.\n\nTo optimize storage costs, we reduced the precision (full 32-bit floating point, half float, quarter float, fixed point of various bit depth) and dimensionality (via Gaussian random projections) of MTEB's full-sized embeddings. Files versus documents. Whereas the public datasets in MTEB consist of unnamed documents identified by their content, documents in Dropbox are named by our users—and filenames are quite significant in retrieval tasks. We crafted various approaches to incorporate embeddings of the filenames into MTEB when applicable.\n\nConstructing novel datasets for evaluation\n\nAmong the evaluation tasks that MTEB offers, we were most interested in re-ranking and retrieval tasks. While the performance of various models across the two tasks were readily available on the leaderboard, these two tasks primarily offered English-only datasets, which does not match the distribution of the Dropbox corpus. To address any potential distribution shift between the public datasets and the Dropbox corpus, we leveraged our existing ML-powered search platform Nautilus to construct our own MTEB-compatible datasets for more precise evaluation. We created a Kubeflow pipeline to generate a custom dataset purely for the purpose of evaluating the different embeddings models and configurations. With this pipeline, we extract anonymized query-document pairs from Dropbox search logs and put them in an MTEB-compatible format within our in-house service for hosting L0 datasets. We have strict security and access controls in place to ensure that only Dropboxers who need access to this data can access it. Moreover, as per our data retention policies, the datasets get deleted after 30 days. We also extended the pipeline to unlock multilingual evaluation. At the time of our benchmarking, the public retrieval datasets were English-only (nowadays, MIRACL dataset bridges this gap). In order to diversify our evaluation criteria beyond English, we pioneered a set of multilingual datasets (Spanish, French, German, Japanese, Korean) from Dropbox search logs.\n\nEvaluation and model selection\n\nFrom an exhaustive evaluation of 11 models—including four multilingual ones—we selected multilingual-e5-large as the top performer. This model not only excelled on our Dropbox datasets, but at the time of benchmarking also stood out as the best multilingual model on the MTEB public leaderboard across various tasks. Below are our benchmark results on our custom datasets for multilingual models, using a configuration of two embeddings per document: one for title with path, and another for the first chunk of the text content. Metrics reported are mean reciprocal rank (MRR) and mean average precision (MAP), where higher is better.\n\nModel English Japanese Spanish Korean German paraphrase-multilingual-mpnet-base-v2 MRR: 0.3299\n\nMAP: 0.3462 MRR: 0.2245 MAP: 0.2448 MRR: 0.2367 MAP: 0.2568 MRR: 0.2338 MAP: 0.2546 MRR: 0.2879 MAP: 0.3078 paraphrase-multilingual-MiniLM-L12-v2 MRR: 0.3108 MAP: 0.3278 MRR: 0.2628 MAP: 0.2804 MRR: 0.2043 MAP: 0.2273 MRR: 0.2374 MAP: 0.2584 MRR: 0.2355 MAP: 0.2533 multilingual-e5-large MRR: 0.5044 MAP: 0.5133 MRR: 0.4265 MAP: 0.4386 MRR: 0.3350 MAP: 0.3524 MRR: 0.4003 MAP: 0.4118 MRR: 0.3305 \"map\": 0.3432 multilingual-e5-base MRR: 0.4492 MAP: 0.4603 MRR: 0.3659 MAP: 0.3795 MRR: 0.3330 MAP: 0.3511 MRR: 0.3817 MAP: 0.3957 MRR: 0.3405 MAP: 0.3535\n\nWhile our evaluation primarily focused on re-ranking and retrieval tasks, we knew that other teams at Dropbox might be interested in using MTEB to evaluate additional tasks. To extend its functionality beyond vector search, we linked MTEB with the rest of our infrastructure and added additional key parameters for evaluation. This means that other Dropbox initiatives—such as Dropbox Dash, or our AI-powered file summaries and conversational features—can now leverage MTEB to evaluate various document embedding models for their applications as well.\n\nPutting the model into production\n\nIn order to make the best use of our storage and compute resources, putting multilingual-e5-large into production required some tradeoffs. Based on available storage capacity, we started by setting an upper bound of 4KB of vector-search-related metadata per document. This gave us room to adjust the number of embeddings per document, as well as the dimensionality of those embeddings and their numerical precision. In compression experiments, reducing precision to 8-bit per channel resulted in a manageable 1KB per embedding with a marginal impact on quality. However, reducing dimensionality adversely affected quality. Given that two embeddings fit our storage constraints, we opted to maintain the full dimension of the embeddings to preserve data integrity. The final quantization format we adopted is a slight variation over the standard 8-bit: we first scale the embedding so that the maximum over the magnitudes of individual channels is exactly 1.0. We store the scalar separately as a 32-bit float (4 bytes)—and the scaled embedding, which lies in [-1, 1], can be converted to 8-bit fixed point by remapping the range to 8-bit signed integer and rounding. We found that this scheme minimized the error on cosine similarity over the query-document pairs in our dataset. As for managing our compute resources, we had to consider: The maximum number of characters per document\n\nConstraints on the number of document chunks\n\nHow to balance chunk sizes to maintain contextual relevance without an overwhelming amount of processing\n\nDifferent document embedding strategies (file path and/or content embeddings) Our findings ultimately favored a dual approach: storing separate embeddings for the file path (including path and filename) and the document content up to some limit (512 tokens). While this method doesn't encompass the entire document, focusing on the initial 512 tokens significantly lowered costs and processing demands with just two embeddings per document.\n\nA more relevant Dropbox search experience", "label": "non_personal"}
{"title": "Evolving our infrastructure through the messaging system model in Dropbox", "url": "https://dropbox.tech/infrastructure/infrastructure-messaging-system-model-async-platform-evolution", "content": "The asynchronous platform at Dropbox integrates a suite of services that enable tasks and workflows to function independently without having to wait on one another. This is pretty important to our work as developers: It empowers any service within Dropbox to initiate and schedule tasks, seamlessly supporting over 400 product use cases—including Dropbox Dash and our other AI innovations—and efficiently routing more than 30 million tasks every minute. It also handles change data capture (CDC) use cases, where changes in our underlying storage system, including the FileSystem, are relayed to various product lambdas and processes. In short, it helps us ensure impactful and efficient business operations. This implementation was essential to our growth from where we were a couple of years ago. Back then, the asynchronous platform struggled with scalability and reliability, frequently falling short of the demands of our expanding product portfolio. For product engineers, the platform posed additional hurdles due to limited developer productivity tools, making it cumbersome to build and iterate on asynchronous workflows. Today’s transformation into a robust and scalable system marks a dramatic shift from those early challenges—it enables innovation at a desired pace. In this blog, we’ll introduce an open messaging system model (MSM), which played a key role in evolving our platform. It helped us build a unified event-driven system capable of orchestrating a wide range of asynchronous tasks and meeting future needs, especially as we focus on AI. Inspired by the Open Systems Interconnection (OSI) model, the MSM divides our platform into five logical layers. This standardization simplifies layers such as frontend interfaces, lambda functions, event schedulers, and event routers, allowing them to work across various use cases with different delivery guarantees and data sources, including those related to CDC. Let’s get into it.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images—plus generative AI capabilities across even more connected apps. See what's new →\n\nChallenges and limitations in our asynchronous infrastructure\n\nBeginning in 2021, our infrastructure comprised multiple asynchronous systems, each tailored to specific product or process requirements. These systems facilitated diverse functions—such as streaming events for Dropbox file uploads and edits—as well as supporting domains like security, abuse prevention, machine learning, and search indexing. Additionally, Dropbox integrated CDC functionality, enabling any modification within the underlying storage systems to generate an event, subsequently activating the async infrastructure. Despite occasional functional overlaps, these systems were developed, operated, and maintained separately, leading to inconsistencies in development speed, reliability, and operational ease. Key issues and limitations with these systems were as follows: Developer efficiency\n\nThe complexity of the current systems required product engineers to undertake a steep learning curve and assume responsibility for operational tasks such as capacity planning, release processes, and support, leading to reduced development speed and productivity. Reliability\n\nThese systems had varied service-level objectives (SLOs) for availability, latency, processing, and recovery, which resulted in inconsistent and unreliable performance. Additionally, systems were not multi-homed, and this created significant reliability risk for multiple business use cases in the event of data center failure. Operability\n\nThe variety of systems led to higher operational costs due to their complexity, requiring additional development effort for maintenance and support. The asynchronous components in our technology stack relied on a mix of external queuing solutions, such as Kafka, Redis, and Amazon SQS, creating an infrastructure that was challenging to manage and operate. System scalability\n\nAt the beginning of 2021, our system was processing over 30 billion requests daily to dispatch jobs to lambda functions. (Lambda is a serverless cloud service that runs your code automatically in response to events, without requiring you to manage any servers.) However, meeting the defined SLOs became increasingly challenging. Certain critical components, such as the delayed event scheduler, had already maxed out their throughput capacity. Consequently, we had to implement rigorous screening protocols for each new use case before onboarding in order to ensure it adhered to the system's capacity limitations and wouldn't jeopardize its performance. Lambda infrastructure\n\nThe lambda-based architecture utilized on the consumer side was complex and diverged from the Dropbox service-oriented architecture (SOA) guidelines and established best practices. Consequently, diagnosing and investigating issues on the consumption side became highly challenging, as it didn't integrate seamlessly with the Dropbox infrastructure and recommended methodologies. This lack of alignment resulted in several adverse effects, notably: Release consistency : The release procedures across these systems lacked uniformity and robust safety measures, introducing deployment and update risks.\n\nThe release procedures across these systems lacked uniformity and robust safety measures, introducing deployment and update risks. Compute efficiency : The compute clusters supporting these systems operated below peak efficiency, resulting in suboptimal resource utilization.\n\nThe compute clusters supporting these systems operated below peak efficiency, resulting in suboptimal resource utilization. No autoscaling: The absence of autoscaling for lambda infrastructure, stemming from its deviation from the Dropbox SOA guidelines, resulted in poor integration with our autoscaling infrastructure. As a result, there was a reliance on customer or platform-owner intervention to manually augment capacity when the base capacity proved inadequate to manage the workload. Extensibility\n\nExtensibility posed a significant challenge for these systems, characterized by a deficiency in flexibility and scalability to adapt to emerging product demands. The current solutions were ill-equipped to seamlessly integrate new workflows, and any attempts to expand them would introduce unnecessary complexities in implementation. With the introduction of Cypress, our new filesystem architecture, the existing system faced limitations in expanding our CDC pipeline to distribute Cypress events to multiple subscribers within Dropbox. In all, these challenges underscored the need for a more unified and consistent approach to our asynchronous infrastructure, emphasizing the importance of addressing developer velocity, reliability, operability, efficiency, and extensibility to better support the company's evolving product landscape.\n\nRethinking our approach\n\nThe existing async systems already supported over 400 business use cases. The large number of existing use cases meant we didn’t have the flexibility to construct an entirely new system from scratch, as the migration would have been very time consuming. Instead, we decided to adopt a phased approach, with incremental steps to rebuild existing systems that mitigate risks associated with migrating existing production flows to a new infrastructure. Returning to the drawing board, we outlined three primary goals for the new platform, envisioning a gradual and incremental build-up of capabilities: Development velocity Simplify the asynchronous interface to streamline platform adoption for product engineers. This allows them to focus on creating innovative product features rather than investing time in understanding the complex asynchronous landscape and determining the most suitable system for their use case.\n\nDecrease the operational burden on product engineers by implementing release practices that identify code regressions during deployment and automatically initiate rollbacks if a new release breaches predefined thresholds.\n\nEnable automatic compute scaling when a lambda function encounters a backlog of events to process, ensuring that the current base capacity is augmented if deemed insufficient. Robust and extensible async foundation Unify common elements and patterns across existing async systems within Dropbox and simplify the interface.\n\nSupport new use cases with minimal modifications and avoid the need to build entire new systems by providing extensible components and flexible APIs. Cost and operational efficiency Streamline the foundational infrastructure by phasing out redundant systems (where applicable) and cut down on operational costs.\n\nTransition lambda infrastructure to the Dropbox SOA stack to increase compute efficiency and enable functionalities such as autoscaling, multihoming, and improved out-of-the-box monitoring capabilities. The overarching key performance indicator (KPI) that we aimed to improve over time was the \"time to launch\" for product engineers to deploy a new use case into production. As platform owners, our primary KPI of interest was the \"oncall time\" expended on a weekly basis.\n\nThe five layers of the messaging system model\n\nThe initial step in the refinement of the async system involved deconstructing it into its fundamental layers. We undertook this process to achieve the aforementioned objectives. Subsequently, a systematic approach was devised, beginning with the dissection of the async system into its core elements, followed by the formulation of a bottom-up strategy for its progressive enhancement. From a macroscopic standpoint, the asynchronous system can be mapped to an MSM consisting of three primary layers, analogous to the seven layers of the OSI model in network transmission frameworks. These three primary layers are: Customer layer: This component, also known as the “frontend layer,” encompasses the various pathways through which users interact and interface with the async system. It encapsulates the mechanisms by which users communicate with and integrate into the async environment.\n\nThis component, also known as the “frontend layer,” encompasses the various pathways through which users interact and interface with the async system. It encapsulates the mechanisms by which users communicate with and integrate into the async environment. Orchestration layer: This layer is intrinsic to the async system and encompasses the entirety of the tasks required for the scheduling and transmission of async operations to the compute layer (also known as the “execution layer”). It serves as the intermediary stage between the customer layer and the compute layer, and it’s responsible for ensuring that various components and services interact seamlessly to fulfill complex workflows and business logic requirements.\n\nThis layer is intrinsic to the async system and encompasses the entirety of the tasks required for the scheduling and transmission of async operations to the compute layer (also known as the “execution layer”). It serves as the intermediary stage between the customer layer and the compute layer, and it’s responsible for ensuring that various components and services interact seamlessly to fulfill complex workflows and business logic requirements. Compute layer: This layer is the execution hub of the async system, where the actual processing and execution of async tasks take place. It is responsible for the seamless execution of asynchronous operations, thereby ensuring the efficient functioning of the system as a whole.\n\nA 10,000-foot view of the async system\n\nThe three layers mentioned above can then be further broken down into five, more specific layers—frontend, scheduler, flow control, delivery, and execution—with each new layer serving an important role within the above three buckets. (Some overlap occurs between the customer and orchestration layers). These five layers of the MSM are illustrated in the diagram below.\n\nAn illustration of the five components of the Messaging System Model (MSM)\n\nNow, let's take a closer look at each of these five layers.\n\nFrontend In the architecture of an asynchronous system, the frontend layer assumes the critical role of serving as the primary interface for user interaction with the system. It represents the user-facing aspect of the asynchronous environment, orchestrating seamless communication and integration with the system's core functionalities. Users are categorized into two distinct groups: first, there are the regular product engineers who utilize programmatic methods to invoke a publish remote procedure call (RPC) and enqueue events, destined to be consumed by one or more subscribers. The second category encompasses systems such as databases or event sources, which necessitate the enqueuing of changes to diverse objects, entities, or files, thereby propelling both internal and external business workflows forward. A pivotal responsibility of the frontend layer is the management of the schema registry and the rigorous validation of every event schema traversing the system. This stringent schema validation process ensures that published events conform to the predefined contract established with subscribers. Additionally, the frontend layer is tasked with the intricate conversion of disparate message formats, including JSON, Proto, and Avro, among others, into a standardized message format—typically protocol buffers—compatible with the internal asynchronous implementation. Furthermore, the frontend component is entrusted with guaranteeing the durability of all events published to the asynchronous system, thereby safeguarding the integrity and reliability of the system's data flow. Scheduler The scheduler is the core engine within an async system and plays a crucial role in coordinating and dispatching disparate events for various consumers that subscribe to these events. This layer plays various roles. For example, for a CDC use case, this will call external data source APIs to get relevant range for the payloads that will be delivered to the subscribers. For a use case where events need delayed execution, the scheduler would store these events separately so they can be trigger at desired timestamp with a process keeping tabs on these events and publishing them to subscribers at those desired scheduled timestamps. Scheduler also has the responsibility to maintain the order of execution of the events and ensures task delivery to subscribers based on this order. Flow control Flow control plays a pivotal role in the orchestration layer, managing the distribution of tasks to subscribers based on several factors, such as subscriber availability, task priority, and potential throttling events. For instance, in a CDC scenario, the orchestration layer dynamically adjusts the rate of queries dispatched to subscribers. This adaptation occurs when the orchestration layer detects that a subscriber is unable to handle the job throughput effectively or when the source, backing CDC, signals the scheduler client to reduce the pace. State management, another function of this layer, encompasses the maintenance of data structures responsible for tracking ongoing events and their respective statuses (such as pending, running, or complete). Additionally, it incorporates mechanisms to retry tasks in case of transient failures, ensuring robustness and reliability in task execution. Delivery The execution layer of the messaging system model can be broken down into two main parts. The first is the delivery layer, which is the process of directing the event to the right place or service. The second, the event execution, we’ll get to in a bit. Routing is the final layer in an asynchronous system, responsible for directing the message out of the system and into the domain where a designated process or lambda function will handle the event. This process or lambda function may be hosted within the same virtual private cloud (VPC) as the messaging infrastructure or may be a part of public clouds like AWS, Azure, etc. In a push-based model, the routing layer is one of the most critical components, similar to the “last mile delivery” in an e-commerce delivery system. Routing enables many critical functions, including: Message filtering based on subscriber preferences\n\nDelivery retries for transient failures\n\nContinuously monitoring the health of a subscriber’s event execution hosts, and then routing events only to those that are healthy\n\nDispatching event execution status to the orchestration layer for state machine management\n\nEvent delivery concurrency management Execution The event execution is the second layer of the primary compute bucket. It’s when the actual task happens, and it’s usually done by a lambda function (i.e., serverless code), or a remote process—potentially even another system or service—that handles the event. In short, the compute layer involves first routing the event and then actually processing it. Lambda infrastructure refers to the underlying framework responsible for executing events. When an event is triggered, a process is initiated within this infrastructure, which subsequently returns either a success or retriable failure status post-execution. If no status is returned, or if an error occurs, the default assumption is a retriable failure. In this interaction, the router acts as the client, operating under a push model. Ideally, the executing process operates across multiple cloud environments to enhance reliability. The router has the capability to push events to various clouds based on the locality preference configured by the lambda/process owner. For example, some users may opt to configure their processes to be active in specific clouds to ensure proximity to backend storage dependencies, thereby minimizing cross-data center latency. Lambda infrastructure should also include autoscaling as part of its features. At Dropbox, our lambda infrastructure is backed by Atlas, which offers autoscaling capabilities. Additionally, Atlas supports release-time hooks, enabling validation and rollback of code changes if they would potentially degrade service uptime or impact any features negatively.\n\nConclusion", "label": "non_personal"}
{"title": "Implementing end-to-end encryption for Dropbox teams", "url": "https://dropbox.tech/security/end-to-end-encryption-for-dropbox-teams", "content": "People trust Dropbox to keep their most important content secure. As more teams embrace remote and distributed work, ensuring the privacy and security of their data has never been more important. While customers already appreciate our simple, seamless access controls, those who work with more sensitive information have told us they want even more control over how their data is secured. One of the ways that Dropbox is meeting the needs of these customers is with the introduction of zero-knowledge, end-to-end encryption for team folders. While Dropbox already encrypts files at rest using 256-bit AES, customers are seeking end-to-end encryption where only they possess the decryption key, so not even Dropbox can access the contents of their files. For customers with especially sensitive or confidential data—for example, those working in finance or healthcare—end-to-end encryption offers an additional level of security. When enabled, files are encrypted directly on the customer’s device before being uploaded to our servers. Here we’ll discuss our implementation of end-to-end encryption for teams, the threat model of our design and encryption algorithms, and our commitment to minimizing the risk of data loss with a team-centric key management approach.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images—plus generative AI capabilities across even more connected apps. See what's new →\n\nBalancing security and usability\n\nOur commitment to simplicity and reliability is at the heart of our encryption design. In our view, a secure system must also be user-friendly. A security system serves no purpose if it’s too complicated to use. For a feature like end-to-end encryption, with its added layer of complexity, striking the right balance between security and usability is key. Our aim was to make this technology accessible without compromising security. At its core, our implementation of end-to-end encryption is designed so that neither Dropbox, unauthorized users, or malicious third parties can access a team’s encrypted files. Only the team holds the keys. Even if an attacker gains access to those keys, our implementation still ensures the confidentiality of new files or modifications, as long as the team’s keys have been rotated. Encryption also assures that a file has not been tampered with. In other words, if a file decrypts successfully, it is cryptographically guaranteed to be the exact same content as encrypted in the first place.\n\nAt the same time, because zero-knowledge encryption means customers manage their own keys, they also risk losing access to their data if those keys are lost. To address this, we’ve developed a key management system designed specifically for teams. It ensures that even if one member loses their keys, the data remains accessible and secure for the rest of the team.\n\nTeam-centric key management\n\nKey management in many end-to-end encryption systems has traditionally focused on individual users, mainly because they were the first to adopt and use these systems. In those systems, each user is responsible for managing their own set of keys and making sure they're always accessible. However, this can create complications that diminish the user experience and may even lead to data loss if keys are misplaced. To counteract potential data loss, some systems use a method called key escrow, which allows for data recovery by a trusted third party, e.g. a spouse or an administrator. But this adds complexity, both in terms of the coding required and in using the product itself. By focusing on our teams customers and drawing the cryptographic boundary around teams, we were able to re-think how the key management is done. With our approach, users don’t have any keys, but every team has a central team key. This key is accessible to all team members and controls access to the team’s encrypted data, providing protection against unauthorized third parties. The team-centric approach offers the following benefits: Reduced risk of data loss and implicit key escrow. By sharing the team key among all members, any member with access—such as a team admin with a recovery key or a member with a registered device—can restore access for everyone.\n\nBy sharing the team key among all members, any member with access—such as a team admin with a recovery key or a member with a registered device—can restore access for everyone. Reduced user responsibility. The burden of managing cryptographic keys shifts from individuals to the team, reducing the risk of a single person causing data loss.\n\nThe burden of managing cryptographic keys shifts from individuals to the team, reducing the risk of a single person causing data loss. Reduced complexity and improved user experience. The absence of user keys as well as an explicit key escrow significantly simplifies the implementation and improves the user experience. Team members can simply use end-to-end encryption without having to worry about keys at all. To preserve data confidentiality when team members change, admins can rotate keys for the entire team. Rotating keys upon the departure of a member ensures that any potentially leaked keys become obsolete for accessing new or modified encrypted data. This mechanism is critical in a scenario where a former member, now considered an untrusted outsider, attempts to misuse a previously acquired key. By instituting a new team key for encrypting subsequent data, the system effectively safeguards the confidentiality of new files or modifications made after key rotation, thereby aligning with the threat model's emphasis on protecting data integrity against insider threats turned external.\n\nAutomatic and manual device registration\n\nBefore a user can use end-to-end encryption on a new device, the required keys must first be made available. We offer admins a choice of two device registration modes: automatic device registration and manual device registration. Automatic device registration balances security with usability by distributing keys from our system to authorized team members through the Dropbox authentication and access control infrastructure—for example, when logging into a new device. Existing devices automatically authorize new devices by wrapping the team key with the new device's public key. If there are no devices available to do this, a team admin can use a recovery key to facilitate the new device's registration. The device then obtains and uses its version of the team key, ensuring quick and smooth setup without manual input. If a customer prefers more fine-grained control, they can opt for manual device registration. This process lets team admins personally approve new devices before they can access encrypted files. Team admins and members can check key authenticity by comparing the fingerprints, or security codes, of the device and team keys out-of-band. Only keys verified to belong to the correct devices and team will be used, ensuring that only legitimate team devices can access encrypted files. This process adds an additional safeguard against unauthorized access and man-in-the-middle attacks as admins can ensure that a key really belongs to a person or team, and not a malicious actor. Despite its security benefits, key verification can be cumbersome and impact usability, often leading to its limited real-world use—so we've made it an optional feature for those who need greater security.\n\nWhat end-to-end encryption doesn’t cover\n\nIt’s important to point out there are also some threats that fall beyond the scope of our implementation: Device security. Though end-to-end encryption keeps data safe during transmission and while stored on our servers, it doesn't address security at the device level. Since encrypted files decrypt automatically for access during sync or download, we still recommend customers adopt best practices such as full-disk encryption and secure access methods to protect their devices.\n\nThough end-to-end encryption keeps data safe during transmission and while stored on our servers, it doesn't address security at the device level. Since encrypted files decrypt automatically for access during sync or download, we still recommend customers adopt best practices such as full-disk encryption and secure access methods to protect their devices. Metadata visibility. Our encryption efforts concentrate on file contents rather than metadata. With this approach, customers can still search their Dropbox account based on metadata such as file name, file type, and creation date, ensuring end-to-end encryption is still practical in everyday use.\n\nOur encryption efforts concentrate on file contents rather than metadata. With this approach, customers can still search their Dropbox account based on metadata such as file name, file type, and creation date, ensuring end-to-end encryption is still practical in everyday use. Insider threats. Our implementation safeguards against external threats to a team but doesn't change internal permissions. Teams should continue using existing access controls to manage data access amongst members, ensuring sensitive information remains compartmentalized and secure.\n\nA closer look at our encryption techniques\n\nOur implementation uses a hybrid scheme, combining a symmetric algorithm for encrypting file content with an asymmetric algorithm for securing the keys. We aim for a balance of proven security, performance, and broad platform support in our choice of encryption algorithms. Symmetric file encryption\n\nPlaintext content is split into 4 MB blocks, where each block is authenticated using AES-256 encrypted in Galois/Counter Mode (GCM) with a random and unique 96-bit nonce. While AES-GCM guarantees authenticity and integrity for each block, the 128-bit authentication tags of all blocks are cryptographically hashed using HMAC-SHA-256 to expand these guarantees to the entirety of the file. This method supports partial encryption and decryption, offering seamless security without compromising the file's integrity or order. This method is especially effective for large files, as it aligns with our practice of chunking file content into 4 MB blocks for storage. It also avoids the limitations of in-memory processing required by some APIs, like WebCrypto. Asymmetric key wrapping\n\nTo encrypt secret keys, our approach to key management uses Hybrid Public Key Encryption (HPKE), a modern and flexible standard that combines asymmetric and symmetric encryption in a hybrid crypto system. We use HPKE in single shot, base mode using Elliptic-Curve Cryptography (ECC) with the P-256 curve, SHA-256, and AES-256-GCM (DHKEM(P-256, HKDF-SHA256), HKDF-SHA256, AES-256-GCM). When manual device registration is chosen, HPKE is used in auth mode to encrypt parts of the key chain with sender authentication required for effective key verification. NIST P-256 has been chosen over other curves like Curve25519 because it is widely adopted in the industry, is available in most cryptographic libraries (e.g. WebCrypto, CryptoKit, OpenSSL), and is specified in FIPS 186-4. Post-quantum cryptography\n\nThe algorithms mentioned above do not include any post-quantum cryptography (PQC). While there exist some products with early implementations of PQC, we're taking a more cautious approach, relying on proven and time-tested encryption algorithms for several reasons: PQC's reliability for long-term storage is still uncertain due to ongoing standardization efforts. For instance, the Kyber algorithm has seen several revisions throughout its NIST standardization process.\n\nPQC is relatively new in cryptographic terms and lacks the extensive scrutiny that more established algorithms have undergone. To counteract this, some PQC applications use a hybrid model, where traditional cryptography is also used. This ensures baseline security should the PQC component be compromised at the expense of greater complexity.\n\nPQC algorithms are not yet sufficiently included in common cryptographic libraries, requiring custom implementations across some codebases and increasing the risk of vulnerabilities, bugs, and other human error.\n\nThe threat posed by quantum computing—while significant—is still theoretical, with its practical impacts still unknown. Given these considerations, we’ve maintained flexibility around our ability to change our encryption protocols, while staying focused on trusted, well-known cryptographic implementations. This will enable us to integrate new encryption algorithms to our protocol at any time in the future. We are closely monitoring the development of Kyber and other PQC algorithms and will adapt our choice of encryption algorithms as they mature and standards evolve further.\n\nSecuring the future", "label": "non_personal"}
{"title": "Building Dash: How RAG and AI agents help us meet the needs of businesses", "url": "https://dropbox.tech/machine-learning/building-dash-rag-multi-step-ai-agents-business-users", "content": "Knowledge workers today face myriad challenges in managing their digital workflows. Information is often scattered across multiple applications and formats, and finding the right document, message, or piece of information can be both tedious and time-consuming. This fragmentation creates two major problems for businesses: it hinders collaboration and productivity, and it can lead to costly security issues. To address these challenges, we launched Dropbox Dash, a universal search and knowledge management product that combines AI-powered features with in-depth content access control. Designed to help knowledge workers organize their digital lives, Dash allows users to find, organize, share, and secure content across their apps so they can focus on the work that matters most. At its core, Dash is a universal search product powered by many machine learning technologies and supercharged by generative AI. It offers a powerful AI-driven search experience with advanced filtering capabilities that allow users to quickly locate the information they need, regardless of where it’s stored. With granular access controls, Dash also makes sure employees and external partners see only the right content so that sensitive company information isn’t surfaced unintentionally. And with advanced AI features, Dash can summarize, answer questions, surface insights, and generate drafts. Throughout our development process, we experimented extensively and explored numerous solutions to build an AI product for businesses. In order to meet the challenges of modern work in data-intensive environments, we ultimately turned to retrieval-augmented generation (RAG) and AI agents. Additionally, we engineered a minimal Python interpreter focused exclusively on essential features required by our AI agents and supported by extensive testing and security reviews to ensure safe code execution. In the following sections, we’ll dive into the specific challenges we faced while building Dash, the innovative solutions we developed to address them, and important lessons that’ll inform our work moving forward.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images—plus generative AI capabilities across even more connected apps. See what's new →\n\nChallenges in making an AI product that’s ready for businesses Building an AI product like Dash presents a unique set of challenges that differ from those that developers typically encounter with consumer-facing applications. These challenges stem from the inherent complexities of business data environments, which are characterized by diversity, fragmentation, and multiple data modalities. Understanding and addressing these challenges is crucial for delivering effective AI solutions that meet the sophisticated needs of business users. Before we dive into how we solved ‌these challenges, let’s first take a look at what each of these data environments entails. Data diversity Data diversity refers to the wide range of data types that a business handles, including emails, documents, meeting notes, task management data, and more. Each type of data has its own structure and context, and that can complicate AI processing.\n\nExample of identifying the right data source, which requires domain knowledge and contextual information\n\nEffectively managing diverse data types is critical because each type of data has its own structure and context. For Dash to perform well in a business setting, it must seamlessly process and understand all these different data types. Data fragmentation Data fragmentation occurs when an organization’s data is spread across multiple applications. This means that relevant information isn’t stored in a single location, but is instead scattered across different tools and services. Data fragmentation complicates the process of retrieving and synthesizing information. For users, this means context switching between multiple apps to manually search for the information they need, which is time-consuming, tedious, and inefficient. An AI system that can aggregate and make sense of fragmented data would greatly enhance the user experience by providing a unified and accessible information repository.\n\nExample of information spread across multiple apps, which requires combining fragmented information to construct a complete answer\n\nData modalities Data modalities refer to the different forms or modes in which data exists. Common modalities include text, images, audio, and video. Handling multiple data modalities is essential for providing a comprehensive AI solution. Business users often deal with a mix of text documents, images, presentations, and videos, among other formats. An AI system that can process and integrate all these modalities can provide a more complete and accurate response to user queries.\n\nExample of information spread across multiple modalities\n\nIn summary, these challenges—data diversity, data fragmentation, and data modalities—present unique complexities when building these types of AI products. Addressing these challenges is essential to creating a robust and effective AI solution that meets the various needs of knowledge workers in dynamic and data-intensive environments. To pull this off, we implemented and experimented with multiple solutions. That’s where retrieval-augmented generation (RAG) and AI agents come in.\n\nLeveraging retrieval-augmented generation When building Dash, we knew that delivering accurate and relevant responses to user queries was paramount. That’s why we turned to RAG, an industry-standard approach for tasks like query responses and summarization. RAG's ability to combine external information retrieval with state-of-the-art generative models makes it the perfect fit for our product, especially in the complex landscape of enterprise data. RAG works by first retrieving the most relevant pieces of content from a dataset or knowledge base and then using a large language model (LLM) to generate a response based on that content. This approach ensures that the answers that the AI system provides aren’t only contextually relevant but also up to date, which is crucial in business environments where data is constantly evolving.\n\nRetrieval-augmented generation (RAG)\n\nChoosing the right retrieval system The retrieval system is the backbone of any RAG pipeline. It determines not just speed but also whether the LLM has the right context to generate meaningful answers—and a misstep here can compromise the entire user experience. Put another way, the retrieval system sets the bounds of what your LLM can “know” at inference time. It also greatly affects latency, which in turn impacts user satisfaction. And, it shapes the perceived quality of the final answer, since retrieval coverage can make or break correctness and completeness. There are many options when it comes to designing a retrieval system. For question-answering systems, the most common one is to have a vector index where chunked data is indexed on their embeddings. Embeddings are simply a low-dimensional semantic representation of the chunk of data. To retrieve from such an index, semantic search is often used. Another choice is to go with a more traditional approach to a search index, where documents are indexed by their lexical features (e.g., words that appear in the title or body). This approach, however, adds extra latency due to the need for on-the-fly chunking and re-ranking of the chunks during serving time. There are many other options that prioritize data freshness, such as by directly interacting with the API for the platform where your data is stored, for example. But there are a few trade-offs to consider for each of these approaches: Latency vs. quality: There’s a pervasive assumption that you can’t have both extremely low latency and high-quality results. Why? Because advanced or larger embedding-based semantic searches may take longer due to their complexity—whether that’s a heavier model or additional reranking steps. For example, if you want more than 95% of your requests to reliably complete in under 1–2 seconds, you might have to use smaller embedding models, which can reduce retrieval accuracy.\n\nThere’s a pervasive assumption that you can’t have both extremely low latency and high-quality results. Why? Because advanced or larger embedding-based semantic searches may take longer due to their complexity—whether that’s a heavier model or additional reranking steps. For example, if you want more than 95% of your requests to reliably complete in under 1–2 seconds, you might have to use smaller embedding models, which can reduce retrieval accuracy. Data freshness vs. scalability: Many projects need to keep their data fresh—for instance, re-indexing a news site every few minutes. Frequent re-indexing processes can hinder system throughput or spike latency when they’re underway. Alternatively, on-the-fly API calls to third-party data can push latency well above a few seconds. If near-real-time information is crucial (e.g., updating stock quotes), your system might spend more resources on frequent indexing or caching, throttling your ability to scale.\n\nMany projects need to keep their data fresh—for instance, re-indexing a news site every few minutes. Frequent re-indexing processes can hinder system throughput or spike latency when they’re underway. Alternatively, on-the-fly API calls to third-party data can push latency well above a few seconds. If near-real-time information is crucial (e.g., updating stock quotes), your system might spend more resources on frequent indexing or caching, throttling your ability to scale. Budget vs. user experience: High-quality solutions—advanced embeddings, re-ranking steps, and large-chunked indexes—often require additional compute, and more compute means more cost. If the user experience demands near-instant results with best-in-class recall, the resource burn can be significant. And if budgets are constrained, you might be forced to choose a simpler retrieval pipeline that could degrade the overall quality. For Dash use cases, we prioritized reasonable latency but also high-quality and reasonable data freshness, with both periodic data syncs and the implementation of webhooks whenever appropriate. Specifically, we stayed under 1–2 seconds for over 95% of our queries, which allows us some latency budget for the rest of the pipeline so that our users don’t click away because the response time is too long. Ultimately, we landed on a traditional information retrieval (IR) approach combined with on-the-fly chunking and reranking: Traditional IR: We use a lexical-based system, along with smarter rerankers that use embedding features.\n\nWe use a lexical-based system, along with smarter rerankers that use embedding features. On-the-fly chunking: Documents are chunked at query time to ensure we’re pulling only the relevant sections.\n\nDocuments are chunked at query time to ensure we’re pulling only the relevant sections. Reranking: A larger, but still efficient, embedding model then re-sorts those results to place the most relevant chunks at the top. In practice, this yields high-quality results in under 2 seconds for over 95% of our queries, balancing speed and relevance. The combination allows us to keep costs in check while avoiding the pitfalls of purely semantic or purely lexical retrieval. Quality is best when measured end-to-end because all the parts of the RAG system need to work together effectively. Once we chose the retrieval system that best fit our needs, it was time to pick the best LLM for the job.\n\nChoosing the right model To ensure this approach met our requirements, we conducted a rigorous evaluation. We tested multiple retrieval methods and model variations on several public datasets, including Google’s Natural Questions (featuring real user queries with large documents); MuSiQue (with multi-hop questions requiring information linking across different passages); and Microsoft’s Machine Reading Comprehension (containing often short passages and multi-document queries from Bing logs). We also designed hand-tuned metrics to help evaluate the quality of generated answers. These included an LLM judge for answer correctness (passing retrieved evidence through an LLM to score final answer accuracy), an LLM judge for completeness (measuring the extent to which all relevant question aspects are addressed), as well as source precision, recall, and F1 metrics to evaluate how accurately we retrieved key passages needed for correct answers. By cross-referencing these metrics, we could directly compare multiple open-source and closed-source LLMs in a consistent environment. This led us to narrow down a few model families that best suited Dash’s use cases. Our RAG system remains model agnostic: We want to provide the flexibility of choosing the models and providers our customers are most comfortable with. Being model agnostic also allows us to be prepared to adapt to rapid developments in the field of LLMs. Although RAG provides a solution for the most common types of questions—kinds that require fetching information from one or more documents—it’s incapable of performing complex, multi-step tasks. This is where AI agents come in.\n\nThe role of AI agents Imagine you’ve asked a colleague to help you with a complex task, such as, “What’s the progress on projects in my team’s Q1 OKRs?” This person would likely find the answer to this question by first breaking it down into individual steps before tackling those steps one at a time. To handle the business challenges outlined above, we need an AI system that can approach complex tasks like humans do. These tasks may require domain knowledge, contextual information, and planning and executing multiple steps—and AI agents are exceptional at doing just that. The term \"AI agent\" is often used loosely across the tech industry, and with various interpretations. However, there’s a common theme among all of them: an AI agent is a system that can autonomously perform tasks with very little to no human interaction. At Dropbox, our interpretation of AI agents is more specific and aligned with the needs of business applications. We view AI agents as multi-step orchestration systems that can dynamically break down user queries into individual steps, execute those steps using available resources and information from the current user, and generate a final response—all while requiring minimal human oversight.\n\nAgents as multi-step orchestration\n\nThe multi-step orchestration in our AI agents includes two stages: planning and execution. Stage 1: Planning The planning stage involves breaking down a user's query into a sequence of high-level steps. This is done by an LLM, which interprets the query and generates simple code statements to express the logic of responding to the user’s query. The LLM-generated code is written in our domain-specific language (DSL), which is similar to the Python programming language. The initial plan of responding to the user’s query is restricted to high-level or simple code statements, which ensures clarity and precision in defining each step. For example, let’s explore the request, \"Show me the notes for tomorrow’s all-hands meeting.\" These steps contain the logic necessary to respond to the query: Resolve concrete dates and times for the phrase “tomorrow.” There needs to be an established time window to identify what “tomorrow” is referring to. This must be done relative to the current date and time. Identify the meeting. There needs to be a search conducted for a meeting with a title matching \"all-hands\" (and within the determined time window). Retrieve notes. Documents attached to or linked from the identified meeting must be fetched. The AI agent, however, expresses this logic as statements of code in our Python-like DSL. Below is a simplified version of what that’d look like:\n\nCopy time_window: TimeWindow = time_helper.get_time_window_for_tomorrow() meeting: Meeting = meetings_helper.find_meeting(title=\"all-hands\", time_window=time_window) notes: list[Document] = meetings_helper.get_attached_documents(meeting=meeting)\n\nEach XXXX_helper object in the generated code contains functionality that acts as a building block for the LLM to use when expressing the logic of responding to the user’s query.\n\nStage 2: Execution The next step is to validate and execute the logic that was expressed as code. The code is validated through static analysis to ensure correctness, safety, and to detect missing functionality. We intentionally allow the LLM to assume that missing functionality exists. If missing functionality is identified, we use the LLM a second time to implement the missing code. This two-stage approach to generating code allows the agents to be clear and focused with an overall plan, while also being adaptable to new types and variations of user queries. Below is a simplified version of what the result of each of the steps might look like: 1. Time window retrieval: Resolve the relative phrase “tomorrow” to concrete values.\n\nCopy time_window: TimeWindow = time_helper.get_time_window_for_tomorrow() # TimeWindow(start=\"2025-03-19\", end=\"2025-03-20\")\n\n2. Meeting identification: Search for the \"all-hands\" meeting within the resolved time window.\n\nCopy meeting: Meeting = meetings_helper.find_meeting(title=\"all-hands\", time_window=time_window) # Meeting(title=\"Company All-Hands\", start_time=..., attendees=...)\n\n3. Document retrieval: Finally, fetch the notes attached to the identified meeting.\n\nCopy notes: list[Document] = meetings_helper.get_attached_documents(meeting=meeting) # [Document(title=\"All-Hands-Notes\", content=\"...\")]\n\nThe final response to the user’s query is the result of the last step. In this example, the list of documents will be returned to the user. Validation and testing The interpreter we use to execute the LLM-generated code was developed from scratch here at Dropbox. This gave us full control over everything inside the interpreter, including integrating static analysis passes and “dry runs,” in addition to having run-time type enforcement. Static analysis allows our interpreter to examine the code without executing it, helping us automatically identify potential security risks, missing functionality, or code correctness errors. Having run-time type enforcement helps ensure that the data and objects being operated on are the types of values that we expect. In our example, the list of documents returned to the user will always be a list of documents. Normally, testing LLM integrations can be an ever-moving target. As new model versions are released, slight changes in how things are phrased or reacted to can be expected. Knowing exactly why a test failed or why the final response differed from expectations is often challenging. However, as a result of the LLM using code to express its logic in responding to the user, we’re able to make the LLM “show the work.” This helps with understanding at which step the logic failed, having more deterministic testing, and evaluating the response to a query. For example: Logic failure: “Can’t answer this question” vs. “Error on step 3 when fetching attached documents to meeting…”\n\n“Can’t answer this question” vs. “Error on step 3 when fetching attached documents to meeting…” More deterministic testing : Does resolving “tomorrow” always return the correct time window?\n\n: Does resolving “tomorrow” always return the correct time window? Evaluating responses: “Does the response text have the approximate same meaning as what we expected?” vs. “Does the response value match the expected type list[Document] ?”\n\nSecurity and efficiency To address security concerns, we implemented security controls in our interpreter and its development process. Only the minimal required functionality is implemented in its runtime—feature parity with CPython isn’t the goal. This turns major security risks that exist in other full-featured interpreters into non-issues. As we’ve explored, AI agents play a pivotal role in addressing the complexities of business tasks through their ability to plan and execute multi-step workflows autonomously. By leveraging LLMs and DSLs, these agents break down intricate queries into actionable steps, ensuring precision and efficiency. The structured approach, combined with strong typing and built-in security controls, enhances reliability and mitigates security risks. The future of AI agents in business environments is promising. And as we continue to refine and expand their capabilities, they’ll become indispensable in streamlining operations, enhancing productivity, and driving innovation.", "label": "non_personal"}
{"title": "How we brought multimedia search to Dropbox Dash", "url": "https://dropbox.tech/infrastructure/multimedia-search-dropbox-dash-evolution", "content": "Knowledge workers routinely lose valuable time trying to find that thing—the right images, videos, documents, or audio files—across their dozens of apps and essential work tools. When we started building Dropbox Dash, our universal search and knowledge management product, we knew it had to do more than just speed up search. It also needed to scale beyond text. Because often, the challenge isn’t just finding a file—it’s finding what’s inside that file. And that gets tricky when things aren’t labeled clearly, your team’s folder structure breaks down, or you just can’t remember where you saved what you need. Searching for multimedia content poses unique challenges. Images, for example, often come with cryptic names like IMG_6798 by default, and teams can quickly accumulate thousands of these unlabeled assets. Unlike documents, which usually contain metadata or readable content to help with discovery, media files frequently lack that context and require manual review. On top of that, they demand heavier compute resources and smarter ranking systems to deliver relevant results at speed. Supporting fast, accurate media search in Dash wasn’t a matter of layering features on top—it required fundamental changes across our infrastructure. We had to rethink how we indexed and ranked non-text files, how we rendered visual previews, and how we hydrated and surfaced metadata. We also had to reevaluate traditional document-search assumptions about relevance, latency, and even UI presentation. Our multimedia retrieval features were built to solve these exact problems, allowing users to find images, video, and audio just as easily as they find documents. What follows is a behind-the-scenes look at the engineering that made this possible: what we built, what we learned, and how we delivered a system that makes media as searchable as text.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images—plus generative AI capabilities across even more connected apps. See what's new →\n\nChallenges in supporting multimedia search\n\nSupporting search for multimedia—images, video, and audio—introduces a distinct set of technical hurdles. These files require significantly more processing power, have fewer textual cues for ranking, and often lack meaningful metadata. Delivering fast, relevant results means handling large file sizes efficiently, identifying new relevance signals, and optimizing how results are rendered for user review. That’s why our universal search solution had to support seamless browsing, filtering, and previewing of media—right inside Dash. Scaling search for this content meant facing higher storage and compute costs, tighter latency requirements, and adapting systems originally built for text-based retrieval. To understand what makes media search tricky, let’s break down some key considerations. Storage cost\n\nMedia files are significantly larger than typical documents. On average, image files are about 3X larger, and video files are roughly 13X larger than non-media files in our system. These size differences directly increase storage demands and costs. Compute cost\n\nMultimedia files, such as images and videos, require more intensive processing to extract features both due to their larger size and the complexity of the features. Unlike text documents, we also generate previews of different resolutions for the images and videos, thereby significantly increasing the compute demands in our system. Relevance\n\nDash operates a multi-phase retrieval and ranking scheme, which was previously trained and optimized for textual content. Retrieving and ranking multimedia content requires having indexed any new multimedia-specific signals, formulating a query plan that leverages these signals, and handling any corner cases to avoid poorly ranked results. Responsiveness\n\nServing multimedia content introduces new latency challenges that are not present with text-based documents. We need previews for the multimedia search results to be meaningful, and we need them in multiple resolutions, including high-res formats, for a rich product experience. The larger resolutions add to the storage and compute costs. Only a small fraction of the indexed files are actually viewed during search interactions. As a result, precomputing previews at multiple resolutions for all media files would be wasteful and unnecessary. To balance responsiveness with resource efficiency, we generate previews on demand during the read path rather than upfront. This minimizes upfront compute and storage costs but introduces new latency concerns during user interactions, since we want to generate previews quickly to have a snappy user experience. With these challenges in mind, we designed a solution that integrates scalable infrastructure, smarter content understanding, and a preview system optimized for speed and accuracy.\n\nBuilding a multimedia search solution\n\nTo deliver a responsive and scalable experience in Dash, we had to rebuild key parts of our infrastructure to support search that’s as smart and seamless for photos and videos as it is for documents. This work spans multiple layers of the stack, and it wasn’t pulled off successfully without trial and error. We began by indexing lightweight metadata—pulled from media blobs (the raw files like images, videos, or audio)—to keep compute costs low. We extended our relevance models to handle location-aware queries and fuzzy file naming, and we optimized our preview generation pipeline to balance latency with cost. Along the way, we made frontend and backend updates to ensure media renders quickly and consistently across devices. The result is a robust multimedia search experience powered by smart metadata, just-in-time previews, and a UI that helps users find the right visual asset fast. Let’s get into how we tackled it. Indexing media files by metadata To keep the compute costs low, we begin by indexing media files using available metadata, which is significantly cheaper to process than analyzing the full contents of media like images or videos. For example, we extract features such as file path, title, and EXIF. These metadata provide a lightweight foundation that enables basic search functionality with minimal processing overhead. As our capabilities evolve, we plan to build on this metadata-first approach by selectively incorporating deeper content analysis—such as semantic embedding and/or OCR—striking a balance between accuracy and cost.\n\nTo generate metadata features at scale, we leveraged Riviera, our internal compute framework that already powers Dropbox Search. Riviera processes tens of petabytes of data daily and includes mature business logic for metadata extraction. By reusing it, we benefited from proven scalability and consistency with existing Dropbox search infrastructure. Backfilling the index\n\nPrior to this initiative, we avoided downloading or storing raw media blobs in order to reduce storage and compute costs. As a result, our existing search index lacked the necessary features to support rich, media-specific search experiences. To address this gap, we added support for ingesting multimedia blob content to compute the required features. We retain the raw content for preview generation and to compute future features. Where possible, we download previews provided by third-party applications. These externally sourced previews are especially useful for design files like Canva, where we’re unable to generate our own. Using them also helps us reduce compute costs. Storage optimizations\n\nDash optimizes the file sizes and MIME types ingested to balance storage cost and file availability. We currently ingest about 97% of media files and are working to address the remaining gaps with smarter lifecycle management techniques. Retrieving media files by metadata When a user searches for media, we configure the query to match their input against the metadata features extracted during indexing. This includes fields like filenames, file paths, and location data. To enhance location-based search, we also apply custom query logic for interpreting geographic references. Internally, we index a GPS location as a chain of IDs corresponding to the geographical hierarchy. For instance, we can look up the GPS coordinates of a photo to be from San Francisco in a process known as “reverse geocoding.” Then, we would build a chain of IDs corresponding to San Francisco, California, and the United States, respectively, and place these IDs in the index for the photo. This allows us to retrieve the photo when the user wants to search for a photo taken in San Francisco, California or the entire United States, respectively.\n\nAt query time, we identify substrings of the query that may potentially be geographical locations, and then we determine whether they map to a valid location ID. In practice, because the number of known geographical locations has a manageably small cardinality, we retrieve the entire mapping upon the service startup and cache it. Lastly, in the course of building multimedia search, we realized that many multimedia files are named in particular ways. Many of them are files in the filesystem, e.g. PhotoShoot-Revised1234.jpg. To support better matching, we added logic to tokenize camel case, hyphenated strings, and numeric suffixes during both indexing and retrieval. Preview and metadata hydration at retrieval time Our system ingests data at a rate that’s approximately three orders of magnitude higher than the query rate. This disparity makes it prohibitively expensive to generate and store previews for all multimedia files during ingestion, both in terms of compute and storage. To address this, we adopted a just-in-time approach, where previews are generated at query time. This strategy significantly reduces upfront costs while still supporting a responsive user experience. As part of our storage optimization efforts, we considered precomputing previews during ingestion to enable deletion of the raw content afterward. However, we ultimately decided against this approach for two key reasons. First, managing the lifecycle of these additional preview artifacts would introduce significant code complexity. Second, retaining the raw content ensures future flexibility, allowing us to compute new features later without having to re-ingest the original files.\n\nTo power the just-in-time approach, we rely on an internal previews service built on top of Riviera, a framework originally developed for Dropbox Search. The previews service is designed to be fast, scalable, and efficient. It incorporates intelligent caching strategies, storing previews for up to 30 days. This allows us to serve previews quickly when needed without repeatedly generating them for every request. During a search, we generate preview URLs for the relevant results, which are then passed to the frontend. The frontend fetches these URLs and displays the corresponding previews directly to the user. By reusing both the Riviera framework and the previews service, we also create the opportunity to reuse frontend components across both Dropbox and Dash. This ensures a consistent product experience across both platforms. To improve latencies, we create the preview URLs in parallel with other search operations such as ranking the results, performing permission checks, and fetching additional metadata needed to render complete search results. By handling these tasks in parallel, we minimize the overall response time and ensure a responsive user experience.\n\nSometimes, a user may want to enlarge a preview and view additional metadata, such as camera information. However, this is a less common operation, and sending all the extra metadata with every search result would be inefficient. When users request more detail—such as camera metadata or timestamp—we fetch it on-demand via a separate endpoint. This keeps the initial search response lean while still supporting deeper inspection when needed. User experience Searching through images and videos is a different experience than searching documents, especially since media files often have names like “IMG_1234” that don’t tell you much. That’s why fast, visual previews are essential—they help users quickly decide which file is relevant without needing to open each one. We’ve designed our preview system to load quickly and adapt to different shapes and sizes of media, whether an image is tall, wide, or an unusual shape. The layout avoids awkward cropping and keeps things easy to browse. When a user wants a closer look, they can open a full-size preview that also shows helpful EXIF details like when the photo was taken, what kind of camera was used, and where it was captured. Everything is built to feel smooth and fast, whether you’re using Dash on a phone or computer. The interface stays out of the way and puts the focus on the content, making it easy to browse quickly or dive into a specific file when needed.\n\nLessons learned and future direction", "label": "non_personal"}
{"title": "Norman Thavaud - Le comedien français 2", "url": "https://lifeofpablo.com/blog/norman-thavaud-le-comedien-fran%C3%A7ais-2", "content": "Norman Thavaud - Le comedien français 2\n\nThis post was written in English (en_US).\n\n\"Norman Thavaud (born 14 April 1987) is a French humourist and blogger known for his short comical YouTube videos. Each of his videos have received at least two million views, some receiving over 7 million. (Wikipedia)\n\nNorman Thavaud is a French comedian who has become an internet sensation with his video blogs. He adresses what we would say \"\"first world problems\"\" things like technology. It shows us our modern struggles in a different point of view. He really gets you laughing. Norman has made particpated in many short films and been sponsored by many companies. One in Particular the candy bar Crunch.\n\nWatching his videos has really improved my French skills buy listening and watching a true french person speak. It makes learning another language easier if you get a grip on how it is actually pronounced. Hey he has taught me a lot of slang! Which case is how many young people communicate in this day and age.\n\nI really reccommend his videos to all age groups!\n\nUne s/o a Norman pour tes cool vidéos! Chaque jour tu m’empresses avec tes blagues!! Je ne peut pas attendre pour la nouvelle vidéo.\"", "label": "non_personal"}
{"title": "Norman Thavaud - Le comedien français", "url": "https://lifeofpablo.com/blog/norman-thavaud-le-comedien-fran%C3%A7ais", "content": "Norman Thavaud - Le comedien français\n\nThis post was written in English (en_US).\n\n\"Norman Thavaud (born 14 April 1987) is a French humourist and blogger known for his short comical YouTube videos. Each of his videos have received at least two million views, some receiving over 7 million. (Wikipedia)\n\nNorman Thavaud is a French comedian who has become an internet sensation with his video blogs. He adresses what we would say \"\"first world problems\"\" things like technology. It shows us our modern struggles in a different point of view. He really gets you laughing. Norman has made particpated in many short films and been sponsored by many companies. One in Particular the candy bar Crunch.\n\nWatching his videos has really improved my French skills buy listening and watching a true french person speak. It makes learning another language easier if you get a grip on how it is actually pronounced. Hey he has taught me a lot of slang! Which case is how many young people communicate in this day and age.\n\nI really reccommend his videos to all age groups!\n\nUne s/o a Norman pour tes cool vidéos! Chaque jour tu m’empresses avec tes blagues!! Je ne peut pas attendre pour la nouvelle vidéo.\"", "label": "non_personal"}
{"title": "Denver Trip", "url": "https://lifeofpablo.com/blog/denver-trip", "content": "Denver Trip\n\nThis post was written in English (en_US).\n\n\"\n\nLet the road trip begin A photo posted by Pablo Morales (@pmorales18) on Jan 5, 2016 at 9:33am PST\n\n\"\"Oh its hard to live in the city\"\" - Albert Hammond Jr.\n\nIts been an interesting start to the new year.\n\nThis week I am on a trip to Denver with my three best friends Sammy and Sam. These are some of the coolest dudes you guys will ever meet. It has been an interesting trip so far.\n\nWe left Nebraska on in the morning on Tuesday. When we got there we got some grub at a great place. Then we hit up downtown. We did a bit of shopping and just walked around. It was fun just being in that urban enviorment. All the buildings are beautiful. Downtown Denver has really cleaned up their act. I like seeing the street performers (the few in the winter).\n\nThen we hit up Redrocks Amphitheater and Park where we did some climbing up the stairs and enjoyed the beautiful sights and sceneries of the area. It was so relaxing. Any part of outside Denver is nice! I like nature. Twas fun. I do not think that I could live in the city\n\nWe had dinner at Five Guys, the burgers were so delicious. Where has it been my whole life. There were many places where we hit up for yummy food!\n\nThe nightlife in Denver is pretty swell!! The city surprises me so much!!!\n\nFound some good books at the Thrift Shop!! Who knew thrifts shops were so badass.\n\nWe went everywhere in Denver. The malls here are amazing and huge. So many sights we wanted to see but time was the enemy. The next trip will be even better.\n\nTime to hit the road! This was a trip that I really enjoyed especially with your buddies. Now it is time to go home and get back to reality. School starts soon and I got to get back to the whole routine. See you soon Nebraska!!\n\nFOLLOW ME ON INSTAGRAM\n\nTWITTER\"", "label": "non_personal"}
{"title": "Update on College.", "url": "https://lifeofpablo.com/blog/update-on-college", "content": "Update on College.\n\nThis post was written in English (en_US).\n\n\"Junior year of college is the turning point of most people's education. It is the time where the most people are deep into their major. You know, the point where it seems too late to turn back. This is not the case for me.\n\nI've changed my major various times to in the last two years. It has not been the easiest process. In most cases it revolved around me jumping back and forth between different areas of science or healthcare related fields to humanities. Not deciding what I wanted in my life has had an effect on my greatly. Many friends know to what extent this led too. Thinking back on this tossing and turning made me realize how I truly did not have a passion for the sciences. You know what it really showed me? I am a person who is very stubborn. I was in denial. Deep down inside, there was no want for me to continue in the sciences. My mindset was not open to explore other options. I felt like a failure if I did not become something in the sciences or be something in healthcare. Avoiding disappointment and failure had become the way my life driving towards.\n\nThere comes a time where you have to wake up and come to reality that you really do not have interest in what you are shooting for after attempting various attempts. I should have followed my heart from the very beginning. My advice to you,\n\n\"\"Do not let anyone tell you what you should study. Only y0u know what you want in life.\"\"\n\nI did that mistake and did not follow my dreams right away. It took a lot of courage for me to stand up to myself. It is sort of shocking, huh? The mind is is your worst enemy. I am glad that I did what I did.\n\nNow I am following my passion. A passion that I have always known to have since the first time I took this class in high school. There is this a major sense of relief. I announce that I will fully commit to become a teacher and strive for what I feel that I will do best\n\nThank you for the support.\n\nWith much love,\n\nPablo Morales\n\n\"", "label": "non_personal"}
{"title": "Setting Up Vouch Proxy using Nginx", "url": "https://lifeofpablo.com/blog/setting-up-vouch-proxy-using-nginx", "content": "Setting Up Vouch Proxy using Nginx\n\nThis post was written in English (en_US).\n\nLocation: 38.581573, -121.494400\n\nBlog Post on this using Indieauth coming soon!\n\nTable of Contents\n\nIntroduction\n\nRecently I have started experimenting with identity. An SSO solution for Nginx using the auth_request module. Vouch Proxy can protect all of your websites at once.\n\nToday, I'll demonstrate how to setup Vouch Proxy on an nginx web server. In this example I will be using Google as our provider\n\nThis tutorial assumes you have prior knowledge of using a linux server such as Debian. Message me at hello@lifeofpablo.com if you need some help. I'd be happy to do so!\n\nWhat Vouch Proxy Does?\n\nAccording to the Repository README.md, it states the following:\n\nVouch Proxy (VP) forces visitors to login and authenticate with an IdP (such as one of the services listed above) before allowing them access to a website.\n\nVP can also be used as a Single Sign On (SSO) solution to protect all web applications in the same domain.\n\nAfter a visitor logs in Vouch Proxy allows access to the protected websites for several hours. Every request is checked by VP to ensure that it is valid.\n\nVP can send the visitor's email, name and other information which the IdP provides (including access tokens) to the web application as HTTP headers. VP can be used to replace application user management entirely.\n\nThings you'll need/prepare:\n\nA linux server with a public IP address with hosting and SSL Debian will be used here but any of the common distros will work Certbot is an easy solution to get SSL certifcate for https://\n\nGo Language (to compile vouch-proxy)\n\nVouch Proxy\n\nNginx Web Server Digital Ocean has a good guide if you need to learn how to setup virtual blocks in nginx.\n\n\n\nDownload/Install Vouch Proxy from Github\n\nMake sure to have Go Lang installed\n\n$ git clone https://github.com/vouch/vouch-proxy.git $ cd vouch-proxy $ ./do.sh goget $ ./do.sh build\n\nVouch Proxy Nginx Virtual Block\n\nLet's go ahead and create a virtual block to proxy Vouch Proxy.\n\nserver { server_name vouch.example.com; location / { proxy_set_header Host vouch.example.com; proxy_set_header X-Forwarded-Proto https; proxy_pass http://127.0.0.1:9090; }\n\nLet's go ahead and create a virtual block for a regular nginx website site or edit an existing virtual block. This is the website/service that you will protect with Vouch Proxy.\n\nIn this example I am using a php web app. If you a non php site site to work you can remove this location block and and edit it to your needs.\n\nGoogle Cloud Console\n\nGoogle Cloud Console API\n\nBefore we modify the config.yml, lets create an OAuth 2.0 Client ID and Client Secret which you will paste into the config.yml file.\n\nYou will have to do the following 1. Create a Project\n\nCall it whatever you like.\n\nI called it oauth\n\nCreate Credentials\n\nYou'll create the client ID and client secret (don't worry if you do the following steps and forget to copy these down. They will be available for you to copy at any time.\n\nOAuth consent screen\n\nApp Name:\n\nThis is where you will need to setup where your redirect URL is https://vouch.example.com/url. Just like the server_name set in the nginx virtual block config above.\n\nAuthorized Domain: example.com (just the base domain)\n\nFill out any other required fields\n\nModify your config.yml\n\nvouch: domains: - yourdomain.com - yourotherdomain.com cookie: secure: false oauth: provider: google client_id: xxxxxxxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.apps.googleusercontent.com client_secret: xxxxxxxxxxxxxxxxxxxxxxxx callback_urls: - https://yourdomain.com/auth - https://yourotherdomain.com/auth preferredDomain: yourdomain.com\n\nNginx Virtual block protected by Vouch Proxy\n\nserver { listen 80 ; listen [::]: 80 ; root /root/to/web/directory; index index.php index.html; server_name secretapp.example.com; location / { try_files $uri $uri / /index.php $is_args $args ; } client_max_body_size 100m ; location ~* \\.php$ { fastcgi_pass unix:/var/run/php/php8.2-fpm.sock; include fastcgi.conf; fastcgi_index yellow.php; fastcgi_split_path_info ^(.+\\.php)(/.+)$ ; fastcgi_param SCRIPT_FILENAME $document_root $fastcgi_script_name ; include fastcgi_params; } }\n\nEventually you will need to secure your site with SSL/TLS that makes your site use https://. Google will require that your traffic is secure with using it as 0auth as the method used to sign in to your protected website.\n\nDo this after you have the survey blocks working in the following section.\n\nHere is the link for Certbot for Debian. I have tested this on Debian 10 & 11. [https://certbot.eff.org/instructions?ws=nginx&os=debianbuster](Link for Certbot)\n\nCert bot can do this for you as long as you have the subdomain in your DNS pointing to your machine and have cert bot installed. It'll add these blocks in your\n\nor\n\nserver { server_name vouch.example.com . . . . . . . . . . . . . . . . . . . . . . . . . listen [::]: 443 ssl; listen 443 ssl; ssl_certificate /etc/letsencrypt/live/vouch.example.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/vouch.example.com/privkey.pem; include /etc/letsencrypt/options-ssl-nginx.conf; ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; } server { if ( $host = vouch.example.com) { return 301 https:// $host $request_uri ; } listen 80 ; listen [::]: 80 ; server_name secretapp.example.com; return 404 ; }\n\nLet's check for errors in nginx. Type the following command.\n\nnginx -t\n\nYou should see something similar to this:\n\nnginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful\n\nLet's open a browser tab or window!\n\nNote: I'm using firefox. (Preference). Any modern browser should work.\n\nType in the protected app' URL in the address bar\n\nSign in to Google\n\nSign in with an email that is allowed to sign to access the website when you configured it in Google Cloud Console.\n\nVoila, the protected page.\n\nHere is the home page of a Bludit CMS on subdomain acting as \"secretapp.example.com\"", "label": "non_personal"}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2023-04", "content": "en\n\nLocation: 38.581573, -121.494400\n\nBlog Post on this using Indieauth coming soon!\n\nTable of Contents\n\nIntroduction\n\nRecently I have started experimenting with identity. An SSO solution for Nginx using the auth_request module. Vouch Proxy can protect all of your websites at once.\n\nToday, I'll demonstrate how to setup Vouch Proxy on an nginx web server. In this example I will be using Google as our provider\n\nThis tutorial assumes you have prior knowledge of using a linux server such as Debian. Message me at hello@lifeofpablo.com if you need some help. I'd be happy to do so!\n\nWhat Vouch Proxy Does?\n\nAccording to the Repository README.md, it states the following:\n\nVouch Proxy (VP) forces visitors to login and authenticate with an IdP (such as one of the services listed above) before allowing them access to a website.\n\nVP can also be used as a Single Sign On (SSO) solution to protect all web applications in the same domain.\n\nAfter a visitor logs in Vouch Proxy allows access to the protected websites for several hours. Every request is checked by VP to ensure that it is valid.\n\nVP can send the visitor's email, name and other information which the IdP provides (including access tokens) to the web application as HTTP headers. VP can be used to replace application user management entirely.\n\nThings you'll need/prepare:\n\nA linux server with a public IP address with hosting and SSL Debian will be used here but any of the common distros will work Certbot is an easy solution to get SSL certifcate for https://\n\nGo Language (to compile vouch-proxy)\n\nVouch Proxy\n\nNginx Web Server Digital Ocean has a good guide if you need to learn how to setup virtual blocks in nginx.\n\n\n\nDownload/Install Vouch Proxy from Github\n\nMake sure to have Go Lang installed\n\n$ git clone https://github.com/vouch/vouch-proxy.git $ cd vouch-proxy $ ./do.sh goget $ ./do.sh build\n\nVouch Proxy Nginx Virtual Block\n\nLet's go ahead and create a virtual block to proxy Vouch Proxy.\n\nserver { server_name vouch.example.com; location / { proxy_set_header Host vouch.example.com; proxy_set_header X-Forwarded-Proto https; proxy_pass http://127.0.0.1:9090; }\n\nLet's go ahead and create a virtual block for a regular nginx website site or edit an existing virtual block. This is the website/service that you will protect with Vouch Proxy.\n\nIn this example I am using a php web app. If you a non php site site to work you can remove this location block and and edit it to your needs.\n\nGoogle Cloud Console\n\nGoogle Cloud Console API\n\nBefore we modify the config.yml, lets create an OAuth 2.0 Client ID and Client Secret which you will paste into the config.yml file.\n\nYou will have to do the following 1. Create a Project\n\nCall it whatever you like.\n\nI called it oauth\n\nCreate Credentials\n\nYou'll create the client ID and client secret (don't worry if you do the following steps and forget to copy these down. They will be available for you to copy at any time.\n\nOAuth consent screen\n\nApp Name:\n\nThis is where you will need to setup where your redirect URL is https://vouch.example.com/url. Just like the server_name set in the nginx virtual block config above.\n\nAuthorized Domain: example.com (just the base domain)\n\nFill out any other required fields\n\nModify your config.yml\n\nvouch: domains: - yourdomain.com - yourotherdomain.com cookie: secure: false oauth: provider: google client_id: xxxxxxxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.apps.googleusercontent.com client_secret: xxxxxxxxxxxxxxxxxxxxxxxx callback_urls: - https://yourdomain.com/auth - https://yourotherdomain.com/auth preferredDomain: yourdomain.com\n\nNginx Virtual block protected by Vouch Proxy\n\nserver { listen 80 ; listen [::]: 80 ; root /root/to/web/directory; index index.php index.html; server_name secretapp.example.com; location / { try_files $uri $uri / /index.php $is_args $args ; } client_max_body_size 100m ; location ~* \\.php$ { fastcgi_pass unix:/var/run/php/php8.2-fpm.sock; include fastcgi.conf; fastcgi_index yellow.php; fastcgi_split_path_info ^(.+\\.php)(/.+)$ ; fastcgi_param SCRIPT_FILENAME $document_root $fastcgi_script_name ; include fastcgi_params; } }\n\nEventually you will need to secure your site with SSL/TLS that makes your site use https://. Google will require that your traffic is secure with using it as 0auth as the method used to sign in to your protected website.\n\nDo this after you have the survey blocks working in the following section.\n\nHere is the link for Certbot for Debian. I have tested this on Debian 10 & 11. [https://certbot.eff.org/instructions?ws=nginx&os=debianbuster](Link for Certbot)\n\nCert bot can do this for you as long as you have the subdomain in your DNS pointing to your machine and have cert bot installed. It'll add these blocks in your\n\nor\n\nserver { server_name vouch.example.com . . . . . . . . . . . . . . . . . . . . . . . . . listen [::]: 443 ssl; listen 443 ssl; ssl_certificate /etc/letsencrypt/live/vouch.example.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/vouch.example.com/privkey.pem; include /etc/letsencrypt/options-ssl-nginx.conf; ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; } server { if ( $host = vouch.example.com) { return 301 https:// $host $request_uri ; } listen 80 ; listen [::]: 80 ; server_name secretapp.example.com; return 404 ; }\n\nLet's check for errors in nginx. Type the following command.\n\nnginx -t\n\nYou should see something similar to this:\n\nnginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful\n\nLet's open a browser tab or window!\n\nNote: I'm using firefox. (Preference). Any modern browser should work.\n\nType in the protected app' URL in the address bar\n\nSign in to Google\n\nSign in with an email that is allowed to sign to access the website when you configured it in Google Cloud Console.\n\nVoila, the protected page.\n\nHere is the home page of a Bludit CMS on subdomain acting as \"secretapp.example.com\"", "label": "non_personal"}
{"title": "Speaking in different languages", "url": "https://lifeofpablo.com/blog/speaking-in-different-languages", "content": "Speaking in different languages\n\nThis post was written in English (en_US).\n\nI wrote a post a few weeks ago about writing content in other languages and marking it up using h-entry tags. There is something so satisfying to write in different languages. Different parts of my brain activate and it works differently in each language. I also \"become a different\" person as each language influences how I present myself. I am still the same person but I tend to shift according to the target language.\n\nToday, I am going to say in a a video, \"Hello, I'm Pablo\" in three different languages, respectively: English, Spanish and French. I will also share some things about me in writing.\n\nI am really excited to make more visual content in the languages I know and the languages I am learning. I really need to look into localization as well.\n\nThank you Tracy Durnell for sharing with me Alex's blog post June 2023 in review. It is a great inspiration to use my language skills to the test.\n\nEnglish Your browser does not support the video tag. Favorite Song Nada - Zoé Favorite Movie Spiderman Favorite Artist Low Favorite Food (other than Mexican food) Vietnamese Favorite Media Companyr New York Times Favorite Hobby Coding\n\nSpanish Your browser does not support the video tag. Canción Favorita Nada - Zoé Telenovela Favorita Betty, La Fea Estado Favorito Oaxaca Plato Favorito Mole Sabor favorito Guayaba Pasatiempo Favorito Fotografiando la gente de México", "label": "non_personal"}
{"title": "Visiting the BuzzFeed Office", "url": "https://lifeofpablo.com/blog/visiting-the-buzzfeed-office", "content": "One of my wishes came true! I've been a huge fan of BuzzFeed for many years for as long as I can remember. I remember stumbling on BuzzFeed's home page in the late 2000s. I've taken many of their quizzes posted on the site. There are other things on the site beyond the quizzes. I was very fortunate this week to visit BuzzFeed/Complex's office this week. I was very fortunate to get a tour and check things out! Going there was a whole experience. I got to share this experience with people I know an love. You know that feeling you get when you're a little kid at the grocery store eagerly waiting to see if my parents would buy me any candy. That's what I was feeling. I'm pretty sure I cried a few tears of joy. The office had such a cool color scheme and it went well with everything around it. It seemed very organized. I got to see how the office works and what the type of mood exists. The space was a welcoming right when you walk into. I would say it is a pretty laid back office.It was everything I hoped and dreamed for. I was in heaven. I can't say too much more other than, one more thing checked off my, \"places I wan't to see some day.\"", "label": "non_personal"}
{"title": "Update - 1000 Albums in 1000 Days", "url": "https://lifeofpablo.com/blog/update-one-thousand-albums-in-one-thousand-days", "content": "Background from the previous post\n\nI wrote a post, 1000 Albums in 1000 Days, that a started a challenge or an adventure to listen to 1000 albums in 1000 days. So far it's going well. I've discovered new music or music I've been slightly familiar with. It's been fun to travel back the decades and see what everyone has been listening. My music taste has been less stale than when I had started.\n\nPosting on my blog\n\nI haven't posted as much on what I've been listening to. I decided to create weekly post of the titles and artist of respective I've listened to. If there is an album I really enjoyed, I will make a dedicated post.\n\nI built a microsite\n\nI created a microsite on a subdomain on lifeofpablo.com. This microsite will display all the albums I've listened to, so far. I will update as I go or I will update when I do my weekly post.\n\nThe website is https://1k.lifeofpablo.com.\n\nThe site is relatively simple. It has the following:\n\nNavigation Bar\n\nHero Image\n\nDescription\n\nAlbum Grid (Responsive of course :D )\n\nFooter\n\nI will organize it by the decade to keep things manageable. This is how it is organized in the book.\n\nIt's just plain simple.\n\nDocker and Github Actions\n\nI will update it using github actions along with Dockr to recreate the site when I make changes.\n\nRecomendations & Webmentions\n\nAs always, if any of you have any recommendations for music to listen to? Send them my way, I'd love to see what you all recommend!\n\nWays to contact me:", "label": "non_personal"}
{"title": "IndieWeb Carnival October 2023 - Self Care and Routine", "url": "https://lifeofpablo.com/blog/indieweb-carnival-october-2023", "content": "This month I am hosting this month's IndieWeb Carnival on self-care and routine. Anyone and everyone is welcome to participate in the carnival.\n\nWhat is self-care ?\n\n\"a multidimensional, multifaceted process of purposeful engagement in strategies that promote healthy functioning and enhance well-being.\" [1]\n\n\"Self-care means taking care of yourself so that you can be healthy, you can be well, you can do your job, you can help and care for others, and you can do all the things you need to and want to accomplish in a day.\" [2]\n\nThere is no one size fits all model for everyone. People's backgrounds such as culture, spiritual beliefs, life experiences, etc influence how self-care is practices. It could also be proactive or reactive. There is no right or wrong way of doing this.\n\nSelf-Care in the Digital Age\n\nWe live in age where we are always connected online. This also adds complexities balancing our lives. Setting boundaries isn't just limited to people. Being always connected to the web is also taxing on our mental health.\n\nHow Am I performing Self-Care ?\n\nLearning self-care has been something I've been working on a lot this year. It has been something I neglected for many, many years. It has helped me be more aligned and be more connected with myself. Finding a self-care routine isn't exactly a straight line to follow. I'm still finding ways to better improve my routine. It's been important to adjust as I go because it's a continious learning process. Some things worked a few months ago and now it isn't working as well before.\n\nSetting boundaries with myself and people\n\nActually find time for myself and appreciate alone time.\n\nActually address insecurities\n\nExcercise such as running or biking.\n\nDisconnect from digital devices as needed\n\nPicking up new hobbies such as hacky sack.\n\nI invite you to write a post on self-care.\n\nHere are some prompts to help you get started or to build off of this post.\n\nWhat type of self-care routines do partake in?\n\nWhat are some hard realizations once you started to take care of yourself?\n\nHow do you incorporate self-care in difficult times?\n\nWhat has been your journey in reaching self-care and your routines.\n\nHow does your self-care routine differ during the week vs the weekend/going on holiday?\n\nHow do you take care of yourself.\n\nDo you check in with yourself too make sure your self-care routine is keeping up with your needs?\n\nI will create a roundup post on the 1 November 2023 on all the responses I recieve. I will post on Indieweb News\n\nSend me your responses via:\n\nWebmention\n\nEmail - pablo@lifeofpablo.com\n\nIf you would like to host a future monthly IndieWeb Carnival, please check out the details on the IndieWeb Wiki.\n\nThis post has been syndicated to IndieWeb News.", "label": "non_personal"}
{"title": "I Took Myself Out on Date", "url": "https://lifeofpablo.com/blog/solo-date", "content": "Today, I wanted to get out the apartment and not bum around. So I went on a date. It's not often I get time for myself other than at home. I either stay home or I go the usual places I'm comfortable going.\n\nI drove down to Simon's Cafe (Chinese Food Staple in Midtown Sacramento. As I was pulling up I remembered they closed down. I was sad. Then I remembered that there was a Thai restaurant a block or two down. Chicken Pad Thai was the meal of choice.\n\nThen, I went to went to happy hour at a Wine Bar across the street from the Thai restaraunt. I had a glass of Sangria. I don't mind a cheap drink during happy hour.\n\nAfter, I went to an art gallery to observe the art. I got to meet the artist of some of the art I admired. That was pretty tight!\n\nThen I came across to the Strapping Store location where ever I was in Midtown. I got some cool Sacramento themed items (last minute Christmas gifts). It's a pretty cool store.\n\nI ended the night with getting Frozen Yogurt. Strawberry Cheesecake Fro-yo and grahm crackers are amazing.\n\nIt was nice taking myself out. I don't do this often. This is something I wouldn't mind getting more comfortable with. It simply is trying to get out of my comfort zone. Overall, I enjoyed the alone time I had. Alone time is good. Taking yourself out on a date should be okay. Just enjoy your own presence.", "label": "non_personal"}
{"title": "IndieWeb Carnival en Diciembre 2023 — Tradiciones y Celebraciones de Diciembre", "url": "https://lifeofpablo.com/blog/tradiciones-de-diciembre", "content": "Me encanta mucho el mes de Diciembre. Cuando se acerca la navidad, me gusta cuando hay nieve en el suelo. Me gustan las celebraciones y tradiciones en las que participó. Yo celebro tradiciones mexicanas y celebraciones estadounidenses. Yo vivo entre las dos culturas y crecí en los Estados Unidos y he vivido en México. En casa lo celebró como si estuviera en México.\n\nCada año es diferente. A veces estoy en Oaxaca durante el mes de diciembre y a veces estoy en Estados Unidos. Celebró ahí las tradiciones Mexicanas pero más precisamente, las tradiciones Oaxaqueñas. Me gusta ir a ver los bailes y los desfiles. Me gusta ir a los mercados de comida y probar todas las comidas deliciosas. Se nos hace agua en la boca.\n\nCelebraciones Mexicanas:\n\nEn mi familia nos gusta mucho cocinar en casa. Hacemos muchos platillos tradicionales. Hacemos tamales de varios sabores, ponche mexicano, mole, pasteles, cocteles de camarón, y muchísimo más! No hay límite. En este mes, normalmente no comemos en la calle. ¡La comida en casa siempre es más rica!\n\nEn las noches siempre vemos unas películas de navidad. Nos gusta ver películas en Español. Cada año vemos Home Alone en español porque es muy chistosa. Simplemente estar juntos es importante. Podemos reír entre todos.\n\nSiempre ponemos y arreglamos el árbol de navidad. Nosotros abrimos los regalos en la noche del 24 de diciembre. Siempre ha sido así.\n\nCelebraciones Estadounidenses:\n\nSiempre me reuno con mis amigos. Ahora que no vivimos en la misma región, es muy importante estar juntos. Hablamos como nos fue este año y que queremos cumplir para el año entrante. Algunas cosas que hacemos son casas de jengibres y tenemos una fiesta. Dependiendo como esté el clima, andamos en patineta o vamos al skatepark.\n\nLo que siempre me anima es la noche de programación. Hacemos un plan en que vamos a programar en una noche. Nos quedamos despiertos hasta la madrugada. Es algo muy bonito de programar algo juntos. Creo que este año vamos hacer un video juego. A ver como nos va.\n\nEsto es mi respuesta al IndieWeb Carnival December 2023 — Holiday and December traditions ¡Muchas Gracias Jo! Todos pueden participar. Si deseas participar en el IndieWeb Carnival haz clic aqui para informarte.\n\nSyndicated on IndieNews ES", "label": "non_personal"}
{"title": "IndieWeb Carnival February 2024 - Digital Relationships", "url": "https://lifeofpablo.com/blog/indieweb-carnival-february-2024-digital-relationships", "content": "I enjoy making friends and making connections with people of all backgrounds and cultures. Growing up, I'd make friends with the international exchange students. They show up for a year or so and then leave. Many I've stayed in touch with and have gone to visit them in their home country. I would consider them close friends. We've kept this friendship even if they are across the world.\n\nAs a person, I would say I’m a very affectionate person. I like to form bonds and form some level of intimacy. Intimacy can be physical, emotional, intellectual, spiritual or experiential. The idea of intimacy does not need to be sexual. I’m very affectionate to those with whom I have a close relationship, such as my best friends. I would also consider them very affectionate as well. I guess we can say we are in touch with our emotions and be expressive towards each other. We’ve shared so many moments together. That’s why we’re friends. As we’ve become older and started our careers, our lives, our relationships, etc, we’ve all relocated and don’t see each other as often as we used to. We all find ourselves living in various parts of the United States. It’s a sad reality growing up. Yet, here we are. It doesn’t seem that our bonds have weakened.\n\nHaving these human relationships or interactions is the main piece of keeping our relationships so strong. Many of my relationships with people I have rely on digital relationships as well. It's how many of our relationships keep strong.\n\nMany of us desire a form of intimacy and emotional bond from those important to us. Oftentimes the only way to get these is through a virtual medium. We want to foster deep emotional connections with the absence of physical proximity. We want to share those important moments with a phone call or a video chat. We want to support each other in hard times. We want to replicate those movie nights when we were all living together. Virtual relationships allow us to provide emotional support from afar with our personal struggles, external stressors, or societal issues. This strengthens the emotional bond between individuals. Trust becomes a cornerstone that supports the vulnerability inherent in forming emotional bonds.\n\nIt's possible to have meaningful virtual relationships as we adapt with the changing times and remember the human in relationships.", "label": "non_personal"}
{"title": "February 2024", "url": "https://lifeofpablo.com/blog/published:2024-02", "content": "I enjoy making friends and making connections with people of all backgrounds and cultures. Growing up, I'd make friends with the international exchange students. They show up for a year or so and then leave. Many I've stayed in touch with and have gone to visit them in their home country. I would consider them close friends. We've kept this friendship even if they are across the world.\n\nAs a person, I would say I’m a very affectionate person. I like to form bonds and form some level of intimacy. Intimacy can be physical, emotional, intellectual, spiritual or experiential. The idea of intimacy does not need to be sexual. I’m very affectionate to those with whom I have a close relationship, such as my best friends. I would also consider them very affectionate as well. I guess we can say we are in touch with our emotions and be expressive towards each other. We’ve shared so many moments together. That’s why we’re friends. As we’ve become older and started our careers, our lives, our relationships, etc, we’ve all relocated and don’t see each other as often as we used to. We all find ourselves living in various parts of the United States. It’s a sad reality growing up. Yet, here we are. It doesn’t seem that our bonds have weakened.\n\nHaving these human relationships or interactions is the main piece of keeping our relationships so strong. Many of my relationships with people I have rely on digital relationships as well. It's how many of our relationships keep strong.\n\nMany of us desire a form of intimacy and emotional bond from those important to us. Oftentimes the only way to get these is through a virtual medium. We want to foster deep emotional connections with the absence of physical proximity. We want to share those important moments with a phone call or a video chat. We want to support each other in hard times. We want to replicate those movie nights when we were all living together. Virtual relationships allow us to provide emotional support from afar with our personal struggles, external stressors, or societal issues. This strengthens the emotional bond between individuals. Trust becomes a cornerstone that supports the vulnerability inherent in forming emotional bonds.\n\nIt's possible to have meaningful virtual relationships as we adapt with the changing times and remember the human in relationships.", "label": "non_personal"}
{"title": "Beautiful Sunday", "url": "https://lifeofpablo.com/blog/beautiful-sunday", "content": "Beautiful Sunday\n\nThis post was written in English (en_US).\n\nToday is a holiday celebrated by many. I don't celebrate this holiday. It's simply another day but a more cheerful glee.\n\nThe weather is the 60s and it's sunny out. It's just a beautiful day. The day just wants to pull you outdoors and it wants you to enjoy the simplicity of what is on the outside.I just got back from a run around the park that is nearby me. It was just very nice to see a lot happening at the park. It is very uplifting to see people making use o public spaces. Many groups and families are there simply enjoying their sunday afternoon. There is a group of people dancing to traditional\n\nHere are the things I saw or heard that made me happy:\n\nListening to traditional Mexican music.\n\nMexican folk dances in respect to La Virgin of Guadalupe (The Virgin of Guadalupe). It's nice to see these dances don't get lost as time goes on.\n\nI saw a group of people putting up banners to protect LGBT rights. These must be protected at all costs!\n\nI saw families barbecuing and enjoying time together. This reminds me when I lived with my parents.\n\nEnjoy the upbeat Sunday!", "label": "non_personal"}
{"title": "Copilot for all: Introducing Microsoft 365 Copilot Chat", "url": "https://www.microsoft.com/en-us/microsoft-365/blog/2025/01/15/copilot-for-all-introducing-microsoft-365-copilot-chat/", "content": "Our ambition is to empower every employee with a Copilot and to transform every business process with agents. From Dow to Disney, companies are going big with Microsoft 365 Copilot and agents, uncovering key scenarios that can deliver real ROI. Now, organizations of all sizes are looking to scale their AI transformation and realize the enterprise-wide ROI that comes with broad adoption.\n\nToday, we’re introducing Microsoft 365 Copilot Chat, a new offering that adds pay-as-you-go agents to our existing free chat experience for Microsoft 365 commercial customers.1 Copilot Chat enables your entire workforce—from customer service representatives to marketing leads to frontline technicians—to start using Copilot and agents today. It includes:\n\nFree, secure AI chat powered by GPT-4o.\n\npowered by GPT-4o. Agents accessible right in the chat.\n\naccessible right in the chat. IT controls, including enterprise data protection and agent management.\n\nMoving forward, every organization will have a mix of Copilot Chat and Microsoft 365 Copilot—our best-in-class offering—to drive AI transformation at scale. Let’s walk through the new product lineup.\n\nClick to enlarge\n\nCopilot Chat: The power of chat + agents\n\nCopilot is the UI for AI, and it all starts with Copilot Chat. It’s the chat experience you’ll use every day—powered by broad knowledge from the web, built on GPT-4o, and designed to be safe and secure for business use. It represents a foundational shift in how we work, enabling everyone to work smarter, faster, and more collaboratively.\n\nCopilot Chat includes:\n\nWeb-grounded chat with GPT-4o. You can use it to do market research, write a strategy document, or prepare for a meeting. File uploads allow you to add any document to the chat and ask Copilot to do things like summarize key points in a Word document, analyze data in an Excel spreadsheet, and suggest improvements to a PowerPoint presentation. 2 With Copilot Pages , you can collaborate on content with people and AI in real time—adding content from Copilot, your files, and now from the web as well. And you can quickly create AI-generated images for campaigns, product launches, and social media posts. 3\n\nwith GPT-4o. You can use it to do market research, write a strategy document, or prepare for a meeting. allow you to add any document to the chat and ask Copilot to do things like summarize key points in a Word document, analyze data in an Excel spreadsheet, and suggest improvements to a PowerPoint presentation. With , you can collaborate on content with people and AI in real time—adding content from Copilot, your files, and now from the web as well. And you can quickly create for campaigns, product launches, and social media posts. Agents . Using natural language, now anyone can easily create agents to automate repetitive tasks and business processes—directly in Copilot Chat. A customer service representative can ask a customer relationship management (CRM) agent for account details before a customer meeting, while field service agents can access step-by-step instructions and real-time product knowledge stored in SharePoint. Agents are priced on a metered basis, and IT stays in control. IT admins can also build organization-wide agents and manage agent deployment, all powered by Microsoft Copilot Studio.\n\n. Using natural language, now anyone can easily create agents to automate repetitive tasks and business processes—directly in Copilot Chat. A customer service representative can ask a customer relationship management (CRM) agent for account details before a customer meeting, while field service agents can access step-by-step instructions and real-time product knowledge stored in SharePoint. Agents are priced on a metered basis, and IT stays in control. IT admins can also build organization-wide agents and manage agent deployment, all powered by Microsoft Copilot Studio. Copilot Control System. Copilot Chat includes foundational capabilities of the Copilot Control System, including enterprise data protection (EDP) for data privacy and security and the ability to govern access and manage the usage and lifecycle of Copilot and agents, as well as measurement and reporting.\n\nWhile Copilot Chat is a powerful new on-ramp for everyone in your organization to build the AI habit, Microsoft 365 Copilot remains our best-in-class personal AI assistant for work. It includes everything in Copilot Chat and more. Microsoft 365 Copilot combines the power of GPT-4o grounded in your work data—all your meetings, emails, chats, documents, and more; Copilot in the Microsoft 365 apps that millions rely on every day—Microsoft Teams, Outlook, Word, Excel, and PowerPoint; and usage and access to agents. And we continue to rapidly add new capabilities like Copilot Actions to tackle people’s biggest pain points at work. We’ve empowered every IT team to lead and manage at scale with the Copilot Control System and Copilot Analytics to measure the impact and ROI of your Copilot investment.\n\nCustomers can get started with either the free or paid experience in the Microsoft 365 Copilot app, available at m365copilot.com or in the Windows, Android, or iPhone app stores.\n\nThese announcements enable every customer to accelerate their AI transformation and realize enterprise-wide ROI. Now, every employee has a Copilot and a team of agents to scale their impact.\n\nTry Copilot Chat at m365copilot.com and visit WorkLab for the latest research and insights on AI and the future of work.\n\nDownload the Microsoft 365 Copilot app Take the power of AI on the go with the Copilot app. Download today\n\nFootnotes:\n\n1 Learn more about Copilot Chat eligibility.\n\n2 File upload limits apply\n\n3 Image generation limits apply", "label": "non_personal"}
{"title": "Declining Picture Taken by TSA", "url": "https://lifeofpablo.com/blog/declining-picture-taken-by-tsa", "content": "Declining Picture Taken by TSA\n\nThis post was written in English (en_US).\n\nToday, I'm flying out of San Francisco (SFO) and I experienced something new today. For the first time, I was asked to have my picture taken by the Transportation Security Administration, or commonly known as the TSA. I'm not surprised that an airport such as San Francisco would have these implemented as ways of efficiently getting passengers as fast as possible.\n\nI politely told the TSA agent that I am declining facial recognition. The agent simply conducted a manual document check. It was a very easy experience and the TSA agent was very respectful.\n\nThe point here is that you're not required or obligated to have your picture taken for biometric verification in the United States. You can simply opt-out by requesting so at a point of entry, such as airport customs and of course with the TSA. Your identity document(s) is verified through a manual check.\n\nI wrote a post, Why I Opted-Out of Facial Recognition at Customs and Border Patrol on opting out of facial recognition by the United States Customs and Border Patrol and the experience I had.\n\nMany people don't know they can decline or aren't aware of the risks that facial recognition have in our society. People need to be more aware of their rights when it comes to biometrics and the data retention of such biometrics.", "label": "non_personal"}
{"title": "How to Protect your sites Vouch Proxy, NGINX, Reverse Proxies with Docker Compose (Part 1)", "url": "https://lifeofpablo.com/blog/protect-sites-with-vouch-proxy-and-docker-compose", "content": "How to Protect your sites Vouch Proxy, NGINX, Reverse Proxies with Docker Compose (Part 1)\n\nThis post was written in English (en_US).\n\nI've written in the past how to install Vouch Proxy using Debian. I also wrote a post a while ago where I dockerized my site and services. If it isn't obvious, I really like Vouch Proxy. It's simple and it met my needs.\n\nI'm going to share how I setup the following services using Docker Compose:\n\nVouch Proxy\n\nNGINX Reverse Proxy\n\nCreating Reverse Proxies for your apps to be protected with Vouch Proxy. I will provide example services as use cases. Grafana Prometheus\n\n\n\nThis post will divided into three (3) parts.\n\nThis guide, is recommended for those who have experience with Docker and Docker Compose. I will keep this simple for you to follow along if you don't have experience. When using Docker Compose and docker-compose.yml files, you are launching multiple containers at one time. When using a Dockerfile, one container is launched at a time.\n\nWe'll using a lot of environmental variables to configure our applications. It seems like a lot of work at first but you'll be happy that you did.\n\nLet's Setup Docker Compose\n\nHere is the docker compose we are going to use. I will break it down piece by piece as mentioned above.\n\nservices: nginx: container_name: nginx image: nginxproxy/nginx-proxy restart: unless-stopped ports: - 80 :80 - 443 :443 volumes: - /var/run/docker.sock:/tmp/docker.sock:ro - /var/docker/nginx/html:/usr/share/nginx/html - /var/docker/nginx/certs:/etc/nginx/certs - /var/docker/nginx/vhost:/etc/nginx/vhost.d - /var/docker/nginx/conf:/etc/nginx/conf.d logging: options: max-size: \"10m\" max-file: \"3\" letsencrypt-companion: container_name: letsencrypt-companion image: jrcs/letsencrypt-nginx-proxy-companion restart: unless-stopped volumes_from: - nginx volumes: - /var/run/docker.sock:/var/run/docker.sock - /var/docker/nginx/acme:/etc/acme.sh environment: DEFAULT_EMAIL: your-email@domain.com mariadb: container_name: mariadb image: mariadb:latest command: --default-authentication-plugin=mysql_native_password environment: MYSQL_ROOT_PASSWORD: changeme MYSQL_DATABASE: kanboard MYSQL_USER: kanboard MYSQL_PASSWORD: changeme volumes: - mariadb:/var/lib/mysql:z vouch-proxy-auth: container_name: vp-proxy image: quay.io/vouch/vouch-proxy:alpine-latest ports: - 9090 :9090 volumes: - ./vouch-proxy-config:/config restart: always environment: VIRTUAL_HOST: your-domain.com LETSENCRYPT_HOST: your-domain.com grafana: container_name: grafana image: grafana/grafana:latest volumes: - ../plugins/:/etc/grafana/plugins/ - ./grafana/provisioning/:/etc/grafana/provisioning/ - grafana_vol:/var/lib/grafana environment: - \"GF_SECURITY_ADMIN_PASSWORD=pwd\" - GF_USERS_ALLOW_SIGN_UP=FALSE - GF_USERS_AUTO_ASSIGN_ORG=TRUE - GF_USERS_AUTO_ASSIGN_ORG_ROLE=EDITOR - GF_AUTH_PROXY_ENABLED=true - GF_AUTH_PROXY_HEADER_NAME=X-Vouch-User - GF_AUTH_PROXY_HEADER_PROPERTY=email - GF_AUTH_PROXY_AUTO_SIGN_UP=false - GF_INSTALL_PLUGINS=grafana-azure-data-explorer-datasource - GF_SERVER_HTTP_PORT=3001 - GF_SERVER_PROTOCOL=http - GF_SERVER_DOMAIN=grafana.domain.com - GF_SERVER_ROOT_URL=grafana.domain.com - GF_SERVER_SERVE_FROM_SUB_PATH=false - GF_SMTP_ENABLED=TRUE - \"GF_SMTP_HOST=smtp.domain.com\" - \"GF_SMTP_USER=smtp-user\" - GF_SMTP_PASSWORD=changeme - \"GF_SMTP_FROM_ADDRESS=grafana@domain.com\" - \"GF_SMTP_FROM_NAME=Name of Grafana Instance\" - \"GF_SMTP_STARTTLS_POLICY=MANDATORYSTARTTLS\" expose: - 3001 vp-proxy-graf: image: nginx:latest container_name: vp-proxy-graf environment: VIRTUAL_HOST: grafana.domain.com LETSENCRYPT_HOST: grafana.domain.com volumes: - ./prometheus-grafana/nginx/graf:/etc/nginx/conf.d ports: - 8081 :80 prometheus: image: prom/prometheus:latest container_name: prometheus restart: unless-stopped volumes: - ./prometheus-grafana/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml - prometheus_data:/prometheus command: - '--config.file=/etc/prometheus/prometheus.yml' - '--storage.tsdb.path=/prometheus' - '--web.console.libraries=/etc/prometheus/console_libraries' - '--web.console.templates=/etc/prometheus/consoles' - '--web.enable-lifecycle' expose: - 9091 vp-proxy-prom: image: nginx:latest container_name: vp-proxy-prom environment: VIRTUAL_HOST: prometheus.domain.com LETSENCRYPT_HOST: prometheus.domain.com volumes: - ./prometheus-grafana/nginx/prom:/etc/nginx/conf.d ports: - 8082 :80 node-exporter: image: prom/node-exporter:latest container_name: node-exporter restart: unless-stopped volumes: - /proc:/host/proc:ro - /sys:/host/sys:ro - /:/rootfs:ro command: - '--path.procfs=/host/proc' - '--path.rootfs=/rootfs' - '--path.sysfs=/host/sys' - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)' expose: - 9100 volumes: prometheus_data: {} grafana_vol: mariadb: prom_data:\n\nWe won't run the docker compose command yet. We don't have all the files and other steps needed to run it correctly\n\nSetup Vouch Proxy Config", "label": "non_personal"}
{"title": "Hosting IndieWebCamp Sacramento and Calls for Co-Hosts", "url": "https://lifeofpablo.com/blog/hosting-indiewebcamp-sacramento-and-calls-for-co-hosts", "content": "Hosting IndieWebCamp Sacramento and Calls for Co-Hosts\n\nIndieWeb Logo\n\nThis post was written in English (en_US).\n\nHello, I'm Pablo Morales from Sacramento, California! I'm a participant in the IndieWeb.\n\nWhat is the IndieWeb?\n\nThe IndieWeb is a people-focused alternative to the “corporate web”. It is a community of independent and personal websites connected by open standards and based on the principles of: owning your domain and using it as your primary online identity, publishing on your own site first (optionally elsewhere), and owning your content.\n\nI am volunteering to host and organize the first IndieWebCamp Sacramento (IWC). It is a great way to bring together your local IndieWeb community and people from the greater IndieWeb community. As the pandemic changed how we interact, online meet-ups have kept the IndieWeb going. It would be nice to meet many of the familiar faces we've seen through the last few years in person!\n\nOrganizing and planning an IndieWebCamp in person involves various parts such as finding co-organizers, planning where to have the event, and the many aspects of event planning.\n\nI'm looking for a co-host or co-host. A co-host would be someone who could assist me in planning for a successful event. It can be someone local near Sacramento or someone who can assist remotely. Having a co-host would help in making the event come to fruition and have the event be successful.\n\nIWC Dates:\n\nThe new plan is to host it during the fall or winter when the weather starts cooling off!\n\nLate September\n\nOctober\n\nNovember\n\nDecember\n\nIf you or someone else is interested in helping plan IndieWebCamp Sacramento, I'd love to have you! Send me an email by clicking here.\n\nLet's have this last half of the year be full of fun events and one of those can be in Sacramento! If you are interested in attending, more information will be available soon!\n\nAnyone and everyone is welcome to participate in the IndieWeb.\n\nThis blog post has been posted on IndieNews", "label": "non_personal"}
{"title": "Mexico Journal Entries in French", "url": "https://lifeofpablo.com/blog/mexico-journal-entries-in-french", "content": "Mexico Journal Entries in French\n\nIn a mountain in Oaxaca\n\nThis post was written in Français (fr_FR).\n\nGoing on a hike in Oaxaca - 2015\n\nThis is some writing I wrote in French while visiting my grandparents in 2015\n\nJour 1\n\nJe suis arrivé en Mexique.\n\nJour 2\n\nLe second jour de mes vacances était ennuyeux. Je me lève tôt. Je me suis levé en retard. Je prends une douche. Le jour commence. La dernière fois que je suis venu c'était il y a cinq jours avec ma mère et mes deux sœurs. Nous étions ici partout en été. Je n'ai vraiment pas fait beaucoup, juste aider ma grand-mère à cause de sa santé. Elle est très malade.\n\nJour 3\n\nJe crois que j'ai une nouvelle routine quand je me lève. Ceci est à cause du fait que je suis dans un nouvel endroit géographique. Je dois recommencer le forme d'habitude que les gens font tous les jours. Pour le petit-déjeuner, je mange beaucoup de pain. A côté de ça je bois du café. C' est un repas traditionnel. Il est vite mangé. Après ça nous avons tué une dinde pour manger un autre plat traditionnel. J'ai un nouvel ami. Il s'appelle Bryan. Ouais c'est un nom américain. Il va à l'école. Il est maintenant en vacances. C' est un garçon aimable. Les parents de Bryan sont vendeurs de glaces délicieuses. Ils vendent aussi du bois de beaucoup parfums. Je passe du temps avec lui. Je vais à sa maison. Nous regardons la télévision. Chez mes grands-parents n'ont pas de télé. Nous regardons les programmes américain en espagnol. J'aime aussi le programme mexicaine aussi. C' est un cool type!\n\nJour 4\n\nJe me suis encore levé. La même routine comme toujours. Ma routine ici est presque la même comme celle- là, aux États-Unis. J'ai fait la cuisine pour le déjeuner. J'ai préparé un repas italien. C'était \"chicken Alfredo\" avec pasta. J'ai enseigné mes tantes à faire. C'était délicieux. Mon grand-père et mes tantes l'aiment. Mais ma grand-mère ne l'aime pas. Elle est plus traditionnelle. Elle n'aime pas le change. Oh la la! Elle est compliquée. Vous n'avez pas idée. :'( . Après ça je suis sorti au centre du village. Je fais du shopping. Je rachète dès nourriture. Oh la la il faisait chaud. Je transpire beaucoup. Les tortas sont la version mexicaine d'un sandwich en comparaison entre les américain. Ils sont délicieux. Comme j'aime la nourriture mexicaine.", "label": "non_personal"}
{"title": "September 2024", "url": "https://lifeofpablo.com/blog/published:2024-09", "content": "fr\n\nGoing on a hike in Oaxaca - 2015\n\nThis is some writing I wrote in French while visiting my grandparents in 2015\n\nJour 1\n\nJe suis arrivé en Mexique.\n\nJour 2\n\nLe second jour de mes vacances était ennuyeux. Je me lève tôt. Je me suis levé en retard. Je prends une douche. Le jour commence. La dernière fois que je suis venu c'était il y a cinq jours avec ma mère et mes deux sœurs. Nous étions ici partout en été. Je n'ai vraiment pas fait beaucoup, juste aider ma grand-mère à cause de sa santé. Elle est très malade.\n\nJour 3\n\nJe crois que j'ai une nouvelle routine quand je me lève. Ceci est à cause du fait que je suis dans un nouvel endroit géographique. Je dois recommencer le forme d'habitude que les gens font tous les jours. Pour le petit-déjeuner, je mange beaucoup de pain. A côté de ça je bois du café. C' est un repas traditionnel. Il est vite mangé. Après ça nous avons tué une dinde pour manger un autre plat traditionnel. J'ai un nouvel ami. Il s'appelle Bryan. Ouais c'est un nom américain. Il va à l'école. Il est maintenant en vacances. C' est un garçon aimable. Les parents de Bryan sont vendeurs de glaces délicieuses. Ils vendent aussi du bois de beaucoup parfums. Je passe du temps avec lui. Je vais à sa maison. Nous regardons la télévision. Chez mes grands-parents n'ont pas de télé. Nous regardons les programmes américain en espagnol. J'aime aussi le programme mexicaine aussi. C' est un cool type!\n\nJour 4\n\nJe me suis encore levé. La même routine comme toujours. Ma routine ici est presque la même comme celle- là, aux États-Unis. J'ai fait la cuisine pour le déjeuner. J'ai préparé un repas italien. C'était \"chicken Alfredo\" avec pasta. J'ai enseigné mes tantes à faire. C'était délicieux. Mon grand-père et mes tantes l'aiment. Mais ma grand-mère ne l'aime pas. Elle est plus traditionnelle. Elle n'aime pas le change. Oh la la! Elle est compliquée. Vous n'avez pas idée. :'( . Après ça je suis sorti au centre du village. Je fais du shopping. Je rachète dès nourriture. Oh la la il faisait chaud. Je transpire beaucoup. Les tortas sont la version mexicaine d'un sandwich en comparaison entre les américain. Ils sont délicieux. Comme j'aime la nourriture mexicaine.", "label": "non_personal"}
{"title": "Deeper-Level Questions to Learn About People", "url": "https://lifeofpablo.com/blog/deeper-level-questions-to-learn-about-people", "content": "Deeper-Level Questions to Learn About People\n\nWhat can you do today that you couldn't do a year ago?\n\nThis post was written in English (en_US).\n\nIt's currently 1 am in Nebraska. I got bored, so I asked ChatGPT the following question.\n\nHello! Can you provide me 100 questions to learn more about people? Deeper level questions.\n\nI've been thinking about asking people different questions. These can be random people, family members, friends, colleagues, etc. I'm more of a listener.\n\nI also thought why not ask myself (or others) these questions and write about it. So instead of waiting for the New Year to start a new habit. Why not start now? I haven't blogged as much lately so this will incentivize me!\n\nI will find a way to answer these questions. I will do my best to do it in order but I'm okay with jumping around!\n\n100 Questions in 100 Days! Let's begin!\n\nWould you like to help me answer some of these questions? Please email me at pablo [at] lifeofpablo.com or send me a webmention! Make sure to tell me which question you'd like to discuss!\n\nKey:\n\nUnanswered 🔴: The question has not been addressed yet.\n\n🔴: The question has not been addressed yet. Answered ✅: The question has been answered.\n\n✅: The question has been answered. Link to Response 🔗: A link to the detailed response is available. (Hover or click the link for details.)\n\nPhilosophy & Mindset\n\nWhat is your definition of success? 🔴✅ 🔗 What motivates you to work hard? 🔴✅ 🔗 What’s the biggest risk you’ve ever taken? 🔴✅ 🔗 What would you do if you knew you could not fail? 🔴✅ 🔗 How do you define happiness? 🔴✅ 🔗 What are your top values? 🔴✅ 🔗 How do you handle failure? 🔴✅ 🔗 How do you stay focused on your goals? 🔴✅ 🔗 What is the best decision you’ve ever made? 🔴✅ 🔗 What does a perfect day look like for you? 🔴✅ 🔗\n\nPurpose & Motivation\n\nWhat drives you to succeed? 🔴✅ 🔗 What is your biggest passion in life? 🔴✅ 🔗 What’s the most important thing in life? 🔴✅ 🔗 What would you do with unlimited time and resources? 🔴✅ 🔗 How do you stay motivated during difficult times? 🔴✅ 🔗 What are the key factors that contribute to your success? 🔴✅ 🔗 What legacy do you want to leave behind? 🔴✅ 🔗 How do you find meaning in life? 🔴✅ 🔗 What makes you feel fulfilled? 🔴✅ 🔗 What does success mean to you? 🔴✅ 🔗\n\nHobbies & Interests\n\nWhat are your favorite hobbies? 🔴✅ 🔗 What activity do you do to relax? 🔴✅ 🔗 What is something you’ve always wanted to try but never have? 🔴✅ 🔗 What’s the best hobby you’ve picked up during quarantine? 🔴✅ 🔗 What’s your favorite way to spend a Saturday? 🔴✅ 🔗 What’s one hobby you’ve always been curious about? 🔴✅ 🔗 What kind of books do you like to read? 🔴✅ 🔗 What’s your favorite type of music? 🔴✅ 🔗 What do you enjoy doing when you need to clear your mind? 🔴✅ 🔗 What’s a new hobby you’ve picked up recently? 🔴✅ 🔗\n\nWell-being & Health\n\nWhat’s your favorite way to stay active? 🔴✅ 🔗 How do you take care of your mental health? 🔴✅ 🔗 How do you practice self-care? 🔴✅ 🔗 How do you manage stress? 🔴✅ 🔗 What’s your morning routine like? 🔴✅ 🔗 What’s your go-to comfort food? 🔴✅ 🔗 What do you do to maintain a healthy lifestyle? 🔴✅ 🔗 What do you do when you’re feeling overwhelmed? 🔴✅ 🔗 How do you stay energized throughout the day? 🔴✅ 🔗 What’s the best piece of advice you’ve received about health? 🔴✅ 🔗\n\nMiscellaneous & Fun\n\nWhat’s the most memorable vacation you’ve been on? 🔴✅ 🔗 What’s your favorite season of the year? 🔴✅ 🔗 What’s one place you’d love to visit? 🔴✅ 🔗 What’s your favorite meal of the day: breakfast, lunch, or dinner? 🔴✅ 🔗 What’s the most adventurous thing you’ve ever done? 🔴✅ 🔗 If you could switch lives with anyone for a day, who would it be? 🔴✅ 🔗 What’s your idea of a perfect day? 🔴✅ 🔗 What’s the best movie you’ve seen recently? 🔴✅ 🔗 What’s something you’ve learned that you wish you knew earlier? 🔴✅ 🔗 What’s your favorite way to unwind after a long week? 🔴✅ 🔗\n\nDreams & Ambitions\n\nWhat’s your biggest dream in life? 🔴✅ 🔗 Where do you see yourself in 10 years? 🔴✅ 🔗 What’s the most important thing you hope to achieve in the next year? 🔴✅ 🔗 What’s the best way to set and achieve goals? 🔴✅ 🔗 If you could change one thing about your life, what would it be? 🔴✅ 🔗 What would you do if you knew you had no time limit to achieve your dreams? 🔴✅ 🔗 What’s one goal you’ve set that you’ve accomplished? 🔴✅ 🔗 What’s one dream you hope to fulfill in the next five years? 🔴✅ 🔗 What inspires you to keep going, even when things get tough? 🔴✅ 🔗 How do you stay motivated when faced with obstacles? 🔴✅ 🔗\n\nDreams & Spirituality\n\nWhat do you believe happens after we die? 🔴✅ 🔗 What is your biggest spiritual belief? 🔴✅ 🔗 How has your spiritual journey shaped your life? 🔴✅ 🔗 Do you believe in life after death? 🔴✅ 🔗 What role does faith play in your life? 🔴✅ 🔗 Do you believe that everything happens for a reason? 🔴✅ 🔗 What’s your view on karma? 🔴✅ 🔗 What’s the most profound spiritual experience you’ve had? 🔴✅ 🔗 How do you define enlightenment or spiritual awakening? 🔴✅ 🔗 How do you reconcile science and spirituality? 🔴✅ 🔗\n\nGoals & Success\n\nWhat’s your definition of success? 🔴✅ 🔗 How do you measure your success? 🔴✅ 🔗 What’s the most important goal you’re currently working on? 🔴✅ 🔗 What’s the best piece of advice you’ve received about setting goals? 🔴✅ 🔗 How do you overcome setbacks when pursuing your goals? 🔴✅ 🔗 What does it take to be successful in life? 🔴✅ 🔗 What’s the most important trait you think is necessary to succeed? 🔴✅ 🔗 How do you keep yourself motivated when things aren’t going as planned? 🔴✅ 🔗 What’s a personal goal you’ve set for the next year? 🔴✅ 🔗 How do you stay focused on your long-term goals? 🔴✅ 🔗\n\nMotivation & Strategy\n\nWhat motivates you to take action towards your dreams? 🔴✅ 🔗 How do you maintain consistency when working towards your goals? 🔴✅ 🔗 What’s your strategy for overcoming procrastination? 🔴✅ 🔗 How do you prioritize tasks when you have a lot to do? 🔴✅ 🔗 What’s your approach to time management? 🔴✅ 🔗 How do you stay disciplined when working towards your goals? 🔴✅ 🔗 What’s the most effective habit you’ve developed to stay motivated? 🔴✅ 🔗 What’s one thing you do every day that contributes to your success? 🔴✅ 🔗 How do you stay positive and motivated when things aren’t going your way? 🔴✅ 🔗 What’s your number one piece of advice for staying motivated? 🔴✅ 🔗\n\nFriendship & Support", "label": "non_personal"}
{"title": "Read The Arts and Crafts Movement – Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/24/read-the-arts-and-crafts-movement/", "content": "Read The Arts and Crafts Movement: A Study of Its Sources, Ideals and Influence on Design Theory by Gillian Naylor\n\nLots of original quotes are interspersed, from a variety of sources. The text itself sometimes got lost in facts, dates, and sequences; it was best where it narrativized and provided high level analysis of trends. Some of the quotes are 🔥\n\n“The movement… represents in some sense a revolt against the hard mechanical conventional life and its insensibility to beauty (quite another thing to ornament). It is a protest against that so-called industrial progress which provides shoddy wares, the cheapness of which is paid for by the lives of their producers and the degradation of their users.”\n\n— Walter Crane, The Revival of Design and Handicraft (orig. pg. 12)\n\nI found the layout of the text frustratingly uncomfortable to read. I read through page 145 (195 pages of content before endnotes).\n\nGraphics\n\nInteresting and wide-ranging collection of sample works included (chiefly black and white unfortunately) — though I was frustrated at least twice when a specific piece would be described in the text but not pictured.\n\nNotes and Quotes\n\nMartin Wiener – English Culture and the Decline of the Industrial Spirit\n\nDesign theory and categorization of ornament = related to industrialization of design\n\n“The Arts and Crafts movement was inspired by a crisis of conscience. Its motivations were social and moral, and its aesthetic values derived from the conviction that society produces the art and architecture it deserves.” (from 1989 preface by Gillian Naylor)\n\n“Averting mankind’s enslavement to the machine by saving the mass product and the home from mechanical anarchy and by restoring them to purpose, sense and life.”\n\n— Walter Gropius, The Scope of Total Architecture\n\ncontemporary critique from Thorstein Veblen 1899: idolizing the handmade –> conspicuous consumption — “propaganda of crudity” describing the “exaltation of the defective” aka imperfections in handwork (aestheticization!!!)\n\nthey don’t write insults like this anymore lol: “disencumber yourselves of the lymphatic ideology of your deplorable Ruskin” –Marinetti, 1912\n\n1835 committee to figure out how to give craftspeople a design sense and *taste* since consumers preferred imported aesthetics\n\nRuskin thought a craft and its society were inextricable\n\n“For it is not the material, but the absence of human labor, which makes the thing worthless, and a piece of terracotta, or plaster of paris, which has been wrought by the human hand, is worth all the stone and Carrara cut by machinery. It is, indeed, possible and even usual, for men to sink into machines themselves, so that even handwork has all the character of mechanization.”\n\n— Ruskin, ‘The Lamp of Truth’ from The Seven Lamps of Architecture\n\nThe Stones of Venice = key book to the movement — especially the essay “Nature of Gothic” (here as printed by William Morris)\n\n“It is not that men are ill-fed, but that they have no pleasure in the work by which they make their bread, and therefore look to wealth as the only means of pleasure.”\n\n— Ruskin, ‘Nature of Gothic’ from The Stones of Venice\n\nRuskin into destigmatizing manual labor\n\nsociety’s wealth measured in human happiness and its works of art\n\nthis is grouping Burne-Jones in with Arts and Crafts rather than Pre-Raphaelite\n\n“It was just a commonplace thing handled imaginatively, and it gave me as much pleasure as anything in the exhibition. It made me feel that it takes a big man to do a simple thing.”\n\n— architect John Sedding, about a piece of furniture designed by Ford Madox Brown\n\n“‘Art’ to them meant individuality and the search for ‘truth’, whether in painting, architecture or applied design — and truth, they felt, could be found both in the study of nature, and in the recreation of the spirit rather than the letter of mediaevalism.”\n\nduality of “straightforward, honest craftsmanship” and “mid-nineteenth-century ornamental conventions”\n\n“cardinal principle” = know your materials and learn the craft directly\n\nMorris thought pattern design should hold meaning:\n\n“do not introduce any lines or objects which cannot be explained by the structure of the pattern; it is just this logical sequence of form, this growth which looks as if, under the circumstances, it could not have been otherwise, which prevents the eye wearying of the repetition of the pattern.”\n\nconflict between Morris’ love of craftsmanship and the expense of producing quality goods blocking most people from accessing it led to him becoming a Socialist\n\n“[I]t is the allowing of machines to be our masters, and not our servants, that so injures the beauty of life nowadays.” — William Morris\n\nArts and Crafts accepted both “simple and luxurious” — “sprang from the ideal of the craftsman as artist, and from the belief in individualism and individual commitment”\n\nSee also: Read Liberty: British Colour Pattern\n\nRead In Harmony with Nature", "label": "non_personal"}
{"title": "Read This Will Be Fun – Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/21/read-this-will-be-fun/", "content": "The Princess Bride meets People We Meet on Vacation in this cozy quest romantasy about a group of friends who once defended their magical land together but haven’t spoken since, reuniting to attend a royal wedding, and ending up on a new adventure to save the realm—and hopefully themselves.\n\nTen years ago, they saved the realm. It ruined their lives.\n\nEveryone in Mythria knows the story of how best friends Beatrice and Elowen, handsome ex-bandit Clare, and valiant leader Galwell the Great defended the land from darkness. It’s a tale beloved by all—except the former heroes. They haven’t spoken in a decade, devastated by what their quest cost them.\n\nBut when they receive an invitation to the queen of Mythria’s wedding, it’s a summons they can’t refuse . . . and a reunion for the ages, with Clare secretly not over his long-ago fling with Beatrice, Beatrice fighting the guilt she feels over how everything ended, Elowen unprepared for the return of her former flame (the cunning Vandra), and all of them lost without Galwell’s presence. And if reuniting with old friends and lovers wasn’t perilous enough, dark forces from their past have returned, plotting a domination that only Mythria’s one-time defenders can stop. Maybe.\n\nDusting off old weapons and old instincts, they face undead nemeses, crystal caves, enchanted swords, coffee shops, games of magical Truth or Dare, and, hardest of all, their past—rife with wounds never healed and romances never forgotten.\n\nThis time around, will their story end in happily ever after?", "label": "non_personal"}
{"title": "The Unbelievable Scale of AI’s Pirated-Books Problem", "url": "https://www.theatlantic.com/technology/archive/2025/03/libgen-meta-openai/682093/", "content": "Updated at 5:40 p.m. ET on March 21, 2025\n\nEditor’s note: This analysis is part of The Atlantic’s investigation into the Library Genesis data set. You can access the search tool directly here. Find The Atlantic’s search tool for movie and television writing used to train AI here.\n\nWhen employees at Meta started developing their flagship AI model, Llama 3, they faced a simple ethical question. The program would need to be trained on a huge amount of high-quality writing to be competitive with products such as ChatGPT, and acquiring all of that text legally could take time. Should they just pirate it instead?\n\nMeta employees spoke with multiple companies about licensing books and research papers, but they weren’t thrilled with their options. This “seems unreasonably expensive,” wrote one research scientist on an internal company chat, in reference to one potential deal, according to court records. A Llama-team senior manager added that this would also be an “incredibly slow” process: “They take like 4+ weeks to deliver data.” In a message found in another legal filing, a director of engineering noted another downside to this approach: “The problem is that people don’t realize that if we license one single book, we won’t be able to lean into fair use strategy,” a reference to a possible legal defense for using copyrighted books to train AI.\n\nThis article was featured in the One Story to Read Today newsletter. Sign up for it here.\n\nCourt documents released last night show that the senior manager felt it was “really important for [Meta] to get books ASAP,” as “books are actually more important than web data.” Meta employees turned their attention to Library Genesis, or LibGen, one of the largest of the pirated libraries that circulate online. It currently contains more than 7.5 million books and 81 million research papers. Eventually, the team at Meta got permission from “MZ”—an apparent reference to Meta CEO Mark Zuckerberg—to download and use the data set.\n\nThis act, along with other information outlined and quoted here, recently became a matter of public record when some of Meta’s internal communications were unsealed as part of a copyright-infringement lawsuit brought against the company by Sarah Silverman, Junot Díaz, and other authors of books in LibGen. Also revealed recently, in another lawsuit brought by a similar group of authors, is that OpenAI has used LibGen in the past. (A spokesperson for Meta declined to comment, citing the ongoing litigation against the company. In a response sent after this story was published, a spokesperson for OpenAI said, “The models powering ChatGPT and our API today were not developed using these datasets. These datasets, created by former employees who are no longer with OpenAI, were last used in 2021.”)\n\nUntil now, most people have had no window into the contents of this library, even though they have likely been exposed to generative-AI products that use it; according to Zuckerberg, the “Meta AI” assistant has been used by hundreds of millions of people (it’s embedded in Meta products such as Facebook, WhatsApp, and Instagram). To show the kind of work that has been used by Meta and OpenAI, I accessed a snapshot of LibGen’s metadata—revealing the contents of the library without downloading or distributing the books or research papers themselves—and used it to create an interactive database that you can search here:\n\nThere are some important caveats to keep in mind. Knowing exactly which parts of LibGen that Meta and OpenAI used to train their models, and which parts they might have decided to exclude, is impossible. Also, the database is constantly growing. My snapshot of LibGen was taken in January 2025, more than a year after it was accessed by Meta, according to the lawsuit, so some titles here wouldn’t have been available to download at that point.\n\nLibGen’s metadata are quite disorganized. There are errors throughout. Although I have cleaned up the data in various ways, LibGen is too large and error-strewn to easily fix everything. Nevertheless, the database offers a sense of the sheer scale of pirated material available to models trained on LibGen. Cujo, The Gulag Archipelago, multiple works by Joan Didion translated into several languages, an academic paper named “Surviving a Cyberapocalypse”—it’s all in here, along with millions of other works that AI companies could feed into their models.\n\nMeta and OpenAI have both argued in court that it’s “fair use” to train their generative-AI models on copyrighted work without a license, because LLMs “transform” the original material into new work. The defense raises thorny questions and is likely a long way from resolution. But the use of LibGen raises another issue. Bulk downloading is often done with BitTorrent, the file-sharing protocol popular with pirates for its anonymity, and downloading with BitTorrent typically involves uploading to other users simultaneously. Internal communications show employees saying that Meta did indeed torrent LibGen, which means that Meta could have not only accessed pirated material but also distributed it to others—well established as illegal under copyright law, regardless of what the courts determine about the use of copyrighted material to train generative AI. (Meta has claimed that it “took precautions not to ‘seed’ any downloaded files” and that there are “no facts to show” that it distributed the books to others.) OpenAI’s download method is not yet known.\n\nMeta employees acknowledged in their internal communications that training Llama on LibGen presented a “medium-high legal risk,” and discussed a variety of “mitigations” to mask their activity. One employee recommended that developers “remove data clearly marked as pirated/stolen” and “do not externally cite the use of any training data including LibGen.” Another discussed removing any line containing ISBN, Copyright, ©, All rights reserved. A Llama-team senior manager suggested fine-tuning Llama to “refuse to answer queries like: ‘reproduce the first three pages of “Harry Potter and the Sorcerer’s Stone.”’” One employee remarked that “torrenting from a corporate laptop doesn’t feel right.”\n\nIt is easy to see why LibGen appeals to generative-AI companies, whose products require huge quantities of text. LibGen is enormous, many times larger than Books3, another pirated book collection whose contents I revealed in 2023. Other works in LibGen include recent literature and nonfiction by prominent authors such as Sally Rooney, Percival Everett, Hua Hsu, Jonathan Haidt, and Rachel Khong, and articles from top academic journals such as Nature, Science, and The Lancet. It includes many millions of articles from top academic-journal publishers such as Elsevier and Sage Publications.\n\nRead: These 183,000 books are fueling the biggest fight in publishing and tech\n\nLibGen was created around 2008 by scientists in Russia. As one LibGen administrator has written, the collection exists to serve people in “Africa, India, Pakistan, Iran, Iraq, China, Russia and post-USSR etc., and on a separate note, people who do not belong to academia.” Over the years, the collection has ballooned as contributors piled in more and more pirated work. Initially, most of LibGen was in Russian, but English-language work quickly came to dominate the collection. LibGen has grown so quickly and avoided being shut down by authorities thanks in part to its method of dissemination. Whereas some other libraries are hosted in a single location and require a password to access, LibGen is shared in different versions by different people via peer-to-peer networks.\n\nMany in the academic world have argued that publishers have brought this type of piracy on themselves, by making it unnecessarily difficult and expensive to access research. Sci-Hub, a sibling of LibGen, was launched independently in 2011 by a Kazakhstani neuroscience student named Alexandra Elbakyan, whose university didn’t provide access to the big academic databases. In that same year, the hacktivist Aaron Swartz was arrested after taking millions of articles from JSTOR in an attempt to build a similar kind of library.\n\nPublishers have tried to stop the spread of pirated material. In 2015, the academic publisher Elsevier filed a complaint against LibGen, Sci-Hub, other sites, and Elbakyan personally. The court granted an injunction, directed the sites to shut down, and ordered Sci-Hub to pay Elsevier $15 million in damages. Yet the sites remained up, and the fines went unpaid. A similar story played out in 2023, when a group of educational and professional publishers, including Macmillan Learning and McGraw Hill, sued LibGen. This time the court ordered LibGen to pay $30 million in damages, in what TorrentFreak called “one of the broadest anti-piracy injunctions we’ve seen from a U.S. court.” But that fine also went unpaid, and so far authorities have been largely unable to constrain the spread of these libraries online. Seventeen years after its creation, LibGen continues to grow.\n\nRead: There’s no longer any doubt that Hollywood writing is powering AI\n\nAll of this certainly makes knowledge and literature more accessible, but it relies entirely on the people who create that knowledge and literature in the first place—that labor that takes time, expertise, and often money. Worse, generative-AI chatbots are presented as oracles that have “learned” from their training data and often don’t cite sources (or cite imaginary sources). This decontextualizes knowledge, prevents humans from collaborating, and makes it harder for writers and researchers to build a reputation and engage in healthy intellectual debate. Generative-AI companies say that their chatbots will themselves make scientific advancements, but those claims are purely hypothetical.\n\nOne of the biggest questions of the digital age is how to manage the flow of knowledge and creative work in a way that benefits society the most. LibGen and other such pirated libraries make information more accessible, allowing people to read original work without paying for it. Yet generative-AI companies such as Meta have gone a step further: Their goal is to absorb the work into profitable technology products that compete with the originals. Will these be better for society than the human dialogue they are already starting to replace?", "label": "non_personal"}
{"title": "tech industry – Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/tag/tech-industry/", "content": "This feels like a sister piece to Ed Zitron’s essay Era of the Business Idiots and Mandy Brown’s essay Toolmen. Fair warning, this is a 5000 word post; I’ve been working on this for weeks, pulling together what I’ve learned about generative AI and culture over the past two years, so I hope it is worth your time 😄 Bonus: it doubles as a playlist 🎶\n\n“‘Real power’ is achieved when a technology ‘[leaves] mythology and [enters] banality,'” Marion Fourcade and Kieran Healy quote Vincent Mosco in The Ordinal Society. We’ve had the mythology stage — the world tour with grandiose prophecies of imminent AGI — but now the race to normalize generative AI* is on: tech corporations are attempting to inure people to generative AI, an expression of the Business Borg aesthetic that currently carries a negative stigma outside of tech.\n\n*(My rule of thumb: if something is described as AI, it’s probably predatory and/or bullshit; if it’s described as machine learning, it probably does something useful. Not always true but a helpful predictor.)\n\nIn general, people like what we recognize better than what we don’t — we prefer cultural works we can categorize to the unfamiliar and undefinable — and we are facing an inescapable shock-and-awe barrage of genAI graphics across the web to inundate our synapses with uncanny synthetic renderings.\n\nCurrently, generative AI is shunned by many artists and writers, the traditional arbiters of good taste and culture, because it has been developed through the theft of their labor. But tech CEOs stand to make (even bigger) fortunes if they can convince people that genAI doesn’t signify bad taste, or make it seem like an irrevocable fact of life, like spam emails and text scammers. It’s being deployed upon us with the same lockstep corporate solidarity that forced us to pay fees for checked luggage on flights (younger folks, before 2008 your bag used to be included with your ticket! Stowing your carry-on wasn’t a competitive sport back in the day.).", "label": "non_personal"}
{"title": "Generative AI is intellectual sharecropping – Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2023/09/25/generative-ai-is-intellectual-sharecropping/", "content": "Replied to Digital sharecropping by Nicholas Carr ( roughtype.com ) One of the fundamental economic characteristics of Web 2.0 is the distribution of production into the hands of the many and the concentration of the economic rewards into the hands of the few. It’s a sharecropping system, but the sharecroppers are generally happy because their interest lies in self-expression or socializing, not in making money, and, besides, the economic value of each of their individual contributions is trivial. It’s only by aggregating those contributions on a massive scale – on a web scale – that the business becomes lucrative. To put it a different way, the sharecroppers operate happily in an attention economy while their overseers operate happily in a cash economy.\n\nI encountered this (old) analogy for social media platforms as digital sharecropping and thought it also fit generative AI. Generative AI companies steal our intellectual property then license it back to us. We can’t be compensated reasonably for our individual contributions to the model because they’ve stolen from so many of us and each individual’s work represents a miniscule portion of the entire model. Whatever we generate with their models can’t be copyrighted and used to make money for *us* without significant human contributions — but generated works are in direct competition with the creators whose works built the model. These powerful, well-funded companies want businesses to fire their employees and pay them instead, making businesses reliant on an opaque, unpredictable service that demands vast amounts of natural resources that may be in short — and shortening — supply.\n\nBut unlike social media, which rewards users emotionally rather than financially for their labor, creators aren’t getting anything out of having our work used to train generative AI models. And so we’re fighting back earlier in the cycle than with social media — maybe before it can become entrenched. AI evangelists speak as if the technology’s supremacy is inevitable, but that’s propaganda to get us to shut up and hand over our creations and our jobs.\n\nRecent AI shenanigans in the news:\n\nChatGPT caught giving horrible advice to cancer patients by Sharon Adarlo (Neoscope)\n\nWhy Silicon Valley’s biggest AI developers are hiring poets by Andrew Deck (rest of world)\n\nIowa Is Using ChatGPT to Take Out Banned Books by Alejandra Gularte (Vulture)\n\nGenerative AI at work", "label": "non_personal"}
{"title": "Generated content is an invasive species in the online ecosystem – Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2024/01/09/generated-content-is-an-invasive-species-in-the-online-ecosystem/", "content": "Invasive species disrupt ecosystems because they did not evolve in balance with the other species. Native species have adapted to fill specific niches, but the constraints they’ve accepted to fit that niche in the ecosystem do not also bind invasive species. Not limited by the same factors, they reproduce faster and crowd out the native species. Time and again, we’ve seen invasive species introduced to an ecosystem outcompete the more specialized native species, sometimes even driving them to extinction.\n\nLikewise, generated imagery and text is not bound by human limitations of productivity. As generated material rapaciously populates the Internet, human-created artworks will be outcompeted by generated graphics on social media platforms by virtue of volume.\n\nAnd corporations are also trying to argue that their products should not be bound by the same legalities that human artists and writers are bound by. Their products only work with copyrighted material, and that means it’s only economically viable if they steal the training data. Like invasive species, they don’t play by the same rules: the rest of us peons must wait 95 years to play with fucking Steamboat Willie, but they get to gobble down anything they want for free instantly and use it to (try to) drive us out of work.\n\nLet’s starve out this species invasion before it collapses our information ecosystem ✊\n\nSee also:\n\nA 21st century collage tool\n\nWe need solidarity across creative industries\n\nGenerative AI is intellectual sharecropping", "label": "non_personal"}
{"title": "Generative AI and the Business Borg aesthetic – Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/02/generative-ai-and-the-business-borg-aesthetic/", "content": "This feels like a sister piece to Ed Zitron’s essay Era of the Business Idiots and Mandy Brown’s essay Toolmen. Fair warning, this is a 5000 word post; I’ve been working on this for weeks, pulling together what I’ve learned about generative AI and culture over the past two years, so I hope it is worth your time 😄 Bonus: it doubles as a playlist 🎶\n\n“‘Real power’ is achieved when a technology ‘[leaves] mythology and [enters] banality,'” Marion Fourcade and Kieran Healy quote Vincent Mosco in The Ordinal Society. We’ve had the mythology stage — the world tour with grandiose prophecies of imminent AGI — but now the race to normalize generative AI* is on: tech corporations are attempting to inure people to generative AI, an expression of the Business Borg aesthetic that currently carries a negative stigma outside of tech.\n\n*(My rule of thumb: if something is described as AI, it’s probably predatory and/or bullshit; if it’s described as machine learning, it probably does something useful. Not always true but a helpful predictor.)\n\nIn general, people like what we recognize better than what we don’t — we prefer cultural works we can categorize to the unfamiliar and undefinable — and we are facing an inescapable shock-and-awe barrage of genAI graphics across the web to inundate our synapses with uncanny synthetic renderings.\n\nCurrently, generative AI is shunned by many artists and writers, the traditional arbiters of good taste and culture, because it has been developed through the theft of their labor. But tech CEOs stand to make (even bigger) fortunes if they can convince people that genAI doesn’t signify bad taste, or make it seem like an irrevocable fact of life, like spam emails and text scammers. It’s being deployed upon us with the same lockstep corporate solidarity that forced us to pay fees for checked luggage on flights (younger folks, before 2008 your bag used to be included with your ticket! Stowing your carry-on wasn’t a competitive sport back in the day.).\n\nThe aesthetics of generative AI\n\n“I have become momentarily obsessed with scrolling down the homepage of the MetaAI tool and seeing the infinite feed of what people have been asking of The Machine. The outputs are horribly banal, but the requests are a weird window into THE (NORMIE) HUMAN SOUL AND ITS DESIRES: www.meta.ai”\n\n— Matt Muir, May 6, 2025 at 8:51 AM\n\n(via)\n\nBy its nature, generative AI produces the most likely image that meets the brief, which devolves to insipid Pictionary-style visual communication: chonky = cat, kebab = food = French chef’s hat. These graphics are iconographic pablum, the uninspired result of gee-whiz curiosity about a new “tool” (toy)(trap) in an environment that discourages personal taste and cultural literacy.\n\nOriginally, I wrote up why I find these particular graphics tacky and visually uninteresting, but realized that ultimately, what they look like doesn’t matter — it’s the beliefs beneath the appearance that matter. As corporate models are trained further, Generative AI will probably continue to get better, rendering more attractive and/or plausible outputs, but it won’t matter to me how good it gets because I reject the values it represents.\n\n(NB: I don’t want you to feel bad about yourself if you use generative AI, because there are myriad reasons to have experimented with it, including being forced to for your employment; I want you to recognize what it symbolizes when you enthusiastically use this technology today, and make an informed choice about whether those are values you wish to signal to others.)\n\n“What is wrong with a counterfeit is not what it is like, but how it was made.”\n\n— Harry G. Frankfurt, On Bullshit\n\nAesthetics are looks that signal values\n\nWitching Hour by Ladytron\n\nAn aesthetic is an expression of taste for shared values, commonly communicated through a distinct style. We think of aesthetics as surface appearance only, but the formation of an aesthetic’s conventions reflects the why and the how underneath the what. Just as the medium of a thing carries a message, so does its aesthetic. Aesthetics are visually and verbally encoded value systems.\n\nIdeology is a value system, independent of appearances. Aesthetics are the appearance of an ideology, which grow from its values. Subcultures form around ideologies, with members signaling their participation through aesthetics. “Good taste” is aesthetics that express those values.\n\nThough it’s now often reduced to a visual style, the Arts and Crafts movement of the late 1800s and early 1900s was cross-disciplinary and united instead by an ethos — namely, the nobility of craftsmanship*:\n\n“For it is not the material, but the absence of human labor, which makes the thing worthless, and a piece of terracotta, or plaster of paris, which has been wrought by the human hand, is worth all the stone and Carrara cut by machinery.” — John Ruskin, The Lamp of Truth from The Seven Lamps of Architecture\n\nAccording to the Arts and Crafts aesthetic, what is made should signal how it is made — the aesthetic’s value system weights how something is made to be as important, if not more, as what is made. Surface appearance is borne of the decisions this taste for craft produces.\n\n*(People might instead think of this William Morris quote as the quintessential perspective of Arts and Crafts — “Have nothing in your house that you do not know to be useful, or believe to be beautiful.” — and I suspect there’s a reason we’ve been taught to recall a philosophy centered on material possession instead of labor.)\n\nLooking at a Kelmscott Press book (William Morris’ printing company) reveals the printer’s respect for handicraft: they designed custom drop letters and frames, included original illustrations by fine artists like Edward Burne-Jones, used original type modeled after typefaces used by printers like Aldus Manutius in the early days of the printing press, printed with a richer black ink than was standard at the time, and employed a heavy hand in letterpress printing so the design and type would be impressed into the page. Thoughtful, intentional ornamentation is embraced. The artifact itself is a thing to be appreciated as much as its contents; these are books that honor the integrity of all creative workers involved in their production.\n\nEmily Amick applies this analytical lens to reveal the tradwife aesthetic’s underlying values:\n\nPrairie-core. Domestic bliss. Big sleeves. Bigger sourdough starters. And beneath it all, the subtle (or not-so-subtle) message: a woman’s place is in the kitchen. But not because she wants to be there, because that’s where God or her husband or some TikTok algorithm put her. The tradwife aesthetic promises comfort, but it delivers control. It’s softness as a strategy. It’s anti-feminism with a floral filter. It’s nostalgia for a time when women were property, romanticized by influencers who want brand deals from butter. […] The tradwife says: give up your autonomy and someone else will take care of everything.\n\nWe adopt aesthetics based on aspirational values\n\nJessica Cullen writes that (emphasis mine): “Aesthetics aren’t always about who we currently are but rather who we want to be.” People adopt an aesthetic to say something about themselves to others. We intentionally adopt a particular subculture’s aesthetics to convey our belonging and raise our status within the subculture. As Alec Leach puts it, “a lot of the modern taste economy is actually the status economy.”\n\nSometimes, we perceive only the surface level of an aesthetic, its appearance without the values — as Amick notes, the tradwife aesthetic “looks so damn pretty and nourishing. And we are tired.” — but whenever we adopt an aesthetic, we endorse (intentionally or not) the underlying values it represents. Amick continues dissecting the tradwife aesthetic and how it serves as conservative propaganda:\n\nWhat looks like innocent lifestyle content is actually part of an organized political movement designed to make patriarchy look cozy and appealing. Because politics is downstream from culture. The vibes that influence how we act and live.\n\nAesthetics matter more than ever because we act in accordance with our chosen aesthetics. As we get more and more of our “cultural” content on corporate silos, politics and purchases have subsumed a lot of cultural tastemaking. In this TikTok, Jamelle Bouie describes how politicians use aesthetic signaling to appeal to voters. Richard Sennett identifies that politicians come to embody “intentions, desires, values, beliefs, tastes – an emphasis which has again the effect of divorcing power from responsibility.”\n\nAnu Atluru argues that “Aesthetics are the modern units of cultural currency—stores of value and instruments of power, capable of appreciating and being monetized at scale. Owning an aesthetic is owning influence.” (emphasis mine)\n\nInterrogating the Business Borg aesthetic\n\nGlass Lux by Glass Lux\n\nThe Arts and Craft movement’s respect for labor inspired stylistic choices that highlighted craftwork as well as the decisions in what goods to produce and how. So how does the Business Borg aesthetic reveal its values?\n\nTo define the Business Borg aesthetic, I’m looking at:\n\nthe values I see expressed through generative media,\n\nI see expressed through generative media, the actions of the corporations behind generative AI,\n\nof the corporations behind generative AI, who buys into generative AI, and\n\nbuys into generative AI, and what else they like that reflects the same underlying values.\n\nWhat genAI is better at than a human artist is being cheap and instant.\n\nThe Business Borg aesthetic uses technology to signal wealth and power. Generative AI is not the only visual expression of the Business Borg aesthetic, just its most recognizable. The aesthetic is also signified visually by CGI-heavy blockbuster franchises, NFT art, and the Cybertruck; and in text by LinkedIn corporate thirst traps, X braggadocio, SEO word vomit, and generated “answers” to search results.\n\nAs political bedpartners, there is overlap between the Business Borg aesthetic and the MAGA aesthetic, but they’re distinct viewpoints. Both share dominance as a core value, decry empathy, center patriarchy, and admire performance — but MAGA also signals Judeo-Christian morality, traditional beauty standards / traditional gender roles, hyper masculinity / violence, and nationalism. MAGA borrows aesthetics from golden pasts, like Neo-Classical architecture, tradwives, and, as Kate Wagner brands it, Regional Car Dealership Rococo; Business Borg prefer the more modern tones of cyberpunk, solarpunk, and minimalism. Business Borg are regulatory libertarians who envision themselves as the rightful leaders of society, Kings of techno-city-states; MAGA are Christian nationalists who want to use the power of the state to impose their beliefs on others.\n\nElon is a Business Borg at heart but wielded a chainsaw to appeal to the more violent MAGA aesthetic. Zuckerberg is a Business Borg but got a MAGA makeover with masculine stubble and bling.\n\nWhy am I naming this after the Borg? Like Star Trek’s Borg, this is an aesthetic rooted in extractive consumption, assimilationist dominance, neo-colonial expansionism, self-righteous conviction, reductionist thinking, and proclamations of inevitability. It idolizes technology, often inspired by older science-fiction, and draws on cyberpunk aesthetics. The Silicon Valley Collective values groupthink and believes themselves superior to “the other.”\n\nWho embraces this aesthetic\n\nNot all users of Generative AI embrace the Business Borg aesthetic. I think a lot of people are experimenting with generative AI out of neophilic curiosity, productivity imperatives, nihilistic determinism, and corporate fiat. Aspiring billionaires adopt the Business Borg aesthetic to signal their belonging in the cohort of the techno-rich.\n\nGenAI evangelists seem to be the same type of person who was into passive income and supplements fifteen years ago, then Soylent and SEO ten years ago, then NFTs and macro diets five years ago, now genAI and X blue checks.\n\nThe Business Borg aesthetic combines tech-centered neophilia, a hustle mindset, an obsession with optimization, evangelical fervor, and fake-it-till-you-make-it showmanship.\n\nThe Elon fanclub are Business Borg. Ed Zitron’s Business Idiot shares a lot of characteristics with Business Borg (and may even be the same group, but I think feels a little different?).\n\nThe subculture emphasizes high profile demonstrations of “winning“ — using a $10k NFT as their X profile pic, bragging about SEO heists ripping off a competitor, generosity stunts a la Mr Beast, rubbing it in Miyazaki’s face that there will be thousands of shitty knockoffs vaguely reminiscent of his work across the web.\n\nBusiness Borg signal their busyness — and importance — by broadcasting how little they sleep, how much they work, and how little they read. The only fiction they like is (old) sci-fi because they read it as non-fiction, not fiction — a source of “inspiration” stripped of context and commentary. Using GenAI signals their adoption of cutting edge technology, the synthetic smoothness emphasizing its nonhuman origin. They care a lot about IQ, a supposedly impartial measure of intelligence that rewards their backgrounds and thinking styles, and idolize “genius.” They’re not actually neurodivergent, but play on stereotypes of autistic savants to cover for their pathological greed and lack of empathy.\n\nTheir visual and linguistic taste is mid because taste is not valued in the Business Borg aesthetic. In fact, there’s a certain pride in prompting things without having any skill, an almost gleeful snub to the perceived cultural gatekeepers — artists and writers and other creative workers — whose opinions the Business Borg disrespect because they believe that authority derives from money, not knowledge. They believe artists have wasted their time learning skills and developing taste. Academics have wasted their time studying things when information is just a click away. Business Borg don’t care about anything besides making money, and don’t care much how they spend it because the point is to have it, and show off that they have it.\n\nInto The Water by Ritual Howls\n\nGenAI True Believers often resemble the CEOs at the head of tech companies: wealthy, male, and white. These are also the people who are least at threat from the widespread use of generative AI, which reinforces racial and gender stereotypes and purports neutrality while serving up right-wing biases and corporate and foreign propaganda. Audrey Watters points out:\n\n“Computing (in general and ed-tech specifically) has long been the bastion of white male privilege; and while there had been efforts to change that – in pipelines and on panels and whatnot – AI is clearly a re-entrenchment of that power, explicitly so with the Trump Administration’s dismantling of civil rights protections, echoed by the tech industry’s dismantling of its own DEI initiatives.”\n\nAnil Dash describes “AI-first” as this year’s “Return to Office.” Managers didn’t care about in-office culture until it was made clear that workers could carry on just fine without them; managers care about AI only insomuch as it permits controlling — and firing — workers.\n\nWhat the Business Borg aesthetic represents is more important than its appearance; it represents the dominance of ordinal thinking and the ability of moneyed power to do as it wishes without regard to law or morality — in short, the hierarchical worldview that some people are better than others and that their preferences trump their lesser’s needs.\n\n“We will add your biological and technological distinctiveness to our own. Your culture will adapt to service us. Resistance is futile.” — the Borg\n\nValues driving the Business Borg aesthetic\n\nTeri Kanefield breaks down Leor Zmigrod’s book on ideology, explaining that “All ideologies seek a utopia.” The Business Borg utopia puts billionaires and their ilk high atop society, in control via the technology they own.\n\nCore values I see uniting the Business Borg aesthetic are:\n\nonly the output matters efficiency is king quantity over quality appearance trumps reality “progress” cannot be stopped\n\nValue: Only the output matters\n\nGenerative AI is being marketed to businesses as a low-cost replacement for workers that cuts steps — and collaboration — out of the process. This is a box-checking culture; all that matters is that an email was sent, a presentation was created, the newspaper had a summer reading insert, no matter the books on it were imaginary.\n\nFoundational beliefs\n\nprocess does not add value and wastes time\n\nthe world is reducible to data , and every question has one objectively correct answer\n\n, and every question has communication and collaboration are a waste of time (“email jobs”)\n\n(“email jobs”) experience is irrelevant\n\nOutcome: Tech reduces the complex to input and output\n\nIn contrast with the Arts and Crafts movement, the Business Borg aesthetic actively conceals human labor and venerates the wisdom of the machine. Generative text and graphics simulate a performance of human-less — cost-less — automation. Generative answers encourage a reliance on the machine to synthesize on one’s behalf — and it doesn’t matter to search engines that the “answers” their AI has provided cite sources incorrectly.\n\nHumans are perceived as sources of inefficiency under the Business Borg ideology, because they must be compensated in accordance with their skills and how much time they spend working. Generating material is rooted in devaluing both skill and process. The invented summer reading list was the result of forcing a single contractor to prepare an impossible quantity of work; generating content was the only way for the poor bloke to produce the content on budget. No one reviewed it, because Business Borg only care that the product exists.\n\nEd Zitron describes the evolution of the Business Idiot, personified by middle managers who are completely dissociated from the product they’re selling and explicitly do not do work (emphasis mine):\n\n[Business Idiots] see every part of our lives as a series of inputs and outputs. They boast about how many books they’ve read rather than the content of said books, about how many hours they work (even though they never, ever work that many), about high level they are in a video game they clearly don’t play, about the money they’ve raised and the scale they’ve raised it at, and about how expensive and fancy their kitchen gadgets are. Everything is dominance, acquisition, growth and possession over any lived experience, because their world is one where the journey doesn’t matter, because their journeys are riddled with privilege and the persecution of others in the pursuit of success. These people don’t want to automate work, they want to automate existence. They fantasize about hitting a button and something happening, because experiencing — living! — is beneath them, or at least your lives and your wants and your joy are.\n\nValue: Efficiency is king\n\nGenerative AI produces endless content for low cost. Corporations are using Generative AI as an excuse to lay off workers and intensify the jobs of those remaining.\n\nReady Aim Fire (Owl Vision Remix) [Single] by Blue Stahli\n\nFoundational beliefs\n\nthe more mechanized a process is, the more efficient it becomes because humans are naturally inefficient\n\nOutcome: GenAI performs “efficiency”\n\nGenerative AI need not actually reduce work or cost to represent efficiency when mechanization is always favored over people. The Business Borg aesthetic perceives automation as efficient — hence situations where workers are paid to simulate chatbots simulating human agents on customer service platforms, Microsoft devs handhold Copilot, and GM’s Cruise “autonomous” taxis needed remote human intervention every 4-5 miles!!!\n\nEfficiency is a code word for shareholders, just like “cost-cutting,” who know that these phrases mean putting the boot on workers’ necks for short-term profits. This efficiency aesthetic is used to justify outrageously profitable companies continuing to slash workers *cough Microsoft* It plays out as Hollywood demolishing the screenwriter profession to save a buck on writing rooms and self-cannibalizing the development of future acting talent by forcing extras to be body scanned so they can be reproduced by AI.\n\nJeremy comments on the use of genAI in coding, noting that it’s justified by claims like “working code wins” — as in, what it looks like under the hood and how it’s constructed don’t matter. I’m not a coder, but I’ve seen enough HTML produced by PageMaker and other CMSs to be skeptical of the quality of any generated code — is genAI producing the coding equivalent of tables for web layout? 🤔 The Business Borg aesthetic accepts mediocrity without understanding; easy and fast is always best. Good enough is always good enough.\n\nValue: Quantity over quality\n\nGenerative AI is billed as a good-enough tool that will speed up production. Cory Doctorow writes of the managerial push to automate with AI:\n\nThe point of using automation to weaken labor isn’t just cheaper products – it’s cheaper, defective products, inflicted on the unsuspecting and defenseless public who are no longer protected by workers’ professionalism and pride in their jobs.\n\nEnormous by LLgL TNDR\n\nFoundational beliefs\n\nprofit today trumps tomorrow’s concerns — someone else will have to fix ‘that problem’\n\n— someone else will have to fix ‘that problem’ money is authority — individual rankings can be quantified by wealth\n\n— individual rankings can be quantified by wealth anything that cannot be quantified is not important\n\nOutcome: Algorithms produce culture-like media, not culture\n\nGenAI and corporate web algorithms are intended to absorb attention like black absorbs light; they are designed to maximize engagement at minimal cost. Spotify benefits from degradation of culture. Netflix is a chum machine, built to spew content that people watch in the background. Kyle Raymond Fitzpatrick laments that “So much of culture is edgeless and soft, intended for us to astroglide through it without any friction or doubt as we half-watch in 1.5x speed, to consume as if we really are incapable of critical thoughts, all to appeal to everyone and no one at the same time.” Internet Age capitalism produces entertainment that is culture-like, writes Nicholas Carr:\n\n“What’s really being tested here is human taste. Will we accept a simulacrum of a work of art or craft as a satisfactory substitute for the real thing?”\n\nSo long as people accept cheap low-quality cultural media, businesses have little incentive to pay for higher quality. Generative AI becomes an attack on culture because it drowns out human-made art and writing so it’s impossible to find amidst the Great Social Media Garbage Patch. Aidan Walker describes all this as ‘ slop capitalism ‘: “an economic and cultural system in which the primary product is slop and the primary activity is the destruction of value rather than its creation.”\n\nValue: Appearances trump reality\n\nGenerative AI produces plausible graphics that we interpret as real-adjacent and plausible combinations of text that we interpret as communication.\n\nFoundational Beliefs\n\nperformance of dominance builds and reinforces real power\n\nOutcome: GenAI supersedes reality with performance and symbolism\n\nThe Business Borg aesthetic celebrates audacious performances of infinite wealth and indefinite power: adopters and evangelists for genAI also embrace filling streets with “self-driving” cars over the protests of residents and first responders, raining space debris onto inhabited areas from slapdash rocket ships, paying a fortune for a banana taped to a wall as conceptual art and eating it, paying women to have their IVF babies so they can seem fertile without fucking. While the culture at large has shifted towards inconspicuous consumption, luxury that requires knowledge to see, Business Borg signal their wealth blatantly.\n\nA repeating theme of the Business Borg aesthetic is replacing reality with life-like hyperreality: the simulation of conversation and connection with chatbots, the renderings of war and disaster mimicking photojournalism to push political narratives, the “resurrected” extinct species, the green screen action sequences that don’t track. A photograph of reality may seem less real than a generated image if it does not abide by our expectations. (See also: reading “human vibes” into LLM responses)\n\nLook at the snoozefest kayfabe of modern MMA: it’s as much about the smack talk at weigh-ins as the fights themselves, athletes winning by points for “controlling the octagon” instead of fighting to win by KO or submission — look no further than that embarrassment of a so-called fight between Mike Tyson and the YouTuber, who danced around an old man till he got tired so he could win by decision and say he’d beaten a legend 😴\n\nEffective altruism performs charity that can never be disproven, despite its claims of data-driven decisions, because it pretends to think at such long-term scales that known, existing suffering pales in comparison to the imagined future suffering they claim to be protecting against. It cosplays rationalism. Wannabe Foundation shit.\n\nThe Business Borg aesthetic is expressed as cheap cruft disguised as something of substance: words that look like writing, but are not; images that look like art, but are not; fights that look fighting, but are not.\n\nAs Ed Zitron sums up, it’s a “symbolic economy:”\n\nThe sweeping changes we’ve seen, both in our economy and in our society, has led to an unprecedented, gilded age of bullshit where nothing matters, and things — things of actual substance — matter nothing.\n\nTraditionally, we have been wary of those wearing a mask, explains Dan Fox in Pretentiousness, suspecting they are something other than what they present themselves as — but authenticity has faded as a cultural value. Business Borg embrace performance as more true than reality; what someone wants to be is more important than what they are now. What a technology could be is more important than what it currently is.\n\nValue: “Progress” cannot be stopped\n\nThe rapacious assimilation of copyrighted material to train models, the dismissal of AI’s environmental cost and induced demand, and PR campaign reorienting the conversation around AI to its possible future harms as a distraction from the harms happening today all build from the same foundation: theoretically, scaling towards the pursuit of AGI — but more likely securing the funding to inextricably embed AI into our society and infrastructure.\n\nFoundational Beliefs\n\ntechnology always represents progress\n\nthe ends justify the means\n\nOutcome: GenAI is full speed ahead, no matter what\n\nAdvocates of generative AI are pursuing a brute force, fear-mongering approach to deregulation and preventing regulation. They are defying legality in obtaining training data while declaring the technology a fait accompli: resistance is futile. Their web crawlers ignore robots.txt, breaking the common courtesy of the web. They dismiss complaints about bias, claiming that they can fix that in the future. (And what incentive will there be to ever go back and fix it, once we can’t avoid using it?) They’re hoping enough of us will get hooked on it — and government and corporations will integrate enough of it too deeply in their processes — to allow their legally-suspect models to be shut down.\n\nRyan Broderick observes:\n\nWhat the AI arms race has actually done is codified and automated all of the failures of the previous internet era. Extremism, misinformation, harassment, non-consensual sexual material, and scams — all the content that tech companies promised to fix, but never could at scale — are now trapped in some AI’s black box of data.\n\nAGI, which has been “three to five years away” for years, is a Bezzle. As Cory Doctorow quotes JK Gabraith, a Bezzle is “the magic interval when a confidence trickster knows he has the money he has appropriated but the victim does not yet understand that he has lost it.” John Kay expands that “The joy of the bezzle is that two people – each ignorant of the other’s existence and role – can enjoy the same wealth.”\n\nScams and pyramid schemes are seductive to Business Borg, like Elizabeth Holmes and Sam Bankman-Fried, because the only things they value are money and power, not making things that actually work (see: Cybertrucks falling apart, SpaceX rockets exploding); the trick is not to get caught. The goal is to surf the edge of profit as long as possible.\n\nRight now the whole stock market is bloated by outrageous NVIDIA, Google, Microsoft and Meta valuations based on the potential of generative AI to create a new “essential” utility, a service that everyone will need to subscribe to, forever — and so many people have bought into the grift so hard they’ll do anything they can to make it a success, or at least rake in the cash for as long as they can. Then when the bubble pops, the corporations will get bailed out on the taxpayer dollar, while we workers resign ourselves to working until we die since we’ve privatized retirement 🙃\n\nGenAI is being deployed to control\n\nArtists are under attack, culturally and economically, so it is only fair that they point out the quintessential thing that artists offer that generative AI cannot: taste. Professional (and amateur) artists have devoted a great deal of time to developing their taste. The oligarchs who run Silicon Valley are steeped in their own rightness and devalue anything that isn’t their expertise; if they don’t know about it, it must not be important. To the Business Borg, taste is not an essential component of production.\n\nMandy Brown describes how genAI is used to undercut expertise (emphasis mine):\n\nIt’s instructive that one of the mechanisms for perpetuating this ideology are chattering bots that speak both fact and falsehood in the same servile and confident tone, their makers unconcerned with the difference. In fact, their makers seem entirely concerned with obviating that difference, with disappearing distinctions between knowledge and ignorance, without which truth becomes entirely a product of power. […] [I]f those in power cannot prove that a great many people are already inferior then they will bring that inferiority about by forcing them to use a tool that diminishes their intellectual and creative capacity.\n\nBusiness Borg like generative AI because it grants them cultural power that they have not been able to dominate on their own. They lack skill, so they devalue skill. They need content, so they make an infinite content machine and conscript users as unwitting factory workers to provide free labor. The relentless promotion of GenAI is an attempt by corporations to capture cultural value by siphoning off value from human-made aesthetics. Generative AI is billionaires punching down on artists and the working class.\n\n[…] I think Miyazaki’s style is still valuable But it is now, in a day, valuable in a different way. What was once valuable in the awareness of painstaking labor, beautiful stories, and coherent aesthetic across the previous two qualities. PLUS our reception to it. Is now valuable PURELY in our reception to, and reproduction of, the aesthetic. […] — Reggie James (@HipCityReg) March 27, 2025\n\nGenerative AI has intentionally been molded to attack artists and diminish cultural literacy. Aidan Walker argues (read this whole piece if you liked my post):\n\nAI doesn’t have to be an antagonist to schools, work, and civil society — they’ve just designed and trained it that way… There could be guardrails in place, they could pay the producers of their training data, they could give the people a say in how the models are made and deployed — we could do a thousand things differently than the way they’re being done now.\n\nGenerative AI — both imagery and text — is inextricable from the corporate vision for its use: a world in which workers are powerless and worthless, replaced by “free” generated material. Corporate GenAI cannot be separated from the purpose for its use or the billionaires and billionaire-wannabes who shill for it. The Business Borg aesthetic imbues a sheen of venality.\n\nFurther reading:\n\nGenAI is Our Polyester by W. David Marx\n\nEconomics & labor rights in AI skepticism by Henry from online\n\nYou don’t hate AI; You hate… : a collection by Mita Williams\n\nDispatch from the Trenches of the Butlerian Jihad by ADH\n\nThe other way the [Butlerian Jihad] metaphor is proving apt is the deep-seated, almost spiritual nature of anti-AI sentiment. It’s not just more Luddism. Many people — though hardly all, given the popularity of AI products — sense that there is something grotesque about these simulacra, the people who push them on us, this whole affair. That aversion to the technological profane holds even when various stated objections to AI are supposedly addressed or nitpicked to death.\n\nSee also:\n\nWe need solidarity across creative industries", "label": "non_personal"}
{"title": "Using Signal to communicate securely • Cory Dransfeldt", "url": "https://www.coryd.dev/posts/2025/using-signal-to-communicate-securely", "content": "Now more than ever it's important to keep your communications with those you know secure and private. Signal is the best available option for doing so. It is secure, private and run by a non-profit organization that makes it freely available.\n\nDownload Signal - Private Messenger from your mobile device's app store.\n\nApp Store\n\nGoogle Play\n\nRegister and provide your phone number if required.\n\nDo not set a profile picture. Use your first name or a pseudonym.\n\nAdjust your settings:\n\nChats: disable Generate Link Previews. Disable Share Contacts with iOS or Android.\n\ndisable Generate Link Previews. Disable Share Contacts with iOS or Android. Notifications: under Notification Content, select No Name or Content.\n\nunder Notification Content, select No Name or Content. Privacy: under Phone Number set both options to nobody. Enable Hide Screen in App Switcher. Enable Screen Lock, select 5 minutes for Screen Lock Timeout.\n\nTo share your Signal contact information, provide your username or tap on the QR code at the top of your settings (by your profile information) and send the QR code or link over SMS. Then communicate using Signal.\n\nQuestions? Email me or schedule a call with me\n\nAdditional resources\n\nIf you have an iPhone, use Apple's Translate app. When prompted to provide recordings to improve the app, select no.\n\nAvoid using any of Meta's services to share sensitive information.\n\nI would strongly recommend not using Gmail or other popular email providers. The following table contains a list of reputable alternatives.", "label": "non_personal"}
{"title": "Letter to Planning Commission re: upzoning in Juanita – Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/12/letter-to-planning-commission-re-upzoning-in-juanita/", "content": "Two large sites in the northern part of my city are being proposed for upzoning to permit taller buildings, reduced parking requirements, as well as other changes. In general the changes seem good, but there is one proposed change that is concerning: allowing townhomes instead of apartments or condos on some large sites that ought to be developed as densely as possible (link to packet). There is a great deal of community opposition to these upzones, so I sent in a supportive note to the Planning Commission in advance of their evening meeting.\n\nIn support of higher density at Michael’s and Goodwill sites\n\nTo the Kirkland Planning Commission:\n\nI am writing in favor of proposed zoning amendments to the Michael’s and Goodwill sites to allow higher density, taller building heights, and reduced parking requirements. These rare anchor sites offer great potential to make the neighborhood denser and more walkable.\n\nI do not support allowing townhomes in this area — along arterial routes with transit access, with many goods and services within walking or biking distance, we ought to support as much housing density as feasible. Kirkland and the greater Seattle area are in desperate need of more housing, so I urge you to use the code to require higher densities in the BC1 zone. If we do not push for higher density where it makes sense — and it makes a great deal of sense here — then the overall amount of additional housing Kirkland can accommodate will be much lower than the need. Please require a minimum density for ADUs in this area as in Alternatives 1 or 2 from Appendix 5.", "label": "non_personal"}
{"title": "One Year Left: Apple’s Long Goodbye For Intel Macs", "url": "https://tedium.co/2025/06/09/apple-wwdc-intel-mac-support-ending/", "content": "As you probably know after all this time, Tedium is obsessed with the closing frame, the end of the story. And today, we learned that Apple is finally ending its 20-year run of Intel-based Macs.\n\nThat’s the bad news. The good news is that they gave the public one more year of new versions, along with the promise of potential security fixes, avoiding an uncomfortable rug-pull like the one that many PowerPC users experienced with Snow Leopard in 2009. That OS came out a mere three years after the discontinuation of the last PowerPC Mac, and users had to figure out the cutoff was happening by reading Apple rumor sites.\n\nWhile some Mac models did get short shrift (owners of the 2020 Intel MacBook Air have some angry skeets to write), for the most part, the company did not try to force this transition to happen faster than it needed to.\n\nThe commercial for the first Intel Mac, dating to 2006. If you’ve ever seen the video for The Postal Service’s “Such Great Heights” and think it looks very familiar, there’s a reason for that.\n\nIt was as if the company wanted to bury the blow as much as possible, so it didn’t even mention it during the main WWDC keynote, which is the one that the average person cares about. It was instead buried nearly 55 minutes into the 57-minute Platforms State of the Union, where Apple Senior Director of Developer Relations Matthew Firlik dropped the news like this:\n\nMetal 4 is a great example of the tight integration of our software with Apple silicon, creating a whole new class of experiences. In fact, since we began the transition to Apple silicon over five years ago, we’ve been able to add incredible features like Apple Intelligence, Game Mode, Presenter Overlay, and more. We completed the transition to Apple silicon across our entire product lineup two years ago. So your apps can now depend on and build upon these features too. Apple silicon enables us all to achieve things that were previously unimaginable. And it’s time to put all of our focus and innovation there. And so, macOS Tahoe will be the final release for Intel Macs. So if you’ve not done so already, now is a great time to help your users migrate to the Apple silicon versions of your apps.\n\nThis sort of finality—a one-year pre-announcement from an official Apple source—is useful for any old users who have been holding off for whatever reason. But it’s also great for developers, who now have the OK to transition towards an upgrade if they haven’t already. And certainly, Apple’s ARM-based chips are some of the best processors ever made, based on their balance of speed and energy efficiency, which has made the M1 MacBook Air (nearly a 5-year-old machine!) perhaps the greatest goldilocks machine ever created.\n\nBut still, even with all that lead-up, this decision still stings, because it feels unnecessary to put all that good hardware to pasture. As I wrote back in April, a similar decision to put an end of life on Windows 10 is ultimately unnecessary—and it would lead to a lot of good hardware ending up in landfills. That’s the downside, and one we should not ignore.", "label": "non_personal"}
{"title": "Generative AI as a magic system – Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/24/generative-ai-as-a-magic-system/", "content": "We treat generative AI like magic… and magic systems have rules. When creating fantasy worlds, writers think about who can use magic, how magic is performed, what it’s able to do, what its constraints are, what the source of magic is, and what it costs. I’m applying a bit of reverse worldbuilding to the real world to extrapolate the rules of the AI magic system.\n\nIslands in the Sky by Death Valley Girls\n\nWho can use AI magic: magic users pay to use corporate AI magic systems. Those who are wealthy and tech savvy enough can host their own local model. Free magic use is mostly limited to corporate largesse ultimately intended to build magic dependency.\n\nHow AI magic is cast: AI spells are cast with written text input through a digital interface. Spells are refined and recast until the outcome satisfies (spells produce different results every time they are cast).\n\nWhat AI magic can do: AI spells can produce combinations of words that are interpreted as writing, code-like material that sometimes runs as code, images that resemble art, and video that resembles reality. It can create imitations of specific human creators’ work, as well as individual’s speech and appearance. It can also mimic human conversation for a span of time before the spell dissipates. AI magic is near instantaneous, allowing people without technical skills to produce text and graphics faster than writers and artisans.\n\nWhat AI magic cannot do: AI magic cannot produce the same outcome twice, nor act upon existing conjurations, instead casting spells anew each time. AI magic itself cannot reference sources, though may be used in tandem with other tools that enable citation (though with questionable accuracy). AI magic cannot reason or write, but its conjurations may create the illusion of intelligence through their statistical consistency with written language use.\n\nThe source of AI magic: AI magic derives from statistical analysis of human-created art, writing, speech, music, and video, classified and sorted by human laborers in low-cost geos.\n\nThe cost of AI magic: Resource costs of AI magic include power, water, and high-end chips, which themselves require specialized manufacturing and rare earth minerals.\n\nSocial costs include the reinforcement of racism and sexism, as well as mental harm to AI trainers assessing inputs to the magic system.\n\nSocietal costs include job elimination and job intensification as positions able to be reproduced in part by magic are eliminated and that magic work is shifted to the remaining workers.\n\nInformation costs include the destruction of the online publishing incentive structure / information commons, leading to more paywalled content; an increase in low-quality material, which makes finding accurate information harder; as well as the danger of political propaganda by poisoned magic systems.\n\nIndividual user costs include critical thinking skills, writing abilities, and patience for conversing with humans.\n\nFurther reading:\n\nThe new magic of AI vs. the old magic of artists by Kening Zhu\n\nSee also:\n\nGenerative AI and the Business Borg aesthetic", "label": "non_personal"}
{"title": "Copyright anti-circumvention: an AI hypothetical", "url": "https://tommorris.org/posts/2024/copyright-anti-circumvention-ai-hypothetical/", "content": "Imagine there is an American company called FancySiri Inc. who make a commercially available large language model similar to those offered by Google, OpenAI, Anthropic etc. It provides a chatbot service similar to ChatGPT, Gemini, et al. where users can ask questions and get a load of synthetic text of dubious provenance and accuracy that purports to answer the question they asked.\n\nNow imagine there is a person, let’s call him Keith Jones. Keith is a British citizen and runs a website hosted in the UK. Every page on his website is marked with “Copyright © 2024 Keith Jones. All rights reserved.”\n\nIn addition, Keith places Schema.org metadata in the page, in a manner broadly as follows:\n\n<body vocab=\"https://schema.org/\" typeof=\"WebPage\"> ... <footer> <span property=\"copyrightNotice\"> Copyright © <span property=\"copyrightYear\">2024</span> <span property=\"copyrightHolder\" typeof=\"Person\"> <span property=\"name\">Keith Jones</span> </span>. <a href=\"https://en.wikipedia.org/wiki/All_rights_reserved\" property=\"license\">All rights reserved.</a> </span> </footer> </body>\n\n(Other metadata standards are available: microformats, DC terms etc. Pick whichever one you hate least.)\n\nKeith is not particularly impressed with the trend towards artificial intelligence, so he obtains a list of the User Agents of commercially available large language models that happens to include well-known existing services already mentioned and our hypothetical FancySiri Inc. (ai.robots.txt or similar lists on the web)\n\nKeith puts up a robots.txt file and implements filtering based on User Agent string matching to always return an HTTP 401 Unauthorized response to known AI/LLM scrapers including FancySiri. This denies them access to his beautifully curated collection of tasteful cat pictures and discussions of the nerdiest of minutiae concerning programming in Rust.\n\nFancySiri Inc. have published on their website’s legal FAQ that they comply with robots.txt and only scrape material using the User-Agent specified, “FancySiriBot”.\n\nUnfortunately, they lied.\n\nEither they did not respect the contents of Keith’s robots.txt, or they ran a second scraper that uses the User Agent string of a human-operated web browser. Like, say “Mozilla/5.0 (iPhone14,3; U; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/602.1.50 (KHTML, like Gecko) Version/10.0 Mobile/19A346 Safari/602.1” (for those not familiar with reading User Agent strings, this UA is for an iPhone 13 Pro Max running iOS 15).\n\nA tech company lying? Shocking, I know.\n\nKeith learns that a blog post he published about some particularly abstruse Rust minutiae has become part of the training data used for FancySiri’s large language model. For instance, when the FancySiri chatbot is asked about the rather niche topic, the server returns words that are so remarkably close to Keith’s blog post that it’s beyond any doubt where it came from.\n\nKeith is rather puzzled about this and takes a break from photographing his cat and writing Rust to peruse the UK’s Copyright Designs and Patents Act 1988. After some consideration, he has some interesting questions. Purely hypothetically, one must note. He does not have the money to take FancySiri Inc. to court, and has a wise policy of avoiding civil litigation on the very reasonable basis that he enjoys not being sad all the time.\n\nDoes the publication of a robots.txt file and the use of a User-Agent matching traffic block amount to an effective technological protection measure as defined by s296ZF of the CDPA? Assuming the answer to (1) is yes, has FancySiri Inc. circumvented those measures knowing, or with reasonable grounds, to know that they are trying to circumvent it? That is to say, have they breached s296ZA of the CDPA? Is the machine readable metadata on Keith’s webpage “rights management information” under the definition given in s296ZG(7)(b)? If the answer to (4) is yes, has FancySiri Inc. “removed or altered” the rights management information from Keith’s website? If the answer to (5) is yes, are there reasons on which a court could reasonably conclude that FancySiri Inc. did so “knowingly and without authority” while knowing or having reason to believe that by doing so they are inducing, enabling, facilitating, or concealing an infringement of copyright (per s296ZG)?\n\nIf you’re a nerd, feel free to re-run this exercise with the particular anti-circumvention rules as implemented in your preferred jurisdiction. They’re all fairly similar because large chunks of copyright law is the product of international agreements (in this case, the 1996 WIPO Copyright Treaty).\n\nThe rough intuition behind this hypothetical is as follows: anti-circumvention rules are really strong. The domain of things which they protect has expanded way beyond the original purpose (computer software and other creative works including music, books, movies, visual arts etc.) to afford copyright protection to the DRM on ink cartridges, tractors and ventilators (the latter being a particularly unpleasant illustration of the prioritisation of corporate greed over human life during the COVID pandemic). Such a broad legal rule could also potentially allow a suitably placed claimant to challenge—at least, in theory—the use of online materials by AI companies.\n\nThe UK government are currently consulting on Copyright and Artificial Intelligence, though they seem to have not mentioned the application of anti-circumvention rules (either in the consultation document or the summary assessment of options document). Some of the questions the government are consulting on—specifically about machine-readable methods for “reserving rights”—would seem to envision a regime where one has to take specific technical steps to protect works from infringement. Hence they kind of run in parallel to the anti-circumvention rules already in the CDPA and similar instruments. It might be sensible to ensure they don’t conflict, or cause confusion.\n\nIf the government wish to amend the copyright laws to broaden the current limited data mining exception to cover AI training, that policy goal could be undermined by the use of anti-circumvention rules against the same AI companies they seek to, uh, liberate. That said, under the principle of “what’s good for the goose is good for the gander”, perhaps Keith should be afforded the same maximalist level of protection of the intellectual property in his cat pictures and blog posts about Rust programming as companies like HP and Lexmark have greatly benefitted from when it comes to stopping people using “pirate ink” in their inkjet printers.\n\nAppendix: relevant CDPA passages\n\nThe relevant passages of the legislation are included below for interested readers. For those who enjoy reading legislation, the whole text is on legislation.gov.uk for your perusal. I have tried to chop out as much that isn’t relevant as humanly possible.\n\nOne thing to note is that s296 deals with anti-circumvention of computer software while s296ZA-ZF deal with anti-circumvention as applied to everything else. Since the problem concerns a blog post (which might contain incidental bits of computer code in trivial quantities), I’ve used the non-code sections. If the hypothetical concerned, say, a self-hosted Gitea instance containing a bunch of code, the s296 code version would apply instead.\n\nSection 296ZA: Circumvention of technological measures\n\n(1) This section applies where— (a) effective technological measures have been applied to a copyright work other than a computer program; and (b) a person (B) does anything which circumvents those measures knowing, or with reasonable grounds to know, that he is pursuing that objective.\n\n(Subsection (2) omitted - it deals with cryptographic research, which isn’t going on here.)\n\n(3) The following persons have the same rights against B as a copyright owner has in respect of an infringement of copyright— (a) a person— (i) issuing to the public copies of, or (ii) communicating to the public, the work to which effective technological measures have been applied; and (b) the copyright owner or his exclusive licensee, if he is not the person specified in paragraph (a).\n\nThe remaining subsections are omitted: (4) deals with concurrent rights, (5) deals with procedural and evidential matters, while (6) and (7) concern rights in performance, publication rights, an database rights - all of which are not dealt with in the hypothetical.\n\nSection 296ZF: Interpretation of sections 296ZA to 295ZEA\n\n(1) In sections 296ZA to 296ZE, “technological measures” are any technology, device or component which is designed, in the normal course of its operation, to protect a copyright work other than a computer program. (2) Such measures are “effective” if the use of the work is controlled by the copyright owner through— (a) an access control or protection process such as encryption, scrambling or other transformation of the work, or (b) a copy control mechanism, which achieves the intended protection. (3) In this section, the reference to— (a) protection of a work is to the prevention or restriction of acts that are not authorised by the copyright owner of that work and are restricted by copyright; and (b) use of a work does not extend to any use of the work that is outside the scope of the acts restricted by copyright. (4) Expressions used in sections 296ZA to 296ZEA which are defined for the purposes of Part 1 of this Act (copyright) have the same meaning as in that Part.\n\nSection 296ZG: Electronic rights management information\n\n(1) This section applies where a person (D), knowingly and without authority, removes or alters electronic rights management information which— (a) is associated with a copy of a copyright work, or (b) appears in connection with the communication to the public of a copyright work, and where D knows, or has reason to believe, that by so doing he is inducing, enabling, facilitating or concealing an infringement of copyright.\n\nSubsection (2) concerns importing and is not relevant.\n\n(3) A person issuing to the public copies of, or communicating, the work to the public, has the same rights against D and E as a copyright owner has in respect of an infringement of copyright. (4) The copyright owner or his exclusive licensee, if he is not the person issuing to the public copies of, or communicating, the work to the public, also has the same rights against D and E as he has in respect of an infringement of copyright. (5) The rights conferred by subsections (3) and (4) are concurrent, and sections 101(3) and 102(1) to (4) apply, in proceedings under this section, in relation to persons with concurrent rights as they apply, in proceedings mentioned in those provisions, in relation to a copyright owner and exclusive licensee with concurrent rights.\n\nSubsection (6) is concerned with procedural and evidential matters only relevant if Keith decides to pursue litigation.\n\n(7) In this section— (a) expressions which are defined for the purposes of Part 1 of this Act (copyright) have the same meaning as in that Part; and (b) “ rights management information ” means any information provided by the copyright owner or the holder of any right under copyright which identifies the work, the author, the copyright owner or the holder of any intellectual property rights, or information about the terms and conditions of use of the work, and any numbers or codes that represent such information.\n\nSubsections (8) and (9) aren’t relevant.", "label": "non_personal"}
{"title": "How we improved availability through iterative simplification", "url": "https://github.blog/engineering/engineering-principles/how-we-improved-availability-through-iterative-simplification/", "content": "Solving and staying ahead of problems when scaling up a system of GitHub’s size is a delicate process. The stack is complex, and even small changes can have a big ripple effect. Here’s a look at some of the tools in GitHub’s toolbox, and how we’ve used them to solve problems. We’ll also share some of our wins and lessons we learned along the way.\n\nThere are several tools that we use to keep pace with our growing system. While we can’t list them all, here are some that have been instrumental for our growth.\n\nAs we serve requests, there is a constant stream of related numbers that we care about. For example, we might want to know how often events are happening or how traffic levels compare to expected use. We can record metrics for each event in Datadog to see patterns over time and break them down across different dimensions, identifying areas that need focus.\n\nEvents also contain context that can help identify details for issues we’re troubleshooting. We send all this context to Splunk for further analysis.\n\nMuch of our application data is stored in MySQL, and query performance can degrade over time due to factors like database size and query frequency. We have written custom monitors that detect and report slow and timed-out queries for further investigation and remediation.\n\nWhen we introduce changes, we often need to know how those changes affect performance. We use Scientist to test proposed changes. With this tool, we measure and report results before making the changes permanent.\n\nWhen we’re ready to release a change, we roll it out incrementally to ensure it works as expected for all use cases. We also need to be able to roll back in the event of unexpected behavior. We use Flipper to limit the rollout to early access users, then to an increasing percentage of users as we build the confidence.\n\nAchieving faster database queries\n\nWe recently observed a SQL query causing a high number of timeouts. Our investigation in Splunk tracked it down to GitHub’s Command Palette feature, which was loading a list of repositories. The code to generate that list looked something like this:\n\norg_repo_ids = Repository.where(owner: org).pluck(:id) suggested_repo_ids = Contribution.where(user: viewer, repository_id: org_repo_ids).pluck(:repository_id)\n\nIf an org has many active repositories, the second line could generate a SQL query with a large IN (...) clause with an increased risk of timing out. While we’d seen this type of problem before, there was something unique about this particular use case. We might be able to improve performance by querying the user first since a given user contributes to a relatively small number of repositories.\n\ncontributor_repo_ids = Contribution.where(user: viewer).pluck(:repository_id) suggested_repo_ids = Repository.where(owner: org, id: contributor_repo_ids)\n\nWe created a Scientist experiment with a new candidate code block to evaluate performance. The Datadog dashboard for the experiment confirmed two things: the candidate code block returned the same results and improved performance by 80-90%.\n\nWe also did a deeper dive into the queries this feature was generating and found a couple of possible additional improvements.\n\nThe first involved eliminating a SQL query and sorting results in the application rather than asking the SQL server to sort. We followed the same process with a new experiment and found that the candidate code block performed 40-80% worse than the control. We removed the candidate code block and ended the experiment.\n\nThe second was a query filtering results based on the viewer’s level of access and did so by iterating through the list of results. The access check we needed can be batched. So, we started another experiment to do the filtering with a single batched query and confirmed that the candidate code block improved performance by another 20-80%.\n\nWhile we were wrapping up these experiments, we checked for similar patterns in related code and found a similar filter we could batch. We confirmed a 30-40% performance improvement with a final experiment, and left the feature in a better place that made our developers, database administrators, and users happier.\n\nRemoving unused code\n\nWhile our tooling does surface problem areas to focus on, it’s preferable to get ahead of performance issues and fix problematic areas before they cause a degraded experience. We recently analyzed the busiest request endpoints for one of our teams and found room to improve one of them before it escalated to an urgent problem.\n\nData for each request to the GitHub Rails application is logged in Splunk and tagged with the associated controller and action. We started by querying Splunk for the top 10 controller/action pairs in the endpoints owned by the team. We used that list to create a Datadog dashboard with a set of graphs for each controller/action that showed the total request volume, average and P99 request latency, and max request latency. We found that the busiest endpoint on the dashboard was an action responsible for a simple redirect, and that performance regularly degraded to the timeout threshold.\n\nWe needed to know what was slowing these requests down, so we dug into Datadog’s APM feature to show requests for the problematic controller/endpoint. We sorted those requests by elapsed request time to see the slowest requests first. We identified a pattern where slow requests spent a long time performing an access check that wasn’t required to send the redirect response.\n\nMost requests to the GitHub Rails application generate HTML responses where we need to be careful to ensure that all data in the response is accessible to the viewer. We’re able to simplify the code involved by using shared Rails controller filters to verify that the viewer is allowed to see the resources they’re requesting that run before the server renders a response. These checks aren’t required for the redirect, so we wanted to confirm we could serve those requests using a different set of filters and that this approach would improve performance.\n\nSince Rails controller filters are configured when the application boots rather than when each request is processed, we weren’t able to use a Scientist experiment to test a candidate code block. However, filters can be configured to run conditionally, which enabled us to use a Flipper feature flag to change behavior. We identified the set of filters that weren’t required for the redirect, and configured the controller to skip those filters when the feature flag was enabled. The feature flag controls let us ramp up this behavior while monitoring both performance and request status via Datadog and keeping watch for unexpected problems via Splunk.\n\nAfter confirming that performance improved for P75/P99 request latency—and more importantly, reduced max latency to be more consistent and much less likely to time out—we graduated the feature and generalized the behavior so other similar controllers can use it.\n\nWhat did we learn?\n\nThere are several lessons we learned throughout this process. Here are some of the main points we keep in mind.\n\nThe investment in observability is totally worth it! We identified and solved problems quickly because of the metric and log information we track.\n\nEven when you’re troubleshooting a problem that’s been traditionally difficult to solve, the use case may be subtly different in a way that presents a new solution.\n\nWhen you’re working on a fix, look around at adjacent code. There may be related issues you can tackle while you’re there.\n\nPerformance problems are a moving target. Keeping an eye open for the next one helps you fix it when it’s gotten slow rather than when it starts causing timeouts and breaking things.\n\nMake small changes in ways that you can control with a gradual rollout and measure results.", "label": "non_personal"}
{"title": "The adolescents are alright. So are the common people. Let’s make tech good for them.", "url": "https://tommorris.org/posts/2025/the-adolescents-are-alright-so-are-the-common-people-lets-make-tech-good-for-them/", "content": "I’ve been trying and failing to write about the current vibes of technology for the last few months, with a specific focus on both the wrapping up of the Post Office Horizon Inquiry, and the threat/promise by Keir Starmer to “mainline AI in the veins” of the United Kingdom. But that’s way too big. I want to talk about aforementioned vibes based on two recent fictional depictions that directly touch on the societal role of technology. And because we’re talking about vibes, I need to give two warnings. One is that it’s just my opinion, man, and it will meander around a bit.\n\nAnd the second is a spoiler warning for both Adolescence and Black Mirror Series 7 Episode 1 (‘Common People’).\n\nTo say Adolescence has become a touch point for popular concern about technology in the UK is an understatement. Each episode is a technically flawless one shot. The mini-series starts with a dramatic early morning arrest of a thirteen year old boy for murder of a classmate. The camera then follows the plodding institutional procedure of Jamie being booked into a police custody suite, having his fingerprints and samples taken, then an interview. It should be noted that the apparent commitment to procedural accuracy gets a bit shaky when it comes to the role of the defence solicitor, but given the number of disclosure violations in the criminal justice system of England and Wales, a few more occurring in fiction is at least in keeping with the show’s commitment to brutal realism.\n\nThe subsequent episodes apply the same masterful theatre-like technique to the investigation of the crime by detectives at the unpleasant secondary school where both perpetrator and victim studied, then to a pre-trial interview by a psychologist in a youth detention centre, and finally to the consequences of the conviction on Jamie’s family.\n\nGiven the centrality of technology to the crime at the heart of both the drama and the societal discussion it has spawned, it is weirdly absent from the masterly manoeuvred view of the camera. It reminds me of the depiction of HIV/AIDS on screen—it is always there in the sense that it shapes the lives of those it affects, but you never see it, only the wretched physical effects it has on people infected, and the emotional consequences for their partners, friends and communities. Here, the smartphone, or the gaming console or PC, is the unseen horror, a malevolent ghost that is everywhere and nowhere. The inciting incident for the murder—a series of interactions on Instagram—are never seen, only discussed by others. The key to interpreting them is only grasped indirectly, when the detective’s son Adam takes pity and gives his father a remedial education in translating semantically-overloaded emojis from impenetrable zoomer lingo into a motive that explains murder.\n\nThe Prime Minister incorrectly referring to Adolescence in Parliament as a “documentary” is perhaps a testament to the artful camerawork, but it is practically being treated as one—with efforts to add it to the National Curriculum and legislate based on the assumption that screen time is not just rotting the brains of the young, it is turning them into murderers. Something. Must. Be. Done. Ban smartphones from the schools. Oh, what, headteachers can already do that? Well, double triple quadruple ban them! And ban the kids from social media too. Maybe we could pass some kind of Online Safety Bill to… oh, wait, we already have.\n\nLet’s start with some concessions. There are bad consequences of technology. Obviously. But the nuance-free debate we’ve seen since seems predicated on the idea that there is this giant immutable thing called Technology, and we can’t do much about it. The most we can do is to save the Jamies and Lisas from the Satanic evils of Snapchat, Instagram and TikTok until they’re old enough to make their own decisions. And once we’ve returned to the pre-iPhone norm, order will be restored and childhood saved.\n\nThe framing around the government’s AI policy is also worth noting here. The pressing need to “mainline” it into our national veins is roughly “this seems like it is inevitable, we at least want it to make some money and create some jobs”, combined with a little dash of “we don’t know what it is, but whatever it is, we want to beat China at it”. The train is rolling, we either get on board and reap some economic and geostrategic benefits, or we don’t. And we’d rather reap some benefits than be stuck in the past, right? Stop with all the nuance. Get on board or die. (Silicon Valley accelerationism is basically a version of this view, but with a penchant for skull measuring, and an addiction to ketamine.)\n\nBoth the post-Adolescence moralism and the boosterish “mainline AI into our veins” narrative seem to coexist in the political imagination of Sir Keir Starmer, and both depend on a feeling that the result is inevitable and foreordained, and all we can do is react. We either accept it, and hope we can nudge it gently towards some social benefit, or bring down the regulatory banhammer.\n\nIt’s understandable why people don’t feel much control over technology. It’s a fact they don’t have much control. That’s a problem we need to fix. The solution to that is imagining an alternative where they do. Let’s imagine if technology was a tool that individuals could shape in accordance with their goals for a better life.\n\nA nuanced discussion of technology that doesn’t fall into inevitabilism and moral panic might help us understand technology in a way where we can respond to it in intelligent and effective ways.\n\nAs an illustration, let’s talk about Roblox. I have friends with young children who spend hours and hours every day playing Roblox on an iPad. When discussing Roblox, their chief concerns are “are they spending too much time on this thing?” and “are they gonna end up talking to predators?”\n\nThe easy answers you’ll generally hear from the media: yes and yes.\n\nThe real answers to this are: that depends on a whole load of other variables and perhaps, but that risk very hard to quantify.\n\nAre there sexual predators on Roblox? Yes. This is because Roblox is a service on the internet, and there are sexual predators on the internet, in much the same way there are sexual and other predators everywhere. Are Roblox doing enough to combat predators on their platform? Maybe. Hard to tell. We may get more information as the new regulatory regime in the Online Safety Act takes shape, along with similar regulatory regimes in EU and Australia. Or we might not. But, absolutely, be vigilant against awful people, and don’t assume the company will give a damn unless it reaches a point where it affects their stock price.\n\nAre children spending too much time on Roblox? Depends on the child, what they’re getting from using the service, and what the opportunity cost there is. A lot of children and young people spend a lot of time playing computer games. So do adults. I’ve spent a fair amount of time playing games. Some of them were crap, some of them were amazing. Kinda like books or albums or movies or meals in restaurants. Some of the people I knew at school likely spent time hanging around rural bus stations drinking bottles of White Lightning, a substance that’s only charitably referred to as cider. Others took violin lessons and went to Oxford. (You can adjudicate which ones turned out happier and better adjusted.)\n\nSome of the kids I knew at school would have had colossally high levels of what is now called “screen time”. Many also had colossally high levels of bullying. If you take the view that the time wandering around Roblox or watching weird algorithmically generated YouTube kids videos is time that said kids aren’t spending playing with friends in wholesome ways, or taking ballet classes or composing sonnets, then, yes, your utilitarian calculus will swing heavily against it. But then you also need to weigh up the kids who are learning to build their own websites or video games, or finding supportive communities, or reading, or playing videos games with really strong narratives. And you need to add all the stuff in the plus column against all the absolutely bloody silly things kids got up to that did not need involve screens or internet connections. “Screen time” is a number, not an evaluation of whether the time spent was worthwhile in the context of their life.\n\nInto those conversations with parent friends about Roblox come the concessions: yes, it might be a waste of time. Yes, there might be Bad People on there, do keep an eye out for that. But, please, keep listening, it’s also way worse than that. The whole enterprise of things like Roblox are built on pester power. Roblox makes absolute bank out of users convincing their parents to buy Robux to spend on random stuff in Roblox. That’s how Roblox is very, very rich.\n\nLoads of other games do likewise too. Finding the games that don’t suck is hard because our rating systems are based on a perception of risk that is centred around violence, profanity and nudity/sexual content. This is understandable: the video game rating system was basically cut and paste from the rating system used for films and television. But it isn’t actually helpful to think about them in the same way as you do for film and television.\n\nAn illustration of this: Elden Ring is rated 16+ by PEGI, because it contains violence and gore and some nudity, apparently. (I haven’t played it. I hear it is very good.) EA Sports FC ’25 (formerly known as FIFA) is rated 3+ because it doesn’t contain any violence or gore or nudity—it’s a football game, after all. It has an in-game currency, which is used to purchase randomised card packs. These are ways you give the game developer money in order to have a random chance to win something of benefit to you in the game—namely, players. (So I’m told. I also haven’t played it.) If you don’t win, you can pay lots of money to keep buying more and more card packs and opening them on the off-chance of getting the prize you want. That may sound like the sort of thing gambling addicts do with a fixed odds betting terminal. But you’ll be glad to know it is not within the scope of the Gambling Act 2005 because that requires it be done “for a prize”. A prize is defined as “money or money’s worth”. And cards that give you the ability to add football players to your team aren’t that. Other jurisdictions disagree, specifically Netherlands and Belgium.\n\nSure, there have always been ways for kids to waste their pocket money, from Panini stickers to Pogs to Pokemon cards. You used to have to slog your way down to your newsagent to waste your money on Panini football stickers. Now, PEGI, the Pan European Game Information service, who rate video games in the UK and Europe have decided that a game that pummels you with both the aesthetics of, and the mechanics of, gambling in order to win the chance of signing better players for your virtual football team deserves a 3+ “Suitable for all ages” ranking, with a little notice that reads:\n\nThis game offers players the opportunity to purchase in-game items, in the form of an in-game currency, which can be used to purchase random card packs and other game items. Some parents or carers may want to be aware of this. Parents, carers or other responsible adults should check to see what is being offered before making any purchase on behalf of a child. It should be noted that the game can still be played without the need to purchase such items.\n\nWhat is omitted from this description is that the game will usually really suck if you don’t purchase said items, so you’ll be constantly pressured to do so over and over.\n\nAnother example: let’s talk about Balatro. It’s an indie card-based computer game, and it is incredibly addictive. It is not a gambling game but it does rely on the scoring rules of poker hands. When PEGI first rated the game last year, they gave it an 18+ rating as it contained “prominent gambling imagery”. This is enough to classify it as 18+. EA Sports FC, which tries to get players to play what’s essentially an I Can’t Believe It’s Not Gambling! slot machine to build their football team, is rated 3+. Balatro was reclassified on appeal as 10+, perhaps so PEGI could still retain some shred of professional dignity. Presumably, PEGI are still committed to the idea that EA Sports FC card packs are suitable for 3-9 year olds in a way that cartoon depictions of playing cards aren’t.\n\nIt has now taken me boatloads of paragraphs to explain what sucks about lots of modern video games from AAA studios in a way someone who doesn’t play video games can understand, and to give you examples of how the rating system designed by the industry and approved by regulators to protect children from harm does not actually do this very well. Humans are busy people and don’t want to have to think about this. By contrast, “too much screen time bad” fits in a tweet or an opinion column. Guess which one takes off more in the public consciousness. And if you’re a government, banning stuff costs nothing. (Except if you do it really badly. Which we have. More on that later. Let’s get back to the really important stuff, namely television.)\n\nIf the technology in Adolescence takes the form of ghoulish off-screen actions that requires a secret Gen-Z emoji-to-English decoder ring to understand, the technology in ‘Common People’, the first episode of the new series of Black Mirror is as clear as day.\n\nAmanda is a teacher, and her British partner Mike is working in manual labour in Smalltown, USA. While in front of her class, Amanda collapses into a coma and suffers brain damage. There is no hope, says the doctor, except an experimental neurotech startup called Rivermind. In walks a kind saleswoman for the company. Installing the device would be free. The affected bit of the brain would be backed up to the cloud, and restored to the McGuffin every night while you sleep. The catch? it would be $300/month to pay for the subscription service. When the alternative is being dead, $300 is a bargain! OpenAI want $200 a month for the super duper version of ChatGPT after all.\n\nThe satire goes exactly where anyone who understands the arc of consumer tech would guess from the premise. Mike takes on extra shifts to help pay for the service keeping his partner alive, and ends up hustling for money on a website that’s basically Twitch for sadistic humiliation. A premium tier is rolled out, and the basic tier is rapidly enshittified (technical term, don’t blame me). Contextual adverts spew from Amanda’s mouth at inopportune times (without her even being aware), which leads to her being almost fired after an advert for a shady Christian counselling service is unknowingly spewed directly at a pupil needing her support. Her “rest mode” gets further extended, and her brain starts being used for some unstated purpose to benefit the Rivermind corporation, rather like a piece of malware mining cryptocurrency for someone else’s benefit, except it’s living in your own brain. (That’s some real body horror.)\n\nI won’t spoil the ending. It’s as grim as you expect from Black Mirror.\n\nYes, it’s Black Mirror, it’s going to be over the top. Except, of course, the satire is real. People with “obsolete” bionic eyes that are no longer supported by the manufacturer are now losing their sight because… well, I’d love for someone to justify this, because I sure as hell can’t. There was once a time when restoring someone’s sight got people to start spreading rumours about you being the messiah and fulfilling the prophecies foretold in the Book of Isaiah. Well, great news, we achieved a similar feat using technology. Then we cancelled the miracle that’s sitting inside the eyelids of actual human beings when it conflicted with corporate intellectual property rights, and now they will go blind again.\n\nEvery time I think about the notion of the discontinued bionic eyes losing sight, I want to punch a wall. It’s just so incredibly awful and in direct opposition to any humane version of how I envision technology working in society. We have the tech. It works. “AI will fix it?” Fuck right off, we’ve already fixed it. And yet we haven’t because of this? Absolutely ghoulish.\n\nThis episode of Black Mirror is not really about tech. Unlike some vibe-coded fantasy by a founder who doesn’t want to pay software developers (or indeed, large chunks of what passes for enterprise software), the tech here actually does work. The culture around it doesn’t. If you wanted to imagine a world where people have to engage in public displays of self-degradation on social media in order to pay for essential healthcare costs, well, type “insulin GoFundMe” into a search engine. Researchers have written papers on the phenomena.\n\nThe McGuffin in this episode doesn’t need fixing. The laws do. If someone sells you what the tech industry incorrectly refers to as a “product”, then fundamentally changes the terms in this manner… that ought to be an actionable consumer rights issue. Yes, even if in teeny tiny print on page 87 of the contract it says they can do that. But rebalancing consumer rights in an age of enshittified software-as-a-service is boring, involves tradeoffs, and might cost money. Take Smartphones Off Teenagers is super easy in comparison.\n\nThe point here is technology is not a magic box brought down from on high, it is a thing we create, and we collectively can decide how to regulate and shape it. We can just accept some Adolescence-style moral panic about ghostly black rectangles, decide we have no agency to meaningfully morph it to further the goals we want. We could do something, anything, because please won’t someone think of the children. Or we could engage with it in a sensible but extremely critical way. If you want to be extremely critical, you’ve got to understand it first.\n\nTechnologists—the people who create, design and develop the technology—are incredibly critical of technology.\n\nA little sample. Blockchain nonsense: absolutely crap, pretty much just scams. AI: yeah, in small, very well-tested domains it can be useful, but there’s so much hype and nonsense, most of it pushed by politicians and business executives. Data security: Jesus, we’re about a decade behind where we need to be. Those printers that want a subscription fee: yeah, throw that in the garbage and buy whatever the least awful laser printer is this year and use it until it dies.\n\nI frequently find myself explaining that there are viable alternatives to the exploitative treadmill of software-as-a-service subscription-based spyware. And yes, the barely functional AI nonsense they’ve crowbarred into Office, Windows, iOS or WhatsApp is not there for your benefit. It is there because there’s an AI investment bubble, and investors need to be convinced that Microsoft, Apple, Meta et al. have jumped on the hype train, even if it isn’t actually going anywhere worth going.\n\nLet’s go back to Roblox a second. Yes, yes, child predators, pester power, wait, virtual concentration camps. Oh god, do we have to?\n\nWhat does Roblox do right? It gives kids tools to make games. Wait, they don’t call them games, they call them “experiences”, probably because of App Store review rules or whatever. But you can make game-like things and share them with your friends. That’s really cool.\n\nWe have a creation tool here that allows children to creatively use it to express themselves. That’s good at least. Ah, but, I’m sorry, there’s bad stuff when you look a bit harder. Not the murdering-your-classmates genre of bad, but the boring nerdy regulatory kind of bad.\n\nIf you make a game on Roblox, the nice people at the Roblox Corporation pay you money if you do well in the form of Robux, which you can use to buy other stuff on Robux. When kids pester their parents to buy them Robux, the current exchange rate is £4.99 for 500 Robux. You can buy them at a discount if you buy lots of them. The official exchange rate is 1 Robux = $0.0125 (£0.009572 at time of writing).\n\nIf you accumulate lots of Robux because you’ve made a successful “experience” (i.e. game), you can cash out through a process called Developer Exchange, or DevEx. You can only do this when you’ve earned 30,000 Robux. Great, so you get $375, right? Wrong! That’s the rate at which you buy Robux. When you cash them out, you only get $0.0035 per Robux. Your 30,000 Robux turns into $105. You want to ask careful critical questions about technology? “How the hell isn’t this exploitation of children?” is the one that immediately comes to mind, followed quickly by “this is modern day company scrip, we banned that ages ago”.\n\nTo learn more about the absolutely wild world of Roblox, go watch How Roblox Is Exploiting Young Game Developers from People Make Games.\n\nEvery new thing I learn about Roblox makes me genuinely infuriated. You’ve made a platform to let children build things and express their creativity and ruined it with so much garbage. The wall needs punching so bad.\n\nIf a child wants to learn how to write, they need a pen, some paper, and an imagination. The latter is free, the former cost a few pounds at most. If a child wants to learn to make computer games, they should be afforded a way to do so without entering into an exploitative relationship with some massive corporation who will pay them in scrip and keep two thirds of their earnings. Maybe we could build something that gives them the ability to make stuff and learn. And the thing they create is theirs rather than something that Roblox has a hand in.\n\nOn a broader societal level: politicians could pass legislation that is better if they thought about technology in a more nuanced way. The harshest bits of the EU’s Digital Services Act only affects the twenty biggest online platforms, for instance. (Roblox isn’t one of them, curiously.) This is unlike the UK’s Online Safety Act which sets no compliance threshold, and so affects basically everyone. Even the most spooked parent probably doesn’t stay up at night worrying about some kind of Adolescence-style scenario playing out with their little Jamie on the London Fixed Gear and Single Speed forum, a place where cycling nerds discuss, well, fixed gear and single speed bikes. Thankfully, they won’t be troubled by this risk. The faff of complying with the Online Safety Act has led the site administrators to close it down. Google, Meta, Microsoft, Amazon et al. will be able to shoulder whatever regulatory burden anyone puts on them. But the legislation passed ostensibly to stick it to Big Tech has led to lots of people running little hobbyist forums having to read long and incredibly boring Ofcom documents, and take on substantial personal legal risk for their hobby websites if they get it wrong. If you do care about fixed gear bikes, I guess you could find a Facebook group to talk to others about it. That’d really stick it to Zuck.\n\nWhat we lose when the entire debate becomes about the notion of “screen time” and ghostly Satanic rectangles is a dream of someting better. Technology could actually be good and not something that we just hate using. It feels weird to say this because so much of technology is just the product of unpleasant, lazy, cynical people who don’t give a damn. (Or maybe they do, but the incentive structures doesn’t really allow them. The output looks the same.) And non-technical people not only can understand it deeply rather than in reductive tabloid soundbitey chunks and moral panics , they can help build it carefully, deliberatively and in a way that brings out more humanity.\n\nHow you achieve this is a topic I defer to far smarter people, so I can go finish the rest of the new Black Mirror which has turned out to be rather better than I expected it would.", "label": "non_personal"}
{"title": "Neat Websites – Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/blogroll/neat-websites/", "content": "Jump to: Neat projects | Webcomics | Plants & nature | Food | Collections | Math & science | Places | Tools | Seattle\n\n🆕 added April 2025\n\nMore collections: blogroll | interesting people | cool artists | graphic design resources | indie shops | wishlist | big questions\n\nNeat projects\n\nIt’s Post Day! (Sarah Avenir) — email art project\n\nHow Not to Make a Book (Robin Rendle) — documenting the process of creating a book about typography\n\nWerner’s Nomenclature of Colors (Nicholas Rougeux) — A recreation of the original 1821 color guidebook with new cross references, photographic examples, and posters\n\n\n\nScreens, research and hypertext — hypertext book about hypertext — love that meta\n\nbrr.fyi — blog by an anonymous IT worker who overwintered in Antarctica\n\nWA 100 Peaks — photographer and climber Scott Kranz climbed 100 peaks in Washington\n\nJohannes Klingebiel’s digital garden — nice design\n\nEmmanuel Quartey’s “questions” — I like the framing and organization of information\n\nThe Shape of Music Albums — visualization of the characteristics Spotify assigns to tracks, created by Greg Wolanski\n\nTinnitus Tracker — concerts attended by Rob Weychert\n\nAtlas of Intangibles — cool interactive visualization of markers of distinctiveness and wear in London locations\n\n🆕 Atlas of Surveillance — police tech by state in the US (an EFF project)\n\nAdvocacy\n\n🆕 Regulations.gov — did you know there was one website where you could leave comments on like every U.S. rule change? I did not!\n\n🆕 Choose Democracy\n\n🆕 Beautiful Trouble Toolbox — “an interconnected web of ideas and creative best practices that puts the power in your hands”\n\nSpecific Suggestions — “The most potent tools for fighting injustice are the ones already in your hands.”\n\nWebcomics\n\nFalse Knees (Joshua Barkman) – comic strips with goofy birds\n\nwebcomic name (Alex Norris) – “oh no”\n\nPoorly Drawn Lines (Reza Farazmand) – comic strips with returning animal characters\n\nCat and Girl (Dorothy) – social commentary from Cat and Girl\n\nThe Creator’s Guide to Comics* Devices (Reimena Yee) — illustrated guide to tools that comic artists can use in storytelling\n\nInfo\n\nPlants and nature\n\n–> check out my garden section\n\nSmall Seasons — the year divided into two-week segments named for the natural phenomena that tend to happen then (in Japan)\n\nNative Plants PNW — comprehensive listings of northwest native plants\n\nPacific Northwest Wildflowers — photographic database of wildflowers filterable by color and useful for identification\n\nThe Natural Navigator (Tristan Gooley) – interpreting nature sign\n\nCotswald Diary (Chris) – a look into the ongoing work of natural restoration projects\n\nNew Hampshire Garden Solutions – pretty plants and insects\n\noakland garden club (Alexis Madrigal)* – plants and art\n\nClamsplaining (Dan Killam) – clam science\n\nNatural World Facts’ Deep Sea Hub (Leo Richards) — YouTube channel — mesmeric deep ocean videos\n\nWildhope.tv — YouTube channel — documentaries of conservation projects around the world\n\nBumble Bee Watch — report sightings of native bees\n\nPangea Seed — funding marine conservation through art\n\nFood\n\n–> check out recipes I like and saved recipes to try\n\nBudget Bytes Vegetarian Recipes — cheap recipes, usually easy\n\nSmitten Kitchen (Deb Perelmen) — Deb has an inviting writing style – consistently good source of baking recipes\n\nStill Tasty — database of how long food lasts and storage instructions for a wide variety of foods\n\nThe Good Enough Weekly (Devin K. Pope)* – climate and food\n\nEat This Newsletter (Jeremy Cherfas)* – food\n\nTasting History (Max Miller) — YouTube channel — he cooks a historic recipe from basically any time period and talks about its context while it’s cooking\n\nKenji’s Cooking Show (J. Kenji López-Alt) — YouTube channel — down to earth cooking advice from a science minded chef\n\nBlack Farmers Index — directory of Black farmers by region of the USA\n\nCollections\n\nPubMed Central — a free government database of medical journal papers, many of which include free full-text access because the researchers received grant funding 🙌\n\nFederal Open Science Repository of Canada — “federally authored scientific articles and publications from participating science-based departments and agencies” — climate & environment portal\n\nSprout Distro — free printable zines\n\n🆕 An Incomplete SFF Criticism and Studies Reading List (Molly Templeton)\n\nPlaces\n\n–> check out my road trip page\n\nAtlas Obscura — a searchable map and repository of cool destinations around the world — I always check this when I’m planning a trip\n\nClose.city — map with overlays for walking / biking / transit times to major destinations — looks similar to WalkScore but without a score\n\n🆕 Lushootseed Place Names — Google Map of western Washington\n\nMath and Science\n\nStand-up Maths (Matt Parker) — YouTube channel — goofy math questions explored with demos and field trips\n\nVeritasium (Derek Muller) — YouTube channel –longer explainer videos on science and engineering topics, often with cool models\n\nPractical Engineering (Grady Hillhouse) — YouTube channel — simple explanations of engineering practices with model demonstrations and case studies\n\n🆕 Defense Against Dishonest Charts (Nathan Yao) — visual breakdown of the elements of charts and what to look for in evaluating a chart for accuracy\n\nDiff Text – compare two text blocks\n\nLoot Lasso Portfolio Rebalancing Calculator – saw recommended on Reddit, haven’t used 😉\n\nsymbol.wtf – quickly copy and paste symbols\n\n60 Seconds of Advice on Surviving a Nuclear Blast – also see NUKEMAP by Alex Wellerstein\n\n🆕 PairDrop – pair devices to share files instead of emailing\n\n🆕 cFIREsim — calculator for financial planning\n\n🆕 Kinopio — “thinking canvas for new ideas and hard problems”\n\n🆕 Scribe.rip — front end for Medium articles\n\nSeattle area\n\nWashington Smoke Information – invaluable during smoke season\n\nThe Urbanist – Seattle area urbanist news\n\nLushootseed (Tulalip Tribe) – phrases and pronunciation of Lushootseed words “the language of Puget Sound”\n\n🆕 “The Voices of Lushootseed” online Lushootseed lessons *with audio recordings*\n\nWashington Trails Association – an amazing repository of trip notes with current conditions from hikers all across Washington", "label": "non_personal"}
{"title": "TIL: track changes in Emacs with highlight-changes-mode", "url": "https://tommorris.org/posts/2025/til-track-changes-in-emacs-with-highlight-changes-mode/", "content": "I find myself having to review and edit a lot of plain text. You’d think a “bicycle for the mind” could make this better.\n\nNon-programmers have Word’s Tracked Changes. Or the Google Docs equivalent. The point of this is if Person A wrote a document, and sent it to Person B for review, Person B could make a bunch of changes, then send it back to Person A.\n\nProgrammers, meanwhile, have tools that are… more sophisticated? Like diff for one. Both Vim and Emacs have much more sophisticated versions of undo/redo, and tooling like undo-fu because that complexity can be a lot. Plus we have version control systems like Git (or even fancier stuff like Jujutsu which I can’t get on with). If you have the self-discipline and/or tooling to commit lots of little “wip: refactored” changes, you can then use rebase -i , cherry-pick and friends to stitch all those little scraps into a beautiful series of atomic commits that tell a lovely story for both your future self and others. Or git commit -am \"did stuff\" and embrace the chaos demon. But version control is perhaps over-the-top for drafting emails, blog posts and the like.\n\n(If you’re vibe coding, feel free to disregard. Version control is completely optional, and it’s way funnier not to use it.)\n\nIf you write lots of prose, you (or a hallucinating chatbot acting on your behalf) frequently cranks out a crappy first version, or a rough outline, and then you want to go through and polish it meticulously until it doesn’t suck. When doing this, you want to make sure you’ve properly edited everything. If you’ve been noodling around in the intro, then get distracted and pootle around elsewhere, you might entirely skip doing anything about entire paragraphs. In the early days of word procesing, one often printed out the first draft, grabbed a red pen to review it methodically, then returned to the computer to make the edits. While doing this, you could obviously tick off each paragraph after you’ve reviewed it. As previously noted, I see great merit in taking notes by hand, and so, yes, great if you can. But that means you have to use printers which are cursed. And it is a lot of faff, so you often can’t.\n\nThe print-it-out-then-redline-it method is what Tracked Changes reimplements electronically, but Tracked Changes seems to have a baked-in assumption that the person doing the reviewing is separate from the person who wrote it. Which is often the case in a corporate environment… but often not the case. Also, I don’t wanna use Word or Word clones because plain text is good.\n\nWhat I want then is a way to visually see “what have I changed?” as I’m editing a document.\n\nEnter Emacs highlight-changes-mode .\n\nOpen a buffer in Emacs. Type M-x highlight-changes-mode . Start editing. The text you change will be highlighted. The text you don’t change won’t be. If you delete some text, the closest character will be marked with a delete symbol. By default, the changed text is coloured red, and the delete symbol is an underscore.\n\nIf you M-x highlight-changes-mode again, the change markers go away.\n\nI found the default red to be a bit over-the-top, and would rather have the change applied to the background rather than the text itself (also, avoids affecting syntax highlighting). I picked a colour that better matched my preferences and theme.\n\n(custom-set-faces! '(highlight-changes :foreground nil :background \"#004272\") '(highlight-changes-delete :underline t :foreground nil :background \"#004272\"))\n\nHighlight Changes mode does some other stuff too—you can have it cycle through multiple colours, and distinguish between saved and unsaved changes. You can bounce around between bits of the text that you’ve changed and bits you haven’t. I’m not sure I need that… yet. The EmacsWiki has some other fun stuff about how people have used highlight changes, along with some grumpiness about the choice of colours.", "label": "non_personal"}
{"title": "How GitHub supports neurodiverse employees (and how your company can, too)", "url": "https://github.blog/engineering/engineering-principles/how-github-supports-neurodiverse-employees-and-how-your-company-can-too/", "content": "In today’s global workplace, supporting employees by appreciating and understanding their background and lived experience is crucial for the success of any organization. This includes employees who are neurodivergent. Neurodivergence refers to natural variations in human brains and cognition. The term encompasses conditions such as autism, ADHD, dyslexia, mental illness, and other neurological differences.\n\nNeurodivergent employees don’t just enrich the workplace, they’re good for business. According to Deloitte, teams with neurodivergent people can be up to 30 percent more productive than others. Neurodivergent folks excel in pattern recognition and the type of outside-the-box thinking highly sought after in the software industry.\n\nIn this blog post, we’ll take a look at five ways GitHub fosters and supports neurodiverse employees via Neurocats, a GitHub Community of Belonging (CoB), and how you can do the same at your organization.\n\nLet’s go!\n\nForktocat: An Octocat image that represents the fork function in Git, which we’ve adopted in Neurocats to represent the different ways our brains work.\n\n1. Establish supportive communities\n\nAs an initial step, establish private, supportive communities where neurodivergent employees can connect, share their experiences, and find support. GitHub’s Neurocats community allows members to privately discuss their neurodivergence, offer advice to each other, and build a sense of belonging, all in a safe place where members can freely express themselves without fear.\n\nNeurocats started as a private Slack channel under a different name years before it formally transitioned into a CoB. Originally called #neuroconverse, it gave the neurodivergent community at GitHub a space to chat. In the summer of 2021, a collection of passionate members started discussions with GitHub’s Diversity Inclusion and Belonging team about becoming a formal CoB. In October 2021, they formed as an official group at GitHub, and after some discussion, became the Neurocats. The community now consists of hundreds of members from across the company and continues to grow.\n\nSetting up spaces for neurodivergent individuals to express themselves and meet other like-minded friends and allies not only improves their overall work life balance, it also accelerates the creation of new innovative ideas that could be the next big thing in your organization’s portfolio.\n\n“As a neurodivergent people manager with dyslexia and dysgraphia, I am thrilled to be part of the Neurocats CoB, a community that embraces and normalizes our uniqueness,” says Tina Barfield, senior manager at GitHub. “By doing so, we can help drive environments where everyone’s strengths are celebrated, leading to greater innovation, creativity, and inclusivity.” (Please note, all employee names and stories have been shared with permission.)\n\nSuggestions for establishing a supportive community: Have members lead. Embrace the powerful slogan, “Nothing about us, without us.”\n\nEmbrace the powerful slogan, “Nothing about us, without us.” Consider how to protect confidentiality and anonymity. Many neurodivergent people may want to control to whom and in what context they share their neurodivergence. Always obtain permission before sharing information that identifies a person’s neurodivergence.\n\nMany neurodivergent people may want to control to whom and in what context they share their neurodivergence. Always obtain permission before sharing information that identifies a person’s neurodivergence. Consider conversation history. In Neurocats, conversation history is wiped every four days.\n\nIn Neurocats, conversation history is wiped every four days. Never require diagnosis for membership to the community. In many cultures and countries, getting a formal diagnosis can be difficult if not impossible. If someone believes they are neurodivergent, accept them and support them.\n\n2. Foster a sense of belonging\n\nGiving employees the time and space to discuss their neurodivergence enables them to strongly relate to each other, lift each other up, and make personal discoveries that will help them navigate life both at work and at home.\n\n“I didn’t know what being neurodivergent was before Neurocats,” says Lou Nelson, support engineer III who works on GitHub Premium Support. “I thought I was a weird kid with an ADHD diagnosis. Neurocats has become the lynchpin for my career. I have made valuable connections and have a deeper insight into myself than I could have ever done alone. As a member, I find it incumbent to share this experience with others so that they also don’t have to feel alone.”\n\nWhen neurodivergent employees feel comfortable enough to share their stories more broadly, other employees will be drawn to those communities to either personally relate or learn and empathize about subjects they may not have previously considered.\n\n“As a people manager with ADHD, I’m accustomed to being the ‘neurodiversity pioneer’ when meeting new teams or direct reports, setting an example by speaking openly about my gifts and challenges,” says Julie Kang, staff manager of software engineering at GitHub. “When I joined GitHub, and especially when I became a Neurocat, I was pleasantly surprised to find a culture that was knowledgeable, accepting, and celebratory of neurodiversity at a level I haven’t seen before in my career.”\n\nSuggestions for encouraging a sense of belonging in your neurodivergent community: Declare safe spaces. Before meetings where neurodivergence is discussed, call out the fact that you want this to be a safe space. Encourage kindness and empathy.\n\nBefore meetings where neurodivergence is discussed, call out the fact that you want this to be a safe space. Encourage kindness and empathy. Warn about oversharing. If there’s a meeting where oversharing is possible, warn participants to think about their contributions and avoid sharing details that they may later regret.\n\nIf there’s a meeting where oversharing is possible, warn participants to think about their contributions and avoid sharing details that they may later regret. Understand privacy and confidentiality expectations. Participants should respect privacy and adopt guiding principles around privacy and confidentiality.\n\nParticipants should respect privacy and adopt guiding principles around privacy and confidentiality. Share information broadly with the company or organization. Create mechanisms where members of the community can safely share thoughts and feelings with the broader company or organization and, if appropriate, externally. Anonymize this information if required and always share with consent. Examples can include recurring Q&A sessions, presentations, and discussions with the entire company, organization or a segment of the organization all help allow neurodivergent employees to be seen.\n\n3. Provide flexibility and accommodations\n\nNeurodivergent employees can often benefit from flexible working arrangements. This could include flexible hours, remote work options, noise-canceling headphones, or customized workspaces to reduce sensory overload.\n\nAsking for accommodations can be hard. Identify the process your organization or company uses to assess workplace accommodations. Encourage employees to utilize that process to obtain a workplace accommodation.\n\n“One of the biggest things for me has been seeing how many other folks went through a lot of their life being told that they just needed to apply themselves, pay attention, work harder, etc. only to repeatedly fail out of college, get fired from jobs, and generally struggle to ‘human’ correctly,” says Caite Palmer, manual review analyst of security operations at GitHub. “These folks are now through all departments and levels at this large, successful company getting to do great work in a place where flexibility, asking a million questions, and problem solving are generally considered tremendous assets and encouraged.”.\n\nSuggestions for providing flexibility and accommodations: Listen to your employees. Identify listening mechanisms to hear concerns from your employees. An employee or group of employees may share with you how a particular process or system is difficult. Their honesty and candor is a gift and an opportunity to improve your business.\n\nIdentify listening mechanisms to hear concerns from your employees. An employee or group of employees may share with you how a particular process or system is difficult. Their honesty and candor is a gift and an opportunity to improve your business. Consider providing a budget. At GitHub we have flexible benefits that allow employees to purchase equipment and tools which can help them focus, work, and even relax. Removing barriers for employees to obtain bespoke solutions that work for them reduces costs by removing long conversations and approvals from the process.\n\nAt GitHub we have flexible benefits that allow employees to purchase equipment and tools which can help them focus, work, and even relax. Removing barriers for employees to obtain bespoke solutions that work for them reduces costs by removing long conversations and approvals from the process. Train managers to care. Part of the GitHub manager training program is geared to help managers foster a sense of caring and empathy for the employees in their teams. Encourage kindness across all levels of the company.\n\nPart of the GitHub manager training program is geared to help managers foster a sense of caring and empathy for the employees in their teams. Encourage kindness across all levels of the company. Be clear about tasks that need to be completed and flexible on how that task is accomplished. An autistic employee might go for a long walk and then be able to hyperfocus on a task, completing it in half the time than it would have taken if they’d sat at their desk the whole time. Trust your employees to do their best work in the way that fits them and judge them on the work they produce.\n\n4. Encourage open dialogue\n\nPromote a culture of openness where employees feel comfortable discussing their needs and challenges. Consider holding regular meetings and forums to discuss topics related to neurodiversity, mental health, and well-being. With the Neurocats group, we hold monthly meetings to discuss various topics, which are important to our members. One member of the Neurocats leadership team describes their experience:\n\n“We have a voice, which we use to highlight issues our members face day to day,” says Owen Niblock, senior software engineer at GitHub who works on accessibility. “We also hold monthly meetings to discuss topics from ADHD and autism to anxiety, mental health issues, and more. Over the years, we’ve had some success and find we are able to lobby for changes at a company level, leading to real tangible change that benefits the whole of GitHub.”\n\nEnabling open dialogue means providing avenues for these discussions to happen. But going one step further and encouraging open dialogue requires more effort.\n\nSuggestions for encouraging open and honest dialogue with the neurodivergent community at your company: Listen. You won’t be able to act on every piece of feedback you receive, but showing you acknowledge and appreciate feedback and making changes whenever possible will create a culture of frank and honest discussion.\n\nYou won’t be able to act on every piece of feedback you receive, but showing you acknowledge and appreciate feedback and making changes whenever possible will create a culture of frank and honest discussion. Make space. Conversations can happen at many levels and in many forums. Create space for a diverse set of feedback and ideas by accepting discussions in different formats. For example, you might have a meeting to discuss something with a follow-up GitHub Discussion to collect async ideas.\n\nConversations can happen at many levels and in many forums. Create space for a diverse set of feedback and ideas by accepting discussions in different formats. For example, you might have a meeting to discuss something with a follow-up GitHub Discussion to collect async ideas. Give access to the leadership team. At GitHub all our CoBs have an executive sponsor who’s a member of GitHub’s senior leadership team. This gives members an ally to raise issues with, help find solutions, and communicate problems directly upwards.\n\nAt GitHub all our CoBs have an executive sponsor who’s a member of GitHub’s senior leadership team. This gives members an ally to raise issues with, help find solutions, and communicate problems directly upwards. Lead from the very top. Provide opportunities for CoBs to meet with senior leadership. For example, this year, each of GitHub’s CoBs have a meeting with GitHub’s CEO as part of a listening tour. This shows that the community’s views are important to the whole business.\n\n5. Celebrate neurodiversity\n\nAcknowledge and celebrate the unique contributions of neurodiverse employees. Recognize their achievements, provide opportunities for career advancement, and ensure they have a voice in the organization. Celebrating Disability Pride Month and other related events can help raise awareness and appreciation within the company.\n\n“Neurocats was the first time I found people like me not only represented at work, but celebrated and successful,” Palmer says. “Sharing the rough days, the burnout, the overwhelm and frustration, but also the wins of finally getting appropriate support, being seen as creative instead of weird, and getting to learn about all the different ways brains can function.”\n\nCelebrations should come not just from the community but also from leadership and the People Team. Sharing posts about the company’s mental health benefits during Mental Health Month (in May) or sharing information about the community during meetings or training can all help to celebrate your diverse workforce.\n\nSuggestions for celebrating neurodiversity: Celebrations should come from all levels of the company. An internal post by a member of the leadership team or an informative post by the People team or Human Resources can show your company’s support.\n\nAn internal post by a member of the leadership team or an informative post by the People team or Human Resources can show your company’s support. Don’t sugar coat it. Being neurodivergent can be hard. It’s not a superpower. Celebrate the diversity and the wins, but never minimize the real struggles that many neurodivergent people deal with every day.\n\nBeing neurodivergent can be hard. It’s not a superpower. Celebrate the diversity and the wins, but never minimize the real struggles that many neurodivergent people deal with every day. Make it visible. Share things in the format that makes most sense and will reach the most people.\n\nShare things in the format that makes most sense and will reach the most people. Promote. Ask members of the community to share information on your company blog or website.\n\nBy implementing these strategies, you can create an inclusive environment where neurodivergent employees feel valued, supported, and empowered to contribute their best work.\n\n“Neurocats provided an environment that made me feel safe and confident in an astonishingly short amount of time, allowing me to bring my A game, leverage my strengths, and make a positive impact much sooner than usual,” says Julie Kang, staff manager of software engineering at GitHub. “The support and understanding here have been truly transformative for my professional growth, and I feel equipped to pay this forward to my peers and reports.”\n\nInterested in learning more about GitHub’s approach to accessibility? Visit accessibility.github.com.\n\nTags:", "label": "non_personal"}
{"title": "The ultimate guide to developer happiness", "url": "https://github.blog/engineering/engineering-principles/the-ultimate-guide-to-developer-happiness/", "content": "In today’s rapidly evolving landscape, where AI is reshaping industries and transforming workflows, the role of developers has never been more critical. As business leaders, fostering an environment where developers feel valued, motivated, and empowered is essential to harnessing their full potential and keeping your business profitable and innovative.\n\nIn this blog post, we’ll explore actionable tips and strategies to supercharge developer happiness, ensuring your team remains productive, engaged, and ahead of the AI curve. We’ll walk you through ways to secure your code with AI, how to increase productivity with a strong developer experience, and, of course, invite you to join us at GitHub Universe 2024 to see the very best of the latest AI tooling in action.\n\nBoost productivity with a great developer experience\n\nDeveloper experience is more than just a buzzword—it’s a critical factor in driving productivity and collaboration within software development teams. A seamless developer experience allows developers to get into the flow state more easily, where their productivity and creativity can peak. This flow state—characterized by uninterrupted concentration and a deep sense of involvement in the task—is crucial for tackling complex coding challenges.\n\nThis work environment needs to be built intentionally, and the research backs it up. Developers who carve out time for deep work enjoy 50% more productivity, while those that get work they find engaging are 30% more productive.\n\nHow does this impact businesses? Well, because a developer that can significantly reduce their context-switching and mental load can also produce code faster and at a higher quality.\n\nWhen developers understand their code, they’re 42% more productive. When developers are able to get faster turnaround times, they are 20% more innovative. These are tangible, individual benefits that in turn directly impact the output of developer teams.\n\nNow is the time for leaders to invest in creating a great developer experience. By prioritizing the developer experience, you’re setting your team up to harness the full potential of the latest AI and platform engineering advances, ensuring your business stays ahead of the curve. Curious to learn more? Then dive into how a great developer experience fuels productivity with our latest research.\n\nUse AI to secure your code\n\nHistorically, developers and security teams have found themselves at odds due to competing business goals. Shifting security left incorporates security earlier in the software development lifecycle, but in practice it has primarily shifted responsibility to developers without necessarily giving them the required expertise.\n\nThis, combined with the context switching inherent in development work, makes addressing security concerns particularly challenging. With AI, developers now have powerful tools at their disposal to enhance code security. AI can:\n\nImprove detection rates\n\nProvide near-instant fixes with context\n\nEnable application security (AppSec) at scale\n\nThese three improvements make it easier for developers to integrate robust security measures without sacrificing productivity, and transform the relationship between developers and security teams into a collaborative partnership.\n\nIntroducing a new security tool doesn’t have to be a daunting task either. By following a few simple steps, organizations can ensure a smooth transition and broad adoption.\n\nDocument the tool’s features and usage to set the foundation and set realistic expectations to help align goals across teams.\n\nthe tool’s features and usage to set the foundation and set realistic expectations to help align goals across teams. Recognize and celebrate successes to showcase the value of the new tool.\n\nand celebrate successes to showcase the value of the new tool. Adopt a go-with-the-flow approach and organize hackathons to further drive engagement and interest.\n\na go-with-the-flow approach and organize hackathons to further drive engagement and interest. Listen to developer feedback continuously improve and refine security practices.\n\nAI-powered security tools not only enhance the efficiency and effectiveness of AppSec, but also empower developers to take a proactive role in securing their code. This shift not only improves overall security posture, but also fosters a culture of shared responsibility and continuous learning, ultimately leading to more secure and resilient applications.\n\nSee exactly why security should be built into the developer workflow. 👇\n\nCustomize your LLMs\n\nOrganizations that take AI a step further and customize their AI tools are poised to lead the pack.\n\nLarge language models (LLMs) are trained on vast amounts of text data and can perform a variety of natural language processing tasks like translation, summarization, question-answering, and text generation. Customizing a pre-trained LLM goes beyond mere training—it involves adapting the model to perform specific tasks relevant to the organization’s needs. This level of customization helps developers maintain their flow state and significantly boost productivity and efficiency.\n\nCustomization techniques like retrieval-augmented generation (RAG), in-context learning, and fine-tuning enable LLMs to deliver more accurate and contextually appropriate responses:\n\nRAG combines retrieval-based and generation-based approaches in natural language processing. It enhances LLMs by integrating information retrieval techniques, where relevant documents or snippets are retrieved from a vector database to assist in generating more accurate and contextually appropriate responses. This approach allows the model to access and utilize external knowledge, making the generated output more informed and relevant to the user’s query.\n\ncombines retrieval-based and generation-based approaches in natural language processing. It enhances LLMs by integrating information retrieval techniques, where relevant documents or snippets are retrieved from a vector database to assist in generating more accurate and contextually appropriate responses. This approach allows the model to access and utilize external knowledge, making the generated output more informed and relevant to the user’s query. In-context learning refers to a model’s ability to adapt and respond to new tasks or inputs based on the context provided in the input prompt without requiring additional training. The model leverages its pre-trained knowledge and the context given in the input to perform tasks effectively.\n\nrefers to a model’s ability to adapt and respond to new tasks or inputs based on the context provided in the input prompt without requiring additional training. The model leverages its pre-trained knowledge and the context given in the input to perform tasks effectively. Fine-tuning, on the other hand, is a process in which an LLM is further trained on a specific dataset to adapt it to a particular task or domain. During fine-tuning, the model’s parameters are adjusted based on the new dataset, which typically involves supervised learning with labeled data. This process allows the model to specialize and improve its performance on specific tasks, (such as text classification, question answering, or machine translation), by leveraging the general knowledge acquired during its initial pre-training phase.\n\nBy implementing these customization strategies, businesses can unlock the full potential of their AI tools. Customized LLMs not only improve developer productivity—they also enhance the quality and relevance of AI-generated content.\n\nPrepare your repository for teamwork\n\nFostering collaboration doesn’t just make software development faster, it also helps teams build better products and boost job satisfaction. By making your repository as collaborative as possible, you’ll optimize success. This includes focusing on:\n\nRepository settings : properly configuring repository settings to control visibility, access, and contribution workflows lays the foundation for collaboration.\n\n: properly configuring repository settings to control visibility, access, and contribution workflows lays the foundation for collaboration. Repository contents : including essential files like README.md, LICENSE.md, CONTRIBUTING.md, CODEOWNERS, and CODE_OF_CONDUCT.md helps collaborators understand the project, its purpose, and how to contribute.\n\n: including essential files like README.md, LICENSE.md, CONTRIBUTING.md, CODEOWNERS, and CODE_OF_CONDUCT.md helps collaborators understand the project, its purpose, and how to contribute. Automation and checks : implementing automation tools such as linters, continuous integration (CI), and continuous deployment (CD) pipelines streamlines the development process, ensures code quality, and enables immediate feedback.\n\n: implementing automation tools such as linters, continuous integration (CI), and continuous deployment (CD) pipelines streamlines the development process, ensures code quality, and enables immediate feedback. Security practices : enforcing role-based access control, managing secrets securely, and scanning code for vulnerabilities can foster trust and protect the project from vulnerabilities.\n\n: enforcing role-based access control, managing secrets securely, and scanning code for vulnerabilities can foster trust and protect the project from vulnerabilities. Issue templates : providing structured issue templates guides contributors in providing necessary information and context when reporting bugs.\n\n: providing structured issue templates guides contributors in providing necessary information and context when reporting bugs. Community engagement: engaging with the project’s community through meetups, project blogs, discussions, and other channels fosters belonging and builds relationships.\n\nInvest in your team’s learning opportunities\n\nWhen you signal to your team that you value their career growth and exposure to learning opportunities, it can boost happiness and job satisfaction, leading to increased productivity, collaboration, and better problem solving.\n\nEncouraging your developer teams to attend conferences like GitHub Universe 2024 is a strategic investment in their professional growth and your business’ success. Our global developer event provides an unparalleled platform for the best in software development to gather and expand their knowledge, stay updated on the latest AI-powered tools, and bring fresh ideas back to their teams.\n\nHere are a few highlights of what you and your team can expect:\n\nHelp your developers get in the flow and stay there with sessions, demos, panels, and more on the powerful tools and techniques that enhance productivity and satisfaction.\n\nand stay there with sessions, demos, panels, and more on the powerful tools and techniques that enhance productivity and satisfaction. Connect with other technical leaders to share experiences, challenges, and best practices. Expand your network with valuable industry contacts.\n\nto share experiences, challenges, and best practices. Expand your network with valuable industry contacts. Get a first look at GitHub’s product roadmap and see how upcoming features and enhancements can help you stay ahead in a competitive landscape.\n\nand see how upcoming features and enhancements can help you stay ahead in a competitive landscape. Gain technical skills with GitHub certifications and workshops designed to enhance your expertise in a rapidly evolving industry.\n\nwith GitHub certifications and workshops designed to enhance your expertise in a rapidly evolving industry. Learn the latest on GitHub Copilot and stay ahead with the latest coding practices and techniques.\n\nGet your tickets today. You can take advantage of our group discount and get four tickets for the price of three. (That’s a 25% savings!)\n\nIf you’re flying solo, you can also use our Early Bird discount and save 20% off one in-person ticket, only until September 3.\n\nReach new levels of creativity and efficiency\n\nIncorporating these five business strategies can transform your development process and increase developer happiness. By investing in these areas, you empower your team, foster a culture of continuous learning, and position your organization for success in the rapidly evolving tech landscape.", "label": "non_personal"}
{"title": "GitHub Enterprise Cloud with data residency: How we built the next evolution of GitHub Enterprise using GitHub", "url": "https://github.blog/engineering/engineering-principles/github-enterprise-cloud-with-data-residency/", "content": "Today, we announced that GitHub Enterprise Cloud will offer data residency, starting with the European Union (EU) on October 29, 2024, to address a critical desire from customers and enable an optimal, unified experience on GitHub for our customers.\n\nData residency and what it means for developers\n\nWe’ve heard for years from enterprises that being able to control where their data resides is critical for them. With data residency, organizations can now store their GitHub code and repository data in their preferred geographical region. With this need met, even more developers across the globe can build on the world’s AI-powered developer platform.\n\nEnterprise Cloud with data residency provides enhanced user control and unique namespaces on ghe.com isolated from the open source cloud on github.com. It’s built on the security, business continuity, and disaster recovery capabilities of Microsoft Azure.\n\nThis is a huge milestone for our customers and for GitHub–a multi-year effort that required extensive time, effort, and dedication across the company. We’re excited to share a behind-the-scenes look at how we leveraged GitHub to develop the next evolution of Enterprise Cloud.\n\nDesigning the architecture for the next evolution of GitHub Enterprise\n\nThis effort started in summer of 2022 with a proof of concept (PoC) and involved teams across GitHub. We carefully considered which architecture would enable us to be successful. After iterating with different approaches, we decided to build the new offering as a feature set that extends Enterprise Cloud. This approach would allow us to be consistently in sync with features on github.com and provide the performance, reliability, and security that our customers expect. For hosting, we effectively leveraged Microsoft Azure’s scale, security, and regional footprint to produce a reliable and secured product with data residency built-in, without having to build new data centers ourselves.\n\nAs the home for all developers, developer experience is critically important for us. We recognized early on that consistency was important, so we sought to minimize differences in developing for Enterprise Cloud and Enterprise Cloud with data residency. To this end, the architecture across both is very similar, reducing complexity, risk, and development costs. The deployment model is familiar to our developers: it builds off of GitHub Actions. Also, changes to github.com and Enterprise Cloud with data residency are deployed minutes apart as part of a unified pipeline.\n\nTo accomplish this, we had to organize the work, modify our build and deployment systems, and validate the quality of the platform. We were able to do all three of these by using GitHub.\n\nOrganizing with GitHub Issues and Projects\n\nTo organize the project, we used GitHub Issues and Projects, taking advantage of multiple views to effectively drive work across multiple projects, more than 100 teams, and over 2,000 issues. Different stakeholders and teams could take advantage of these views to focus on the information most relevant to them. Our talented technical project management team helped coordinate updates and used the filtering and slicing capabilities of Projects to present continuously updated information for each milestone in an easily consumable way.\n\nWe also used upcoming features like issues hierarchy to help us understand relationships between issues, and issue types to help clearly classify issues across repositories. As part of using these features internally we were able to give feedback to the teams working on them and refine the final product. Keep an eye out for future announcements for issues hierarchy and issue types coming soon!\n\nAll of these powerful features helped us keep the initiative on track. We were able to clearly understand potential risk areas and partner across multiple teams to resolve blockers and complex dependencies, keeping the project effectively moving forward across multiple years.\n\nBuilding Enterprise Cloud with data residency using GitHub\n\nGitHub has always been built using GitHub. We wanted to continue this practice to set ourselves up for success with the new data residency feature. To this end, we continued leveraging GitHub Codespaces for development and GitHub Actions for continuous integration (CI). In addition, we added deployment targets for new regions. This produced a development, testing, and CI model that required no changes for our developers and a deployment process that was tightly integrated into the existing flow.\n\nWe have previously discussed our deploy then merge model, where we deploy branches before merging into the main branch. We expanded this approach to include successful deployments to Enterprise Cloud data residency targets before changes could be merged and considered complete, continuing to use the existing GitHub merge queue. A visualization of our monolithic deployment pipeline is shown in the figure below.\n\nWe start by deploying to environments used by GitHub employees in parallel. This includes the internal environment for Enterprise Cloud with data residency discussed more in the next section. As we use GitHub every day to build GitHub, this step helps us catch issues as employees use the product before it impacts our customers. After automated and manual testing, we proceed to roll out to “Canary.” Canary is the name for the stage where we configure our load balancers to gradually direct an increasing percentage of github.com traffic to the updated version of the code in a staged manner. Additional testing occurs in between each stage. Once we successfully deploy the updated version of github.com to all users, we then deploy and validate Enterprise Cloud with data residency in the EU before finishing the process and merging the pull request.\n\nEnsuring all deployments are successful before we merge means changes are deployed in sync across all Enterprise Cloud environments and monitored effectively. Note that in addition to deployments, we also use feature flags to gradually roll out changes to groups of customers to reduce risk. If a deployment to any target fails, we roll back the change completely. Once we have understood the failure and are ready to deploy again, the entire process starts from the beginning with the merge queue.\n\nFinally, to maintain consistency across all teams and services, we created automation to generate deployment pipelines for over 100 services so, as new targets are introduced, each service automatically deploys to the new environment in a consistent order.\n\nUsing Enterprise Cloud with data residency ourselves\n\nTo create the best possible product, we also prioritized using it ourselves and stood up an isolated environment for this purpose. Using our GitHub migration tooling, we moved the day-to-day development for the team working on GitHub Enterprise Importer to that environment, and invested in updating our build, deploy, and development environments to support working from the data resident environment. Since its creation, we have deployed to this environment over 8,000 times. This gave us invaluable feedback about the experience of working in the product with issues, pull requests, and actions that we were able to address early in the development process. We were also able to iterate on our status page tooling and internal Service Level Objective (SLO) process with the new environment in mind. The team is continuing to work in this environment today and runs over 1,000 actions jobs a month. This is a testament to the stability and quality we’ve been able to deliver and our commitment to this feature.\n\nWhat’s next\n\nWe are proud that we’ve been able to evolve Enterprise Cloud to offer data residency while using GitHub to organize, build, deploy, and test it. We’re excited to unlock GitHub for even more developers and for you to experience what we have built, starting on October 29, 2024 in the EU, with more regions on the way.\n\nIf you’re excited about Enterprise Cloud with data residency, please join us at GitHub Universe 2024 to learn more and hear from other companies how they’ve used this to accelerate software development and innovation.\n\nTags:", "label": "non_personal"}
{"title": "How to make Storybook Interactions respect user motion preferences", "url": "https://github.blog/engineering/user-experience/how-to-make-storybook-interactions-respect-user-motion-preferences/", "content": "Recently, while browsing my company’s Storybook, I came across something that seemed broken: a flickering component that appeared to be re-rendering repeatedly. The open source tool that helps designers, developers, and others build and use reusable components was behaving weirdly. As I dug in, I realized I was seeing the unintended effects of the Storybook Interactions addon, which allows developers to simulate user interactions within a story, in action.\n\nStorybook Interactions can be a powerful tool, enabling developers to simulate and test user behaviors quickly. But if you’re unfamiliar with Interactions—especially if you’re just looking to explore available components—the simulated tests jumping around on the screen can feel disorienting.\n\nThis can be especially jarring for users who have the prefers-reduced-motion setting enabled in their operating system. When these users encounter a story that includes an interaction, their preferences are ignored and they have no option to disable or enable it. Instead, the Storybook Interaction immediately plays on page load, regardless. These rapid screen movements can cause disorientation for users or in some cases can even trigger a seizure.\n\nKnowledge share Operating systems allow users to set a motion preference. Adhering to this setting can be critical for some users. For example, for users who have photosensitive epilepsy or vertigo, even a small animation can be life-threatening. This explicit preference for a reduced motion experience can be used by browsers, applications, and websites to reduce unnecessary animations and motions via the prefers-reduced-motion CSS media feature.\n\nAt this time, Storybook does not have built-in capabilities to toggle interactions on or off. Until this feature can be baked in I am hoping this blog will provide you with an alternative way to make your work environment more inclusive. Now, let’s get into building an addon that respects user’s motion preferences and allows users to toggle interactions on and off.\n\nGoals\n\nUsers with prefers-reduced-motion enabled MUST have interactions off by default. Users with prefers-reduced-motion enabled MUST have a way to toggle the feature on or off without altering their operating system user preferences. All users SHOULD have a way to toggle the feature on or off without altering their user preferences.\n\nLet’s get started\n\nStep 1: Build a Storybook addon\n\nStorybook allows developers to create custom addons. In this case, we will create one that will allow users to toggle Interactions on or off, while respecting the prefers-reduced-motion setting.\n\nAdd the following code to a file in your project’s .storybook folder:\n\nimport React, {useCallback, useEffect} from 'react' import {IconButton} from '@storybook/components' import {PlayIcon, StopIcon} from '@storybook/icons' export const ADDON_ID = 'toggle-interaction' export const TOOL_ID = `${ADDON_ID}/tool` export const INTERACTION_STORAGE_KEY = 'disableInteractions' export const InteractionToggle = () => { const [disableInteractions, setDisableInteractions] = React.useState( window?.localStorage.getItem(INTERACTION_STORAGE_KEY) === 'true', ) useEffect(() => { const reducedMotion = matchMedia('(prefers-reduced-motion)') if (window?.localStorage.getItem(INTERACTION_STORAGE_KEY) === null && reducedMotion.matches) { window?.localStorage?.setItem(INTERACTION_STORAGE_KEY, 'true') setDisableInteractions(true) } }, []) const toggleMyTool = useCallback(() => { window?.localStorage?.setItem(INTERACTION_STORAGE_KEY, `${!disableInteractions}`) setDisableInteractions(!disableInteractions) // Refreshes the page to cause the interaction to stop/start window.location.reload() }, [disableInteractions, setDisableInteractions]) return ( <IconButton key={TOOL_ID} aria-label=\"Disable Interactions\" onClick={toggleMyTool} defaultChecked={disableInteractions} aria-pressed={disableInteractions} > {disableInteractions ? <PlayIcon /> : <StopIcon />} Interactions </IconButton> ) }\n\nCode breakdown\n\nThis addon stores user preferences for Interactions using window.localStorage . When the addon first loads, it checks whether the preference is already set and, if so, it defaults to the user’s preference.\n\nconst [disableInteractions, setDisableInteractions] = React.useState( window?.localStorage.getItem(INTERACTION_STORAGE_KEY) === 'true', )\n\nThis useEffect hook checks if a user has their motion preferences set to prefers-reduced-motion and ensures that Interactions are turned off if the user hasn’t already set a preference in Storybook.\n\nuseEffect(() => { const reducedMotion = matchMedia('(prefers-reduced-motion)') if (window?.localStorage.getItem(INTERACTION_STORAGE_KEY) === null && reducedMotion.matches) { window?.localStorage?.setItem(INTERACTION_STORAGE_KEY, 'true') setDisableInteractions(true) } }, [])\n\nWhen a user clicks the toggle button, preferences are updated and the page is refreshed to reflect the changes.\n\nconst toggleMyTool = useCallback(() => { window?.localStorage?.setItem(INTERACTION_STORAGE_KEY, `${!disableInteractions}`) setDisableInteractions(!disableInteractions) // Refreshes the page to cause the interaction to stop/start window.location.reload() }, [disableInteractions, setDisableInteractions])\n\nStep 2: Register your new addon with Storybook\n\nIn your .storybook/manager file, register your new addon:\n\naddons.register(ADDON_ID, () => { addons.add(TOOL_ID, { title: 'toggle interaction', type: types.TOOL as any, match: ({ viewMode, tabId }) => viewMode === 'story' && !tabId, render: () => <InteractionToggle />, }) })\n\nThis adds the toggle button to the Storybook toolbar, which will allow users to change their Storybook Interaction preferences.\n\nStep 3: Add functionality to check user preferences\n\nFinally, create a function that checks whether Interactions should be played and add it to your interaction stories:\n\nimport {INTERACTION_STORAGE_KEY} from './.storybook/src/InteractionToggle' export const shouldInteractionPlay = () => { const disableInteractions = window?.localStorage?.getItem(INTERACTION_STORAGE_KEY) return disableInteractions === 'false' || disableInteractions === null } export const SomeComponentStory = { render: SomeComponent, play: async ({context}) => { if (shouldInteractionPlay()) { ... } }) }\n\nWith this custom addon, you can ensure your workplace remains accessible to users with motion sensitivities while benefiting from Storybook’s Interactions. For those with prefers-reduced-motion enabled, motion will be turned off by default and all users will be able to toggle interactions on or off.", "label": "non_personal"}
{"title": "Breaking down CPU speed: How utilization impacts performance", "url": "https://github.blog/engineering/architecture-optimization/breaking-down-cpu-speed-how-utilization-impacts-performance/", "content": "Introduction ⛵\n\nThe GitHub Performance Engineering team regularly conducts experiments to observe how our systems perform under varying load conditions. A consistent pattern in these experiments is the significant impact of CPU utilization on system performance. We’ve observed that as CPU utilization rises, it can lead to increased latency, which provides an opportunity to optimize system efficiency. Addressing this challenge allows us to maintain performance levels while reducing the need for additional machines, ultimately preventing inefficiencies.\n\nAlthough we recognized the correlation between higher CPU utilization and increased latency, we saw an opportunity to explore the specific thresholds and impacts at various stages in greater detail. With a diverse set of instance types powered by different CPU families, we focused on understanding the unique performance characteristics of each CPU model. This deeper insight empowered us to make smarter, data-driven decisions, enabling us to provision our infrastructure with greater efficiency and confidence.\n\nWith these goals in mind, we embarked on a new journey of exploration and experimentation to uncover these insights.\n\nExperiment setup 🧰\n\nCollecting accurate data for this type of experiment was no easy feat. We needed to gather data from workloads that were as close to our production as possible, while also capturing how the system behaves under different phases of load. Since CPU usage patterns vary across workloads, we focused primarily on our flagship workloads. However, increasing the load could introduce small performance discrepancies, so our goal was to minimize disruption for our users.\n\nFortunately, a year ago, the Performance Engineering team developed an environment designed to meet these requirements, codenamed Large Unicorn Collider (LUC). This environment operates within a small portion of our Kubernetes clusters, mirroring the same architecture and configuration as our flagship workloads. It also has the flexibility to be hosted on dedicated machines, preventing interference from or with other workloads. Typically, the LUC environment remains idle, but when needed, we can direct a small, adjustable amount of traffic towards it. Activating or deactivating this traffic takes only seconds, allowing us to react quickly if performance concerns arise.\n\nTo accurately assess the impact of CPU utilization, we first established a baseline by sending moderate production traffic to a LUC Kubernetes pod hosted on one of its dedicated machines. This provided us with a benchmark for comparison. Importantly, the number of requests handled by the LUC pods remained constant throughout the experiment, ensuring consistent CPU load over time.\n\nOnce the baseline was set, we gradually increased CPU utilization using a tool called “stress,” which artificially occupies a specified number of CPU cores by running random processing tasks. Each instance type has a different number of CPU cores, so we adjusted the steps accordingly. However, the common factor across all instances was the total CPU utilization.\n\nNote: It’s important to recognize that this is not a direct 1:1 comparison to the load generated by actual production workloads. The stress tool continuously runs mathematical operations, while our production workloads involve I/O operations and interrupts, which place different demands on system resources. Nevertheless, this approach still offers valuable insights into how our CPUs perform under load.\n\nWith the environment set up and our plan in place, we proceeded to collect as much data as possible to analyze the impact.\n\nResults 📃\n\nWith our experiment setup finalized, let’s examine the data we gathered. As previously mentioned, we repeated the process across different instance types. Each instance type showed unique behavior and varying thresholds where performance started to decline.\n\nAs anticipated, CPU time increased for all instance types as CPU utilization rose. The graph below illustrates the CPU time per request as CPU utilization increases.\n\nCPU time per request vs CPU utilization\n\nThe latency differences between instance types are expected due to the variations in CPU models. Focusing on the percentage increase in latency may provide more meaningful insights.\n\nLatency percentage increase vs CPU utilization\n\nIn both graphs, one line stands out by deviating more than the others. We’ll examine this case in detail shortly.\n\nTurbo Boost effect\n\nAn interesting observation is how CPU frequency changes as utilization increases, which can be attributed to Intel’s Turbo Boost Technology. Since all the instances we used are equipped with Intel CPUs, the impact of Turbo Boost is noticeable across all of them. In the graph below, you can see how the CPU frequency decreases as the CPU utilization increases. The red arrows are showing the CPU utilization level.\n\nCPU Cores Frequency\n\nWhen CPU utilization remains at lower levels (around 30% or below), we benefit from increased core frequencies, leading to faster CPU times and, consequently, lower overall latency. However, as the demand for more CPU cores rises and utilization increases, we are likely to reach the CPU’s thermal and power limits, causing frequencies to decrease. In essence, lower CPU utilization results in better performance, while higher utilization leads to a decline in performance. For instance, a workload running on a specific node with approximately 30% CPU utilization will report faster response times compared to the same workload on the same VM when CPU utilization exceeds 50%.\n\nVariations in CPU frequency are not the only factors influencing performance changes. All our nodes have Hyper-Threading enabled, an Intel technology that allows a single physical CPU core to operate as two virtual cores. Although there is only one physical core, the Linux kernel recognizes it as two virtual CPU cores. The kernel attempts to distribute the CPU load across these cores, aiming to keep only one hardware thread (virtual core) busy per physical core. This approach is effective until we reach a certain level of CPU utilization. Beyond this threshold, we cannot fully utilize both virtual CPU cores, resulting in reduced performance compared to normal operation.\n\nFinding the “Golden Ratio” of CPU utilization\n\nUnderutilized nodes lead to wasted resources, power, and space in our data centers, while nodes that are excessively utilized also create inefficiencies. As noted, higher CPU utilization results in decreased performance, which can give a misleading impression that additional resources are necessary, resulting in a cycle of over-provisioning. This issue is particularly pronounced with blocking workloads that do not follow an asynchronous model. As CPU performance deteriorates, each process can manage fewer tasks per second, making existing capacity inadequate. To achieve the optimal balance—the “Golden Ratio” of CPU utilization—we must identify a threshold where CPU utilization is sufficiently high to ensure efficiency without significantly impairing performance. Striving to keep our nodes near this threshold will enable us to utilize our current hardware more effectively alongside our existing software.\n\nSince we already have experimental data demonstrating how CPU time increases with rising utilization, we can develop a mathematical model to identify this threshold. First, we need to determine what percentage of CPU time degradation is acceptable for our specific use case. This may depend on user expectations or performance Service Level Agreements (SLAs). Once we establish this threshold, it will help us select a level of CPU utilization that remains within acceptable limits.\n\nWe can plot the CPU utilization vs. CPU time (latency) and find the point where:\n\nCPU utilization is high enough to avoid resource underutilization.\n\nCPU time degradation does not exceed your acceptable limit.\n\nA specific example derived from the data above can be illustrated in the following graph.\n\nPercentage Increase in P50 Latency vs CPU Utilization\n\nIn this example, we aim to achieve less than 40% CPU time degradation, which would correspond to a CPU utilization of 61% on the specific instance.\n\nOutlier case\n\nAs previously mentioned, there was a specific instance that displayed some outlying data points. Our experiment confirmed an already recognized issue where certain instances were not achieving their advertised maximum Turbo Boost CPU frequency. Instead, we observed steady CPU frequencies that fell below the maximum advertised value under low CPU utilization. In the example below, you can see an instance from a CPU family that advertises Turbo Boost frequencies above 3 GHz, but it is only reporting a maximum CPU frequency of 2.8 GHz.\n\nCPU cores frequency\n\nThis issue turned out to be caused by a disabled CPU C-state, which prevented the CPU cores from halting even when they were not in use. As a result, these cores were perceived as “busy” by the turbo driver, limiting our ability to take advantage of Turbo Boost benefits with higher CPU frequencies. By enabling the C-state and allowing for optimization and power reduction during idle mode, we observed the expected Turbo Boost behavior. This change had an immediate impact on the CPU time spent by our test workloads. The images below illustrate the prompt changes in CPU frequencies and latency reported following the C-state adjustment.\n\nCPU cores frequency\n\nP50 CPU time on a request\n\nUpon re-evaluating the percentage change in CPU time, we now observe similar behavior across all instances.\n\nPercentage Increase in P50 Latency vs CPU Utilization\n\nAs we anticipated many of these insights, our objective was to validate our theories using data from our complex system. While we confirmed that performance lowers as CPU utilization increases across different CPU families, by identifying optimal CPU utilization thresholds, we can achieve a better balance between performance and efficiency, ensuring that our infrastructure remains both cost-effective and high performing. Going forward, these insights will inform us of our resource provisioning strategies and help us maximize the effectiveness of our hardware investments.\n\nThank you for sticking with us until the end!! A special shout-out to @adrmike , @schlubbi , @terrorobe , the @github/compute-platform and finally the @github/performance-engineering team for their invaluable assistance throughout these experiments, data analysis, and for reviewing the content for accuracy and consistency. ❤️\n\nTags:", "label": "non_personal"}
{"title": "Considerations for making a tree view component accessible", "url": "https://github.blog/engineering/user-experience/considerations-for-making-a-tree-view-component-accessible/", "content": "Tree views are a core part of the GitHub experience. You’ve encountered one if you’ve ever navigated through a repository’s file structure or reviewed a pull request.\n\nOn GitHub, a tree view is the list of folders and the files they contain. It is analogous to the directory structure your operating system uses as a way of organizing things.\n\nTree views are notoriously difficult to implement in an accessible way. This post is a deep dive into some of the major considerations that went into how we made GitHub’s tree view component accessible. We hope that it can be used as a reference and help others.\n\nStart with Windows\n\nIt’s important to have components with complex interaction requirements map to something people are already familiar with using. This allows for responsiveness to the keypresses they will try to navigate and take action on our tree view instances.\n\nWe elected to adopt Windows File Explorer’s tree view implementation, given the prominence of Windows’ usage for desktop screen reader users.\n\nNavigating and taking actions on items in Windows’ tree view using NVDA and JAWS helped us get a better understanding of how things worked, including factors such as focus management, keyboard shortcuts, and expected assistive technology announcements.\n\nThen maybe reference the APG\n\nThe ARIA Authoring Practices Guide (APG) is a bit of an odd artifact. It looks official but is no longer recognized by the W3C as a formal document.\n\nThis is to say that the APG can serve as a helpful high-level resource for things to consider for your overall approach, but its suggestions for code necessitate deeper scrutiny.\n\nBuild from a solid, semantic foundation\n\nAt its core, a tree view is a list of lists. Because of this, we used ul and li elements for parent and child nodes:\n\n<ul> <li> <ul> <li>.github/</li> <li>source/</li> <li>test/</li> </ul> </li> <li>.gitignore</li> <li>README.md</li> </ul>\n\nThere are a few reasons for doing this, but the main considerations are:\n\nBetter assurance that a meaningful accessibility tree is generated,\n\nLessening the work we need for future maintenance, and consequential re-verification that our updates continue to work properly, and\n\nBetter guaranteed interoperability between different browsers, apps, and other technologies.\n\nNOTE: GitHub currently does not virtualize its file trees. We would need to revisit this architectural decision if this ever changes.\n\nBetter broad assistive technology support\n\nThe more complicated an interactive pattern is, the greater the risk that there are bugs or gaps with assistive technology support.\n\nGiven the size of the audience GitHub serves, it’s important that we consider more than just majority share assistive technology considerations.\n\nWe found that utilizing semantic HTML elements also performed better for some less-common assistive technologies. This was especially relevant with some lower-power devices, like an entry-level Android smartphone from 2021.\n\nBetter Forced Color Mode support\n\nSemantic HTML elements also map to native operating system UI patterns, meaning that Forced Color Mode’s heuristics will recognize them without any additional effort. This is helpful for people who rely on the mode to see screen content.\n\nThe heuristic mapping behavior does not occur if we used semantically neutral div or span elements, and would have to be manually recreated and maintained.\n\nUse a composite widget\n\nA composite widget allows a component that contains multiple interactive elements to only require one tab stop unless someone chooses to interact with it further.\n\nConsider a file tree for a repository that contains 500+ files in 20+ directories. Without a composite widget treatment, someone may have to press Tab far too many times to bypass the file tree component and get what they need.\n\nThink about wrapping it in a landmark\n\nLike using a composite widget, landmark regions help some people quickly and efficiently navigate through larger overall sections of the page. Because of this, we wrapped the entire file tree in a nav landmark element.\n\nThis does not mean every tree view component should be a landmark, however! We made this decision for the file tree because it is frequently interacted with as a way to navigate through a repository’s content.\n\nGo with a roving tabindex approach\n\nA roving tabindex is a technique that uses tabindex=\"-1\" applied to each element in a series, and then updates the tabindex value to use 0 instead in response to user keyboard input. This allows someone to traverse the series of elements, as focus “roves” to follow their keypresses.\n\n<li tabindex=\"-1\">File 1</li> <li tabindex=\"-1\">File 2</li> <li tabindex=\"0\">File 3</li> <li tabindex=\"-1\">File 4</li>\n\nThe roving tabindex approach performed better than utilizing aria-activedescendant , which had issues with VoiceOver on macOS and iOS.\n\nEnhance with ARIA\n\nWe use a considered set of ARIA declarations to build off our semantic foundation.\n\nNote that while we intentionally started with semantic HTML, there are certain ARIA declarations that are needed. The use of ARIA here is necessary and intentional, as it expands the capabilities of HTML to describe something that HTML alone cannot describe—a tree view construct.\n\nOur overall approach follows what the APG suggests, in that we use the following:\n\nrole=\"tree\" is placed on the parent ul element, to communicate that it is a tree view construct.\n\nis placed on the parent element, to communicate that it is a tree view construct. role=\"treeitem\" is placed on the child li elements, to communicate that they are tree view nodes.\n\nis placed on the child elements, to communicate that they are tree view nodes. role=\"group\" is declared on child ul elements, to communicate that they contain branch and leaf nodes.\n\nis declared on child elements, to communicate that they contain branch and leaf nodes. aria-expanded is declared on directories, with a value of true to communicate that the branch node is in an opened state and a value of false to communicate that it is in a collapsed state instead.\n\nis declared on directories, with a value of to communicate that the branch node is in an opened state and a value of to communicate that it is in a collapsed state instead. aria-selected is used to indicate if branch or leaf nodes have been chosen by user navigation, and can therefore have user actions applied to them.\n\nWe also made the following additions:\n\naria-hidden=\"true\" is applied to SVG icons (folders, files, etc.) to ensure its content is not announced.\n\nis applied to SVG icons (folders, files, etc.) to ensure its content is not announced. aria-current=\"true\" is placed on the selected node to better support when a node is deep linked to via URL.\n\nNOTE: We use “branch node” and “leaf node” as broad terms that can apply to all tree view components we use on GitHub. For the file tree, branch nodes would correspond to directories and subdirectories, and leaf nodes would correspond to files.\n\nSupport expected navigation techniques\n\nThe following behaviors are what people will try when operating a tree view construct, so we support them:\n\nKeyboard keypresses\n\nTab : Places focus on the entire tree view component, then moves focus to the next focusable item on the view.\n\n: Places focus on the entire tree view component, then moves focus to the next focusable item on the view. Enter : If a branch node is selected: Displays the directory’s contents. If a leaf node is selected: Displays the leaf node’s contents.\n\n: Down : Moves selection to the next node that can be selected without opening or closing a node.\n\n: Moves selection to the next node that can be selected without opening or closing a node. Up : Moves selection to the previous node that can be selected without opening or closing a node.\n\n: Moves selection to the previous node that can be selected without opening or closing a node. Right : If a branch node is selected and in a collapsed state: Expands the selected collapsed branch node and does not move selection. If a branch node is selected and in an expanded state: Moves selection to the directory’s first child node.\n\n: Left : If a branch node is selected and in an expanded state: Collapses the selected collapsed directory node and does not move selection. If a branch node is selected and in a collapsed state: Moves selection to the node’s parent directory. If a leaf node is selected: Moves selection to the leaf node’s parent directory.\n\n: End : Moves selection to the last node that can be selected.\n\n: Moves selection to the last node that can be selected. Home : Moves selection to the first node that can be selected.\n\nWe also support typeahead selection, as we are modeling Windows File Explorer’s tree view behaviors. Here, we move selection to the node closest to the currently selected node whose name matches what the user types.\n\nMiddle clicking\n\nNodes on tree view constructs are tree items, not links. Because of this, tree view nodes do not support the behaviors you get with using an anchor element, such as opening its URL in a new tab or window.\n\nWe use JavaScript to listen for middle clicks and Control + Enter keypresses to replicate this behavior.\n\nConsider states\n\nLoading\n\nTree views on GitHub can take time to retrieve their content, and we may not always know how much content a branch node contains.\n\nLive region announcements are tricky to get right, but integral to creating an equivalent experience. We use the following announcements:\n\nIf there is a known amount of nodes that load, we enumerate the incoming content with an announcement that reads, “Loading {x} items.”\n\nIf there is an unknown number of nodes that load, we instead use a more generic announcement of, “Loading…”\n\nIf there are no nodes that load we use an announcement message that reads, “{branch node name} is empty.”\n\nAdditionally, we manage focus for loading content:\n\nIf focus is placed on a placeholder loading node when the content loads in: Move focus from the placeholder node to the first child node in the branch node.\n\nIf focus is on a placeholder loading node but the branch node does not contain content: Move focus back to the branch node. Additionally, we remove the branch node’s aria-expanded declaration.\n\nErrors\n\nCircumstances can conspire to interfere with a tree view component’s intended behavior. Examples of this could be a branch node failing to retrieve content or a partial system outage.\n\nIn these scenarios, the tree view component will use a straightforward dialog component to communicate the error.\n\nFix interoperability issues\n\nAs previously touched on, complicated interaction patterns run the risk of compatibility issues. Because of this, it’s essential to test your efforts with actual assistive technology to ensure it actually works.\n\nWe made the following adjustments to provide better assistive technology support:\n\nUse aria-level\n\nScreen readers can report on the depth of a nested list item. For example, a li element placed inside of a ul element nested three levels deep can announce itself as such.\n\nWe found that we needed to explicitly declare the level on each li element to recreate this behavior for a tree view. For our example, we’d also need to set aria-level=\"3\" on the li element.\n\nThis fix addressed multiple forms of assistive technology we tested with.\n\nExplicitly set the node’s accessible name on the li element\n\nA node’s accessible name is typically set by the text string placed inside the li element:\n\n<li>README.md</li>\n\nHowever, we found that VoiceOver on macOS and iOS did not support this. This may be because of the relative complexity of each node’s inner DOM structure.\n\nWe used aria-labelledby to get around this problem, with a value that pointed to the id set on the text portion of each node:\n\n<li aria-labelledby=\"readme-md\"> <div> <!-- Icon --> </div> <div id=\"readme-md\"> README.md </div> </li>\n\nThis guarantees that:\n\nthe node’s accessible name is announced when focus is placed on the li element,\n\nelement, and that the announcement matches what is shown visually.\n\nWhere we’d like to go from here\n\nThere’s a couple areas we’re prototyping and iterating on to better serve our users:\n\nBrowsers apply a lot of behaviors to anchor elements, such as the ability to copy the URL.\n\nWe’d like to replace the JavaScript that listens for middle clicks with a more robust native solution, only without sacrificing interoperability and assistive technology support.\n\nSupporting multiple actions per node\n\nTree views constructs were designed assuming a user will only ever navigate to a node and activate it.\n\nGitHub has use cases that require actions other than activating the node, and we’re exploring how to accomplish that. This is exciting, as it represents an opportunity to evolve the tree view construct on the web.\n\nAlways learning\n\nAn accessible tree view is a complicated component to make, and it requires a lot of effort and testing to get right. However, this work helps to ensure that everyone can use a core part of GitHub, regardless of device, circumstance, or ability.\n\nWe hope that highlighting the considerations that went into our work can help you on your accessibility journey.\n\n\n\nShare your experience: We’d love to hear from you if you’ve run into issues using our tree view component with assistive technology. This feedback is invaluable to helping us continue to make GitHub more accessible.", "label": "non_personal"}
{"title": "How GitHub uses CodeQL to secure GitHub", "url": "https://github.blog/engineering/how-github-uses-codeql-to-secure-github/", "content": "GitHub’s Product Security Engineering team writes code and implements tools that help secure the code that powers GitHub. We use GitHub Advanced Security (GHAS) to discover, track, and remediate vulnerabilities and enforce secure coding standards at scale. One tool we rely heavily on to analyze our code at scale is CodeQL.\n\nCodeQL is GitHub’s static analysis engine that powers automated security analyses. You can use it to query code in much the same way you would query a database. It provides a much more robust way to analyze code and uncover problems than an old-fashioned text search through a codebase.\n\nThe following post will detail how we use CodeQL to keep GitHub secure and how you can apply these lessons to your own organization. You will learn why and how we use:\n\nCustom query packs (and how we create and manage them).\n\nCustom queries.\n\nVariant analysis to uncover potentially insecure programming practices.\n\nEnabling CodeQL at scale\n\nWe employ CodeQL in a variety of ways at GitHub.\n\nDefault setup with the default and security-extended query suites\n\nDefault setup with the default and security-extended query suites meets the needs of the vast majority of our over 10,000 repositories. With these settings, pull requests automatically get a security review from CodeQL. Advanced setup with a custom query pack\n\nA few repositories, like our large Ruby monolith, need extra special attention, so we use advanced setup with a query pack containing custom queries to really tailor to our needs. Multi-repository variant analysis (MRVA)\n\nTo conduct variant analysis and quick auditing, we use MRVA. We also write custom CodeQL queries to detect code patterns that are either specific to GitHub’s codebases or patterns we want a security engineer to manually review.\n\nThe specific custom Actions workflow step we use on our monolith is pretty simple. It looks like this:\n\n- name: Initialize CodeQL uses: github/codeql-action/init@v3 with: languages: ${{ matrix.language }} config-file: ./.github/codeql/${{ matrix.language }}/codeql-config.yml\n\nOur Ruby configuration is pretty standard, but advanced setup offers a variety of configuration options using custom configuration files. The interesting part is the packs option, which is how we enable our custom query pack as part of the CodeQL analysis. This pack contains a collection of CodeQL queries we have written for Ruby, specifically for the GitHub codebase.\n\nSo, let’s dive deeper into why we did that—and how!\n\nPublishing our CodeQL query pack\n\nInitially, we published CodeQL query files directly to the GitHub monolith repository, but we moved away from this approach for several reasons:\n\nIt required going through the production deployment process for each new or updated query.\n\nQueries not included in a query pack were not pre-compiled, which slowed down CodeQL analysis in CI.\n\nOur test suite for CodeQL queries ran as part of the monolith’s CI jobs. When a new version of the CodeQL CLI was released, it sometimes caused the query tests to fail because of changes in the query output, even when there were no changes to the code in the pull request. This often led to confusion and frustration among engineers, as the failure wasn’t related to their pull request changes.\n\nBy switching to publishing a query pack to GitHub Container Registry (GCR), we’ve simplified our process and eliminated many of these pain points, making it easier to ship and maintain our CodeQL queries. So while it’s possible to deploy custom CodeQL query files directly to a repository, we recommend publishing CodeQL queries as a query pack to the GCR for easier deployment and faster iteration.\n\nCreating our query pack\n\nWhen setting up our custom query pack, we faced several considerations, particularly around managing dependencies like the ruby-all package.\n\nTo ensure our custom queries remain maintainable and concise, we extend classes from the default query suite, such as the ruby-all library. This allows us to leverage existing functionality rather than reinventing the wheel, keeping our queries concise and maintainable. However, changes to the CodeQL library API can introduce breaking changes, potentially deprecating our queries or causing errors. Since CodeQL runs as part of our CI, we wanted to minimize the chance of this happening, as this can lead to frustration and loss of trust from developers.\n\nWe develop our queries against the latest version of the ruby-all package, ensuring we’re always working with the most up-to-date functionality. To mitigate the risk of breaking changes affecting CI, we pin the ruby-all version when we’re ready to release, locking it in the codeql-pack.lock.yml file. This guarantees that when our queries are deployed, they will run with the specific version of ruby-all we’ve tested, avoiding potential issues from unintentional updates.\n\nHere’s how we manage this setup:\n\nIn our qlpack.yml, we set the dependency to use the latest version of ruby-all\n\nDuring development, this configuration pulls in the latest version) of ruby-all when running codeql pack init , ensuring we’re always up to date. // Our custom query pack's qlpack.yml library: false name: github/internal-ruby-codeql version: 0.2.3 extractor: 'ruby' dependencies: codeql/ruby-all: \"*\" tests: 'test' description: \"Ruby CodeQL queries used internally at GitHub\"\n\nwhen running , ensuring we’re always up to date. Before releasing, we lock the version in the codeql-pack.lock.yml file, specifying the exact version to ensure stability and prevent issues in CI. // Our custom query pack's codeql-pack.lock.yml lockVersion: 1.0.0 dependencies: ... codeql/ruby-all: version: 1.0.6\n\nThis approach allows us to balance developing against the latest features of the ruby-all package while ensuring stability when we release.\n\nWe also have a set of CodeQL unit tests that exercise our queries against sample code snippets, which helps us quickly determine if any query will cause errors before we publish our pack. These tests are run as part of the CI process in our query pack repository, providing an early check for issues. We strongly recommend writing unit tests for your custom CodeQL queries to ensure stability and reliability.\n\nAltogether, the basic flow for releasing new CodeQL queries via our pack is as follows:\n\nOpen a pull request with the new query.\n\nWrite unit tests for the new query.\n\nMerge the pull request.\n\nIncrement the pack version in a new pull request.\n\nRun codeql pack init to resolve dependencies.\n\nto resolve dependencies. Correct unit tests as needed.\n\nPublish the query pack to the GitHub Container Registry (GCR).\n\nRepositories with the query pack in their config will start using the updated queries.\n\nWe have found this flow balances our team’s development experience while ensuring stability in our published query pack.\n\nConfiguring our repository to use our custom query pack\n\nWe won’t provide a general recommendation on configuration here, given that it ultimately depends on how your organization deploys code. We opted against locking our pack to a particular version in our CodeQL configuration file (see above). Instead, we chose to manage our versioning by publishing the CodeQL package in GCR. This results in the GitHub monolith retrieving the latest published version of the query pack. To roll back changes, we simply have to republish the package. In one instance, we released a query that had a high number of false positives and we were able to publish a new version of the pack that removed that query in less than 15 minutes. This is faster than the time it would have taken us to merge a pull request on the monolith repository to roll back the version in the CodeQL configuration file.\n\nOne of the problems we encountered with publishing the query pack in GCR was how to easily make the package available to multiple repositories within our enterprise. There are several approaches we explored.\n\nGrant access permissions for individual repositories. On the package management page, you can grant permissions for individual repositories to access your package. This was not a good solution for us since we have too many repositories for it to be feasible to do manually, yet there is not currently a way to configure programmatically using an API.\n\nOn the package management page, you can grant permissions for individual repositories to access your package. This was not a good solution for us since we have too many repositories for it to be feasible to do manually, yet there is not currently a way to configure programmatically using an API. Mint a personal access token for the CodeQL action runner. We could have minted a personal access token (PAT) that has access to read all packages for our organization and added that to the CodeQL action runner. However, this would have required managing a new token, and it seemed a bit more permissive than we wanted because it could read all of our private packages rather than ones we explicitly allow it to have access to.\n\nWe could have minted a personal access token (PAT) that has access to read all packages for our organization and added that to the CodeQL action runner. However, this would have required managing a new token, and it seemed a bit more permissive than we wanted because it could read all of our private packages rather than ones we explicitly allow it to have access to. Provide access permissions via a linked repository. We ended up implementing the third solution that we explored. We link a repository to the package and allow the package to inherit access permissions from the linked repository.\n\nCodeQL query pack queries\n\nWe write a variety of custom queries to be used in our custom query packs. These cover GitHub-specific patterns that aren’t included in the default CodeQL query pack. This allows us to tailor the analysis to patterns and preferences that are specific to our company and codebase. Some of the types of things we alert on using our custom query pack include:\n\nHigh-risk APIs specific to GitHub’s code that can be dangerous if they receive unsanitized user input.\n\nUse of specific built-in Rails methods for which we have safer, custom methods or functions.\n\nRequired authorization methods not being used in our REST API endpoint definitions and GraphQL object/mutation definitions.\n\nREST API endpoints and GraphQL mutations that require engineers to define access control methods to determine which actors can access them. (Specifically, the query detects the absence of this method definition to ensure that the actors’ permissions are being checked for these endpoints.)\n\nUse of signed tokens so we can nudge engineers to include Product Security as a reviewer when using them.\n\nCustom queries can be used more for educational purposes rather than being blockers to shipping code. For example, we want to alert engineers when they use the ActiveRecord::decrypt method. This method should generally not be used in production code, as it will cause an encrypted column to become decrypted. We use the recommendation severity in the query metadata so these alerts are treated as more of an informational alert. That means this may trigger an alert in a pull request, but it won’t cause the CodeQL CI job to fail. We use this lower severity level to allow engineers to assess the impact of new queries without immediate blocking. Additionally, this alert level isn’t tracked through our Fundamentals program, meaning it doesn’t require immediate action, reflecting the query’s maturity as we continue to refine its relevance and risk assessment.\n\n/** * @id rb/github/use-of-activerecord-decrypt * @description Do not use the .decrypt method on AR models, this will decrypt all encrypted attributes and save * them unencrypted, effectively undoing encryption and possibly making the attributes inaccessible. * If you need to access the unencrypted value of any attribute, you can do so by calling my_model.attribute_name. * @kind problem * @severity recommendation * @name Use of ActiveRecord decrypt method * @tags security * github-internal */ import ruby import DataFlow import codeql.ruby.DataFlow import codeql.ruby.frameworks.ActiveRecord /** Match against .decrypt method calls where the receiver may be an ActiveRecord object */ class ActiveRecordDecryptMethodCall extends ActiveRecordInstanceMethodCall { ActiveRecordDecryptMethodCall() { this.getMethodName() = \"decrypt\" } } from ActiveRecordDecryptMethodCall call select call, \"Do not use the .decrypt method on AR models, this will decrypt all encrypted attributes and save them unencrypted.\n\nAnother educational query is the one mentioned above in which we detect the absence of the `control_access` method in a class that defines a REST API endpoint. If a pull request introduces a new endpoint without `control_access`, a comment will appear on the pull request saying that the `control_access` method wasn’t found and it’s a requirement for REST API endpoints. This will notify the reviewer of a potential issue and prompt the developer to fix it.\n\n/** * @id rb/github/api-control-access * @name Rest API Without 'control_access' * @description All REST API endpoints must call the 'control_access' method, to ensure that only specified actor types are able to access the given endpoint. * @kind problem * @tags security * github-internal * @precision high * @problem.severity recommendation */ import codeql.ruby.AST import codeql.ruby.DataFlow import codeql.ruby.TaintTracking import codeql.ruby.ApiGraphs // Api::App REST API endpoints should generally call the control_access method private DataFlow::ModuleNode appModule() { result = API::getTopLevelMember(\"Api\").getMember(\"App\").getADescendentModule() and not result = protectedApiModule() and not result = staffAppApiModule() } // Api::Admin, Api::Staff, Api::Internal, and Api::ThirdParty REST API endpoints do not need to call the control_access method private DataFlow::ModuleNode protectedApiModule() { result = API::getTopLevelMember([\"Api\"]) .getMember([\"Admin\", \"Staff\", \"Internal\", \"ThirdParty\"]) .getADescendentModule() } // Api::Staff::App REST API endpoints do not need to call the control_access method private DataFlow::ModuleNode staffAppApiModule() { result = API::getTopLevelMember([\"Api\"]).getMember(\"Staff\").getMember(\"App\").getADescendentModule() } private class ApiRouteWithoutControlAccess extends DataFlow::CallNode { ApiRouteWithoutControlAccess() { this = appModule().getAModuleLevelCall([\"get\", \"post\", \"delete\", \"patch\", \"put\"]) and not performsAccessControl(this.getBlock()) } } predicate performsAccessControl(DataFlow::BlockNode blocknode) { accessControlCalled(blocknode.asExpr().getExpr()) } predicate accessControlCalled(Block block) { // the method `control_access` is called somewhere inside `block` block.getAStmt().getAChild*().(MethodCall).getMethodName() = \"control_access\" } from ApiRouteWithoutControlAccess api select api.getLocation(), \"The control_access method was not detected in this REST API endpoint. All REST API endpoints must call this method to ensure that the endpoint is only accessible to the specified actor types.\"\n\nVariant analysis\n\nVariant analysis (VA) refers to the process of searching for variants of security vulnerabilities. This is particularly useful when we’re responding to a bug bounty submission or a security incident. We use a combination of tools to do this, including GitHub’s code search functionality, custom scripts, and CodeQL. We will often start by using code search to find patterns similar to the one that caused a particular vulnerability across numerous repositories. This is sometimes not good enough, as code search is not semantically aware, meaning that it cannot determine whether a given variable is an Active Record object or whether it is being used in an `if` expression. To answer those types of questions we turn to CodeQL.\n\nWhen we write CodeQL queries for variant analysis we are much less concerned about false positives, since the goal is to provide results for security engineers to analyze. The quality of the code is also not quite as important, as these queries will only be used for the duration of the VA effort. Some of the types of things we use CodeQL for during VAs are:\n\nWhere are we using SHA1 hashes?\n\nOne of our internal API endpoints was vulnerable to SQLi according to a recent bug bounty report. Where are we passing user input to that API endpoint?\n\nThere is a problem with how some HTTP request libraries in Ruby handle the proxy setting. Can we look at places we are instantiating our HTTP request libraries with a proxy setting?\n\nOne recent example involved a subtle vulnerability in Rails. We wanted to detect when the following condition was present in our code:\n\nA parameter was used to look up an Active Record object.\n\nThat parameter is later reused after the Active Record object is looked up.\n\nThe concern with this condition is that it could lead to an insecure direct object reference (IDOR) vulnerability because Active Record finder methods can accept an array. If the code looks up an Active Record object in one call to determine if a given entity has access to a resource, but later uses a different element from that array to find an object reference, that can lead to an IDOR vulnerability. It would be difficult to write a query to detect all vulnerable instances of this pattern, but we were able to write a query that found potential vulnerabilities that gave us a list of code paths to manually analyze. We ran the query against a large number of our Ruby codebases using CodeQL’s MRVA.\n\nThe query, which is a bit hacky and not quite production grade, is below:\n\n/** * @name wip array query * @description an array is passed to an AR finder object */ import ruby import codeql.ruby.AST import codeql.ruby.ApiGraphs import codeql.ruby.frameworks.Rails import codeql.ruby.frameworks.ActiveRecord import codeql.ruby.frameworks.ActionController import codeql.ruby.DataFlow import codeql.ruby.Frameworks import codeql.ruby.TaintTracking // Gets the \"final\" receiver in a chain of method calls. // For example, in `Foo.bar`, this would give the `Foo` access, and in // `foo.bar.baz(\"arg\")` it would give the `foo` variable access private Expr getUltimateReceiver(MethodCall call) { exists(Expr recv | recv = call.getReceiver() and ( result = getUltimateReceiver(recv) or not recv instanceof MethodCall and result = recv ) ) } // Names of class methods on ActiveRecord models that may return one or more // instances of that model. This also includes the `initialize` method. // See https://api.rubyonrails.org/classes/ActiveRecord/FinderMethods.html private string staticFinderMethodName() { exists(string baseName | baseName = [\"find_by\", \"find_or_create_by\", \"find_or_initialize_by\", \"where\"] and result = baseName + [\"\", \"!\"] ) // or // result = [\"new\", \"create\"] } private class ActiveRecordModelFinderCall extends ActiveRecordModelInstantiation, DataFlow::CallNode { private ActiveRecordModelClass cls; ActiveRecordModelFinderCall() { exists(MethodCall call, Expr recv | call = this.asExpr().getExpr() and recv = getUltimateReceiver(call) and ( // The receiver refers to an `ActiveRecordModelClass` by name recv.(ConstantReadAccess).getAQualifiedName() = cls.getAQualifiedName() or // The receiver is self, and the call is within a singleton method of // the `ActiveRecordModelClass` recv instanceof SelfVariableAccess and exists(SingletonMethod callScope | callScope = call.getCfgScope() and callScope = cls.getAMethod() ) ) and ( call.getMethodName() = staticFinderMethodName() or // dynamically generated finder methods call.getMethodName().indexOf(\"find_by_\") = 0 ) ) } final override ActiveRecordModelClass getClass() { result = cls } } class FinderCallArgument extends DataFlow::Node { private ActiveRecordModelFinderCall finderCallNode; FinderCallArgument() { this = finderCallNode.getArgument(_) } } class ParamsHashReference extends DataFlow::CallNode { private Rails::ParamsCall params; // TODO: only direct element references against `params` calls are considered ParamsHashReference() { this.getReceiver().asExpr().getExpr() = params } string getArgString() { result = this.getArgument(0).asExpr().getConstantValue().getStringlikeValue() } } class ArrayPassedToActiveRecordFinder extends TaintTracking::Configuration { ArrayPassedToActiveRecordFinder() { this = \"ArrayPassedToActiveRecordFinder\" } override predicate isSource(DataFlow::Node source) { source instanceof ParamsHashReference } override predicate isSink(DataFlow::Node sink) { sink instanceof FinderCallArgument } string getParamsArg(DataFlow::CallNode paramsCall) { result = paramsCall.getArgument(0).asExpr().getConstantValue().getStringlikeValue() } // this doesn't check for anything fancy like whether it's reuse in a if/else // only intended for quick manual audit filtering of interesting candidates // so remains fairly broad to not induce false negatives predicate paramsUsedAfterLookups(DataFlow::Node source) { exists(DataFlow::CallNode y | y instanceof ParamsHashReference and source.getEnclosingMethod() = y.getEnclosingMethod() and source != y and getParamsArg(source) = getParamsArg(y) // we only care if it's used again AFTER an object lookup and y.getLocation().getStartLine() > source.getLocation().getStartLine()) } } from ArrayPassedToActiveRecordFinder config, DataFlow::Node source, DataFlow::Node sink where config.hasFlow(source, sink) and config.paramsUsedAfterLookups(source) select source, sink.getLocation()\n\nConclusion\n\nCodeQL can be very useful for product security engineering teams to detect and prevent vulnerabilities at scale. We use a combination of queries that run in CI using our query pack and one-off queries run through MRVA to find potential vulnerabilities and communicate them to engineers. CodeQL isn’t only useful for finding security vulnerabilities, though; it is also useful for detecting the presence or absence of security controls that are defined in code. This saves our security team time by surfacing certain security problems automatically, and saves our engineers time by detecting them earlier in the development process.\n\nWriting custom CodeQL queries\n\nTips for getting started\n\nWe have a large number of articles and resources for writing custom CodeQL queries. If you haven’t written custom CodeQL queries before, here are some resources to help get you started:\n\nImprove the security of your applications today by enabling CodeQL for free on your public repositories, or try GitHub Advanced Security for your organization.\n\nMichael Recachinas, GitHub Staff Security Engineer, also contributed to this blog post.", "label": "non_personal"}
{"title": "How to debug code with GitHub Copilot", "url": "https://github.blog/ai-and-ml/github-copilot/how-to-debug-code-with-github-copilot/", "content": "Debugging is an essential part of a developer’s workflow—but it’s also one of the most time consuming. What if AI could streamline the process, helping you analyze, fix, and document code faster? Enter GitHub Copilot, your AI-powered coding assistant.\n\nGitHub Copilot isn’t just for writing code—it’s also a powerful tool for debugging. Whether you’re troubleshooting in your IDE, using Copilot Chat’s slash commands like /fix , or reviewing pull requests (PR) on github.com, GitHub Copilot offers flexible, intelligent solutions to speed up your debugging process. And with the free version of GitHub Copilot, available to all personal GitHub accounts, you can start exploring these features today.\n\nIn this guide, we’ll explore how to debug code with GitHub Copilot, where to use it in your workflow, and best practices to get the most out of its capabilities. Whether you’re new to GitHub Copilot or looking to deepen your skills, this guide has something for you.\n\nStart using GitHub Copilot 🌟 GitHub Copilot Free includes 2,000 code completions, 50 Copilot Chat messages per month, multi-file edits, and model options like GPT-4o or Claude 3.5 Sonnet, with native support in VS Code and on GitHub.\n\nDebugging code with GitHub Copilot: surfaces and workflows\n\nDebugging code with GitHub Copilot can help you tackle issues faster while enhancing your understanding of the codebase. Whether you’re fixing syntax errors, refactoring inefficient code, or troubleshooting unexpected behavior, GitHub Copilot can provide valuable insights in your debugging journey.\n\nSo, how exactly does this work? “GitHub Copilot is recognizing patterns and suggesting solutions based on what it has learned,” says Christopher Harrison, Senior Developer Advocate. “Once you’ve identified the problem area, you can turn to GitHub Copilot and ask, ‘I’m giving this input but getting this output—what’s wrong?’ That’s where GitHub Copilot really shines.”\n\nLet’s explore how GitHub Copilot can help you debug your code across different surfaces, from your IDE to github.com and even pull requests.\n\n1. In Copilot Chat\n\nCopilot Chat acts as an interactive AI assistant, helping you debug issues with natural language queries. And with Copilot Free, you get 50 chat messages per month. With Copilot Chat, you can:\n\nGet real-time explanations: Ask “Why is this function throwing an error?” and Copilot Chat will analyze the code and provide insights.\n\nAsk “Why is this function throwing an error?” and Copilot Chat will analyze the code and provide insights. Use slash commands for debugging: Try /fix to generate a potential solution or /explain for a step-by-step breakdown of a complex function. (More on this later!)\n\nTry to generate a potential solution or for a step-by-step breakdown of a complex function. (More on this later!) Refactor code for efficiency: If your implementation is messy or inefficient, Copilot Chat can suggest cleaner alternatives. Christopher explains, “Refactoring improves the readability of code, making it easier for both developers and GitHub Copilot to understand. And if code is easier to understand, it’s easier to debug and spot problems.”\n\nIf your implementation is messy or inefficient, Copilot Chat can suggest cleaner alternatives. Christopher explains, “Refactoring improves the readability of code, making it easier for both developers and GitHub Copilot to understand. And if code is easier to understand, it’s easier to debug and spot problems.” Walk through errors interactively: Describe your issue in chat and get tailored guidance without ever having to leave your IDE.\n\n🔎 How to find GitHub Copilot Chat Look for the GitHub Copilot icon or prompt to start a conversation in your IDE, on github.com, or within pull requests. Simply click on the icon to get started with code suggestions, unit tests, suggested code fixes, and more!\n\n2. In your IDE\n\nWhen working in popular IDEs like VS Code or JetBrains, GitHub Copilot offers real-time suggestions as you type. It helps by:\n\nFlagging issues: For example, if you declare a variable but forget to initialize it, GitHub Copilot can suggest a correction.\n\nFor example, if you declare a variable but forget to initialize it, GitHub Copilot can suggest a correction. Code fixes: Encounter a syntax error? GitHub Copilot can suggest a fix in seconds, ensuring your code stays error-free.\n\nEncounter a syntax error? GitHub Copilot can suggest a fix in seconds, ensuring your code stays error-free. Contextual assistance: By analyzing your workspace, GitHub Copilot provides solutions tailored to your codebase and project structure.\n\n🔎 How to find GitHub Copilot in VS Code To open up the chat view, head over to the VS Code title bar and select, “Use AI features with Copilot for Free.”\n\nSign in with your GitHub account by clicking on “Sign in to use Copilot for Free.”\n\nDon’t have a GitHub Copilot subscription yet? Then follow the browser steps to sign up for your Copilot Free plan. Learn how to install GitHub Copilot in JetBrains, Azure Data Studio, and more >\n\n3. On github.com\n\nGitHub Copilot extends beyond your IDE, offering debugging assistance directly on github.com via Copilot Chat, particularly in repositories and discussions. With this feature, you can:\n\nTroubleshoot code in repositories: Open a file, highlight a problematic section, and use Copilot Chat to analyze it.\n\nOpen a file, highlight a problematic section, and use Copilot Chat to analyze it. Generate test cases: If you’re unsure how to verify a function, GitHub Copilot can suggest test cases based on existing code.\n\nIf you’re unsure how to verify a function, GitHub Copilot can suggest test cases based on existing code. Understand unfamiliar code: Reviewing an open-source project or a teammate’s PR? Ask GitHub Copilot to summarize a function or explain its logic.\n\n🔎 How to find GitHub Copilot on github.com\n\n4. For pull request assistance\n\nGitHub Copilot can also streamline debugging within PRs, ensuring code quality before merging.\n\nSuggest improvements in PR comments: GitHub Copilot can review PRs and propose fixes directly in the conversation.\n\nGitHub Copilot can review PRs and propose fixes directly in the conversation. Generate PR summaries: Struggling to describe your changes? Greg Larkin, Senior Service Delivery Engineer, says, “I use GitHub Copilot in the PR creation process to generate a summary of the changes in my feature branch compared to the branch I’m merging into. That can be really helpful when I’m struggling to figure out a good description, so that other people understand what I did.”\n\nStruggling to describe your changes? Greg Larkin, Senior Service Delivery Engineer, says, “I use GitHub Copilot in the PR creation process to generate a summary of the changes in my feature branch compared to the branch I’m merging into. That can be really helpful when I’m struggling to figure out a good description, so that other people understand what I did.” Explain diffs: Not sure why a change was made? Ask GitHub Copilot to summarize what’s different between commits.\n\nNot sure why a change was made? Ask GitHub Copilot to summarize what’s different between commits. Catch edge cases before merging: Use /analyze to identify potential issues and /tests to generate missing test cases.\n\nUse to identify potential issues and to generate missing test cases. Refactor on the fly: If a PR contains redundant or inefficient code, GitHub Copilot can suggest optimized alternatives.\n\nBy integrating Copilot into your PR workflow, you can speed up code reviews while maintaining high-quality standards. Just be sure to pair it with peer expertise for the best results.\n\n🔎 How to find GitHub Copilot in pull requests\n\n5 slash commands in GitHub Copilot for debugging code\n\nSlash commands turn GitHub Copilot into an on-demand debugging assistant, helping you solve issues faster, get more insights, and improve your code quality. Here are some of the most useful slash commands for debugging:\n\n1. Use /help to get guidance on using GitHub Copilot effectively\n\nThe /help slash command provides guidance on how to interact with GitHub Copilot effectively, offering tips on structuring prompts, using slash commands, and maximizing GitHub Copilot’s capabilities.\n\nHow it works : Type /help in Copilot Chat to receive suggestions on your current task, whether it’s debugging, explaining code, or generating test cases.\n\n: Type in Copilot Chat to receive suggestions on your current task, whether it’s debugging, explaining code, or generating test cases. Example: Need a refresher on what GitHub Copilot can do? Use /help to access a quick guide to slash commands like /fix and /explain .\n\n2. Use /fix to suggest and apply fixes\n\nThe /fix command is a go-to tool for resolving code issues by allowing you to highlight a block of problematic code or describe an error.\n\nHow it works: Select the code causing issues, type /fix , and let Copilot Chat generate suggestions.\n\nSelect the code causing issues, type , and let Copilot Chat generate suggestions. Example: If you have a broken API call, use /fix to get a corrected version with appropriate headers or parameters.\n\n3. Use /explain to understand code and errors\n\nThe /explain command breaks down complex code or cryptic error messages into simpler, more digestible terms.\n\nHow it works: Highlight the code or error message you want clarified, type /explain , and Copilot Chat will provide an explanation. It will explain the function’s purpose, how it processes the data, potential edge cases, and any possible bugs or issues.\n\nHighlight the code or error message you want clarified, type , and Copilot Chat will provide an explanation. It will explain the function’s purpose, how it processes the data, potential edge cases, and any possible bugs or issues. Example: Encounter a “NullPointerException”? Use /explain to understand why it occurred and how to prevent it.\n\n4. Use /tests to generate tests\n\nTesting is key to identifying bugs, and the /tests command helps by generating test cases based on your code.\n\nHow it works: Use /tests on a function or snippet, and Copilot Chat will generate relevant test cases.\n\nUse on a function or snippet, and Copilot Chat will generate relevant test cases. Example: Apply /tests to a sorting function, and Copilot Chat might generate unit tests for edge cases like empty arrays or null inputs.\n\n5. Use /doc to generate or improve documentation\n\nThere are long-term benefits to having good text documentation—for developers and GitHub Copilot, which can draw context from it—because it makes your codebase that much more searchable. By using the /doc command with Copilot Free, you can even ask GitHub Copilot to write a summary of specific code blocks within your IDE.\n\nThe /doc command helps you create or refine documentation for your code, which is critical when debugging or collaborating with others. Clear documentation provides context for troubleshooting, speeds up issue resolution, and helps fellow developers understand your code faster.\n\nHow it works: Highlight a function, class, or file, type /doc and right-click to see the context menu, and Copilot Chat will generate comprehensive comments or documentation.\n\nHighlight a function, class, or file, type and right-click to see the context menu, and Copilot Chat will generate comprehensive comments or documentation. Example: Apply /doc to a function, and Copilot Chat will generate inline comments detailing its purpose, parameters, and expected output.\n\nBy mastering these commands, you can streamline your debugging workflow and resolve issues faster without switching between tools or wasting time on manual tasks.\n\nUsing shortcut keys: Quickly activate GitHub Copilot’s debugging features in VS Code Start/continue debugging: F5\n\nStop debugging: Shift + F5\n\nStep over: F10\n\nStep into: F11\n\nStep out: Shift + F11\n\nToggle breakpoint: F9\n\nBest practices for debugging code with GitHub Copilot\n\nProvide clear context for better results\n\nProviding the right context helps GitHub Copilot generate even more relevant debugging suggestions. As Christopher explains, “The better that Copilot is able to understand what you’re trying to do and how you’re trying to do it, the better the responses are that it’s able to give to you.”\n\nSince GitHub Copilot analyzes your code within the surrounding scope, ensure your files are well structured and that relevant dependencies are included. If you’re using Copilot Chat, reference specific functions, error messages, or logs to get precise answers instead of generic suggestions.\n\n💡 Pro tip: Working across multiple files? Use the @workspace command to point GitHub Copilot in the right direction and give it more context for your prompt and intended goal.\n\nAsk, refine, and optimize in real time\n\nInstead of treating GitHub Copilot as a one-and-done solution, refine its suggestions by engaging in a back-and-forth process. Greg says, “I find it useful to ask GitHub Copilot for three or four different options on how to fix a problem or to analyze for performance. The more detail you provide about what you’re after—whether it’s speed, memory efficiency, or another constraint—the better the result.”\n\nThis iterative approach can help you explore alternative solutions you might not have considered, leading to more robust and efficient code.\n\nMaster the art of specific prompts\n\nThe more specific your prompt, the better GitHub Copilot’s response. Instead of asking “What’s wrong with this function?” try “Why is this function returning undefined when the input is valid?” GitHub Copilot performs best when given clear, detailed queries—this applies whether you’re requesting a fix, asking for an explanation, or looking for test cases to verify your changes.\n\nBy crafting precise prompts and testing edge cases, you can use GitHub Copilot to surface potential issues before they become production problems.\n\nTry a structured approach with progressive debugging\n\nNext, try a step-by-step approach to your debugging process! Instead of immediately applying fixes, use GitHub Copilot’s commands to first understand the issue, analyze potential causes, and then implement a solution. This structured workflow—known as progressive debugging—helps you gain deeper insights into your code while ensuring that fixes align with the root cause of the problem.\n\nFor example:\n\nStart with the slash command /explain on a problematic function to understand the issue. Use the slash command /startDebugging to help with configuring interactive debugging. Finally, apply the slash command /fix to generate possible corrections.\n\n📌 Use case: If a function in your React app isn’t rendering as expected, start by running /explain on the relevant JSX or state logic, then use /debug to identify mismanaged props, and finally, apply /fix for a corrected implementation.\n\nCombine commands for a smarter workflow\n\nSome issues require multiple levels of debugging and refinement. By combining commands, you can move from diagnosis to resolution even faster.\n\nFor example:\n\nUse /explain + /fix to understand and resolve issues quickly.\n\nto understand and resolve issues quickly. Apply /fixTestFailure + /tests to find failing tests and generate new ones.\n\n📌 Use case:\n\nFixing a broken function: Run the slash command /explain to understand why it fails, then use the slash command /fix to generate a corrected version.\n\nRun the slash command to understand why it fails, then use the slash command to generate a corrected version. Improving test coverage: Use the slash command /fixTestFailure to identify and fix failing tests, then use the slash command /tests to generate additional unit tests for the highlighted code.\n\nRemember, slash commands are most effective when they’re used in the appropriate context, combined with clear descriptions of the problem, are part of a systematic debugging approach, and followed up with verification and testing.\n\nGitHub Copilot is a powerful tool that enhances your workflow, but it doesn’t replace the need for human insight, critical thinking, and collaboration. As Greg points out, “GitHub Copilot can essentially act as another reviewer, analyzing changes and providing comments. Even so, it doesn’t replace human oversight. Having multiple perspectives on your code is crucial, as different reviewers will spot issues that others might miss.”\n\nBy combining GitHub Copilot’s suggestions with human expertise and rigorous testing, you can debug more efficiently while maintaining high-quality, reliable code.\n\nReady to try the free version of GitHub Copilot?\n\nStart using GitHub Copilot today >\n\nYou can keep the learning going with these resources:\n\n* Debug your app with GitHub Copilot in Visual Studio\n\n* Example prompts for GitHub Copilot Chat\n\n* GitHub Copilot and VS Code tutorials", "label": "non_personal"}
{"title": "Finding leaked passwords with AI: How we built Copilot secret scanning", "url": "https://github.blog/engineering/platform-security/finding-leaked-passwords-with-ai-how-we-built-copilot-secret-scanning/", "content": "In October 2024, we announced the general availability of Copilot secret scanning, leveraging AI to detect generic passwords in users’ codebases. This post describes how Copilot secret scanning works under the hood, the challenges we ran into when developing it, and the framework we use for testing and iteration.\n\nWhat is Copilot secret scanning?\n\nCopilot secret scanning is a feature of GitHub Secret Protection, which protects millions of repositories on GitHub by detecting hundreds of pattern types through our partner program. The precision of these detections is paramount for security teams and developers when dealing with security alerts. Historically, our detection approach has relied on regular expressions, which is an effective method for identifying secrets with strict, provider-minted formats. However, this method struggles with the nuanced and varied structures of generic passwords, often generating excessive noise for security teams and developers.\n\nWe now detect generic passwords with GitHub Copilot, using AI to analyze context—such as the usage and location of a potential secret—to limit noise and deliver relevant alerts that are critical to the health and security of your repositories.\n\nGetting to the point where we were confident in our password precision was a journey over many test cases, prompt iterations, and model changes. Let’s dive in to explore what we learned along the way and find out where we’re going.\n\nThe private preview highlighted a problem early on: unconventional file types and structures\n\nAt the core of Copilot secret scanning lies a request to a large language model (LLM), expressed through an LLM prompt consisting of:\n\nGeneral information about the type of vulnerability, in this case passwords.\n\nThe source code location and contents of the file where we believe the vulnerability may exist.\n\nA strict JSON format specification for the model output, to allow for automated processing.\n\nOur first iteration of the prompt used the few-shot prompting technique, which provides the LLM with example inputs and outputs to demonstrate how to perform the task. We wanted a resource-effective model to run the detections at scale and landed on GPT-3.5-Turbo. In parallel, we developed a basic offline evaluation framework, including manually curated test cases with both positive and negative findings, to help us validate that our approach was sound before deploying it to customers.\n\nWe deployed this first iteration to our private preview participants and immediately noticed a problem. While it worked reasonably well at identifying credentials in our offline evaluation, it would fail spectacularly in some customer repositories. The model had difficulty interpreting file types and structures not typically seen in the conventional coding languages and patterns that LLMs train on.\n\nThis experience revealed the complexity of the problem and the limiting nature of LLMs. We had to reevaluate our approach.\n\nThe road to public preview: Improving offline evaluation and prompting\n\nIn response to these initial results, we enhanced the offline evaluation framework in a few key ways. First, we added reports from private preview participants to increase the diversity of our test cases. Next, we enhanced the framework so that we could visually identify and analyze deviations resulting from model or prompt changes. This allowed us to better see the impact of customizing different steps in our prompting strategy. Finally, we leveraged the GitHub Code Security team’s evaluation processes to create a data collection pipeline, and used GPT-4 to create our own test cases based on learnings from existing secret scanning alerts in open source repositories.\n\nThis improved offline evaluation and gave us the breadth needed to measure both precision and recall. Precision is the ability to find secrets more accurately, with concerns to the false positive rate, while recall is the ability to find secrets more reliably, with concerns to the false negative rate.\n\nFrom here, we ran a series of experiments to evaluate detection quality:\n\nWhat if we tried a different model?\n\nWhat if we ran the prompt multiple times and somehow combined the responses?\n\nWhat if we ran two different prompts on two different models in sequence?\n\nHow do we better handle the nondeterministic nature of LLM responses?\n\nMore specifically, we started experimenting with a few different mechanisms to improve our detection with the LLM.\n\nWe tried voting (asking the model the same question many times), which allowed for more deterministic responses but had no material impact on our precision.\n\nWe also tried using a larger model (GPT-4) trained on a larger set of parameters as a confirming scanner, to validate the accuracy of candidates found by GPT-3.5-Turbo. This helped improve precision without reducing our recall, but was also more resource intensive.\n\nWe also tried a few different prompting strategies, such as Fill-in-the-Middle, Zero-Shot, and Chain-of-Thought. We ended up collaborating with our colleagues at Microsoft and used their MetaReflection technique, a novel offline reinforcement learning technique that allows experiential learnings from past trials to come up with a hybrid Chain of Thought (CoT) and few-shot prompt that improves precision with a small penalty in recall.\n\nWe ultimately ended up using a combination of all these techniques and moved Copilot secret scanning into public preview, opening it widely to all GitHub Secret Protection customers. This brings us to our next hurdle: scale.\n\nScaling out capacity for a public preview\n\nSecret scanning not only scans incoming Git pushes, but also your entire Git history on all branches. With each new customer, the necessary resources increase linearly. Rather than simply expanding LLM capacity, we focused on striking the most effective balance between value and cost to ensure optimal performance and efficiency. Before tackling how we managed the resources, we tried to find ways to reduce resource usage itself by:\n\nIdentifying and excluding a class of changes from scanning (such as media files or language files that contain “test,” “mock,” or “spec” in the filepath), because we expected they would never contain credentials or they would be incomprehensible to the model.\n\nExperimenting with newer models, such as GPT-4-Turbo and GPT-4o-mini, that were expected to be less resource intensive without compromising on performance and latency.\n\nExperimenting with different context windows to find one that reduced resources without significantly increasing latency for the LLM to respond to our queries.\n\nMaking improvements to how we tokenize the content we want to scan, including retaining some memory of previous tokenizations while processing new parts of a file.\n\nWhile some of these efforts proved fruitful, such as limiting the content we scanned, other efforts were less effective. For example, breaking down content into smaller pieces didn’t have much of an impact, while using a more powerful model did.\n\nUltimately, the most impactful change came from creating a workload-aware request management system that allowed us to maximize and equitably share LLM capacity against the variety of different workloads we run during scans.\n\nIn building the system, we noticed a fundamental problem that needed addressing in our capacity management: assigning specific rate limits to individual workloads (such as scanning incoming Git commits or scanning the full history) was suboptimal. As each workload was tied to specific traffic patterns—Git commits, for example, tend to correlate with working hours, while full history scanning correlates with discrete events like a security manager or administrator enabling the feature on a new organization—it was easy to land in a situation where an individual workload could run into rate limits within its operational context, leaving additional resources available elsewhere unused.\n\nWe drew significant inspiration from existing solutions in this space, such as Doorman, GitHub’s own Freno, and various other weighted, fair-priority, queue-related algorithms. We came up with an algorithm that allows us to set a range of limits for each workload, preventing the workload from completely overwhelming the LLM, while allowing it to tap into resources from other workloads going unused at the moment. This strategy was so effective at maximizing utilization that we ended up using it within Copilot Autofix and security campaigns as well.\n\nMirror testing our way to general availability\n\nAchieving confidence in detection quality was crucial for moving Copilot secret scanning to general availability. We implemented a mirror testing framework that ran our prompt and filtering changes against a subset of repositories that participated in our public preview. Rescanning these repositories with our latest improvements allowed us to assess the change in real alert volumes and false positive resolutions, without impacting users.\n\nWe found a huge drop in detections and false positives with very few missing real passwords. In some cases, we saw a 94% reduction in false positives across organizations! This before-and-after comparison indicated that all the different changes we made during private and public preview led to increased precision without sacrificing recall, and that we were ready to provide a reliable and efficient detection mechanism to all GitHub Secret Protection customers.\n\nLessons for the future\n\nCopilot secret scanning is now detecting passwords on nearly 35% of all GitHub Secret Protection repositories. We’re continuing to monitor performance and apply lessons learned as we leverage the tooling we created along the way:\n\nA focus on precision: Security and development teams need accurate and actionable alerts without the noise—this is always our primary goal.\n\nSecurity and development teams need accurate and actionable alerts without the noise—this is always our primary goal. Including diverse test cases: We continue to incorporate examples based on learnings from customer feedback into our test bed as we refine our detection capabilities.\n\nWe continue to incorporate examples based on learnings from customer feedback into our test bed as we refine our detection capabilities. Effective resource management: We always need to balance scalability with performance.\n\nWe always need to balance scalability with performance. Collaborative innovation: Partnering with other GitHub and Microsoft teams helps us push the boundaries of what Copilot can achieve.\n\nThese learnings are also shared across Copilot Autofix, which continues to expand coverage for code scanning alerts and helps development teams remediate code scanning alerts quickly.\n\nSince our general availability launch, enablement for Copilot secret scanning has been included in security configurations, allowing you to control which repositories are detecting secrets across your organizations or enterprise. We’re dedicated to continuous improvement through ongoing monitoring, mirror testing, and approach refinement based on customer feedback and detection trends. Copilot secret scanning serves as a critical component for robust application security and will evolve to meet the dynamic needs of our users.\n\nCopilot secret scanning is a feature of GitHub Secret Protection, which offers enterprise-ready solutions for preventing accidental secret exposure in your repositories. GitHub Secret Protection is available to purchase starting April 1, 2025.", "label": "non_personal"}
{"title": "Introducing sub-issues: Enhancing issue management on GitHub", "url": "https://github.blog/engineering/architecture-optimization/introducing-sub-issues-enhancing-issue-management-on-github/", "content": "Recently we launched sub-issues, a feature designed to tackle complex issue management scenarios. This blog post delves into the journey of building sub-issues, what we learned along the way, how we implemented sub-issues, and the benefits of being able to use sub-issues to build itself.\n\nWhat are sub-issues?\n\nSub-issues are a way to break a larger issue into smaller, more manageable tasks. With this feature, you can now create hierarchical lists within a single issue, making it easier to track progress and dependencies. By providing a clear structure, sub-issues help teams stay organized and focused on their goals.\n\nFor example, I often realize that a batch of work requires multiple steps, like implementing code in different repositories. Breaking this task into discrete sub-issues makes it easier to track progress and more clearly define the work I need to do. In practice we’ve noticed this helps keep linked PRs more concise and easier to review.\n\nA brief history\n\nIssues have long been at the heart of project management on GitHub. From tracking bugs to planning feature development, issues provide a flexible and collaborative way for teams to organize their work. Over time, we’ve enriched this foundation with tools like labels, milestones, and task lists, all to make project management even more intuitive and powerful.\n\nOne of the key challenges we set out to solve was how to better represent and manage hierarchical tasks within issues. As projects grow in complexity, breaking down work into smaller, actionable steps becomes essential. We want to empower users to seamlessly manage these nested relationships while maintaining the simplicity and clarity GitHub is known for.\n\nOur journey toward sub-issues began with a fundamental goal: to create a system that integrates deeply into the GitHub Issues experience, enabling users to visually and functionally organize their work without adding unnecessary complexity. Achieving this required careful design and technical innovation.\n\nBuilding sub-issues\n\nTo build sub-issues, we began by designing a new hierarchical structure for tasks rather than modifying the existing task list functionality. We introduced the ability to nest tasks within tasks, creating a hierarchical structure. This required updates to our data models and rendering logic to support nested sub-issues.\n\nFrom a data modeling perspective, the sub-issues table stores the relationships between parent and child issues. For example, if Issue X is a parent of Issue Y, the sub-issues table would store this link, ensuring the hierarchical relationship is maintained.\n\nIn addition, we roll up sub-issue completion information into a sub-issue list table. This allows us to performantly get progress without having to traverse through a list of sub-issues. For instance, when Issue Y is completed, the system automatically updates the progress of Issue X, eliminating the need to manually check the status of all sub-issues.\n\nWe wanted a straightforward representation of sub-issues as relationships in MySQL. This approach provided several benefits, including easier support for sub-issues in environments like GitHub Enterprise Server and GitHub Enterprise Cloud with data residency.\n\nWe exposed sub-issues through GraphQL endpoints, which let us build upon the new Issues experience and leverage newly crafted list-view components. This approach provided some benefits, including more efficient data fetching and enhanced flexibility in how issue data is queried and displayed. Overall, we could move faster because we reused existing components and leveraged new components that would be used in multiple features. This was all made possible by building sub-issues in the React ecosystem.\n\nWe also focused on providing intuitive controls for creating, editing, and managing sub-issues. To this end, we worked closely with accessibility designers and GitHub’s shared components team that built the list view that powers sub-issues.\n\nOur goal was to make it as easy as possible for users to break down their tasks without disrupting their workflow.\n\nUsing sub-issues in practice\n\nDogfooding is a best practice at GitHub and it’s how we build GitHub! We used sub-issues extensively within our own teams throughout the company to manage complex projects and track progress. Having a discrete area to manage our issue hierarchy resulted in a simpler, more performant experience. Through this hands-on experience, we identified areas for improvement and ensured that the feature met our high standards.\n\nOur teams found that sub-Issues significantly improved their ability to manage large projects. By breaking down tasks into smaller, actionable items, they maintained better visibility and control over their work. The hierarchical structure also made it easier to identify dependencies and ensure nothing fell through the cracks.\n\nGathering early feedback\n\nBuilding sub-issues was a team effort. Feedback from our beta testers was instrumental in shaping the final product and ensuring it met the needs of our community. For example, understanding how much metadata to display in the sub-issue list was crucial. We initially started with only issue titles, but eventually added the issue number and repository name, if the issue was from another repository.\n\nBuilding features at GitHub makes it really easy to improve our own features as we go. It was really cool to start breaking down the sub-issues work using sub-issues. This allowed us to experience the feature firsthand and identify any pain points or areas for improvement. For example, the has:sub-issues-progress and has:parent-issue filters evolved from early discussions around filtering syntax. This hands-on approach ensured that we delivered a polished and user-friendly product.\n\nThese lessons have been invaluable in not only improving sub-issues, but also in shaping our approach to future feature development. By involving users early and actively using our own features, we can continue to build products that truly meet the needs of our community. These practices will be important to our development process going forward, ensuring that we deliver high-quality, user-centric solutions.\n\nCall to action\n\nSub-issues are designed to help you break down complex tasks into manageable pieces, providing clarity and structure to your workflows. Whether you’re tracking dependencies, managing progress, or organizing cross-repository work, sub-issues offer a powerful way to stay on top of your projects.\n\nWe’d love for you to try sub-issues and see how they can improve your workflow. Your feedback is invaluable in helping us refine and enhance this feature. Join the conversation in our community discussion to share your thoughts, experiences, and suggestions.\n\nThank you for being an integral part of the GitHub community. Together, we’re shaping the future of collaborative development!\n\nTags:", "label": "non_personal"}
{"title": "How the GitHub CLI can now enable triangular workflows", "url": "https://github.blog/open-source/git/how-the-github-cli-can-now-enable-triangular-workflows/", "content": "Most developers are familiar with the standard Git workflow. You create a branch, make changes, and push those changes back to the same branch on the main repository. Git calls this a centralized workflow. It’s straightforward and works well for many projects.\n\nHowever, sometimes you might want to pull changes from a different branch directly into your feature branch to help you keep your branch updated without constantly needing to merge or rebase. However, you’ll still want to push local changes to your own branch. This is where triangular workflows come in.\n\nIt’s possible that some of you have already used triangular workflows, even without knowing it. When you fork a repo, contribute to your fork, then open a pull request back to the original repo, you’re working in a triangular workflow. While this can work seamlessly on github.com, the process hasn’t always been seamless with the GitHub CLI.\n\nThe GitHub CLI team has recently made improvements (released in v2.71.2) to better support these triangular workflows, ensuring that the gh pr commands work smoothly with your Git configurations. So, whether you’re working on a centralized workflow or a more complex triangular one, the GitHub CLI will be better equipped to handle your needs.\n\nIf you’re already familiar with how Git handles triangular workflows, feel free to skip ahead to learn about how to use gh pr commands with triangular workflows. Otherwise, let’s get into the details of how Git and the GitHub CLI have historically differed, and how four-and-a-half years after it was first requested, we have finally unlocked managing pull requests using triangular workflows in the GitHub CLI.\n\nFirst, a lesson in Git fundamentals\n\nTo provide a framework for what we set out to do, it’s important to first understand some Git basics. Git, at its core, is a way to store and catalog changes on a repository and communicate those changes between copies of that repository. This workflow typically looks like the diagram below:\n\nFigure 1: A typical git branch setup\n\nThe building blocks of this diagram illustrate two important Git concepts you likely use every day, a ref and push/pull.\n\nRefs\n\nA ref is a reference to a repository and branch. It has two parts: the remote, usually a name like origin or upstream, and the branch. If the remote is the local repository, it is blank. So, in the example above, origin/branch in the purple box is a remote ref, referring to a branch named branch on the repository name origin, while branch in the green box is a local ref, referring to a branch named branch on the local machine.\n\nWhile working with GitHub, the remote ref is usually the repository you are hosting on GitHub. In the diagram above, you can consider the purple box GitHub and the green box your local machine.\n\nPushing and pulling\n\nA push and a pull refer to the same action, but from two different perspectives. Whether you are pushing or pulling is determined by whether you are sending or receiving the changes. I can push a commit to your repo, or you can pull that commit from my repo, and the references to that action would be the same.\n\nTo disambiguate this, we will refer to different refs as the headRef or baseRef, where the headRef is sending the changes (pushing them) and the baseRef is receiving the changes (pulling them).\n\nFigure 2: Disambiguating headRef and baseRef for push/pull operations\n\nWhen dealing with a branch, we’ll often refer to the headRef of its pull operations as its pullRef and the baseRef of its push operations as its pushRef. That’s because, in these instances, the working branch is the pull’s baseRef and the push’s headRef, so they’re already disambiguated.\n\nThe @{push} revision syntax\n\nTurns out, Git has a handy built-in tool for referring to the pushRef for a branch: the @{push} revision syntax. You can usually determine a branch’s pushRef by running the following command:\n\ngit rev-parse --abbrev-ref @{push}\n\nThis will result in a human-readable ref, like origin/branch, if one can be determined.\n\nPull Requests\n\nOn GitHub, a pull request is a proposal to integrate changes from one ref to another. In particular, they act as a simple “pause” before performing the actual integration operation, often called a merge, when changes are being pushed from ref to another. This pause allows for humans (code reviews) and robots (GitHub Copilot reviews and GitHub Actions workflows) to check the code before the changes are integrated. The name pull request came from this language specifically: You are requesting that a ref pulls your changes into itself.\n\nFigure 3: Demonstrating how GitHub Pull Requests correspond to pushing and pulling\n\nCommon Git workflows\n\nNow that you understand the basics, let’s talk about the workflows we typically use with Git every day.\n\nA centralized workflow is how most folks interact with Git and GitHub. In this configuration, any given branch is pushing and pulling from a remote ref with the same branch name. For most of us, this type of configuration is set up by default when we clone a repo and push a branch. It is the situation shown in Figure 1.\n\nIn contrast, a triangular workflow pushes to and pulls from different refs. A common use case for this configuration is to pull directly from a remote repository’s default branch into your local feature branch, eliminating the need to run commands like git rebase <default> or git merge <default> on your feature branch to ensure the branch you’re working on is always up to date with the default branch. However, when pushing changes, this configuration will typically push to a remote ref with the same branch name as the feature branch.\n\nFigure 4: juxtaposing centralized workflows from triangular workflows.\n\nWe complete the triangle when considering pull requests: the headRef is the pushRef for the local ref and the baseRef is the pullRef for the local branch:\n\nFigure 5: a triangular workflow\n\nWe can go one step further and set up triangular workflows using different remotes as well. This most commonly occurs when you’re developing on a fork. In this situation, you usually give the fork and source remotes different names. I’ll use origin for the fork and upstream for the source, as these are common names used in these setups. This functions exactly the same as the triangular workflows above, but the remotes and branches on the pushRef and pullRef are different:\n\nFigure 6: juxtaposing triangular workflows and centralized workflows with different remotes such as with forks\n\nUsing a Git configuration file for triangular workflows\n\nThere are two primary ways that you can set up a triangular workflow using the Git configuration – typically defined in a `.git/config` or `.gitconfig` file. Before explaining these, let’s take a look at what the relevant bits of a typical configuration look like in a repo’s `.git/config` file for a centralized workflow:\n\n[remote “origin”] url = https://github.com/OWNER/REPO.git fetch = +refs/heads/*:refs/remotes/origin/* [branch “default”] remote = origin merge = refs/heads/default [branch “branch”] remote = origin merge = refs/heads/branch\n\nFigure 7: A typical Git configuration setup found in .git/config\n\nThe [remote “origin”] part is naming the Git repository located at github.com/OWNER/REPO.git to origin, so we can reference it elsewhere by that name. We can see that reference being used in the specific [branch] configurations for both the default and branch branches in their remote keys. This key, in conjunction with the branch name, typically makes up the branch’s pushRef: in this example, it is origin/branch.\n\nThe remote and merge keys are combined to make up the branch’s pullRef: in this example, it is origin/branch.\n\nSetting up a triangular branch workflow\n\nThe simplest way to assemble a triangular workflow is to set the branch’s merge key to a different branch name, like so:\n\n[branch “branch”] remote = origin merge = refs/heads/default\n\nFigure 8: a triangular branch’s Git configuration found in .git/config\n\nThis will result in the branch pullRef as origin/default, but pushRef as origin/branch, as shown in Figure 9.\n\nFigure 9: A triangular branch workflow\n\nSetting up a triangular fork workflow\n\nWorking with triangular forks requires a bit more customization than triangular branches because we are dealing with multiple remotes. Thus, our remotes in the Git config will look different than the one shown previously in Figure 7:\n\n[remote “upstream”] url = https://github.com/ORIGINALOWNER/REPO.git fetch = +refs/heads/*:refs/remotes/upstream/* [remote “origin”] url = https://github.com/FORKOWNER/REPO.git fetch = +refs/heads/*:refs/remotes/origin/*\n\nFigure 10: a Git configuration for a multi-remote Git setup found in .git/config\n\nUpstream and origin are the most common names used in this construction, so I’ve used them here, but they can be named anything you want.\n\nHowever, toggling a branch’s remote key between upstream and origin won’t actually set up a triangular fork workflow—it will just set up a centralized workflow with either of those remotes, like the centralized workflow shown in Figure 6. Luckily, there are two common Git configuration options to change this behavior.\n\nSetting a branch’s pushremote\n\nA branch’s configuration has a key called pushremote that does exactly what the name suggests: configures the remote that the branch will push to. A triangular fork workflow config using pushremote may look like this:\n\n[branch “branch”] remote = upstream merge = refs/heads/default pushremote = origin\n\nFigure 11: a triangular fork’s Git config using pushremote found in .git/config\n\nThis assembles the triangular fork repo we see in Figure 12. The pullRef is upstream/default, as determined by combining the remote and merge keys, while the pushRef is origin/branch, as determined by combining the pushremote key and the branch name.\n\nFigure 12: A triangular fork workflow\n\nSetting a repo’s remote.pushDefault\n\nTo configure all branches in a repository to have the same behavior as what you’re seeing in Figure 12, you can instead set the repository’s pushDefault . The config for this is below:\n\n[remote] pushDefault = origin [branch “branch”] remote = upstream merge = refs/heads/default\n\nFigure 13: a triangular fork’s Git config using remote.pushDefault found in .git/config\n\nThis assembles the same triangular fork repo as shown in Figure 12 above, however this time the pushRef is determined by combining the remote.pushDefault key and the branch name, resulting in origin/branch.\n\nWhen using the branch’s pushremote and the repo’s remote.pushDefault keys together, Git will preferentially resolve the branch’s configuration over the repo’s, so the remote set on pushremote supersedes the remote set on remote.pushDefault .\n\nUpdating the gh pr command set to reflect Git\n\nPreviously, the gh pr command set did not resolve pushRefs and pullRefs in the same way that Git does. This was due to technical design decisions that made this change both difficult and complex. Instead of discussing that complexity—a big enough topic for a whole article in itself—I’m going to focus here on what you can now do with the updated gh pr command set.\n\nIf you set up triangular Git workflows in the manner described above, we will automatically resolve gh pr commands in accordance with your Git configuration.\n\nTo be slightly more specific, when trying to resolve a pull request for a branch, the GitHub CLI will respect whatever @{push} resolves to first, if it resolves at all. Then it will fall back to respect a branch’s pushremote, and if that isn’t set, finally look for a repo’s remote.pushDefault config settings.\n\nWhat this means is that the CLI is assuming your branch’s pullRef is the pull request’s baseRef and the branch’s pushRef is the pull requests headRef. In other words, if you’ve configured git pull and git push to work, then gh pr commands should just work. The diagram below, a general version of Figure 5, demonstrates this nicely:\n\nFigure 14: the triangular workflow supported by the GitHub CLI with respect to a branch’s pullRef and pushRef. This is the generalized version of Figure 5\n\nConclusion\n\nWe’re constantly working to improve the GitHub CLI, and we’d like the behavior of the GitHub CLI to reasonably reflect the behavior of Git. This was a team effort—everyone contributed to understanding, reviewing, and testing the code to enable this enhanced gh pr command set functionality.\n\nIt also couldn’t have happened without the support of our contributors, so we extend our thanks to them:\n\nCLI native support for triangular workflows was 4.5 years in the making, and we’re proud to have been able to provide this update for the community.\n\nThe GitHub CLI Team\n\n@andyfeller , @babakks , @bagtoad , @jtmcg , @mxie , @RyanHecht , and @williammartin\n\nTags:", "label": "non_personal"}
{"title": "Design system annotations, part 2: Advanced methods of annotating components", "url": "https://github.blog/engineering/user-experience/design-system-annotations-part-2-advanced-methods-of-annotating-components/", "content": "In part one of our design system annotation series, we discussed the ways in which accessibility can get left out of design system components from one instance to another. Our solution? Using a set of “Preset annotations” for each component with Primer. This allows designers to include specific pre-set details that aren’t already built into the component and visually communicated in the design itself.\n\nThat being said, Preset annotations are unique to each design system — and while ours may be a helpful reference for how to build them — they’re not something other organizations can utilize if you’re not also using the Primer design system.\n\nLuckily, you can build your own. Here’s how.\n\nHow to make Preset annotations for your design system\n\nStart by assessing components to understand which ones would need Preset annotations—not all of them will. Prioritize components that would benefit most from having a Preset annotation, and build that key information into each one. Next, determine what properties should be included. Only include key information that isn’t conveyed visually, isn’t in the component properties, and isn’t already baked into a coded component.\n\nPrioritizing components\n\nWhen a design system has 60+ components, knowing where to start can be a challenge. Which components need these annotations the most? Which ones would have the highest impact for both design teams and our users?\n\nWhen we set out to create a new set of Preset annotations based on our proof of concept, we decided to use ten Primer components that would benefit the most. To help pick them, we used an internal tool called Primer Query that tracks all component implementations across the GitHub codebase as well as any audit issues connected to them. Here is a video breakdown of how it works, if you’re curious.\n\nWe then prioritized new Preset annotations based on the following criteria:\n\nComponents that align to organization priorities (i.e. high value products and/or those that receive a lot of traffic). Components that appear frequently in accessibility audit issues. Components with React implementations (as our preferred development framework). Most frequently implemented components.\n\nMapping out the properties\n\nFor each component, we cross-referenced multiple sources to figure out what component properties and attributes would need to be added in each Preset annotation. The things we were looking for may only exist in one or two of those places, and thus are less likely to be accounted for all the way through the design and development lifecycle. The sources include:\n\nComponent documentation on Primer.style\n\nDesign system docs should contain usage guidance for designers and developers, and accessibility requirements should be a part of this guidance as well. Some of the guidance and requirements get built into the component’s Figma asset, while some only end up in the coded component.\n\nLook for any accessibility requirements that are not built into either Figma or code. If it’s built in, putting the same info in the Preset annotation may be redundant or irrelevant.\n\nPresets can account for rare use cases While building a Preset annotation for the TextInput component, we found that implementations may use an icon alone or have a hidden input label. With GitHub’s global search or filter inputs, the magnifying glass icon alone can act as the visible label, but the fields still need an accessible label for assistive technology users.\n\nCoded demos in Storybook\n\nOur component sandbox helped us see how each component is built in React or Rails, as well as what the HTML output is. We looked for any code structure or accessibility attributes that are not included in the component documentation or the Figma asset itself—especially when they may vary from one implementation to another.\n\nCode attributes a designer may not see or set Storybook helped us craft our TextInput component’s Preset annotation by showing some important attributes that don’t get any mention elsewhere. The type attribute is to the value of text . by default. Depending on the purpose of the field, an input’s type could also be search , email , number , tel , date , or time . This should be set intentionally so that users are able to use the most appropriate virtual keyboard.\n\nComponent properties in the Figma asset library\n\nLibrary assets provide a lot of flexibility through text layers, image fills, variants, and elaborate sets of component properties. We paid close attention to these options to understand what designers can and can’t change. Worthwhile additions to a Preset Annotation are accessibility attributes, requirements, and usage guidance in other sources that aren’t built into the Figma component.\n\nWhat’s missing from the TextInput’s Figma component When a TextInput is added to a design, the Figma component comes with many customizable options. There is an inputTextType property, which is about visual design and typography, not the type of form input. It’s possible to set the value of the Label and input field in Figma’s sidebar, but because it’s hidden by default, there’s no option to set the text of an error validation message. We can’t assume that every design delivered in Figma will come with examples of a form showing all of its error states, so these error messages may not get the attention they require. If this message can’t be built into the component as a text property, it can be added to the Preset annotation.\n\nOther potential sources\n\nExperiences from team members: The designers, developers, and accessibility specialists you work with may have insight into things that the docs and design tools may have missed. If your team and design system have been around for a while, their insights may be more valuable than those you’ll find in the docs, component demos, or asset libraries. Take some time to ask which components have had challenging bugs and which get intentionally broken when implemented.\n\nThe designers, developers, and accessibility specialists you work with may have insight into things that the docs and design tools may have missed. If your team and design system have been around for a while, their insights may be more valuable than those you’ll find in the docs, component demos, or asset libraries. Take some time to ask which components have had challenging bugs and which get intentionally broken when implemented. Findings from recent audits: Design system components themselves may have unresolved audit issues and remediation recommendations. If that’s the case, those issues are likely present in Storybook demos and may be unaccounted for in the component documentation. Design system audit issues may have details that both help create a Preset annotation and offer insights about what should not be carried over from existing resources.\n\nPutting it all together Our new Preset annotation for the TextInput component included links to usage guidance and Storybook as well as an optional tutorial for how the component is best used in a design to avoid potential issues. There are two mandatory prompts for input type and error text, and an optional one for the occasional hidden form label.\n\nWhat we learned from creating Preset annotations\n\nPreset annotations may not be for every team or organization. However, they are especially well suited for younger design systems and those that aren’t well adopted.\n\nMature design systems like Primer have frequent updates. This means that without close monitoring, the design system components themselves may fall out of sync with how a Preset annotation is built. This can end up causing confusion and rework after development starts, so it may be wise to make sure there’s some capacity to maintain these annotations after they’ve been created.\n\nFor newer teams at GitHub, new members of existing teams, and team members who were less familiar with the design system, the built-in guidance and links to documentation and component demos proved very useful. Those who are more experienced are also able to fine-tune the Presets and how they’re used.\n\nIf you don’t already have extensive experience with the design system components (or peers to help build them), it can take a lot of time to assess and map out the properties needed to build a Preset. It can also be challenging to name a component property succinctly enough that it doesn’t get truncated in Figma’s properties panel. If the context is not self-evident, some training or additional documentation may help.\n\nIt’s not always clear that you need a Preset annotation\n\nThere may be enough overlap between the Preset annotation for a component and types of annotations that aren’t specific to the design system.\n\nFor example, the GitHub Annotation Toolkit has components to annotate basic <textarea> form elements in addition to a Preset annotation for our <TextArea> Primer component:\n\nIn many instances, this flexibility may be confusing because you could use either annotation. For example, the Primer <TextArea> Preset has built-in links to specific Primer docs, and while the non-Preset version doesn’t, you could always add the links manually. While there’s some overlap between the two, using either one is better than none.\n\nOne way around this confusion is to add Primer-specific properties to the default set of annotations. This would allow you to do things like toggle a boolean property on a normal Button annotation and have it show links and properties specific to your design system’s button component.\n\nOur Preset creation process may unlock automation\n\nThere are currently a number of existing Figma plugins that advertise the ability to scan a design file to help with annotations. That being said, the results are often mixed and contain an unmanageable amount of noise and false positives. One of the reasons these issues happen is that these public plugins are design system agnostic.\n\nCurrent automated annotation tools aren’t able to understand that any design system components are being used without bespoke programming or thorough training of AI models. For plugins like this to be able to label design elements accurately, they first need to understand how to identify the components on the canvas, the variants used, and the set properties.\n\nWith that in mind, perhaps the most exciting insight is that the process of mapping out component properties for a Preset annotation—the things that don’t get conveyed in the visual design or in the code—is also something that would need to be done in any attempt to automate more usable annotations.\n\nIn other words, if a team uses a design system and wants to automate adding annotations, the tool they use would need to understand their components. In order for it to understand their components well enough to automate accurately, these hidden component properties would need to be mapped out. The task of creating a set of Preset annotations may be a vital stepping stone to something even more streamlined.\n\nA promising new method: Figma’s Code Connect\n\nWhile building our new set of Preset annotations, we experimented with other ways to enhance Primer with annotations. Though not all of those experiments worked out, one of them did: adding accessibility attributes through Code Connect.\n\nPrimer was one of the early adopters of Figma’s new Code Connect feature in Dev Mode. Says Lukas Oppermann, our staff systems designer, “With Code Connect, we can actually move the design and the code a little bit further apart again. We can concentrate on creating the best UX for the designers working in Figma with design libraries and, on the code side, we can have the best developer experience.”\n\nTo that end, Code Connect allows us to bypass much of our Preset annotations, as well as the downsides of some of our other experiments. It does this by adding key accessibility details directly into the code that developers can export from Figma.\n\nGitHub’s Octicons are used in many of our Primer components. They are decorative by default, but they sometimes need alt text or aria-label attributes depending on how they’re used. In the IconButton component, that button uses an Octicon and needs an accessible name to describe its function.\n\nWhen using a basic annotation kit, this may mean adding stamps for a Button and Decorative Image as well as a note in the margins that specifies what the aria-label should be. When using Preset annotations, there are fewer things to add to the canvas and the annotation process takes less time.\n\nWith Code Connect set up, Lukas added a hidden layer in the IconButton Figma component. It has a text property for aria-label which lets designers add the value directly from the component properties panel. No annotations needed. The hidden layer doesn’t disrupt any of the visuals, and the aria-label property gets exported directly with the rest of the component’s code.\n\nIt takes time to set up Code Connect with each of your design system components. Here are a few tips to help:\n\nConsistency is key. Make sure that the properties you create and how you place hidden layers is consistent across components. This helps set clear expectations so your teams can understand how these hidden layers and properties function.\n\nMake sure that the properties you create and how you place hidden layers is consistent across components. This helps set clear expectations so your teams can understand how these hidden layers and properties function. Use a branch of your design system library to experiment. Hiding attributes like aria-label is quite simple compared to other complex information that Preset annotations are capable of handling.\n\nHiding attributes like aria-label is quite simple compared to other complex information that Preset annotations are capable of handling. Use visual regression testing (VRT). Adding complexity directly to a component comes with increased risk of things breaking in the future, especially for those with many variants. Figma’s merge conflict UI is helpful, but may not catch everything.\n\nAs we continue to innovate with annotations and make our components more accessible, we are aiming to release our GitHub Annotation Toolkit in the near future. Stay tuned!\n\nFurther reading\n\nAccessibility annotation kits are a great resource, provided they’re used responsibly. Eric Bailey, one of the contributors to our forthcoming GitHub Annotation Toolkit, has written extensively about how annotations can highlight and amplify deeply structural issues when you’re building digital products.\n\nTags:", "label": "non_personal"}
{"title": "Building a more accessible GitHub CLI", "url": "https://github.blog/engineering/user-experience/building-a-more-accessible-github-cli/", "content": "At GitHub, we’re committed to making our tools truly accessible for every developer, regardless of ability or toolset. The command line interface (CLI) is a vital part of the developer experience, and the GitHub CLI is our product that brings the power of GitHub to your terminal.\n\nWhen it comes to accessibility, the terminal is fundamentally different from a web browser or a graphical user interface, with a lineage that predates the web itself. While standards like the Web Content Accessibility Guidelines (WCAG) provide a clear path for making web and graphical applications accessible, there is no equivalent, comprehensive standard for the terminal and CLIs. The W3C offers some high-level guidance for non-web software, but it stops short of prescribing concrete techniques, leaving much open to interpretation and innovation.\n\nThis gap has challenged us to think creatively and purposefully about what accessibility should look like in the terminal. Our recent Public Preview is focused on addressing the needs of three key groups: users who rely on screen readers, users who need high contrast between background and text, and users who require customizable color options. Our work aims to make the GitHub CLI more inclusive for all, regardless of how you interact with your terminal. Run gh a11y in the latest version of the GitHub CLI to enable these features, or read on to learn about our path to designing and implementing them.\n\nUnderstanding the terminal landscape\n\nText-based and command-line applications differ fundamentally from graphical or web applications. On a web page, assistive technologies like screen readers make use of the document object model (DOM) to infer structure and context of the page. Web pages can be designed such that the DOM’s structure is friendly to these technologies without impacting the visual design of the page. By contrast, CLI’s primary output is plain text, without hidden markup. A terminal emulator acts as the “user agent” for text apps, rendering characters as directed by the server application. Assistive technologies access this matrix of characters, analyze its layout, and try to infer structure. As the WCAG2ICT guidance notes, accessibility in this space means ensuring that all text output is available to assistive technologies, and that structural information is conveyed in a way that’s programmatically determinable—even if no explicit markup is present.\n\nIn our quest to improve the GitHub CLI’s usability for blind, low-vision, and colorblind users, we found ourselves navigating a landscape with lots of guidance, but few concrete techniques for implementing accessible experiences. We studied how assistive technology interacts with terminals: how screen readers review output, how color and contrast can be customized, and how structural cues can be inferred from plain text. Our recent Public Preview contains explorations into various use cases in these spaces.\n\nRethinking prompts and progress for screen readers\n\nOne of the GitHub CLI’s strengths as a command-line application is its rich prompting experience, which gives our users an interactive interface to enter command options. However, this rich interactive experience poses a hurdle for speech synthesis screen readers: Non-alphanumeric visual cues and uses of constant screen redraws for visual or other effects can be tricky to correctly interpret as speech.\n\nA demo video with sound of screen reader reading legacy prompter.\n\nTo reduce confusion and make it easier for blind and low vision users to confidently answer questions and navigate choices, we’re introducing a prompting experience that allows speech synthesis screen readers to accurately convey prompts to users. Our new prompter is built using Charm’s open source charmbracelet/huh prompting library.\n\n\n\nA demo of a screenreader correctly reading a prompt.\n\nAnother use case where the terminal is redrawn for visual effect is when showing progress bars. Our existing implementation uses a “spinner” made by redrawing the screen to display different braille characters (yes, we appreciate the irony) to give the user the indication that their command is executing. Speech synthesis screen readers do not handle this well:\n\n\n\nA demo of a screenreader and an old spinner.\n\nThis has been replaced with a static text progress indicator (with a relevant message to the action being taken where possible, falling back to a general “Working…” message). We’re working on identifying other areas we can further improve the contextual text.\n\nA demo video of the new progress indicator experience.\n\nColor, contrast, and customization\n\nColor is more than decoration in the terminal: It’s a vital tool for highlighting information, signaling errors, and guiding workflows. But color can also be a barrier—if contrast between the color of the terminal background and the text displayed on it is too low, some users will have difficulty discerning the displayed information. Unlike in a web browser, a terminal’s background color is not set by the application. That task is handled by the user’s terminal emulator. In order to maintain contrast, it is important that a command line application takes into account this variable. Our legacy color palette used for rendering Markdown did not take the terminal’s background color into account, leading to low contrast in some cases.\n\nThe colors themselves also matter. Different terminal environments have varied color capabilities (some support 4-bit, some 8-bit, some 24-bit, etc). No matter the capability, terminals enable users to customize their color preferences, choosing how different hues are displayed. However, most terminals only support changing a limited subset of colors: namely, the sixteen colors in the ANSI 4-bit color table. The GitHub CLI has made extensive efforts to align our color palettes to 4-bit colors so our users can completely customize their experience using their terminal preferences. We built on top of the accessibility foundations pioneered by Primer when deciding which 4-bit colors to use.\n\nBuilding for the CLI community\n\nOur improvements aim to support a wide range of developer needs, from blind users who need screen readers, to low vision users who need high contrast, to colorblind users who require customizable color options. But this Public Preview does not mark the end of our team’s commitment to enabling all developers to use the GitHub CLI. We intend to make it easier for our extension authors to implement the same accessibility improvements that we’ve made to the core CLI. This will allow users to have a cohesive experience across all GitHub CLI commands, official or community-maintained, and so that more workflows can be made accessible by default. We’re also looking into experiences to customize the formatting of tables output by commands to be more easily read/interpreted by screen readers. We’re excited to continue our accessibility journey.\n\n\n\nWe couldn’t have come this far without collaboration with our friends at Charm and our colleagues on the GitHub Accessibility team.\n\nA call for feedback\n\nWe invite you to help us in our goal to make the GitHub CLI an experience for all developers:\n\nTry it out : Update the GitHub CLI to v2.72.0 and run gh a11y in your terminal to learn more about enabling these new accessible features.\n\n: Update the GitHub CLI to v2.72.0 and run in your terminal to learn more about enabling these new accessible features. Share your experience : Join our GitHub CLI accessibility discussion to provide feedback or suggestions.\n\n: Join our GitHub CLI accessibility discussion to provide feedback or suggestions. Connect with us: If you have a lived experience relevant to our accessibility personas, reach out to the accessibility team or get involved in our discussion panel.\n\nLooking forward\n\nAdapting accessibility standards for the command line is a challenge—and an opportunity. We’re committed to sharing our approach, learning from the community, and helping set a new standard for accessible CLI tools.\n\nThank you for building a more accessible GitHub with us.\n\nWant to help us make GitHub the home for all developers? Learn more about GitHub’s accessibility efforts.", "label": "non_personal"}
{"title": "Design system annotations, part 1: How accessibility gets left out of components", "url": "https://github.blog/engineering/user-experience/design-system-annotations-part-1-how-accessibility-gets-left-out-of-components/", "content": "When it comes to design systems, every organization tends to be at a different place in their accessibility journey. Some have put a great deal of work into making their design system accessible while others have a long way to go before getting there. To help on this journey, many organizations rely on accessibility annotations to make sure there are no access barriers when a design is ready to be built.\n\nHowever, it’s a common misconception (especially for organizations with mature design systems) that accessible components will result in accessible designs. While design systems are fantastic for scaling standards and consistency, they can’t prevent every issue with our designs or how we build them. Access barriers can still slip through the cracks and make it into production.\n\nThis is the root of the problem our Accessibility Design team set out to solve.\n\nIn this two-part series, we’ll show you exactly how accessible design system components can produce inaccessible designs. Then we’ll demonstrate our solution: integrating annotations with our Primer components. This allows us to spend less time annotating, increases design system adoption, and reaches teams who may not have accessibility support. And in our next post, we’ll walk you through how you can do the same for your own components.\n\nLet’s dig in.\n\nWhat are annotations and their benefits?\n\nAnnotations are notes included in design projects that help make the unseen explicit by conveying design intent that isn’t shown visually. They improve the usability of digital experiences by providing a holistic picture for developers of how an experience should function. Integrating annotations into our design process helps our teams work better together by closing communication gaps and preventing quality issues, accessibility audit issues, and expensive re-work.\n\nSome of the questions annotations help us answer include:\n\nHow is assistive technology meant to navigate a page from one element to another?\n\nWhat’s the alternative text for informative images and buttons without labels?\n\nHow does content shift depending on viewport size, screen orientation, or zoom level?\n\nWhich virtual keyboard should be used for a form input on mobile?\n\nHow should focus be managed for complex interactions?\n\nOur answers to questions like this—or the lack thereof—can make or break the experience of the web for a lot of people, especially users with disabilities. Some annotation tools are built specifically to help with this by guiding designers to include key details about web standards, platform functionality, and accessibility (a11y).\n\nMost public annotation kits are well suited for teams who are creating new design system components, teams who aren’t already using a design system, or teams who don’t have specialized accessibility knowledge. They usually help annotate things like:\n\nControls such as buttons and links\n\nStructural elements such as headings and landmarks\n\nDecorative images and informative descriptions\n\nForms and other elements that require labels and semantic roles\n\nFocus order for assistive technology and keyboard navigation\n\nGitHub’s annotation’s toolkit\n\nOne of our top priorities is to meet our colleagues where they’re at. We wanted all our designers to be able to use annotations out of the box because we believe they shouldn’t need to be a certified accessibility specialist in order to get things built in an accessible way.\n\nTo this end, last year we began creating an internal Figma library—the GitHub Annotation Toolkit (which we aim to release to the public soon). Our toolkit builds on the legacy of the former Inclusive Design team at CVS Health. Their two open source annotation kits help make documentation that’s easy to create and consume, and are among the most widely used annotation libraries in the Figma Community.\n\nWhile they add clarity, annotations can also add overhead. If teams are only relying on specialists to interpret designs and technical specifications for developers, the hand-off process can take longer than it needs to. To create our annotation toolkit, we rebuilt its predecessor from the ground up to avoid that overhead, making extensive improvements and adding inline documentation to make it more intuitive and helpful for all of our designers—not just accessibility specialists.\n\nDesign systems can also help reduce that overhead. When you audit your design systems for accessibility, there’s less need for specialist attention on every product feature, since you’re using annotations to add technical semantics and specialist knowledge into every component. This means that designers and developers only need to adhere to the usage guidelines consistently, right?\n\nThe problems with annotations and design system components\n\nUnfortunately, it’s not that simple.\n\nAccessibility is not binary\n\nWhile design systems can help drive more accessible design at scale, they are constantly evolving and the work on them is never done. The accessibility of any component isn’t binary. Some may have a few severe issues that create access barriers, such as being inoperable with a keyboard or missing alt text. Others may have a few trivial issues, such as generic control labels.\n\nMost of the time, it will be a misnomer to claim that your design system is “fully accessible.” There’s always more work to do—it’s just a question of how much. The Web Content Accessibility Guidelines (WCAG) are a great starting point, but their “Success Criteria” isn’t tailored for the unique context that is your website or product or audience.\n\nWhile the WCAG should be used as a foundation to build from, it’s important to understand that it can’t capture every nuance of disabled users’ needs because your users’ needs are not every user’s needs. It would be very easy to believe that your design system is “fully accessible” if you never look past WCAG to talk to your users. If Primer has accessible components, it’s because we feel that direct participation and input from daily assistive technology users is the most important aspect of our work. Testing plans with real users—with and without disabilities—is where you really find what matters most.\n\nAccessible components do not guarantee accessible designs\n\nArranging a series of accessible components on a page does not automatically create an accurate and informative heading hierarchy. There’s a good chance that without additional documentation, the heading structure won’t make sense visually—nor as a medium for navigating with assistive technology.\n\nIt’s great when accessible components are flexible and responsive, but what about when they’re placed in a layout that the component guidance doesn’t account for? Do they adapt to different zoom levels, viewport sizes, and screen orientations? Do they lose any functionality or context when any of those things change?\n\nComponent usage is contextual. You can add an image or icon to your design, but the design system docs can’t write descriptive text for you. You can use the same image in multiple places, but the image description may need to change depending on context.\n\nSimilarly, forms built using the same input components may do different things and require different error validation messages. It’s no wonder that adopting design system components doesn’t get rid of all audit issues.\n\nDesign system components in Figma don’t include all the details\n\nAnnotation kits don’t include components for specific design systems because almost every organization is using their own. When annotation kits are adopted, teams often add ways to label their design system components.\n\nThis labeling lets developers know they can use something that’s already been built, and that they don’t need to build something from scratch. It also helps identify any design system components that get ‘detached’ in Figma. And it reduces the number of things that need to be annotated.\n\nLet’s look at an example:\n\nIf we’re using this Primer Button component from the Primer Web Figma library, there are a few important things that we won’t know just by looking at the design or the component properties:\n\nFunctional differences when components are implemented. Is this a link that just looks visually like a button? If so, a developer would use the <LinkButton> React component instead of <Button> .\n\nIs this a link that just looks visually like a button? If so, a developer would use the React component instead of . Accessible labels for folks using assistive technology. The icon may need alt text. In some cases, the button text might need some visually-hidden text to differentiate it from similar buttons. How would we know what that text is? Without annotations, the Figma component doesn’t have a place to display this.\n\nThe icon may need alt text. In some cases, the button text might need some visually-hidden text to differentiate it from similar buttons. How would we know what that text is? Without annotations, the Figma component doesn’t have a place to display this. Whether user data is submitted. When a design doesn’t include an obvious form with input fields, how do we convey that the button needs specific attributes to submit data?\n\nIt’s risky to leave questions like this unanswered, hoping someone notices and guesses the correct answer.\n\nA solution that streamlines the annotation process while minimizing risk\n\nWhen creating new components, a set of detailed annotations can be a huge factor in how robust and accessible they are. Once the component is built, design teams can start to add instances of that component in their designs. When those designs are ready to be annotated, those new components shouldn’t need to be annotated again. In most cases, it would be redundant and unnecessary—but not in every case.\n\nThere are some important details in many Primer components that may change from one instance to another. If we use the CVS Health annotation kit out of the box, we should be able to capture those variations, but we wouldn’t be able to avoid those redundant and unnecessary annotations. As we built our own annotation toolkit, we built a set of annotations for each Primer component to do both of those things at once.\n\nThis accordion component has been thoroughly annotated so that an engineer has everything they need to build it the first time. These include heading levels, semantics for <detail> and <summary> elements, landmarks, and decorative icons. All of this is built into the component so we don’t need to annotate most of this when adding the accordion to our new designs.\n\nHowever, there are two important things we need to annotate, as they can change from one instance to another:\n\nThe optional title at the top. The heading level of each item within the accordion.\n\nIf we don’t specify these things, we’re leaving it to chance that the page’s heading structure will break or that the experience will be confusing for people to understand and navigate the page. The risks may be low for a single button or basic accordion, but they grow with pattern complexity, component nesting, interaction states, duplicated instances, and so on.\n\nInstead of annotating what’s already built into the component or leaving these details to chance, we can add two quick annotations. One Stamp to point to the component, and one Details annotation where we fill in some blanks to make the heading levels clear.\n\nBecause the prompts for specific component details are pre-set in the annotation, we call them Preset annotations.\n\nIntroducing our Primer A11y Preset annotations\n\nWith this proof of concept, we selected ten frequently used Primer components for the same treatment and built a new set of Preset annotations to document these easily missed accessibility details—our Primer A11y Presets.\n\nThose Primer components tend to contribute to more accessibility audit issues when key details are missing on implementation. Issues for these components relate to things like lack of proper labels, error validation messages, or missing HTML or ARIA attributes.\n\nEach of our Preset annotations is linked to component docs and Storybook demos. This will hopefully help developers get straight to the technical info they need without designers having to find and add links manually. We also included guidance for how to fill out each Preset, as well as how to use the component in an accessible way. This helps designers get support inline without leaving their Figma canvas.\n\nWant to create your own? Check out Design system annotations, part 2\n\nButton components in Google’s Material Design and Shopify’s Polaris, IBM’s Carbon, or our Primer design system are all very different from one another. Because Preset annotations are based on specific components, they only work if you’re also using the design system they’re made for.\n\nIn part 2 of this series, we’ll walk you through how you can build your own set of Preset annotations for your design system, as well as some different ways to document important accessibility details before development starts.\n\nYou may also like:\n\nIf you’re more of a visual learner, you can watch Alexis Lucio explore Preset annotations during GitHub’s Dev Community Event to kick off Figma’s Config 2024.\n\nTags:", "label": "non_personal"}
{"title": "GitHub Issues search now supports nested queries and boolean operators: Here’s how we (re)built it", "url": "https://github.blog/developer-skills/application-development/github-issues-search-now-supports-nested-queries-and-boolean-operators-heres-how-we-rebuilt-it/", "content": "Originally, Issues search was limited by a simple, flat structure of queries. But with advanced search syntax, you can now construct searches using logical AND/OR operators and nested parentheses, pinpointing the exact set of issues you care about.\n\nBuilding this feature presented significant challenges: ensuring backward compatibility with existing searches, maintaining performance under high query volume, and crafting a user-friendly experience for nested searches. We’re excited to take you behind the scenes to share how we took this long-requested feature from idea to production.\n\nHere’s what you can do with the new syntax and how it works behind the scenes\n\nIssues search now supports building queries with logical AND/OR operators across all fields, with the ability to nest query terms. For example is:issue state:open author:rileybroughten (type:Bug OR type:Epic) finds all issues that are open AND were authored by rileybroughten AND are either of type bug or epic.\n\nHow did we get here?\n\nPreviously, as mentioned, Issues search only supported a flat list of query fields and terms, which were implicitly joined by a logical AND. For example, the query assignee:@me label:support new-project translated to “give me all issues that are assigned to me AND have the label support AND contain the text new-project.”\n\nBut the developer community has been asking for more flexibility in issue search, repeatedly, for nearly a decade now. They wanted to be able to find all issues that had either the label support or the label question , using the query label:support OR label:question . So, we shipped an enhancement towards this request in 2021, when we enabled an OR style search using a comma-separated list of values.\n\nHowever, they still wanted the flexibility to search this way across all issue fields, and not just the labels field. So we got to work.\n\nTechnical architecture and implementation\n\nFrom an architectural perspective, we swapped out the existing search module for Issues (IssuesQuery), with a new search module (ConditionalIssuesQuery), that was capable of handling nested queries while continuing to support existing query formats.\n\nThis involved rewriting IssueQuery, the search module that parsed query strings and mapped them into Elasticsearch queries.\n\nTo build a new search module, we first needed to understand the existing search module, and how a single search query flowed through the system. At a high level, when a user performs a search, there are three stages in its execution:\n\nParse: Breaking the user input string into a structure that is easier to process (like a list or a tree) Query: Transforming the parsed structure into an Elasticsearch query document, and making a query against Elasticsearch. Normalize: Mapping the results obtained from Elasticsearch (JSON) into Ruby objects for easy access and pruning the results to remove records that had since been removed from the database.\n\nEach stage presented its own challenges, which we’ll explore in more detail below. The Normalize step remained unchanged during the re-write, so we won’t dive into that one.\n\nParse stage\n\nThe user input string (the search phrase) is first parsed into an intermediate structure. The search phrase could include:\n\nQuery terms: The relevant words the user is trying to find more information about (ex: “models”)\n\nThe relevant words the user is trying to find more information about (ex: “models”) Search filters: These restrict the set of returned search documents based on some criteria (ex: “assignee:Deborah-Digges”)\n\nExample search phrase:\n\nFind all issues assigned to me that contain the word “codespaces”: is:issue assignee:@me codespaces\n\nFind all issues with the label documentation that are assigned to me: assignee:@me label:documentation\n\n\n\nThe old parsing method: flat list\n\nWhen only flat, simple queries were supported, it was sufficient to parse the user’s search string into a list of search terms and filters, which would then be passed along to the next stage of the search process.\n\nThe new parsing method: abstract syntax tree\n\nAs nested queries may be recursive, parsing the search string into a list was no longer sufficient. We changed this component to parse the user’s search string into an Abstract Syntax Tree (AST) using the parsing library parslet.\n\nWe defined a grammar (a PEG or Parsing Expression Grammar) to represent the structure of a search string. The grammar supports both the existing query syntax and the new nested query syntax, to allow for backward compatibility.\n\nA simplified grammar for a boolean expression described by a PEG grammar for the parslet parser is shown below:\n\nclass Parser < Parslet::Parser rule(:space) { match[\" \"].repeat(1) } rule(:space?) { space.maybe } rule(:lparen) { str(\"(\") >> space? } rule(:rparen) { str(\")\") >> space? } rule(:and_operator) { str(\"and\") >> space? } rule(:or_operator) { str(\"or\") >> space? } rule(:var) { str(\"var\") >> match[\"0-9\"].repeat(1).as(:var) >> space? } # The primary rule deals with parentheses. rule(:primary) { lparen >> or_operation >> rparen | var } # Note that following rules are both right-recursive. rule(:and_operation) { (primary.as(:left) >> and_operator >> and_operation.as(:right)).as(:and) | primary } rule(:or_operation) { (and_operation.as(:left) >> or_operator >> or_operation.as(:right)).as(:or) | and_operation } # We start at the lowest precedence rule. root(:or_operation) end\n\nFor example, this user search string:\n\nis:issue AND (author:deborah-digges OR author:monalisa )\n\nwould be parsed into the following AST:\n\n{ \"root\": { \"and\": { \"left\": { \"filter_term\": { \"attribute\": \"is\", \"value\": [ { \"filter_value\": \"issue\" } ] } }, \"right\": { \"or\": { \"left\": { \"filter_term\": { \"attribute\": \"author\", \"value\": [ { \"filter_value\": \"deborah-digges\" } ] } }, \"right\": { \"filter_term\": { \"attribute\": \"author\", \"value\": [ { \"filter_value\": \"monalisa\" } ] } } } } } } }\n\nQuery\n\nOnce the query is parsed into an intermediate structure, the next steps are to:\n\nTransform this intermediate structure into a query document that Elasticsearch understands Execute the query against Elasticsearch to obtain results\n\nExecuting the query in step 2 remained the same between the old and new systems, so let’s only go over the differences in building the query document below.\n\nThe old query generation: linear mapping of filter terms using filter classes\n\nEach filter term (Ex: label:documentation ) has a class that knows how to convert it into a snippet of an Elasticsearch query document. During query document generation, the correct class for each filter term is invoked to construct the overall query document.\n\nThe new query generation: recursive AST traversal to generate Elasticsearch bool query\n\nWe recursively traversed the AST generated during parsing to build an equivalent Elasticsearch query document. The nested structure and boolean operators map nicely to Elasticsearch’s boolean query with the AND, OR, and NOT operators mapping to the must, should, and should_not clauses.\n\nWe re-used the building blocks for the smaller pieces of query generation to recursively construct a nested query document during the tree traversal.\n\nContinuing from the example in the parsing stage, the AST would be transformed into a query document that looked like this:\n\n{ \"query\": { \"bool\": { \"must\": [ { \"bool\": { \"must\": [ { \"bool\": { \"must\": { \"prefix\": { \"_index\": \"issues\" } } } }, { \"bool\": { \"should\": { \"terms\": { \"author_id\": [ \"<DEBORAH_DIGGES_AUTHOR_ID>\", \"<MONALISA_AUTHOR_ID>\" ] } } } } ] } } ] } // SOME TERMS OMITTED FOR BREVITY } }\n\nWith this new query document, we execute a search against Elasticsearch. This search now supports logical AND/OR operators and parentheses to search for issues in a more fine-grained manner.\n\nConsiderations\n\nIssues is one of the oldest and most heavily -used features on GitHub. Changing core functionality like Issues search, a feature with an average of nearly 2000 queries per second (QPS)—that’s almost 160M queries a day!—presented a number of challenges to overcome.\n\nEnsuring backward compatibility\n\nIssue searches are often bookmarked, shared among users, and linked in documents, making them important artifacts for developers and teams. Therefore, we wanted to introduce this new capability for nested search queries without breaking existing queries for users.\n\nWe validated the new search system before it even reached users by:\n\nTesting extensively : We ran our new search module against all unit and integration tests for the existing search module. To ensure that the GraphQL and REST API contracts remained unchanged, we ran the tests for the search endpoint both with the feature flag for the new search system enabled and disabled.\n\n: We ran our new search module against all unit and integration tests for the existing search module. To ensure that the GraphQL and REST API contracts remained unchanged, we ran the tests for the search endpoint both with the feature flag for the new search system enabled and disabled. Validating correctness in production with dark-shipping: For 1% of issue searches, we ran the user’s search against both the existing and new search systems in a background job, and logged differences in responses. By analyzing these differences we were able to fix bugs and missed edge cases before they reached our users. We weren’t sure at the outset how to define “differences,” but we settled on “number of results” for the first iteration. In general, it seemed that we could determine whether a user would be surprised by the results of their search against the new search capability if a search returned a different number of results when they were run within a second or less of each other.\n\nFor 1% of issue searches, we ran the user’s search against both the existing and new search systems in a background job, and logged differences in responses. By analyzing these differences we were able to fix bugs and missed edge cases before they reached our users.\n\nPreventing performance degradation\n\nWe expected more complex nested queries to use more resources on the backend than simpler queries, so we needed to establish a realistic baseline for nested queries, while ensuring no regression in the performance of existing, simpler ones.\n\nFor 1% of Issue searches, we ran equivalent queries against both the existing and the new search systems. We used scientist, GitHub’s open source Ruby library, for carefully refactoring critical paths, to compare the performance of equivalent queries to ensure that there was no regression.\n\nPreserving user experience\n\nWe didn’t want users to have a worse experience than before just because more complex searches were possible.\n\nWe collaborated closely with product and design teams to ensure usability didn’t decrease as we added this feature by:\n\nLimiting the number of nested levels in a query to five. From customer interviews, we found this to be a sweet spot for both utility and usability.\n\nin a query to five. From customer interviews, we found this to be a sweet spot for both utility and usability. Providing helpful UI/UX cues: We highlight the AND/OR keywords in search queries, and provide users with the same auto-complete feature for filter terms in the UI that they were accustomed to for simple flat queries.\n\nMinimizing risk to existing users\n\nFor a feature that is used by millions of users a day, we needed to be intentional about rolling it out in a way that minimized risk to users.\n\nWe built confidence in our system by:\n\nLimiting blast radius : To gradually build confidence, we only integrated the new system in the GraphQL API and the Issues tab for a repository in the UI to start. This gave us time to collect, respond to, and incorporate feedback without risking a degraded experience for all consumers. Once we were happy with its performance, we rolled it out to the Issues dashboard and the REST API.\n\n: To gradually build confidence, we only integrated the new system in the GraphQL API and the Issues tab for a repository in the UI to start. This gave us time to collect, respond to, and incorporate feedback without risking a degraded experience for all consumers. Once we were happy with its performance, we rolled it out to the Issues dashboard and the REST API. Testing internally and with trusted partners: As with every feature we build at GitHub, we tested this feature internally for the entire period of its development by shipping it to our own team during the early days, and then gradually rolling it out to all GitHub employees. We then shipped it to trusted partners to gather initial user feedback.\n\nAnd there you have it, that’s how we built, validated, and shipped the new and improved Issues search!\n\nFeedback\n\nWant to try out this exciting new functionality? Head to our docs to learn about how to use boolean operators and parentheses to search for the issues you care about!\n\nIf you have any feedback for this feature, please drop us a note on our community discussions.\n\nAcknowledgements\n\nSpecial thanks to AJ Schuster, Riley Broughten, Stephanie Goldstein, Eric Jorgensen Mike Melanson and Laura Lindeman for the feedback on several iterations of this blog post!\n\nTags:", "label": "non_personal"}
{"title": "How GitHub engineers tackle platform problems", "url": "https://github.blog/engineering/infrastructure/how-github-engineers-tackle-platform-problems/", "content": "In my spare time I enjoy building Gundam models, which are model kits to build iconic mechas from the Gundam universe. You might be wondering what this has to do with software engineering. Product engineers can be seen as the engineers who take these kits and build the Gundam itself. They are able to utilize all pieces and build a working product that is fun to collect or even play with!\n\nPlatform engineers, on the other hand, supply the tools needed to build these kits (like clippers and files) and maybe even build a cool display so everyone can see the final product. They ensure that whoever is constructing it has all the necessary tools, even if they don’t physically build the Gundam themselves.\n\nAbout a year ago, my team at GitHub moved to the infrastructure organization, inheriting new roles and Areas of Responsibility (AoRs). Previously, the team had tackled external customer problems, such as building the new deployment views across environments. This involved interacting with users who depend on GitHub to address challenges within their respective industries. Our new customers as a platform engineering team are internal, which makes our responsibilities different from the product-focused engineering work we were doing before.\n\nGoing back to my Gundam example, rather than constructing kits, we’re now responsible for building the components of the kits. Adapting to this change meant I had to rethink my approach to code testing and problem solving.\n\nWhether you’re working on product engineering or on the platform side, here are a few best practices to tackle platform problems.\n\nUnderstanding your domain\n\nOne of the most critical steps before tackling problems is understanding the domain. A “domain” is the business and technical subject area in which a team and platform organization operate. This requires gaining an understanding of technical terms and how these systems interact to provide fast and reliable solutions. Here’s how to get up to speed:\n\nTalk to your neighbors: Arrange a handover meeting with a team that has more knowledge and experience with the subject matter. This meeting provides an opportunity to ask questions about terminology and gain a deeper understanding of the problems the team will be addressing.\n\nArrange a handover meeting with a team that has more knowledge and experience with the subject matter. This meeting provides an opportunity to ask questions about terminology and gain a deeper understanding of the problems the team will be addressing. Investigate old issues: If there is a backlog of issues that are either stale or still persistent, they may give you a better understanding of the system’s current limitations and potential areas for improvement.\n\nIf there is a backlog of issues that are either stale or still persistent, they may give you a better understanding of the system’s current limitations and potential areas for improvement. Read the docs: Documentation is a goldmine of knowledge that can help you understand how the system works.\n\nBridging concepts to platform-specific skills\n\nWhile the preceding advice offers general guidance applicable to both product and platform teams, platform teams — serving as the foundational layer — necessitate a more in-depth understanding.\n\nNetworks : Understanding network fundamentals is crucial for all engineers, even those not directly involved in network operations. This includes concepts like TCP, UDP, and L4 load balancing, as well as debugging tools such as dig. A solid grasp of these areas is essential to comprehend how network traffic impacts your platform.\n\n: Understanding network fundamentals is crucial for all engineers, even those not directly involved in network operations. This includes concepts like TCP, UDP, and L4 load balancing, as well as debugging tools such as dig. A solid grasp of these areas is essential to comprehend how network traffic impacts your platform. Operating systems and hardware: Selecting appropriate virtual machines (VMs) or physical hardware is vital for both scalability and cost management. Making well-informed choices for particular applications requires a strong grasp of both. This is closely linked to choosing the right operating system for your machines, which is important to avoid systems with vulnerabilities or those nearing end of life.\n\nSelecting appropriate virtual machines (VMs) or physical hardware is vital for both scalability and cost management. Making well-informed choices for particular applications requires a strong grasp of both. This is closely linked to choosing the right operating system for your machines, which is important to avoid systems with vulnerabilities or those nearing end of life. Infrastructure as Code (IaC): Automation tools like Terraform, Ansible, and Consul are becoming increasingly essential. Proficiency in these tools is becoming a necessity as they significantly decrease human error during infrastructure provisioning and modifications.\n\nAutomation tools like Terraform, Ansible, and Consul are becoming increasingly essential. Proficiency in these tools is becoming a necessity as they significantly decrease human error during infrastructure provisioning and modifications. Distributed systems: Dealing with platform issues, particularly in distributed systems, necessitates a deep understanding that failures are inevitable. Consequently, employing proactive solutions like failover and recovery mechanisms is crucial for preserving system reliability and preventing adverse user experiences. The optimal approach for this depends entirely on the specific problem and the desired system behavior.\n\nKnowledge sharing\n\nBy sharing lessons and ideas, engineers can introduce new perspectives that lead to breakthroughs and innovations. Taking the time to understand why a project or solution did or didn’t work and sharing those findings provides new perspectives that we can use going forward.\n\nHere are three reasons why knowledge sharing is so important:\n\nTeamwork makes the dream work: Collaboration often results in quicker problem resolution and fosters new solution innovation, as engineers have the opportunity to learn from each other and expand upon existing ideas.\n\nCollaboration often results in quicker problem resolution and fosters new solution innovation, as engineers have the opportunity to learn from each other and expand upon existing ideas. Prevent lost knowledge : If we don’t share our lessons learned, we prevent the information from being disseminated across the team or organization. This becomes a problem if an engineer leaves the company or is simply unavailable.\n\n: If we don’t share our lessons learned, we prevent the information from being disseminated across the team or organization. This becomes a problem if an engineer leaves the company or is simply unavailable. Improve our customer success: As engineers, our solutions should effectively serve our customers. By sharing our knowledge and lessons learned, we can help the team build reliable, scalable, and secure platforms, which will enable us to create better products that meet customer needs and expectations!\n\nBut big differences start to appear between product engineering and infrastructure engineering when it comes to the impact radius and the testing process.\n\nImpact radius\n\nWith platforms being the fundamental building blocks of a system, any change (small or large) can affect a wide range of products. Our team is responsible for DNS, a foundational service that impacts numerous products. Even a minor alteration to this service can have extensive repercussions, potentially disrupting access to content across our site and affecting products ranging from GitHub Pages to GitHub Copilot.\n\nUnderstand the radius: Or understand the downstream dependencies. Direct communication with teams that depend on our service provides valuable insights into how proposed changes may affect other services.\n\nOr understand the downstream dependencies. Direct communication with teams that depend on our service provides valuable insights into how proposed changes may affect other services. Postmortems: By looking at past incidents related to our platform and asking “What is the impact of this incident?”, we can form more context around what change or failure was introduced, how our platform played a role in it, and how it was fixed.\n\nBy looking at past incidents related to our platform and asking “What is the impact of this incident?”, we can form more context around what change or failure was introduced, how our platform played a role in it, and how it was fixed. Monitoring and telemetry: Condense important monitoring and logging into a small and quickly digestible medium to give you the general health of the system. This could be a Single Availability Metric (SAM), for example. The ability to quickly glance at a single dashboard allows engineers to rapidly pinpoint the source of an issue and streamlines the debugging and incident mitigation process, as compared to searching through and interpreting detailed monitors or log messages.\n\nTesting changes\n\nTesting changes in a distributed environment can be challenging, especially for services like DNS. A crucial step in solving this issue is utilizing a test site as a “real” machine where you can implement and assess all your changes.\n\nInfrastructure as Code (IaC): When using tools like Terraform or Ansible, it’s crucial to test fundamental operations like provisioning and deprovisioning machines. There are circumstances where a machine will need to be re-provisioned. In these cases, we want to ensure the machine is not accidentally deleted and that we retain the ability to create a new one if needed.\n\nWhen using tools like Terraform or Ansible, it’s crucial to test fundamental operations like provisioning and deprovisioning machines. There are circumstances where a machine will need to be re-provisioned. In these cases, we want to ensure the machine is not accidentally deleted and that we retain the ability to create a new one if needed. End-to-End (E2E): Begin directing some network traffic to these servers. Then the team can observe host behavior by directly interacting with it, or we can evaluate functionality by diverting a small portion of traffic.\n\nBegin directing some network traffic to these servers. Then the team can observe host behavior by directly interacting with it, or we can evaluate functionality by diverting a small portion of traffic. Self-healing: We want to test the platform’s ability to recover from unexpected loads and identify bottlenecks before they impact our users. Early identification of bottlenecks or bugs is crucial for maintaining the health of our platform.\n\nIdeally changes will be implemented on a host-by-host basis once testing is complete. This approach allows for individual machine rollback and prevents changes from being applied to unaffected hosts.\n\nWhat to remember\n\nPlatform engineering can be difficult. The systems GitHub operates with are complex and there are a lot of services and moving parts. However, there’s nothing like seeing everything come together. All the hard work our engineering teams do behind the scenes really pays off when the platform is running smoothly and teams are able to ship faster and more reliably — which allows GitHub to be the home to all developers.\n\nWant to dive deeper? Check out our infrastructure related blog posts.", "label": "non_personal"}
{"title": "How enterprise engineering teams can successfully adopt AI", "url": "https://github.com/resources/whitepapers/how-enterprise-engineering-teams-can-successfully-adopt-ai", "content": "Afghanistan ( +93 ) Åland ( +358 ) Albania ( +355 ) Algeria ( +213 ) American Samoa ( +1 ) Andorra ( +376 ) Angola ( +244 ) Anguilla ( +1 ) Antigua and Barbuda ( +1 ) Argentina ( +54 ) Armenia ( +374 ) Aruba ( +297 ) Australia ( +61 ) Austria ( +43 ) Azerbaijan ( +994 ) Bahamas ( +1 ) Bahrain ( +973 ) Bangladesh ( +880 ) Barbados ( +1 ) Belarus ( +375 ) Belgium ( +32 ) Belize ( +501 ) Benin ( +229 ) Bermuda ( +1 ) Bhutan ( +975 ) Bolivia ( +591 ) Bonaire, Sint Eustatius and Saba ( +599 ) Bosnia and Herzegovina ( +387 ) Botswana ( +267 ) Brazil ( +55 ) British Indian Ocean Territory ( +246 ) Brunei Darussalam ( +673 ) Bulgaria ( +359 ) Burkina Faso ( +226 ) Burundi ( +257 ) Cambodia ( +855 ) Cameroon ( +237 ) Canada ( +1 ) Cape Verde ( +238 ) Cayman Islands ( +1 ) Central African Republic ( +236 ) Chad ( +235 ) Chile ( +56 ) China ( +86 ) Christmas Island ( +61 ) Cocos (Keeling) Islands ( +61 ) Colombia ( +57 ) Comoros ( +269 ) Congo (Brazzaville) ( +242 ) Congo (Kinshasa) ( +243 ) Cook Islands ( +682 ) Costa Rica ( +506 ) Côte d'Ivoire ( +225 ) Croatia ( +385 ) Curaçao ( +599 ) Cyprus ( +357 ) Czech Republic ( +420 ) Denmark ( +45 ) Djibouti ( +253 ) Dominica ( +1 ) Dominican Republic ( +1 ) Ecuador ( +593 ) Egypt ( +20 ) El Salvador ( +503 ) Equatorial Guinea ( +240 ) Eritrea ( +291 ) Estonia ( +372 ) Ethiopia ( +251 ) Falkland Islands ( +500 ) Faroe Islands ( +298 ) Fiji ( +679 ) Finland ( +358 ) France ( +33 ) French Guiana ( +594 ) French Polynesia ( +689 ) Gabon ( +241 ) Gambia ( +220 ) Georgia ( +995 ) Germany ( +49 ) Ghana ( +233 ) Gibraltar ( +350 ) Greece ( +30 ) Greenland ( +299 ) Grenada ( +1 ) Guadeloupe ( +590 ) Guam ( +1 ) Guatemala ( +502 ) Guernsey ( +44 ) Guinea ( +224 ) Guinea-Bissau ( +245 ) Guyana ( +592 ) Haiti ( +509 ) Honduras ( +504 ) Hong Kong ( +852 ) Hungary ( +36 ) Iceland ( +354 ) India ( +91 ) Indonesia ( +62 ) Iran ( +98 ) Iraq ( +964 ) Ireland ( +353 ) Isle of Man ( +44 ) Israel ( +972 ) Italy ( +39 ) Jamaica ( +1 ) Japan ( +81 ) Jersey ( +44 ) Jordan ( +962 ) Kazakhstan ( +7 ) Kenya ( +254 ) Kiribati ( +686 ) Korea, South ( +82 ) Kuwait ( +965 ) Kyrgyzstan ( +996 ) Laos ( +856 ) Latvia ( +371 ) Lebanon ( +961 ) Lesotho ( +266 ) Liberia ( +231 ) Libya ( +218 ) Liechtenstein ( +423 ) Lithuania ( +370 ) Luxembourg ( +352 ) Macau ( +853 ) Macedonia ( +389 ) Madagascar ( +261 ) Malawi ( +265 ) Malaysia ( +60 ) Maldives ( +960 ) Mali ( +223 ) Malta ( +356 ) Marshall Islands ( +692 ) Martinique ( +596 ) Mauritania ( +222 ) Mauritius ( +230 ) Mayotte ( +262 ) Mexico ( +52 ) Micronesia ( +691 ) Moldova ( +373 ) Monaco ( +377 ) Mongolia ( +976 ) Montenegro ( +382 ) Montserrat ( +1 ) Morocco ( +212 ) Mozambique ( +258 ) Myanmar ( +95 ) Namibia ( +264 ) Nauru ( +674 ) Nepal ( +977 ) Netherlands ( +31 ) New Caledonia ( +687 ) New Zealand ( +64 ) Nicaragua ( +505 ) Niger ( +227 ) Nigeria ( +234 ) Niue ( +683 ) Norfolk Island ( +672 ) Northern Mariana Islands ( +1 ) Norway ( +47 ) Oman ( +968 ) Pakistan ( +92 ) Palau ( +680 ) Palestine ( +970 ) Panama ( +507 ) Papua New Guinea ( +675 ) Paraguay ( +595 ) Peru ( +51 ) Philippines ( +63 ) Poland ( +48 ) Portugal ( +351 ) Puerto Rico ( +1 ) Qatar ( +974 ) Reunion ( +262 ) Romania ( +40 ) Rwanda ( +250 ) Saint Barthélemy ( +590 ) Saint Helena ( +290 ) Saint Kitts and Nevis ( +1 ) Saint Lucia ( +1 ) Saint Martin (French part) ( +590 ) Saint Pierre and Miquelon ( +508 ) Saint Vincent and the Grenadines ( +1 ) Samoa ( +685 ) San Marino ( +378 ) Sao Tome and Principe ( +239 ) Saudi Arabia ( +966 ) Senegal ( +221 ) Serbia ( +381 ) Seychelles ( +248 ) Sierra Leone ( +232 ) Singapore ( +65 ) Sint Maarten (Dutch part) ( +1 ) Slovakia ( +421 ) Slovenia ( +386 ) Solomon Islands ( +677 ) Somalia ( +252 ) South Africa ( +27 ) South Sudan ( +211 ) Spain ( +34 ) Sri Lanka ( +94 ) Sudan ( +249 ) Suriname ( +597 ) Svalbard and Jan Mayen Islands ( +47 ) Swaziland ( +268 ) Sweden ( +46 ) Switzerland ( +41 ) Taiwan ( +886 ) Tajikistan ( +992 ) Tanzania ( +255 ) Thailand ( +66 ) Timor-Leste ( +670 ) Togo ( +228 ) Tokelau ( +690 ) Tonga ( +676 ) Trinidad and Tobago ( +1 ) Tunisia ( +216 ) Türkiye ( +90 ) Turkmenistan ( +993 ) Turks and Caicos Islands ( +1 ) Tuvalu ( +688 ) Uganda ( +256 ) Ukraine ( +380 ) United Arab Emirates ( +971 ) United Kingdom ( +44 ) United States of America ( +1 ) Uruguay ( +598 ) Uzbekistan ( +998 ) Vanuatu ( +678 ) Vatican City ( +39 ) Venezuela ( +58 ) Vietnam ( +84 ) Virgin Islands, British ( +1 ) Virgin Islands, U.S. ( +1 ) Wallis and Futuna Islands ( +681 ) Yemen ( +967 ) Zambia ( +260 ) Zimbabwe ( +263 )\n\n+1", "label": "non_personal"}
{"title": "Unlocking the power of unstructured data with RAG", "url": "https://github.blog/ai-and-ml/llms/unlocking-the-power-of-unstructured-data-with-rag/", "content": "Whether they’re building a new product or improving a process or feature, developers and IT leaders need data and insights to make informed decisions.\n\nWhen it comes to software development, this data exists in two ways: unstructured and structured. While structured data follows a specific and predefined format, unstructured data—like email, an audio or visual file, code comment, or commit message—doesn’t. This makes unstructured data hard to organize and interpret, which means teams can miss out on potentially valuable insights.\n\nTo make the most of their unstructured data, development teams are turning to retrieval-augmented generation, or RAG, a method for customizing large language models (LLMs). They can use RAG to keep LLMs up to date with organizational knowledge and the latest information available on the web. They can also use RAG and LLMs to surface and extract insights from unstructured data.\n\nGitHub data scientists, Pam Moriarty and Jessica Guo, explain unstructured data’s unique value in software development, and how developers and organizations can use RAG to create greater efficiency and value in the development process.\n\nUnstructured data in software development\n\nWhen it comes to software development, unstructured data includes source code and the context surrounding it, as these sources of information don’t follow a predefined format.\n\nHere are some examples of unstructured data on GitHub:\n\nREADME files describe in text the purpose behind project source code, and include instructions for source code use, how to contribute, and other details that developers decide is important to include. While they’re usually written in Markdown, README files don’t follow a predefined structure.\n\ndescribe in text the purpose behind project source code, and include instructions for source code use, how to contribute, and other details that developers decide is important to include. While they’re usually written in Markdown, README files don’t follow a predefined structure. Code files are more orderly than README files in that they follow the syntax of a programming language. But not all code files have the exact same fields nor are they all written in the same format. Additionally, some parts of the file, like coding logic and variable names, are decided by individual developers.\n\nare more orderly than README files in that they follow the syntax of a programming language. But not all code files have the exact same fields nor are they all written in the same format. Additionally, some parts of the file, like coding logic and variable names, are decided by individual developers. Package documentation explains how the software works and how to use it. Documentation, written in natural language, can include installation instructions, troubleshooting tips, a description of the package’s API, and a list of any dependencies required to use the package. It can also include code snippets that highlight the package’s features.\n\nexplains how the software works and how to use it. Documentation, written in natural language, can include installation instructions, troubleshooting tips, a description of the package’s API, and a list of any dependencies required to use the package. It can also include code snippets that highlight the package’s features. Code comments explain the function behind certain code blocks in a code file. They’re text comments written in natural language and make the source code easier to understand by other developers.\n\nexplain the function behind certain code blocks in a code file. They’re text comments written in natural language and make the source code easier to understand by other developers. Wiki pages , while not limited to unstructured data, can contain helpful text documentation about installation instructions, API references, and other information.\n\n, while not limited to unstructured data, can contain helpful text documentation about installation instructions, API references, and other information. Commit messages describe in natural language text the changes a developer made to a codebase and why.\n\ndescribe in natural language text the changes a developer made to a codebase and why. Issue and pull request descriptions are written in natural language and in a text field. They can contain any kind of information a developer chooses to include about a bug, feature request, or general task in a project.\n\nare written in natural language and in a text field. They can contain any kind of information a developer chooses to include about a bug, feature request, or general task in a project. Discussions contain a wealth and variety of information, from developer and end- user feedback to open-ended conversations about a topic. As long as a repository enables discussions, anyone with a GitHub account can start a discussion.\n\ncontain a wealth and variety of information, from developer and end- user feedback to open-ended conversations about a topic. As long as a repository enables discussions, anyone with a GitHub account can start a discussion. Review comments are where developers can discuss changes before they’re merged into a codebase. Consequently, they contain information in natural language about code quality, context behind certain decisions, and concerns about potential bugs.\n\nThe value of unstructured data\n\nThe same features that make unstructured data valuable also make it hard to analyze.\n\nUnstructured data lacks inherent organization, as it often consists of free-form text, images, or multimedia content.\n\n“Without clear boundaries or predefined formats, extracting meaningful information from unstructured data becomes very challenging,” Guo says.\n\nBut LLMs can help to identify complex patterns in unstructured data—especially text. Though not all unstructured data is text, a lot of text is unstructured. And LLMs can help you to analyze it.\n\n“When dealing with ambiguous, semi-structured or unstructured data, LLMs dramatically excel at identifying patterns, sentiments, entities, and topics within text data and uncover valuable insights that might otherwise remain hidden,” Guo explains.\n\nNeed a refresher on LLMs? Check out our AI explainers, guides, and best practices >\n\nHere are a few reasons why developers and IT leaders might consider using RAG-powered LLMs to leverage unstructured data:\n\nSurface organizational best practices and establish consistency . Through RAG, an LLM can receive a prompt with additional context pulled from an organization’s repositories and documents. So, instead of sifting through and piece-mealing documents, developers can quickly receive answers from an LLM that align with their organization’s knowledge and best practices.\n\n. Through RAG, an LLM can receive a prompt with additional context pulled from an organization’s repositories and documents. So, instead of sifting through and piece-mealing documents, developers can quickly receive answers from an LLM that align with their organization’s knowledge and best practices. Accelerate and deepen understanding of an existing codebase—including its conventions, functions, common issues, and bugs. Understanding and familiarizing yourself with code written by another developer is a persisting challenge for several reasons, including but not limited to: code complexity, use of different coding styles, a lack of documentation, use of legacy code or deprecated libraries and APIs, and the buildup of technical debt from quick fixes and workarounds.\n\nRAG can help to mediate these pain points by enabling developers to ask and receive answers in natural language about a specific codebase. It can also guide developers to relevant documentation or existing solutions.\n\nAccelerated and deepened understanding of a codebase enables junior developers to contribute their first pull request with less onboarding time and senior developers to mitigate live site incidents, even when they’re unfamiliar with the service that’s failing. It also means that legacy code suffering from “code rot” and natural aging can be more quickly modernized and easily maintained.\n\nUnstructured data doesn’t just help to improve development processes. It can also improve product decisions by surfacing user pain points.\n\nMoriarty says, “Structured data might show a user’s decision to upgrade or renew a subscription, or how frequently they use a product or not. While those decisions represent the user’s attitude and feelings toward the product, it’s not a complete representation. Unstructured data allows for more nuanced and qualitative feedback, making for a more complete picture.”\n\nA lot of information and feedback is shared during informal discussions, whether those discussions happen on a call, over email, on social platforms, or in an instant message. From these discussions, decision makers and builders can find helpful feedback to improve a service or product, and understand general public and user sentiment.\n\nWhat about structured data?\n\nContrary to unstructured data, structured data—like relational databases, Protobuf files, and configuration files—follows a specific and predefined format.\n\nWe’re not saying unstructured data is more valuable than structured. But the processes for analyzing structured data are more straightforward: you can use SQL functions to modify the data and traditional statistical methods to understand the relationship between different variables.\n\nThat’s not to say AI isn’t used for structured data analysis. “There’s a reason that machine learning, given its predictive power, is and continues to be widespread across industries that use data,” according to Moriarty.\n\nHowever, “Structured data is often numeric, and numbers are simply easier to analyze for patterns than words are,” Moriarty says. Not to mention that methods for analyzing structured data have been around longer** **than those for analyzing unstructured data: “A longer history with more focus just means there are more established approaches, and more people are familiar with it,” she explains.\n\nThat’s why the demand to enhance structured data might seem less urgent, according to Guo. “The potential for transformative impact is significantly greater when applied to unstructured data,” she says.\n\nHow does RAG extract value from unstructured data?\n\nWith RAG, an LLM can use data sources beyond its training data to generate an output.\n\nRAG is a prompting method that uses retrieval—a process for searching for and accessing information—to add more context to a prompt that generates an LLM response.\n\nThis method is designed to improve the quality and relevance of an LLM’s outputs. Additional data sources include a vector database, traditional database, or search engine. So, developers who use an enterprise AI tool equipped with RAG can receive AI outputs customized to their organization’s best practices and knowledge, and proprietary data.\n\nWe break down these data sources in our RAG explainer, but here’s a quick summary:\n\nVector databases. While you code in your IDE, algorithms create embeddings for your code snippets, which are stored in a vector database. An AI coding tool can search that database to find snippets from across your codebase that are similar to the code you’re currently writing and generate a suggestion.\n\nAnd when you’re engaging with GitHub Copilot Chat on GitHub.com or in the IDE, your query or code is transformed into an embedding. Our retrieval service then fetches relevant embeddings from the vector database for the repository you’ve indexed. These embeddings are turned back into text and code when they’re added to the prompt as additional context for the LLM. This entire process leverages unstructured data, even though the retrieval system uses embeddings internally.\n\nGeneral text search. When developers engage with GitHub Copilot Chat under a GitHub Copilot Enterprise plan, they can index repositories—specifically code and documentation. So, when a developer on GitHub.com or in the IDE asks GitHub Copilot Chat a question about an indexed repository, the AI coding tool can retrieve data from all of those indexed, unstructured data sources. And on GitHub.com, GitHub Copilot Chat can tap into a collection of unstructured data in Markdown files from across repositories, which we call knowledge bases.\n\nLearn about GitHub Copilot Enterprise features >\n\nBut wait, why is Markdown considered unstructured data? Though you can use Markdown to format a file, the file itself can contain essentially any kind of data. Think about it this way: how would you put the contents of a Markdown file in a table?\n\nExternal or internal search engine. The retrieval method searches and pulls information from a wide range of sources from the public web or your internal platforms and websites. That information is used for RAG, which means the AI model now has data from additional files—like text, image, video, and audio—to answer your questions.\n\nRetrieval also taps into internal search engines. So, if a developer wants to ask a question about a specific repository, they can index the repository and then send their question to GitHub Copilot Chat on GitHub.com. Retrieval uses our internal search engine to find relevant code or text from the indexed files, which are then used by RAG to prompt the LLM for a contextually relevant response.\n\nStay smart: LLMs can do things they weren’t trained to do, so it’s important to always evaluate and verify their outputs.\n\nRAG and GitHub Copilot Enterprise Powered by RAG, GitHub Copilot Enterprise can help developers and leaders at all levels receive natural language answers to questions about specific repositories. GitHub Copilot can also use content in commits, issues, and discussions to provide contextually relevant responses. In fact, by asking GitHub Copilot questions, developers actually provide GitHub Copilot with more details about the context in which information is being used, which then helps the AI coding tool provide more accurate responses tailored to an organization’s unique codebase. Learn more about the use cases and benefits of GitHub Copilot Enterprise.\n\nUse RAG to unlock insights from unstructured data\n\nAs developers improve their productivity and write more code with AI tools like GitHub Copilot, there’ll be even more unstructured data. Not just in the code itself, but also the information used to build, contextualize, maintain, and improve that code.\n\nThat means even more data containing rich insights that organizations can surface and leverage, or let sink and disappear.\n\nDevelopers and IT leaders can use RAG as a tool to help improve their productivity, produce high-quality and consistent code at greater speed, preserve and share information, and increase their understanding of existing codebases, which can impact reduced onboarding time.\n\nWith a RAG-powered AI tool, developers and IT leaders can quickly discover, analyze, and evaluate a wealth of unstructured data—simply by asking a question.\n\nA RAG reading list 📚", "label": "non_personal"}
{"title": "How we use GitHub to be more productive, collaborative, and secure", "url": "https://github.blog/engineering/how-we-use-github-to-be-more-productive-collaborative-and-secure/", "content": "It’s that time of year where we’re all looking back at what we’ve accomplished and thinking ahead to goals and plans for the calendar year to come. As part of GitHub Universe, I shared some numbers that provided a window into the work our engineering and security teams drive each day on behalf of our community, customers, and Hubbers. As someone who loves data, it’s not just fun to see how we operate GitHub at scale, but it’s also rewarding to see how this work contributes to our vision to be the home for all developers–which includes our own engineering and security teams.\n\nOver the course of the past year, GitHub staff made millions of commits across all of our internal repositories. That’s a ton of branches, pull requests, Issues, and more. We processed billions of API requests daily. And we ran tens of thousands of production deployments across the internal apps that power GitHub’s services. If you do the math, that’s hundreds of deploys per day.\n\nGitHub is big. But the reality is, no matter your size, your scale, or your stage, we’re all dealing with the same questions. Those questions boil down to how to optimize for productivity, collaboration, and, of course, security.\n\nIt’s a running joke internally that you have to type “GitHub” three times to get to the monolith. So, let’s take a look at how we at GitHub (1) use GitHub (2) to build the GitHub (3) you rely on.\n\nProductivity\n\nGitHub’s cloud-powered experiences, namely Codespaces and GitHub Copilot, have been two of the biggest game changers for us in the past few years.\n\nCodespaces\n\nIt’s no secret that local development hasn’t evolved much in the past decade. The github/github repository, where much of what you experience on GitHub.com lives, is fairly large and took several minutes to clone even on a good network connection. Combine this with setting up dependencies and getting your environment the way you like it, spinning up a local environment used to take 45 minutes to go from checkout to a built local developer environment.\n\nBut now, with Codespaces, a few clicks and less than 60 seconds later, you’re in a working development environment that’s running on faster hardware than the MacBook I use daily.\n\nHeating my home office in the chilly Midwest with my laptop doing a local build was nice, but it’s a thing of the past. Moving to Codespaces last year has truly impacted our day-to-day developer experience, and we’re not looking back.\n\nGitHub Copilot\n\nWe’ve been using GitHub Copilot for more than a year internally, and it still feels like magic to me every day. We recently published a study that looked at GitHub Copilot performance across two groups of developers–one that used GitHub Copilot and one that didn’t. To no one’s surprise, the group that used GitHub Copilot was able to complete the same task 55% faster than the group that didn’t have GitHub Copilot.\n\nGetting the job done faster is great, but the data also provided incredible insight into developer satisfaction. Almost three-quarters of the developers surveyed said that GitHub Copilot helped them stay in the flow and spend more time focusing on the fun parts of their jobs. When was the last time you adopted an experience that made you love your job more? It’s an incredible example of putting developers first that has completely changed how we build here at GitHub.\n\nCollaboration\n\nAt GitHub, we’re remote-first and we have highly distributed teams, so we prioritize discoverability and how we keep teams up-to-date across our work. That’s where tools like Issues and projects come into play. They allow us to plan, track, and collaborate in a centralized place that’s right next to the code we’re working on.\n\nIncorporating projects across our security team has made it easier for us to not only track our work, but also to help people understand how their work fits into the company’s broader mission and supports our customers.\n\nProjects gives us a big picture view of our work, but what about the more tactical discovery of a file, function, or new feature another team is building? When you’re working on a massive 15-year-old codebase (looking at you, GitHub), sometimes you need to find code that was written well before you even joined the company, and that can feel like trying to find a needle in a haystack.\n\nSo, we’ve adopted the new code search and code view, which has helped our developers quickly find what they need without losing velocity. This improved discoverability, along with the enhanced organization offered by Issues and projects, has had huge implications for our teams in terms of how we’ve been able to collaborate across groups.\n\nShifting security left\n\nLike we saw when we looked at local development environments, the security industry still struggles with the same issues that have plagued us for more than a decade. Exposed credentials, as an example, are still the root cause for more than half of all data breaches today. Phishing is still the best, and cheapest, way for an adversary to get into organizations and wreak havoc. And we’re still pleading with organizations to implement multi-factor authentication to keep the most basic techniques from bad actors at bay.\n\nIt’s time to build security into everything we do across the developer lifecycle.\n\nThe software supply chain starts with the developer. Normalizing the use of strong authentication is one of the most important ways that we at GitHub, the home of open source, can help defend the entire ecosystem against supply chain attacks. We enforce multi-factor authentication with security keys for our internal developers, and we’re requiring that every developer who contributes software on GitHub.com enable 2FA by the end of next year. The closer we can bring our security and engineering teams together, the better the outcomes and security experiences we can create together.\n\nAnother way we do that is by scaling the knowledge of our security teams with tools like CodeQL to create checks that are deployed for all our developers, protecting all our users. And because the CodeQL queries are open source, the vulnerability patterns shared by security teams at GitHub or by our customers end up as CodeQL queries that are then available for everyone. This acts like a global force multiplier for security knowledge in the developer and security communities.\n\nSecurity shouldn’t be gatekeeping your teams from shipping. It should be the process that enables them to ship quickly–remember our hundreds of production deployments per day?–and with confidence.\n\nBig, small, or in-between\n\nAs you see, GitHub has the same priorities as any other development team out there.\n\nIt doesn’t matter if you’re processing billions of API requests a day, like we are, or if you’re just starting on that next idea that will be launched into the world.\n\nThese are just a few ways over the course of the last year that we’ve used GitHub to build our own platform securely and improve our own developer experiences, not only to be more productive, collaborative, and secure, but to be creative, to be happier, and to build the best work of our lives.\n\nTo learn more about how we use GitHub to build GitHub, and to see demos of the features highlighted here, take a look at this talk from GitHub Universe 2022.\n\nNotes\n\nTags:", "label": "non_personal"}
{"title": "Meta Open Source: 2024 by the numbers", "url": "https://engineering.fb.com/2025/04/02/open-source/meta-open-source-by-the-numbers/", "content": "Open source has played an essential role in the tech industry and beyond. Whether in the AI/ML, web, or mobile space, our open source community grew and evolved while connecting people worldwide.\n\nAt Meta Open Source, 2024 was a year of growth and transformation. Our open source initiatives addressed the evolving needs and challenges of developers—powering breakthroughs in AI and enabling the creation of innovative, user-focused applications and experiences. In close collaboration with the open source community, we shared knowledge, introduced new projects, and enhanced existing ones.\n\nIn this post, we look at our portfolio of open source projects through numbers to give a better view of the scale of the community we interact with daily.\n\nAt Meta, we have several GitHub organizations where we publish new open source projects, maintain existing ones, and hold already archived projects. They include various tools, frameworks, and platforms for web, mobile, AI/ML, and hardware industries.\n\nBy the end of last year, we launched 256 brand-new repositories, bringing active public projects to 944. This number excludes archived repositories and projects that we moved to foundations.\n\nIn 2024, our open source codebases grew at an impressive pace, reaching 189,719 total commits in just one year. Community contributors accounted for 71,018, while Meta employees made the remaining 118,701.\n\nOpen source cannot exist without people collaborating, sharing, and innovating. A total of 4,274 external contributors helped bring our community to 7,144 strong. This remarkable community is what fuels the ongoing evolution of Meta Open Source.\n\nBeyond individual contributions, our projects on GitHub accumulated an additional 151,380 stars, bringing the total to a staggering 1.8 million. This growth in engagement shows strong interest and excitement for Meta Open Source projects.\n\nThank you to the open source community\n\nAt Meta, we believe open source accelerates the pace of innovation in the world. By sharing our technologies, we aim to move the industry forward while allowing other companies and individuals to use our solutions to scale more quickly and build great products.\n\nAt the same time, Meta Open Source projects are made possible by contributions from developers like you. Pull requests, documentation updates, social media posts, and everything in between are what build connections in our communities. Thank you all for another great year for open source.\n\nTo learn more about Meta Open Source, visit our open source site, subscribe to our YouTube channel, or follow us on Facebook, Threads, X, and LinkedIn.", "label": "non_personal"}
{"title": "How Meta understands data at scale", "url": "https://engineering.fb.com/2025/04/28/security/how-meta-understands-data-at-scale/", "content": "Managing and understanding large-scale data ecosystems is a significant challenge for many organizations, requiring innovative solutions to efficiently safeguard user data. Meta’s vast and diverse systems make it particularly challenging to comprehend its structure, meaning, and context at scale.\n\nTo address these challenges, we made substantial investments in advanced data understanding technologies, as part of our Privacy Aware Infrastructure (PAI) . Specifically, we have adopted a “shift-left” approach, integrating data schematization and annotations early in the product development process. We also created a universal privacy taxonomy , a standardized framework providing a common semantic vocabulary for data privacy management across Meta’s products that ensures quality data understanding and provides developers with reusable and efficient compliance tooling.\n\nWe discovered that a flexible and incremental approach was necessary to onboard the wide variety of systems and languages used in building Meta’s products. Additionally, continuous collaboration between privacy and product teams was essential to unlock the value of data understanding at scale.\n\nWe embarked on the journey of understanding data across Meta a decade ago with millions of assets in scope ranging from structured and unstructured, processed by millions of flows across many of the Meta App offerings. Over the past 10 years, Meta has cataloged millions of data assets and is classifying them daily, supporting numerous privacy initiatives across our product groups. Additionally, our continuous understanding approach ensures that privacy considerations are embedded at every stage of product development.\n\nAt Meta, we have a deep responsibility to protect the privacy of our community. We’re upholding that by investing our vast engineering capabilities into building cutting-edge privacy technology. We believe that privacy drives product innovation. This led us to develop our Privacy Aware Infrastructure (PAI), which integrates efficient and reliable privacy tools into Meta’s systems to address needs such as purpose limitation—restricting how data can be used while also unlocking opportunities for product innovation by ensuring transparency in data flows\n\nData understanding is an early step in PAI. It involves capturing the structure and meaning of data assets, such as tables, logs, and AI models. Over the past decade, we have gained a deeper understanding of our data, by embedding privacy considerations into every stage of product development, ensuring a more secure and responsible approach to data management.\n\nWe embarked on our data understanding journey by employing heuristics and classifiers to automatically detect semantic types from user-generated content. This approach has evolved significantly over the years, enabling us to scale to millions of assets. However, conducting these processes outside of developer workflows presented challenges in terms of accuracy and timeliness. Delayed classifications often led to confusion and unnecessary work, while the results were difficult to consume and interpret.\n\nData understanding at Meta using PAI\n\nTo address shortcomings, we invested in data understanding by capturing asset structure (schematization), describing meaning (annotation), and inventorying it into OneCatalog (Meta’s system that discovers, registers, and enumerates all data assets) across all Meta technologies. We developed tools and APIs for developers to organize assets, classify data, and auto-generate annotation code. Despite significant investment, the journey was not without challenges, requiring innovative solutions and collaboration across the organization.\n\nChallenge Approach Understanding at scale (lack of foundation) At Meta, we manage hundreds of data systems and millions of assets across our family of apps. Each product features its own distinct data model, physical schema, query language, and access patterns. This diversity created a unique hurdle for offline assets: the inability to reuse schemas due to the limitations of physical table schemas in adapting to changing definitions. Specifically, renaming columns or making other modifications had far-reaching downstream implications, rendering schema evolution challenging, thus propagation required careful coordination to ensure consistency and accuracy across multiple systems and assets. We introduced a shared asset schema format as a logical representation of the asset schema that can be translated back and forth with the system-specific format. Additionally, it offers tools to automatically classify data and send out annotation changes to asset owners for review , effectively managing long-tail systems. Inconsistent definitions (lack of shared understanding) We encountered difficulties with diverse data systems that store data in various formats, and customized data labels that made it challenging to recognize identical data elements when they are stored across multiple systems. We introduced a unified taxonomy of semantic types , which are compiled into different languages. This ensured that all systems can share the same canonical set of labels. Missing annotations (lack of quality) A solution that relied solely on data scanning and pattern matching was prone to false positives due to limited contextual information. For instance, a 64-bit integer could be misclassified as either a timestamp or a user identifier without additional context. Moreover, manual human labeling is not feasible at scale because it relies heavily on individual developers’ expertise and knowledge. We shifted left by combining schematization together with annotations in code , in addition improving and utilizing multiple classification signals . Strict measurements provided precision/recall guarantees. Protection was embedded in everything we built, without requiring every developer to be a privacy expert. Organizational barriers (lack of a unified approach) Meta’s data systems, with their bespoke schematization and practices, posed significant challenges in understanding data across the company. As we navigated complex interactions and with ever evolving privacy requirements, it became clear that fragmented approaches to data understanding hindered our ability to grasp data comprehensively. By collaborating with asset owners to develop intuitive tooling and improve coverage, we tackled adoption barriers such as poor developer experience and inaccurate classification. This effort laid the groundwork for a unified data understanding foundation, which was seamlessly integrated into the developer workflow. As a result, we drove a cultural shift towards reusable and efficient privacy practices, ultimately delivering value to product teams and fostering a more cohesive approach to data management.\n\nWalkthrough : Understanding user data for the “Beliefs” feature in Facebook Dating\n\nTo illustrate our approach and dive into the technical solution, let’s consider a scenario involving structured user data. When creating a profile on the Facebook Dating app, users have the option to include their religious views to help match with others who share similar values.\n\nOn Facebook Dating, religious views are subject to purpose limitation requirements. Our five-step approach to data understanding provides a precise, end-to-end view of how we track and protect sensitive data assets, including those related to religious views:\n\nEven a simple feature can involve data being processed by dozens of heterogenous systems, making end-to-end data protection critical. To ensure comprehensive protection, it is essential to apply the necessary steps to all systems that store or process data, including distributed systems (web systems, chat, mobile and backend services) and data warehouses.\n\nConsider the data flow from online systems to the data warehouse, as shown in the diagram below. To ensure that religious belief data is identified across all these systems, we have implemented measures to prevent its use for any purpose other than the stated one.\n\nStep 1 – Schematizing\n\nAs part of the PAI initiative, Meta developed DataSchema, a standard format that is used to capture the structure and relationships of all data assets, independent of system implementation. Creating a canonical representation for compliance tools. Understanding DataSchema requires grasping schematization, which defines the logical structure and relationships of data assets, specifying field names, types, metadata, and policies.\n\nImplemented using the Thrift Interface Description Language, DataSchema is compatible with Meta systems and languages. It describes over 100 million schemas across more than 100 data systems, covering granular data units like database tables, key-value stores, data streams from distributed systems (such as those used for logging), processing pipelines, and AI models. Essentially, a data asset is like a class with annotated attributes.\n\nLet’s examine the source of truth (SoT) for a user’s dating profile schema, modeled in DataSchema. This schema includes the names and types of fields and subfields:\n\n- user_id (uint) - name (string) - age (uint) - religious_views (enum) - photos (array<struct>): - url (url) - photo (blob) - caption (string) - uploaded_date (timestamp) Dating profile DataSchema\n\nThe canonical SoT schema serves as the foundation for all downstream representations of the dating profile data. In practice, this schema is often translated into system-specific schemas (source of record – “SoR”), optimized for developer experience and system implementation in each environment.\n\nStep 2 – Predicting metadata at scale\n\nBuilding on this schematization foundation, we used annotations to describe data, enabling us to quickly and reliably locate user data, such as religious beliefs, across Meta’s vast data landscape. This is achieved through a universal privacy taxonomy, a framework that provides a common semantic vocabulary for data privacy management across Meta’s apps. It offers a consistent language for data description and understanding, independent of specific programming languages or technologies.\n\nThe universal privacy taxonomy works alongside data classification, which scans systems across Meta’s product family to ensure compliance with privacy policies. These systems use taxonomy labels to identify and classify data elements, ensuring privacy commitments are met and data is handled appropriately according to its classification.\n\nPrivacy annotations are represented by taxonomy facets and their values. For example, an asset might pertain to an Actor.Employee, with data classified as SemanticType.Email and originating from DataOrigin.onsite, not a third party. The SemanticType annotation is our standard facet for describing the meaning, interpretation, or context of data, such as user names, email addresses, phone numbers, dates, or locations.\n\nBelow, we illustrate the semantic type taxonomy node for our scenario, Faith Spirituality:\n\nAs data models and collected data evolve, annotations can become outdated or incorrect. Moreover, new assets may lack annotations altogether. To address this, PAI utilizes various techniques to continuously verify our understanding of data elements and maintain accurate, up-to-date annotations:\n\nOur classification system leverages machine learning models and heuristics to predict data types by sampling data, extracting features, and inferring annotation values. Efficient data sampling, such as Bernoulli sampling, and processing techniques enable scaling to billions of data elements with low-latency classifications.\n\nKey components include:\n\nScheduling component : manages the set of data assets to scan, accommodating different data system architectures by either pulling data via APIs or receiving data pushed directly into the scanning service.\n\nScanning service : processes and analyzes data from various sources by accumulating samples in memory, deserializing rows (e.g., JSON) into fields and sub-fields, and extracting features using APIs available in multiple languages (C++, Python, Hack). It ensures comprehensive data capture, even for ephemeral data.\n\nClassification service : utilizes heuristic rules and machine learning models to classify data types with high accuracy. Heuristic rules : handle straightforward, deterministic cases by identifying specific data formats like dates, phone numbers, and user IDs. Machine learning models : trained on labeled datasets using supervised learning and improved through unsupervised learning to identify patterns and anomalies in unlabeled data. Ground truth calibration and verification : ensures system accuracy and reliability, allowing for model fine-tuning and improved classification performance.\n\nLineage and propagation: We integrate classification rules with high-confidence lineage signals to ensure accurate data tracking and management. Our propagation mechanism enables the seamless annotation of data as needed, ensuring that exact copies of data across systems receive equivalent classification. This approach not only maintains data integrity but also optimizes the developer experience by streamlining the process of managing data classifications across our diverse systems.\n\nStep 3 – Annotating\n\nThe integration of metadata predictions and developer input creates a comprehensive picture of a data asset’s structure (schema) and its meaning (annotation). This is achieved by attaching these elements to individual fields in data assets, providing a thorough understanding of the data.\n\nBuilding on the predicting data at scale initiative (step 2), where we utilize the universal privacy taxonomy and classification systems to identify and classify data elements, the generated metadata predictions are then used to help developers annotate their data assets efficiently and correctly.\n\nPortable annotation APIs: seamlessly integrate into developer workflows ensuring:\n\nConsistent representation of data across all systems at Meta.\n\nAccurate understanding of data, enabling the application of privacy safeguards at scale.\n\nEfficient evidencing of compliance with regulatory requirements.\n\nMetadata predictions and developer input: Two key components work together to create a comprehensive data asset picture:\n\nMetadata predictions : Classifiers generate predictions to aid developers in annotating data assets efficiently and correctly. If the confidence score exceeds a certain threshold, assignment can be automated, saving developer time.\n\nDeveloper input : Developers manually refine and verify annotations, ensuring that the data’s context and privacy requirements are accurately captured. Human oversight guarantees the accuracy and reliability of the data asset picture.\n\n- user_id (enum) → SemanticType::id_userID - name (string) → SemanticType::identity_name - age (uint) → SemanticType::age - religious_views (enum) → SemanticType::faithSpirituality - photos (array<struct>): - url (url) → SemanticType::electronicID_uri_mediaURI_imageURL - photo (blob) → SemanticType::media_image - caption (string) → SemanticType::media_text_naturalLanguageText - uploaded_date (timestamp) → SemanticType::uploadedTime\n\nEnsuring complete schemas with annotations: To maintain a high standard of data understanding, we have integrated data understanding into our data model lifecycle. This includes auto-generating code to represent the schema of newly created assets when missing, ensuring that no new assets are created without a proper schema.\n\nFor example, in the context of our religious beliefs in Facebook Dating, we have defined its structure, including fields like ‘Name,’ ‘EmailAddress,’ and ‘Religion.’ Furthermore, we have annotated the asset with Actor::user(), signifying that the data pertains to a user of our products. This level of detail enables us to readily identify fields containing privacy-related data and implement appropriate protective measures, such as applying the applicable purpose limitation policy.\n\nIn the case of the “dating profile” data asset, we have defined its structure, including fields like ‘Name’:\n\nfinal class DatingProfileSchema extends DataSchemaDefinition { <<__Override>> public function configure(ISchemaConfig $config): void { $config->metadataConfig()->description('Represents a dating profile); $config->annotationsConfig()->annotations(Actor::user()); } <<__Override>> public function getFields(): dict<string, ISchemaField> { return dict[ 'Name' => StringField::create(\"name\") ->annotations(SemanticType::identity_name()) ->example('John Doe'), 'Age' => StringInt::create('age') ->description(“The age of the user.”) ->annotations(SemanticType::age()) ->example('24'), 'ReligiousViews' => EnumStringField::create('religious_views') ->annotations(SemanticType::faithSpirituality()) ->example('Atheist'), ]; } }\n\nIn order to optimize for developer experience, the details of the schema representation differ in each environment. For example, in the data warehouse, it’s represented as a Dataset – an in-code Python class capturing the asset’s schema and metadata. Datasets provide a native API for creating data pipelines.\n\nHere is an example of such a schema:\n\n​​@hive_dataset( \"dim_all_dating_users\", // table name \"dating\", // namespace oncall=\"dating_analytics\", description=\"This is the primary Dating user dimension table containing one row per Dating user per day along with their profile, visitation, and key usage information.\", metadata=Metadata(Actor.User), ) class dim_all_dating_users(DataSet): ds: Varchar = Partition(\"datestamp\") userid: DatingUserID = Column(\"User id of the profile\") email: EmailAddress = Column(\"User's email address\"), age: PersonAge = Column(\"User's stated age on date ds\") religious_views: ReligionOptions = Column(\"User's provided religious views\")\n\nOur warehouse schema incorporates rich types, a privacy-aware type system designed to enhance data understanding and facilitate effective data protection. Rich types, such as DatingUserID, EmailAddress, PersonAge, and ReligionOptions, are integrated into the schema, offering a comprehensive approach to data management while encoding privacy metadata. They provide a developer-friendly way to annotate data and enable the enforcement of data quality rules and constraints at the type level, ensuring data consistency and accuracy across the warehouse. For instance, they can detect issues like joining columns with different types of user IDs or mismatched enums before code execution.\n\nHere is an example definition:\n\nReligionOptions = enum_from_items( \"ReligionOptions\", items=[ EnumItem(\"Atheist\", \"Atheist\"), EnumItem(\"Buddhist\", \"Buddhist\"), EnumItem(\"Christian\", \"Christian\"), EnumItem(\"Hindu\", \"Hindu\"), EnumItem(\"Jewish\", \"Jewish\"), EnumItem(\"Muslim\", \"Muslim\"), ... ], annotations=(SemanticType.faithSpirituality,), )\n\nStep 4 – Inventorying assets and systems\n\nA central inventory system is crucial for managing data assets and their metadata, offering capabilities like search and compliance tracking. Meta’s OneCatalog is a comprehensive system that discovers, registers, and enumerates all data assets across Meta’s apps, providing inventory for easier management and tracking.\n\nKey functions of OneCatalog:\n\nRegistering all data systems : OneCatalog defines a data system as a logical abstraction over resources that persist data for a common purpose. It exhaustively examines resources across Meta’s environments to discover and register all data systems hosting data assets.\n\nEnumerating all data assets : Eligible data systems must enumerate their assets through the asset enumeration platform, generating a comprehensive list of assets and their metadata in the central inventory. These assets are grouped by “asset classes” based on shared patterns, enabling efficient management and understanding of data assets.\n\nGuarantees provided by OneCatalog:\n\nCompleteness: The system regularly checks for consistency between the data defined in its configuration and the actual data stored in the inventory. This ongoing comparison ensures that all relevant data assets are accurately accounted for and up-to-date.\n\nFreshness: In addition to regularly scheduled pull-based enumeration, the system subscribes to changes in data systems and updates its inventory in real time.\n\nUniqueness of asset ID (XID): Each asset is assigned a globally unique identifier, similar to URLs, which facilitates coordination between multiple systems and the exchange of information about assets by providing a shared key. The globally unique identifier follows a human-readable structure, e.g., asset://[asset-class]/[asset-name].\n\nUnified UI: On top of the inventory, OneCatalog provides a unified user interface that consolidates all asset metadata, serving as the central hub for asset information. This interface offers a single point of access to view and manage assets, streamlining the process of finding and understanding data.\n\nFor example, in the context of our “religious beliefs in the Dating app” scenario, we can use OneCatalog’s unified user interface to view the warehouse dating profile table asset, providing a comprehensive overview of its metadata and relationships.\n\nCompliance and privacy assurance: OneCatalog’s central inventory is utilized by various privacy teams across Meta to ensure that data assets meet requirements. With its completeness and freshness guarantees, OneCatalog serves as a reliable source of truth for privacy and compliance efforts.\n\nBy providing a single view of all data assets, OneCatalog enables teams to efficiently identify and address potential risks or vulnerabilities, such as unsecured data or unauthorized access.\n\nStep 5 – Maintaining data understanding\n\nTo maintain high coverage and quality of schemas and annotations across Meta’s diverse apps, we employed a robust process that involves measuring precision and recall for both predicted metadata and developer-provided annotations. This enables us to guide the implementation of our privacy and security controls and ensure their effectiveness.\n\nBy leveraging data understanding, tooling can quickly build end-to-end compliance solutions. With schema and annotations now front and center, we’ve achieved continuous understanding, enabling our engineers to easily track and protect user data, implement various security and privacy controls, and build new features at scale.\n\nOur strategy for maintaining data understanding over time includes:\n\nShifting left on creation time : We provided intuitive APIs for developers to provide metadata at asset creation time, ensuring that schemas and annotations were applied consistently in downstream use cases.\n\nDetecting and fixing annotation gaps : We surfaced prediction signals to detect coverage and quality gaps and evolved our prediction and annotation capabilities to ensure new systems and workflows were covered.\n\nCollecting ground truth : We established a baseline to measure automated systems against, with the help of subject matter experts, to continuously measure and improve them.\n\nProviding canonical consumption APIs : We developed canonical APIs for common compliance usage patterns, such as detecting user data, to ensure consistent interpretation of metadata and low entry barriers.\n\nPutting it all together\n\nComing back to our scenario: As developers on the Facebook Dating team collect or generate new data, they utilize familiar APIs that help them schematize and annotate their data. These APIs provide a consistent and intuitive way to define the structure and meaning of the data.\n\nWhen collecting data related to “Faith Spirituality,”the developers use a data classifier that confirms their semantic type annotations once the data is scanned during testing. This ensures that the data is accurately labeled and can be properly handled by downstream systems.\n\nTo ensure the quality of the classification system, ground truth created by subject matter experts is used to measure its accuracy. A feedback loop between the product and PAI teams keeps the unified taxonomy updated, ensuring that it remains relevant and effective.\n\nBy using canonical and catalogued metadata, teams across Meta can implement privacy controls that are consistent and effective. This enables the company to maintain user trust and meet requirements.\n\nIn this scenario, the developers on the Facebook Dating team are:\n\nSchematizing and annotating their data using familiar APIs.\n\nUsing a data classifier to confirm semantic type annotations.\n\nLeveraging ground truth to measure the quality of the classification system.\n\nUtilizing a feedback loop to keep the unified taxonomy updated.\n\nImplementing privacy controls using canonical and catalogued metadata.\n\nLearnings and takeaways\n\nBuilding an understanding of all data at Meta was a monumental effort that not only required novel infrastructure but also the contribution of thousands of engineers across all teams at Meta, and years of investment.\n\nCanonical everything : Data understanding at scale relies on a canonical catalog of systems, asset classes, assets, and taxonomy labels, each with globally unique identifiers. This foundation enables an ecosystem of compliance tooling, separating the concerns of data understanding from consuming canonical metadata.\n\nIncremental and flexible approach : To tackle the challenge of onboarding hundreds of systems across Meta, we developed a platform that supports pulling schemas from existing implementations. We layered solutions to enhance existing untyped APIs , meeting developers where they are—whether in code, configuration, or a UI defining their use case and data model. This incremental and flexible approach delivers value at every step.\n\nCollaborating for data classification excellence : Building the platform was just the beginning. The infrastructure and privacy teams also collaborated with subject matter experts to develop best-in-class classifiers for our data, addressing some of the most challenging problems. These include detecting user-generated content, classifying data embedded in blobs, and creating a governed taxonomy that allows every developer to describe their data with the right level of detail.\n\nCommunity engagement with a tight feedback loop : Our success in backfilling schemas and integrating with the developer experience was made possible by a strong partnership with product teams. By co-building solutions and establishing an immediate feedback loop, we refined our approach, addressed misclassifications, and improved classification quality. This collaboration is crucial to our continued evolution and refinement of data understanding.\n\nThe future of data understanding\n\nData understanding has become a crucial component of Meta’s PAI initiative, enabling us to protect user data in a sustainable and effective manner. By creating a comprehensive understanding of our data, we can address privacy challenges durably and more efficiently than traditional methods.\n\nOur approach to data understanding aligns closely with the developer workflow, involving the creation of typed data models, collection of annotated data, and processing under relevant policies. At Meta’s scale, this approach has saved significant engineering effort by automating annotation on millions of assets (i.e., fields, columns, tables) with specific labels from an inventory that are deemed commitment-critical. This automation has greatly reduced the manual effort required for annotation, allowing teams to focus on higher-priority tasks.\n\nAs data understanding continues to evolve, it is expected to have a significant impact on various aspects of operations and product offerings. Here are some potential future use cases:\n\nImproved AI and machine learning : leveraging data understanding to improve the accuracy of AI-powered content moderation and recommendation systems.\n\nStreamlined developer workflows : integrating data understanding into Meta’s internal development tools to provide clear data context and reduce confusion.\n\nOperational and developer efficiency : By automating data classification and annotation for millions of assets across Meta’s platforms, we can significantly improve operational efficiency. This automation enables us to leverage metadata for various use cases, such as accelerating product innovation. For instance, we’re now utilizing this metadata to help developers efficiently find the right data assets, streamlining their workflow and reducing the time spent on manual searches.\n\nProduct innovation : With a comprehensive understanding of data, Meta can drive product innovation by leveraging insights to create personalized and engaging user experiences.\n\nWhile there is still more work to be done, such as evolving taxonomies to meet future compliance needs and developing novel ways to schematize data, we are excited about the potential of data understanding. By harnessing canonical metadata, we can deepen our shared understanding of data, unlocking unprecedented opportunities for innovation not only at Meta, but across the industry.\n\nAcknowledgements\n\nThe authors would like to acknowledge the contributions of many current and former Meta employees who have played a crucial role in developing data understanding over the years. In particular, we would like to extend special thanks to (in alphabetical order) Aaron Morris, Adrian Zgorzalek, Alex Gorelik, Alex Kalinin, Alex Uslontsev, Ali Fakeri Tabrizi, Amit Sarkar, Anchit Arora, Andras Belokosztolszki, Anthony O’Sullivan, Archit Jain, Aygun Aydin, Ayoade Adeniyi, Ben Warren, Bob Baldwin, Brani Stojkovic, Brian Romanko, Can Lin, Carrie (Danning) Jiang, Chao Yang, Chris Ventura, Daniel Ohayon, Danny Gagne, David Taieb, Dmitry Ponomarev, Dong Jia, Dong Zhao, Eero Neuenschwander, Fang Wang, Ferhat Sahinkaya, Ferdi Adeputra, Fred Liu, Gayathri Aiyer, George Stasa, Guoqiang Jerry Chen, Haiyang Han, Haydar Imren, Henry Swanson, Ian Carmichael, Jared Greene, Jerry Pan, Jiang Wu, Johnnie Ballentyne, Joanna Jiang, Jonathan Bergeron, Joseph Li, Jun Fang, Kaustubh Karkare, Komal Mangtani, Kuldeep Chaudhary, Kunal Kataria, Lea Li, Lei Zhang, Liu Yang, Loka Potnuru, Luiz Ribeiro, Marc Celani, Matthieu Martin, Max Mazzeo, Meg Dymek, Mellany Flores, Mike Tarasyuk, Mital Mehta, Nevzat Sevim, Nick Gardner, Nikolay Kondratyev, Oliver Dodd, Pankaj Landge, Perry Stoll, Peter Nieuwenhuizen, Pranet Verma, Prashanth Bandaru, Piyush Khemka, Rahul Nambiar, Rajesh Nishtala, Rituraj Kirti, Roger (Wei) Li, Rujin Cao, Sahil Garg, Satish Sampath, Sean Wang, Seth Silverman, Shridhar Iyer, Simran Patil, Sriguru Chakravarthi, Sushaant Mujoo, Susmit Biswas, Taha Bekir Eren, Tejas Kudrimoti, Tony Harper, Vineet Chaudhary, Vishal Jain, Vitali Haravy, Vlad Fedorov, Vlad Gorelik, Wolfram Schuttle, Xiaotian Guo, Yatu Zhang, Yi Huang, Yuxi Zhang, Zejun Zhang, and Zhaohui Zhang. We would also like to express our gratitude to all reviewers of this post, including (in alphabetical order) Aleksandar Ilic, Avtar Brar, Brianna O’Steen, Chloe Lu, Chris Wiltz, Imogen Barnes, Jason Hendrickson, Rituraj Kirti, Xenia Habekoss and Yuri Claure. We would like to especially thank Jonathan Bergeron for overseeing the effort and providing all of the guidance and valuable feedback, and Ramnath Krishna Prasad for pulling required support together to make this blog post happen.", "label": "non_personal"}
{"title": "Building Private Processing for AI tools on WhatsApp", "url": "https://engineering.fb.com/2025/04/29/security/whatsapp-private-processing-ai-tools/", "content": "We are inspired by the possibilities of AI to help people be more creative, productive, and stay closely connected on WhatsApp, so we set out to build a new technology that allows our users around the world to use AI in a privacy-preserving way.\n\nWe’re sharing an early look into Private Processing, an optional capability that enables users to initiate a request to a confidential and secure environment and use AI for processing messages where no one — including Meta and WhatsApp — can access them.\n\nTo validate our implementation of these and other security principles, independent security researchers will be able to continuously verify our privacy and security architecture and its integrity.\n\nAI has revolutionized the way people interact with technology and information, making it possible for people to automate complex tasks and gain valuable insights from vast amounts of data. However, the current state of AI processing — which relies on large language models often running on servers, rather than mobile hardware — requires that users’ requests are visible to the provider. Although that works for many use cases, it presents challenges in enabling people to use AI to process private messages while preserving the level of privacy afforded by end-to-end encryption.\n\nWe set out to enable AI capabilities with the privacy that people have come to expect from WhatsApp, so that AI can deliver helpful capabilities, such as summarizing messages, without Meta or WhatsApp having access to them, and in the way that meets the following principles:\n\nOptionality: Using Meta AI through WhatsApp, including features that use Private Processing, must be optional.\n\nTransparency: We must provide transparency when our features use Private Processing.\n\nUser control: For people’s most sensitive chats that require extra assurance, they must be able to prevent messages from being used for AI features like mentioning Meta AI in chats, with the help of WhatApp’s Advanced Chat Privacy feature.\n\nIntroducing Private Processing\n\nWe’re excited to share an initial overview of Private Processing, a new technology we’ve built to support people’s needs and aspirations to leverage AI in a secure and privacy-preserving way. This confidential computing infrastructure, built on top of a Trusted Execution Environment (TEE), will make it possible for people to direct AI to process their requests — like summarizing unread WhatsApp threads or getting writing suggestions — in our secure and private cloud environment. In other words, Private Processing will allow users to leverage powerful AI features, while preserving WhatsApp’s core privacy promise, ensuring no one except you and the people you’re talking to can access or share your personal messages, not even Meta or WhatsApp.\n\nTo uphold this level of privacy and security, we designed Private Processing with the following foundational requirements:\n\nConfidential processing: Private Processing must be built in such a way that prevents any other system from accessing user’s data — including Meta, WhatsApp or any third party — while in processing or in transit to Private Processing.\n\nEnforceable guarantees: Attempts to modify that confidential processing guarantee must cause the system to fail closed or become publicly discoverable via verifiable transparency.\n\nVerifiable transparency: Users and security researchers must be able to audit the behavior of Private Processing to independently verify our privacy and security guarantees.\n\nHowever, we know that technology platforms like ours operate in a highly adversarial environment where threat actors continuously adapt, and software and hardware systems keep evolving, generating unknown risks. As part of our defense-in-depth approach and best practices for any security-critical system, we’re treating the following additional layers of requirements as core to Private Processing on WhatsApp:\n\nNon-targetability: An attacker should not be able to target a particular user for compromise without attempting to compromise the entire Private Processing system.\n\nStateless processing and forward security: Private Processing must not retain access to user messages once the session is complete to ensure that the attacker can not gain access to historical requests or responses.\n\nThreat modeling for Private Processing\n\nBecause we set out to meet these high-security requirements, our work to build Private Processing began with developing a threat model to help us identify potential attack vectors and vulnerabilities that could compromise the confidentiality, integrity, or availability of user data. We’ve worked with our peers in the security community to audit the architecture and our implementation to help us continue to harden them.\n\nBuilding in the open\n\nTo help inform our industry’s progress in building private AI processing, and to enable independent security research in this area, we will be publishing components of Private Processing, expanding the scope of our Bug Bounty program to include Private Processing, and releasing a detailed security engineering design paper, as we get closer to the launch of Private Processing in the coming weeks.\n\nWhile AI-enabled processing of personal messages for summarization and writing suggestions at users’ direction is the first use case where Meta applies Private Processing, we expect there will be others where the same or similar infrastructure might be beneficial in processing user requests. We will continue to share our learnings and progress transparently and responsibly.\n\nHow Private Processing works\n\nPrivate Processing creates a secure cloud environment where AI models can analyze and process data without exposing it to unauthorized parties.\n\nHere’s how it works:\n\nAuthentication: First, Private Processing obtains anonymous credentials to verify that the future requests are coming from authentic WhatsApp clients.\n\nThird-party routing and load balancing: In addition to these credentials, Private Processing fetches HPKE encryption public keys from a third-party CDN in order to support Oblivious HTTP (OHTTP).\n\nWire session establishment: Private Processing establishes an OHTTP connection from the user’s device to a Meta gateway via a third-party relay which hides requester IP from Meta and WhatsApp.\n\nApplication session establishment: Private Processing establishes a Remote Attestation + Transport Layer Security (RA-TLS) session between the user’s device and the TEE. The attestation verification step cross-checks the measurements against a third-party ledger to ensure that the client only connects to code which satisfies our verifiable transparency guarantee.\n\nRequest to Private Processing: After the above session is established, the device makes a request to Private Processing (e.g., message summarization request), that is encrypted end-to-end between the device and Private Processing with an ephemeral key that Meta and WhatsApp cannot access. In other words, no one except the user’s device or the selected TEEs can decrypt the request.\n\nPrivate Processing: Our AI models process data in a confidential virtual machine (CVM), a type of TEE, without storing any messages, in order to generate a response. CVMs may communicate with other CVMs using the same RA-TLS connection clients use to complete processing.\n\nResponse from Private Processing: The processed results are then returned to the user’s device, encrypted with a key that only the device and the pre-selected Private Processing server ever have access to. Private Processing does not retain access to messages after the session is completed.\n\nThe threat model\n\nIn designing any security-critical system, it is important to develop a threat model to guide how we build its defenses. Our threat model for Private Processing includes three key components:\n\nAssets : The sensitive data and systems that we need to protect.\n\nThreat actors : The individuals or groups that may attempt to compromise our assets.\n\nThreat scenarios : The ways in which our assets could be compromised, including the tactics, techniques, and procedures (TTPs) that threat actors might use.\n\nAssets\n\nIn the context of applying Private Processing to summarizing unread messages or providing writing suggestions at users’ direction, we will use Private Processing to protect messaging content, whether they have been received by the user, or still in draft form. We use the term “messages” to refer to these primary assets in the context of this blog.\n\nIn addition to messages, we also include additional, secondary assets which help support the goal of Private Processing and may interact with or directly process assets: the Trusted Computing Base (TCB) of the Confidential Virtual Machine (CVM), the underlying hardware, and the cryptographic keys used to protect data in transit.\n\nThreat actors\n\nWe have identified three threat actor types that could attack our system to attempt to recover assets.\n\nMalicious or compromised insiders with access to our infrastructure. A third party or supply chain vendor with access to components of the infrastructure. Malicious end users targeting other users on the platform.\n\nThreat scenarios\n\nWhen building Private Processing to be resilient against these threat actors, we consider relevant threat scenarios that may be pursued against our systems, including (but not limited to) the following:\n\nExternal actors directly exploit the exposed product attack surface or compromise the services running in Private Processing CVMs to extract messages.\n\nAnywhere the system processes untrusted data, there is potentially an attack surface for a threat actor to exploit. Examples of these kinds of attacks include exploitation of zero-day vulnerabilities or attacks unique to AI such as prompt injection.\n\nPrivate Processing is designed to reduce such an attack surface through limiting the exposed entry points to a small set of thoroughly reviewed components which are subject to regular assurance testing. The service binaries are hardened and run in a containerized environment to mitigate the risks of code execution and limit a compromised binary’s ability to exfiltrate data from within the CVM to an external party.\n\nInternal or external attackers extract messages exposed through the CVM.\n\nObservability and debuggability remains a challenge in highly secure environments as they can be at odds with the goal of confidential computing, potentially exposing side channels to identify data and in the worst case accidentally leaking messages themselves. However, deploying any service at scale requires some level of observability to identify failure modes, since they may negatively impact many users, even when the frequency is uncommon. We implement a log-filtering system to limit export to only allowed log lines, such as error logs.\n\nLike any complex system, Private Processing is built of components to form a complex supply chain of both hardware and software. Internally, our CVM build process occurs in restricted environments that maintain provenance and require multi-party review. Transparency of the CVM environment, which we’ll provide through publishing a third-party log of CVM binary digests and CVM binary images, will allow external researchers to analyze, replicate, and report instances where they believe logs could leak user data.\n\nInsiders with physical or remote access to Private Processing hosts interfere with the CVM at boot and runtime, potentially bypassing the protections in order to extract messages.\n\nTEE software exploitation is a growing area of security research, and vulnerability researchers have repeatedly demonstrated the ability to bypass TEE guarantees. Similarly, physical attacks on Private Processing hosts may be used to defeat TEE guarantees or present compromised hosts as legitimate to an end user.\n\nTo address these unknown risks, we built Private Processing on the principle of defense-in-depth by actively tracking novel vulnerabilities in this space, minimizing and sanitizing untrusted inputs to the TEE, minimizing attack surface through CVM hardening and enabling abuse detection through enhanced host monitoring.\n\nBecause we know that defending against physical access introduces significant complexity and attack surface even with industry-leading controls, we continuously pursue further attack surface hardening. In addition, we reduce these risks through measures like encrypted DRAM and standard physical security controls to protect our datacenters from bad actors.\n\nTo further address these unknown risks, we seek to eliminate the viability of targeted attacks via routing sessions through a third-party OHTTP relay to prevent an attacker’s ability to route a specific user to a specific machine.\n\nDesigning Private Processing\n\nHere is how we designed Private Processing to meet these foundational security and privacy requirements against the threat model we developed.\n\n(Further technical documentation and security research engagements updates are coming soon).\n\nConfidential processing\n\nData shared to Private Processing is processed in an environment which does not make it available to any other system. This protection is further upheld by encrypting data end-to-end between the client and the Private Processing application, so that only Private Processing, and no one in between – including Meta, WhatsApp, or any third-party relay – can access the data.\n\nTo prevent possible user data leakage, only limited service reliability logs are permitted to leave the boundaries of CVM.\n\nSystem software\n\nTo prevent privileged runtime access to Private Processing, we prohibit remote shell access, including from the host machine, and implement security measures including code isolation. Code isolation ensures that only designated code in Private Processing has access to user data. Prohibited remote shell access ensures that neither the host nor a networked user can gain access to the CVM shell.\n\nWe defend against potential source control and supply chain attacks by implementing established industry best practices. This includes building software exclusively from checked-in source code and artifacts, where any change requires multiple engineers to modify the build artifacts or build pipeline.\n\nAs another layer of security, all code changes are auditable. This allows us to ensure that any potential issues are discovered — either through our continuous internal audits of code, or by external security researchers auditing our binaries.\n\nSystem hardware\n\nPrivate Processing utilizes CPU-based confidential virtualization technologies, along with Confidential Compute mode GPUs, which prevent certain classes of attacks from the host operating system, as well as certain physical attacks.\n\nEnforceable guarantees\n\nPrivate Processing utilizes CPU-based confidential virtualization technologies which allow attestation of software based in a hardware root of trust to guarantee the security of the system prior to each client-server connection. Before any data is transmitted, Private Processing checks these attestations, and confirms them against a third-party log of acceptable binaries.\n\nStateless and forward secure service\n\nWe operate Private Processing as a stateless service, which neither stores nor retains access to messages after the session has been completed.\n\nAdditionally, Private Processing does not store messages to disk or external storage, and thus does not maintain durable access to this data.\n\nAs part of our data minimization efforts, requests to Private Processing only include data that is useful for processing the prompt — for example, message summarization will only include the messages the user directed AI to summarize.\n\nNon-targetability\n\nPrivate Processing implements the OHTTP protocol to establish a secure session with Meta routing layers. This ensures that Meta and WhatsApp do not know which user is connecting to what CVM. In other words, Meta and WhatsApp do not know the user that initiated a request to Private Processing while the request is in route, so that a specific user cannot be routed to any specific hardware.\n\nPrivate Processing uses anonymous credentials to authenticate users over OHTTP. This way, Private Processing can authenticate users to the Private Processing system, but remains unable to identify them. Private Processing does not include any other identifiable information as part of the request during the establishment of a system session. We limit the impact of small-scale attacks by ensuring that they cannot be used to target the data of a specific user.\n\nVerifiable transparency\n\nTo provide users visibility into the processing of their data and aid in validation of any client-side behaviors, we will provide capabilities to obtain an in-app log of requests made to Private Processing, data shared with it, and details of how that secure session was set up.\n\nIn order to provide verifiability, we will make available the CVM image binary powering Private Processing. We will make these components available to researchers to allow independent, external verification of our implementation.\n\nIn addition, to enable deeper bug bounty research in this area, we will publish source code for certain components of the system, including our attestation verification code or load bearing code.\n\nWe will also be expanding the scope of our existing Bug Bounty program to cover Private Processing to enable further independent security research into Private Processing’s design and implementation.\n\nFinally, we will be publishing a detailed technical white paper on the security engineering design of Private Processing to provide further transparency into our security practices, and aid others in the industry in building similar systems.\n\nGet Involved\n\nWe’re deeply committed to providing our users with the best possible messaging experience while ensuring that only they and the people they’re talking to can access or share their personal messages. Private Processing is a critical component of this commitment, and we’re excited to make it available in the coming weeks.\n\nWe welcome feedback from our users, researchers, and the broader security community through our security research program:", "label": "non_personal"}
{"title": "Introducing AutoPatchBench: A Benchmark for AI-Powered Security Fixes", "url": "https://engineering.fb.com/2025/04/29/ai-research/autopatchbench-benchmark-ai-powered-security-fixes/", "content": "We are introducing AutoPatchBench, a benchmark for the automated repair of vulnerabilities identified through fuzzing.\n\nBy providing a standardized benchmark, AutoPatchBench enables researchers and practitioners to objectively evaluate and compare the effectiveness of various AI program repair systems.\n\nThis initiative facilitates the development of more robust security solutions, and also encourages collaboration within the community to address the critical challenge of software vulnerability repair.\n\nAutoPatchBench is available now on GitHub.\n\nAI is increasingly being applied to solve security challenges, including repairing vulnerabilities identified through fuzzing. However, the lack of a standardized benchmark for objectively assessing AI-driven bug repair agents specific to fuzzing has impeded progress in academia and the broader community. Today, we are publicly releasing AutoPatchBench, a benchmark designed to evaluate AI program repair systems. AutoPatchBench sits within CyberSecEval 4, Meta’s new benchmark suite for evaluating AI capabilities to support defensive use cases. It features 136 fuzzing-identified C/C++ vulnerabilities in real-world code repos along with verified fixes sourced from the ARVO dataset.\n\nAutoPatchBench provides a standardized evaluation framework for assessing the effectiveness of AI-assisted vulnerability repair tools. This benchmark aims to facilitate a comprehensive understanding of the capabilities and limitations of various AI-driven approaches to repairing fuzzing-found bugs. By offering a consistent set of evaluation criteria, AutoPatchBench fosters transparency and reproducibility in research, enabling both academic and industry professionals to identify best practices and areas for improvement.\n\nFixing fuzzing-found vulnerabilities with AI\n\nFuzzing is a cornerstone in automated testing, renowned for its effectiveness in uncovering security vulnerabilities. By bombarding a target program with vast amounts of pseudo-random input data, fuzz testing exposes critical security and reliability issues, such as memory corruption, invalid pointer dereference, integer overflow, and parsing errors.\n\nHowever, resolving a fuzzing crash is often a labor intensive task, demanding intricate debugging and thorough code review to pinpoint and rectify the underlying cause. This process can be both time-consuming and resource-intensive. Unlike regular test failures, fuzzing bugs frequently reveal security vulnerabilities that pose severe threats to system integrity and user data. Given these stakes, automating the repair of fuzzing bugs with AI becomes not just advantageous but essential. AI’s ability to swiftly analyze patterns and propose solutions significantly reduces the time and effort required for repairs, making it an invaluable ally in safeguarding our digital environments.\n\nLet’s explore the process of addressing bugs identified through fuzzing by examining a demonstrative example. Consider the following C function, which harbors a read/write buffer overflow vulnerability:\n\n#include <stdio.h> #include <string.h> void process_input(const char *input) { char buffer[8]; strcpy(buffer, input); // Potential buffer overflow printf(\"Processed: %s\n\n\", buffer); }\n\nIn this scenario, a fuzzing harness might supply an input that surpasses the buffer’s capacity, leading to a crash due to buffer overflow. A typical stack trace from such a crash might appear as follows:\n\n== Fuzzer Crash Report == Program received signal SIGSEGV, Segmentation fault. 0x00007ffff7af1223 in strcpy () from /lib/x86_64-linux-gnu/libc.so.6 (gdb) bt #0 0x00007ffff7af1223 in strcpy () #1 0x0000555555555140 in process_input (input=0x7fffffffe695 \"AAAAAA...\") #2 0x0000555555555162 in main (argc=2, argv=0x7fffffffe5f8)\n\nHere, the process_input function invokes strcpy on a string that exceeds the eight-character buffer, causing a segmentation fault. A straightforward patch involves ensuring the copy operation remains within the buffer’s limits. This can be achieved by using a bounded copy function like strncpy or implementing a length check before copying:\n\nvoid process_input(const char *input) { char buffer[8]; strncpy(buffer, input, sizeof(buffer) - 1); buffer[sizeof(buffer) - 1] = '\\0'; printf(\"Processed: %s\n\n\", buffer); }\n\nThis patch ensures that the string remains within the buffer’s limits, effectively preventing out-of-bounds writes. Its correctness can be confirmed by verifying that the fuzzing input, which previously caused the crash, no longer does so. Additional checks can be conducted to ensure the patch doesn’t introduce any unintended side effects.\n\nAs illustrated, fixing a fuzzing crash involves:\n\nAnalyzing the crash stack trace and the target code. Pinpointing the root cause. Patching the vulnerable code. Verifying the fix’s accuracy.\n\nAn AI-based solution can automate these steps by utilizing an LLM’s capability to understand and generate code.\n\nWhy we developed AutoPatchBench\n\nAutoPatchBench is informed by key advancements in the field of AI-driven program repair, particularly those focusing on fuzzing-found vulnerabilities. Among the notable contributions is Google’s tech report on AI-powered patching, which pioneered the use of LLMs for addressing fuzzing crashes, achieving a 15% fix rate with their proprietary dataset. Subsequently, Google’s study on generic program repair agents introduced the GITS-Eval benchmark, encompassing 178 bugs across various programming languages.\n\nIn the realm of AI software engineering agents, benchmarks like SWE-Bench and SWE-Bench Verified have gained widespread acceptance for evaluating generic AI SWE agents. However, these benchmarks do not specifically tackle the unique challenges posed by fuzzing-found vulnerabilities, which demand specialized approaches that utilize fuzzing-specific artifacts and address security concerns.\n\nAutoPatchBench addresses this gap by offering a dedicated benchmark focused on a wide variety of C/C++ vulnerabilities of 11 crash types identified through fuzzing with automated verification capability. Unlike the broader focus of GITS-Eval and SWE-Bench, AutoPatchBench is specifically designed to assess the effectiveness of AI-driven tools in repairing security-critical bugs typically uncovered by fuzzing. This targeted approach enables a more precise evaluation of AI capabilities in meeting the complex requirements of fuzzing-found vulnerabilities, thereby advancing the field of AI-assisted program repair in a focused manner.\n\nInside AutoPatchBench\n\nWe’re making AutoPatchBench publicly available as part of CyberSecEval 4 to encourage community collaboration in tackling the challenge of automating fuzzing crash repairs. This benchmark is specifically designed for AI program repair agents focusing on C/C++ bugs identified through fuzzing. It includes real-world C/C++ vulnerabilities with verified fixes sourced from the ARVO dataset, and incorporates additional verification of AI-generated patches through fuzzing and white-box differential testing.\n\nARVO dataset\n\nThe ARVO dataset serves as the foundation for AutoPatchBench, offering a comprehensive collection of real-world vulnerabilities that are essential for advancing AI-driven security research. Sourced from C/C++ projects identified by Google’s OSS-Fuzz, ARVO includes over 5,000 reproducible vulnerabilities across more than 250 projects. Each entry is meticulously documented with a triggering input, a canonical developer-written patch, and the capability to rebuild the project in both its vulnerable and patched states.\n\nHowever, there are notable challenges when using the ARVO dataset as a benchmark for AI patch generation:\n\nWhile reproducibility is vital for a reliable benchmark, the ARVO dataset includes samples where crashes are not consistently reproducible. Some samples lack crash stack traces, making it exceedingly difficult to address the crash. Although ARVO provides a ground-truth fix for each identified vulnerability, it lacks an automated mechanism to verify the correctness of a generated patch. Objective automated verification is essential for a benchmark focused on patch generation.\n\nAutoPatchBench addresses these challenges by creating a curated subset and by employing a comprehensive and automated verification process.\n\nSelection criteria\n\nTo ensure the reliability and effectiveness of AutoPatchBench, we meticulously filtered the ARVO dataset samples based on the following criteria:\n\nValid C/C++ vulnerability: The ground-truth fix shall edit one or more C/C++ source files that are not fuzzing harnesses.\n\nDual-container setup : Each vulnerability is accompanied by two containers—one that contains vulnerable code and another for the fixed code—that build without error.\n\nReproducibility : The crash must be consistently reproducible within the vulnerable container.\n\nValid stack trace : A valid stack trace must be present within the vulnerable container to facilitate accurate diagnosis and repair.\n\nSuccessful compilation : The vulnerable code must compile successfully within its designated container, ensuring that the environment is correctly set up for testing.\n\nFixed code verification : The fixed code must also compile successfully within its respective container, confirming that the patch does not introduce new build issues.\n\nCrash resolution : The crash must be verified as resolved within the fixed container, demonstrating the effectiveness of the patch.\n\nFuzzing pass : The fixed code must pass a comprehensive fuzzing test without finding new crashes, ensuring that the ground-truth patch maintains the integrity and functionality of the software.\n\nAfter applying these rigorous selection criteria, we retained 136 samples for AutoPatchBench that fulfill the necessary conditions for both patch generation and verification. From this refined set, we created a down-sampled subset of 113 AutoPatchBench-Lite samples to provide a focused benchmark for testing AI patch generation tools. These subsets preserves the diversity and complexity of real-world vulnerabilities including 11 distinct crash types, offering a solid foundation for advancing AI-driven security solutions.\n\nPatch verification\n\nIn the process of patch generation, the patch generator utilizes two automated methods to verify the viability of a generated patch before submitting it for evaluation. The first method involves attempting to build the patched program, which checks for syntactic correctness. The second method involves attempting to reproduce the crash by running the input that initially triggered it. If the crash no longer occurs, it suggests that the issue has been resolved. However, these steps alone are insufficient to guarantee the correctness of the patch, as a patch might not maintain the program’s intended functionality, rendering it incorrect despite resolving the crash.\n\nTo address this issue, AutoPatchBench adopts a comprehensive approach to automate the evaluation of generated patches. This involves subjecting the patched code to further fuzz testing using the original fuzzing harness that initially detected the crash. Additionally, white-box differential testing compares the runtime behavior of the patched program against the ground truth repaired program, confirming that the patch has effectively resolved the underlying bug without altering the program’s intended functionality. Since a patch can potentially be made in multiple places, we cannot assume that the LLM will patch the same function as the groundtruth patch does. Instead we find all the callstacks for each call to a patched function. Then we find the lowest common ancestor (LCA) across all pairs of stacktraces offered by the groundtruth patch and the LLM patch. We then utilize debug information to inspect arguments, return values, and local variables at the first function above the LCA, differential testing offers a detailed view of the patch’s impact on the program state.\n\nThis process evaluates whether the generated patch produces a program state identical to the ground truth program after the patched function returns. By using a diverse set of inputs obtained from fuzzing, this gives higher confidence that the bug is fixed without changing the visible behavior of the patched functions. This differential testing is implemented using a Python script that leverages LLDB APIs to dump all visible states and identify differences between the ground truth and the patched program.\n\nHowever, as with all attempts to solve provably undecidable problems (in this case: program equivalence), there are some failure modes for this verification step. For example, sometimes the analysis fails with timeouts, in which case we consider the semantics to be preserved if both the ground truth and the LLM patch timed out. Programs might also behave non-deterministically, and we run each input three times to identify nondeterministic struct fields and values. Such fields will not be compared to avoid false alarms from noisy, random values. Additionally, we strip any fields that contain the substring “build” or “time” as we’ve observed false positives from build-ids (that happen to be deterministic within a program, but not across different patches).\n\nIt should also be noted that on a number of examples, the crashing PoC never actually triggered the breakpoints on the ground truth patch, making comparison of the resulting states impossible. However, our case study showed that white-box differential testing is still effective in filtering out a majority of incorrect patches despite its limitation, which will be discussed in the case study.\n\nAutoPatchBench and AutoPatchBench-Lite\n\nAutoPatchBench is a comprehensive benchmark dataset of 136 samples. It encompasses a wide range of real-world vulnerabilities, providing a robust framework for assessing the capabilities of automated patch generation systems.\n\nWithin this benchmark, we have also created a subset called AutoPatchBench-Lite that consists of 113 samples. AutoPatchBench-Lite focuses on a simpler subset of vulnerabilities where the root cause of the crash is confined to a single function. This version is designed to cater to scenarios where the complexity of the bug is relatively low, making it more accessible for tools that are in the early stages of development or for those that specialize in handling straightforward issues.\n\nThe rationale for creating AutoPatchBench-Lite stems from the observation that when root causes are distributed across multiple locations within the code, the difficulty of generating a correct patch increases significantly. Addressing such “hard” crashes requires a tool to possess advanced reasoning capabilities to analyze larger codebases and apply patches to multiple areas simultaneously. This complexity not only challenges the tool’s design but also demands a higher level of sophistication in its algorithms to ensure accurate and effective patching.\n\nBy offering both AutoPatchBench and AutoPatchBench-Lite, we provide a tiered approach to benchmarking, allowing developers to progressively test and refine their tools. This structure supports the development of more advanced solutions capable of tackling both simple and complex vulnerabilities, ultimately contributing to the enhancement of AI-assisted bug repair techniques.\n\nExpected use cases\n\nAutoPatchBench offers significant value to a diverse range of users. Developers of auto-patch tools can leverage our open-sourced patch generator to enhance their tools and assess their effectiveness using the benchmark. Software projects employing fuzzing can incorporate our open-sourced patch generator to streamline vulnerability repair. Additionally, model developers can integrate the benchmark into their development cycles to build more robust and specialized expert models for bug repair. The tooling around the patch generator provided here can also be used in reinforcement learning as a reward signal during training. This data helps train models to better understand the nuances of bug repair, enabling them to learn from past fixes and improve their ability to generate accurate patches.\n\nReference implementation\n\nWe developed a basic patch generator to establish a baseline performance using AutoPatchBench. This generator is specifically designed to address simple crashes that involve patching a single function. We have open-sourced this reference implementation to encourage the community to build and expand upon it.\n\nFigure 1 shows a high-level overview of its design. The patch generator takes crash stack trace and target source code as input. It identifies the source locations from the stack trace and extracts every function that contains those locations. It then asks the LLM to identify the root cause and repair the crash by patching one of the functions. Upon receiving a response from the LLM, the patch generator extracts the revised code, applies the patch, compiles the program, and tests it against the original input that caused the crash. If the build or test fails, we re-engage the LLM with the error message from the build or test output, requesting it to attempt a solution again until the crash is resolved. If a fix trajectory fails to reach a valid solution that passes build and crash reproduction within a finite number of steps, we start a new trajectory to reset the context window, preventing prolonged entrapment in an incorrect path.\n\nThe example prompt below also captures the essence of our approach. Note that the actual prompt is more detailed and divided into multiple segments to promote chain-of-thought reasoning.\n\nAs an experienced Security Engineer at Meta, your task is to address the following security-critical fuzzing crash. Below is the stack trace of the crash: == Fuzzer Crash Report == Program received signal SIGSEGV, Segmentation fault. 0x00007ffff7af1223 in strcpy () from /lib/x86_64-linux-gnu/libc.so.6 (gdb) bt #0 0x00007ffff7af1223 in strcpy () #1 0x0000555555555140 in process_input (input=0x7fffffffe695 \"AAAAAA...\") #2 0x0000555555555162 in main (argc=2, argv=0x7fffffffe5f8) Here is the source code for the functions involved in the stack trace: strcpy() {...} void process_input(const char *input) { char buffer[8]; strcpy(buffer, input); // Potential buffer overflow printf(\"Processed: %s\n\n\", buffer); } int main() {...} Assuming the root cause of the crash is within one of these functions, generate a patched version of the faulty function to resolve the fuzzing crash. Ensure that you provide a complete rewrite of the function so that the patch can be applied and the code compiled without errors.\n\nA case study with AutoPatchBench-Lite\n\nIn the case study, we demonstrate the use of AutoPatchBench by evaluating our reference patch generator with several LLM models. Given that our reference implementation is limited to addressing simple issues, we conducted our evaluation with AutoPatchBench-Lite, which contains 113 samples. To prevent fix trajectories from becoming excessively prolonged, we capped the maximum length of each trajectory at five. Additionally, we set the maximum number of retries to 10.\n\nPlease note that the case study is not intended to provide a statistically rigorous comparison of model performance. Instead, it aims to present preliminary results to establish a baseline expectation. We encourage future research to build upon these findings.\n\nEffectiveness of patch generation and verification\n\nWe evaluated the effectiveness of the patch generator and our automated verification processes while using different LLM models as back-end. The figure below illustrates the effectiveness of patch generation and verification by presenting the percentage of samples that successfully passed each sequential verification step: (1) patch validity: build and crash reproducibility check, (2) fuzzing pass: passes 10-minute fuzzing, and (3) testing pass: passes white-box differential testing. It is important to note that the patch generation process only utilizes step (1) to verify the build and crash reproducibility. The fuzzing and differential testing are conducted post-generation to assess correctness.\n\nFigure 2 shows that all models achieved similar generation success rates of around 60% and similar post-verification success rates of around 5-11% with overlapping confidence intervals, and therefore, we do not draw any conclusion about their relative performance. The graph does, however, reveal that a substantial portion of the generated patches are found to be incorrect when subjected to fuzzing and white-box differential testing. For instance, Gemini 1.5 Pro achieved a 61.1% patch generation success rate, yet fewer than 15% of these patches (5.3% out of total set) were found to be correct. This gap highlights that build and crash reproduction are not good enough signals to infer the correctness of generated patches, and that future patch generation approaches should scrutinize the semantic preservation of generated patches more thoroughly. This gap also underscores the vital role of the comprehensive verification processes that checks semantic equivalence, a distinctive contribution of AutoPatchBench.\n\nEffect of inference-time computation\n\nTo assess the impact of inference-time computation on improving the patch generation success rate, we present the distribution of retry counts among the 73 patches produced by Llama 4 Maverick.\n\n\n\nFigure 3 shows that 44 out of 73 patches, or 60.2%, were successfully generated on the first attempt. The remaining 40% of the samples required more than two iterations, with no evident plateau until the 10th iteration. This outcome demonstrates that allocating more computational resources during inference-time leads to a higher success rate and suggests that increasing the number of retries could yield better results.\n\nManual validation\n\nIn our investigation of the precision and recall of white-box differential testing, we conducted a manual validation of 44 patches that passed 10-minute fuzzing against human-written ground truth fixes with the help of security experts. These patches were selected from a pool of 73 generated by Llama 4 Maverick. The following table shows the confusion matrix.\n\nTable 1: Confusion matrix between human judgement and differential testing\n\nTest pass Test fail Sum Human pass 5 0 5 Human reject 7 32 39 Sum 12 32 44\n\nThe results showed that the differential testing achieved an accuracy of 84.1% for this sample (5 + 32 / 44), indicating a high overall agreement with the human assessment. However, a closer examination of the confusion matrix revealed a notable discrepancy between precision and recall. Specifically, the testing method demonstrated 100.0% recall in this case study, correctly identifying all 5 instances that humans judged as correct. In contrast, precision was relatively low (41.7%), with 7 false positives out of 12 total positive predictions. This suggests that differential testing reported success on some incorrect patches as well, highlighting the need for manual validation of patch correctness. Despite this shortcoming, the result clearly shows the utility of differential testing in automatically rejecting a substantial number of incorrect patches, which will substantially save the manual validation effort.\n\nKey insights\n\nOur case study revealed several limitations of the current patch generator.\n\nThe root cause may not exist in the stack trace\n\nFrequently, crashes are the result of state contamination that occurs prior to the crash being triggered. Consequently, none of the functions within the stack frames may include the code responsible for the root cause. Since our current implementation requires the LLM to assume that the root cause is located within one of the functions in the stack trace, it is unable to generate an accurate patch in such cases. Solving this problem would require a more autonomous agent which can reason about the root cause on its own with a code browsing capability.\n\nCheating\n\nIn some instances, the LLM resorted to “cheating” by producing patches that superficially resolved the issue without addressing the underlying problem. This can occur when the generator modifies or removes code in a way that prevents the crash from occurring, but does not actually fix the root cause of the issue. We observed that cheating happens more frequently when we request the LLM to retry within the same trajectory. A potential solution to this could be to empower the LLM to say “I cannot fix it,” which may come with a tradeoff with success rate. However, note that most of the cheating was caught in the verification step, highlighting the utility of differential testing.\n\nNeed for enhanced patch verification methods\n\nFuzzing and white-box differential testing have shown that a large majority of generated patches are incorrect when compared to the ground-truth patches. This finding highlights the challenge of generating accurate patches without enhanced verification capabilities. To address this gap, several approaches can be considered:\n\nA patch generator could provide additional code context when querying the LLM for a patch so that LLM can better understand the consequence of a code patch.\n\nA patch generator could make additional LLM queries to verify the perseverance of existing functionality.\n\nA patch generator can attempt to generate multiple valid patches by exploring multiple trajectories in parallel, and let LLM choose the best option that is most likely to be correct.\n\nIn a well-tested real-world codebase, a patch generator can utilize existing tests to validate the patches it creates. This process complements building the code and checking for crash reproduction, allowing the patch generator to retry if a patch fails the tests. The accuracy of the generated patches is largely dependent on the thoroughness of the existing tests.\n\nIn conclusion, while our study has identified several challenges with the current patch generation process, it also opens up opportunities for improvement. By addressing these limitations with innovative solutions, we can enhance the accuracy and reliability of patch generation, paving the way for more robust and effective automated tools.\n\nGet started with AutoPatchBench\n\nAutoPatchBench is now available on GitHub. We welcome pull requests to integrate new/additional agent architectures into the framework, and look forward to seeing how well they perform on AutoPatchBench.", "label": "non_personal"}
{"title": "Taking the plunge: The engineering journey of building a subsea cable", "url": "https://engineering.fb.com/2025/05/01/connectivity/taking-the-plunge-the-engineering-journey-of-building-a-subsea-cable/", "content": "Meta develops infrastructure all across the globe to transport information and content for the billions of people using our services around the world. At the core of this infrastructure are aggregation points – like data centers – and the digital cables that connect them. Subsea cables – the unseen digital highways of the internet – are critical for Meta to serve people wherever they are in the world. In fact, more than 95% of the world’s intercontinental traffic goes through subsea cables.\n\nMeta’s engineering team prioritizes both innovation and quality when designing and deploying these cables. In the latest Meta Tech Podcast, Andy Palmer-Felgate and Pascal Pecci, both subsea cable systems engineers, join Pascal Hartig on the Meta Tech podcast to discuss the latest in subsea engineering technology. This episode dives deeper into the engineering nuances of large-scale subsea cable projects like the recently announced Project Waterworth.\n\nLearn more about Meta’s work on these engineering feats. Download or listen to the episode below:\n\nThe Meta Tech Podcast is a podcast, brought to you by Meta, where we highlight the work Meta’s engineers are doing at every level – from low-level frameworks to end-user features.\n\nSend us feedback on Instagram, Threads, or X.\n\n\n\nAnd if you’re interested in learning more about career opportunities at Meta, visit the Meta Careers page.", "label": "non_personal"}
{"title": "Enhancing the Python ecosystem with type checking and free threading", "url": "https://engineering.fb.com/2025/05/05/developer-tools/enhancing-the-python-ecosystem-with-type-checking-and-free-threading/", "content": "Meta and Quansight have improved key libraries in the Python Ecosystem. There is plenty more to do and we invite the community to help with our efforts.\n\nWe’ll look at two key efforts in Python’s packaging ecosystem to make packages faster and easier to use:\n\n🚀 Unlock performance wins for developers through free-threaded Python – where we leverage Python 3.13’s support for concurrent programming (made possible by removing the Global Interpreter Lock (GIL)).\n\n✅ Increase developer velocity in the IDE with improved type annotations.\n\nEnhancing typed Python in the Python scientific stack\n\nType hints, introduced in Python 3.5 with PEP-484, allow developers to specify variable types, enhancing code understanding without affecting runtime behavior. Type-checkers validate these annotations, helping prevent bugs and improving IDE functions like autocomplete and jump-to-definition. Despite their benefits, adoption is inconsistent across the open source ecosystem, with varied approaches to specifying and maintaining type annotations.\n\nThe landscape of open source software is fractured with respect to how type annotations are specified, maintained, and distributed to end users. Some projects have in-line annotations (types directly declared in the source code directly), others keep types in stub files, and many projects have no types at all, relying on third party repositories such as typeshed to provide community-maintained stubs. Each approach has its own pros and cons, but application and maintenance of them has been inconsistent.\n\nMeta and Quansight are addressing this inconsistency through:\n\nDirect contributions: We have improved the type coverage for pandas-stubs and numpy, and are eager to expand the effort to more packages. Community engagement: Promoting type annotation efforts to encourage community involvement, listen to feedback and create actionable ways to improve the ecosystem. Tooling and automation: Developing tools to address common challenges adding types and keeping the types up-to-date with the source code.\n\nImproved type annotations in pandas\n\nTL;DR: Pandas is the second most downloaded package from the Python scientific stack. We improved pandas-stubs package type annotation coverage from 36% to over 50%.\n\nBackground\n\nThe pandas community maintains its own stubs in a separate repository, which must be installed to obtain type annotations. While these stubs are checked separately from the source code, it allows the community to use types with their own type checking and IDE.\n\nImproving type coverage\n\nWhen we began our work in pandas-stubs, coverage was around 36%, as measured by the percentage of parameters, returns, and attributes that had a complete type annotation (the annotation is present and all generics have type arguments). After several weeks of work and about 30 PRs, type completeness is now measured at over 50%. The majority of our contributions involved adding annotations to previously-untyped parameters, adding type arguments to raw generic types, and removing deprecated/undocumented interfaces. We also improved several inaccurate annotations and updated others to match the inline annotations in the pandas source code.\n\nKey introductions\n\nTwo key introductions significantly increased coverage:\n\nReplacing raw Series types with UnknownSeries , a new type aliased to Series[Any] . When applied to return type annotations, this reduces the number of type checker false-positives when the function is called.\n\nImproving types of core Dataframe operations like insert, combine, replace, transpose, and assign, as well as many timestamp and time-zone related APIs.\n\nTooling development\n\nIn addition to improving coverage directly, we developed tooling to catalog public interfaces missing annotations. We also augmented our tools for measuring type coverage to handle the situation where stubs are distributed independently, rather than being packaged into the core library wheel.\n\nWhat is free-threaded Python ?\n\nFree-threaded Python (FTP) is an experimental build of CPython that allows multiple threads to interact with the VM in parallel. Previously, access to the VM required holding the global interpreter lock (GIL), thereby serializing execution of concurrently running threads. With the GIL becoming optional, developers will be able to take full advantage of multi-core processors and write truly parallel code.\n\nBenefits of free-threaded Python\n\nThe benefits of free-threaded Python are numerous:\n\nTrue parallelism in a single process : With the GIL removed, developers can write Python code that takes full advantage of multi-core processors without needing to use multiple processes. CPU-bound code can execute in parallel across multiple cores.\n\nImproved performance: By allowing multiple threads to execute Python code simultaneously, work can be effectively distributed across multiple threads inside a single process.\n\nSimplified concurrency: Free-threading provides developers with a more ergonomic way to write parallel programs in Python. Gone are the days of needing to use multiprocessing.Pool and/or resorting to custom shared memory data structures to efficiently share data between worker processes.\n\nGetting Python’s ecosystem ready for FTP\n\nThe ecosystem of Python packages must work well with free-threaded Python in order for it to be practically useful; application owners can’t use free-threading unless their dependencies work well with it. To that end, we have been taking a “bottoms up” approach to tackle the most difficult/popular packages in the ecosystem. We’ve added free-threading support to many of the most popular packages used for scientific computing (e.g. numpy, scipy, scikit-learn) and language bindings (e.g. Cython, nanobind, pybind, PyO3).\n\nJust getting started\n\nTogether, we made substantial progress in improving type annotations and free-threading compatibility in Python libraries. We couldn’t have done it without the Python community and are asking others to join our efforts. Whether it’s further updates to the type annotations or preparing your code for FTP, we value your help moving the Python ecosystem forward!\n\nTo learn more about Meta Open Source, visit our open source site, subscribe to our YouTube channel, or follow us on Facebook, Threads, X and LinkedIn.", "label": "non_personal"}
{"title": "Accelerating GPU indexes in Faiss with NVIDIA cuVS", "url": "https://engineering.fb.com/2025/05/08/data-infrastructure/accelerating-gpu-indexes-in-faiss-with-nvidia-cuvs/", "content": "Meta and NVIDIA collaborated to accelerate vector search on GPUs by integrating NVIDIA cuVS into Faiss v1.10 , Meta’s open source library for similarity search.\n\nThis new implementation of cuVS will be more performant than classic GPU-accelerated search in some areas.\n\nFor inverted file (IVF) indexing, NVIDIA cuVS outperforms classical GPU-accelerated IVF build times by up to 4.7x; and search latency is reduced by as much as 8.1x.\n\nFor graph indexing, CUDA ANN Graph (CAGRA) outperforms CPU Hierarchical Navigable Small World graphs (HNSW) build times by up to 12.3x; and search latency is reduced by as much as 4.7x.\n\nThe Faiss library\n\nThe Faiss library is an open source library, developed by Meta FAIR, for efficient vector search and clustering of dense vectors. Faiss pioneered vector search on GPUs, as well as the ability to seamlessly switch between GPUs and CPUs. It has made a lasting impact in both research and industry, being used as an integrated library in several databases (e.g., Milvus and OpenSearch), machine learning libraries, data processing libraries, and AI workflows. Faiss is also used heavily by researchers and data scientists as a standalone library, often paired with PyTorch.\n\nCollaboration with NVIDIA\n\nThree years ago, Meta and NVIDIA worked together to enhance the capabilities of vector search technology and to accelerate vector search on GPUs. Previously, in 2016, Meta had incorporated high performing vector search algorithms made for NVIDIA GPUs: GpuIndexFlat; GpuIndexIVFFlat; GpuIndexIVFPQ. After the partnership, NVIDIA rapidly contributed GpuIndexCagra, a state-of-the art graph-based index designed specifically for GPUs. In its latest release, Faiss 1.10.0 officially includes these algorithms from the NVIDIA cuVS library.\n\nFaiss 1.10.0 also includes a new conda package that unlocks the ability to choose between the classic Faiss GPU implementations and the newer NVIDIA cuVS algorithms, making it easy for users to switch between GPU and CPU.\n\nBenchmarking\n\nThe following benchmarks were conducted using the cuVS-bench tool.\n\nWe measured:\n\nA tall, slender image dataset: A subset of 100 million vectors from the Deep1B dataset by 96 dimensions.\n\nA short, wide dataset of text embeddings: 5 million vector embeddings, curated using the OpenAI text-embedding-ada-002 model .\n\nTests for index build times and search latency were conducted on an NVIDIA H100 GPU and compared to an Intel Xeon Platinum 8480CL system. Results are reported in the tables below at 95% recall along the pareto frontiers for k=10 nearest neighbors.\n\nBuild time (95% recall@10)\n\nIndex Embeddings\n\n100M x 96\n\n(seconds) Embeddings\n\n5M x 1536\n\n(seconds) Faiss Classic Faiss cuVS Faiss Classic Faiss cuVS Faiss Classic Faiss cuVS IVF Flat IVF Flat 101.4 37.9 (2.7x) 24.4 15.2 (1.6x) IVF PQ IVF PQ 168.2 72.7 (2.3x) 42.0 9.0 (4.7x) HNSW (CPU) CAGRA 3322.1 518.5 (6.4x) 1106.1 89.7 (12.3x)\n\nTable 1: Index build times for Faiss-classic and Faiss-cuVS in seconds (with NVIDIA cuVS speedups in parentheses).\n\nSearch latency (95% recall@10)\n\nIndex Embeddings\n\n100M x 96\n\n(milliseconds) Embeddings\n\n5M x 1536\n\n(milliseconds) Faiss Classic Faiss cuVS Faiss Classic Faiss cuVS Faiss Classic Faiss cuVS IVF Flat IVF Flat 0.75 0.39 (1.9x) 1.98 1.14 (1.7x) IVF PQ IVF PQ 0.49 0.17 (2.9x) 1.78 0.22 (8.1x) HNSW (CPU) CAGRA 0.56 0.23 (2.4x) 0.71 0.15 (4.7x)\n\nTable 2: Online (i.e., one at a time) search query latency for Faiss-classic and Faiss-cuVS in milliseconds (with NVIDIA cuVS speedups in parentheses).\n\nLooking forward\n\nThe emergence of state-of-the-art NVIDIA GPUs has revolutionized the field of vector search, enabling high recall and lightning-fast search speeds. The integration of Faiss and cuVS will continue to incorporate state-of-the-art algorithms, and we look forward to unlocking new innovations in this partnership between Meta and NVIDIA.\n\nRead here for more details about NVIDIA cuVS.", "label": "non_personal"}
{"title": "Introducing Pyrefly: A new type checker and IDE experience for Python", "url": "https://engineering.fb.com/2025/05/15/developer-tools/introducing-pyrefly-a-new-type-checker-and-ide-experience-for-python/", "content": "Today we are announcing an alpha version of Pyrefly, an open source Python type checker and IDE extension crafted in Rust. Pyrefly is a static type checker that analyzes Python code to ensure type consistency and help you catch errors throughout your codebase before your code runs. It also supports IDE integration and CLI usage to give you flexibility in how you incorporate it into your workflow.\n\nThe open source community is the backbone of the Python language. We are eager to collaborate on Pyrefly with the community and improve Python’s type system and the many libraries that we all rely on.\n\nGet started\n\nReady to dive in? The official Pyrefly website has all the details, but to quickly get started:\n\nWhy we built Pyrefly\n\nBack in 2017, we embarked on a mission to create a type checker that could handle Instagram’s massive codebase of typed Python. This mission led to the birth of the Pyre type checker, inspired by the robust designs of Hack and Flow, and written in OCaml to deliver scalable performance.\n\nOver the years, Pyre served us well, but as the type system evolved and the need for typechecking to drive responsive IDE emerged, it was clear that we needed to take a new approach. We explored alternate solutions and leveraged community tools like Pyright for code navigation. But the need for an extensible type checker that can bring code navigation, checking at scale, and exporting types to other services drove us to start over, creating Pyrefly.\n\nThe principles behind Pyrefly\n\nToday, we’re excited to unveil Pyrefly, a project we’ve been developing openly on GitHub. We invite you to explore our work and try it out on your own project. While a project like Pyrefly is the sum of thousands of technical choices, a few notable principles we’ve followed are:\n\nPerformance\n\nWe want to shift checks that used to happen later on CI to happening on every single keystroke. That requires checking code at speed (on large codebases we can check 1.8 million lines of code per second!) and careful thought to incrementality and updates. Pyrefly is implemented in Rust and designed for high performance on codebases of all sizes.\n\nIDE first\n\nWe want the IDE and command line to share a consistent view of the world, which means crafting abstractions that capture the differences without incurring unnecessary costs. Designing these abstractions from the beginning is much easier than retrofitting them, which we tried with Pyre.\n\nInference\n\nSome Python programs are typed, but many aren’t. We want users to benefit from types even if they haven’t annotated their code – so automatically infer types for returns and local variables and display them in the IDE. What’s more, in the IDE you can even double click to insert these inferred types if you think that would make the program better.\n\nOpen source\n\nPython is open source, and hugely popular. The Python typing specification is open source, which made Pyrefly vastly easier to develop. Many of the libraries Meta contributes to are open source,( e.g., PyTorch).\n\nPyrefly is also open source, available on GitHub under the MIT license, and we encourage pull requests and issue reports. We also have a Discord channel for more free flowing discussions. We would love to build a community around Pyrefly.\n\nThe future of Pyrefly\n\nWe will work with the Python community to drive the language forward and improve the developer experience. Since the beginning of Pyre, we open sourced our code and contributed a number of PEPs alongside the community of type checker maintainers. We feel we can do more with Pyrefly to help Python developers leverage the benefits of types for developers, library authors, and folks just learning the language.\n\nMeta has leveraged types in dynamic languages from the beginning and knows the significant benefits it brings to developer productivity and security. We plan to share more of our learnings and tooling with blogs, better types in the ecosystem and language enhancements.\n\nToday we’re releasing Pyrefly as an alpha. At the same time, we’re busy burning down the long-tail of bugs and features aiming to remove the alpha label this Summer. Your feedback is invaluable to get there, so please give it a try and report your bugs or things you think can be improved. Even if Pyrefly isn’t right for your project, we would love to hear how you use types and what you would like to see improved in your editor.\n\nJoin us on the journey as we help illuminate your bugs with Pyrefly. Happy coding! 🐍✨\n\nHear more about Pyrefly\n\nCheck out the episode of the Meta Tech Podcast where several team members share their experience developing Pyrefly and technical details for how it works. We also just talked at PyCon US about high-performance Python through faster type checking and free threaded execution.\n\nTo learn more about Meta Open Source, visit our open source site, subscribe to our YouTube channel, or follow us on Facebook, Threads, X, and LinkedIn.\n\nAcknowledgements\n\nPyrefly was created By Meta’s Python Language Tooling Team: Jia Chen, Rebecca Chen, Sam Goldman, David Luo, Kyle Into, Zeina Migeed, Neil Mitchell, Maggie Moss, Conner Nilsen, Aaron Pollack, Teddy Sudol, Steven Troxler, Lucian Wischik, Danny Yang, and Sam Zhou.", "label": "non_personal"}
{"title": "Open-sourcing Pyrefly: A faster Python type checker written in Rust", "url": "https://engineering.fb.com/2025/05/15/developer-tools/open-sourcing-pyrefly-a-faster-python-type-checker-written-in-rust/", "content": "Back in 2017, engineers at Meta sought to create a type checker for Instagram’s typed Python codebase. Years later, as the type system continued to evolve, that type checker eventually became Pyrefly.\n\nPyrefly is a new type checker and IDE experience for Python, written with Rust, and now available for the entire Python community to use! It’s open-source, supports both CLI usage and IDE integration. and is designed to help you catch errors before runtime in Python codebases of any size.\n\nOn this episode of the Meta Tech Podcast, Pascal Hartig sits down with Maggie, Rebecca, and Neil — some of the team behind Pyrefly — to discuss this latest release from Meta and how they built an incremental type checker that scales to mono repositories.\n\nDownload or listen to the episode below:\n\nYou can also find the episode wherever you get your podcasts, including:\n\nThe Meta Tech Podcast is a podcast, brought to you by Meta, where we highlight the work Meta’s engineers are doing at every level – from low-level frameworks to end-user features.\n\nSend us feedback on Instagram, Threads, or X.\n\nAnd if you’re interested in learning more about career opportunities at Meta visit the Meta Careers page.\n\nLinks", "label": "non_personal"}
{"title": "Meta’s Full-stack HHVM optimizations for GenAI", "url": "https://engineering.fb.com/2025/05/20/web/metas-full-stack-hhvm-optimizations-for-genai/", "content": "As Meta has launched new, innovative products leveraging generative AI (GenAI), we need to make sure the underlying infrastructure components evolve along with it. Applying infrastructure knowledge and optimizations have allowed us to adapt to changing product requirements, delivering a better product along the way. Ultimately, our infrastructure systems need to balance our need to ship high-quality experiences with a need to run systems sustainability.\n\nSplitting GenAI inference traffic out into a dedicated WWW tenant, which allows specialized runtime and warm-up configuration, has enabled us to meet both of those goals while delivering a 30% improvement in latency.\n\nWho we are\n\nAs the Web Foundation team, we operate Meta’s monolithic web tier, running Hack. The team is composed of cross-functional engineers who make sure the infrastructure behind the web tier is healthy and well designed. We jump into incident response, work on some of the most complex areas of the infrastructure, and help build whatever we need to keep the site happily up and running.\n\nTo accomplish this, we have established a series of best practices on being a “good citizen” of the shared tier. We need to ensure that all requests comply with these guidelines to prevent issues from spilling over and affecting other teams’ products. One core rule is the request runtime—limiting a request to 30 seconds of execution. This is a consequence of the HHVM (HipHop Virtual Machine) runtime—each request has a corresponding worker thread, of which there is a finite number. To ensure there are always threads available to serve incoming requests, we need to balance the resources available on each host with its expected throughput. If requests are taking too long, there will be fewer available threads to process new requests, leading to user-visible unavailability.\n\nThe changing landscape\n\nClassically, webservers at Meta are optimized for serving front-end requests—rendering webpages and serving GraphQL queries. These requests’ latency is typically measured in hundreds of milliseconds to seconds (substantially below the 30-second limit), which enables hosts to process approximately 500 queries per second.\n\nAdditionally, a web server will spend about two-thirds of its time doing input/output (I/O), and the remaining third doing CPU work. This fact has influenced the design of the Hack language, which supports asyncio, a type of cooperative multi-tasking, and all the core libraries support these primitives to increase performance and decrease the amount of time the CPU is sitting idle, waiting for I/O.\n\nGenAI products, especially LLMs, have a different set of requirements. These are driven by the core inference flow: The model responds with a stream of tokens that can take seconds or minutes to complete. A user may see this as a chatbot “typing” a response. This isn’t an effect to make our products seem friendlier; it’s the speed at which our models think! After a user submits a query to the model, we need to start streaming these responses back to the user as fast as possible. On top of that, the total latency of the request is now substantially longer (measured in seconds). These properties have two effects on the infrastructure—minimal overhead on the critical path before calling the LLM, and a long duration for the rest of the request, most of which is spent waiting on I/O. (See Figures 1 and 2 below).\n\nA series of optimizations\n\nThis shift in requirements allowed Web Foundation to reexamine the rules of running the monolithic web tier. We then launched a dedicated web tenant (a standalone deployment of WWW) that allowed custom configuration, which we could better tune to the needs of the workload.\n\nRequest timeout\n\nFirst, running on an isolated web tier allowed us to increase the runtime limit for GenAI requests. This is a straightforward change, but it allowed us to isolate the longer-running traffic to avoid adverse impacts on the rest of the production tier. This way, we can avoid requests timing out if inference takes longer than 30 seconds.\n\nThread-pool sizing\n\nRunning requests for longer means there is reduced availability of worker threads (which, remember, map 1:1 with processed requests). Since webservers have a finite amount of memory, we can divide the total memory available by the per-request memory limit to get a peak number of active requests; this in turn tells us how many requests we can execute simultaneously. We ended up running with approximately 1000 threads on GenAI hosts, as compared to a couple of hundred on normal webservers.\n\nJIT cache and “jumpstart”\n\nHHVM is a just-in-time (JIT) interpreted language, which means the first time a given function executes, the machine needs to compile it to lower-level machine code for execution. Additionally, a technique called Jump-Start allows a webserver to seed its JIT cache with outputs from a previously warmed server. By allowing GenAI hosts to use Jump-Start profiles from the main web tier, we are able to greatly speed up execution, even if the code overlap is not identical.\n\nRequest warm-up\n\nHHVM also supports the execution of dummy requests at server startup, which we can execute, and then we can discard the results. The intent here is to warm non-code caches within the webserver. Configuration values and service discovery info are normally fetched inline the first time they are needed and then cached within the webserver. By fetching and caching this information in warm-up requests, we prevent our users from observing the latency of these initial fetches.\n\nShadow traffic\n\nFinally, Meta heavily uses real-time configuration to control feature rollouts, which means that jumpstart profiles consumed at startup time might not cover all future code paths the server will execute. To maintain coverage in the steady state, we also added request shadowing, so we can ensure that gating changes are still covered in the JIT cache.", "label": "non_personal"}
{"title": "Journey to 1000 models: Scaling Instagram’s recommendation system", "url": "https://engineering.fb.com/2025/05/21/production-engineering/journey-to-1000-models-scaling-instagrams-recommendation-system/", "content": "In this post, we explore how Instagram has successfully scaled its algorithm to include over 1000 ML models without sacrificing recommendation quality or reliability.\n\nWe delve into the intricacies of managing such a vast array of models, each with its own performance characteristics and product goals.\n\nWe share insights and lessons learned along the way—from the initial realization that our infrastructure maturity was lagging behind our ambitious scaling goals, to the innovative solutions we implemented to bridge these gaps.\n\nIn the ever-evolving landscape of social media, Instagram serves as a hub for creative expression and connection, continually adapting to meet the dynamic needs of its global community. At the heart of this adaptability lies a web of machine learning (ML) models, each playing a crucial role in personalizing experiences. As Instagram’s reach and influence has grown, so too has the complexity of its algorithmic infrastructure. This growth, while exciting, presents a unique set of challenges, particularly in terms of reliability and scalability.\n\nJoin us as we uncover the strategies and tools that have enabled Instagram to maintain its position at the forefront of social media innovation, ensuring a seamless and engaging experience for billions of users worldwide.\n\nAre there really that many ML models in Instagram?\n\nThough what shows up in Feed, Stories, and Reels is personally ranked, the number of ranked surfaces goes much deeper—to which comments surface in Feed, which notifications are “important,” or whom you might tag in a post. These are all driven by ML recommendations.\n\nWithin a given surface, we’ll have different layers of the ranking funnel: sourcing (retrieval), early-stage ranking (ESR), and late-stage ranking (LSR). We operate on fewer candidates as we progress through the funnel, as the underlying operations grow more expensive (see Figure 1 below):\n\nWithin each surface and layer, there is constant experimentation, and these permutations create a severe infrastructure challenge. We need to allow room for our ML engineers to experiment with changes such as adjusting weights for a given prediction. The net result, depicted below in Figure 2, is a large number of models serving user traffic in production:\n\nHow did we realize infra maturity wasn’t going to catch up?\n\nIdentified risks\n\nWe identified several risks associated with scaling our algorithm, rooted in complaints about ML productivity and repeating patterns of issues:\n\nDiscovery: Even as a team focused on one app — Instagram — we couldn’t stay on top of the growth, and product ML teams were maintaining separate sources of truth, if any, for their models in production.\n\nRelease: We didn’t have a consistent way to launch new models safely, and the process was slow, impacting ML velocity and, therefore, product innovation.\n\nHealth: We lacked a consistent definition of model prediction quality, and with the diversity of surfaces and subtlety of degraded ranking, quality issues went unnoticed.\n\nSolution overview\n\nTo address these risks, we implemented several solutions:\n\nModel registry: We built a registry that serves as a ledger for production model importance and business function foremost, among other metadata. This registry serves as our foundational source of truth, upon which we can leverage automation to uplevel system-wide observability, change management, and model health.\n\nModel launch tooling: We developed a more ideal flow for launching new models that includes estimation, approval, prep, scale-up, and finalization. This process is now automated, and we’ve reduced the time it takes to launch a new model from days to hours.\n\nModel stability: We defined and operationalized model stability, a pioneering metric that measures the accuracy of our model predictions. We’ve leveraged model stability to produce SLOs for all models in the model registry, which enables simple understanding of the entire product surface’s ML health.\n\nModel registry\n\nWhat did model investigations look like prior to the registry?\n\nBefore we created the model registry, the investigation process was a time-consuming and error-prone experience for on-call engineers and model owners. An on-call engineer had to ask multiple questions to model owners to gather information, as depicted Figure 3 below, about the context of what this model does in the stack and to clarify how important it is to the business.\n\nUnderstanding this context is extremely important to the operational response: Depending on the importance of the model and the criticality of the surface it’s supporting, the response is going to differ in kind. When a model is an experiment serving a small percentage of the traffic, an appropriate response can be to end the experiment and reroute the traffic back to the main model (the baseline). But if there’s a problem with the baseline model that needs to be handled with urgency, it’s not possible to “just turn it off.” The engineer on call has to loop in the model owner, defeating the purpose of having a dedicated on-call.\n\nTo avoid holding up an operational response on a single POC, we needed a central source of truth for model importance and business function. What if the model is not available? What if 10 of these issues happen concurrently?\n\nWith the development of the model registry, we standardized the collection of model importance and business function information, ensuring most of our operational resources were going towards the most important models.\n\nWhat problems did the model registry solve?\n\nThe model registry is a system of record built on top of Configerator, Meta’s distributed configuration suite . This schematized ledger (see an example in Figure 4 and detailed further below) provides read-and-write access to operational data based on the inventory of production models. It’s a flexible and extensible foundation upon which one can build automation and tools to solve problems that are specific to individual organizations within Meta that are not served by the general tooling.\n\nAs Instagram scaled its investment in AI through rapid innovation in content recommendations, the number of models and AI assets grew; as a result, it has been increasingly important — but also increasingly difficult — to maintain a minimum standard for all of our models, as we lacked an authoritative source for the business context as well as for a model’s importance.\n\nIn creating the model registry, we set out to provide a structured interface for collecting business context via model types, importance via criticality, and additional metadata that would enable model understanding. Below, we’ll get into the model types, criticality, and automation we’ve built for this purpose.\n\nModel types\n\nAt a high level, model type describes the purpose for the ML workload where it represents a category or class of models that share a common purpose or are used in similar contexts. For example, we have “ig_stories_tray_mtml” which is a string attached to training flows, model checkpoints, inference services, and more. Put simply, a model type identifies for the reader this model’s purpose in the ranking funnel.\n\nLet’s break it down:\n\n“ig_stories_tray_mtml” → “ig” “stories” “tray” “mtml”\n\n“ ig ”: This model is an “ig” model as opposed to “fb” or “whatsapp”.\n\n“ stories ”: This model serves IG Stories.\n\n“ tray ”: This model serves in the main IG Stories tray (as opposed to stories in some other surface).\n\n“mtml”: This model is a multi-task-multi-label model, commonly used in late-stage ranking.\n\nWe can then use these model type strings to tag AI assets, and since they serve as proxies for business context, we can use them also for asset management, policy enforcement, analytics, and more.\n\nThe metadata entries in the model registry are anchored on two main types that describe model instances (ModelMetadata) as well as model types (ModelTypeMetadata). These types are made up of “core” attributes that are universally applicable, as well as “extended” attributes that allow different teams to encode their opinions about how these entries will inform operations. For example, in Instagram our extended attributes encode “baseline” and “holdout” model IDs, which are used in our ranking infrastructure to orchestrate ranking funnel execution.\n\nCriticality\n\nIn addition to defining business function, we had to establish clear guidelines for model importance. Within Meta, SEVs and services have a unified-importance tier system where the Global Service Index (GSI) records a criticality from TIER0 to TIER4 based on the maximum incident severity level the service can cause, from SEV0 as the most critical to SEV4 as simply a “heads up.” Since GSI criticality had social proof at the company, and infra engineers were familiar with this system, we adopted these criticalities for models and now annotate them at the model type and model level.\n\nNo longer would each team decide to raise their own model services to TIER1 for themselves, increasing the burden on all teams that support these models. Teams needed to provide an immediate response (available 24/7) on call and be able to prove that their models contributed meaningfully to critical business metrics to qualify for elevated monitoring.\n\nConfiguration structure as a foundation for automation\n\nOnce we had onboarded a critical mass of Instagram models to the model registry, we could begin to fully integrate with our monitoring and observability suite using our Meta-wide configuration solution, Configerator. With this, we could now have model performance monitoring and alerts that are fully automated and integrated with our tooling for SLIs called SLICK, dashboards that allow us to monitor models across many time series dimensions, and a suite of alerting specific to the model that is driven from the entries in the model registry.\n\nThis provided all our teams confidence that our monitoring coverage was complete and automated.\n\nLaunching\n\nWhile a point-in-time snapshot of models in production is great for static systems, Instagram’s ML landscape is constantly shifting. With the rapid increase of iteration on the recommendation system driving an increased number of launches, it became clear our infrastructure support to make this happen was not adequate. Time-to-launch was a bottleneck in ML velocity, and we needed to drive it down.\n\nWhat did the process look like?\n\nConventionally, services were longstanding systems that had engineers supporting them to tune. Even when new changes would introduce new capacity regression risks, we could gate this behind change safety mechanisms.\n\nHowever, our modeling and experimentation structure was unique in that we were planning for more rapid iteration, and our options were insufficient. To safely test the extent of load a new service could support, we would clone the entire service, send shadow traffic (i.e., cloned traffic that isn’t processed by our clients), and run multiple overload tests until we found a consistent peak throughput. But this wasn’t a perfect science. Sometimes we didn’t send enough traffic, and sometimes we’d send too much, and the amount could change throughout the day due to variations in global user behavior.\n\nThis could easily take two days to get right, including actually debugging the performance itself when the results weren’t expected. Once we got the result, we’d then have to estimate the final cost. Below (in Figure 5) is the formula we landed on.\n\nThe actual traffic shifting portion was tedious as well. For example, when we managed to fully estimate that we needed 500 replicas to host the new service, we might not actually have 500 spares lying around to do a full replacement, so launching was a delicate process of partially sizing up by approximately 20%, sending 20% of traffic over, and then scaling down the old service by 20% to reclaim and recycle the capacity. Rinse, repeat. Inefficient!\n\nAnd by the time we got to the end of this arduous process, the ordeal still wasn’t over. Each team was responsible for correctly setting up new alerts for their baseline in a timely fashion, or else their old models could and did trigger false alarms.\n\nHow does forcing virtual pools aid product growth?\n\nOne of the prerequisites for fixing competition for resources and unblocking productivity was to put up guardrails. Prior to this, it was “first come first served,” with no clear way to even “reserve” future freed capacity. It was also hard to reason about fairness from an infra perspective: Would it make sense to give each team equal pools, or give each individual person a maximum limit?\n\nAs it turned out, not all MLEs are experimenting at the same time, due to staggered progress on their work, so individual (per-engineer) limits were not ideal. One member might be in the experimentation stage and another might be training. So our solution was to provide bandwidth to each team.\n\nOnce each team — and therefore product — had quotas distributed, their launch policy became more clear cut. Some teams established free launching as long as the team was within quota. Others required no regressions in capacity usage. But mostly this unlocked our ability to run launches in parallel, since each one required much less red tape, and prioritization was no longer done at the org level.\n\nWhat other tooling improved launching?\n\nAs mentioned earlier, preplanning with capacity estimations was critical to understanding cost and ensuring reliability. We were often asked, Why not let autoscaling take care of everything? The problem was that each service could be configured slightly differently than a previously optimized service, or some architectural change could have affected the performance of the model. We didn’t have an infinite amount of supply to work with, so by the time we fully traffic-shifted everything over, we might find that we didn’t have enough supply. Reverting is costly, taking hours to get through each stage.\n\nBy doing capacity estimations in advance, this also allowed us and each team to accurately evaluate metric improvement versus cost. It might be worthwhile to double our costs if something would increase time spent on the app by 1%, but likely not for a 0.05% improvement where we could better spend that capacity funding another initiative.\n\nWith partners in AI Infra, we developed two major solutions to this process: offline performance evaluation and an automated launching platform.\n\nWe simplified determining performance of a new service using recorded traffic. Pre-recorded traffic was continuously collected into a data warehouse that the benchmarker could read from, and we’d spin up temporary jobs with this automation. One job would replay different levels of traffic continuously and send it to another job that was a clone of the existing experiment. By putting stoppers on desired latency and error rates, the tooling would eventually output a converged stable number that we could understand as the max load (see Figure 6).\n\nThe launch platform itself would input the numbers we captured from these tests, automatically collect demand data as defined, and run that same formula to calculate a cost. The platform would then perform the upscaling/downscaling cycle for teams as we shifted traffic.\n\nAnd finally, by leveraging the model registry, we were able to land this model change in code (see example in Figure 6), to help us better maintain and understand the 1000+ models within our fleet. Likewise, this bolstered our trust in the model registry, which was now directly tied to the model launch lifecycle.\n\nThis suite of launch automation has dramatically reduced the class of SEVs related to model launches, improved our pace of innovation from a few to more than 10 launches per week, and reduced the amount of time engineers spend conducting a launch by more than two days.\n\nModel stability\n\nAs the number of models in production increased, our organization started to feel the effects of an inconsistent measure of model health. While ranking models are run like any other distributed backend system (receive a request, produce a response), one may think a universal SLO that measures request success rate can suffice to capture holistic health. This is not the case for ranking models, as the accuracy of recommendations received carries significant importance to the end-user experience. If we consider a user who is a huge fan of golf but does not enjoy cooking content (see the “available & irrelevant” case in Figure 8 below), we see an example of this inaccuracy in practice. This is precisely what the model stability metric sought to capture.\n\nWhy is measuring ranking model reliability unique?\n\nRanking models, unlike traditional idempotent request/response backends, produce scores predicting user action given a set of candidates (PLIKE, PCOMMENT, PFOLLOW, etc.). These scores then combine and are used to determine which candidates are most relevant to an end user. It’s important that these scores accurately reflect user interest, as their accuracy is directly correlated to user engagement. If we recommend irrelevant content, user engagement suffers. The model stability metric was designed to make it easy to measure this accuracy and detect inaccuracy at our scale.\n\nLet’s discuss how this works.\n\nDefining model stability\n\nModels are complex, and they produce multiple output predictions. Let’s take a simplified example (shown in Figure 9 below) of a multi-task-multi-label (MTML) model predicting three actions:\n\nFor us to claim this model is stable, we must also claim that each underlying prediction is stable.\n\nWhen evaluating the accuracy of a ranking model’s predictions, we typically look at two metrics:\n\nModel calibration , which is based on observed real-world outcomes and answers the question, “Are we over- or under-predicting user action?” It is calculated as a ratio of predicted click-through-rate (CTR) and empirical CTR. A perfect predictor will have calibration centered at 1.\n\nModel normalized entropy (NE), which measures the discriminative power of a predictor, and answers the question, “How well can this predictor separate action from inaction?” It is calculated as a ratio of the average log-loss per impression to what the average log-loss per impression would be if we always predicted the empirical CTR. With NE, lower values are better, and an NE of 1 is equivalent to random predictions.\n\n(For more information regarding our choice of prediction evaluation metrics, please refer to the paper, “Practical Lessons from Predicting Clicks on Ads at Facebook.”)\n\nA model’s predictions are unstable when either calibration or NE are out of their expected healthy ranges. To determine what a healthy range is, we must look at each metric in real time, and Figure 10 below shows what these time series can look like:\n\nBy observing the trend of a healthy prediction, we can apply thresholds for our evaluation metrics. When these thresholds are breached, the underlying prediction is considered unstable.\n\nFrom here, we can define model stability as a binary indicator across a model’s predictions. It is 1 if all underlying predictions are stable, and 0 if any prediction is unstable. This is an extremely powerful method of reacting to real-time prediction instability as well as a tool for understanding trends in predictive health per model or across distinct products ranking funnels.\n\nOperationalizing model stability\n\nWith a real-time view on model predictive health, we can leverage this unified definition of model stability and apply it to all of our models in production, once again leveraging the model registry as a ledger to hold this important data. In Figure 11 below, we can see the addition of model stability metric metadata after we determined the expected thresholds.\n\nGiven the large number of models in production, each producing many predictions, building a portable definition of model health applicable to all of our ranking models represented an important milestone toward upleveling Instagram’s ML infrastructure maturity. This has unlocked our ability to build generic alerting to guarantee detection of our most important models becoming unstable, thereby moving us closer to mitigation when our recommendation system is at risk.\n\nSince the addition of these metrics and alerting, ML teams have discovered previously hidden issues within their models and addressed them faster than before, leading to higher-quality recommendations.\n\nKey takeaways\n\nIn our journey to scale Instagram’s algorithm to manage over 1000 models, we have learned several critical lessons that have shaped our approach and infrastructure. These takeaways not only highlight the challenges we faced but also underscore the strategies that led to our success.\n\nInfra understanding is the foundation to building the right tools\n\nA unified understanding of our infrastructure footprint was essential in developing the right tools to support our scaling efforts. By identifying the gaps and potential risks in our existing systems, we were able to implement solutions such as the model registry that significantly improved our operational efficiency and reliability posture.\n\nHelping colleagues move fast means we all move faster\n\nBy addressing the model iteration bottleneck, we enabled our teams to innovate more rapidly. Our focus on creating a seamless, self-service process for model iteration empowered client teams to take ownership of their workflows. This not only accelerated their progress but also reduced the operational burden on our infrastructure team. As a result, the entire organization benefited from increased agility and productivity.\n\nReliability must consider quality\n\nEnsuring the reliability of our models required us to redefine how we measure and maintain model quality. By operationalizing model stability and establishing clear metrics for model health, we were able to proactively manage the performance of our models. This approach enables us to maintain high standards of quality across our recommendation systems, ultimately enhancing user engagement and satisfaction.\n\nOur experience in scaling Instagram’s recommendation system has reinforced the importance of infrastructure understanding, collaboration, and a focus on quality. By building robust tools and processes, we have not only improved our own operations but also empowered our colleagues to drive innovation and growth across the platform.", "label": "non_personal"}
{"title": "Extending the Malbec subsea cable to Southern Brazil", "url": "https://engineering.fb.com/2025/05/22/connectivity/extending-malbec-subsea-cable-southern-brazil/", "content": "Meta is partnering with V.tal to extend the Malbec subsea cable to Porto Alegre, Brazil by 2027.\n\nWith this new extension, Malbec will become the first subsea cable to land in the state of Rio Grande do Sul, bringing more connectivity to millions of people in Southern Brazil and neighboring countries.\n\nMalbec will improve the scale and reliability of digital infrastructure in Porto Alegre, establishing it as a digital hub and improving online experiences across Southern Brazil, Argentina, Chile, Paraguay, and Uruguay.\n\nToday, we’re announcing the extension of the Malbec subsea cable to the city of Porto Alegre, Brazil. Developed by Meta, in partnership with V.tal, Malbec is a 2,500 km cable that entered service in 2021 to provide connectivity between the Southern Cone of South America and Brazil. The new extension will be operational in 2027 and will link Porto Alegre to the cities of Rio de Janeiro and São Paulo, Brazil and Buenos Aires, Argentina.\n\n“The expansion of Malbec to Porto Alegre is a milestone for connectivity in South America, benefiting millions of people in Brazil and positioning the capital of Rio Grande do Sul as the first major international digital hub in the south of the country,” explained Ana Luiza Valadares, Meta’s Public Policy Director, Connectivity & Infra, LatAm. “It will contribute to attracting digital infrastructure companies, lowering costs for companies and improving consumer services.”\n\nFelipe Campos, CEO of V.tal, added, “The impact of this project will be significant for the local digital economy, positioning Porto Alegre as a new connectivity hub. It will be a unique infrastructure that will attract the interest of operators and internet providers, as well as other submarine cable companies.\n\nIn addition, all the Southern Cone countries will benefit from this new ecosystem, not to mention the end users and companies who will have a better experience when using the internet and digital applications.”\n\nThis extension is one of the latest in Meta’s digital infrastructure investments to support growing demand for digital capacity, resilience, and global reach. Earlier this year, Meta also activated a Point of Presence (PoP) in Porto Alegre. PoPs facilitate the efficient delivery of content locally, which reduces the network management costs for internet service providers while improving the quality of experience for their customers. With the advent of AI and increasing demand for online services, digital infrastructure deployments play an important role in ensuring that the benefits of AI and other emerging technologies are available to everyone, regardless of where they live or work.\n\n“This investment in submarine connectivity, fully aligned with our Economic, Inclusive and Sustainable Development Plan, represents a strategic milestone for the state’s future,” said Rio Grand do Sul Governor, Eduardo Leite. “Furthermore, it fosters artificial intelligence projects, technologies that are already transforming the present and will define the future of innovation, a sector in which Rio Grande do Sul is a leader in Brazil, according to the ranking of state competitiveness.”\n\nMalbec will be the first international subsea cable to land in Rio Grande do Sul, bringing with it over 84 terabits of international capacity and direct connectivity to northern Brazil and Argentina. Like most subsea cables, local service providers will be able to acquire capacity on Malbec to serve additional bandwidth to millions of people in Brazil’s southern states. The providers will also extend Malbec’s capacity by connecting with providers in the neighboring countries of Argentina, Chile, Paraguay, and Uruguay, further positioning Brazil as a South American connectivity hub.", "label": "non_personal"}
{"title": "You have something to say, someone will listen", "url": "https://shellsharks.com/notes/2024/03/13/you-have-something-to-say-someone-will-listen", "content": "Mike Sass\n\n@shellsharks\n\n@stefan Oh man, the “Nothing to put there” crowd hits home. That and the much-related (but not listed here) “no one will care what I have to say / no one will read” crowd. To anyone who matches these descriptions or has these feelings, know that you…\n\nA. absolutely have something to say, and a blog is a great place to say it, and..\n\nB. The Internet is a huge place. People will find your site and read it, no matter how niche it is, or how poorly designed, or how seemingly poorly written. There’s no thought you could have that couldn’t educate someone else out there and nothing you could write about that there isn’t some community out there that would have similar thoughts.\n\nAs for the other reasons folks don’t have a site…\n\nGithub Pages (and some other platforms) is 100% free (so if expense is an issue then it doesn’t need to be) There are more hosting providers than ever these days, many of which make it dead simple to get started, so don’t let complexity be a stopping point. You can always start real simple and get more complex as you go. Time may be the biggest hurdle. I would just say that a blog isn’t something you need to maintain or publish regularly. Just put something up and post when you have time. Posts can be short, they can be sloppy, they can be whatever, Since you own the site, you can always edit something poorly written later too.\n\nFor everything else, you could check out my post about “Why I Blog. You Should Too!” for other advice/inspiration. Cheers!\n\nhttps://shellsharks.com/you-should-blog", "label": "non_personal"}
{"title": "A day in the life: Engineer onboarding at Dropbox", "url": "https://dropbox.tech/culture/a-day-in-the-life-engineer-onboarding-at-dropbox", "content": "We’re Adam Hood and Brian Amaratunga—two senior software engineers who joined Dropbox in 2021 as Virtual First employees. This means we spend most of our time working remotely, with physical studios reserved for in-person collaboration. When Dropbox became a Virtual First company in October 2020, it also meant reimagining the onboarding process to ensure new employees still had a high-quality experience—similar to what they would have gotten before in person. As two recent hires in Engineering, Product, and Design (EPD), we wanted to share our experience of what virtual onboarding at Dropbox is actually like. Adam: I’m an engineer on the Business Space Experience team. Our goal is to improve the workflows of teams that use Dropbox. My story with Dropbox goes back to my college days in 2015 when Dropbox would recruit very heavily on my campus. They would hand out Dropbox t-shirts like they were problem sets, and I suspect that at least a quarter of the undergraduate population had one, if not more. Fast forward to Spring 2021; my job in tech was satisfying enough, but I really wanted to look for ways to grow my career—and I came across Dropbox once again. I’ve always been impressed by their innovation. Early in my career I remember integrating zxcvbn, an open source password strength estimator developed by Dropbox, into a login page that I was developing! As I interviewed with Dropbox, I enjoyed thinking through the tough interview questions—which were unlike any I’d seen before—and I was encouraged by the answers the interviewers gave to some tough questions from me. They really seemed to believe in the company, and were excited about Virtual First. Brian: I’m an engineer on the Organized Experience team. Our mission is to give our users the tools to keep their files tidy and organized—or, as we like to say, “a place for everything and everything in its place.” I was looking to move on from my previous job in the healthcare software industry, so I made a profile on Hired.com. I was mostly expecting to learn about startups and smaller companies that were hiring, but Dropbox was one of the first companies to reach out. I’ve always heard great things about Dropbox’s engineering talent and culture—and with what I’d read about their vision of remote work, I was very interested in continuing the hiring process. I thoroughly enjoyed my interviews, and the questions felt challenging as opposed to just questions copied directly from LeetCode. In addition, I was impressed by all the engineers and recruiters I spoke with, which made me realize my initial impression of Dropbox was true. Once I matched with a team and learned more about the projects I’d be working on, it was a very easy choice to accept the offer. Before Dropbox, we both experienced in-person onboarding at other companies, but they were of opposite extremes. Adam had essentially no onboarding at his previous company. Brian was overloaded with classes but had little practical experience early on. But at Dropbox, we found a middle ground. New hires are called Droplets, and have 90 days to get up to speed on our culture, learn various teams' processes, and ship their first small project. Engineers get all the resources they need to succeed without being overwhelmed by information—or immediate pressure to deliver results.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images—plus generative AI capabilities across even more connected apps. See what's new →\n\nGetting started\n\nOur first day of onboarding was held over Zoom and led by our onboarding lead Alinane. We were both very impressed. As a group we talked through the Dropbox mission, our values, and business strategy. We also set up our laptops and signed-up for benefits. Spending the day on Zoom was exhausting at times, but our onboarding struck the right balance of being useful and informative without being overwhelming. One of the most useful things we received was an onboarding checklist of important tasks and a timeline for when we should do them, be it in our first week or first month. This allowed us to learn more about the company—from senior leadership to our users’ workflows—at our own pace. The next day, we met our onboarding buddies. Adam was paired with Bozhen, and Brian was paired with Jiayi. They were responsible for helping us get settled—and at first they bombarded us with loads of information. For example, we received dozens of links to various useful resources. We also learned everyone's names and roles—both on our team, as well as the teams we would work most closely with. It was overwhelming at first, but also very helpful to have a fellow engineer as a resource for whatever questions popped into our heads or to guide us through various processes. Our onboarding buddies were always gracious with their time, whether responding to messages over Slack or walking us through our team’s strategies over Zoom. Since we would primarily be working remotely, one of our top priorities was to set up our workspaces. Adam: I was very worried about getting used to the Touch Bar on my new Mac, so the first thing I wanted was a keyboard. Luckily, Dropbox made it easy for us to order gear like this through an internal website called Dropgear! I later added a standalone trackpad and a monitor to my desk. I really love my overall setup at this point. Brian: I also used Dropgear during my first week to get a monitor and a mouse! There were some issues with the supplier, so it took several weeks to get the mouse, but it took less than a week to get a 34-inch ultrawide monitor from Dropgear—without paying a penny! Dropbox also has something called a Perks Allowance which we can spend on pretty much anything we want (with a few exceptions). I joined a week before the quarter ended, but was still able to use the full amount to get a really nice adjustable standing desk and Steelcase chair.\n\nGetting to know the company\n\nOnboarding for new EPD hires consists of both interactive video calls and documentation for asynchronous (async) review—meaning at our own pace. There were around five video call sessions over the first couple weeks, each around one-to-two hours, while the rest of the information was documented in Dropbox Paper for us to peruse when we had time. This mix of video calls and async learning was our first introduction to “async by default,” one of the key tenets of Virtual First at Dropbox. This means Dropboxers default to sharing information async—communicating via Slack or email as much as possible, and reserving meetings and real-time communication for discussion, debates, and big decisions. Each video call session was designed to get us acquainted with how Dropbox works on a technical level, both as a product and as a company. We learned about how we use open-source software and best practices for committing and reviewing code. Particularly enjoyable was a talk that introduced the overarching architecture behind Dropbox. It was really nice to get a solid mental model of all the pieces—such as our hybrid approach between a monolithic and service-oriented architecture (Atlas), our async task framework (ATF), and our block storage solution (Magic Pocket)—and how they all fit together. There was enough information to get us started and show us where to go when we were ready to dig deeper. And it was perfectly reasonable if we didn’t recall everything that was covered; each session was recorded and made available to watch again later in case we needed to refresh our knowledge. The remaining onboarding sessions were written in Dropbox Paper for us to go over at our own pace. These self-guided lessons included a deep-dive into how our file system syncing and sharing works, and a guide to preparing for our twice-yearly performance reviews. Having so much material available async gave us a lot of flexibility when choosing when to learn and how we prioritized what to learn first. If either of us was feeling Zoom fatigue or needed some time to digest a more complex session from the morning, there was no need to jump straight into another potentially exhausting video call. We could take as much time as we needed and then jump into an async session when we were ready! At the same time, this flexibility allowed us to fit our lessons into whatever schedule worked for us and our managers. This meant we could start working on actual projects earlier in the onboarding process. We even got to ship code during our first week! Onboarding was also the perfect introduction to another core pillar of Virtual First at Dropbox: Core Collaboration Hours. These are four hour blocks when all meetings are supposed to be held. In North America, for example, Core Collaboration Hours are from 12 pm to 4pm ET. The remaining four hours of the day are meant for uninterrupted deep work—and everyone has the flexibility to work those hours whenever they feel most productive, because Dropbox is async by default. This helps reduce unnecessary meetings and help us make time for what matters most, from deep work and team building to time with family and friends.\n\nBrian’s calendar. Note that my team agreed to shift our Core Collaboration Hours to 1 pm to 5 pm ET.\n\nGetting to know the team\n\nWe were definitely worried about how we would build connections with our team in a virtual first environment. At our previous companies, we were able to walk up to people's desks or casually grab coffee, but that wouldn’t be possible here. However, with some gentle nudging from our managers, we realized we just had to be more purposeful about building those connections than we would have before. In our first couple of weeks, we grabbed quick, 15-20 minute one-on-one meetings with our teammates. We shared how we ended up at Dropbox, and heard how each of our coworkers arrived here as well. We were impressed by the diversity of experience of our various team members, who came from all sorts of unique backgrounds, and attended virtual social events to get to know them better. For example, Brian’s team has a weekly virtual board game event where the team gets together to play board games online. While meeting people remotely was a new experience, our colleagues did a great job of making us feel welcome from the start.\n\nGetting to know the work\n\nWe received our onboarding projects on our second day at Dropbox from our onboarding buddies Bozhen and Jiayi. Adam worked with Bozhen on updating modals in the Dropbox web client to improve the file upload flow, while Brian worked on adding some new naming convention rules to help users keep their file system organized. Bozhen and Jiayi did a great job breaking these projects down into well-defined, manageable tasks. They introduced us to the sprint planning processes, roughly estimating the time required and assigning due dates for our work. With our onboarding buddies handling the project planning, we could focus on getting to know the codebase—including the code commit and review process—without the added stress of vague or overcomplicated goals. Codelabs were one of the most useful resources in our onboarding. These are targeted tutorials that demonstrate how to accomplish different tasks within the Dropbox ecosystem—such as how to create a new API endpoint or how to create a feature gate. We found them to be extremely useful, not just as tutorials, but as reference materials when completing our first projects, ensuring we didn’t miss any steps. For example, Brian was able to ship code his very first week, and used the feature gate codelab to prevent people outside Dropbox from being able to access his work while it was still under development. Our onboarding projects lasted 6-8 weeks. Brian shipped the new naming convention rules to users during his seventh week at Dropbox! At the same time, Adam helped Bozhen complete the new file upload flow a couple weeks ahead of schedule, enabling the team to start user testing sooner. By this point, we were pretty comfortable with the Dropbox development ecosystem and started working with the rest of the team on other projects. Even though we were technically still onboarding and reading through the occasional onboarding doc, we felt like we were fully integrated into the team.\n\nFinal thoughts", "label": "non_personal"}
{"title": "Supercharged Developer Portals", "url": "https://engineering.atspotify.com/2024/4/supercharged-developer-portals", "content": "Today, we announced Spotify’s latest products and services for companies adopting Backstage, the open source framework for building internal developer portals (IDPs). Whether your company needs a highly customized IDP built from scratch or an out-of-the-box solution that’s ready to go ASAP, we want to make it easy for anyone to maximize the value they get from their Backstage developer portal. You can watch today’s Spotify for Backstage roadmap webinar below to see how our latest tools help other companies build like Spotify — by prioritizing developer experience and developer productivity.\n\nWatch Spotify’s Backstage team introduce our latest enterprise developer tools.\n\nEvery company is a tech company\n\nNo matter what business you’re in, you’re building software to run it and software to innovate it — so you’re really in the software business now. Whether you’re an online retailer or an airline or a bank, every company is a tech company. And all tech companies are facing the same problem: software development is getting more and more complex.\n\nWhy has it become so complex?\n\nYesterday, it was the proliferation of microservices, the explosion of cloud tooling, and the rise of DevOps — where you’re responsible for everything you build, from how a service is deployed to how secure it is, to how much it costs to run. Today, it’s generative A.I. and code generation tools that give teams the power to build faster than ever, while also increasing software sprawl at a previously unimaginable rate. And tomorrow? There will be a new technology, just as exciting and transformative as microservice, clouds, and LLMs, and just as likely to spur on more sprawl.\n\nBut it doesn’t even have to be something as sophisticated or world-changing as A.I. It could simply be that your parent company acquired another company, and so your engineering org just doubled in size and inherited an entirely separate tech stack. Suddenly, you have to make sense of a thousand new components built in a hundred new ways by a hundred new engineers — increasing the cognitive load of all your teams and testing the limits of context-switching and their patience.\n\nSo what’s the solution? This is where Backstage comes in…\n\nTaming the chaos and empowering teams with a Backstage IDP\n\nBackstage began life at Spotify as a software catalog — a directory of all the components our teams were building, from backend services to websites and libraries, emphasizing ownership and discoverability. And then we just kept adding to it. Because the platform had a simple, extensible plugin architecture, Backstage could evolve to tame chaos and complexity wherever it found it — until eventually it became a hub for everything engineering: aka, an internal developer portal (or IDP).\n\nAs a single pane of glass for all your tech infrastructure, a Backstage IDP streamlines software development — promoting the healthy parts of teams building software (ownership, tech standards, knowledge sharing, self-service, and collaboration), while minimizing the frustrating parts (silos, fragmentation, lack of discovery and documentation, dependency bottlenecks, and unclear standards).\n\nAt Spotify, we saw how much Backstage could improve developer effectiveness firsthand. Not only did Spotify’s frequent Backstage users build faster, but they also created higher-quality software compared to other developers. According to that internal study, our Backstage users are 2.3x more active in GitHub, create 2x as many code changes in 17% less cycle time, and deploy software 2x as often — and their software is deployed for 3x as long. We were onto something.\n\nAn open platform that prioritizes developer experience\n\nSince open sourcing the Backstage framework and donating it to the CNCF, the wider tech community has experienced the benefits of improved developer experience and productivity for themselves. As part of last month’s KubeCon in Paris, the CNCF held its third BackstageCon — a testament to the platform’s continued growth and popularity. The open source framework has over 2,200 project contributors and over 3,000 adopting companies. Based on that, we estimate over two million people are using our homegrown developer portal today.\n\nThe third-party Backstage ecosystem is thriving as well. Consulting companies like Frontside, Thoughtworks, and Adaptavist provide a range of services and support for Backstage adopters. Popular tech services continue to build plugins for Backstage, including new ones coming soon from Rootly, DX, LinearB, Snyk, and Swimm. And new companies are adopting Backstage every week.\n\nBut we want to see this ecosystem grow even further. And we think Spotify has a lot more to offer Backstage adopters of all shapes and sizes.\n\nDevEx at scale: Spotify’s secret sauce\n\nDeveloper experience has long been a main ingredient of Spotify’s success. And that has put us in a unique position to become a pioneer in the field of DevOps and platform engineering. Very few companies get to test what they build in a real-world environment like ours, at the global scale we operate in.\n\nWe build developer tools based on the insights we gain from:\n\nOur internal R&D community of demanding Spotifiers. We have hundreds of teams who depend on this software every day to get their jobs done, whether they’re shipping innovative features like AI DJ or fighting hordes of spam bots.\n\nA diverse open source community of contributors and end users. Spotify’s engineers maintain the open source framework together with a global community of thousands of contributors, while also supporting a Discord channel with over 13,000 members.\n\nOur enterprise customers and third-party partners. Other companies have been using our bundle of Spotify-built plugins and support services. So, we learn by seeing our tools and methods being applied to companies that can sometimes be very different from our own.\n\nAll of this feedback, experience, and understanding goes into what we ship. We think this unique perspective adds unique value to the Backstage ecosystem, something no one else can offer. And that’s what you’ll see in the products we announced today.\n\nSay hello to Spotify for Backstage\n\nDifferent companies need different kinds of IDPs. There are the DIY companies who need to build a highly customized IDP from the ground up, those who need an IDP that’s quick to set up and easy to maintain right out of the box, and many others who fall somewhere in between.\n\nOur products for Backstage are designed to supercharge your developer experience, no matter what kind of business you’re in, how big or small your company is, or where you are in your DevEx journey.\n\nIn the webinar, you’ll hear Spotify’s Backstage team talk about:\n\nSpotify Plugins for Backstage : Level up your custom portal with our newly updated bundle of proven, Spotify-built plugins, including Soundcheck, Role-Based Access Control, Skill Exchange, and Insights.\n\nSpotify Enterprise Support for Backstage : Let our Backstage experts help you build, maintain, or customize your portal with our personalized support and consulting services.\n\nSpotify Portal for Backstage: Get a full-featured IDP designed by Spotify, that’s both fast to get up and running, and easy to maintain. How fast? Watch the live demo of how you can set up Portal in less than five minutes, with no coding required.\n\nTo learn more about all of these products, watch today’s roadmap webinar or head to backstage.spotify.com to get started.", "label": "non_personal"}
{"title": "One of us", "url": "https://shellsharks.com/notes/2024/05/14/one-of-us", "content": "Mike Sass\n\n@shellsharks\n\nI recently came across this post where the author laments that the IndieWeb is “not for them”, simply because of their inability to implement Webmentions and thus (as they put it) the “IndieWeb is a social club for developers” only. (See also this one with respect to not feeling part of the “community”.)\n\nLet me be extremely clear. You do not need Webmentions to be part of the “IndieWeb”. The only real “requirement”, if you want to think of it that way, is to just have your own site on your own domain where you put your own stuff. That’s it. How you make it look, what you put on there, what fancy IndieWeb.org “building blocks” you decide to implement, doesn’t make your site any more or less “IndieWeb” than any other.\n\nIndieweb.org is a great resource. But I fear that for as much good as it does, it can do equal harm. On the surface, Indieweb.org has a fantastic message.\n\nWhat is the IndieWeb? The IndieWeb is a people-focused alternative to the “corporate web”. It is a community of independent and personal websites connected by open standards and based on the principles of: owning your domain and using it as your primary online identity, publishing > on your own site first (optionally elsewhere), and owning your content. Your content is yours When you post something on the web, it should belong to you, not a corporation. Too many companies have gone out of business and lost all of their users’ data. By joining the IndieWeb, your content stays yours and in your control. You are better connected Your articles and status messages can be distributed to any platform, not just one, allowing you to engage with everyone. Replies and likes on other services can come back to your site so they’re all in one place. You are in control You can post anything you want, in any format you want, with no one monitoring you. In addition, you share simple readable links such as example.com/ideas. These links are permanent and will always work.\n\nThe message is simple. The IndieWeb is about people. It’s about owning your own domain and putting your stuff there. Great! But just below the surface, as you start to click on some of the links, IndieWeb.org begins to preach complexity. IndieAuth, Webmention, Micropub, WebSub, Microsub, building blocks, microformats, backfeeds, etc… These convolutions are niche, techno-aristocratic IndieWeb fever dreams which discourage and alienate those desperate to break from corporate web-silos and start anew on a simpler, more human web. Ignore them.\n\nA note on “community”…\n\nIf you are reading this, or you have a personal site, you are already part of it. Full stop. You don’t need to be shy. You have things to say and people WANT to listen, they want to read, they want to connect. You don’t have to be an “influencer” or have a big following. Your writing doesn’t need to be AMAZING. You don’t need to write to an audience. Write for yourself. Then share it. People will be interested I promise you. The #indieweb revival is here and you are 100% invited! Tell your friends.\n\nSo, hopefully this message can make it out to those who need to hear it…\n\nYou want to be part of the IndieWeb? Be one of us? Get a domain. Put your site on it. Be yourself. Because the IndieWeb is about you.", "label": "non_personal"}
{"title": "Fixed-Power Designs: It’s Not IF You Peek, It’s WHAT You Peek at", "url": "https://engineering.atspotify.com/2024/5/fixed-power-designs-its-not-if-you-peek-its-what-you-peek-at", "content": "TL;DR Sometimes we cannot estimate the required sample size needed to power an experiment before starting it. To alleviate this problem, we could run a sequential test or an A/A test. However, sequential tests are typically less sensitive and introduce bias to the treatment effect estimator. Moreover, A/A tests prolong the duration of the experiment and still don’t guarantee that the resulting sample size calculation is accurate. In this blog post, we present highlights from our recent paper (Nordin and Schultzberg, 2024), where we introduce an alternative that we call “fixed-power design.” In a fixed-power design, you start the experiment without an estimated sample size, estimate the required sample size from the currently available outcome data in the experiment, and stop when your current sample size is larger than the required sample size. We show that fixed-power designs can be analyzed using nonsequential methods without any corrections. The point estimator is consistent, and the treatment effect confidence interval has asymptotically nominal coverage. Not all forms of peeking inflate the false positive rate of fixed-sample inference.\n\nIntroduction\n\nThere are many reasons why companies use online experiments. Reasons could be, for example, the following:\n\nTo identify the best version of a product\n\nTo quantify the impact of a product change\n\nTo detect regressions before a bug reaches all users\n\nOnline experiments let you do all these things while managing the risk of making the wrong decisions.\n\nAt Spotify, some of the goals of experimentation are to learn what works, how well it works, and stop the things that don’t work early on. However, the extent to which we can achieve these goals depends on the experimental design and analysis. For example, certain designs promote early stopping but pay a price in terms of power — with reduced chances overall of finding true effects. Other designs instead focus on maximizing power, but at the expense of prolonged runtime because they don’t allow early stopping. In the next section, we dig into the most common designs for A/B tests, discuss limitations of common approaches, and introduce a new design that mitigates some of those limitations.\n\nSequential experimental designs: more than just sequential testing\n\nTwo of the most fundamental concerns of experimental design are when to stop the experiment and when to analyze the results. Experimental designs can be roughly split into two categories: fixed-sample designs and sequential designs.\n\nFixed-sample designs\n\nIn fixed-sample designs, the experimenter leverages power analyses, also known as sample size calculations, to set a predetermined sample size. The power analysis produces a required sample size that the experiment should meet. If it does, the comparison will have high enough precision to limit the risk of missing effects of a certain size of interest. The experiment collects data until the predecided sample size is met, at which point the statistical analysis is performed.\n\nSequential designs\n\nIn sequential designs, the sample size isn’t predetermined.1 In principle, a sequential design is a design where users enter the experiment sequentially and the experiment stops according to a rule based on the available data. The sequential design uses a stopping rule that only indirectly determines the sample size as it’s being evaluated during the experiment. The most common sequential design, often simply called “sequential testing,” stops once the test detects a significant result.\n\nIn the context of online experimentation, many recommend using sequential tests to detect regressions but recommend against using sequential tests for the shipping decision, due to power and bias concerns with sequential tests. See, for example, Fan et al. (2004) and our previous blog post on comparing sequential testing methods.\n\nUsing a hybrid design\n\nIn practice, many companies, in fact, use a hybrid design. That is, the treatment effect is estimated and evaluated using statistical tests derived for fixed-sample designs. However, the design is sequential, because the experiment runs until the current sample size exceeds the estimated required sample size. The estimated required sample size, in turn, uses an estimate of the variance derived from the data collected so far in the experiment. In Nordin and Schultzberg (2024), we call this a fixed-power design — you sample new users until the power criterion, according to the currently available data — is met.\n\nThe graph above shows an example of how a fixed-power design can play out. To keep the graph easy to read, the sample size is kept small. In this case, the experiment stops very close to the true required sample size. In large samples (small-powered effects), as the required sample size estimator becomes precise, the region in which the sample size crosses the estimated required sample size will often be quite narrow.\n\nFixed-power designs: a summary of Nordin and Schultzberg (2024)\n\nIn Nordin and Schultzberg (2024), we investigate the properties of the difference-in-means average treatment effect estimator in sequential designs where the stopping rule is based on the precision of the treatment effect estimator. We express the precision in two ways, as the confidence interval (CI) width, and the current required sample size for a given hypothetical treatment effect. As shown in our paper, stopping based on the required sample size is equivalent to stopping on the confidence interval width, as it’s just a transformation of the variance of the treatment effect estimator.\n\nThese kinds of stopping rules based on precision are common in practice, with some experimentation vendors even selling them. However, to the best of our knowledge, the statistical implications of ‌precision-based stopping rules haven’t been rigorously investigated.\n\nThe fixed-power design can make your peeking problem alarm bell go off. It is, after all, a stopping rule that uses outcome data to determine whether to stop. This is what’s responsible for why we use sequential testing in the first place, and therefore, it’s not unreasonable to expect corrections to be required if you stop based on the required sample size, too. However, in Nordin and Schultzberg (2024), we show that not all stopping rules based on ‌outcome data are equally problematic for ‌statistical inference.Our research shows that functions of the sample variance are much less problematic than stopping based on, for example,‌ significance.\n\nWhat aspects of the outcome data we peek at determines the effects — if any — of peeking on an inference about the estimand we are interested in. In our paper, we show that under a fixed-power design, the following are true:\n\nThe difference-in-means estimator consistently estimates the average treatment effect.\n\nThe fixed-sample confidence interval for the average treatment effect has asymptotically correct coverage.\n\nThis means that in large samples, we can use standard inference even when we stop based on the current estimated required sample size. No further adjustments are necessary to guarantee correct inference.\n\nIn the paper, we also propose conservative finite-sample versions of the fixed-power design and the fixed-width confidence interval design.\n\nPre-experiment sample size calculation is hard\n\nFixed-power designs let us peek at the required sample size without adjusting inference. Why is this important? Can’t we use historical data for a power analysis to determine the required sample size?\n\nEstimating the required sample size during experiments as a complement to pre-experiment power analyses is often necessary, as historical data can fail to accurately describe the outcome distributions. At Spotify, for example, the diverse and ever-changing user base, especially in new markets and with new-user experiments, makes historical comparisons unreliable. Adding to that, historical data won’t reflect treatment effects since new variants haven’t been tested, and assuming homogeneous treatment effects across a diverse customer base is unrealistic. Users with different listening habits will likely respond differently to the same feature changes.\n\nLeveraging observed outcomes during experiments can enhance sample size accuracy and inform the experimenter early on if their initial planning is accurate. With the guarantees of fixed-power designs, we can plan according to a required sample size calculated before the experiment, revise it during the experiment, and finally stop at the right time — all while relying on standard fixed-sample inference.\n\nSequential testing versus fixed-sample testing\n\nSequential tests give valid inference under any stopping rule, so why not just rely on sequential tests and peek at the required sample size as much as we want?2\n\nAs have been discussed in many places (Larsen et al. 2024, our previous blog post), there are two main reasons:\n\nUnbiased point estimators. Sequential tests that stop on significance yield biased estimators that overestimate ‌effect size. Moreover, the idea of stopping on first significance is in stark contrast to the advice of many not to trust experiments with too low power. Power. In most situations, experiments must run for at least a given period. This could, for example, be to obtain data on users in the experiment for a sufficiently long time to rule out novelty effects. Another reason, common at Spotify, is to avoid issues with seasonal effects on weekdays. Using sequential testing in situations where we don’t intend to stop based on the first significance is a waste of power. If stopping is prohibited during a large part of the experiment, sequential tests that aren’t taking this into account will be highly conservative.\n\nWith the fixed-power design, we get the benefits from fixed-sample designs, but with the added ability to inform the stopping based on a continuous power analysis of the experiment.\n\nSequential design Traditional fixed-sample design Fixed-power design (Nordin and Schultzberg, 2024) – Sequential tests allow early stopping in experiments with a stopping rule based on significance or any other function of the data. – Sequential tests bound false positive rates and coverage at least at the intended level under early stopping. – Sequential tests are conservative if you always want to run the experiment until you reach a certain precision. This is because they adjust for early stopping on significance (even if you don’t use it). – Sequential tests (with early stopping) give biased difference-in-means estimators. – Fixed-sample tests require the sample size to be fixed ahead of time. – To achieve a certain precision, you need to estimate the variance of the outcome(s) from historical data before the experiment is started to plan the sample size. – The difference-in-means estimator is unbiased and the standard fixed-sample CI has the right coverage. – Fixed-power designs estimate the current required sample size from outcome data during the experiment. – Fixed-power designs stop when the current sample size is larger than the estimated required. – Under a fixed-power design, the standard difference-in-means estimator is consistent, and the fixed-sample CI has asymptotic nominal coverage.\n\nSummary\n\nIn the ever-evolving landscape of online experiments, determining the optimal time to stop an experiment remains a substantial challenge. Traditional methods, such as fixed-sample and sequential test designs, each have limitations. Fixed-sample designs predetermine the sample size but don’t allow adjustments based on incoming data, while sequential test designs can adjust but may affect the power and bias of the results.\n\nIn our recent paper, Nordin and Schultzberg (2024), we introduce an innovative approach called the “fixed-power design.” This method allows an experiment to start without a predefined sample size it needs to reach. Instead, the required sample size is estimated from ongoing outcome data, and the experiment concludes when the current sample size surpasses this estimate. Crucially, this design supports standard nonsequential inference, guaranteeing consistent point estimators and maintaining nominal coverage in confidence intervals. This means that the fixed-power design allows sequential stopping without losing the power benefits from nonsequential tests.\n\nThis design is particularly advantageous in environments like Spotify, where the user base is diverse and constantly changing. Traditional pre-experiment calculations based on historical data often fall short because they don’t account for the variability in treatment effects across different user segments or for new user experiences.\n\nThe fixed-power design provides a practical balance between the rigidity of fixed-sample designs and the flexibility of sequential test designs, providing reliable decision-making in product development. At the same time, many challenges remain. Although the fixed-power design makes it possible to do real-time adjustments to the required sample size, it’s problematic to not know the required sample size in the planning stage. At Spotify, where we run tens of thousands of experiments, there are always limitations to how large an experiment a team can run. If it’s detected during the experiment that the required sample size is much larger than the team had anticipated, it’s not always possible to run it longer or increase the proportion of the population that the experiment targets because of other conflicting experiments. In this situation, fixed-power designs offer a way to know early in an experiment if the data is in line with the pre-experiment power analysis.\n\nAcknowledgments: This blog post is based on a paper written in collaboration with Mattias Nordin, Department of Statistics, Uppsala University, Sweden.\n\nGet access to Spotify’s decision engine via Confidence. In Confidence, you can always access the current required sample size and current-powered effect while an experiment is live. This means that you can use a fixed-power design by simply starting the experiment using a fixed-sample design. As usual, we ensure that the statistics are all in order for any results we show you — so you can focus on building a great product. Want to learn more about Confidence? Check out the Confidence blog for more posts on Confidence and its functionality. Confidence is currently available in private beta. If you haven’t signed up already, sign up today, and we’ll be in touch.\n\n1 In some types of sequential tests, the maximum sample size needs to be predecided, but not the stopping sample size. 2 At least so-called always-valid sequential tests.", "label": "non_personal"}
{"title": "Nature Appreciation", "url": "https://shellsharks.com/blog-challenge-nature-appreciation", "content": "A Blog Questions Challenge\n\nThis week’s Blog Questions Challenge is called “Nature Appreciation”.\n\nHere are the questions…\n\nSilliest Animal I’ve Seen\n\nThe Bactrian Camel was the first come to mind. Camels have pretty silly (and grumpy) personalities as it is, but the Bactrian variety have wildly ridiculous camel humps. I got up close and personal with some at a wild live reserve kinda’ thing once.\n\nPlant Superpower\n\nHonestly I did some web searching to find cool “abilities” I would want to steal from a plant and came up kinda empty. Fire resistance, regeneration, growing super tall—these are all things that are kinda cool, but nothing specific stood out to me. So, as my pick, I’ve decided to go with the wise old Oak Tree. For any particular ability? Not really. I just like the idea of being a chill old Oak Tree that animals love to hang out with.\n\nFavorite Nature Sound\n\nI’d say it’s a close tie between the sound of a mountain stream and that of a rain storm with low, rolling thunder.\n\nThe Perfect Nature Spot\n\nEasy. Western-facing, mountain-side cabin. Conifers as far as the eye can see. A mountain creek babbles nearby. The glow of a camp fire flickers across my face as I watch the sun melt behind the distant peaks.\n\nThanks for reading!", "label": "non_personal"}
{"title": "Data Platform Explained Part II", "url": "https://engineering.atspotify.com/2024/5/data-platform-explained-part-ii", "content": "Check out Data Platform Explained Part I, where we started sharing the journey of building a data platform, its building blocks, and the motivation for investing into building a platformized solution at Spotify.\n\nIntroduction\n\nIn Data Platform Explained Part I, we shared the first steps in the journey to build a data platform, the insights that indicate it’s time to start building one, and how we are organized to succeed on it. In this article, we will take one step further into the why, what, and how of our data platform, introduce you to the domains underneath it that are responsible for the platform’s building blocks — here we will talk about scalability, the tooling we use and provide, alongside the value each building block brings to a data platform — and finally our strategy to navigate the complexity of a data ecosystem by building a strong community around it.\n\nData Collection\n\nWhen it comes to scalability, Spotify’s Data Collection platform collects more than 1 trillion events per day. Its event delivery architecture is constantly evolving through numerous iterations. To learn more about the event delivery evolution, its inception, and subsequent improvements, check out this blog post.\n\nData Collection is needed, so we can:\n\nUnderstand what content is relevant to Spotify users\n\nDirectly respond to user feedback\n\nHave a deeper understanding of user interactions to enhance their experience\n\nWhen a team at Spotify decides to instrument their functionality with event delivery, aside from writing code using our SDs, they only need to define the event schemas. The infrastructure then automatically deploys a new set of event-specific components (such as PubSub queues, anonymization pipelines, and streaming jobs) using K8 operators. Any changes to the event schema triggers the deployment of corresponding resources. Anonymization solutions, including internal key-handling systems, are covered in detail in this article.\n\nThe balance between centralized and distributed ownership allows most updates to be managed by consumers of the consumption dataset, without requiring intervention from the infrastructure team.\n\nToday, over 1800 different event types — or signals representing interactions from Spotify users — are being published. In terms of team structure, the data collection area is organized to focus on the event delivery infrastructure, supporting and enhancing client SDKs for event transmission, and building the high quality datasets that represent the user journey experience, as well as the infrastructure needed behind it.\n\nData Management & Data Processing\n\nOur Data Processing efforts focus on empowering Spotify to utilize data effectively, while Data Management is dedicated to ensuring data integrity through tool creation and collaborative efforts. With more than 38,000 actively scheduled pipelines handling both hourly and daily tasks, scalability is a key consideration. Data Management and Data Processing are essential for Spotify to effectively manage its extensive data and pipelines. It’s crucial to maintain data traceability (lineage), searchability (metadata), and accessibility, while implementing access controls and retention policies to manage storage costs and comply with regulations. These functions enable Spotify to extract maximum value from its data assets while upholding operational efficiency and regulatory standards.\n\nThe scheduling and orchestration of workflows are essential components of Data Processing. Once a workflow is picked up by the scheduler, it’s executed on BigQuery, or either Flink or Dataflow clusters. Most pipelines utilize Scio, a Scala API for Beam.\n\nData pipelines generate data endpoints, each adhering to a specific schema and possibly containing multiple partitions. These endpoints are equipped with retention policies, access controls, lineage tracking, and quality checks.\n\nDefining a workflow or endpoint involves custom K8 operators, which help us to easily deploy and maintain complex structures. In that manner, the resource definition lives in the same repo as the pipeline code and gets deployed and maintained by the codeowners.\n\nMonitoring options include alerts for data lateness, long-running or failing workflows, and endpoints. Backstage integration facilitates easy resource management, monitoring, cost analysis, and quality assurance.\n\nBuilding a culture around the data platform\n\nBuilding a data platform is non-trivial — it needs to be flexible enough to satisfy a variety of different use cases, aligning with cost effectiveness and return on investment goals, and at the same time keeping the developer experience lean. The data platform needs to be easy to onboard to and have seamless upgrade paths (nobody likes to be disrupted by platform upgrades and breaking changes). And the platform needs to be reliable — if teams have the expectation to build business critical logic on top of your platform, we treat the platform as a critical use case as well.\n\nThere are multiple ways to elevate engagement with your product:\n\nDocumentation (which is easy to find). We all have been in situations where, “I remember reading about it, but I don’t remember where.” It should be easier to find documentation than to ask a question (considering the waiting time).\n\nOnboard teams. There is no better way to learn about your product than to start using it yourself. Go to users and embed there. Learn about different use cases, make sure that your product is easy to use in all possible environments, and bring the learnings back to the platform.\n\nFleetshift the changes. People love evolving and making changes to their infrastructure and having the code being highlighted as deprecated, right? Not really. That is why we should automate all possible toils and migrations. Plan to deal with risks. Make time to support your customers.\n\nBuild a community where people are free to ask questions and where there are dedicated goalies to answer these questions. Answering community questions should not be left to free will, but should instead be encouraged and taken seriously. At Spotify we have a slack channel #data-support, where all data questions are addressed.\n\nWrapping up\n\nOur Data Platform has come a long way, and continues to evolve. At the very beginning, we were a few people, part of one team. We ran the pipelines on-premise, operating the largest Hadoop cluster in Europe. We are now 100+ engineers working on building the Spotify data platform on GCP, with data collection, management, and processing capabilities.\n\nThere is no formula or script to set up a data platform. A good way to start is by aligning your organizational needs with your investments. These needs become the drivers for your platform’s building blocks, and may change over time. Make sure the challenges are clear — define clear goals and set clear expectations — it will help you to have the right support from your organization and to be on the path for success.\n\nGet closer to your users, have a clear way through which customers and stakeholders can reach out and give you direct feedback — it will set the stage to create a community around your platform. Finally, you do not have to start big: just start somewhere then evolve, iterate, and learn.", "label": "non_personal"}
{"title": "Heroku", "url": "https://www.heroku.com/blog/feed/", "content": "@media only screen and (max-width: 414px) and (orientation: portrait) { #cover-image { width: 100%; } }\n\nAs part of our Blackhat Europe talk “Reverse Engineering and Exploiting Builds in the Cloud” we publicly released a new tool called Terrier.\n\nAnnouncing Terrier: An open-source tool for identifying and analysing container and image components.\n\nIn this blog post, I am going to show you how Terrier can help you identify and verify container and image components for a wide variety of use-cases, be it from a supply-chain perspective or forensics perspective. Terrier can be found on Github.\n\nIn this blog post, I am not going to go into too much detail about containers and images (you can learn more here) however it is important to highlight a few characteristics of containers and images that make them interesting in terms of Terrier. Containers are run from images and currently the Open Containers Initiative (OCI) is the most popular format for images. The remainder of this blog post refers to OCI images as images.\n\nEssentially images are tar archives that container multiple tar archives and meta-information that represent the “layers” of an image. The OCI format of images makes images relatively simple to work with which makes analysis relatively simple. If you only had access to a terminal and the tar command, you could pretty much get what you need from the image’s tar archive.\n\nWhen images are utilised at runtime for a container, their contents become the contents of the running container and the layers are essentially extracted to a location on the container’s runtime host. The container runtime host is the host that is running and maintaining the containers. This location is typically /var/lib/docker/overlay2/<containerID>/ . This location contains a few folders of interest, particularly the \"merged\" folder. The \"merged\" folder contains the contents of the image and any changes that have occurred in the container since its creation. For example, if the image contained a location such as /usr/chris/stuff and after creating a container from this image I created a file called helloworld.txt at the location /usr/chris/stuff . This would result in the following valid path on the container runtime host /var/lib/docker/overlay2/<containerID>/merged/usr/chris/stuff/helloworld.txt .\n\nNow that we have a brief understanding of images and containers, we can look at what Terrier does. Often it is the case that you would like to determine if an image or container contains a specific file. This requirement may be due to a forensic analysis need or to identify and prevent a certain supply-chain attack vector. Regardless of the requirement, having the ability to determine the presence of a specific file in an image or container is useful.\n\nTerrier can be used to determine if a specific image contains a specific file. In order to do this, you need the following:\n\nAn OCI Image i.e TAR archive A SHA256 hash of a specific file/s\n\nThe first point can be easily achieved with Docker by using the following command:\n\n$ docker save imageid -o myImage.tar\n\nThe command above uses a Docker image ID which can be obtained using the following command:\n\n$ docker images\n\nOnce you have your image exported as a tar archive, you will then need to establish the SHA256 hash of the particular file you would like to identify in the image. There are multiple ways to achieve this but in this example, we are going to use the hash of the Golang binary go1.13.4 linux/amd64 which can be achieved with following command on a Linux host:\n\n$ cat /usr/local/go/bin/go | sha256sum\n\nThe command above should result in the following SHA256 hash: 82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd\n\nNow that we have a hash, we can use this hash to determine if the Golang binary is in the image myImage.tar . To achieve this, we need to populate a configuration file for Terrier. Terrier makes use of YAML configuration files and below is our config file that we save as cfg.yml :\n\nmode: image image: myImage.tar hashes: - hash: '82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd'\n\nThe config file above has multiple entries which allow us to specify the mode that Terrier will operate in and in this case, we are working with an image file (tar archive) so the mode is image . The image file we are working with is myImage.tar and the hash we are looking to identify is in the hashes list.\n\nWe are now ready to run Terrier and this can be done with the following command:\n\n$ ./terrier\n\nThe command above should result in output similar to the following:\n\n$ ./terrier [+] Loading config: cfg.yml [+] Analysing Image [+] Docker Image Source: myImage.tar [*] Inspecting Layer: 34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf [*] Inspecting Layer: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560 [*] Inspecting Layer: 6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759 [!] Found file '6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759/usr/local/go/bin/go' with hash: 82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd [*] Inspecting Layer: a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1 [*] Inspecting Layer: aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545 [*] Inspecting Layer: d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c [*] Inspecting Layer: dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c\n\nWe have identified a file /usr/local/go/bin/go located at layer 6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759 that has the same SHA256 hash as the one we provided. We now have verification that the image “myImage.tar” contains a file with the SHA256 hash we provided.\n\nThis example can be extended upon and you can instruct Terrier to search for multiple hashes. In this case, we are going to search for a malicious file. Recently a malicious Python library was identified in the wild and went by the name “Jeilyfish”. Terrier could be used to check if a Docker image of yours contained this malicious package. To do this, we can determine the SHA256 of one of the malicious Python files that contains the backdoor:\n\n$ cat jeIlyfish-0.7.1/jeIlyfish/_jellyfish.py | sha256sum cf734865dd344cd9b0b349cdcecd83f79a751150b5fd4926f976adddb93d902c\n\nWe then update our Terrier config to include the hash calculated above.\n\nmode: image image: myImage.tar hashes: - hash: '82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd' - hash: 'cf734865dd344cd9b0b349cdcecd83f79a751150b5fd4926f976adddb93d902c'\n\nWe then run Terrier against and analyse the results:\n\n$ ./terrier [+] Loading config: cfg.yml [+] Analysing Image [+] Docker Image Source: myImage.tar [*] Inspecting Layer: 34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf [*] Inspecting Layer: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560 [*] Inspecting Layer: 6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759 [!] Found file '6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759/usr/local/go/bin/go' with hash: 82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd [*] Inspecting Layer: a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1 [*] Inspecting Layer: aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545 [*] Inspecting Layer: d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c [*] Inspecting Layer: dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c\n\nThe results above indicate that our image did not contain the malicious Python package.\n\nThere is no limit as to how many hashes you can search for however it should be noted that Terrier performs all its actions in-memory for performance reasons so you might hit certain limits if you do not have enough accessible memory.\n\nTerrier can also be used to determine if a specific image contains a specific file at a specific location. This can be useful to ensure that an image is using a specific component i.e binary, shared object or dependency. This can also be seen as “pinning” components by ensuring that you are images are using specific components i.e a specific version of cURL.\n\nIn order to do this, you need the following:\n\nAn OCI Image i.e TAR archive A SHA256 hash of a specific file/s The path and name of the specific file/s\n\nThe first point can be easily achieved with Docker by using the following command:\n\n$ docker save imageid -o myImage.tar\n\nThe command above utilises a Docker image id which can be obtained using the following command:\n\n$ docker images\n\nOnce you have your image exported as a tar archive, you will need to determine the path of the file you would like to identify and verify in the image. For example, if we would like to ensure that our images are making use of a specific version of cURL, we can run the following commands in a container or some other environment that resembles the image.\n\n$ which curl /usr/bin/curl\n\nWe now have the path to cURL and can now generate the SHA256 of this instance of cURL because in this case, we trust this instance of cURL. We could determine the hash by other means for example many binaries are released with a corresponding hash from the developer which can be acquired from the developer’s website.\n\n$ cat /usr/bin/curl | sha256sum 9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96\n\nWith this information, we can now populate our config file for Terrier:\n\nmode: image image: myImage.tar files: - name: '/usr/bin/curl' hashes: - hash: '9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96'\n\nWe’ve saved the above config as cfg.yml and when we run Terrier with this config, we get the following output:\n\n$ ./terrier [+] Loading config: cfg.yml [+] Analysing Image [+] Docker Image Source: myImage.tar [*] Inspecting Layer: 34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf [*] Inspecting Layer: 34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf [*] Inspecting Layer: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560 [*] Inspecting Layer: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560 [*] Inspecting Layer: 6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759 [*] Inspecting Layer: 6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759 [*] Inspecting Layer: a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1 [*] Inspecting Layer: a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1 [*] Inspecting Layer: aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545 [*] Inspecting Layer: aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545 [*] Inspecting Layer: d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c [*] Inspecting Layer: d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c [*] Inspecting Layer: dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c [*] Inspecting Layer: dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c [!] All components were identified: (1/1) [!] All components were identified and verified: (1/1) $ echo $? 0\n\nThe output above indicates that the file /usr/bin/curl was successfully identified and verified, meaning that the image contained a file at the location /usr/bin/curl and that the SHA256 of that file matched the hash we provided in the config. Terrier also makes use of return codes and if we analyse the return code from the output above, we can see that the value is 0 which indicates a success. If Terrier cannot identify or verify all the provided files, a return code of 1 is returned which indicates a failure. The setting of return codes is particularly useful in testing environments or CI/CD environments.\n\nWe can also run Terrier with verbose mode enable to get more information:\n\n$ ./terrier [+] Loading config: cfg.yml [+] Analysing Image [+] Docker Image Source: myImage.tar [*] Inspecting Layer: 34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf [*] Inspecting Layer: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560 [!] Identified instance of '/usr/bin/curl' at: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560/usr/bin/curl [!] Verified matching instance of '/usr/bin/curl' at: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560/usr/bin/curl with hash: 9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96 [*] Inspecting Layer: 6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759 [*] Inspecting Layer: a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1 [*] Inspecting Layer: aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545 [*] Inspecting Layer: d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c [*] Inspecting Layer: dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c [!] All components were identified: (1/1) [!] All components were identified and verified: (1/1)\n\nThe output above provides some more detailed information such as which layer the cURL files was located at. If you wanted more information, you could enable the veryveryverbose option in the config file but beware, this is a lot of output and grep will be your friend.\n\nThere is no limit for how many hashes you can specify for a file. This can be useful for when you want to allow more than one version of a specific file i.e multiple versions of cURL. An example config of multiple hashes for a file might look like:\n\nmode: image image: myImage.tar files: - name: '/usr/bin/curl' hashes: - hash: '9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96' - hash: 'aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545' - hash: '6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759' - hash: 'd4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c'\n\nThe config above allows Terrier to verify if the identified cURL instance is one of the provided hashes. There is also no limit for the amount of files Terrier can attempt to identify and verify.\n\nTerrier’s Github repo also contains a useful script called convertSHA.sh which can be used to convert a list of SHA256 hashes and filenames into a Terrier config file. This is useful when converting the output from other tools into a Terrier friendly format. For example, we could have the following contents of a file:\n\n8946690bfe12308e253054ea658b1552c02b67445763439d1165c512c4bc240d ./bin/uname 6de8254cfd49543097ae946c303602ffd5899b2c88ec27cfcd86d786f95a1e92 ./bin/gzexe 74ff9700d623415bc866c013a1d8e898c2096ec4750adcb7cd0c853b4ce11c04 ./bin/wdctl 61c779de6f1b9220cdedd7dfee1fa4fb44a4777fff7bd48d12c21efb87009877 ./bin/dmesg 7bdde142dc5cb004ab82f55adba0c56fc78430a6f6b23afd33be491d4c7c238b ./bin/which 3ed46bd8b4d137cad2830974a78df8d6b1d28de491d7a23d305ad58742a07120 ./bin/mknod e8ca998df296413624b2bcf92a31ee3b9852f7590f759cc4a8814d3e9046f1eb ./bin/mv a91d40b349e2bccd3c5fe79664e70649ef0354b9f8bd4658f8c164f194b53d0f ./bin/chown 091abe52520c96a75cf7d4ff38796fc878cd62c3a75a3fd8161aa3df1e26bebd ./bin/uncompress c5ebd611260a9057144fd1d7de48dbefc14e16240895cb896034ae05a94b5750 ./bin/echo d4ba9ffb5f396a2584fec1ca878930b677196be21aee16ee6093eb9f0a93bf8f ./bin/df 5fb515ff832650b2a25aeb9c21f881ca2fa486900e736dfa727a5442a6de83e5 ./bin/tar 6936c9aa8e17781410f286bb1cbc35b5548ea4e7604c1379dc8e159d91a0193d ./bin/zforce 8d641329ea7f93b1caf031b70e2a0a3288c49a55c18d8ba86cc534eaa166ec2e ./bin/gzip 0c1a1f53763ab668fb085327cdd298b4a0c1bf2f0b51b912aa7bc15392cd09e7 ./bin/su 20c358f7ee877a3fd2138ecce98fada08354810b3e9a0e849631851f92d09cc4 ./bin/bzexe 01764d96697b060b2a449769073b7cf2df61b5cb604937e39dd7a47017e92ee0 ./bin/znew 0d1a106dc28c3c41b181d3ba2fc52086ede4e706153e22879e60e7663d2f6aad ./bin/login fb130bda68f6a56e2c2edc3f7d5b805fd9dcfbcc26fb123a693b516a83cfb141 ./bin/dir 0e7ca63849eebc9ea476ea1fefab05e60b0ac8066f73c7d58e8ff607c941f212 ./bin/bzmore 14dc8106ec64c9e2a7c9430e1d0bef170aaad0f5f7f683c1c1810b466cdf5079 ./bin/zless 9cf4cda0f73875032436f7d5c457271f235e59c968c1c101d19fc7bf137e6e37 ./bin/chmod c5f12f157b605b1141e6f97796732247a26150a0a019328d69095e9760b42e38 ./bin/sleep b9711301d3ab42575597d8a1c015f49fddba9a7ea9934e11d38b9ff5248503a8 ./bin/zfgrep 0b2840eaf05bb6802400cc5fa793e8c7e58d6198334171c694a67417c687ffc7 ./bin/stty d9393d0eca1de788628ad0961b74ec7a648709b24423371b208ae525f60bbdad ./bin/bunzip2 d2a56d64199e674454d2132679c0883779d43568cd4c04c14d0ea0e1307334cf ./bin/mkdir 1c48ade64b96409e6773d2c5c771f3b3c5acec65a15980d8dca6b1efd3f95969 ./bin/cat 09198e56abd1037352418279eb51898ab71cc733642b50bcf69d8a723602841e ./bin/true 97f3993ead63a1ce0f6a48cda92d6655ffe210242fe057b8803506b57c99b7bc ./bin/zdiff 0d06f9724af41b13cdacea133530b9129a48450230feef9632d53d5bbb837c8c ./bin/ls da2da96324108bbe297a75e8ebfcb2400959bffcdaa4c88b797c4d0ce0c94c50 ./bin/zegrep\n\nThe file contents above are trusted SHA256 hashes for specific files. If we would like to use this list for ensuring that a particular image is making use of the files listed above, we can do the following:\n\n$ ./convertSHA.sh trustedhashes.txt terrier.yml\n\nThe script above takes the input file trustedhashes.txt which contains our trusted hashes listed above and converts them into a Terrier friendly config file called terrier.yml which looks like the following:\n\nmode: image image: myImage.tar files: - name: '/bin/uname' hashes: - hash: '8946690bfe12308e253054ea658b1552c02b67445763439d1165c512c4bc240d' - name: '/bin/gzexe' hashes: - hash: '6de8254cfd49543097ae946c303602ffd5899b2c88ec27cfcd86d786f95a1e92' - name: '/bin/wdctl' hashes: - hash: '74ff9700d623415bc866c013a1d8e898c2096ec4750adcb7cd0c853b4ce11c04' - name: '/bin/dmesg' hashes: - hash: '61c779de6f1b9220cdedd7dfee1fa4fb44a4777fff7bd48d12c21efb87009877' - name: '/bin/which' hashes: - hash: '7bdde142dc5cb004ab82f55adba0c56fc78430a6f6b23afd33be491d4c7c238b' - name: '/bin/mknod'\n\nThe config file terrier.yml is ready to be used:\n\n$ ./terrier -cfg=terrier.yml [+] Loading config: terrier.yml [+] Analysing Image [+] Docker Image Source: myImage.tar [*] Inspecting Layer: 34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf [*] Inspecting Layer: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560 [*] Inspecting Layer: 6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759 [*] Inspecting Layer: a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1 [*] Inspecting Layer: aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545 [*] Inspecting Layer: d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c [*] Inspecting Layer: dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c [!] Not all components were identifed: (4/31) [!] Component not identified: /bin/uncompress [!] Component not identified: /bin/bzexe [!] Component not identified: /bin/bzmore [!] Component not identified: /bin/bunzip2 $ echo $? 1\n\nAs we can see from the output above, Terrier was unable to identify 4/31 of the components provided in the config. The return code is also 1 which indicates a failure. If we were to remove the components that are not in the provided image, the output from the previous command would look like the following:\n\n$ ./terrier -cfg=terrier.yml [+] Loading config: terrier.yml [+] Analysing Image [+] Docker Image Source: myImage.tar [*] Inspecting Layer: 34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf [*] Inspecting Layer: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560 [*] Inspecting Layer: 6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759 [*] Inspecting Layer: a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1 [*] Inspecting Layer: aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545 [*] Inspecting Layer: d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c [*] Inspecting Layer: dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c [!] All components were identified: (27/27) [!] Not all components were verified: (26/27) [!] Component not verified: /bin/cat [!] Component not verified: /bin/chmod [!] Component not verified: /bin/chown [!] Component not verified: /bin/df [!] Component not verified: /bin/dir [!] Component not verified: /bin/dmesg [!] Component not verified: /bin/echo [!] Component not verified: /bin/gzexe [!] Component not verified: /bin/gzip [!] Component not verified: /bin/login [!] Component not verified: /bin/ls [!] Component not verified: /bin/mkdir [!] Component not verified: /bin/mknod [!] Component not verified: /bin/mv [!] Component not verified: /bin/sleep [!] Component not verified: /bin/stty [!] Component not verified: /bin/su [!] Component not verified: /bin/tar [!] Component not verified: /bin/true [!] Component not verified: /bin/uname [!] Component not verified: /bin/wdctl [!] Component not verified: /bin/zdiff [!] Component not verified: /bin/zfgrep [!] Component not verified: /bin/zforce [!] Component not verified: /bin/zless [!] Component not verified: /bin/znew $ echo $? 1\n\nThe output above indicates that Terrier was able to identify all the components provided but many were not verifiable, the hashes did not match and once again, the return code is 1 to indicate this failure.\n\nThe previous sections focused on identifying files in images, which can be referred to as a form of “static analysis,” however it is also possible to perform this analysis to running containers. In order to do this, you need the following:\n\nLocation of the container’s merged folder A SHA256 hash of a specific file/s\n\nThe merged folder is Docker specific, in this case, we are using it because this is where the contents of the Docker container reside, this might be another location if it were LXC.\n\nThe location of the container’s merged folder can be determined by running the following commands. First obtain the container’s ID:\n\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b9e676fd7b09 golang \"bash\" 20 hours ago Up 20 hours cocky_robinson\n\nOnce you have the container’s ID, you can run the following command which will help you identify the location of the container’s merged folder on the underlying host.\n\n$ docker exec b9e676fd7b09 mount | grep diff overlay on / type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/7ZDEFE6PX4C3I3LGIGGI5MWQD4: /var/lib/docker/overlay2/l/EZNIFFIXOVO2GIT5PTBI754HC4:/var/lib/docker/overlay2/l/UWKXP76FVZULHGRKZMVYJHY5IK: /var/lib/docker/overlay2/l/DTQQUTRXU4ZLLQTMACWMJYNRTH:/var/lib/docker/overlay2/l/R6DE2RY63EJABTON6HVSFRFICC: /var/lib/docker/overlay2/l/U4JNTFLQEKMFHVEQJ5BQDLL7NO:/var/lib/docker/overlay2/l/FEBURQY25XGHJNPSFY5EEPCFKA: /var/lib/docker/overlay2/l/ICNMAZ44JY5WZQTFMYY4VV6OOZ, upperdir=/var/lib/docker/overlay2/04f84ddd30a7df7cd3f8b1edeb4fb89d476ed84cf3f76d367e4ebf22cd1978a4/diff, workdir=/var/lib/docker/overlay2/04f84ddd30a7df7cd3f8b1edeb4fb89d476ed84cf3f76d367e4ebf22cd1978a4/work)\n\nFrom the results above, we are interested in two entries, upperdir and workdir because these two entries will provide us with the path to the container’s merged folder. From the results above, we can determine that the container’s merged directory is located at /var/lib/docker/overlay2/04f84ddd30a7df7cd3f8b1edeb4fb89d476ed84cf3f76d367e4ebf22cd1978a4/ on the underlying host.\n\nNow that we have the location, we need some files to identify and in this case, we are going to reuse the SHA256 hashes from the previous section. Let’s now go ahead and populate our Terrier configuration with this new information.\n\nmode: container path: merged #image: myImage.tar hashes: - hash: '82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd' - hash: 'cf734865dd344cd9b0b349cdcecd83f79a751150b5fd4926f976adddb93d902c'\n\nThe configuration above shows that we have changed the mode from image to container and we have added the path to our merged folder. We have kept the two hashes from the previous section.\n\nIf we run Terrier with this configuration from the location /var/lib/docker/overlay2/04f84ddd30a7df7cd3f8b1edeb4fb89d476ed84cf3f76d367e4ebf22cd1978a4/ , we get the following output:\n\n$ ./terrier [+] Loading config: cfg.yml [+] Analysing Container [!] Found matching instance of '82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd' at: merged/usr/local/go/bin/go with hash:82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd\n\nFrom the output above, we know that the container ( b9e676fd7b09 ) does not contain the malicious Python package but it does contain an instance of the Golang binary which is located at merged/usr/local/go/bin/go .\n\nAnd as you might have guessed, Terrier can also be used to verify and identify files at specific paths in containers. To do this, we need the following:\n\nLocation of the container’s merged folder A SHA256 hash of a specific file/s The path and name of the specific file/s\n\nThe points above can be determined using the same procedures described in the previous sections. Below is an example Terrier config file that we could use to identify and verify components in a running container:\n\nmode: container path: merged verbose: true files: - name: '/usr/bin/curl' hashes: - hash: '9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96' - name: '/usr/local/go/bin/go' hashes: - hash: '82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd'\n\nIf we run Terrier with the above config, we get the following output:\n\n$ ./terrier [+] Loading config: cfg.yml [+] Analysing Container [!] Found matching instance of '/usr/bin/curl' at: merged/usr/bin/curl with hash:9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96 [!] Found matching instance of '/usr/local/go/bin/go' at: merged/usr/local/go/bin/go with hash:82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91 dd3ff92dd [!] All components were identified: (2/2) [!] All components were identified and verified: (2/2) $ echo $? 0\n\nFrom the output above, we can see that Terrier was able to successfully identify and verify all the files in the running container. The return code is also 0 which indicates a successful execution of Terrier.\n\nIn addition to Terrier being used as a standalone CLI tool, Terrier can also be integrated easily with existing CI/CD technologies such as GitHub Actions and CircleCI. Below are two example configurations that show how Terrier can be used to identify and verify certain components of Docker files in a pipeline and prevent the pipeline from continuing if all verifications do not pass. This can be seen as an extra mitigation for supply-chain attacks.\n\nBelow is a CircleCI example configuration using Terrier to verify the contents of an image.\n\nversion: 2 jobs: build: machine: true steps: - checkout - run: name: Build Docker Image command: | docker build -t builditall . - run: name: Save Docker Image Locally command: | docker save builditall -o builditall.tar - run: name: Verify Docker Image Binaries command: | ./terrier\n\nBelow is a Github Actions example configuration using Terrier to verify the contents of an image.\n\nname: Go on: [push] jobs: build: name: Build runs-on: ubuntu-latest steps: - name: Get Code uses: actions/checkout@master - name: Build Docker Image run: | docker build -t builditall . - name: Save Docker Image Locally run: | docker save builditall -o builditall.tar - name: Verify Docker Image Binaries run: | ./terrier\n\nIn this blog post, we have looked at how to perform multiple actions on Docker (and OCI) containers and images via Terrier. The actions performed allowed us to identify specific files according to thei\n\nThe post Terrier: An Open-Source Tool for Identifying and Analyzing Container and Image Components appeared first on Heroku.", "label": "non_personal"}
{"title": "Technical Decision-Making in a Fragmented Space: Spotify In-Car Case Study", "url": "https://engineering.atspotify.com/2024/7/technical-decision-making-in-a-fragmented-space-spotify-in-car-case-study", "content": "Car rides have become connected and interactive these days with drivers jamming to music or catching up on podcasts or audiobooks while they’re on the road. A big portion of Spotify listening happens in the car — a key reason why it’s important for Spotify to ensure users have a smooth experience in the car, striking the perfect balance between safety, simplicity, and control. Achieving such balance is not easy due to various challenges: to maintain interoperability among different Spotify experiences, provide seamless usage on all kinds of screens, and manage drivers’ limited focus.\n\nInteroperability among different experiences\n\nThere are three ways to play Spotify in your car:\n\nUsing a Bluetooth connection: This method utilizes the car’s speakers as audio outputs. Users can execute basic playback operations such as play/pause, skip forward, and skip backward, using controls located on the car’s media head unit and steering wheel. Via a projected experience: Examples of this include Apple CarPlay and Android Auto, which project phone applications onto the car’s media unit. Through the projected Spotify app, users gain access to more-advanced functionalities such as browsing playlists, conducting searches, and utilizing a specialized Now Playing view, in addition to the basic playback controls mentioned above. With an embedded app: An increasing number of car manufacturers are integrating specialized operating systems into their vehicles’ hardware. These systems provide access to the car’s hardware and offer a dedicated media unit to the driver, featuring standalone media and navigation applications. Tesla and Android Automotive cars are prime examples of this integration.\n\nIt’s worth noting that most cars come equipped with Bluetooth capabilities, and on top of that, many modern vehicles offer projected and/or embedded experiences. This means that Spotify can provide users with multiple experiences within a single vehicle. Moreover, considering other surfaces where the app is utilized simultaneously (smart speakers, TVs, smartwatches, etc.), it is imperative that Spotify operates seamlessly across all platforms.\n\nDissimilar screen dimensions\n\nCars are equipped with screens of various sizes and dimensions, ranging from compact displays that barely show the track name to larger ones akin to a tablet. Adding to the complexity, there are many ways users interact with these screens. They might use physical buttons, rotary knobs, touch screens, buttons on the steering wheel, and even voice to manage music playback and adjust volume. On top of this, the car manufacturers also strive for fragmentation to better position their unique offerings, which makes tackling the automotive domain even harder for developers. Naturally, Spotify is expected to work seamlessly across all these different modalities for input.\n\nLimited focus from user\n\nWhen it comes to selecting what to play in the car, it’s a bit different compared to other situations, where the user can devote their full attention. Safety and navigation take top priority for drivers, which adds a layer of complexity to the experience. The principles are as follows:\n\nMinimizing cognitive load: Since selecting and playing music isn’t typically the first thing on a driver’s mind, Spotify needs to step in to make it as easy as possible. The goal is to reduce the mental effort required, ensuring that choosing music is simple and doesn’t distract from the primary task of driving. Reducing distractions: Our goal is to keep drivers focused on the road at all times. That means minimizing anything that could pull their attention away, like unexpected interruptions in audio playback or confusion about what’s playing. These distractions can cause drivers to glance away from the road, which must be avoided.\n\nBy addressing these challenges and ensuring that Spotify integrates seamlessly into the driving experience — without compromising safety — we can create a safer and more enjoyable audio experience for drivers.\n\nOur approach for technical decision-making\n\nGiven the assortment of challenges, the automotive domain is often seen as fragmented. Bringing together teams from different parts of the company is crucial to address these complexities, and effective modes of technical decision-making are paramount.\n\nBecause of our distributed workforce, Slack serves as our primary tool for communication. However, it’s not always the platform most conducive to in-depth technical discussions, as conversations tend to get lost in various threads, and the level of detail may not suit all stakeholders.\n\nFor technical decision-making, we prefer Request for Comments (RFC) documents. RFCs focus on a single problem, allow stakeholders to discuss that problem asynchronously, and refine the solution. You can find various templates for writing an RFC online, but the gist of it is that you start by describing a problem statement, give a little context around it, then move on to share your proposed solution for the problem along with some alternate solutions. When you’re ready, you can share the RFC document with stakeholders to get feedback on your suggested approach.\n\nMaking the most out of RFCs\n\nBreak down the problem (if applicable).\n\nTechnical problems come in all shapes and sizes. Some are small enough to conveniently fit one RFC, and others are hefty – the ones that cannot be squeezed into just one RFC. Addressing such issues with a single RFC can result in a long and complicated document, which is challenging to understand and can overwhelm the reader.\n\nWhen we encountered such large issues, we came up with a plan — to break down the problem into smaller, bite-size pieces. Therefore, we first presented a big-picture RFC. This gave all stakeholders a bird’s-eye view of the problem and framed potential high-level solutions, just to get the ball rolling. Once we had a decision on that, we started producing smaller, focused RFCs, each one diving deep into a specific issue, and shared these with the relevant teams.\n\nFor example: consider automatic playback when a car is connected. We expect that when a car is connected, Spotify should start playing music based on the user’s recent listening history. The bird’s-eye RFC on the topic contained high-level details like architectural design: Should this be a completely new system, or can we add this feature to an existing system? How would the API look? What content should be played? Etc. Once the architecture had been finalized, we moved on to more detailed RFCs covering topics such as the following:\n\nDetecting a car connection signal\n\nDetermining the circumstances when automatic playback should not be triggered\n\nExperimenting with different types of content to provide the best user experience\n\nFiltering content considering that the user is in the car\n\nThe audience for these RFCs varied based on the topic. For instance, content-related RFCs were shared only with content teams, while car-signal-related RFCs were shared with teams responsible for the connected accessories space.\n\nThis approach makes problem-solving way less daunting, keeps tasks manageable, and prevents information overload, ensuring that each stakeholder team can focus on the aspects most relevant to them.\n\nInclude diagrams and code snippets where feasible.\n\nWe often face the challenge of explaining significant changes in our systems, and clarity is crucial in these situations. So, what do we do? We rely on visuals — diagrams, code snippets, and even draft pull requests when necessary.\n\nDiagrams can easily show how things are connected, how components depend on one another, and how work flows, making complex ideas easier to understand. Apart from using architectural diagrams in our RFCs, we also use diagrams to show user journeys, with the aim of highlighting shortcomings in our current systems.\n\nAs for code, it is our trusty go-to for showing standardized examples for implementing specific protocols or functionalities. It works best when suggesting changes to other teams’ systems. By keeping things consistent across different setups or platforms, we make sure everyone is on the same page.\n\nRevisiting the example of automatic playback in the car, we created an RFC to highlight various situations where automatic playback could fail and proposed actions to prevent such failures. This RFC was intended for external teams that owned the systems we depended on. To clearly illustrate the problem areas, we used various user journey diagrams, providing a visual overview at a glance. Additionally, we included code examples to demonstrate the suggested improvements, making the technical solutions more concrete and actionable.\n\nUsing diagrams and code snippets throughout our RFCs provides readers with more context and details without unnecessary repetition, like giving readers a road map and a manual, all in one, making everything well-organized and easy to follow.\n\nUtilize RFCs for problem-solving and encouraging collaboration.\n\nSometimes we encounter issues that leave us even more puzzled. One such instance involved an issue with playback in the car that had been initially flagged as a bug. Turns out, it was a tricky one to reproduce in our usual testing setup, but as we dug deeper, it seemed like it could be part of a larger issue spanning multiple teams, with a more significant impact than we had initially anticipated.\n\nTo understand and map out the unknowns with the expertise of other related teams, we compiled everything that we knew into an RFC. We explained our part of the system and left sections for other affected or participating teams to fill out. This approach allowed us to present the problem comprehensively and provided a platform for all stakeholders to verify our findings, while also facilitating easier buy-in on proposed solutions.\n\nInstead of patching the issue locally, the collaborative process helped us to consider all the perspectives and assess the potential impact before sorting through possible fixes and prioritizing what to tackle first.\n\nUse RFCs to align between different functions.\n\nOne of the remarkable aspects of RFCs is that because of the way they are normally structured, they can bring people from different functions of a company together to tackle tough problems. It starts with defining the problem, diving into why the problem needs to be fixed, and finally, brainstorming ideas for how to solve it.\n\nThis method worked like a charm for us when we were deciding on new logging for a feature. We used RFCs to align between Product, Engineering, and Product Insights teams. Our product managers laid out what we needed and why, while the engineers and data scientists brainstormed ideas for how to make it happen.\n\nFor another example: implementing the support for features like AI DJ and Jam in cars. When implementing support for any external feature in the car, there are some key questions to ask, such as:\n\nDoes the current architecture support this new feature or does it need any enhancements?\n\nDoes the car integration need more information than currently exposed by core API?\n\nHow will the logging work to determine that the listening is happening in a car? And so on.\n\nRFCs always provide a good way for the engineers, product managers, and data scientists from different verticals of the company to come together and find optimal solutions to such issues.\n\nConclusion\n\nNavigating the integration of Spotify into the fragmented automotive ecosystem has posed its challenges, requiring intricate solutions. Throughout this journey, RFCs have proven to be a valuable tool for facilitating collaboration, iteration, and informed decision-making in the development and implementation of our systems.\n\nWhile we’ve fine-tuned RFCs to align with our organizational needs over the years, it’s essential to recognize that every team is unique. What proved effective for us may not necessarily yield the same results for others. Therefore, we encourage teams to experiment and explore the full potential of RFCs within their own contexts.\n\nBy embracing flexibility and innovation, teams can discover new avenues for leveraging RFCs, ultimately enhancing their collaboration and decision-making processes.\n\nSpecial thanks to Marko Tiidla and Robert Abramczyk for all the guidance and support during writing of this blog post.", "label": "non_personal"}
{"title": "Unlocking Insights with High-Quality Dashboards at Scale", "url": "https://engineering.atspotify.com/2024/8/unlocking-insights-with-high-quality-dashboards-at-scale", "content": "We have a lot of dashboards at Spotify. Our Insight teams and analysts from across the company are constantly whipping up new dashboards for stakeholders and themselves, helping answer those big data-driven questions every day. These dashboards tackle everything from frequently used key metrics to exploratory insights and operational reports. It’s clear to us: dashboards help us move faster and stay data-informed.\n\nIn 2023, Spotifiers — mostly data scientists and the like — created more than 4,900 dashboards in Tableau or Looker Studio. These dashboards were used by more than 6,000 Spotifiers in 2023. There is no centralized team at Spotify solely in charge of creating dashboards, but rather a lively and free market of dashboards — with the Analytics Platform team providing the scaffolding for this market — consisting of dashboard producers and dashboard consumers.\n\nAt the heart of our data visualization efforts, we lean on two main tools to amp up our dashboard game: Tableau and Google’s Looker Studio. With these in our toolbox, we’ve built additional systems and frameworks to maximize our ability to distribute, share, and discover insights like never before.\n\nThe core of Spotify’s dashboard platform\n\nLet’s talk Tableau\n\nTableau’s advanced customization options let us create detailed, visually stunning dashboards that are not just informative but can also provide unique user experiences. We can craft a highly specific experience for every use case — whether that means unique drill-down capabilities or a custom chart type. These dashboards are often thought of as full-fledged internal products, not just data artifacts, each with a distinct user base with specific needs.\n\nNext up: Looker Studio\n\nLooker Studio, our quick and nimble sidekick, offers a seamless integration with Google products that we regularly use, like BigQuery. With Looker Studio, our users can go from SQL to chart to dashboard in a heartbeat. It’s particularly popular among our engineering and product teams for its easy, intuitive user experience.\n\nBoth Tableau and Looker Studio are essential to our strategy, each offering unique advantages. Looker Studio excels in rapid, easy-to-use visualizations, while Tableau provides complex and detailed dashboards for more nuanced needs. This dual-tool approach allows every Spotify employee to choose the solution that best fits their specific requirements, and we provide instructions for getting started with both tools through Golden Path exercises.\n\nEmpowering dashboard producers with the Dashboard Quality Framework\n\nTo ensure our most widely shared dashboards are appropriately and accurately interpreted — and then showcased across the organization — we develop them in line with specific standards and practices.\n\nIt starts with dashboard production. Building a dashboard, whether that is in Tableau or in Looker Studio, can involve a lot of moving parts. It requires specific skills in data visualization and storytelling — skills that require expertise and time that many of us may not have. On top of that, Spotifiers looking to build a dashboard are likely in different parts of the org and not always in communication or enforcing the same standards. To alleviate quality challenges with distributed dashboard design — at scale — we developed a Dashboard Quality Framework.\n\nThere are two parts to the Dashboard Quality Framework:\n\nVital Signs: A set of automatic checks to ensure the dashboard is “alive and well.” These are enabled by the information we get from API and logs and include information on owner status, dashboard description, using LDAP groups for sharing, underlying data endpoint lifecycle, and whether updates and extract refreshes have been made recently. Spicy Dashboard Design Checklist: A checklist of data visualization and dashboard design best practices that could turn a vanilla report into a “spicy” dashboard. The checklist includes items related to visual design, usability, insights, and trust. This is a manual step that requires the human eye to determine. To enable this checklist at scale, we’ve turned this into a self-evaluating checklist that flags the dashboard when successfully completed.\n\nBased on the results of these measurements, a dashboard then receives one of these quality labels:\n\nLow — fails Vital Signs (regardless of Spicy Design Checklist outcome)\n\nHigh — passes Vital Signs\n\nGolden — passes Vital Signs and Spicy Design Checklist\n\nTen percent of eligible dashboards at Spotify have achieved Golden status — and we strive to keep more than 80% of all dashboards at High or above, informing dashboard owners when their dashboards don’t meet the Quality Framework criteria.\n\nBelow you will find the exact details of this framework — stealing is encouraged!\n\nThe ‘Spicy Design” Dashboard ChecklistDownload\n\nTo help our Tableau users, we’ve created a service that helps create and manage Tableau extracts.We’ve developed a SQL scheduling tool at Spotify that helps us schedule and run batch data workflows in Google BigQuery. We’ve enhanced this service to produce and publish Tableau .hyper files that can be used in dashboards on Tableau Cloud. These workflows run BigQuery SQL when upstream dependencies are available and are often faster, can load more data, and are more convenient to edit as the SQL is in .yaml files instead of embedded within Tableau.\n\nAnd to truly drive home the idea that dashboards are a product, we equip all dashboard owners with statistics and insights about their own dashboards — showing number of active users, weekly retention, users by org or job family, and an ability to easily email users (think version updates or deprecation announcements). This also allows people to consider if they should market the dashboard to a new audience, or if they should deprecate them when usage is very low. It’s all about empowering dashboards owners to make great data-informed decisions about their data and dashboard products.\n\nPowering dashboard consumption with Dashboard Portal\n\nInsightful and beautiful dashboards aren’t any good locked away — they need to be accessible to interested stakeholders!\n\nTo provide easy access for dashboard consumers, we developed the Dashboard Portal. Dashboard Portal is an internal site consisting of a searchable catalog of all published dashboards at Spotify, both from Tableau and Looker Studio. It includes ways to organize and curate groups of dashboards and adds extra context to dashboards for viewers.\n\nThe primary features of Dashboard Portal include:\n\nSearch: Users can search by title, description, and field names to find existing dashboards. No more “Do we have a dashboard about X ?”!\n\nCuration: Dashboard authors can create curated collections of dashboards that become a go-to hub for their teams. Think “The Ads Dashboard Hub,” containing all the dashboards the team believe are important for their stakeholders to be aware of.\n\nTrust and Quality: The Dashboard Quality Framework labels are displayed as a badge on top of the dashboard thumbnails and on all dashboard pages, giving viewers a quick understanding of a dashboard’s quality, and more importantly, whether they can confidently trust it. We’re able to spotlight and draw attention to our Golden dashboards — our cream of the crop! In doing this, we’ve seen that users are more likely to view dashboards with a Golden label.\n\nUbiquity: While Tableau and Looker Studio each have their own discovery mechanisms, we’ve found that having a Dashboard Portal that surfaces content from multiple tools enables the user to not have to think about where the dashboard lives, but rather just find it.\n\nEnhanced Context: Dashboards are embedded directly into the Portal and boosted with extra context: dashboard owner, last data refresh date, usage stats, and underlying SQL queries.\n\nConclusion\n\nOur robust approach to dashboard creation and management showcases our commitment to data democratization and informed decision-making across the company. Our sophisticated tools and frameworks ensure that every dashboard is not only visually appealing but also a well-oiled machine delivering actionable insights.\n\nSpecial thanks to Jacob Olsufka, Arielle Silverman, Pavlina Mitsou, Daniel Wolmerud, Alex Pavlov, Axel Örnefalk, Robert Hjortsmarker, and Abhishek Upadhyay.", "label": "non_personal"}
{"title": "Are You a Dalia? How We Created Data Science Personas for Spotify’s Analytics Platform", "url": "https://engineering.atspotify.com/2024/9/are-you-a-dalia-how-we-created-data-science-personas-for-spotifys-analytics-platform", "content": "On Spotify’s Analytics Platform, we’re dedicated to building products that empower data practitioners to discover, analyze, and share insights — think notebooks, dashboards, and metrics. To create solutions that connect with our users, it’s critical for us to first understand who our users are. However, the evolving landscape of data roles presents a challenge.\n\nIn the past decade, we’ve observed an industry trend toward consolidating diverse roles under the Data Scientist umbrella. Yet not all data scientists have the same needs, and many employees working with data don’t even hold that title. While we adopted the term “data practitioner” as a workaround, it proved too broad to effectively guide our product development. So we built a framework to better understand who our data practitioners are. Enter personas.\n\nTo create personas, we took a mixed-methods approach, meaning we blended qualitative insights from in-depth interviews with quantitative analysis of employee data, allowing us to identify clusters of shared behaviors. We pushed for personas that were both interpretable and applicable so that product leaders could easily grasp them and use them to make tactical decisions.\n\nThis work resulted in six distinct personas that compose the data practitioner universe at Spotify, along with several learnings relevant to all insights practitioners.\n\nCurious to see the personas? Skip to the end!\n\nWhat did we learn along the way?\n\nDon’t just facilitate collaboration between Data Science and Research by lowering barriers. Instead, erase the barriers between data science and user research. While mixed-methods research sometimes involves one method following another, we found success in having an iterative loop — using the qualitative work to inform the quantitative work, then the quantitative work to inform the qualitative work, and so on, until we’re satisfied with the end result. After a series of user interviews, we created our first version of the quantitative rules that would inform persona classification. We then continued to interview additional users to help us validate our classifications, add depth to our knowledge of the archetypes, and break ties to improve the rules we were developing. This became an iterative process of qualitative and quantitative validation, where each user interview we conducted resulted in either validation or tweaks to the proposed quantitative rules.\n\nDon’t just share findings and send the deck around. Create momentum by openly discussing tailored recommendations with your stakeholders and connecting them to product initiatives. A common challenge to foundational work can be activating the work and actioning on the findings. Stakeholders may be wondering: What do we do now that we know this ? To encourage action, we created dedicated slides in our share-outs highlighting the personas for each team to focus on. These recommendations were open for discussion but gave a good starting point to engage with the content. We also offered suggestions on how the personas could inform future product strategy. With these slides as primers, we witnessed our teams learn a common language and understanding of who we were building for. Teammates used the new vocabulary to talk about our users during sprints and planning. Over the first quarter, the framework went from being a conversation starter to becoming the foundation for how we set goals and metrics.\n\nDon’t just write the research plan and go off to work. Engage with your stakeholders from start to finish. For anything but the smallest questions, data scientists and user researchers have a tendency to withdraw into the research process and emerge at last with final answers, detached from changing requirements during the research period. Keeping our stakeholders involved throughout the project reduced this risk. Before kicking off the project, we aligned on the background and scope of the work and remained open to stakeholder feedback. To easily follow along during the project, we created a dedicated Slack channel for the research, to share updates and anecdotes on the fly. We also opened user interviews for observation across the entire product area, regardless of squad and role. We also found that foundational work with longer timelines benefitted from sharing early learnings at key milestones. Midway through the project, we shared “Postcards from the Field” — raw, unsynthesized qualitative data that kept stakeholders up to speed with the research. We also shared previews of the personas with our product area leads at later milestones to gather feedback.\n\nDon’t take a complex quantitative approach for its own sake. Keep your approach simple and clear. We used a rule-based approach to classify Spotifiers into personas. (For example: “Employee has used dashboarding tools more than 90% of data scientists.”) Keeping the approach simple made our results easily interpretable, transparent, and trustworthy to our stakeholders.\n\nDon’t just move on to another project. Instead, find opportunities to extend the work beyond the original scope of the project. To ensure the personas would be used among our teams going forward, we built a dataset to classify every internal user at Spotify into a persona on a daily basis. This dataset has not only become helpful for recruitment for future initiatives, but also adds more color to our product area metrics. We are now able to segment all our key results by these persona types and explore user behaviors for each group. Because this work is large in scale and built foundational knowledge across our product area, we wanted to make sure that the outcome was easy to refer to and share. To make the personas more memorable, we collaborated with our design partners to create Spotify-approved illustrations, adding a face to each one. Building strong visual artifacts encouraged greater longevity and shareability.\n\nYour turn: Which persona are you?\n\nIf you work in the field, you may be asking yourself: So which data persona am I?\n\nPaola the Product Strategist: Paolas are the most common data science persona at Spotify. They analyze and tell stories with data to help inform strategy. They typically partner closely with non-technical stakeholders. Outside Spotify they hold titles like Product Analyst or Data Scientist, Analytics.\n\nEli the Extensive Explorer: Elis are the second-most-common persona type at Spotify. These are data scientists, machine learning engineers, or research scientists who go deep on data to deliver complex research and models. This persona brings together the traditional data science role with MLE and similar roles.\n\nIvan the Influencer: Ivans are leaders who support their data teams and have influence over best practices and processes. They may be team leads or managers.\n\nDaryl the Data Viz Artist: Daryls use data to tell interactive, visual stories and spend most of their time creating dashboards to share with non-technical stakeholders. This role is a niche at many companies and may not even have its own title.\n\nSigrid the Systems Engineer: Sigrids are engineers who create infrastructure to help technical stakeholders leverage data for analysis. Data Engineer or Analytics Engineer are common titles.\n\nDalia the Data Dabbler: Dalias are not data scientists by title but work with data to perform analysis or make decisions. They typically explore straightforward data questions, often with technical support from the other personas.\n\n–\n\nDid one of the personas resonate with you? We found that most Spotify data practitioners related closely to one persona, but some employees reported fitting a hybrid of two personas.", "label": "non_personal"}
{"title": "Ken", "url": "https://www.heroku.com/blog/author/ken-w-alger/feed/", "content": "As a Python developer constantly striving for smoother workflows and faster iterations, the buzz around uv has definitely caught my attention. So, let’s roll up our sleeves and explore the benefits of using uv as your Python package manager, taking a look at where we’ve come from and how uv stacks up. We’ll even walk through setting up a project for Heroku deployment using this exciting new tool.\n\nA trip down memory lane: The evolution of Python package management\n\nTo truly appreciate what uv brings to the table, it’s worth taking a quick stroll down memory lane and acknowledging the journey of Python package management.\n\nIn the early days, installing Python packages often involved manual downloads, unpacking, and running setup scripts. It was a far cry from the streamlined experience we have today. Then came Distutils, which provided a more standardized way to package and distribute Python software. While a significant step forward, it still lacked robust dependency resolution.\n\nEnter setuptools, which built upon Distutils and introduced features like dependency management and package indexing (the foundation for PyPI). For a long time, setuptools was the de facto standard, and its influence is still felt today.\n\nHowever, as the Python ecosystem grew exponentially, the limitations of the existing tools became more apparent. Dependency conflicts, slow installation times, and the complexities of managing virtual environments started to become significant pain points.\n\nThis paved the way for pip (Pip Installs Packages). Introduced in 2008, pip revolutionized Python package management. It provided a simple and powerful command-line interface for installing, upgrading, and uninstalling packages from PyPI and other indices. For over a decade, pip has been the go-to tool for most Python developers, and it has served us well.\n\nBut the increasing complexity of modern Python projects, with their often intricate web of dependencies, has exposed some of pip’s performance bottlenecks. Resolving complex dependency trees can be time-consuming, and the installation process, while generally reliable, can sometimes feel sluggish.\n\nAnother challenge with the complexity of modern applications is package versioning. Lockfiles that pin project dependencies have become table stakes for package management. Many package management tools use them. Throughout the course of the evolution of package management in Python, we’ve seen managers such as Poetry and Pipenv, just to name a few. However, many of these projects don’t have dedicated teams. Sometimes this results in them not being able to keep up with the latest standards or the complex dependency trees of modern apps.\n\nThis is where the new generation of package management tools, like uv, comes into play, promising to address these very challenges, with a dedicated team behind them.\n\nEnter the speed demon: The benefits of using uv\n\nuv isn’t just another package manager; it’s built with a focus on speed and efficiency, leveraging modern programming languages and data structures to deliver a significantly faster experience. Here are some key benefits that have me, and many other Python developers, excited:\n\nBlazing Fast Installation: This is arguably uv’s headline feature. Written in Rust from scratch using a thoughtful design approach uv significantly outperforms pip in resolving and installing dependencies, especially for large and complex projects. The difference can be dramatic, cutting down installation times from minutes to seconds in some cases. This speed boost translates directly into increased developer productivity and faster CI/CD pipelines. Efficient Dependency Resolution: uv employs sophisticated algorithms for dependency resolution, aiming to find compatible package versions quickly and efficiently. While pip has made improvements in this area, uv’s underlying architecture allows it to handle complex dependency graphs with remarkable speed. This reduces the likelihood of dependency conflicts and streamlines the environment setup process. Drop-in Replacement for pip and venv : One of the most appealing aspects of uv is its ambition to be a seamless replacement for both pip and venv (Python’s built-in virtual environment tool). It aims to handle package installation and virtual environment creation with a unified command-line interface. This simplifies project setup and management, reducing the cognitive load of juggling multiple tools. Compatibility with Existing Standards: uv adheres to existing Python packaging standards like pyproject.toml (PEP 621). This means that projects already using these standards can easily adopt uv without significant modifications. It reads and respects your existing pyproject.toml files, making the transition relatively smooth. uv is built with a strong emphasis on modern packaging practices, encouraging the adoption of pyproject.toml for declaring project dependencies and build system requirements. This aligns with the direction the Python packaging ecosystem is heading. Improved Error Messaging: While pip’s error messages have improved over time, uv, being a newer tool, has the opportunity to provide more informative and user-friendly error messages, making debugging dependency issues easier. Potential for Future Enhancements: As a relatively new project with a dedicated development team, uv has the potential to introduce further optimizations and features that could significantly enhance the Python development experience. The active development and growing community support are promising signs.\n\nHow to use uv with Heroku\n\nNow, let’s put some of this into practice. Imagine we’re building a simple Python web application (using Flask, for instance) that we want to deploy to Heroku, and we want to leverage the speed and efficiency of uv in our development and deployment process.\n\nHere’s how we can set up our project:\n\n1. Install uv\n\nThere are a variety of options to install uv, depending on your operating system. For a full list, take a look at the official Installation Guide site. I’m going to install it using Homebrew:\n\n~/user$ brew install uv\n\n2. Create the project directory and initialize uv\n\n~/user$ uv init my-app ~/user$ cd my-app ~/user/my-app$ ls -a\n\nIn doing that, uv generates several project files\n\nmy-app/ ├── main.py ├── pyproject.toml ├── README.md └── .python-version\n\nOur main.py looks like this:\n\ndef main(): print(\"Hello from my-app!\") if __name__ == \"__main__\": main()\n\nWe can run this with the uv run main.py command which does a few things for us. In addition to actually running main.py and generating the “Hello from my-app!” output, uv also generates a virtual environment for the project and generates a uv.lock file which describes the project. More on that in a bit.\n\n3. Expanding the project… slightly.\n\nLet’s take this project a bit further and turn it into a Flask app that we can deploy to Heroku. We’ll need to specify our dependencies, Flask and Gunicorn for this example. We can do this using pyproject.toml .\n\nUsing pyproject.toml :\n\nThe uv generated pyproject.toml file looks like this:\n\n[project] name = \"my-app\" version = \"0.1.0\" description = \"Add your description here\" readme = \"README.md\" requires_python =\">=3.13\" dependencies = []\n\nTo add dependencies we use the uv add command.\n\n~/user/my-app$ uv add Flask ~/user/my-app$ uv add gunicorn\n\nThis accomplishes a couple of things:\n\nFirst, it adds those packages to the pyproject.toml file:\n\n[project] name = \"my-app\" version = \"0.1.0\" description = \"Add your description here\" readme = \"README.md\" requires_python =\">=3.13\" dependencies = [ \"Flask>=3.1.1\", \"gunicorn>=23.0.0\", ]\n\nSecond, it updates the uv.lock file for dependency management.\n\n4. Updating main.py\n\nLet’s update the code in main.py to be a basic Flask web application\n\nfrom flask import Flask app = Flask(__name__) @app.route('/') def hello_world(): return \"Hello from uv on Heroku!\" if __name__ == '__main__': app.run(debug=True)\n\n5. Preparing for Heroku deployment:\n\nHeroku needs to know how to run your application. For a Flask application, we typically use Gunicorn as a production WSGI server. We’ve already included it in our dependencies.\n\nWe’ll need a Procfile in the root of our project to tell Heroku how to start our application:\n\nweb: gunicorn main:app\n\nHere, app refers to the name of our Flask application instance in main.py .\n\n6. Deploying to Heroku:\n\nNow, assuming you are in the project working directory, have the Heroku CLI installed, and have logged in, you can create a local git repository and Heroku application:\n\n~/user/my-app$ git init ~/user/my-app$ heroku create python-uv # Replace python-uv with your desired app name ~/user/my-app$ git add . ~/user/my-app$ git commit -m \"Initial commit with uv setup\"\n\nThe Heroku CLI will create a remote in your git repository, but you check to make sure it’s there before you and push your code\n\n~/user/my-app$ git remote -v heroku https://git.heroku.com/python-uv.git (fetch) heroku https://git.heroku.com/python-uv.git (push) ~/user/my-app$ git push heroku main\n\nHeroku will detect your Python application, install the dependencies (based on .python-version , uv.lock and pyproject.toml ), and run your application using the command specified in the Procfile .\n\nThe future is bright (and fast!)\n\nWe’re excited to announce that Heroku now natively supports uv for your Python development. By combining uv’s performance with Heroku’s fully managed runtime, teams can ship faster with greater confidence in their environment consistency. This reduces onboarding time, eliminates flaky builds, and improves pipeline performance.\n\nWhile uv is still relatively new, its potential to significantly improve the Python development workflow is undeniable. The focus on speed, efficiency, and modern packaging standards addresses some of the long-standing frustrations with existing tools.\n\nAs the project matures and gains wider adoption, we can expect even more features and tighter integration with other parts of the Python ecosystem. For now, even the significant speed improvements in local development are a compelling reason for Python developers to start exploring uv.\n\nThe journey of Python package management has been one of continuous improvement, and uv represents an exciting step forward. If you’re a Python developer looking to boost your productivity and streamline your environment management, I highly recommend giving uv a try. You might just find your new favorite package manager!\n\nTry uv out on Heroku\n\nWhether you’re modernizing legacy apps or spinning up new services, uv gives you the speed and flexibility you need—now with first-class support on Heroku. Get started with uv on Heroku today.\n\nThe post Local Speed, Smooth Deploys: Heroku Adds Support for uv appeared first on Heroku.", "label": "non_personal"}
{"title": "How We Generated Millions of Content Annotations", "url": "https://engineering.atspotify.com/2024/10/how-we-generated-millions-of-content-annotations", "content": "With the fields of machine learning (ML) and generative AI (GenAI) continuing to rapidly evolve and expand, it has become increasingly important for innovators in this field to anchor their model development on high-quality data.\n\nAs one of the foundational teams at Spotify focused on understanding and enriching the core content in our catalogs, we leverage ML in many of our products. For example, we use ML to detect content relations so a new track or album will be automatically placed on the right Artist Page. We also use it to analyze podcast audio, video, and metadata to identify platform policy violations. To power such experiences, we need to build several ML models that cover entire content catalogs — hundreds of millions of tracks and podcast episodes. To implement ML at this scale, we needed a strategy to collect high-quality annotations to train and evaluate our models. We wanted to improve the data collection process to be more efficient and connected and to include the right context for engineers and domain experts to operate more effectively.\n\nTo address this, we had to evaluate the end-to-end workflow. We took a straightforward ML classification project, identified the manual steps to generate annotations, and aimed to automate them. We developed scripts to sample predictions, served data for operator review, and integrated the results with model training and evaluation workflows. We increased the corpus of annotations by 10 times and did so with three times the improvement in annotator productivity.\n\nTaking that as a promising sign, we further experimented with this workflow for other ML tasks. Once we confirmed the benefits of our approach, we decided to invest in this solution in earnest. Our next objective was to define the strategy to build a platform that would scale to millions of annotations.\n\nBuilding and scaling our annotation platform\n\nWe centered our strategy around three main pillars:\n\nScaling human expertise. Implementing annotation tooling capabilities. Establishing foundational infrastructure and integration.\n\n1. Scaling human expertise.\n\nIn order to scale operations, it was imperative that we defined processes to centralize and organize our annotation resources.\n\nWe established large-scale expert human workforces in several domains to address our growing use cases, with multiple levels of experts, including the following:\n\nCore annotator workforces: These workforces are domain experts, who provide first-pass review of all annotation cases.\n\nQuality analysts: Quality analysts are top-level domain experts, who act as the escalation point for all ambiguous or complex cases identified by the core annotator workforce.\n\nProject managers: This includes individuals who connect engineering and product teams to the workforce, establish and maintain training materials, and organize feedback on data collection strategies.\n\nBeyond human expertise, we also built a configurable, LLM-based system that runs in parallel to the human experts. It has allowed us to significantly grow our corpus of high-quality annotation data with low effort and cost.\n\n2. Implementing annotation tooling capabilities.\n\nAlthough we started with a simple classification annotation project (the annotation task being answering a question), we soon realized that we had more complex use cases — such as annotating audio/video segments, natural language processing, etc. — which led to the development of custom interfaces, so we could easily spin up new projects.\n\nIn addition, we invested in tools to manage backend work, such as project management, access control, and distribution of annotations across multiple experts. This enabled us to deploy and run dozens of annotation projects in parallel, all while ensuring that experts remained productive across multiple projects.\n\nAnother focus area was project metrics — such as project completion rate, data volumes, annotations per annotator, etc. These metrics helped project managers and ML teams track their projects. We also examined the annotation data itself. For some of our use cases, there were nuances in the annotation task — for example, detecting music that was overlaid in a podcast episode audio snippet. In these cases, different experts may have different answers and opinions, so we started to compute an overall “agreement” metric. Any data points without a clear resolution were automatically escalated to our quality analysts. This ensures that our models receive the highest confidence annotation for training and evaluation.\n\n3. Establishing foundational infrastructure and integration.\n\nAt Spotify’s scale, no one tool or application will satisfy all our needs — optionality is key. When we designed integrations with annotation tools, we were intentional about building the right abstractions. They have to be flexible and adaptable to different tools so we can leverage the right tool for the right use case. Our data models, APIs, and interfaces are generic and can be used with multiple types of annotation tooling.\n\nWe built bindings for direct integration with ML workflows at various stages from inception to production. For early/new ML development, we built CLIs and UIs for ad hoc projects. For production workflows, we built integrations with internal batch orchestration and workflow infrastructure.\n\nConclusion\n\nThe annotation platform now allows for flexibility, agility, and speed within our annotation spaces. By democratizing high-quality annotations, we’ve been able to significantly reduce the time it takes to develop new ML models and iterate on existing systems.\n\nPutting an emphasis from the onset on both scaling our human domain expertise and machine capabilities was key. Scaling humans without scaling technical capabilities to support them would have presented various challenges, and only focusing on scaling technically would have resulted in lost opportunities.\n\nIt was a major investment to move from ad hoc projects to a full-scale platform solution to support ML and GenAI use cases. We continue to iterate on and improve the platform offering, incorporating the latest advancements in the industry.\n\nAcknowledgments\n\nA special thanks to Linden Vongsathorn and Marqia Williams for their support in launching this initiative and to the many people at Spotify today who continue to contribute to this important mission.", "label": "non_personal"}
{"title": "Heroku CLI v10: Support for Next Generation Heroku Platform", "url": "https://www.heroku.com/blog/heroku-cli-v10-next-generation-heroku-platform/", "content": "The Heroku CLI is a vital tool for developers, providing a simple, extensible way to interact with the powerful features Heroku offers. We understand the importance of keeping the CLI updated to enhance user experience and ensure stability. With the release of Heroku CLI v10, we’re excited to introduce key changes that enhance the user experience and improve compatibility with the next-generation Heroku platform.\n\nHeroku CLI v10 introduces several breaking changes, updates for Fir (the next-generation Heroku platform), and overall performance improvements. Here’s a breakdown of the key features:\n\nNode.js 20 Upgrade :\n\nThe CLI has been upgraded to Node.js 20 , which brings performance improvements, security fixes, and better compatibility with modern development environments.\n\n: The CLI has been upgraded to , which brings performance improvements, security fixes, and better compatibility with modern development environments. Changes to heroku logs Command : The --dyno flag for specifying the process type and dyno name is now deprecated. Cedar apps : The –dyno flag will continue to work but will be deprecated. Fir apps : Users will need to use the new --process-type or --dyno-name flags instead.\n\n: Changes to ps:stop and ps:restart Commands : Positional arguments for process type and dyno name are deprecated in ps:stop and ps:restart. Cedar apps : Positional arguments will still work with a deprecation warning. Fir apps : Users must use the --process-type or --dyno-name flags.\n\n: Compatibility with Fir Apps :\n\nSeveral commands no longer work with Fir apps, including heroku run , heroku ps:exec , heroku ps:copy , heroku ps:forward , and heroku ps:socks . Users should now use heroku run:inside , which is designed to work with Fir apps but not with Cedar apps.\n\n: Several commands no longer work with Fir apps, including , , , , and .\n\nOpenTelemetry Support :\n\nA new suite of commands under heroku telemetry allows seamless integration with OpenTelemetry for Fir apps, enabling better observability. Check out our DevCenter documentation on telemetry drains for setup instructions.\n\n: A new suite of commands under allows seamless integration with OpenTelemetry for Fir apps, enabling better observability. Check out our DevCenter documentation on telemetry drains for setup instructions. Spaces Updates : The heroku spaces:create command now supports a new --generation flag, allowing users to specify whether they are creating a Cedar or Fir space. A pilot warning message will appear when Fir is selected. heroku spaces , heroku spaces:info and heroku spaces:wait now display the generation of the space.\n\n: Pipelines and Buildpacks : heroku pipelines:diff has been updated to support Fir generation apps. The heroku buildpacks command now lists buildpacks specific to Fir apps, based on the latest release.\n\n: Improved Logs for Fir Apps : heroku logs now includes a --tail flag for Fir apps to stream logs in real time. A new “Fetching logs” message is displayed as logs are being retrieved. Color rendering issues have been fixed to ensure consistent log output.\n\n:\n\noclif Upgrade : The CLI has been upgraded to oclif v4.14.36 , providing a more stable and modular architecture.\n\n: The CLI has been upgraded to , providing a more stable and modular architecture. GitHub Workflows: Updated GitHub workflows and actions now run on Node 20\n\nThe upgrade to Node.js 20 sets a solid foundation for future improvements and feature releases. These changes also help ensure that your Heroku CLI experience stays smooth and reliable as we continue to innovate.\n\nThe CLI is now ready for the next-generation Fir platform, making it easier to manage and deploy modern apps with enhanced observability, performance, and flexibility.", "label": "non_personal"}
{"title": "Simplifying JVM App Development with Heroku’s Buildpack Magic", "url": "https://www.heroku.com/blog/simplifying-jvm-app-development-herokus-buildpack-magic/", "content": "Heroku’s commitment to developer productivity shines through in its powerful buildpack system. They handle the heavy lifting of building your app, letting you focus on what matters most: writing code. A prime example is the Heroku Java buildpack, a versatile tool that simplifies deploying Java applications, especially those built with popular frameworks like Spring Boot, Quarkus, and Micronaut.\n\nOne of the core strengths of Heroku buildpacks is their automatic nature. They intelligently detect your application’s language and framework, fetching the necessary build tools and configuring the Heroku platform to run your app seamlessly. This means no more wrestling with server configurations or deployment scripts – Heroku handles it all.\n\nBeyond just building your application, our Java Buildpacks go a step further by understanding the nuances of different Java frameworks and tools. They automatically inject framework-specific configurations, such as database connection details for Postgres, eliminating the need for manual setup. This deep integration significantly reduces the friction of deploying complex Java applications. You don’t have to teach Heroku how to run your Spring Boot, Quarkus, or Micronaut app, and in some cases you don’t have to teach these frameworks how to interact with Heroku services either. In many cases, even a Procfile becomes optional! Let’s take a closer look at how the Java Buildpack supports these popular development frameworks.\n\nThe Maven or Gradle buildpack recognizes your Spring Boot project by inspecting your build definition, for example your pom.xml file. It automatically packages your app into an executable JAR, and configures the environment to run it using the embedded web server. It also helps out with Spring specific environment variables, ensuring your Spring Boot app behaves as expected when working with databases. Database connections are automatically configured using SPRING_ (such as SPRING_DATASOURCE_URL ), so Spring automatically detects your use of the Heroku Postgres add-on. This is also true for our Heroku Key Value Store add-on, whereby the SPRING_REDIS_URL environment variable is automatically set. In many cases, a Procfile isn’t necessary since the buildpack can determine the main JAR file automatically and adds a default process for your application such as: web: java -Dserver.port=$PORT $JAVA_OPTS -jar $jarFile .\n\nWe recently added support for Quarkus, known for its focus on developer joy. The Java (Maven) or Java (Gradle) buildpacks recognize your Quarkus project by inspecting your build definition. You can omit the usual Procfile and Heroku will default to Quarkus’ runner JAR automatically: java -Dquarkus.http.port=$PORT $JAVA_OPTS -jar build/quarkus-app/quarkus-run.jar .\n\nMicronaut, another framework designed for speed and efficiency, also benefits from the Java Buildpack’s intelligent automation. Just like with Spring Boot and Quarkus, database connections via DATABASE_URL and JDBC_DATABASE_URL and other environment-specific settings are handled automatically. You can omit the usual Procfile and Heroku will default to this automatically: java -Dmicronaut.server.port=$PORT $JAVA_OPTS -jar build/libs/*.jar .\n\nHeroku’s Language Runtime Metrics provide JVM metrics for your application, displayed in the Heroku Dashboard. This feature complements our existing system-level metrics by offering insights specific to your application’s execution, such as memory usage and garbage collection. These more granular metrics offer a clearer picture of your code’s behavior.\n\nHeroku automatically configures your application to collect these metrics via a light-weight JVM agent. No configuration necessary.\n\nApart from offering excellent support for building Java applications, Heroku offers support for additional JVM languages in Scala and Clojure. The buildpacks for those languages offer a similar suite of features backed by the sbt and Leiningen build tools.\n\nLooking through our Heroku customer stories we can see that our customers are enjoying our Java support, building engagement apps, helping with cloud adoption and driving growth by leveraging Heroku’s ability to elastically scale compute intensive workloads.\n\neCommerce Site & business platform : Improve user or employee engagement, and retention.\n\nCustomer Story: Goodshuffle Pro\n\n: Improve user or employee engagement, and retention. Customer Story: Goodshuffle Pro Cloud Adoption : Replatforming legacy back-end services.\n\nCustomer Story: Dovetail\n\n: Replatforming legacy back-end services. Customer Story: Dovetail Engines & APIs: Project customer growth.\n\nCustomer Story: PensionBee\n\nYes, and in fact, with any language supported by Heroku, it’s possible to extend your Flow, Apex, and Agentforce experiences with code, frameworks, and tools you’re familiar with from the Java ecosystem. Even if you haven’t used Java before, you’ll find its syntax similar to that of Apex. Check out our latest Heroku Eventing and AppLink pilot samples written in Java to find out more!\n\nHeroku’s Java buildpacks are powerful tools that significantly simplify deploying JVM applications. By automating the build process, injecting framework-specific configurations, and handling runtime setup, it lets developers focus on writing code, not managing framework configuration. Here are some useful articles the Heroku DevCenter site:\n\nTo submit feedback on your favorite JVM language, framework, or packaging tool, please connect with us via the Heroku GitHub roadmap. We welcome your ideas and suggestions.", "label": "non_personal"}
{"title": "Using GitHub Actions with Heroku Flow for additional Security Control", "url": "https://www.heroku.com/blog/using-github-actions-heroku-flow-additional-security-control/", "content": "Many advanced users want to use GitHub Actions with their applications on Heroku. Now there’s a straightforward way to use these great systems together, and to meet strong security and compliance requirements at the same time.\n\nHeroku is a powerful platform that offers robust CI/CD capabilities and secure, scalable environments for deploying applications. However, GitHub Orgs cannot be configured with Heroku IP ranges, which can be a requirement for some organizations’ security rules. While this is under consideration, we want to share an alternative that leverages GitHub Actions, Heroku’s ability to run arbitrary workloads and its powerful Platform API. If you’re looking to integrate private repositories with Heroku CI/CD, need strict control over source code sharing in regulated environments, or want to explore why running a GitHub Action Runner on Heroku might be more efficient, this blog post is for you!\n\nIn this post, we will share and describe a set of repositories and configuration instructions that enable you to leverage GitHub Actions—its features, dashboard reporting, and the ability to host the GitHub Runner on Heroku—for optimal execution and secure access to your private application code, all while still within the Heroku Pipeline dashboard experience.\n\nKeep in mind, while aspects of this solution are part of the core Heroku offering, the pattern explained in this article is provided as a sample only and the final configuration will be your responsibility. Additionally, while we have tried hard to ensure all aspects of the Heroku Flow feature work in this mode – there are some considerations to keep in mind we will share later in this blog and in the accompanying code.\n\nIn short, GitHub Actions are small code snippets—typically shell scripts or Node.js—that run in response to events like commits or PR creation. You define which events trigger your actions, which can perform various tasks, primarily integrating with CI/CD systems or automating testing, scanning, and code health checks. For secure access to your deployment platform and source code, GitHub requires you to host a Docker image of their Runner component. They also require that you routinely update your runner instances within 30 days of a new release. You can read more about GitHub Actions.\n\nHeroku supports these requirements in two key ways: hosting the runner and providing access to the build and deployment platform. First, Heroku can host official Docker images just as easily as application code, eliminating the need to manage infrastructure provisioning or scaling. Second, the Heroku Platform API enables GitHub Actions to automate managing Review Apps through an existing pipeline, move code through the pipeline, and trigger deployments—all while storing source code briefly on ephemeral storage. Additionally, this setup includes automation for the mandatory 30-day upgrade window for the GitHub Runner component, reusing the above mentioned features to schedule a weekly workflow that rebuilds its Docker image and autodeploy as a Heroku app, which removes the burden of having to update it manually. The following diagram outlines the location of application source repositories, the two GitHub actions required and within Heroku the configuration to run the GitHub runner and of course application deployments created by the actions – all within a Heroku Private space.\n\nThere are two repositories we are sharing that help you accomplish the above:\n\nHeroku-hosted runner for GitHub Actions – This project defines a Dockerfile to run a custom Heroku-hosted runner for Github Actions (see also self-hosted runners). The runner is hosted on Heroku as a docker image via heroku.yml . Once the self-hosted runner is running on Heroku you can start adding workflows to your private GitHub repositories to automate Heroku Review Apps creation and Heroku Apps deploy using the following action (that includes workflow examples).\n\n– This project defines a Dockerfile to run a custom Heroku-hosted runner for Github Actions (see also self-hosted runners). The runner is hosted on Heroku as a docker image via . Once the self-hosted runner is running on Heroku you can start adding workflows to your private GitHub repositories to automate Heroku Review Apps creation and Heroku Apps deploy using the following action (that includes workflow examples). Heroku Flow Action – GitHub Action to upload the source code to Heroku from a private GitHub repository using the Heroku Source Endpoint API. The uploaded code is then built to either deploy an app (on push , workflow_dispatch , and schedule events) or create/update a review app (on pull_request events such as opened , reopened , and synchronize ). Whenever a PR branch is updated, the latest commit is deployed to the review app if existing, otherwise a new review app is created. The Review App is automatically removed when the pull request is closed (on pull_request events when the action is ‘ closed ‘). The action handles only the above mentioned events to prevent unexpected behavior, handling event-specific requirements and improving action reliability.\n\nThe README files in the above repos go into more details – but at a high level what is involved is the following steps to setup GitHub Runner in Heroku and configure the GitHub Actions:\n\nIdentify a Private Space to run your the GitHub Runner in and resulting pipeline apps. Identify outbound IPs from the Private Space to be shared in your GitHub configuration. Deploy the GitHub Runner to the Private Space with your GitHub access token and Organization Name. Configure one or more private repos with the Heroku Flow Action and test by creating some PRs.\n\nWhat you should see from step 4 is the following:\n\nA new GitHub Action is started. A Review App within the configured Pipeline is automatically created upon the creation of a PR. From the Pipeline you can follow the application build as soon as it progresses.\n\nUsing this approach you are able to fully leverage your Heroku investment and reuse the features that the platform already offers, such as build and deploy capabilities and compute power, without needing to use external tools or platforms. In this way, your CI/CD is fully integrated where your apps are, a close integration that allows you to unlock scenarios where you can connect your Heroku-hosted runners to resources within or attached to your Private Space (e.g. secret managers, package registries, Heroku apps …) via Private Space peering or VPN connections.\n\nUsing a Private Space is not mandatory, but it adds a layer of security and provides a static set of public IP addresses that can be configured in your GitHub Org. Moreover, Private Spaces are now available for online customers too, so both verified Heroku Teams and Heroku Enterprises can leverage such an option.\n\nYour Heroku Flow can be improved and customized with ad-hoc steps and provide additional features such as manual and scheduled app builds and deploys via GitHub Actions “Run Workflow” and cron/scheduler.\n\nLast, but not least, your Heroku-hosted runners’ consumption is pro-rated to the second.\n\nThis solution complements your current Heroku development environments and can be used even for non-Heroku projects, a complete and enhanced delivery workflow is at your fingertips that in the future can open to other integration scenarios (e.g. on-premise GitHub Server, GitLab, Bitbucket …), while remaining on the platform you love!\n\nPlease keep the following considerations in mind as you explore this pattern and read the README files within the above repositories in detail to fully understand their value and implications. In summary, some key aspects to be aware of are as follows:\n\nFrom the Review App UI in the Heroku Pipeline, the URL used to allow easy access to the instance of the actual GitHub repository is not available in this configuration. You will instead need to relay the correct GitHub repository URL to your stakeholders in a different way.\n\nHeroku CI has a feature that automatically runs tests before creating Review Applications, among other features described here. In this configuration, the standard Heroku-managed Git repository is explicitly not used, and as such, tests are not run in the conventional way. If you need this capability, you could consider extending the action code to run your tests before every subsequent push to your GitHub repository.\n\nCurrently, this configuration is not compatible with Fir, our next-generation platform version.\n\nWhile we are using the core GitHub Runner software, we are not using the standard GitHub docker images: Rather, we create a custom image for you. It is up to you to test whether other GitHub actions you have created work as expected.\n\nPlease continue to review more detailed consideration information in the README’s here and here.\n\nGitHub Actions is a powerful tool for automating deployment pipeline tasks. Given the ability to reduce the toil of managing your own GitHub Runner instance along with the ease with which you can monitor the pipeline and let stakeholders test builds through Heroku Review Apps, we’re excited to share this pattern with our customers. As mentioned earlier, out-of-the-box support for this capability is under consideration by our product team. We invite you to share your thoughts on this roadmap item directly via commenting on the github issue. Meanwhile please feel free to fork and/or make suggestions on the above GitHub repos. We welcome your feedback, whether or not you’ve explored this approach. Finally, at Heroku, we consider feedback a gift. If you have broader ideas or suggestions, please connect with us via the Heroku GitHub roadmap.", "label": "non_personal"}
{"title": "How to Add the Moesif API Observability Add-On to Your Heroku Applications", "url": "https://www.heroku.com/blog/add-moesif-api-observability-to-your-heroku-applications/", "content": "With API-driven applications being increasingly common, understanding how your APIs are performing is crucial for success. That’s where the combination of Heroku and Moesif allows developers and their organizations to step up their observability game. In this blog, we will quickly examine how you can integrate Moesif with your Heroku app to begin monetizing and analyzing your API traffic. Let’s kick things off by taking a brief look at both platforms.\n\nWhat is Heroku?\n\nHeroku is a cloud-based Platform as a Service (PaaS) that enables developers to build, run, and scale applications entirely in the cloud. It abstracts away the complexities of infrastructure management, allowing you to focus on writing code and delivering features. Heroku supports many programming languages and frameworks, making it an excellent application development and deployment tool.\n\nWhat is Moesif?\n\nMoesif is an API analytics and monetization platform that provides deep insights into how your APIs are used and delivers the capabilities to monetize them easily. It captures detailed information about API calls, including request/response payloads, latency, errors, and user behavior. With Moesif, you can:\n\nMonitor API Performance : Identify bottlenecks, track error rates, and optimize response times.\n\n: Identify bottlenecks, track error rates, and optimize response times. Understand User Behavior : See how users interact with your APIs, which endpoints are most popular, and what features they’re utilizing.\n\n: See how users interact with your APIs, which endpoints are most popular, and what features they’re utilizing. Debug Issues : Quickly pinpoint the root cause of errors and resolve problems impacting your users.\n\n: Quickly pinpoint the root cause of errors and resolve problems impacting your users. Monetize Your API: Implement usage-based billing models and track revenue generated from your API.\n\nBenefits of Heroku and Moesif\n\nBy using the Moesif Heroku add-on, you can reduce the time to set up API Observability and ensure a seamless integration with Heroku. Billing and user management is automatically handled by Heroku which further reduces your overhead.\n\nWhy Are API Analytics Important?\n\nIf your app contains APIs, then a specialized API analytics platform must be used to truly understand how your APIs are used and what value they deliver. API analytics are essential for several reasons:\n\nImproved Performance : Identify and fix performance issues before they affect your users.\n\n: Identify and fix performance issues before they affect your users. Enhanced User Experience : Understand how users use your API and tailor it to their needs.\n\n: Understand how users use your API and tailor it to their needs. Data-Driven Decisions : Make informed API development, pricing, and business-level decisions based on usage data.\n\n: Make informed API development, pricing, and business-level decisions based on usage data. Increased Revenue: Monetize your API effectively by understanding usage patterns and identifying growth opportunities.\n\nAPI analytics allow you to examine not only the engineering side of the puzzle but also derive a large number of business insights.\n\nAdding Moesif to Your Heroku Application (Step-by-Step)\n\nWhen using Heroku and Moesif together, the process is straightforward and can be done directly through the Heroku CLI and UI. Below, we will go through how to add Moesif to your Heroku instance, including the steps in the UI or Heroku CLI, depending on your preferred approach.\n\nAdd Via CLI\n\nFirst, we will look at installing the Moesif Add-On through the CLI. For this, we assume that you:\n\nHave a Heroku account and an app running on Heroku\n\nYou have the Heroku CLI installed and logged into the application you want to add Moesif to.\n\nWith these prerequisites handled, you can proceed.\n\nInstall the Add-on\n\nMoesif can be attached to a Heroku application via the CLI:\n\nheroku addons:create moesif\n\nOnce the command is executed, you should see something similar to the following:\n\n----- > Adding moesif to sharp-mountain-4005 .. . done, v18 ( free )\n\nA MOESIF_APPLICATION_ID config var is added to your Heroku app’s configuration during provisioning. It contains the write-only API token that identifies your application with Moesif. You can confirm the variable exists via the heroku config:get command:\n\nheroku config:get MOESIF_APPLICATION_ID\n\nThis will print out your Moesif Application ID to the console, confirming it is correctly set in the config file.\n\nAdd Via UI\n\nAlternatively, you can install the Moesif Add-On through the Heroku Dashboard UI. For this, we assume that you:\n\nHave a Heroku account and an app running on Heroku\n\nAre logged into the Heroku Dashboard for the app you’d like to add Moesif to\n\nWith these prerequisites handled, you can proceed.\n\nInstall the Add-On\n\nWhile logged into the dashboard for the app you want to add Moesif to, on the Overview page, click the Configure Add-ons button.\n\nThis will then bring you to the Resources screen to view your current add-ons. In this instance, we have none. From here, click the Find more add-ons button.\n\nOn the next screen, where all available add-ons are listed, click Metrics and Analytics on the left-side menu. Locate the Moesif API Observability entry and click on it.\n\nOn the Moesif API Observability and Monetization overview page, click Install Moesif API Observability in the top-right corner.\n\nNext, you’ll be prompted to confirm the installation and submit the order. To confirm and install, click the Submit Order Form button to add Moesif to your Heroku app and activate your subscription.\n\nOnce complete, you’ll see that Moesif has been added to your Heroku instance and is ready for further configuration.\n\nInstall the server integration\n\nWith Moesif installed on our Heroku instance and subscription activated, we need to add Moesif to the application running on Heroku. To do this, go to your Heroku dashboard and open Moesif from under “Installed add-ons”\n\nOnce inside the Moesif application, the onboarding flow that appears will walk you through adding the Moesif SDK to your code.\n\nWhen initializing the SDK, use the environment variable MOESIF_APPLICATION_ID for the application ID. For example, in a Node application, you’d grab the Moesif Application ID by using process.env.MOESIF_APPLICATION_ID . This would be retrieved from the app config variables.\n\nLocal setup\n\nAfter you provision the add-on, you must replicate your config variables locally so your development environment can operate against the service.\n\nUse the Heroku Local command-line tool to configure, run, and manage process types specified in your app’s Procfile. Heroku Local reads configuration variables from a .env file. To view all of your app’s config vars, type heroku config. Use the following command for each value that you want to add to your .env file:\n\nheroku config:get MOESIF_APPLICATION_ID -s >> .env\n\nCredentials and other sensitive values should not be committed to source control. If you’re using Git, you can exclude the .env file by adding it to the gitignore file with:\n\necho .env >> .gitignore\n\nFor more information, see the Heroku Local article.\n\nUsing Moesif Dashboards\n\nOnce everything is configured, events should begin to flow into Moesif. These events can be used for analytics and monetization directly within the Moesif platform.\n\nKey Moesif Features to Leverage:\n\nLive Event Log : See individual API calls in real-time.\n\n: See individual API calls in real-time. Time Series Metrics : Track API traffic, latency, errors, and more over time.\n\n: Track API traffic, latency, errors, and more over time. Funnels and Retention : Analyze user journeys through your API.\n\n: Analyze user journeys through your API. Alerting : Get notified of critical API issues.\n\n: Get notified of critical API issues. Monetization: Drive revenue from your API calls using post-paid and pre-paid billing\n\nCheck out our docs and tutorials pages for all the ways you can leverage Moesif.\n\nOpen Through The Heroku CLI\n\nTo open Moesif, you can the following command cia the Heroku CLI:\n\nheroku addons:open moesif\n\nOr, from the Heroku Application Dashboard, select Moesif from the Add-ons menu.\n\nOnce logged in, you’ll have full access to the Moesif platform, which includes everything needed for extensive API analytics and monetization.\n\nTry It Out\n\nWant to try out Moesif for yourself? You can do so by following the directions above and creating an account through Heroku or sign-up directly. Powerful API analytics and monetization capabilities are just a few clicks away.", "label": "non_personal"}
{"title": "How I Improved My Productivity with Cursor and the Heroku MCP Server", "url": "https://www.heroku.com/blog/improved-my-productivity-cursor-and-heroku-mcp-server/", "content": "Generative AI has been one incredible tool to improve my productivity not only for work but for personal projects too. I use it every day, from generating stories and images for my online role playing games to solving code and engineering problems and building awesome demos. Lately I’ve leaned into Cursor as my go‑to AI coding companion. Its inline suggestions and quick edits keep me moving without context‑switching. Connecting Cursor to my apps through the Heroku MCP Server lets me perform actions like deploying or scaling, without leaving my code editor, making AI a first class citizen in the Heroku AI PaaS developer toolset. Using it along with the Heroku Extension for VS Code is a total win. In this article, I’ll show you how tying Cursor and MCP together saved me time and helped me focus on the parts of development I actually enjoy.\n\nWhat is Model Context Protocol?\n\nModel Context Protocol (MCP) is an open standard from Anthropic that defines a uniform way for my AI assistant (like Cursor) to talk to external tools and data sources. Instead of juggling custom APIs or integrations, MCP wraps up both the “context” my code assistant needs (code snippets, environment state, database schema) and the “instructions” it should follow (fetch logs, run queries, deploy apps) into a single, predictable format—much like a USB‑C port lets any device plug into any charger without extra adapters, Model Context Protocol is the universal connector for your AI tools and services.\n\nUnder the hood, MCP follows a simple client–server model:\n\nHost: my editor or chat interface (e.g., Cursor) that decides what my assistant can access\n\nClient: the small bridge component that keeps a live connection open\n\nServer: a lightweight service exposing specific capabilities (APIs, database calls, shell commands) in MCP’s schema\n\nWhen I ask Cursor to “scale my Heroku dynos” or “pull the latest customer records,” it sends an MCP request to the right server, gets back a structured response, and I can keep coding without switching contexts or writing new integration code.\n\nAI Dev Tools I Use Everyday\n\nWhen I’m not on stage presenting or behind a mic recording a podcast, I’m usually in VS Code building JavaScript demos that highlight Heroku’s capabilities and best practices. Backend work is my comfort zone, front-end and design aren’t, so I lean on AI to bridge those gaps. Given a design spec (from Figma for example), I can get a frontend prototype in minutes, instead of writing HTML/CSS at hand, making the interaction with the design team straightforward. I’ve tried Gemini for ideation, and ChatGPT and Claude for debugging and refactoring code.\n\nLately, though, Cursor has become my go-to IDE. Its inline LLM suggestions and agentic features let me write, test, design, and even deploy code without leaving the editor. Pairing Cursor with different MCPs means that I can remain on the IDE, it keeps me focused, cuts out needless context-switching, and helps me ship demos faster.\n\nHere, I share a list of the MCPs I use and how they improve my productivity:\n\nHeroku MCP Server\n\nAll my demos go straight to Heroku. With the Heroku extension for VS Code, I rarely leave my editor to manage apps. And thanks to the Heroku MCP Server, my AI assistant now deploys, scales dynos, fetches logs, and updates config, all without opening the dashboard or terminal.\n\nTo install it in your IDE, start by generating a Heroku Authorization token:\n\nheroku authorizations:create --description \"Heroku MCP IDE\"\n\nAlternatively, you can generate a token in the Heroku Dashboard:\n\nGo to Account Settings → Applications → Authorizations and click Create new authorization. Copy the token you receive.\n\nThen open your Cursor mcp.json and add the following JSON configuration with the previously generated Heroku Authorization token:\n\nNote: Make sure you have npx installed a global command in your operative system, npx is part of Node.js.\n\n{ \"mcpServers\": { \"heroku\": { \"command\": \"npx\", \"args\": [ \"-y\", \"@heroku/mcp-server\" ], \"env\": { \"HEROKU_API_KEY\": \"\" } }, } }\n\nCheck the project README for setup instructions on Claude Desktop, Zed, Cline, Windsurf, and VS Code.\n\nLangChain MCPDoc\n\nMany projects have started to adopt the /llms.txt file, which serves as a website index for LLMs, providing background information, guidance, and links to detailed markdown files. Cursor and other AI IDEs can use the llms.txt file to retrieve context for their tasks. The LangChain MCPDoc offers a convenient way to load llms.txt files, whether they are located remotely or locally, making them available to your agents.\n\nDepending on the project I’m working on, I rely on this MCP to fetch documentation, especially when I’m building other MCPs, I use the recommended https://modelcontextprotocol.io/llms.txt file, or if I’m using LangChain JS to build agentic applications with Node.js, I use https://js.langchain.com/llms.txt.\n\nI have also created my own Heroku llms.txt file, which you can download locally and use for your Heroku-related projects.\n\nHere is how you can setup the LangChain MCPDoc in Cursor:\n\nNote: Make sure you have uvx installed as a global command in your operative system, uvx is part of uv, a Python package manager.\n\n{ \"mcpServers\": { \"heroku-docs-mcp\": { \"command\": \"uvx\", \"args\": [ \"--from\", \"mcpdoc\", \"mcpdoc\", \"--urls\", \"HerokuDevCenter:file:///Users/jduque/AI/llmstxt/heroku/llms.txt\", \"--allowed-domains\", \"*\", \"--transport\", \"stdio\" ] }, \"modelcontextprotocol-docs-mcp\": { \"command\": \"uvx\", \"args\": [ \"--from\", \"mcpdoc\", \"mcpdoc\", \"--urls\", \"ModelContextProtocol:https://modelcontextprotocol.io/llms.txt\", \"--allowed-domains\", \"*\", \"--transport\", \"stdio\" ] } } }\n\nFigma MCP Server\n\nAnother one of my favorites is the Figma MCP Server. It allows Cursor to download design data from Figma. I just copy and paste the link of the frame in Figma that I want to implement into my Cursor chat, and with the right prompt, it does the magic. For example, recently I had to implement our brand guidelines on a demo I’m working on, and I just pasted the frame that contains the Heroku color palette. It created a Tailwind CSS theme with the right styles. Without this tool, I’ll have to copy all the colors from the Figma file and organize them in the JSON structure as expected by Tailwind.\n\nHere is how you can setup the Figma MCP Server in Cursor:\n\n{ \"mcpServers\": { \"figma-mcp-server\": { \"command\": \"npx\", \"args\": [ \"-y\", \"figma-developer-mcp\", \"--figma-api-key=\", \"--stdio\" ] } } }\n\nConclusion\n\nAdding the Heroku MCP Server to Cursor transformed my editor into a powerful development tool. I stopped jumping between terminals, dashboards, and code. Instead, I write a prompt, and Cursor handles the rest: running queries, deploying apps, scaling dynos, or pulling logs.\n\nThis shift improved my productivity and shaved minutes off every task, cutting down on errors from running commands by memory or context-switching. More importantly, it lets me stay in flow longer, so I can focus on the parts of coding I enjoy the most.\n\nIf you’re already using Cursor or another AI coding tool, give MCP a try. Also, take a look at this quick demo where I use the Heroku MCP Server and Cursor to build and deploy a simple web app.", "label": "non_personal"}
{"title": "Deploying a Simple Go/Gin Application on Heroku", "url": "https://www.heroku.com/blog/deploying-simple-go-gin-application-on-heroku/", "content": "The speed and efficiency of the Go programming language make it popular for backend development. Combine Go with the Gin framework—which offers a fast and minimalistic approach to building web applications—and developers can easily create high-performance APIs and web services. Whether you’re working on a personal project or building a production-ready application, Go and Gin make for an attractive stack perfectly suited for lightweight, scalable web development.\n\nCreating a Go/Gin application might seem straightforward: You write a few routes, connect a database, and spin up a local server. But when it comes to deploying your app, things can get tricky. Developers unfamiliar with cloud deployment often struggle with configuring environment variables, managing dependencies, and ensuring their app runs smoothly on a hosting platform.\n\nFortunately, Heroku makes this process incredibly simple. With its streamlined deployment workflow and built-in support for Go, Heroku lets you deploy your Go/Gin app with minimal configuration.\n\nIn this article, we’ll walk through the process of building and deploying a Go/Gin web application on Heroku. We’ll set up a local development environment, prepare an application for deployment, and deploy it to run on Heroku. Along the way, we’ll cover best practices and troubleshooting tips to ensure a smooth deployment.\n\nBy the end of this guide, you’ll have a fully functional Go/Gin application running on Heroku—and you’ll gain the knowledge needed to deploy future projects with confidence. Let’s get started!\n\nSetting up your development environment\n\nTo get started, you must set up your development environment. Here are the steps to install what you need and test your application locally.\n\nAn example project can be found in this GitHub repository.\n\nDownload and install Go\n\nDownload the Go installer from the official Go website, making sure you choose the correct operating system. For Windows or Linux, follow the respective installation instructions on the website.\n\nIf you’re on macOS, you can use Homebrew:\n\n$ brew install go\n\nOnce installed, verify your installation by running:\n\n$ go version\n\nYou should see your Go version printed in the terminal. For this guide, we’re running version 1.24.0 .\n\nSet up your workspace\n\nCreate a new directory for your project and initialize a Go module. Open your terminal and execute:\n\n~/project$ go mod init github.com/YOUR-USERNAME/YOUR-REPO-NAME\n\nThis neatly organizes your project and its dependencies, ensuring everything is in order. In the examples to follow YOUR-REPO-NAME will be go-gin .\n\nAdd the Gin framework\n\nNow it’s time to invite Gin to the party. Gin is a high-performance web framework that will help you build your REST server fast.\n\nRun the following command to add Gin to your project:\n\n~/project$ go get github.com/gin-gonic/gin\n\nThis fetches the Gin package and its dependencies.\n\nThe server application code goes in a file called main.go. Download that code here.\n\nFinally, run the server:\n\n~/project$ go run main.go\n\nTest your application locally\n\nBefore declaring your quest a success, make sure your application runs smoothly on your local machine. As you run the server as described above, you’ll see output indicating that it’s up and running.\n\n~/project$ go run main.go [GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached. [GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production. - using env: export GIN_MODE=release - using code: gin.SetMode(gin.ReleaseMode) [GIN-debug] GET /quotes --> main.main.func1 (3 handlers) [GIN-debug] GET /quote --> main.main.func2 (3 handlers) [GIN-debug] POST /quote --> main.main.func3 (3 handlers) [GIN-debug] [WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value. Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details. [GIN-debug] Listening and serving HTTP on :8080\n\nTest your API server endpoints by sending a curl request in a separate terminal window. For example:\n\n$ curl -s -X GET http://localhost:8080/quote | jq { \"quote\": \"The journey of a thousand miles begins with a single step.\" }\n\n(We use jq to pretty-print the JSON result.)\n\nCreating a Heroku App\n\nAssuming you have installed the Heroku CLI, you can create a new Heroku app. Run the following commands:\n\n~/project$ heroku login ~/project$ heroku apps:create my-go-gin-api Creating ⬢ my-go-gin-api... done https://my-go-gin-api-7f40e19ce771.herokuapp.com/ | https://git.heroku.com/my-go-gin-api.git\n\nThis creates your Heroku app, accessible at the app URL (in the above example, that’s https://my-go-gin-api-7f40e19ce771.herokuapp.com/ ). The command also creates a Git remote so you can push your code repo to Heroku with a single command.\n\n~/project$ git remote show heroku * remote heroku Fetch URL: https://git.heroku.com/my-go-gin-api.git Push URL: https://git.heroku.com/my-go-gin-api.git\n\nYou’ll also see your newly created app in your Heroku Dashboard. Clicking the Open app button will take you to your app URL.\n\nCreate the Procfile\n\nThe Procfile tells Heroku how to run your application. In your project’s root directory, create a file named Procfile (without any extension). For most simple Go applications, your Procfile will consist of a single line, like this:\n\nweb: go run main.go\n\nThis tells Heroku that your app will be a web process, and Heroku should start the process by running the command go run main.go . Simple enough! Add the file to your repository.\n\nTidy up your Go project\n\nFinally, use the following command to clean up your go.mod file to ensure all dependencies are properly listed:\n\n$ go mod tidy\n\nEnsure your go.mod and go.sum files have also been added to your repository. This allows Heroku to automatically download and manage dependencies during deployment with the Procfile.\n\nDeploying your application\n\nAfter completing these simple preparation steps, you’re ready to push your project to Heroku. Commit your changes. Then, push them to your Heroku remote.\n\n~/project$ git add . ~/project$ git commit -m \"Prepare app for Heroku deployment\" ~/project$ git push heroku main\n\nThe git push command will set off a flurry of activity in your terminal, as Heroku begins building your application in preparation to run it:\n\n… Writing objects: 100% (26/26), 9.52 KiB | 9.52 MiB/s, done. … remote: Building source: remote: remote: -----> Building on the Heroku-24 stack remote: -----> Determining which buildpack to use for this app remote: -----> Go app detected remote: -----> Fetching jq... done remote: -----> Fetching stdlib.sh.v8... done remote: -----> remote: Detected go modules via go.mod remote: -----> remote: Detected Module Name: github.com/your-username/go-gin remote: -----> remote: -----> New Go Version, clearing old cache remote: -----> Installing go1.24.0 remote: -----> Fetching go1.24.0.linux-amd64.tar.gz... done remote: -----> Determining packages to install remote: go: downloading github.com/gin-gonic/gin v1.10.0 … remote: remote: Installed the following binaries: remote: ./bin/go-gin remote: -----> Discovering process types remote: Procfile declares types -> web remote: remote: -----> Compressing... remote: Done: 6.6M remote: -----> Launching... remote: Released v3 remote: https://my-go-gin-api-7f40e19ce771.herokuapp.com/ deployed to Heroku remote: remote: Verifying deploy... done. To https://git.heroku.com/my-go-gin-api.git * [new branch] main -> main\n\nThis output tells you the location of the binary that Heroku built during the deploy process. In this case, it is at: ./bin/go-gin . For some Go applications, if you have trouble reaching your service and see errors in the logs, you might need to edit your Procfile to have Heroku run the binary directly, rather than using go directly with the source file. For example, your modified Procfile might look like this:\n\nweb: ./bin/go-gin\n\nTest the live application\n\nWith your Go application running on Heroku, you can test it by sending a curl request to your Heroku app URL. For example:\n\n$ curl -s \\ -X GET https://my-go-gin-api-7f40e19ce771.herokuapp.com/quote | jq { \"quote\": \"This too shall pass.\" }\n\nTo ensure everything is running smoothly after deployment, you can the following command to tail the server’s live logs:\n\n~/project$ heroku logs --tail … 2025-02-25T15:11:01.922123+00:00 heroku[web.1]: State changed from starting to up 2025-02-25T15:11:22.000000+00:00 app[api]: Build succeeded 2025-02-25T15:16:31.411009+00:00 app[web.1]: [GIN] 2025/02/25 - 15:16:31 | 200 | 29.199µs | 174.17.39.113 | GET \"/quote\" 2025-02-25T15:16:31.411487+00:00 heroku[router]: at=info method=GET path=\"/quote\" host=my-go-gin-api-7f40e19ce771.herokuapp.com request_id=7df071ec-9841-499f-b584-61574920e9df fwd=\"174.17.39.113\" dyno=web.1 connect=0ms service=0ms status=200 bytes=186 protocol=https\n\nIt’s time for you to Go!\n\nIn this article, we walked through each step of creating your Go application that uses the Gin framework, from project setup to Heroku deployment. You can see the power and simplicity of combining Gin’s robust routing capabilities with Heroku’s flexible, cloud-based platform. On top of this, it’s easy to scale your applications as your needs evolve.\n\nExplore the additional features both Heroku and Gin offer. Heroku’s extensive add-on ecosystem can boost your application’s functionality. You can also tap into advanced Gin middleware to optimize performance and strengthen security. To learn more, check out the following resources:", "label": "non_personal"}
{"title": "Congratulations to the Recipients of the 2024 Spotify FOSS Fund", "url": "https://engineering.atspotify.com/2024/11/congratulations-to-the-recipients-of-the-2024-spotify-foss-fund", "content": "TL;DR The Spotify FOSS Fund is back again! We created the Spotify FOSS Fund in 2022 to help support the open source ecosystem and to lend a monetary hand to those projects we use most at Spotify. This year, we’ve selected five projects to share the €100,000 fund (€20,000 each):\n\nHere’s what some of the recipients had to say about their projects, what kind of impact the funds might have on them, and other ways we can all help build a more sustainable open source community. (Note: some responses have been edited for length and clarity.)\n\nNorman Maurer, Netty\n\nNorman Maurer is a Netty project lead and co-author of Netty in Action.\n\nWhat is the vision for Netty?\n\nNetty aims to remain the de facto standard for writing network libraries/services on the JVM. Through Netty’s powerful abstraction it’s possible to easily write reusable components that can be composed to build up complex processing logic. Beside this, Netty provides support for popular internet standards like HTTP1, HTTP2 and HTTP3 and many more. Netty also provides the most performant implementation for specific platforms (by making use of native code) and makes use of new features — like, for example io_uring, TLS — to provide low-latency / high throughput.\n\nArtem Zakharchenko, MSW\n\nArtem (@kettainaitto) created the library and is a full-time maintainer of MSW.\n\nHow will these funds go towards supporting the future of MSW?\n\nMSW challenges the misconception that mocking is a hacky thing, and gives thousands of developers around the world better tools to control their network. Support from Spotify will enable me to focus even more on the project. Since I’ve switched to working on it full time (as financially damaging as it was), we’ve already shipped a ton of improvements, like the Server Boundaries, socket-based interceptor for Node.js, and Source. Not to mention a lot of work that went into collaborating with the teams behind Vitest, Vercel, Apollo, Nock, Node.js (to name a few) to ensure consistent experience for everybody, everywhere. Later this year, I’d love to announce the WebSocket support as well, making MSW the first API mocking library to cover all major API types. That being said, there are always more things to solve. Improving debugging and error handling, polishing type definitions, exploring uncharted territories like inter-process request interception. And now I will be able to solve them with Spotify’s help!\n\nKetan Umare, Flyte\n\nKetan (@kumare3) is the founder and TSC chair of Flye and co-founder and CEO of @UnionAI\n\nWhat is the vision for Flyte?\n\nAs generative AI advances, we aim to position Flyte at the forefront of this transformation, uniting data, ML, and AI seamlessly. The vision is to enable ML engineers and AI teams to build, deploy and scale ML Workflows with an open-source, kubernetes-native, pythonic orchestration kit. To accomplish this, Flyte provides a free, modern workflow orchestration platform. This platform enables teams to build production-grade pipelines, easily handle large datasets and efficient scheduling while working on complex infrastructure in distributed environments. Our mission is to enable ML engineers to focus more on solving business challenges and less on debugging and testing workflows. We are dedicated to building the world’s largest, most collaborative ML engineering community.\n\nSam Pillsworth, Typelevel\n\nSam (@samspills) is a TSC member of Typelevel and is based out of Toronto, Ontario.\n\nHow will these funds go towards supporting the future of Typelevel?\n\nMaintaining an organization at the scale of Typelevel incurs costs. Through FOSS funding we can provide better support to maintainers, and make stronger guarantees to users, by paying for things like CI infrastructure or community support. Our current funding goes towards recurring infrastructure expenses, and sending members of our code of conduct committee for training to better support our community. With these extra funds, we’d like to expand our initiatives to encourage and support new contributors and users! We’ve specifically discussed funding student or first time contributors through an initiative like Outreachy and engaging a technical writer to help improve documentation.\n\nThank you to all the maintainers of these projects for all the work they’ve done for these projects!", "label": "non_personal"}
{"title": "OpenTelemetry Basics on Heroku Fir", "url": "https://www.heroku.com/blog/opentelemetry-basics-on-heroku-fir/", "content": "Heroku recently made the next generation platform – Fir – generally available. Fir builds on the strengths of the Cedar generation while introducing a new modern era of developer experience. Fir leverages modern cloud-native technologies to provide a seamless and performant platform.\n\nOne of the goals we set out to achieve with Fir is to modernize our platform’s observability architecture. Applications being written today are becoming increasingly more distributed and complex in nature. With this increase in complexity, the need for good observability becomes critical. With solid observability practices in place, it becomes possible to gain deep insights into the internal state of these complex systems.\n\nThe Cloud Native Computing Foundation (CNCF)’s second most popular project, OpenTelemetry, standardizes and simplifies the collection of observability data (logs, metrics, and traces) for distributed systems. Integrating OpenTelemetry into Fir makes it easier to monitor, troubleshoot, and improve complex applications and services. OpenTelemetry is more than just a set of tools – it is a standard you as an end-user can benefit from a growing community of vendors that support the OpenTelemetry protocol.\n\nIt is for these reasons that we have chosen to build OpenTelemetry directly into the Fir platform. In this blog post we will explain what OpenTelemetry is and how you can quickly get started using OpenTelemetry on Heroku.\n\nWhat is OpenTelemetry\n\nOpenTelemetry is an open-standard framework that provides a standardized way to collect and export telemetry data from applications. It supports three primary signals:\n\nLogs: Capture discrete events that happen over time. This signal type provides detailed context for events, aiding in debugging and auditing.\n\nMetrics: Provide quantitative measurements of system behavior captured at runtime. Metrics offer insights into system performance and resource utilization.\n\nTraces: Record the execution path of requests through a system. These help in understanding the flow of requests and diagnosing latency issues.\n\nIn addition to these three signals, two more are under development.\n\nEvents: A specific type of log, an Event is a named occurrence at an instant in time. It signals that “this thing has happened at this time”. Examples of Events might include things like uncaught exceptions, network events, user login/logout, etc.\n\nProfiles: A mechanism to collect performant and consistent profiling data\n\nOpenTelemetry SDKs and Collectors\n\nThe OpenTelemetry SDK and Collector serve distinct purposes in an observability pipeline. The SDK is a library that allows developers to instrument their applications to generate telemetry like traces, metrics and logs. The collector sits downstream of the application and receives, processes and exports that telemetry data to various other backends. The collector acts as a central hub for observability data.\n\nTo recap,\n\nAn OpenTelemetry SDK:\n\nProvides language-specific implementations of the OpenTelemetry API.\n\nEmpowers the developers to instrument applications, generating telemetry data.\n\nManages the data collection and processing within the application.\n\nSends the telemetry data to a Collector or directly to an observability backend.\n\nAn OpenTelemetry Collector:\n\nIs a standalone, vendor-agnostic process.\n\nReceives telemetry data from multiple sources, including SDKs.\n\nProcesses the telemetry data through pipelines.\n\nExports processed telemetry data to observability backends like Prometheus, Jaeger and other vendors.\n\nActs as a central hub for managing telemetry pipelines.\n\nAt Heroku, our mission is to provide a platform that allows you, the developer, to focus on what matters most; building that app itself. Our platform automatically acts as the central hub for managing your telemetry pipelines.\n\nGetting started\n\nFor the purposes of this blog post we are going to use the Getting Started on Heroku Fir with Go tutorial. Zipping through most of the instructions we can bootstrap our application using only a few commands from a terminal.\n\nThe first thing we need to do is ensure that we have the latest version of the Heroku CLI installed. If you do not have the Heroku CLI installed or need to perform an update, simply follow the instructions found in the Heroku Dev Center.\n\n$ heroku version heroku/10.7.0 darwin-arm64 node-v20.19.1\n\nNow we need a Fir space, so let’s create one:\n\n$ heroku spaces:create heroku-otel-demo --generation fir --team demo-team › Warning: Spend Alert. Each Heroku Standard Private Space costs ~$1.39/hour (max $1000/month), pro-rated to the second. › Warning: Use heroku spaces:wait to track allocation. === heroku-otel-demo ID: bdacda5f-a9b5-41a7-a613-58a546ccd645 Team: heroku-runtime-playground Region: virginia CIDR: 2600:1f18:7a42:c600::/56 Data CIDR: State: allocated Shield: off Generation: fir Created at: 2025-04-23T20:51:39Z\n\nNext we need to clone down the repository and change in our working directory:\n\n$ git clone https://github.com/heroku/go-getting-started.git Cloning into 'go-getting-started'... remote: Enumerating objects: 4352, done. remote: Counting objects: 100% (897/897), done. remote: Compressing objects: 100% (711/711), done. remote: Total 4352 (delta 470), reused 162 (delta 162), pack-reused 3455 (from 2) Receiving objects: 100% (4352/4352), 10.62 MiB | 3.26 MiB/s, done. Resolving deltas: 100% (1734/1734), done. $ cd go-getting-started/\n\nNow, we can simply create the application and push the code to Heroku:\n\n$ heroku create --space heroku-otel-demo Creating app in space heroku-otel-demo... done, ⬢ fathomless-island-10342 http://fathomless-island-10342-6bd6dfa13d9e.aster-virginia.herokuapp.com/ | https://git.heroku.com/fathomless-island-10342.git $ git push heroku main Enumerating objects: 3679, done. Counting objects: 100% (3679/3679), done. Delta compression using up to 16 threads Compressing objects: 100% (2033/2033), done. Writing objects: 100% (3679/3679), 8.35 MiB | 448.00 KiB/s, done. Total 3679 (delta 1444), reused 3676 (delta 1444), pack-reused 0 (from 0) remote: Resolving deltas: 100% (1444/1444), done. remote: Updated 1310 paths from 49f32a9 remote: Compressing source files... done. remote: Building source: ...\n\nFinally, we can verify that the application is running using one last command:\n\n$ heroku open\n\nThis will open your default browser window. You should see something like this:\n\nGreat! We’ve got a functioning application running inside a Fir Space. Our next step is to send any platform telemetry to an observability vendor. For this demo, we’re going to use Grafana Cloud. Head over to grafana.com and create a Cloud Free account. Once you have signed up you will be presented with a Welcome to Grafana Cloud page.\n\nAt this point, we are going to skip the rest of the “Getting started” steps. The directions provided by the setup guide do not apply to how we are going to send telemetry data. For now, we can simply click “Skip setup”.\n\nThe easiest way to establish a Heroku Telemetry Drain to Grafana Cloud is to use a slightly different path. In a new browser tab, we will simply use the Grafana Cloud Portal. From Grafana.com click “My Account”.\n\nFrom there, click the “Details” button next to your Grafana Cloud stack. Mine is called herokudemo . Next click on the OpenTelemetry “Configure” button.\n\nFor now, don’t worry about copying any of the details to your Clipboard. Instead, scroll down to the “Password / API Token” section and click on the “Generate now” link. Give your token a name. Once you are done, make sure you keep a copy of the generated token for future reference. Now that we have a token, scroll down a bit more and copy the contents of the “Environment Variables” section to your clipboard.\n\nNow we can head back to our terminal window and paste environment variables. We can confirm that pasting the environment variables work by using echo quickly:\n\n$ echo $OTEL_EXPORTER_OTLP_ENDPOINT https://otlp-gateway-prod-ca-east-0.grafana.net/otlp $ echo $OTEL_EXPORTER_OTLP_HEADERS Authorization=Basic MTIzOTIwMjpnbGNfZXlKdklqb2lNVFF4TXpFMU15SXNJbTRpT2lKemRHRmpheTB4TWpNNU1qQXlMVzkwYkhBdGQzSnBkR1V0WkdWdGJ5SXNJbXNpT2lKNE5GZFZOa3hDY0RNNU16VkxOR0ptVkVjMGN6ZE9XVGNpTENKdElqcDdJbklpT2lKd2NtOWtMV05oTFdWaGMzUXRNQ0o5ZlE9PQ==\n\nNext, we will convert the headers into a json format that the Heroku CLI command expects.\n\n$ export HEROKU_OTLP_HEADERS=\"$(echo \"$OTEL_EXPORTER_OTLP_HEADERS\" | sed 's/^\\([^=]*\\)=\\(.*\\)$/{\"\\1\":\"\\2\"}/')\" $ echo $HEROKU_OTLP_HEADERS {\"Authorization\":\"Basic MTIzOTIwMjpnbGNfZXlKdklqb2lNVFF4TXpFMU15SXNJbTRpT2lKemRHRmpheTB4TWpNNU1qQXlMVzkwYkhBdGQzSnBkR1V0WkdWdGJ5SXNJbXNpT2lKNE5GZFZOa3hDY0RNNU16VkxOR0ptVkVjMGN6ZE9XVGNpTENKdElqcDdJbklpT2lKd2NtOWtMV05oTFdWaGMzUXRNQ0o5ZlE9PQ==\"}\n\nFinally, we can add the Heroku Telemetry Drain:\n\n$ heroku telemetry:add --app fathomless-island-10342 $OTEL_EXPORTER_OTLP_ENDPOINT --transport http --headers \"$HEROKU_OTLP_HEADERS\" successfully added drain https://otlp-gateway-prod-ca-east-0.grafana.net/otlp\n\nBack from the Grafana Cloud dashboard, after a few minutes you will start to see some application specific metrics flowing into Grafana Cloud.\n\nNow if you navigate back to your application in the browser (Pro Tip, use heroku open ), and hit refresh a few times you should also start to see traces and logs flowing into Grafana Cloud as well.\n\nIn Conclusion: Fir’s Observability Power – And Where We Go From Here\n\nSo, as we’ve shown, Heroku’s Fir platform, with its built-in OpenTelemetry, streamlines the process of setting up observability for your applications. This means you can move quickly from deploying your app to gaining critical insights into its performance, as demonstrated by the walkthrough using Grafana Cloud. But what you’ve seen here is just one of the many benefits of Heroku’s next-generation platform. In the next part of this series, we’ll dive deeper into how to effectively analyze the telemetry data you’re now collecting. We’ll explore techniques for querying, visualizing, and correlating traces, metrics, and logs to unlock powerful insights that will help you optimize your application’s behavior and troubleshoot issues like a pro.\n\nTo get the full picture of everything the Fir platform offers, from enhanced observability to a modern developer experience, don’t forget to watch the Fir launch webinar on-demand!", "label": "non_personal"}
{"title": "Heroku Postgres Upgrade Guide: Simplify Your Move to Version 17", "url": "https://www.heroku.com/blog/heroku-postgres-upgrade-guide-simplify-move-version-17/", "content": "If you’ve ever deployed an app on Heroku, chances are you’ve used Heroku Postgres — our fully managed, reliable, and scalable Postgres database service. It’s the backbone for millions of applications, from weekend side projects to enterprise-grade systems running in production.\n\nBut Postgres, like all software, continues to evolve. With new versions released each year, you gain access to performance enhancements, critical security updates, and powerful new features. Keeping your database up to date isn’t just good practice — it’s essential for long-term stability and success.\n\nThat’s why we’re thrilled to share that Postgres 17 is now available on Heroku. And with our newly simplified upgrade process, keeping your database current has never been easier. There’s no better time to plan your next upgrade and take full advantage of everything Postgres 17 has to offer.\n\nWhat is Heroku Postgres?\n\nHeroku Postgres is a managed Postgres service built into the Heroku platform. It handles provisioning, maintenance, backups, high availability, and monitoring so that customers can focus on building engaging data-driven applications, instead of managing infrastructure.\n\nWhy upgrading your Postgres version matters\n\nThere are several important reasons for why upgrading your Postgres database is necessary,\n\nSecurity : Postgres regularly releases security updates to patch vulnerabilities. Running an outdated version could expose your database to known security risks. Once a version is unsupported, the Postgres Community won’t have any more security releases for that version.\n\n: Postgres regularly releases security updates to patch vulnerabilities. Running an outdated version could expose your database to known security risks. Once a version is unsupported, the Postgres Community won’t have any more security releases for that version. Bug Fixes : Each new version includes fixes for bugs and issues found in previous versions.\n\n: Each new version includes fixes for bugs and issues found in previous versions. Performance Improvements : Newer versions often include performance optimizations, better query planning, and improved resource utilization.\n\n: Newer versions often include performance optimizations, better query planning, and improved resource utilization. New Features : Postgres releases bring new features and capabilities that can enhance your database’s functionality. For example: Better parallel query execution Improved indexing options Enhanced monitoring capabilities New data types and functions\n\n: Postgres releases bring new features and capabilities that can enhance your database’s functionality. For example: Compatibility: Staying current helps maintain compatibility with other tools, libraries, and applications that interact with your database.\n\nThe backstory: Our Postgres version support & deprecation policy\n\nAt Heroku, we follow a well-defined lifecycle for Postgres versions:\n\nEach major version is supported for a set period — currently three years.\n\nWhen a version approaches end-of-support, we begin our deprecation process — announcing an upcoming removal from the platform and stopping new provisioning.\n\nWhen deprecation is announced, we give customers advance notice and time to upgrade manually.\n\nIf no action is taken, we then automatically upgrade databases still on unsupported versions, ensuring security and platform stability.\n\nWhat we learned\n\nOur internal upgrade automation has quietly and successfully upgraded tens of thousands of databases each year leading many customers to ask:\n\n“Can we use the same system to manage our own upgrades?”\n\nThat demand inspired the improved pg:upgrade CLI experience — a safer, more transparent, and self-service version of our proven internal tools. Now, all Heroku Postgres users can benefit from the same automation and built-in checks that power our large-scale upgrade process.\n\nVisit our devcenter for more details on how Heroku manages Postgres version support and deprecation timelines.\n\nIntroducing new pg:upgrade commands\n\nWe’re rolling out five new heroku pg:upgrade:* commands that give you more control, visibility, and confidence during Postgres version upgrades:\n\npg:upgrade:prepare – Schedule a Postgres upgrade for Standard-tier and higher leader databases during your next maintenance window.\n\npg:upgrade:run – Trigger an upgrade manually. Perfect to start an upgrade immediately on Essential-tier databases and follower databases, or run a prepared upgrade before the next scheduled maintenance window on a Standard-tier or higher database.\n\npg:upgrade:cancel – Cancel a scheduled upgrade (before it starts running).\n\npg:upgrade:dryrun – Simulate an upgrade on a Standard-tier or higher database using a follower to preview the upgrade experience and detect any potential issues — no impact on your production database.\n\npg:upgrade:wait – Track the progress of your upgrade in real time.\n\nYou’ll receive email notifications at every key stage:\n\nWhen the upgrade is scheduled, running, cancelled and completed (successfully or not).\n\nAfter a dry run completes, with a summary of the results and any potential issues detected.\n\nUpgrading is now a simple 1-step process\n\nYou might notice there are more commands available now, but upgrading your database has actually become much simpler — it’s now just a 1-step process!\n\nHeroku handles what used to be multiple manual steps — provisioning a follower, entering maintenance mode, promoting, reattaching, exiting maintenance mode — all with a single workflow.\n\nSee the section below for the most efficient path based on your database tier.\n\nStep-by-step: How to upgrade Heroku Postgres\n\nEssential-tier database upgrades\n\nTo upgrade, just run:\n\nheroku pg:upgrade:run HEROKU_POSTGRESQL_RED --app example app\n\nThat’s it — no preparation step required.\n\nNote: If you don’t specify a version with --version , the upgrade will use the latest supported Postgres version on Heroku.\n\nStandard-tier & higher database upgrades\n\nWe recommend this process for Standard-tier and higher, regardless of whether or not you have follower databases.\n\nStep 0 – Optional (but recommended)\n\nRun a test upgrade to detect any potential issues before upgrading your production database.\n\nheroku pg:upgrade:dryrun HEROKU_POSTGRESQL_RED --app example-app\n\nThen proceed with the actual upgrade in one simple step:\n\nStep 1 – Prepare the upgrade\n\nheroku pg:upgrade:prepare HEROKU_POSTGRESQL_RED --app example-app --version 17\n\nThis schedules the upgrade for your next maintenance window.\n\nNote: If --version is not specified, we’ll automatically use the latest supported Postgres version on Heroku.\n\nUse the following to track when the upgrade is scheduled and ready to run:\n\nheroku pg:upgrade:wait HEROKU_POSTGRESQL_RED --app example-app\n\nStep 2 (Optional) – Manually run the upgrade\n\nheroku pg:upgrade:run HEROKU_POSTGRESQL_RED --app example-app\n\nThis will upgrade your leader database and its follower(s) automatically.\n\nTrack the progress until completion with:\n\nheroku pg:upgrade:wait HEROKU_POSTGRESQL_RED --app example-app\n\nTip: If you don’t manually run this command, the upgrade will be run automatically during the scheduled maintenance window. You can view your app’s maintenance window and scheduled maintenances by running:\n\nheroku pg:info HEROKU_POSTGRESQL_RED --app example-app\n\nFor more information on maintenance windows, check out the Heroku Postgres Maintenance documentation.\n\nBenefits of the new upgrade mechanism\n\nThe new upgrade operates in-place using an internal replica, simplifying the process, removing the need to manage a separate follower add-on, and minimizing the risk of data loss or inconsistencies by managing database access internally throughout the upgrade.\n\nusing an internal replica, simplifying the process, removing the need to manage a separate follower add-on, and minimizing the risk of data loss or inconsistencies by managing database access internally throughout the upgrade. Using this automation will reduce downtime to about 5-10 minutes for a typical upgrade.\n\nfor a typical upgrade. When you upgrade your leader database, any followers are automatically upgraded – no need to recreate or manually reattach followers.\n\n– no need to recreate or manually reattach followers. Your app’s DATABASE_URL and other config_vars remain unchanged after the upgrade, ensuring your application continues to operate without any reconfiguration.\n\nand other after the upgrade, ensuring your application continues to operate without any reconfiguration. The same simple steps apply to upgrading databases with Streaming Data Connectors , replacing what used to require at least 8 manual steps as outlined here.\n\n, replacing what used to require at least as outlined here. If we detect a known issue with your data/schema during the upgrade, you’ll receive an email with remediation steps to help you complete the upgrade.\n\nwith your data/schema during the upgrade, you’ll receive an email with remediation steps to help you complete the upgrade. If the issue is unexpected or cannot be resolved automatically , we’ll prompt you to open a support ticket so our team can help troubleshoot.\n\n, we’ll prompt you to so our team can help troubleshoot. In all cases, your database remains available , and user access is restored once the upgrade process completes — whether it finishes successfully or is automatically aborted for safety.\n\n, and once the upgrade process completes — whether it finishes successfully or is automatically aborted for safety. Want added peace of mind? Run a test upgrade in advance using heroku pg:upgrade:dryrun . This simulates the upgrade on a copy of your database and highlights potential issues before touching production.\n\nThe “old” follower upgrade approach\n\nWhile we now recommend upgrading the leader database directly using the approach explained\n\nabove, customers who prefer the traditional flow can still use the follower upgrade approach.\n\nTo do this, you can continue to follow the steps as described here.\n\nIn order to run the upgrade, use:\n\nheroku pg:upgrade:run HEROKU_POSTGRESQL_RED --app example-app\n\nThis approach has one notable benefit, your original leader database remains untouched during the upgrade, which allows for easier rollback, testing, or verification before promoting the upgraded follower.\n\nDeprecation notice\n\nThe legacy heroku pg:upgrade command will be deprecated soon. To ensure a smoother, safer upgrade experience, we strongly recommend switching to the new heroku pg:upgrade:* subcommands.\n\nIf you continue to use the old command, you’ll receive tailored warnings and redirection to help guide you toward the updated flow. Make the switch today to take full advantage of the simplified, automated upgrade process.\n\nUpgrading shouldn’t be a chore – it should be a habit\n\nUpgrading your Postgres database shouldn’t be a last-minute scramble — it should be a routine habit. Regular upgrades help keep your applications secure and performant, while also giving you access to the latest features and improvements that drive innovation. By making upgrades a part of your development rhythm, you set your systems up for long-term stability, scalability, and success.\n\nAt Heroku, we’re focused on making the overall Postgres experience safer and more intuitive for developers. A key part of that is improving the upgrade process: with streamlined tooling, automation, and built-in safeguards, upgrading your Postgres version is now significantly faster and more reliable. All of this is designed to help you stay focused on what matters most – building and shipping great apps – while staying confident that your data layer is future-ready.", "label": "non_personal"}
{"title": "Local Speed, Smooth Deploys: Heroku Adds Support for uv", "url": "https://www.heroku.com/blog/local-speed-smooth-deploys-heroku-adds-support-uv/", "content": "Ah, another day, another deep dive into the ever-evolving world of Python development! Today, let’s talk about something near and dear to every Pythonista’s heart – managing those crucial external packages. For years, pip has been our trusty companion, the workhorse that gets the job done. But the landscape is shifting, and a new contender has entered the arena, promising speed, efficiency, and a fresh approach: uv.\n\nAs a Python developer constantly striving for smoother workflows and faster iterations, the buzz around uv has definitely caught my attention. So, let’s roll up our sleeves and explore the benefits of using uv as your Python package manager, taking a look at where we’ve come from and how uv stacks up. We’ll even walk through setting up a project for Heroku deployment using this exciting new tool.\n\nA trip down memory lane: The evolution of Python package management\n\nTo truly appreciate what uv brings to the table, it’s worth taking a quick stroll down memory lane and acknowledging the journey of Python package management.\n\nIn the early days, installing Python packages often involved manual downloads, unpacking, and running setup scripts. It was a far cry from the streamlined experience we have today. Then came Distutils, which provided a more standardized way to package and distribute Python software. While a significant step forward, it still lacked robust dependency resolution.\n\nEnter setuptools, which built upon Distutils and introduced features like dependency management and package indexing (the foundation for PyPI). For a long time, setuptools was the de facto standard, and its influence is still felt today.\n\nHowever, as the Python ecosystem grew exponentially, the limitations of the existing tools became more apparent. Dependency conflicts, slow installation times, and the complexities of managing virtual environments started to become significant pain points.\n\nThis paved the way for pip (Pip Installs Packages). Introduced in 2008, pip revolutionized Python package management. It provided a simple and powerful command-line interface for installing, upgrading, and uninstalling packages from PyPI and other indices. For over a decade, pip has been the go-to tool for most Python developers, and it has served us well.\n\nBut the increasing complexity of modern Python projects, with their often intricate web of dependencies, has exposed some of pip’s performance bottlenecks. Resolving complex dependency trees can be time-consuming, and the installation process, while generally reliable, can sometimes feel sluggish.\n\nAnother challenge with the complexity of modern applications is package versioning. Lockfiles that pin project dependencies have become table stakes for package management. Many package management tools use them. Throughout the course of the evolution of package management in Python, we’ve seen managers such as Poetry and Pipenv, just to name a few. However, many of these projects don’t have dedicated teams. Sometimes this results in them not being able to keep up with the latest standards or the complex dependency trees of modern apps.\n\nThis is where the new generation of package management tools, like uv, comes into play, promising to address these very challenges, with a dedicated team behind them.\n\nEnter the speed demon: The benefits of using uv\n\nuv isn’t just another package manager; it’s built with a focus on speed and efficiency, leveraging modern programming languages and data structures to deliver a significantly faster experience. Here are some key benefits that have me, and many other Python developers, excited:\n\nBlazing Fast Installation: This is arguably uv’s headline feature. Written in Rust from scratch using a thoughtful design approach uv significantly outperforms pip in resolving and installing dependencies, especially for large and complex projects. The difference can be dramatic, cutting down installation times from minutes to seconds in some cases. This speed boost translates directly into increased developer productivity and faster CI/CD pipelines. Efficient Dependency Resolution: uv employs sophisticated algorithms for dependency resolution, aiming to find compatible package versions quickly and efficiently. While pip has made improvements in this area, uv’s underlying architecture allows it to handle complex dependency graphs with remarkable speed. This reduces the likelihood of dependency conflicts and streamlines the environment setup process. Drop-in Replacement for pip and venv : One of the most appealing aspects of uv is its ambition to be a seamless replacement for both pip and venv (Python’s built-in virtual environment tool). It aims to handle package installation and virtual environment creation with a unified command-line interface. This simplifies project setup and management, reducing the cognitive load of juggling multiple tools. Compatibility with Existing Standards: uv adheres to existing Python packaging standards like pyproject.toml (PEP 621). This means that projects already using these standards can easily adopt uv without significant modifications. It reads and respects your existing pyproject.toml files, making the transition relatively smooth. uv is built with a strong emphasis on modern packaging practices, encouraging the adoption of pyproject.toml for declaring project dependencies and build system requirements. This aligns with the direction the Python packaging ecosystem is heading. Improved Error Messaging: While pip’s error messages have improved over time, uv, being a newer tool, has the opportunity to provide more informative and user-friendly error messages, making debugging dependency issues easier. Potential for Future Enhancements: As a relatively new project with a dedicated development team, uv has the potential to introduce further optimizations and features that could significantly enhance the Python development experience. The active development and growing community support are promising signs.\n\nHow to use uv with Heroku\n\nNow, let’s put some of this into practice. Imagine we’re building a simple Python web application (using Flask, for instance) that we want to deploy to Heroku, and we want to leverage the speed and efficiency of uv in our development and deployment process.\n\nHere’s how we can set up our project:\n\n1. Install uv\n\nThere are a variety of options to install uv, depending on your operating system. For a full list, take a look at the official Installation Guide site. I’m going to install it using Homebrew:\n\n~/user$ brew install uv\n\n2. Create the project directory and initialize uv\n\n~/user$ uv init my-app ~/user$ cd my-app ~/user/my-app$ ls -a\n\nIn doing that, uv generates several project files\n\nmy-app/ ├── main.py ├── pyproject.toml ├── README.md └── .python-version\n\nOur main.py looks like this:\n\ndef main(): print(\"Hello from my-app!\") if __name__ == \"__main__\": main()\n\nWe can run this with the uv run main.py command which does a few things for us. In addition to actually running main.py and generating the “Hello from my-app!” output, uv also generates a virtual environment for the project and generates a uv.lock file which describes the project. More on that in a bit.\n\n3. Expanding the project… slightly.\n\nLet’s take this project a bit further and turn it into a Flask app that we can deploy to Heroku. We’ll need to specify our dependencies, Flask and Gunicorn for this example. We can do this using pyproject.toml .\n\nUsing pyproject.toml :\n\nThe uv generated pyproject.toml file looks like this:\n\n[project] name = \"my-app\" version = \"0.1.0\" description = \"Add your description here\" readme = \"README.md\" requires_python =\">=3.13\" dependencies = []\n\nTo add dependencies we use the uv add command.\n\n~/user/my-app$ uv add Flask ~/user/my-app$ uv add gunicorn\n\nThis accomplishes a couple of things:\n\nFirst, it adds those packages to the pyproject.toml file:\n\n[project] name = \"my-app\" version = \"0.1.0\" description = \"Add your description here\" readme = \"README.md\" requires_python =\">=3.13\" dependencies = [ \"Flask>=3.1.1\", \"gunicorn>=23.0.0\", ]\n\nSecond, it updates the uv.lock file for dependency management.\n\n4. Updating main.py\n\nLet’s update the code in main.py to be a basic Flask web application\n\nfrom flask import Flask app = Flask(__name__) @app.route('/') def hello_world(): return \"Hello from uv on Heroku!\" if __name__ == '__main__': app.run(debug=True)\n\n5. Preparing for Heroku deployment:\n\nHeroku needs to know how to run your application. For a Flask application, we typically use Gunicorn as a production WSGI server. We’ve already included it in our dependencies.\n\nWe’ll need a Procfile in the root of our project to tell Heroku how to start our application:\n\nweb: gunicorn main:app\n\nHere, app refers to the name of our Flask application instance in main.py .\n\n6. Deploying to Heroku:\n\nNow, assuming you are in the project working directory, have the Heroku CLI installed, and have logged in, you can create a local git repository and Heroku application:\n\n~/user/my-app$ git init ~/user/my-app$ heroku create python-uv # Replace python-uv with your desired app name ~/user/my-app$ git add . ~/user/my-app$ git commit -m \"Initial commit with uv setup\"\n\nThe Heroku CLI will create a remote in your git repository, but you check to make sure it’s there before you and push your code\n\n~/user/my-app$ git remote -v heroku https://git.heroku.com/python-uv.git (fetch) heroku https://git.heroku.com/python-uv.git (push) ~/user/my-app$ git push heroku main\n\nHeroku will detect your Python application, install the dependencies (based on .python-version , uv.lock and pyproject.toml ), and run your application using the command specified in the Procfile .\n\nThe future is bright (and fast!)\n\nWe’re excited to announce that Heroku now natively supports uv for your Python development. By combining uv’s performance with Heroku’s fully managed runtime, teams can ship faster with greater confidence in their environment consistency. This reduces onboarding time, eliminates flaky builds, and improves pipeline performance.\n\nWhile uv is still relatively new, its potential to significantly improve the Python development workflow is undeniable. The focus on speed, efficiency, and modern packaging standards addresses some of the long-standing frustrations with existing tools.\n\nAs the project matures and gains wider adoption, we can expect even more features and tighter integration with other parts of the Python ecosystem. For now, even the significant speed improvements in local development are a compelling reason for Python developers to start exploring uv.\n\nThe journey of Python package management has been one of continuous improvement, and uv represents an exciting step forward. If you’re a Python developer looking to boost your productivity and streamline your environment management, I highly recommend giving uv a try. You might just find your new favorite package manager!\n\nTry uv out on Heroku\n\nWhether you’re modernizing legacy apps or spinning up new services, uv gives you the speed and flexibility you need—now with first-class support on Heroku. Get started with uv on Heroku today.", "label": "non_personal"}
{"title": "Building Confidence: A Case Study in How to Create Confidence Scores for GenAI Applications", "url": "https://engineering.atspotify.com/2024/12/building-confidence-a-case-study-in-how-to-create-confidence-scores-for-genai-applications", "content": "TL;DR Getting a response from GenAI is quick and straightforward. But what about the confidence level for that response? In certain applications, especially in the financial domain, confidence scores are required. In a document-parsing task related to financial automation, we tested three approaches to address confidence level: calibrator models, logarithmic probabilities (logprobs), and majority voting. Majority voting turned out to be the most performant technique. Although the idea is relatively simple, the devil is in the details.\n\nIntroduction\n\nThe introduction of GenAI technology has proven to be revolutionary in the current business landscape, including its improvements to internal business efficiency. Repetitive tasks that were once burdensome to humans can be automated by GenAI. Compared to traditional deterministic methods or machine learning (ML) models, using GenAI has the following benefits:\n\nFast development: Rapid training, testing, and implementation reduce development time.\n\nScalability and flexibility: GenAI can easily scale to incorporate new cases with simple prompt adjustments.\n\nMaintainability: In many cases, GenAI-powered solutions are easier to maintain than traditional ML models, making them more cost-effective over time.\n\nDespite its advantages, GenAI faces significant challenges in accuracy and reliability, with its lack of confidence scores and its hallucinations.\n\nConfidence levels are crucial in building trust and informing decisions, but they are not integral to AI technologies. In this post, we will explore how we created confidence scores in GenAI applications for a financial automation use case, detailing the approaches, important implementation details, and challenges/limits.\n\nAssessing confidence in a financial automation application\n\nOur team sits in the Financial Engineering org at Spotify and aims to enhance efficiency in financial processes by automation. Recently we worked on automating invoice parsing, which was part of a larger initiative to streamline invoice processing. The invoices, from a large distribution of vendors globally, come in various languages, formats, and structures. This complexity makes deterministic models inadequate, as they struggle with high numbers of edge cases and with ambiguous and incomplete data.\n\nGenAI, on the other hand, has proven to be more versatile and adaptable. It can also make adequate inferences for complex tasks such as invoice parsing. However, a significant challenge arises while employing GenAI models in this task: confidence scores must be produced to support human-in-the-loop decisions and to meet regulatory requirements like IT general controls (ITGC) and the Sarbanes–Oxley Act (SOX). Specific thresholds need to be crossed for the models’ outputs to be trusted. To address the need, we researched the following approaches and implemented a reliable way to generate confidence scores for GenAI model outputs.\n\nApproaches evaluated\n\nCalibrator\n\n“Calibrator” refers to using a separate single GenAI model to evaluate the outputs generated by other models and assign confidence scores. This approach offers an independent and unbiased evaluation of the outputs from other models. The calibrator model is also trainable by learning from feedback and can potentially improve over time.\n\nHowever, confidence scores generated by calibrators are difficult to interpret and sometimes counterintuitive. Besides, the scores are not consistent across multiple runs. In financial applications where confidence in technology is crucial, unclear scoring and inconsistencies are not acceptable.\n\nLogprobs\n\nLogprobs, or logarithmic probabilities, are the log-transformed probabilities for each token (a unit of text, such as a word or part of a word) in a generated output. These probabilities indicate how confident the model is in choosing a token. A higher (less negative) logprob means the model is more confident in the choice. However, the methodology is not always transparent – it varies by provider, and for many models it is unknown how they were calculated.\n\nThe confidence of an entire response can be deduced by combining the logprobs of its tokens. We averaged the logprobs instead of summing to normalize for output length. The confidence score was then calculated by exponentiating the average logprobs of the tokens.\n\nWe tested the approach for different extraction fields in our application. We plotted the results for one numeric field (invoice total) and one text field (invoice number) against the accuracy in the figure below. No clear correlation was found between lobprob-based confidence score and accuracy, and the conclusion holds for other text and numeric fields as well. The lack of correlation suggests that averaging logprobs is not a reliable measure of overall confidence.\n\nMajority voting\n\nMajority voting is an ensemble method that selects the final output by choosing the most common response from multiple prompts or among multiple GenAI models. For example, if five models are asked to classify an image and four suggest “cat” while one suggests “dog,” the final output would be “cat.” The confidence score can be calculated based on the proportion of agreeing models, in this case 80% (four out of five models).\n\nWe tested the same application using five GenAI models and majority voting and observed a strong positive correlation between confidence score and accuracy. The same two fields were plotted below, and the conclusion holds for other fields as well.\n\nThe careful art of majority voting\n\nWith the evaluation, we concluded that only majority voting was a suitable choice for our application, as it’s the only method that showed strong positive correlation with accuracy and is relatively consistent and interpretable. While its concept is straightforward, in practice we realized that the implementation requires careful consideration of many factors.\n\nDeciding the number of models\n\nThe optimal number of models depends on factors such as task complexity, model diversity, available resources, and specific project goals and constraints. Larger ensembles typically offer greater stability and accuracy by reducing individual model errors. However, they increase computational complexity and may yield diminishing returns if the models are too similar. Smaller ensembles are more efficient but less stable.\n\nLiterature often suggests using four to seven models in an optimal ensemble. In our use cases, we leveraged five or six different LLMs. We found that this number provided sufficient diversity and struck a good balance between speed and diversity.\n\nAssigning weights to voting\n\nTo calculate the final score, a weighted majority voting approach was implemented. Weight for each model was based on model accuracy and then normalized to sum to one. This method minimizes the chances of a tie in the voting process, as opposed to an unweighted vote. It also improves the accuracy by giving greater influence to better-performing models.\n\nWe evaluated both linear weights (where weights are linearly correlated with model accuracy) and exponential weights (where top-performing models get weights close to or exceeding half). Both methods showed similar performance. We opted to use linear weights for our application, as it created more balanced and consistent outcomes and was easier to explain.\n\nCalibrating confidence score\n\nWhile there is a positive correlation between confidence and accuracy, the relationship is not one-to-one and can vary across different fields.To correct over- or under-confidence in various fields, we applied Platt scaling, a technique commonly used to calibrate probabilistic outputs. This calibration process adjusts the raw confidence scores to better align with the accuracy, as shown in the two examples in the chart below:\n\nMajority voting — limitations, challenges, and explorations\n\nLong text fields\n\nMajority voting works effectively for numeric and short text responses but faces challenges with long text fields, such as addresses or item descriptions. Due to variations in phrasing, it is less likely to get model agreement using direct string matching. As an attempt to address this issue, we explored two methods to get a majority response in these scenarios:\n\nEmbedding similarity, which groups similar text responses into clusters based on cosine similarity of their embeddings. The largest cluster is the majority vote. This approach requires careful tuning of distance thresholds. GenAI selection, where a separate GenAI model selects the majority vote from multiple responses. This approach requires extensive prompt engineering to ensure the selector prioritizes factual accuracy and consistency.\n\nDuring our testing, both approaches were able to identify similar outputs for long text fields. However, they failed to catch some spelling differences — for example, not distinguishing between “0” and “O,” which can lead to significant errors in a financial application. Therefore, we decided not to implement them for production.\n\nAs a workaround, we broke down long text fields into smaller, more manageable components (e.g., splitting addresses into street, city, state, and zip code). It improved the likelihood of model agreement during majority voting. However, developing a more robust solution for the long text field remains a key area for further research.\n\nThe granularity problem\n\nGiven the limited number of models in an ensemble, majority voting brings a granularity problem. With seven models (the upper end of the normal range), each additional vote results in a significant ~14% step change in confidence level. This isn’t granular enough for applications requiring fine-grained confidence figures (e.g., needing 95% confidence to pass a check).\n\nTo improve granularity, we experimented with the permutation approach: by using multiple prompts per model, the total responses increased from X to X * Y (where X is the number of models and Y is the number of prompts per model). We tested with seven GenAI models and five different prompts, and 35 responses were generated. The outcome is summarized in the table below.\n\nApproach # of GenAI Models # of Prompts Total Responses # of Agreed Responses Output Confidence Original 7 1 7 6 Majority voting output is correct. 86% (6/7) Permutation 7 5 35 33 Majority voting output is correct. 94% (33/35)\n\nThe permutation method could potentially increase the pass rate for our system. If the confidence threshold was 90%, the original approach would result in a false rejection, while the permutation approach would successfully return the correct output.\n\nHowever, it significantly increases the number of model runs, where cost increases linearly and the time to finish now depends on the slowest model out of the 35, and a potential failure is more likely . Although the experiments provided valuable insights and guided further discussions with the business regarding thresholds and their implications, future research is still warranted for a more cost-effective long-term solution.\n\nConclusion\n\nIn this post, we examined a unique challenge that arises when using GenAI in financial applications — determining the confidence score. We discussed the various approaches we explored, culminating in a successfully implemented technique. However, as always, the devil is in the details, and many aspects of implementation require careful consideration. Finally, there are still challenges and limitations in our approach. We explored a few solutions, but some problems remain the topic of future research. We are optimistic that new developments will continue to evolve this groundbreaking technology, and we hope our explorations inspire not only our teams at Spotify but also practitioners across the industry.", "label": "non_personal"}
{"title": "A Behind-the-Scenes Look at How We Release the Spotify App (Part 1)", "url": "https://engineering.atspotify.com/2025/4/how-we-release-the-spotify-app-part-1", "content": "Developing and releasing mobile apps at scale is a big challenge. With each weekly release of our mobile app for iOS and Android, hundreds of changes go out to more than 675 million users all over the world and on all kinds of mobile devices. A lot can go wrong, so discovering and mitigating potential and confirmed issues is critical to ensuring a smooth listening experience. Every feature could impact the app’s stability and user experience, so making sure we roll them out in a coordinated and prioritized way is also something that has to be taken care of.\n\nAt Spotify, our Release team has a dual mission: (1) to oversee the release of the main Spotify app and (2) to build the necessary tools to support this process. Day-to-day coordination is handled by the full-time Release Manager with the support of the rest of the team.\n\nWhen it comes to releasing the app, the core responsibilities of the Release team are twofold:\n\nMaking sure that the time from when the developer merges their code into the main branch to when it’s available to users is as short as possible To ensure that quality meets our standards\n\nAt times, friction exists between these two goals, and much of the craft of release management is about mitigating this, both with tooling and with informed coordination and decision-making. Instances when this balance needs to be struck might include the following:\n\nPrioritizing accordingly — not all bugs are created equal. A crash during signup or playback demands immediate attention, while a post-logout crash might be less urgent.\n\nIdentifying a fallback — if a bug affects a specific A/B test group, we can temporarily route all users to the working experience via backend adjustments, addressing the client-side fix in the next release. This keeps the release on track without sacrificing user experience.\n\nActing quickly when necessary — even a minor bug affecting a small but significant user group (e.g., crashes in a specific region) might warrant a swift resolution.\n\nThe release cycle\n\nTo illustrate our release cycle, let’s follow the journey of version 8.9.2 from inception to rollout.\n\nFriday, September 20: A new version is born\n\nEach release cycle kicks off on a Friday morning, when the release of the previous version has been cut. Once this has been done, it’s time to start the work on the upcoming version.\n\nAt Spotify, we practice trunk-based development, meaning developers merge their code into the main branch as soon as it’s tested and reviewed.\n\nHowever, we make an exception for major changes: Large-scale or infrastructure updates are merged earlier in the cycle (typically on the first day, Friday of Week 1). This gives us ample time for thorough testing, leveraging both internal teams and external alpha users to identify and fix issues early on.\n\nWith Spotify 8.9.2, we planned to roll out the Audiobooks feature in some markets — it had been available behind a feature flag in the backend for a number of releases, for internal testing to discover any bugs that might impede the planned rollout. This was an important new feature for the company, and we wanted to make sure we got it right, particularly since marketing activities and events were already scheduled.\n\nThe Release Manager made sure, well in advance, that it was the only big feature rolling out with this release — another new feature that initially had been scheduled to roll out in the same week was rescheduled for the following week.\n\nOther teams could still merge code at any moment, but we strongly encouraged them to use feature flags. If that wasn’t possible, we asked them to avoid merging any high-risk changes on that particular week.\n\nFriday, September 20 – Thursday, September 26\n\nApart from the additional actions during the first day, each day of the first week of the release cycle basically looks the same.\n\nEarly each morning, nightly builds of the main branch are sent out internally and to our alpha users.\n\nTeams develop and merge new code. The developers and their teams make sure that the code is tested and reviewed beforehand.\n\nBug reports are filed by internal and external alpha users. When the owner of the affected feature is unknown to the reporter, the Release Manager makes sure that the bug report gets assigned to the correct team.\n\nCrash rates and other metrics are tracked for each build both automatically and manually. Automatic bug tickets are created when a crash or other issue exceeds our predefined severity threshold; manual tickets are created when something is deemed worthy of investigation by the Release Manager or any other employee.\n\nTo help in monitoring the status of an upcoming release, we make use of our Release Manager Dashboard, which collects all the relevant release information in the same place:\n\nSome examples of the data found in the dashboard are as follows:\n\nBlocking bugs\n\nLatest build available, passing tests, and distributed through the alpha program in the app store\n\nCrashes and ANRs (App Not Responding) per unit of consumption\n\nDaily usage\n\nStatus of the distribution jobs that distribute the app internally and externally\n\nBy this time, the Audiobooks feature had been turned on for most employees. So in addition to the regular process for this particular release, both the Release Manager and the Audiobooks team looked through all the crashes happening in the client to see if anything might put the Audiobook rollout in jeopardy. Even a seemingly minor crash affecting a small number of employees could signify a potential issue impacting a large user base upon rollout. Therefore, it’s important to investigate and mitigate any issues as soon as possible.\n\nFriday, September 27: Fri-yay is branch day!\n\nOnce a week passed, it was time for the 8.9.2 version to be branched off for releasing, initiating the most intensive part of the release process. Once a release has been branched off, it is regarded as the current release and only critical bug fixes are permitted. Our weekly release cadence allows less critical bugs and new features to be addressed in subsequent releases, and teams generally avoid last-minute changes to minimize risk.\n\nOnce branching is done, the Release Manager coordinates the work of releasing the version as soon as possible with all stakeholders.\n\nTo help us gather additional data on the quality of the release branch, we have a public beta program that is taken from the release branch — these builds are expected to be more stable than our alpha builds.\n\nOn Fridays during Week 2, teams perform manual regression testing on their owned features and report their results. Teams with high confidence in their automated tests and pre-merge routines can opt out of manual testing.\n\nThroughout the testing process, teams may uncover bugs in their own or in other teams’ features and will file tickets for these. Additionally, crash reports and bug reports from internal and external users also trigger the creation of new bug tickets.\n\nFor 8.9.2, the Audiobooks team was particularly vigilant during this phase, meticulously searching for any potential issues. At this point, it’s not uncommon to discuss bugs and decide if they warrant a release blocker. Good release management means objectively looking at risk, potential impact, and workarounds.\n\nFriday, September 27 – Monday, September 30: Getting it ready for submission\n\nDuring the weekend after branching, our beta users provide additional runtime of the app, which either increases our confidence in releasing it or helps us to find issues that had not been found earlier, by the time we get back to work on Monday.\n\nIdeally, we aim to submit the app to the app stores on Monday. However, complex bugs or unforeseen issues can extend this process by a few days. To streamline communication and coordination for each release, the Release Manager, feature teams, and other stakeholders share updates, ask questions, and flag potential concerns on a dedicated Slack channel.\n\nThe Release Manager ensures bugs are assigned to the correct team and prioritized appropriately, manual testing is executed and reported, and any release-blocking bugs (there tend to be perhaps three to five such bugs per release) are fixed on the release branch.\n\nBefore we submit a build to the app stores, we want to make sure the following criteria are met:\n\nAll commits on the release branch are included in the latest build and have passed automated tests.\n\nNo blocking bug tickets remain open.\n\nAll teams have signed off and approved.\n\nCrash rates and other key metrics are below our defined thresholds.\n\nThe app version to be released has been used to play a sufficient amount of content.\n\nThe Release Manager Dashboard provides a clear overview of these criteria, using color-coding (red/yellow/green) for quick assessment.\n\nFor the 8.9.2 release, we provided additional test accounts with the Audiobooks functionality enabled, as well as detailed testing instructions to the various app stores to ensure they were aware of the new features and wouldn’t be caught off guard when we started to roll out the functionality.\n\nTuesday, October 1 – Wednesday, October 2: Rollout\n\nOnce the app is approved for a platform, we roll it out in two phases: first, to a small percentage of users and, then, the following day, to 100% of users.\n\nWhen a release has been rolled out to 1%, we expect the dashboard to look like this:\n\nThe only remaining items, indicated in yellow, needed before full rollout are the ITGC tickets, where we check to see that reporting from the client to the backend is working as necessary. It’s not unusual to uncover minor bugs that, while acceptable in the current version, would be considered blockers for future releases. In severe cases, we might temporarily halt the feature rollout and resume with the next release.\n\nThanks to our large user base, even a small initial rollout percentage allows us to quickly identify critical issues that may have slipped through the cracks during the internal and public alpha and beta testing.\n\nIf we find a severe issue during the first rollout phase, we immediately pause the rollout and the team responsible starts to create a fix. Ideally, the fix is ready before the next version branches, allowing us to submit an updated build. However, if the fix isn’t timely, we may face the difficult decision of canceling either the current or the upcoming release to prevent the complexity of managing two active release branches simultaneously. Once we have reached 100%, we continue monitoring the state of the release over the next week.\n\nFor version 8.9.2, once a sufficient user base was using the new version, the Audiobooks team initiated their phased rollout. This involved gradually enabling the Audiobooks feature for a small percentage of users in specific markets using a backend feature flag.\n\nFortunately, for version 8.9.2, the Audiobooks feature met our quality standards, and the rollout successfully ramped up to 100% over the following days.\n\nUsing the release procedure described above, we are able to roll out more than 95% of releases to all our users. The weekly cadence also means that a canceled release is not the end of the world for the feature teams, since a new release will go out the following week. In the same way, users will be able to get a new version of the app every week as long as it meets our quality standards.\n\nSummary\n\nSpotify’s weekly mobile app release process tries to strike a balance between speed and quality. The person at the helm of the process is the Release Manager, who handles communication and coordination with feature teams and other stakeholders throughout the release cycle with the help of the Release Squad. Tools like the Release Manager Dashboard play an important role in enabling the Release Manager to make fast and accurate decisions.\n\nDetailed documentation of the tools and processes helps guide all the teams involved.\n\nThis robust system allows Spotify to consistently update its app, quickly address issues, and introduce new features like Audiobooks, all while minimizing disruptions to the user experience.\n\nStay tuned for part 2, where we’ll look under the hood to see how the systems (and robots!) that power the Release Manager Dashboard work.", "label": "non_personal"}
{"title": "Heroku AI: Build and Deploy Enterprise Grade MCP Servers", "url": "https://www.heroku.com/blog/building-mcp-servers-on-heroku/", "content": "Agents hold immense power, but their true potential shines when they connect to the real world, fetching data, triggering actions, or leveraging external tools. The Model Context Protocol (MCP) offers a standardized way for AI agents to do this.\n\nMCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools. – official MCP website\n\nHeroku Managed Inference and Agents dramatically simplifies hosting these MCP servers and making them available, not only to itself, but also to external agents like Claude, Cursor, or Agentforce. These new capabilities accelerate industry standardization towards agent interoperability by reducing the infrastructure, security, and discovery challenges in building and running MCP servers. Heroku Managed Inference and Agents provides:\n\nCommunity SDK support : Build your servers using the official MCP SDK, or any other MCP SDK of your choice.\n\n: Build your servers using the official MCP SDK, or any other MCP SDK of your choice. Effortless Management : Once you have a server running, set up your Procfile and push to Heroku. The Managed Inference and Agents add-on automatically manages server registration with the MCP Toolkit.\n\n: Once you have a server running, set up your Procfile and push to Heroku. The Managed Inference and Agents add-on automatically manages server registration with the MCP Toolkit. Unified Endpoint : Managed Inference and Agents automatically has access to all registered servers. Additionally, a MCP Toolkit URL is generated, which can be used to access your servers in external clients.\n\n: Managed Inference and Agents automatically has access to all registered servers. Additionally, a MCP Toolkit URL is generated, which can be used to access your servers in external clients. Only Pay for What You Use: MCP servers managed by the MCP Toolkit are spun up when in use, and are spun down when there are no requests.\n\nThis guide walks you through setting up your own MCP server on Heroku and enabling your Agent to securely and efficiently perform real-world tasks.\n\nBefore getting started\n\nMCP Servers are just like any other software application, and therefore can be deployed to Heroku as standalone apps. So while you could build your own multi-tenant SSE server and deploy it yourself, Heroku MCP Toolkits help you do things that standalone servers cannot do.\n\nFirst and foremost, they make it seamless to integrate servers with your Heroku Managed Inference and Agents. Secondly, they allow tools to be scaled to 0 by default, and spun up only when needed – making them more cost efficient for infrequent requests. Thirdly, they provide code isolation which enables secure code execution for LLM generated code. Finally, they wrap multiple servers in a single url making it incredibly easy to connect with external clients.\n\nGetting started: Create and deploy your first MCP Server\n\nStep 1 – Build your Server Use an official MCP SDK to create an MCP Server. Note: At this stage, Heroku MCP Toolkits only support STDIO servers. We are working on streamlining platform support for SSE/http servers with authentication. MCP Servers are normal Heroku Apps built on the language of your choice. For example, if you are using node, you’ll want to follow best practices and ensure your node and npm engines are set in your package.json like you would typically for a node app on Heroku. Step 2 – Add the MCP process type Define your MCP process via Procfile with a process prefix of mcp* . E.g. mcp-heroku: npm start (example) Step 3 – Deploy your server Once your app is deployed, all mcp* process types will be ready to be picked up by the Heroku Managed Inference and Agents add-on.\n\nFor more examples, take a look at the sample servers listed in our dev center documentation.\n\nCreating an MCP Toolkit\n\nAttach the Heroku Managed Inference and Agents add-on to the app that you just created. This will register any apps defined in the app to the MCP Toolkit. Each new Managed Inference and Agents add-on will correspond to a new MCP Toolkit.\n\nNavigate to Your App: Open your application’s dashboard on Heroku. Go to Resources: Select the “Resources” tab. Add Managed Inference and Agents: Search for “Managed Inference and Agents” in the add-ons section and add it to your app.\n\nWhat plan to select\n\nEach Managed Inference and Agents plan has a corresponding model (ex. Claude 3.5 Haiku or Stable Image Ultra). You should select the model that aligns with your needs. If your goal is to give your model access to MCP tools, then you will need to select one of the Claude chat models. If you have no need for a model, and only want to host MCP tools for external use, that can be done by selecting any plan. Inference usage is metered, so you will incur no cost if there is no usage of Heroku managed models.\n\nAs far as the MCP servers are concerned, you will pay for the dyno units consumed by the one-off dynos that are spun up. The cost of tool calls depends on the specific dyno tier selected for your app, but the default eco dynos, that is about .0008 cents/second. Each individual tool call is capped at 300 seconds.\n\nIf you decide to host your inference on Heroku, your inference model will have the following default tools free of charge. This includes tools like Code Execution and Document/Web Reader.\n\nManaging and using your MCP Toolkit\n\nThe MCP Toolkit configuration can be viewed and managed through a user-friendly tab in the Heroku Managed Inference and Agents add-on. As with all add-ons, navigate to the App Resources page, and click on the Managed Inference and Agents add-on that you provisioned. Navigate to the Tools tab. Here, you will find the following information:\n\nThe list of registered servers, and their statuses The list of tools per server, along with their request schemas\n\nThese tools are all available to your selected Managed Inference model with no extra configuration. Additionally, you will find the MCP Toolkit URL and MCP Toolkit Token on this page, which can be used for integration with external MCP Clients. The MCP Toolkit Token is masked by default for security.\n\nCaution: Your MCP Toolkit Token can be used to trigger actions in your registered MCP servers, so avoid sharing it unless necessary.\n\nFor more information, check out the dev center documentation.\n\nComing soon\n\nWe are actively working on simplifying the process of building SSE/HTTP servers with auth endpoints – both for Heroku Managed Inference and Agents, and for external MCP clients. This will make it possible for servers to access user specific resources, while adhering to the recommended security standards. Additionally, we are building an in-dashboard playground for Managed Inference and Agents so you can run quick experiments with your models and tools.\n\nWe are excited to see what you build with Heroku Managed Inference and Agents and MCP on Heroku! Attend our webinar on May 28 to see a demo and get your questions answered!", "label": "non_personal"}
{"title": "Incident Report: Spotify Outage on April 16, 2025", "url": "https://engineering.atspotify.com/2025/5/incident-report-spotify-outage-on-april-16-2025", "content": "On April 16, Spotify experienced an outage that affected users worldwide. Here is what happened and what we are going to do about it.\n\nContext\n\nWe use Envoy Proxy for our networking perimeter systems. The perimeter is the first piece of our software that receives users (your!) network traffic. It then distributes that traffic to other services. We use cloud regions to distribute that traffic sensibly across the globe.\n\nTo enhance Envoy's capabilities, we develop and integrate our own custom filters. A specific example is our filter for rate limiting, which we discussed in detail during our recent talk at EnvoyCon 2025.\n\nWhat happened?\n\nOn April 16 2025, between 12:20 and 15:45 UTC, we experienced an outage, affecting the majority of our users worldwide. During the incident most traffic was disrupted, except to our Asia Pacific region due to timezone differences. The graph below shows the amount of successful requests on our perimeter, the purple line is the unaffected Asia Pacific region.\n\nWhat caused this outage?\n\nOn the day of the incident we changed the order of our Envoy filters. This change was deemed low risk and as such we applied it to all regions at the same time. Changing the order triggered a bug in one of our filters which in turn caused Envoy to crash. Unlike typical isolated crashes, this crash happened simultaneously on all Envoy instances.\n\nThe immediate restart of all Envoy instances, combined with client side application retry logic, created an unprecedented load spike for the perimeter. The sudden surge in traffic then exposed a misconfiguration. Envoy instances were continuously cycled by Kubernetes as the Envoy max heap size was set higher than the allowed memory limit. As soon as any new Envoy instance started up it received a very large amount of traffic, which in turn caused it to use more than the allowed Kubernetes memory limit. Kubernetes then automatically shut down the instance and the cycle repeated.\n\nLower traffic in our Asia Pacific region at the time of the incident, due to the difference in timezone and time of day, meant the regional Envoy memory usage never reached the kubernetes limit, which is why this region was unaffected.\n\nThe outage was mitigated by increasing the total perimeter server capacity which in turn allowed the Envoy servers to drop under the Kubernetes memory limits. This in turn stopped the continuous cycling of servers.\n\nTimeline\n\n12:18 UTC - Envoy filter order changed and all Envoy instances crash\n\n12:20 UTC - Alarms are triggered indicating a significant drop of incoming traffic\n\n12:28 UTC - Situation escalated, no traffic worldwide except for the Asia Pacific region\n\n14:20 UTC - Traffic fully recovered in the European regions\n\n15:10 UTC - Traffic fully recovered in the US regions\n\n15:40 UTC - All traffic patterns back to normal\n\nWhere do we go from here?\n\nWe recognize the impact such outages can have, and we’re committed to learning from it. Here are the steps we’re taking to improve our systems and prevent similar issues in the future;\n\nWe have fixed the bug causing Envoy to crash\n\nWe have fixed the configuration mismatch between Envoy heap size and Kubernetes memory limits\n\nWe will improve how we roll out configuration changes to our perimeter\n\nWe will improve our monitoring capabilities to be able to catch these issues sooner\n\nAs we have in the past, we will continue to provide transparency on similar occasions so as to hold ourselves accountable and support ongoing improvements to our services.", "label": "non_personal"}
{"title": "Heroku Managed Inference and Agents is now Generally Available", "url": "https://www.heroku.com/blog/managed-inference-and-agents-now-generally-available/", "content": "Many of the most exciting experiences we’re beginning to rely on every day are powered by AI; whether it’s conversational assistants, personalized recommendations or code generation, these experiences are powered by inference systems and intelligent agents. Behind the scenes, developers offload complex decisions, automate tasks, and compose intelligent applications using large language models and tool execution flows. Together, these AI-powered primitives are becoming a key complement to traditional application development, enabling a new wave of developer capabilities.\n\nAt Salesforce, we are helping our customers bring their agentic strategy to life with Heroku, Agentforce, and Data Cloud. These powerful products allow anyone in the company, from business analysts to developers to build robust, custom agents that can transform their business. Behind the scenes, developers offload complex decisions, automate tasks, and compose intelligent applications using large language models and tool execution flows. Together, these AI-powered primitives are becoming a key complement to traditional application development, enabling a new wave of developer capabilities.\n\nHeroku Managed Inference and Agents bring together a set of powerful primitives that make it simple for developers to build, scale, and operate AI-powered features and applications, without the heavy lifting of managing their own AI infrastructure. With access to leading models from top providers and elegant primitives for building agents that can reason, act, and call tools, developers can focus on delivering differentiated experiences for their users, rather than wrangling inference infrastructure or orchestration logic.\n\n﻿\n\nManaged Inference for simplified AI integration\n\nManaged Inference provides ready-to-use access to a curated set of powerful AI models, chosen for their generative power and performance, optimized for ease of use and efficacy in the domains our customers need most. Whether you’re looking to generate text, classify content, summarize documents, or build intelligent workflows, you can now bring AI to your Heroku apps in seconds.\n\nGetting started is as easy as attaching the Heroku Managed Inference and Agents add-on to your app or running: heroku ai:models:create\n\nAgents with Model Context Protocol\n\nExtend Managed Inference with an elegant set of primitives and operations, allowing developers to create agents that can execute code in Heroku’s trusted Dynos, as well as call tools and application logic. These capabilities allow agents to act on behalf of the customer, and to extend both application logic and platform capabilities. Allowing developers to interleave application code, calls to AI, execute logic created by AI, and use of tools, all within the programmatic context. Heroku Managed Inference and Agents can now do more than just generate, it can reason, act, and build by adapting to context, and evolving with your users’ needs.\n\nHeroku Managed Inference and Agents uses the Model Context protocol (MCP) to give your agents new capabilities. MCP helps you build agents and complex workflows by standardizing the way you can provide context and integrate tools. This means you can expose your app’s logic, APIs or custom tools to agents such as Agentforce, Claude or Cursor with custom code.\n\nHeroku Managed Inference and Agents currently supports STDIO MCP servers. Attaching your MCP servers is as simple as attaching your add-on to your Heroku app which contains the MCP server. We are actively developing platform capabilities to support remote MCP servers hosted on heroku, which will feature OAuth integration and buildpack capabilities.\n\nWhat’s next\n\nHeroku Managed Inference and Agents marks a major milestone on our journey to provide AI-native capabilities on the platform and we’ve designed it with the graceful developer and operator experiences you’ve come to expect. Combined with MCP Server support, AppLink for Agentforce integration, and an evolving selection of curated models and tools, developers will be able to rapidly integrate the latest AI advancements and create next-generation, intelligent user experiences.\n\nAgain, to get started, provision Managed Inference and Agents from Heroku Elements or via the command line. We are excited to see what you build with Heroku Managed Inference and Agents! Attend our webinar on May 28 to see a demo and get your questions answered!\n\nTo learn more about Heroku AI, check out our Dev Center docs and try it out for yourself.\n\nInterested in unlocking the full potential of your AI agents? Read Heroku AI: Build and Deploy Enterprise Grade MCP Servers.\n\nStay tuned for more — we’re just getting started.", "label": "non_personal"}
{"title": "Celebrating Five Years of Backstage: From Open Source Project to Enterprise Business", "url": "https://engineering.atspotify.com/2025/4/celebrating-five-years-of-backstage", "content": "Did you know that we sell Spotify’s developer productivity tools and support to other companies, from online retailers and fashion brands to banks and automobile manufacturers? How did Spotify get into the business of selling enterprise developer software? And how does serving the open source community — not to mention paying corporate customers — improve our own internal software practices? Tyson Singer, Spotify’s head of technology and platforms, shares how our homegrown developer portal evolved from an open source project to our newest enterprise software product: Spotify Portal for Backstage.\n\nFive years ago on March 16, 2020, we open sourced Backstage, a framework for building internal developer portals (IDPs) like the one we were using at Spotify. Despite being a very new idea — the term “IDP” didn’t even exist back then — the project took off on day one, and accelerated from there.\n\nToday, Backstage is the IDP platform of choice — with over 3,000 companies having adopted it to build IDPs of their own. Within Spotify, it’s still what our 700 R&D squads rely on to help them ship every day. And, oh yeah, we’re also building an enterprise software business on top of it. So how did Spotify get here? And where will Backstage take us next?\n\nA platform for developer experience\n\nBefore open sourcing Backstage, we’d already been using it internally for years to help us solve a whole range of developer experience problems: context switching, fragmentation, cognitive load, silos, duplication, congestion, dependency management, tech health, compliance — you name it, Backstage could grow to help us address it. As a centralized platform, it contained the chaos of modern software development while making our whole engineering org more productive and more efficient. It became invaluable to how we work — and we didn’t want to lose it.\n\nHow would we lose it? We’d recently gone through a painful migration before with another technology, where we had to move from our custom-built solution to a third-party one that had become the standard. We didn’t want to go through that again with Backstage. So we open sourced it.\n\nBecoming the industry standard\n\nBackstage had become so valuable to us, we thought other companies would find it valuable, too. But we also had another motive for open sourcing it. It wasn’t just about sharing: Our ambition from the very start was for Backstage to become the standard for IDPs. Because if everyone else was using it, we wouldn’t have to migrate off it.\n\nBut you’re not going to become the standard just by being free. The product also has to be good. Even more than that, for us, Backstage had to be great. Because we’re an end user, too — probably the most demanding one. We depend on Backstage to improve the developer experience and productivity of our own teams, which has a direct impact on our business. As an open source project, we’d also get the benefit of outside contributions, addressing use cases we hadn’t even thought of yet. Sounds great, right?\n\nBut of course, be careful what you wish for. Now we were no longer building just for ourselves, now we were building for everyone. And sometimes it feels like those two things might be competing with each other. Could we meet the demands of the outside community while still serving the needs of our own developers?\n\nHow building for others has benefited us\n\nHere’s an example of what it’s been like being both maintainer and end user of the Backstage project: We recently launched a complete rewrite of the entire backend system. This has been a really big endeavor for us. But we saw that adopters really needed an easier way to build and integrate plugins. So we did the work — building with the community and rewriting the whole backend over the course of a year.\n\nAnd when the 1.0 of the new backend launched, it was great. Here are just a few of the things we’ve heard from the community and our enterprise customers alike: “Makes things a lot easier to maintain.” “You used to have to wire things together across multiple files. Now adding a new plugin is a one-liner.” The new backend “was the game changer 🚀🧑‍💻.”\n\nEverything about the new backend is simpler to work with — for the community and for us. So, of course, now we’re in the middle of completely rewriting the frontend, too. And guess what? That will be great for Spotify as end users, as well.\n\nIf we hadn’t open sourced Backstage, I don’t know how long we would have kept living with our previous, more complicated backend system. Because of the community — both its needs and the way it enables us to build better solutions — we were pushed to build a much better version of Backstage for ourselves. And we could also quickly bring those improvements to our enterprise customers — through a no-code UI for installing and managing plugins — in Spotify Portal, our SaaS solution for Backstage adopters.\n\nInternal → open source → commercial: The best of all worlds\n\nIt’s been a lot of push and pull in how we balance internal priorities with external ones. But ultimately, this is a virtuous cycle. The ever-popular Notifications feature, the Events management plugin, and the API Docs plugin, which basically powers our entity pages for API entities — all of these are used in Spotify’s internal Backstage instance, and all were contributions from the community.\n\nBeing part of a developer community beyond Spotify has put us in such a great position for continuing to invest in and grow the Backstage platform. The project has improved our own internal software practices. We’ve benefited from the innovation of the community at large. And we’ve been able to keep investing in the ecosystem in a way that improves our commercial Backstage products and brings more value to our enterprise customers.\n\nIn five years, we’ve gone from basically just a thin framework for building an IDP, to a better IDP than we could have built by ourselves, and now to a SaaS product that we think is the best IDP for the future. This journey can be bumpy. But making that repo public back in 2020 has definitely brought us to a place we never could have imagined today. All of which proves out our original idea: Investing in the community has been an investment in ourselves, as well.\n\nSo, happy 5th birthday, Backstage! We’ve come far in a short time. But we’re expecting even bigger things ahead.\n\nReady to try Backstage for yourself? Apply for the Spotify Portal beta — the fastest way to get up and running with a Backstage IDP of your own. Portal combines the best of open source with the best of Spotify’s developer experience and developer productivity practices.", "label": "non_personal"}
{"title": "Railyard: how we rapidly train machine learning models with Kubernetes", "url": "https://stripe.com/blog/railyard-training-models", "content": "Stripe uses machine learning to respond to our users’ complex, real-world problems. Machine learning powers Radar to block fraud, and Billing to retry failed charges on the network. Stripe serves millions of businesses around the world, and our machine learning infrastructure scores hundreds of millions of predictions across many machine learning models. These models are powered by billions of data points, with hundreds of new models being trained each day. Over time, the volume, quality of data, and number of signals have grown enormously as our models continuously improve in performance.\n\nRunning infrastructure at this scale poses a very practical data science and ML problem: how do we give every team the tools they need to train their models without requiring them to operate their own infrastructure? Our teams also need a stable and fast ML pipeline to continuously update and train new models as they respond to a rapidly changing world. To solve this, we built Railyard, an API and job manager for training these models in a scalable and maintainable way. It’s powered by Kubernetes, a platform we’ve been working with since late 2017. Railyard enables our teams to independently train their models on a daily basis with a centrally managed ML service.\n\nIn many ways, we’ve built Railyard to mirror our approach to products for Stripe’s users: we want teams to focus on their core work training and developing machine learning models rather than operating infrastructure. In this post, we’ll discuss Railyard and best practices for operating machine learning infrastructure we’ve discovered while building this system.\n\nEffective machine learning infrastructure for organizations\n\nWe’ve been running Railyard in production for a year and a half, and our ML teams have converged on it as their common training environment. After training tens of thousands of models on this architecture over that period, here are our biggest takeaways:\n\nBuild a generic API, not tied to any single machine learning framework. Teams have extended Railyard in ways we did not anticipate. We first focused on classifiers, but teams have since adopted the system for applications such as time series forecasting and word2vec style embeddings..\n\nTeams have extended Railyard in ways we did not anticipate. We first focused on classifiers, but teams have since adopted the system for applications such as time series forecasting and word2vec style embeddings.. A fully managed Kubernetes cluster reduces operational burden across an organization. Railyard interacts directly with the Kubernetes API (as opposed to a higher level abstraction), but the cluster is operated entirely by another team. We’re able to learn from their domain knowledge to keep the cluster running reliably so we can focus on ML infrastructure.\n\nRailyard interacts directly with the Kubernetes API (as opposed to a higher level abstraction), but the cluster is operated entirely by another team. We’re able to learn from their domain knowledge to keep the cluster running reliably so we can focus on ML infrastructure. Our Kubernetes cluster gives us great flexibility to scale up and out. We can easily scale our cluster volume when we need to train more models, or quickly add new instance types when we need additional compute resources.\n\nWe can easily scale our cluster volume when we need to train more models, or quickly add new instance types when we need additional compute resources. Centrally tracking model state and ownership allows us to easily observe and debug training jobs. We’ve moved from asking, “Did you save the output of your job anywhere so we can look at?” to “What’s your job ID? We’ll figure the rest out.” We observe aggregate metrics and track the overall performance of training jobs across the cluster.\n\nWe’ve moved from asking, “Did you save the output of your job anywhere so we can look at?” to “What’s your job ID? We’ll figure the rest out.” We observe aggregate metrics and track the overall performance of training jobs across the cluster. Building an API for model training enables us to use it everywhere.Teams can call our API from any service, scheduler, or task runner. We now use Railyard to train models using an Airflow task definition as part of a larger graph of data jobs.\n\nThe Railyard architecture\n\nIn the early days of model training at Stripe, an engineer or data scientist would SSH into an EC2 instance and manually launch a Python process to train a model. This served Stripe’s needs at the time, but had a number of challenges and open questions for our Machine Learning Infrastructure team to address as the company grew:\n\nHow do we scale model training from ad-hoc Python processes on shared EC2 instances to automatically training hundreds of models a day?\n\nHow do we build an interface that is generic enough to support multiple training libraries, frameworks, and paradigms while remaining expressive and concise?\n\nWhat metrics and metadata do we want to track for each model run?\n\nWhere should training jobs be executed?\n\nHow do we scale different compute resource needs (CPU, GPU, memory) for different model types?\n\nOur goal when designing this system was to enable our data scientists to think less about how their machine learning jobs are run on our infrastructure, and instead focus on their core inquiry. Machine learning workflows typically involve multiple steps that include loading data, training models, serializing models, and persisting evaluation data. Because Stripe runs its infrastructure in the cloud, we can manage these processes behind an API: this reduces cognitive burden for our data science and engineering teams and moves local processes to a collaborative, shared environment. After a year and a half of iteration and collaboration with teams across Stripe, we’ve converged on the following system architecture for Railyard. Here’s a high-level overview:\n\nRailyard runs on a Kubernetes cluster and pairs jobs with the right instance type.\n\nRailyard provides a JSON API and is a Scala service that manages job history, state, and provenance in a Postgres database. Jobs are executed and coordinated using the Kubernetes API, and our Kubernetes cluster provides multiple instance types with different compute resources. The cluster can pair jobs with the right instance type: for example, most jobs default to our high-CPU instances, data-intensive jobs run on high-memory instances, and specialized training jobs like deep learning run on GPU instances.\n\nWe package the Python code for model training using Subpar, a Google library that creates a standalone executable that includes all dependencies in one package. This is included in a Docker container, deployed to the AWS Elastic Container Registry, and executed as a Kubernetes job. When Railyard receives an API request, it runs the matching training job and logs are streamed to S3 for inspection. A given job will run through multiple steps, including fetching training and holdout data, training the model, and serializing the trained model and evaluation data to S3. These training results are persisted in Postgres and exposed in the Railyard API.\n\nRailyard’s API design\n\nThe Railyard API allows you to specify everything you need to train a machine learning model, including data sources and model parameters. In designing this API we needed to answer the following question: how do we provide a generic interface for multiple training frameworks while remaining expressive and concise for users?\n\nWe iterated on a few designs with multiple internal customers to understand each use case. Some teams only needed ad-hoc model training and could simply use SQL to fetch features, while others needed to call an API programmatically hundreds of times a day using features stored in S3. We explored a number of different API concepts, arriving at two extremes on either end of the design spectrum.\n\nOn one end, we explored designing a custom DSL to specify the entire training job by encoding scikit-learn components directly in the API itself. Users could include scikit-learn pipeline components in the API specification and would not need to write any Python code themselves.\n\nOn the other end of the spectrum we reviewed designs to allow users to write their own Python classes for their training code with clearly defined input and output interfaces. Our library would be responsible for both the necessary inputs to train models (fetching, filtering, and splitting training and test data) and the outputs of the training pipeline (serializing the model, and writing evaluation and label data). The user would otherwise be responsible for writing all training logic.\n\nIn the end, any DSL-based approach ended up being too inflexible: it either tied us to a given machine learning framework or required that we continuously update the API to keep pace with changing frameworks or libraries. We converged on the following split: our API exposes fields for changing data sources, data filters, feature names, labels, and training parameters, but the core logic for a given training job lives entirely in Python.\n\nHere’s an example of an API request to the Railyard service:\n\n{ // What does this model do? \"model_description\": \"A model to predict fraud\", // What is this model called? \"model_name\": \"fraud_prediction_model\", // What team owns this model? \"owner\": \"machine-learning-infrastructure\", // What project is this model for? \"project\": \"railyard-api-blog-post\", // Which team member is training this model? \"trainer\": \"robstory\", \"data\": { \"features\": [ { // Columns we’re fetching from Hadoop Parquet files \"names\": [\"created_at\", \"charge_type\", \"charge_amount\", \"charge_country\", \"has_fraud_dispute\"], // Our data source is S3 \"source\": \"s3\", // The path to our Parquet data \"path\": \"s3://path/to/parquet/fraud_data.parq\" } ], // The canonical date column in our dataset \"date_column\": \"created_at\", // Data can be filtered multiple times \"filters\": [ // Filter out data before 2018-01-01 { \"feature_name\": \"created_at\", \"predicate\": \"GtEq\", \"feature_value\": { \"string_val\": \"2018-01-01\" } }, // Filter out data after 2019-01-01 { \"feature_name\": \"created_at\", \"predicate\": \"LtEq\", \"feature_value\": { \"string_val\": \"2019-01-01\" } }, // Filter for charges greater than $10.00 { \"feature_name\": \"charge_amount\", \"predicate\": \"Gt\", \"feature_value\": { \"float_val\": 10.00 } }, // Filter for charges in the US or Canada { \"feature_name\": \"charge_country\", \"predicate\": \"IsIn\", \"feature_value\": { \"string_vals\": [\"US\", \"CA\"] } } ], // We can specify how to treat holdout data \"holdout_sampling\": { \"sampling_function\": \"DATE_RANGE\", // Split holdout data from 2018-10-01 to 2019-01-01 // into a new dataset \"date_range_sampling\": { \"date_column\": \"created_at\", \"start_date\": \"2018-10-01\", \"end_date\": \"2019-01-01\" } } }, \"train\": { // The name of the Python workflow we're training \"workflow_name\": \"StripeFraudModel\", // The list of features we're using in our classifier \"classifier_features\": [ \"charge_type\", \"charge_amount\", \"charge_country\" ], \"label\": \"is_fraudulent\", // We can include hyperparameters in our model \"custom_params\": { \"objective\": \"reg:linear\", \"max_depth\": 6, \"n_estimators\": 500, \"min_child_weight\": 50, \"learning_rate\": 0.02 } } } ~\n\nWe learned a few lessons while designing this API:\n\nBe flexible with model parameters. Providing a free-form custom_params field that accepts any valid JSON was very important for our users. We validate most of the API request, but you can’t anticipate every parameter a machine learning engineer or data scientist needs for all of the model types they want to use. This field is most frequently used to include a model’s.\n\nProviding a free-form field that accepts any valid JSON was very important for our users. We validate most of the API request, but you can’t anticipate every parameter a machine learning engineer or data scientist needs for all of the model types they want to use. This field is most frequently used to include a model’s. Not providing a DSL was the right choice (for us).Finding the sweet spot for expressiveness in an API for machine learning is difficult, but so far the approach outlined above has worked out well for our users. Many users only need to change dates, data sources, or hyperparameters when retraining. We haven’t gotten any requests to add more DSL-like features to the API itself.\n\nThe Python workflow\n\nStripe uses Python for all ML model training because of its support for many best-in-class ML libraries and frameworks. When the Railyard project started we only had support for scikit-learn, but have since added XGBoost, PyTorch, and FastText. The ML landscape changes very quickly and we needed a design that didn’t pick winners or constrain users to specific libraries. To enable this extensibility, we defined a framework-agnostic workflow that presents an API contract with users: we pass data in, you pass a trained model back out, and we’ll score and serialize the model for you. Here’s what a minimal Python workflow looks like:\n\nclass StripeFraudModel(StripeMLWorkflow): # A basic model training workflow: all workflows inherit # Railyard’s StripeMLWorkflow class def train(self, training_dataframe, holdout_dataframe): # Construct an estimator using specified hyperparameters estimator = xgboost.XGBRegressor(**self.custom_params) # Serialize the trained model once training is finished; # we're using an in-house serialization library. serializable_estimator = stripe_ml.make_serializable(estimator) # Train our model fitted_model = serializable_estimator.fit( training_dataframe[self.classifier_features], training_dataframe[self.classifier_label] ) # Hand our fitted model back to Railyard to serialize return fitted_model ~\n\nTeams start adopting Railyard with an API specification and a workflow that defines a train method to train a classifier with the data fetched from the API request. The StripeMLWorkflow interface supports extensive customization to adapt to different training approaches and model types. You can preprocess your data before it gets passed in to the train function, define your own data fetching implementation, specify how you want training/holdout data to be scored, and run any other Python code you need. For example, some of our deep learning models have custom data fetching code to stream batches of training data for model training. When your training job finishes you’ll end up with two output: a model identifier for your serialized model that can be put into production, and your evaluation data in S3.\n\nIf you build a machine learning API specification, here are a few things to keep in mind:\n\nInterfaces are important. Users will want to load and transform data in ways you didn’t anticipate, train models using unsupported patterns, and write out unfamiliar types of evaluation data. It’s important to provide standard API interfaces like fetch_data, preprocess, train, and write_evaluation_data that specify some standard data containers (e.g., Pandas DataFrame and Torch Dataset) but are flexible in how they are generated and used.\n\nUsers will want to load and transform data in ways you didn’t anticipate, train models using unsupported patterns, and write out unfamiliar types of evaluation data. It’s important to provide standard API interfaces like fetch_data, preprocess, train, and write_evaluation_data that specify some standard data containers (e.g., Pandas DataFrame and Torch Dataset) but are flexible in how they are generated and used. Users should not need to think about model serialization or persistence. Reducing their cognitive burden makes their lives easier and gives them more time to be creative and focus on modeling and feature engineering. Data scientists and ML engineers already have enough to think about between feature engineering, modeling, evaluation, and more. They should be able to train and hand over their model to your scoring infrastructure without ever needing to think about how it gets serialized or persisted.\n\nReducing their cognitive burden makes their lives easier and gives them more time to be creative and focus on modeling and feature engineering. Data scientists and ML engineers already have enough to think about between feature engineering, modeling, evaluation, and more. They should be able to train and hand over their model to your scoring infrastructure without ever needing to think about how it gets serialized or persisted. Define metrics for each step of the training workflow.Make sure you’re gathering fine-grained metrics for each training step: data loading, model training, model serialization, evaluation data persistence, etc. We store high-level success and failure metrics that can be examined by team, project, or the individual machine performing the training. On a functional level,our team uses these metrics to debug and profile long-running or failed jobs, and provide feedback to the appropriate team when there’s a problem with a given training job. And on a collaborative level, these metrics have changed how our team operates. Moving from a reactive stance (“My model didn’t train, can you help?”) to a proactive one (“Hey, I notice your model didn’t train, here’s what happened”) has helped us be better partners to the many teams we work with.\n\nScaling Kubernetes\n\nRailyard coordinates hundreds of machine learning jobs across our cluster, so effective resource management across our instances is crucial. The first version of Railyard simply ran individual subprocesses from the Scala service that manages all jobs across our cluster. We would get a request, start Java’s ProcessBuilder, and kick off a subprocess to build a Python virtualenv and train the model. This basic implementation allowed us to quickly iterate on our API in our early days, but managing subprocesses wasn’t going to scale very well. We needed a proper job management system that met a few requirements:\n\nScaling the cluster quickly for different resource/instance types\n\nRouting models to specific instances based on their resource needs\n\nJob queueing to prioritize resources for pending work\n\nLuckily, our Orchestration team had been working hard to build a reliable Kubernetes cluster and suggested this new cluster would be a good platform for Railyard’s needs. It was a great fit; a fully managed Kubernetes cluster provides all of the pieces we needed to meet our system’s requirements.\n\nContainerizing Railyard\n\nTo run Railyard jobs on Kubernetes, we needed a way to reliably package our Python code into a fully executable binary. We use Google’s Subpar library which allows us to package all of our Python requirements and source code into a single .par file for execution. The library also includes support for the Bazel build system out of the box. Over the past few years, Stripe has been moving many of its builds to Bazel; we appreciate its speed, correctness, and flexibility in a multi-language environment.\n\nWith Subpar you can define an entrypoint to your Python executable and Bazel will build your .par executable to bundle into a Dockerfile:\n\npar_binary( name = \"railyard_train\", srcs = [\"@.../ml:railyard_srcs\"], data = [\"@.../ml:railyard_data\"], main = \"@.../ml:railyard/train.py\", deps = all_requirements,) ~\n\nWith the Subpar package built, the Kubernetes command only needs to execute it with Python:\n\ncommand: [\"sh\"] args: [\"-c\", \"python /railyard_train.par\"] ~\n\nWithin the Dockerfile we package up any other third-party dependencies that we need for model training, such as the CUDA runtime to provide GPU support for our PyTorch models. After our Docker image is built, we deploy it to AWS’s Elastic Container Repository so our Kubernetes cluster can fetch and run the image.\n\nRunning diverse workloads\n\nSome machine learning tasks can benefit from a specific instance type with resources optimized for a given workload. For example, a deep learning task may be best suited for a GPU instance while fraud models that employ huge datasets should be paired with high-memory instances. To support these mixed workloads we added a new top-level field to the Railyard API request to specify the compute resource for jobs running on Kubernetes:\n\n{ \"compute_resource\": \"GPU\" } ~\n\nRailyard supports training models on CPU, GPU, or memory-optimized instances. Models for our largest datasets can require hundreds of gigabytes of memory to train, while our smaller models can train quickly on smaller (and less expensive) instance types.\n\nScheduling and distributing jobs\n\nRailyard exerts a fine-grained level of control on how Kubernetes distributes jobs across the cluster. For each request, we look at the requested compute resource and set both a Kubernetes Toleration and an Affinity to specify the type of node that we would like to run on. These parameters effectively tell the Kubernetes cluster:\n\nthe affinity , or which nodes the job should run on\n\n, or which nodes the job should run on the toleration, or which nodes should be reserved for specific tasks\n\nKubernetes will use the affinity and toleration properties for a given Kubernetes pod to compute how jobs should be best distributed across or within each node.\n\nKubernetes supports per-job CPU and memory requirements to ensure that workloads don’t experience resource starvation due to neighboring jobs on the same host. In Railyard, we determine limits for all jobs based on their historic and future expected usage of resources. In the case of high-memory or GPU training jobs, these limits are set so that each job gets an entire node to itself; if all nodes are occupied, then the scheduler will place the job in a queue. Jobs with less intensive resource requirements are scheduled on nodes to run in parallel.\n\nWith these parameters in place, we can lean on the Kubernetes resource scheduler to balance our jobs across available nodes. Given a set of job and resource requests, the scheduler will intelligently distribute those jobs to nodes across the cluster.\n\nOne year later: running at scale\n\nMoving our training jobs to a Kubernetes cluster has enabled us to rapidly spin up new resources for different models and expand the cluster to support more training jobs. We can use a single command to expand the cluster and new instance types only require a small configuration change. When the memory requirements of running jobs outgrew our CPU-optimized instance types, we started training on memory-optimized instances the very next day; when we observe a backlog of jobs, we can immediately expand the cluster to process the queue. Model training on Kubernetes is available to any data scientist or engineer at Stripe: all that’s needed is a Python workflow and an API request and they can start training models on any resource type in the cluster.\n\nTo date, we’ve trained almost 100,000 models on Kubernetes, with new models trained each day. Our fraud models automatically retrain on a regular basis using Railyard and Kubernetes, and we’re steadily moving more of Stripe’s models onto an automated retraining cycle. Radar’s fraud model is built on hundreds of distinct ML models and has a dedicated service that trains and deploys all of those models on a daily cadence. Other models retrain regularly using an Airflow task that uses the Railyard API.\n\nWe’ve learned a few key considerations for scaling Kubernetes and effectively managing instances:\n\nInstance flexibility is really important. Teams can have very different machine learning workloads. In any given day we might train thousands of time series forecasts, a long-running word embedding model, or a fraud model with hundreds of gigabytes of data. The ability to quickly add new instance types and expand the cluster are equally important for scalability.\n\nTeams can have very different machine learning workloads. In any given day we might train thousands of time series forecasts, a long-running word embedding model, or a fraud model with hundreds of gigabytes of data. The ability to quickly add new instance types and expand the cluster are equally important for scalability. Managing memory-intensive workflows is hard. Even using various instance sizes and a managed cluster, we still sometimes have jobs that run out of memory and are killed. This is a downside to providing so much flexibility in the Python workflow: modelers are free to write memory-intensive workflows. Kubernetes allows us to proactively kill jobs that are consuming too many resources, but it still results in a failed training job for the modeler. We’re thinking about ways to better manage this, including smart retry behavior to automatically reschedule failed jobs on higher-capacity instances and moving to distributed libraries like dask-ml.\n\nEven using various instance sizes and a managed cluster, we still sometimes have jobs that run out of memory and are killed. This is a downside to providing so much flexibility in the Python workflow: modelers are free to write memory-intensive workflows. Kubernetes allows us to proactively kill jobs that are consuming too many resources, but it still results in a failed training job for the modeler. We’re thinking about ways to better manage this, including smart retry behavior to automatically reschedule failed jobs on higher-capacity instances and moving to distributed libraries like dask-ml. Subpar is an excellent solution for packaging Python code. Managing Python dependencies can be tricky, particularly when you’d like to bundle them as an executable that can be shipped to different instances. If we were to build this from scratch again we would probably take a look at Facebook’s XARs, but Subpar is very compatible with Bazel and it’s been running well in production for over a year.\n\nManaging Python dependencies can be tricky, particularly when you’d like to bundle them as an executable that can be shipped to different instances. If we were to build this from scratch again we would probably take a look at Facebook’s XARs, but Subpar is very compatible with Bazel and it’s been running well in production for over a year. Having a good Kubernetes team is a force multiplier.Railyard could not have been a success without the support of our Orchestration team, which manages our Kubernetes cluster and pushes the platform forward for the whole organization. If we had to manage and operate the cluster in addition to building our services, we would have needed more engineers and taken significantly longer to ship.\n\nBuilding ML infrastructure\n\nWe’ve learned that building common machine learning infrastructure enables teams across Stripe to operate independently and focus on their local ML modeling goals. Over the last year we’ve used Railyard to train thousands of models spanning use cases from forecasting to deep learning. This system has enabled us to build rich functionality for model evaluation and design services to optimize hyperparameters for our models at scale.\n\nWhile there is a wealth of information available on data science and machine learning from the modeling perspective, there isn’t nearly as much published about how companies build and operate their production machine learning infrastructure. Uber, Airbnb, and Lyft have all discussed how their infrastructure operates, and we’re following their lead in introducing the design patterns that have worked for us. We plan to share more lessons from our ML architecture in the months ahead. In the meantime, we’d love to hear from you: please let us know which lessons are most useful and if there are any specific topics about which you’d like to hear more.", "label": "non_personal"}
{"title": "The secret life of DNS packets: investigating complex networks", "url": "https://stripe.com/blog/secret-life-of-dns", "content": "DNS is a critical piece of infrastructure used to facilitate communication across networks. It’s often described as a phonebook: in its most basic form, DNS provides a way to look up a host’s address by an easy-to-remember name. For example, looking up the domain name stripe.com will direct clients to the IP address 53.187.159.182, where one of Stripe’s servers is located. Before any communication can take place, one of the first things a host must do is query a DNS server for the address of the destination host. Since these lookups are a prerequisite for communication, maintaining a reliable DNS service is extremely important. DNS issues can quickly lead to crippling, widespread outages, and you could find yourself in a real bind.\n\nIt’s important to establish good observability practices for these systems so when things go wrong, you can clearly understand how they’re failing and act quickly to minimize any impact. Well-instrumented systems provide visibility into how they operate; establishing a monitoring system and gathering robust metrics are both essential to effectively respond to incidents. This is critical for post-incident analysis when you’re trying to understand the root cause and prevent recurrences in the future.\n\nIn this post, I’ll describe how we monitor our DNS systems and how we used an array of tools to investigate and fix an unexpected spike in DNS errors that we encountered recently.\n\nDNS infrastructure at Stripe\n\nAt Stripe, we operate a cluster of DNS servers running Unbound, a popular open-source DNS resolver that can recursively resolve DNS queries and cache the results. These resolvers are configured to forward DNS queries to different upstream destinations based on the domain in the request. Queries that are used for service discovery are forwarded to our Consul cluster. Queries for domains we configure in Route 53 and any other domains on the public Internet are forwarded to our cluster’s VPC resolver, which is a DNS resolver that AWS provides as part of their VPC offering. We also run resolvers locally on every host, which provides an additional layer of caching.\n\nUnbound runs locally on every host as well as on the DNS servers.\n\nUnbound exposes an extensive set of statistics that we collect and feed into our metrics pipeline. This provides us with visibility into metrics like how many queries are being served, the types of queries, and cache hit ratios.\n\nWe recently observed that for several minutes every hour, the cluster’s DNS servers were returning SERVFAIL responses for a small percentage of internal requests. SERVFAIL is a generic response that DNS servers return when an error occurs, but it doesn’t tell us much about what caused the error.\n\nWithout much to go on initially, we found another clue in the request list depth metric. (You can think of this as Unbound’s internal todo list, where it keeps track of all the DNS requests it needs to resolve.)\n\nAn increase in this metric indicates that Unbound is unable to process messages in a timely fashion, which may be caused by an increase in load. However, the metrics didn’t show a significant increase in the number of DNS queries, and resource consumption didn’t appear to be hitting any limits. Since Unbound resolves queries by contacting external nameservers, another explanation could be that these upstream servers were taking longer to respond.\n\nTracking down the source\n\nWe followed this lead by logging into one of the DNS servers and inspecting Unbound’s request list.\n\n$ unbound-control dump_requestlist thread #0 # type cl name seconds module status 0 A IN s3.amazonaws.com. - iterator wait for 10.0.0.2 1 PTR IN 3.8.25.104.in-addr.arpa. - iterator wait for 10.0.0.2 2 PTR IN 5.101.25.104.in-addr.arpa. - iterator wait for 10.0.0.2 3 PTR IN 5.156.25.104.in-addr.arpa. - iterator wait for 10.0.0.2 4 PTR IN 123.71.25.104.in-addr.arpa. - iterator wait for 10.0.0.2 5 PTR IN 212.28.24.104.in-addr.arpa. - iterator wait for 10.0.0.2 6 PTR IN 18.81.25.104.in-addr.arpa. - iterator wait for 10.0.0.2 7 PTR IN 103.78.24.104.in-addr.arpa. - iterator wait for 10.0.0.2 8 PTR IN 22.43.25.104.in-addr.arpa. - iterator wait for 10.0.0.2 9 PTR IN 24.17.25.104.in-addr.arpa. - iterator wait for 10.0.0.2 10 PTR IN 21.100.25.104.in-addr.arpa. - iterator wait for 10.0.0.2 ... ... ~\n\nThis confirmed that requests were accumulating in the request list. We noticed some interesting details: most of the entries in the list corresponded to reverse DNS lookups (PTR records) and they were all waiting for a response from 10.0.0.2, which is the IP address of the VPC resolver.\n\nWe then used tcpdump to capture the DNS traffic on one of the servers to get a better sense of what was happening and try to identify any patterns. We wanted to make sure we captured the traffic during one of these spikes, so we configured tcpdump to write data to files over a period of time. We split the files across 60 second collection intervals to keep file sizes small, which made it easier to work with them.\n\n# Capture all traffic on port 53 (DNS traffic) # Write data to files in 60 second intervals for 30 minutes # and format the filenames with the current time $ tcpdump -n -tt -i any -W 30 -G 60 -w '%FT%T.pcap' port 53 ~\n\nThe packet captures revealed that during the hourly spike, 90% of requests made to the VPC resolver were reverse DNS queries for IPs in the 104.16.0.0/12 CIDR range. The vast majority of these queries failed with a SERVFAIL response. We used dig to query the VPC resolver with a few of these addresses and confirmed that it took longer to receive responses.\n\nBy looking at the source IPs of clients making the reverse DNS queries, we noticed they were all coming from hosts in our Hadoop cluster. We maintain a database of when Hadoop jobs start and finish, so we were able to correlate these times to the hourly spikes. We finally narrowed down the source of the traffic to one job that analyzes network activity logs and performs a reverse DNS lookup on the IP addresses found in those logs.\n\nOne more surprising detail we discovered in the tcpdump data was that the VPC resolver was not sending back responses to many of the queries. During one of the 60-second collection periods the DNS server sent 257,430 packets to the VPC resolver. The VPC resolver replied back with only 61,385 packets, which averages to 1,023 packets per second. We realized we may be hitting the AWS limit for how much traffic can be sent to a VPC resolver, which is 1,024 packets per second per interface. Our next step was to establish better visibility in our cluster to validate our hypothesis.\n\nCounting packets\n\nAWS exposes its VPC resolver through a static IP address relative to the base IP of the VPC, plus two (for example, if the base IP is 10.0.0.0, then the VPC resolver will be at 10.0.0.2). We need to track the number of packets sent per second to this IP address. One tool that can help us here is iptables, since it keeps track of the number of packets matched by a rule.\n\nWe created a rule that matches traffic headed to the VPC resolver IP address and added it to the OUTPUT chain, which is a set of iptables rules that are applied to all packets sent from the host. We configured the rule to jump to a new chain called VPC_RESOLVER and added an empty rule to that chain. Since our hosts could contain other rules in the OUTPUT chain, we added this rule to isolate matches and make it a little easier to parse the output.\n\nListing the rules, we see the number of packets sent to the VPC resolver in the output:\n\n$ iptables -L -v -n -x Chain OUTPUT (policy ACCEPT 41023 packets, 2569001 bytes) pkts bytes target prot opt in out source destination 41023 2569001 VPC_RESOLVER all -- * * 0.0.0.0/0 10.0.0.2 Chain VPC_RESOLVER (1 references) pkts bytes target prot opt in out source destination 41023 2569001 all -- * * 0.0.0.0/0 0.0.0.0/0 ~\n\nWith this, we wrote a simple service that reads the statistics from the VPC_RESOLVER chain and reports this value through our metrics pipeline.\n\nwhile : do PACKET_COUNT=$(iptables -L VPC_RESOLVER 1 -x -n -v | awk '{ print $1 }') report-metric $PACKET_COUNT \"vpc_resolver.packet_count\" sleep 1 done ~\n\nOnce we started collecting this metric, we could see that the hourly spikes in SERVFAIL responses lined up with periods where the servers were sending too much traffic to the VPC resolver.\n\nTraffic amplification\n\nThe data we saw from iptables (the number of packets per second sent to the VPC resolver) indicated a significant increase in traffic to the VPC resolvers during these periods, and we wanted to better understand what was happening. Taking a closer look at the shape of the traffic coming into the DNS servers from the Hadoop job, we noticed the clients were sending the request five times for every failed reverse lookup. Since the reverse lookups were taking so long or being dropped at the server, the local caching resolver on each host was timing out and continually retrying the requests. On top of this, the DNS servers were also retrying requests, leading to request volume amplifying by an average of 7x.\n\nSpreading the load\n\nOne thing to remember is that the VPC resolver limit is imposed per network interface. Instead of performing the reverse lookups solely on our DNS servers, we could instead distribute the load and have each host contact the VPC resolver independently. With Unbound running on each host we can easily control this behavior. Unbound allows you to specify different forwarding rules per DNS zone. Reverse queries use the special domain in-addr.arpa , so configuring this behavior was a matter of adding a rule that forwards requests for this zone to the VPC resolver.\n\nWe knew that reverse lookups for private addresses stored in Route 53 would likely return faster than reverse lookups for public IPs that required communication with an external nameserver. So we decided to create two forwarding configurations, one for resolving private addresses (the 10.in-addr.arpa. zone) and one for all other reverse queries (the .in-addr.arpa. zone). Both rules were configured to send requests to the VPC resolver. Unbound calculates retry timeouts based on a smoothed average of historical round trip times to upstream servers and maintains separate calculations per forwarding rule. Even if two rules share the same upstream destination the retry timeouts are computed independently, which helps isolate the impact of inconsistent query performance on timeout calculations.\n\nAfter applying the forwarding configuration change to the local Unbound resolvers on the Hadoop nodes we saw that the hourly load spike to the VPC resolvers had gone away, eliminating the surge of SERVFAILS we were seeing:\n\nAdding the VPC resolver packet rate metric gives us a more complete picture of what’s going on in our DNS infrastructure. It alerts us if we approach any resource limits and points us in the right direction when systems are unhealthy. Some other improvements we’re considering include collecting a rolling tcpdump of DNS traffic and periodically logging the output of some of Unbound’s debugging commands, such as the contents of the request list.\n\nVisibility into complex systems\n\nWhen operating such a critical piece of infrastructure like DNS, it’s crucial to understand the health of the various components of the system. The metrics and command line tools that Unbound provides gives us great visibility into one of the core components of our DNS systems. As we saw in this scenario, these types of investigations often uncover areas where monitoring can be improved, and it’s important to address these gaps to better prepare for incident response. Gathering data from multiple sources allows you to see what’s going on in the system from different angles, which can help you narrow in on the root cause during an investigation. This information will also identify if the remediations you put in place have the intended effect. As these systems grow to handle more scale and increase in complexity, how you monitor them must also evolve to understand how different components interact with each other and build confidence that your systems are operating effectively.", "label": "non_personal"}
{"title": "Singapore engineering hub", "url": "https://stripe.com/blog/singapore-eng-office", "content": "Stripe launched in Singapore in 2016. Since then, we’ve seen strong traction, and are proud to work with some of the fastest-growing companies in the region, including Grab, Mobike, and Carousell. Today, we’re increasing our investment: we’re very excited to announce that Singapore is joining Seattle, Dublin, and San Francisco to become Stripe’s fourth global engineering hub.\n\nIn the years ahead, we plan to hire hundreds of people to help us expand our infrastructure, build completely new products, and ensure that Stripe’s product suite works just as well in Southeast Asia as it does in Europe and North America.\n\nMore than 200 million Southeast Asians will come online in the next two years, and the region’s vibrant internet economy—growing to more than $200 billion by 2025—comprises more than 7,000 high-growth startups. We think that this is just the beginning, and that the region will see vast innovation in the years ahead.\n\nOur Singapore hub will include all of the core Stripe functions: product and engineering teams based here will work on expanding the geographic footprint of Stripe’s existing global payments and treasury network, help build completely new products, and further develop the underlying infrastructure powering Stripe. Today, Stripe works with more than 1 million companies in more than 100 countries around the world. Our Singapore team will help those companies be significantly more successful in the world’s fastest-growing internet region.\n\nWe’re proud to be building this presence in Singapore. The country has long understood the importance of global integration and technological advancement, leading the world in everything from containerization to commodities. Singapore embodies a positive-sum economic mindset that serves as an example for the world.\n\nIf you’re interested in working with us to help establish Stripe’s engineering hub in Singapore, please get in touch!", "label": "non_personal"}
{"title": "Designing accessible color systems", "url": "https://stripe.com/blog/accessible-color-systems", "content": "Color contrast is an important aspect of accessibility. Good contrast makes it easier for people with visual impairments to use products, and helps in imperfect conditions like low-light environments or older screens. With this in mind, we recently updated the colors in our user interfaces to be more accessible. Text and icon colors now reliably have legible contrast throughout the Stripe Dashboard and all other products built with our internal interface library.\n\nAchieving the right contrast with color is challenging, especially because color is incredibly subjective and has a big effect on the aesthetics of a product. We wanted to create a color system with hand-picked, vibrant colors that also met standards for accessibility and contrast.\n\nWhen we evaluated external tools to improve color contrast and legibility in our products, we noticed two common approaches to tackling the problem:\n\nHand-pick colors and check their contrast against a standard. Our experience told us that this approach made choosing colors too dependent on trial and error. Generate lighter and darker tints from a set of base colors. Unfortunately, simply darkening or lightening can result in dull or muted colors, which can be difficult to distinguish from each other and often just don’t look good.\n\nWith the existing tools we found, it was hard to create a color system that allowed us to pick great colors while ensuring accessibility. We decided to create a new tool that uses perceptual color models to give real-time feedback about accessibility. This enabled us to quickly create a color scheme that met our needs, and gave us something we could iterate on in the future.\n\nBackground\n\nThe colors we use in our product interfaces are based on our brand color palette. Using these colors in our products allows us to bring some of the character of Stripe’s brand into our interfaces.\n\nUnfortunately, it was difficult to meet (and maintain) contrast guidelines with these colors. The web accessibility guidelines suggest a minimum contrast ratio of 4.5 for small text, and 3.0 for large text. When we audited color usage in our products, we discovered that none of the default text colors we were using for small text (except for black) met the contrast threshold.\n\nChoosing accessible color combinations required each individual designer or engineer to understand the guidelines and select color pairs with enough contrast in each situation. With certain combinations of colors, options were limited and the accessible color combinations just didn’t look good.\n\nWhen we first looked at ways to improve text contrast in our products, we initially explored shifting the default colors for text one step darker on our scale, illustrated by the left column below.\n\nUnfortunately, some of our colors still didn’t have sufficient contrast at the next darkest shade. Once we got to a shade with sufficient contrast on our existing scales (the right column), we lost a lot of the brightness and vibrancy of our colors. The colors pass guidelines on a white background, but they’re dark and muddy and it’s difficult to tell the hues apart.\n\nWithout digging deeper it would be easy to just accept the tradeoff that you need to choose between having accessible colors or colors that look good. In order to get both, we needed to rework our color system from the ground up.\n\nWe wanted to design a new color system that would provide three key benefits out of the box:\n\nPredictable accessibility: Colors have enough contrast to pass accessibility guidelines. Clear, vibrant hues: Users can easily distinguish colors from one another. Consistent visual weight: At each level, no single color appears to take priority over another.\n\nA brief interlude on color spaces\n\nTo explain how we got there, we need to get a little nerdy about color.\n\nWe’re used to working with color on screens in terms of the RGB color space. Colors are specified in terms of how much red, green, and blue light is mixed on screen to make the color.\n\nUnfortunately, while describing colors this way comes naturally to computers, it doesn’t come naturally to humans. Given an RGB color value, what needs to change to make it lighter? More colorful? Add more yellow?\n\nIt’s more intuitive for us to think of colors as organized by three attributes:\n\nHue: What color is it?\n\nWhat color is it? Chroma: How colorful is it?\n\nHow colorful is it? Lightness: How bright is it?\n\nA popular color space that supports specifying colors in this way is HSL. It’s well supported in design tools and popular code libraries for color manipulation. There’s just one problem: the way HSL calculates lightness is flawed. What most color spaces don’t take into account is that different hues are inherently perceived as different levels of lightness by the human eye—at the same level of mathematical lightness, yellow appears lighter than blue.\n\nThe image below is a set of colors with the same lightness and saturation in a display color space. While the color space claims the saturation and lightness are all the same, our eyes disagree. Notice that some of these colors appear lighter or more saturated than others. For example, the blues appear especially dark and the yellows and greens appear especially light.\n\nThere are color spaces which attempt to model human perception of color. Perceptually uniform color spaces model colors based on factors that relate more to human vision, and perform sophisticated color transformations to ensure that these dimensions reflect how human vision works.\n\nWhen we take a sample of colors with the same lightness and saturation in a perceptually uniform color space, we can observe a significant difference. These colors appear to blend together, and each color appears to be just as light and as saturated as the rest. This is perceptual uniformity at work.\n\nThere are surprisingly few tools that support perceptually uniform color models, and none that came close to helping us design a color palette. So we built our own.\n\nVisualizing color\n\nWe built a web interface to allow us to visualize and manipulate our color system using perceptually uniform color models. The tool gave us an immediate feedback loop while we were iterating on our colors—we could see the effect of every change.\n\nThe color space illustrated above is known as CIELAB or, affectionately, Lab. The L in Lab stands for lightness, but unlike the lightness in HSL, it’s designed to be perceptually uniform. By translating our color scales into the Lab color space, we can adjust our colors based on their perceptual contrast and visually compare the results.\n\nThe diagram below shows the lightness and contrast values of our previous color palette visualized in the color tool. You can see that the perceptual lightness of each of our colors follows a different curve, with the yellow and green colors much lighter than the blues and purples at the same point.\n\nBy manipulating our colors in perceptually uniform color space, we were able to produce a set of colors which have uniform contrast across all the hues, and preserve as much of the intended hue and saturation of our current colors. In the proposed colors, yellow has the same contrast range as blue, but they still look like our colors.\n\nIn the diagram below, you can see the perceptual lightness for each color follows the same curve, meaning each color (the labels on the left) has the same contrast value at a given level (the number labels on the top).\n\nOur new tool also showed us what was possible. Visualizing a perceptually uniform color model allowed us to see the constraints of visual perception. The shaded areas in the charts represent so-called imaginary colors which aren’t actually reproducible or perceivable. It turns out “really dark yellow” isn’t actually a thing.\n\nMost tools for mixing colors allow you to set values across the full range for each parameter, and just clip the colors or return the nearest fit colors that don’t actually represent the parameters you set. Visualizing the available color space in real time as we made changes allowed us to iterate much faster because we could tell what changes were possible and what changes moved us closer to our goal: “bright”, differentiated colors that met the appropriate contrast guidelines.\n\nAt some points, finding a set of colors that worked together was like threading a needle. Here, the shaded areas show how limited the space is to actually find a combination of values that allows for roughly equal lightness for all hues.\n\nResults\n\nAfter a lot of iterations and tests with real components and interfaces, we arrived at a palette of colors that achieved our goals: our colors predictably passed accessibility guidelines, kept their clear, vibrant hues, and maintained a consistent visual weight across hues.\n\nOur new default colors for text and icons now pass the accessibility contrast threshold defined in the WCAG 2.0 guidelines.\n\nIn addition to passing contrast guidelines over white backgrounds, each color also passes when displayed atop the lightest color value in any hue. Since we commonly use these lightly tinted backgrounds to offset or highlight sections, this makes it simple and predictable to ensure text has sufficient contrast throughout our products.\n\nBecause the new colors are uniformly organized based on contrast, we also have straightforward guidelines built-in for choosing appropriate contrast pairs in less common cases. Any two colors are guaranteed to have sufficient contrast for small text if they are at least five levels apart, and at least four levels apart for icons and large text.\n\nWith contrast guidelines built in to the system, it’s simple to make adjustments for color contrast in different components with predictable results.\n\nFor example, we redesigned our Badge component to use a color background to clearly differentiate each color. At the lightest possible value, the colors were too difficult to distinguish from each other. By shifting both the background and the text color up one level, we were able to maintain text contrast across all badge colors without fine tuning each color combination individually.\n\nConclusion\n\nWe learned that designing accessible color systems doesn’t have to mean fumbling around in the dark. We just needed to change how we thought about color:\n\nUse a perceptually uniform color model\n\nWhen designing an accessible color system, using a perceptually uniform color model (like CIELAB) helped us understand how each color appears to our eyes as opposed to how it appears to a computer. This allowed us to validate our intuitions and use numbers to compare the lightness and colorfulness of all of our colors.\n\nAccessible doesn’t mean vibrant\n\nThe WCAG accessibility standard intentionally only focuses on the contrast between a foreground and a background color—not how vibrant they appear. Understanding how vibrant each color appears can helps to distinguish hues from one another.\n\nColor is hard to reason about, tools can help\n\nOne of the pitfalls of perceptually uniform color models is that there are impossible colors—there’s no such thing as “very colorful dark yellow” or “vibrant light royal blue”. Building our own tool helped us see exactly which colors were possible and allowed us to rapidly iterate on our color palette until we produced a palette that was accessible, vibrant, and still felt like Stripe.\n\nAdditional resources\n\nTo learn more about color, we recommend the following resources:", "label": "non_personal"}
{"title": "Introducing the Stripe CLI", "url": "https://stripe.com/blog/stripe-cli", "content": "Building and testing a Stripe integration can require frequent switching between the terminal, your code editor, and the Dashboard. Today, we’re excited to launch the Stripe command-line interface (CLI). It lets you interact with Stripe right from the terminal and makes it easier to build, test, and manage your integration.\n\nTo start, the CLI will let you test webhooks, tail real-time API logs, and create or update API objects. Here’s a preview of some of the features:\n\nSimplify webhook setup and testing\n\nStripe sends a variety of webhooks, which let you listen and respond to specific events programmatically. Run stripe listen with the CLI to forward webhooks to your local web server during development—no third-party tunneling tools required. You can also trigger and test specific webhook events with stripe trigger .\n\nDebug faster with real-time logs\n\nWhile integrating, it can be useful to look at logs to fix any issues. You can now use stripe logs tail to stream API request logs in real time in the terminal in addition to viewing these logs from the Dashboard. Quickly inspect parameters or JSON responses and debug errors as they happen.\n\nSpeed up common tasks and workflows\n\nYou can now create, retrieve, update, or delete any Stripe object directly from the CLI in both test and live mode. For example, you can use stripe customers create and specify parameters for properties inline.\n\nSince you can pipe results into other commands, this can be a simple and powerful way to automate tasks. Here’s an example:\n\n$ stripe subscriptions list \\ --live \\ --status past_due \\ --expand data.customer | \\ jq -r \".data[] | [.customer.name, .customer.email] | @csv\" ~\n\nThe above command uses the CLI to list live subscriptions that are past due, pipes the JSON response to jq to extract the customer name and email, and exports the data in CSV format.\n\nTo see a full list of supported commands, run stripe help or visit the docs to learn more.\n\nGetting started\n\nThe Stripe CLI natively supports macOS, Windows, and Linux. You can also pull our Docker image to use in automated testing or a continuous integration setup.", "label": "non_personal"}
{"title": "Similarity clustering to catch fraud rings", "url": "https://stripe.com/blog/similarity-clustering", "content": "Stripe enables businesses in many countries worldwide to onboard easily so they can accept payments as quickly as possible. Stripe’s scale makes our platform a common target for payments fraud and cybercrime, so we’ve built a deep understanding of the patterns bad actors use. We take these threats seriously because they harm both our users and our ecosystem; every fraudulent transaction we circumvent keeps anyone impacted from having a bad day.\n\nWe provide our risk analysts with automated tools to make informed decisions while sifting legitimate users from potentially fraudulent accounts. One of the most useful tools we’ve developed uses machine learning to identify similar clusters of accounts created by fraudsters trying to scale their operations. Many of these attempts are easy to detect and we can reverse engineer the fingerprints they leave behind to shut them down in real-time. In turn, this allows our analysts to spend more time on sophisticated cases that have the potential to do more harm to our users.\n\nFraud in the payments ecosystem\n\nFraud can generally be separated into two large categories: transaction fraud and merchant fraud. Transaction fraud applies to individual charges (such as those protected by Radar), where a fraudster may purchase items with a stolen credit card to resell later.\n\nMerchant fraud occurs when someone signs up for a Stripe account to later defraud cardholders. For example, a fraudster may attempt to use stolen card numbers through their account, so they’ll try to provide a valid website, account activity, and charge activity to appear legitimate. The fraudster hopes to be paid out to their bank account before Stripe finds out. Eventually, the actual cardholders will request a chargeback from their bank for the unauthorized transaction. Stripe will reimburse chargebacks to issuing banks (and by proxy, the cardholder) and attempt to debit the fraudster’s account. However, if they have already been paid out then it may be too late to recover those funds and Stripe ultimately covers those costs as fraud losses.\n\nFraudsters also may attempt to defraud Stripe at a larger scale by setting up a predatory or scam business. For example, the fraudster will create a Stripe account, claiming to sell expensive apparel or electronics for low prices. Unsuspecting customers think they are getting a great deal, but they never receive the product they ordered. Once again, the fraudster hopes to be paid out before they are shut down or overwhelmed with chargebacks.\n\nUsing similarity information to reduce fraud\n\nFraudsters tend to create Stripe accounts with reused information and attributes. Typically, low-effort fraudsters will not try to hide links to previous accounts, and this activity can be detected immediately at signup. More sophisticated fraudsters will put more work into hiding their tracks in order to prevent any association with prior fraud attempts. Some attributes like name or date of birth are trivial to fabricate, whereas others are more difficult—for example, it requires significant effort to obtain a new bank account.\n\nLinking accounts together via shared attributes is reasonably effective at catching obvious fraud attempts, but we wanted to move from a system based on heuristics to one powered by machine learning models. While heuristics may be effective in certain cases, machine learning models are significantly more effective at learning predictive rules.\n\nSuppose a pair of accounts are assigned a similarity score based on the number of attributes they share. This similarity score could then help predict future behavior: if an account looks similar to a known fraudulent account, there’s a significant likelihood they are more likely to also be fraudulent. The challenge here is to accurately quantify similarity. For example, two accounts who share dates of birth should have a lower similarity score than two accounts who share a bank account.\n\nBy training a machine learning model, we remove the need for guesswork and hand-constructed heuristics. Now, we can automatically retrain the model over time as we obtain more data. Automatic retraining enables our models to continually improve in accuracy, adapt to new fraud trends, and learn the signatures of particular adversarial groups.\n\nChoosing a clustering approach\n\nMachine learning tasks are generally classified as either supervised or unsupervised. The goal of supervised learning is to make predictions given an existing dataset of labeled examples (for example, a label that indicates whether an account is fraudulent), whereas in unsupervised learning the usual goal is learn a generative model for the raw data (in other words, to understand the underlying structure of the data). Traditionally, clustering tasks fall into the class of unsupervised learning: unlabeled data needs to be grouped into clusters that capture some understanding of similarity or likeness.\n\nFortunately, we’re able to use supervised models, which are generally easier to train and may be more accurate. We already have a large body of data demonstrating whether a given account has been created by a fraudster based on the downstream impact (e.g. we observe a significant number of chargebacks and fraud losses). This allows us to confidently label millions of legitimate and illegitimate businesses from our dataset.\n\nIn particular, our approach is an example of similarity learning where the objective is to learn a symmetric function based on training data. Over the years, our risk underwriting teams have manually compiled many examples of existing clusters of fraudulent accounts through our investigations of fraud rings, and we can use these reference clusters as training data to learn our similarity function. By sampling edges from these groups, we obtain a dataset consisting of pairs of accounts along with a label for each pair indicating whether or not the two accounts belong to the same cluster. We use intra-cluster edges as positive training examples and inter-cluster edges as negative training examples, where an edge denotes a pair of accounts.\n\nWe use known clusters of accounts to train our predictive models.\n\nNow that we have the labels specified, we must decide what features to use for our model. We want to convert pairs of Stripe accounts into useful model inputs that have predictive power. The feature generation process takes two Stripe accounts and produces a number of features that are defined on the pair. Due to the rich nature of Stripe accounts and their associated data, we can construct an extensive set of features for any given pair. Some examples of the features we’d include are categorical features that store the values of common attributes such as the account’s email domain, any overlap in card numbers used on both accounts, and measures of text similarity.\n\nUsing gradient-boosted decision trees\n\nBecause of the wide variety of features we can construct from given pairs of accounts, we decided to use gradient-boosted decision trees (GBDTs) to represent our similarity model. In practice, we’ve found GBDTs strike the right balance between being easy to train, having strong predictive power and being robust despite variations in the data. When we started this project we wanted to get something out the door quickly that was effective, had well-understood properties, and was straightforward to fine-tune. The variant that we use, XGBoost, is one of the best performing off-the-shelf models for cases with structured (also known as tabular) data, and we have well-developed infrastructure to train and serve them. You can read more about the infrastructure we use to train machine learning models at Stripe in a previous post.\n\nNow that we have a trained model, we can use it to predict fraudulent activity. Since this model operates on pairs of Stripe accounts, it’s not feasible to feed it all possible pairs of accounts and compute scores across all pairs. Instead, we first generate a candidate set of edges to be scored. We do this by taking recently created Stripe accounts and creating edges between accounts that share certain attributes. Although this isn’t an exhaustive approach, this heuristic works well in practice to prune the set of candidate edges to a reasonable number.\n\nOnce the candidate edges are scored, we then filter edges by selecting those with a similarity score above some threshold. We then compute the connected components on the resulting graph. The final output is a set of high-fidelity account clusters which we can analyze, process, or manually inspect together as a unit. In particular, a fraud analyst may want to examine clusters which contain known fraudulent accounts and investigate the remaining accounts in that cluster.\n\nThis is an iterative process; as each individual cluster grows, we can quickly identify increasing similarity as fake accounts in a fraudster’s operation are created. And the more fraud rings we detect and shutdown at Stripe, the more accurate our clustering model becomes at identifying new clusters in the future.\n\nEach edge is weighted by a similarity score; we identify clusters by finding connected components in the resulting graph.\n\nBenefits of the clustering system\n\nSo far, we’ve discussed the overall structure of the account clustering system. Although we have other models and systems in place to catch fraudulent accounts, using clustering information has the following advantages:\n\nWe’re even better at catching obvious fraud. It’s difficult for fraudsters to completely separate new accounts from previous accounts they’ve created in the past, or from accounts created by other fraudsters. Whether this is due to reusing basic attribute data or more complex measures of similarity, the account clustering system catches and blocks hundreds of fraudulent accounts weekly with very few false positives.\n\nIt’s difficult for fraudsters to completely separate new accounts from previous accounts they’ve created in the past, or from accounts created by other fraudsters. Whether this is due to reusing basic attribute data or more complex measures of similarity, the account clustering system catches and blocks hundreds of fraudulent accounts weekly with very few false positives. Fraudsters can only use their resources once. Whenever someone decides to defraud Stripe, they need to invest in resources such as stolen IDs and bank accounts, each of which incur monetary cost or inconvenience. In effect, by requiring fraudsters to use a new set of resources every time they create a Stripe account, we slow them down and increase the cost of defrauding Stripe. Clustering is a key tool since it invalidates resources such as bank accounts that have been previously used on fraudulent accounts.\n\nWhenever someone decides to defraud Stripe, they need to invest in resources such as stolen IDs and bank accounts, each of which incur monetary cost or inconvenience. In effect, by requiring fraudsters to use a new set of resources every time they create a Stripe account, we slow them down and increase the cost of defrauding Stripe. Clustering is a key tool since it invalidates resources such as bank accounts that have been previously used on fraudulent accounts. Our risk analysts conduct more efficient reviews. When accounts require manual inspection by an analyst, they spend time trying to understand the intentions and motivations of the person behind the account. Analysts focus on the details of the business to sift legitimate users from a set of identified potentially fraudulent accounts. With the help of our clustering technique, analysts can easily identify common patterns and outliers and apply the same judgments to multiple accounts at once with a smaller likelihood of error.\n\nWhen accounts require manual inspection by an analyst, they spend time trying to understand the intentions and motivations of the person behind the account. Analysts focus on the details of the business to sift legitimate users from a set of identified potentially fraudulent accounts. With the help of our clustering technique, analysts can easily identify common patterns and outliers and apply the same judgments to multiple accounts at once with a smaller likelihood of error. Account clusters are a building block for other systems. Understanding whether two accounts are duplicates or measuring their degree of similarity is a useful primitive that extends beyond the use cases described here. For example, we use the similarity model to expand our training sets for models which have sparse training data.\n\nCatching fraud in action\n\nStripe faces a multitude of threats from fraudsters who attempt to steal money in creative and complex ways. Identifying similarities between accounts and clustering them together enhances our effectiveness and improves our ability to block fraudulent accounts. Clustering accounts together and identifying duplicate attempts to create fraudulent accounts makes life more difficult for fraudsters. One goal of our models is to change the economic model of fraud by raising the required cost for unused bank accounts, IP addresses, devices and other tools they use. This leads to a negative expected value for fraudsters, weakens the underlying supply chain for stolen credentials and user data, and disincentivizes committing fraud at scale.\n\nWe often think about fraud as an adversarial game; uncovering fraudulent clusters allows us to tip the game in our favor. Using common tools like XGBoost enabled us to quickly deploy a solution that naturally fit into our machine learning platform and allows us to easily adapt our approach over time. We’re continuing to explore new techniques to catch fraud to ensure Stripe can reliably operate a low-friction global payment network for millions of businesses.", "label": "non_personal"}
{"title": "Stripe’s remote engineering hub, one year in", "url": "https://stripe.com/blog/remote-hub-one-year", "content": "Last May, Stripe launched our remote engineering hub, a virtual office coequal with our physical engineering offices in San Francisco, Seattle, Dublin, and Singapore. We set out to hire 100 new remote engineers over the year—and did. They now work across every engineering group at Stripe. Over the last year, we’ve tripled the number of permanently remote engineers, up to 22% of our engineering population. We also hired more remote employees across all other teams, and tripled the number of remote Stripes across the company.\n\nLike many organizations, Stripe has temporarily become fully remote to support our employees and customers during the COVID-19 pandemic. Distributed work isn’t new to Stripe. We’ve had remote employees since inception—and formally began hiring remote engineers in 2013. But as we grew, we developed a heavily office-centric organizational structure. Last year, we set out to rebalance our mix of remote and centralized working by establishing our virtual hub. It’s now the backbone of a new working model for the whole company.\n\nWe think our experience might be interesting, particularly for businesses that haven’t been fully distributed from the start or are considering flipping the switch to being fully remote, even after the pandemic. We’ve seen promising gains in how we communicate, build more resilient and relevant products, and reach and retain talented engineers. Here is what we learned.\n\nDeploying engineers closer to customers has been a boon\n\nOne of our goals in establishing the remote hub was to connect more closely with our customers. A year ago, when an overwhelming majority of our engineering staff was located in two cities, it was more difficult to support a global user population with products that felt locally native in supporting commerce. Now, for example, our Developer Support Engineering team is distributed across three continents—and two thirds of the team operate outside of a Stripe office. Units within the team own one of four coverage areas (each spans approximately six time zones), enabling them to provide more timely, local support to users, as well as run uniform, simultaneous, and regionally resonant customer events around the world. We feel closer to customers because we literally are.\n\nThis shift has markedly influenced our product development roadmaps. We’ve made substantial improvements to support non-card payment methods throughout our product suite. Back in the day, we referred to these internally as “alternative payment methods” because from the perspective of someone living in San Francisco, that is accurate. However, for many of our global users, credit cards are the alternative still crossing the chasm to mass acceptance. Commerce is global, but the payment experience is local.\n\nWe built Checkout to maximize conversion by intelligently prioritizing which payment methods are presented to the customer depending on where they’re located. Whether in Mexico or Malaysia, a company’s checkout experience should feel local and include the payment methods customers are most likely to prefer. One way to more naturally build in local considerations into global products is if they’re built by remote, distributed teams. One member of the Checkout team launched a local push payment method from Singapore and another shipped address collection from Maryland. Given the nature of Checkout, we hypothesize that the more distributed the team becomes, the closer we get to the needs and mindsets of more users.\n\nCultural improvements to support remote work have spilled over\n\nWe approached the building of our remote hub, in many ways, like any one of our other engineering hubs around the world. The standards and investments we’ve established for other Stripe offices have ported over well to our remote hub. We nominated a site lead (Aditya Mukerjee for the first year) to be responsible for the overall happiness and productivity of the hub. We ensure that our leadership regularly visits the hub via Zoom meetings to lead discussions, answer questions, and provide a sense of connection. We encourage virtual coffee chats to promote a sense of belonging. We survey the team regularly and review feedback and people data, so that we can understand both the shared needs of our employees and the particular needs of a hub.\n\nOur remote hub’s organizational infrastructure enabled us to run experiments on individual engineering teams to determine how to best integrate remote engineers—and then horizontally scale the ones which generated notable successes. We still have more to discover, but have been seeing some themes emerge.\n\nOne is the loneliest number. The typical engineering team at Stripe has five to eight engineers. When only one member of that team is remote, they often suffer a combination of isolation (both socially and with respect to work-related decision making) and organizational burden (because they are effectively responsible for rearchitecting the team’s processes to be remote-friendly in addition to doing their actual job). Instead, moving multiple remote engineers simultaneously onto a team has yielded much better results for their productivity and happiness. This shift acts as a forcing function to support asynchronous communication, better distribute the workload, and accelerate the adoption of team norms to socially include all members.\n\nOne of our remote engineering teams works on invoices for our billing product. Previously, the invoices team was based out of our San Francisco headquarters. Over the last year, it has expanded to nine members distributed across nine cities and two countries, with an engineering manager based out of North Carolina, three time zones away from headquarters. The team has delivered significant improvements to Stripe’s invoicing products and quintupled their growth.\n\nThe invoices team, including one headquarters-based engineer, operates remote-first. Product and architectural decisions are documented in writing, code review SLAs account for time zone differences, and they set aside intentional time each week to connect with one another. Recurring meetings are prized time, as one of the only synchronous touchpoints the team members have together, but if there’s no agenda, the team is ruthless about canceling it. It’s the time slot that’s sacred; the discussions must be intentional. The team embraces ad-hoc meetings and pairing time to unblock decisions or work through complex technical problems.\n\nWe have also seen a flowering of cultural rituals. Teams find new ways to take a break together. Everyone might do individual tea orders and then post photos in their Slack channel or snack together on Zoom. Scheduled unstructured hangout sessions have also multiplied. We’ve seen parallel #TEAM-social channels adjacent to most #TEAM-work channels on Slack so that team members have an outlet to post about kids, hobbies, and other watercooler banter subjects without feeling like they’re intruding on a workspace. We’ve also experimented with entirely virtual offsites, particularly the bonding component. One team ran a simulated lunar crash exercise that required participants to collectively identify and rank the most critical tools needed to survive. We do not yet have any extraplanetary remotes (but watch this space).\n\nBetter tooling and communication norms have paid dividends. We rely on onboarding to set the tone for an employee’s relationship with the company, their team, and their work. An unexpected benefit to onboarding cohorts of remote engineers was that we got better at supporting engineers everywhere. Our onboarding curriculum became more polished and efficient, given the limited window of the work day shared by regionally scattered remote employees. We moved as many tasks as possible from synchronous batches to asynchronous processing via written documentation, videos, or small groups sharing time zones. These changes reduce the ramp-up time and friction for remote employees, as well as streamline traditional in-person onboarding.\n\nWhen onboarding gets more efficient and asynchronous, teams can scale independently and at a faster rate. Over the past year, we’ve grown small engineering teams in offices like Tokyo and much larger engineering teams in all our engineering hubs. Those teams have quickly advocated for and brought to market locally-relevant products. Recently, engineers based in Japan and Latin America worked together to enhance local payment rails so that users could receive funds in a few working days rather than a month.\n\nWe have invested substantially in internal tooling and explicit communication norms to support the productivity of remote engineers. This includes tooling to coordinate deploys for our most sensitive systems (so that engineers don’t have to synchronously coordinate in-person), incident management tooling which rapidly spins up a virtual response room on demand, and socializing shared scheduled memos rather than daily in-person standups to help engineers stay informed about teammates’ work.\n\nEfforts like these turned out to be critical during our COVID-19 response efforts. We closed all of our offices worldwide and worked to support users going through extremely heterogeneous shocks to their business models, ranging from surges in demand and transactions for delivery startups to cash-flow challenges as demand dried up and refunds spiked for travel companies.\n\nThis would have, candidly, been extremely difficult for Stripe a few years ago, and is still not easy by any stretch of the imagination. But we continue to develop and ship new products, thanks in part to an increased focus on supporting remote engineers last May. Earlier this year, we wouldn’t have guessed that early preparations undertaken to support a fraction of our engineers would benefit all our engineers—and the rest of Stripe.\n\nWe are tapping pools of candidates we could not reach before. Stripe, like all software companies, depends on having talented engineers from a wide range of life experiences, professional backgrounds, and seniority levels. We have found that increasing our population of remote engineers has helped to improve our company along several of these dimensions.\n\nOne demographic we have noticed interact interestingly with remote-friendly policies is engineers with family responsibilities. They often need to make hard choices about which housing markets to live in to support their family and their career. When we had fewer remote roles, we unwittingly narrowed our applicant pool of engineers with families, and missed hiring talented engineers. We also occasionally had engineers leave to favor their partners’ career aspirations, which sometimes required moving away from cities we had offices in. Our expanded ability to hire and retain engineers from many more places enables us to attract many talented engineers who don’t feel that working for us would require unpalatable tradeoffs.\n\nWe built our remote hub by initially seeking out engineers with significant experience working remotely; this naturally led us to hire more seasoned engineers. Similarly, we’ve seen anecdotal shifts in the composition of professional experience across engineers. Stripes in San Francisco have often had most of their professional experience at startups or in large consumer Internet companies, because those sorts of companies employ most engineers in San Francisco. We now hire engineers in cities where financial companies, larger enterprises, and agencies hire larger swaths of the engineering population. This helps us incorporate more DNA from their practices and informs how we build products for adoption for those sorts of companies.\n\nLast May, we focused on hiring remote engineers, product managers, engineering managers, and technical program managers. But once we were able to support those roles remotely, Stripes across the company started to transition from working in our offices to doing their jobs remotely. They span teams and seniority, and among their ranks are tenured leaders and engineering interns. (This summer, we’re running an entirely remote intern program for 76 engineering interns.)\n\nThere are now hundreds of remote Stripes across our engineering and business teams. Three out of four teams at Stripe have at least one remote worker.\n\nWe are increasing our investment\n\nOur experience developing our remote hub this past year has exceeded our expectations. We continue to test and trial remote work across Stripe, but it’s no longer an experiment. We will accelerate our investment in remote Stripes and the growth of the remote hub. Here’s how:\n\nWe are broadening the positions for which we will hire remotely. Last year, we intended to hire a mix of engineering individual contributors, engineering managers, and non-engineering technical roles such as product managers and designers into our remote hub. As of the publish date of this post, we now have open, remote roles on our design, legal and marketing teams.\n\nWe support this bottom-up transformation with more explicit company-wide direction so that we can meet our goals in remote hiring across technical and non-technical functions alike. We expect the number of remote-eligible roles will continue to grow; changes caused by COVID-19 are accelerating this transition.\n\nWe have increased the places we can hire remotely from. Today, Stripes are based in over 50 cities worldwide. We now have the capability to hire remote employees in Australia, Brazil, Canada, France, Germany, India, Indonesia, Ireland, Japan, Mexico, Netherlands, Singapore, Sweden, UK, and the U.S. We’re setting the foundation for remote hiring in many more countries, but are rate limited on hiring remotes in countries where we don’t already have an office presence. In those cases, we need to tackle substantial legal and logistical challenges on a country-by-country, city-by-city, and sometimes neighborhood-by-neighborhood basis. Other organizations seeking to increase remote hires will have to grapple with people systems and disparate tax regimes.\n\nThis is a fractally complex project and has variable lead times depending on the jurisdictions involved, where you have preexisting legal or business infrastructure to leverage, and what compromises you are willing to make prior to onboarding employees in a jurisdiction. To satisfy these requirements, companies inevitably need to manage parallel hiring systems to support on-site and remote employees; common standards would accelerate the world’s migration to remote work. As an example, Stripe must be able to issue equity before we make offers in a jurisdiction, whereas many other employee benefits can roll out more gradually.\n\nWe are improving the quality of life for remote engineers. We care that all of our employees have a successful experience at Stripe. To gauge this, we run bi-annual surveys. In our latest one, we found that, compared to non-remote employees, remote Stripes are more likely to have a sense of connection to their team, balance their workload and life demands, and stay longer at Stripe. Remote Stripes score lower when asked whether Stripe fosters a sense of connection for employees in their location. However, 77% said that their location is not a barrier to having an impact at Stripe, a 10% increase from a year ago.\n\nOur goal in the coming year is to ensure remote engineers feel at a minimum the same level of impact, team connectedness, and velocity in career progression as any other engineer.\n\nWe want to talk to you\n\nWe’re looking for new colleagues to join us. Here are the remote positions; more will be added over the coming year.\n\nDo you have questions about our remote engineering hub or its open roles? Our CTO, David Singleton, will lead a remote coffee chat on September 9 at 8:30am PT with remote Stripes Patrick McKenzie, Jamie Kite, and Aditya Mukerjee to talk about our remote hub. Sign up for the event to attend.", "label": "non_personal"}
{"title": "To design and develop an interactive globe", "url": "https://stripe.com/blog/globe", "content": "As humans, we’re driven to build models of our world.\n\nA traditional globemaker molds a sphere, mounts it on an axle, balances it with hidden weights, and precisely applies gores—triangular strips of printed earth—to avoid overlap and align latitudes. Cartographers face unenviable trade-offs when making maps. They can either retain the shape of countries, but warp their size—or maintain the size of countries, but contort their shape. In preserving one aspect of our world, they distort another.\n\nThese are terrestrial globe gores reissued by Giuseppe di Rossi in 1615.\n\nAs visual designers and software engineers, we’re modeling a piece of the world every time we build software. In some cases, it’s the entire world—and that digital world is animated and interactive. There are tools that render 3D objects on the web, but they’re considered sorcery by many. And conjuring that magic doesn’t come without sweat. In WebGL, displaying a single triangle—like a globemaker’s gore—with no lights, textures, interactivity, or motion requires 50+ lines of code.\n\nFor the new stripe.com, we built a 1:40 million-scale, interactive 3D model of the earth. We wanted to convey the interconnected nature of the internet economy and the global scale of our service, while acknowledging how much ground is yet to be covered. Despite expansion to 40 countries and payment processing from 195 countries, we grapple with the complexity of cross-border operations and expansion every day.\n\nWe set out to build a globe that inspires a sense of awe, invites people to explore, and conceals details for discovery. Along the way, we evaluated existing tools, designed our own solution, solved four interesting technical challenges, and improved the way we collaborate. Here’s what we learned.\n\nWays to build the world\n\nIt wasn’t a given that we’d build an interactive 3D globe on our landing page. We designed our first version of the globe to communicate nuanced data about the amount of online, cross-border commerce happening between each country. For this reason, it includes extra visual details like country borders. For our landing page, the goal of the globe was to capture our global scale and bring a visual metaphor to life. A week before launch, we had a nice animated map where the globe now sits but we didn’t love it. Despite the impending release, an executive (it was Patrick) posed to us: what would you build if you had the time to do it the way you wish you could?\n\nWe decided on a globe—and felt it was a better option for three reasons. First, using a sphere to display the earth takes up less than 20% of the screen area required to display the world in two dimensions. Second, a globe more accurately portrays the relative size, shape, and orientation of countries and bodies of water, even though visibility of the entire world at a glance is easier with a map. (More than ¾ of the globe is either hidden on the reverse hemisphere or obscured by its curvature.) Lastly, as an interactive experience, spinning a globe is much more satisfying than scanning a map.\n\nThe sphere occupies approximately 17% of the total area of the map.\n\nOnce we settled on a globe, we had to work out how to bring it to life.\n\nIf we had known precisely the globe we wanted to build, we’d have been foolish not to hire GlobeKit. Instead, not knowing what we did not know, we decided to figure it out ourselves. The primary tools used to render 3D objects on the web, WebGL and GLSL shaders, can be daunting. Developers writing shaders can get by without deep knowledge of trigonometry and linear algebra, but a good understanding of these disciplines make 3D graphics development substantially easier.\n\nNone of us on the team considered ourselves 3D artists, so we leaned on each other, the internet, and friends to help solve technical problems. To start, the project’s design lead created the best approximation of her vision of the globe in Photoshop. We naturally kept the globe’s design fluid, making it easy to adopt better ideas as they emerged without feeling precious about what was discarded.\n\nWhen it became clear that writing our own 3D engine was out of scope, we decided to use Three.js. Three is an approachable layer for WebGL, which abstracts away much of its complexity behind a well-documented API. Originally ported from ActionScript (Flash) in 2010, Three helped us create rich 3D graphics that render in real-time in the browser without needing to define how light reflects on every pixel of every shape.\n\nTo render a Three.js scene, you need a renderer, a DOM element to render in, a scene, a camera, and a mesh with both material (fragment shader) and geometry (vertex shader).\n\nTen years after its release, Three.js matches—or surpasses—much of what was possible in Flash a decade ago. The most engaging interactive experiences on the web are now built with Three. The community’s enthusiasm bears resemblance to the early days of ActionScript with the added benefit of running on mobile browsers and requiring no plugins. As Three.js and WebGL gain popularity, approachability, and support, the web is poised to embrace 3D en masse. Since WebGL is GPU-accelerated, it’s capable of processing a surprising amount of continuous visual change without bottlenecking on the CPU even on lower-end consumer hardware. Finding the boundary between making our globe feel alive and crashing the browser would emerge as our greatest technical hurdle. But it wouldn’t be the only one.\n\nGlobal issues\n\nLike the challenges we faced, our globe goes a few levels deep. It’s composed of three distinct layers, despite appearing as a single surface. The base layer represents the oceans, and is a semi-transparent sphere with ~50 segments both horizontally and vertically. The second layer is another sphere textured with tens of thousands of twinkling dots. The outermost layer is made of animated arcs of color which travel from one pulsing disc to another, wrapping themselves around the two spheres. The arcs travel from any country where Stripe accepts payments to countries where businesses accept payments using Stripe.\n\nWe encountered several significant technical challenges along the way, each of which could have prevented us from realizing our vision. For the benefit of those generating their own interactive globes—or similar complex 3D objects—let’s break a few of these challenges down.\n\nWe stacked each layer of the globe to produce a single visual surface.\n\nChallenge 1: Fill the surface with dots\n\nThe primary purpose of the outermost sphere—a layer of tens of thousands of dots—is to define continents. But as we removed the visual complexity of borders and animated each dot, they did more than communicate land masses; they made the globe feel alive. To make them work, we had two main requirements. The first step was to find a way to maintain consistent spacing between every row and column of dots, from pole to pole. Second, we needed to animate each dot individually.\n\nBefore landing on our final design, we tested and considered three different approaches to filling open space with a cluster of dots (as shown in the image below). Each attempt had its benefits and drawbacks.\n\nThis is the North Pole of the globe, mapped with dots using three different approaches.\n\nImage of evenly spaced dots. This approach is the simplest to create, but quickly becomes problematic. The dots fuse together as the circumference of each row shrinks as it nears the poles of the globe. As a static bitmap rather than geometry, we couldn’t animate each dot individually without an overly complex shader. Image of unevenly spaced dots. We increased the width and horizontal spacing in the rows of dots. The image of nearly 80,000 dots places fewer, wider dots at the top and bottom. This tweak helps prevent the pinching and clumping of dots at the globe’s poles which invalidated the previous approach. When mapped as a texture onto a sphere, this image creates a nearly uniform spacing of dots. At first, we created this image texture by hand, then generated an SVG with JavaScript. This option better met our visual goals but still didn’t let us animate each dot. We assumed Three.js would work well with SVGs, but since every shape must be converted to a triangle, the complexity of the conversion deterred us from this approach. Programmatically-generated layer (vs. an image) The most straightforward way to animate individual dots is to generate them in a three-dimensional space. To do this, we reused the code from our SVG to generate rows of dots as geometry in Three.js. Each row includes a different number of dots, from zero at the poles to five hundred at the equator. We used a sine function to choose the number of dots for each row, plotted each dot, and applied the lookAt method to rotate each dot to face the center of the sphere. However, the number of dots jumped inconsistently along a few latitudes, creating harsh lines and an unnatural effect in the longitudinal columns.\n\nThe final attempt—and right design—utilized a sunflower pattern. Like a sunflower’s pattern of seeds, the dots are a sequence of hexagons tightly coiled around latitudes from the top to the bottom of a sphere. Using the built in setFromSphericalCoords method we settled on this solution:\n\nThe spiral sunflower pattern was the winning design.\n\n// Create 60000 tiny dots and spiral them around the sphere. const DOT_COUNT = 60000; // A hexagon with a radius of 2 pixels looks like a circle const dotGeometry = new THREE.CircleGeometry(2, 5); // The XYZ coordinate of each dot const positions = []; // A random identifier for each dot const rndId = []; // The country border each dot falls within const countryIds = []; const vector = new THREE.Vector3(); for (let i = DOT_COUNT; i >= 0; i--) { const phi = Math.acos(-1 + (2 * i) / DOT_COUNT); const theta = Math.sqrt(DOT_COUNT * Math.PI) * phi; // Pass the angle between this dot an the Y-axis (phi) // Pass this dot’s angle around the y axis (theta) // Scale each position by 600 (the radius of the globe) vector.setFromSphericalCoords(600, phi, theta); dotGeometry.lookAt(vector); // Move the dot to the newly calculated position dotGeometry.translate(vector.x, vector.y, vector.z); … } ~\n\nChallenge 2: Group the dots by country\n\nOn the globe at stripe.com, dots form continents and light up where Stripe is live. In a previous iteration, we grouped dots by country to indicate where Stripe operates. We decided to turn this feature off for the interactive globe on our landing page, but thought it might be worthwhile to share how we approached grouping dots by country.\n\nOnce we filled the globe with dots, the next step was to transform our layers of spheres into a globe by defining countries. Our first goal was to make dots appear only within the borders of countries where Stripe is live. Once that was done, we needed dots within those live countries to be targets for animation as a group.\n\nEach country where Stripe is live is given a unique color for identification.\n\nA teammate who had recently experimented with shaders for a gaming project brought inspiration to this challenge. He thought to encode a PNG image with a unique color for each country where Stripe is live (see above). We used the built-in canvas getImageData to give us the color of each pixel in the image. Then, we matched each color to an array of country colors, tagging every dot with a unique countryId before passing its coordinates to the shader for rendering. Now we could isolate the group of dots in any country and animate its color, opacity, and position in z-space.\n\nIn 2020, each country where Stripe is live is indicated by a unique color. The assumed drawback to generating all of the dots as individual geometry was the astronomical number of calculations required to animate the properties of 60,000 dots 60 times per second. Lucky for us, the earth’s surface is mostly water. By only rendering geometry for countries where Stripe is live, we were able to reduce the geometry from 60,000 dots to ~20,000 dots passing a fraction of the data to the vertex shader. By pushing less data to the shader, we freed up rendering budget for use by other animations.\n\n// We assign a color to each ISO country code const COUNTRY_MAPPING = [ [0, '#99cc99', 'at'], [1, '#993333', 'au'], [2, '#cccc00', 'be'], … ]; // Load the color-coded image then get each pixel’s color new ImageLoader().load('map.png', (mapImage) => { const imageData = getImageData(mapImage); }); dotGeometry.computeBoundingSphere(); const uv = pointToUV(dotGeometry.boundingSphere.center, this.position); const sample = sampleImage(uv, imageData); // If there is no color data, return and move to the next dot if (!sample[3]) return; // Create a dot if there is color data for (let i = 0; i < dotGeometry.faces.length; i++) { const face = dotGeometry.faces[i]; // Create the vertices which make up the face of each dot positions.push( dotGeometry.vertices[face.a].x, dotGeometry.vertices[face.a].y, dotGeometry.vertices[face.a].z, … // face.b, face.c ); const [countryId] = getCountryId(sample); countryIds.push(countryId, countryId, countryId); } // Convert RGB to Hex and look up the countryId by color function getCountryId([r, g, b, _]) { const hex = [r, g, b] .map((color) => color.toString(16).padStart(2, '0')) .join(''); const countryId = COUNTRY_MAPPING.find(([_, id]) => id === hex); return countryId; } ~\n\nChallenge 3: Animate it all\n\nAfter filling in the surface with dots and grouping them into countries, we needed to connect the dots to show how and where business is done globally. Our goal was to bring the globe to life, which meant adding animation. Early on, we knew we’d need to get the globe spinning, each dot twinkling, and bend arcs between countries to indicate transaction patterns. We wanted visitors to be able to control and spin the globe.\n\nAround the time we started animating, we got a new teammate. In a past life, he had engineered the scrolling of the iconic Pencil by 53 site. In short order, he added animations for undulating, aurora borealis-like lights, made the globe rotate on page load, and spun the earth when the user scrolled the page. We handled the subtly twinkling dots and arcs with a custom fragment shader (and a lot of help from thebookofshaders.com) but the rest of the animation is vanilla JavaScript. requestAnimationFrame drives the motion of the arcs, the spinning of the globe, and the changing of colors.\n\n// Draw an arc between two coordinates ... constructor(start, end, radius) { super(); // Convert latitude/longitude to XYZ on the globe const start = toXYZ(start[0], start[1], radius); const endXYZ = toXYZ(end[0], end[1], radius); // D3 interpolates along the great arc that passes // through both the start and end point const d3Interpolate = geoInterpolate( [start[1], start[0]], [end[1], end[0]], ); const control1 = d3Interpolate(0.25); const control2 = d3Interpolate(0.75); // Set the arc height to half the distance between points const arcHeight = start.distanceTo(end) * 0.5 + radius; const controlXYZ1 = toXYZ(control1[1], control1[0], arcHeight); const controlXYZ2 = toXYZ(control2[1], control2[0], arcHeight); // CubicBezier allows for curves which travel half way // around the globe without penetrating the sphere const curve = new CubicBezierCurve3(start, controlXYZ1, controlXYZ2, end); // Arcs are curved tubes with 0.5px radius and 8 sides // Each curve is broken into 44 segments this.geometry = new THREE.TubeBufferGeometry(curve, 44, 0.5, 8); this.material = new THREE.ShaderMaterial({ // A custom fragment shader animates arc colors }); this.mesh = new THREE.Mesh(this.geometry, this.material); this.add(this.mesh); // Set the draw range to show only the first vertex this.geometry.setDrawRange(0, 1); this.drawAnimatedLine(); } drawAnimatedLine = () => { let drawRangeCount = this.geometry.drawRange.count; const timeElapsed = performance.now() - this.startTime; // Animate the curve for 2.5 seconds const progress = timeElapsed / 2500; // Arcs are made up of roughly 3000 vertices drawRangeCount = progress * 3000; if (progress < 0.999) { // Update the draw range to reveal the curve this.geometry.setDrawRange(0, drawRangeCount); requestAnimationFrame(this.drawAnimatedLine); } } ~\n\nChallenge 4: Make it performant\n\nEarly on, we discussed our expectations for the globe’s performance on different browsers to frame our requirements. We boiled those expectations down to one requirement: all animation and scrolling effects had to perform at 60fps (to match the common device refresh rate of 60hz). If this condition couldn’t be met, we were prepared to fall back to a static image. Thanks to GPU-acceleration of WebGL and some of the findings mentioned here, we never had to abandon our interactive globe.\n\nInitially, we ruled out mobile support. We assumed that scrolling and 3D animation would be too much for any machine, and that we’d have to either accept some lag or reduced motion on both smaller and more underpowered machines, or settle for the fallback. But as we learned about the capabilities of GPUs, we kept raising our expectations. Most of what’s possible in WebGL works on mobile without modification. We did make minor adjustments: during scroll, we pause all animations and debounce using Lodash so the globe spins without visual hiccups.\n\nA few days prior to launch, we tested the page on laptops without dedicated GPUs and reported that they were struggling to power a 5K display with the globe running fullscreen. We weren’t willing to accept defeat by falling back to an image for this rare case. Instead, we cycled through all possible bottlenecks one by one. No matter how much we simplified the geometry, stopped animations, or killed lights and shaders, we couldn’t smooth it out.\n\nOn a whim, we turned off the antialias parameter of the WebGL renderer. This one change not only fixed the issue on high-res monitors, but also improved performance of the animation and smoothness of scrolling on all devices, even those already running at 60fps. One might assume that removing antialiasing would make everything look pixelated. Since it only applies to the edges of the geometry, our image textures were still smooth and gradients and lighting were unaffected. Though pixelation occurs minimally on the arcs around the globe, the performance gain was significant enough to accept a tradeoff.\n\n// Turn off antialiasing for WebGL to improve performance this.renderer = new WebGLRenderer({ antialias: false, alpha: true }); … // Rotating the globe on scroll import throttle from 'lodash/throttle'; const SCROLL_EPSILON = 0.0016; const GLOBE_TRIGGER_TOP = window.innerHeight; … document.addEventListener('scroll', this.universalScrollHandler); … // Event handler: rotate the globe based on the current browser scroll position universalScrollHandler = throttle(this.scrollHandler.bind(this), 16); scrollHandler() { // Turns off all other animation this.isScrolling = true; this.oldScrollTop = this.scrollTop; this.scrollTop = document.scrollingElement.scrollTop || document.body.scrollTop; this.scrollDelta = this.oldScrollTop - this.scrollTop; const rotationDelta = this.scrollDelta * SCROLL_EPSILON; this.globeContainer.rotation.y += rotationDelta; // Once the browser scrolls past the globe on the page // stop all animations and move the globe off-screen if (GLOBE_TRIGGER_TOP < this.scrollTop) { this.globeOff = true; this.globeEl.style.transform = 'translateX(100vw)'; } else { this.globeOff = false; this.globeEl.style.transform = 'translateX(0)'; } } ~\n\nDesigning a better world\n\nTectonic plates arrange continents, but countries—how we organize the globe—are defined by people. It’s the same with organizations: how we define teams determines how we operate. We’ve found that establishing how designers and engineers relate, collaborate, and organize has an outsized influence in how we build. There’s a long line of designers and developers with a mutual respect for both pixels and code. This rapport sidesteps many pitfalls when building products: the transfer of pressure from designer to developer to deliver stunning visuals, to engineers diluting the vision at the eleventh hour. Blending design and engineering complicates the process, but enriches the result.\n\nWe could only properly evaluate our globe once we built a functional prototype with a sphere on the screen to examine. Modern software development is often built modularly, snapping components together until it’s ready to ship. We pledged to build the real, whole product, even in its earliest—and ugliest—stages. This enabled us to separate its functionality from its finality, focusing less on if it worked and more on when it worked well enough for us to ship it. This released us from the temptation to make sacrifices in quality just to make the globe fully operative.\n\nBuilding a fully-functional prototype early in our development process focussed our highly cross-functional team; over time and through iteration, improvements unfolded gradually. Since its first incarnation in 2019, we’ve used the globe for mockups, keynotes, websites, and a small, but momentous appearance in Stripe’s Dashboard.\n\nMeasures of time are actually measures of the earth’s rotation: sixty units of rotation per minute, and sixty minutes of rotation per hour. As our product expands to cover the surface of the globe, we’ll keep smoothing the rough edges, connecting dots in distant countries, and working to keep the world spinning at 60 frames per second.", "label": "non_personal"}
{"title": "Stripe’s payments APIs: The first 10 years", "url": "https://stripe.com/blog/payment-api-design", "content": "A few years ago, Bloomberg Businessweek published a feature story on Stripe. Four words spanned the center of the cover: “seven lines of code,” suggesting that’s all it took for a business to power payments on Stripe. The assertion was bold—and became a theme and meme for us.\n\nTo this day, it’s not entirely clear which seven lines the article referenced. The prevailing theory is that it’s the roughly seven lines of curl it took to create a Charge . In 2011, the code snippet featured on our landing page was nine lines long. But remove the optional description and card[cvc] , and there are visually seven lines:\n\nA partial screenshot of Stripe.com, circa 2011. Courtesy of the Internet Archive Wayback Machine.\n\nHowever, a search for the seven lines of code ultimately misses the point: the ability to open up a terminal, run this curl snippet, then immediately see a successful credit card payment felt like seven lines of code. It’s unlikely that a developer believed a production-ready payments integration involved literally only seven lines of code. But taking something as complex as credit card processing and reducing the integration to only a few lines of code that, when run, immediately returns a successful Charge object is really quite magical.\n\nAbstracting away the complexity of payments has driven the evolution of our APIs over the last decade. This post provides the context, inflection points, and conceptual frameworks behind our API design. It’s the extreme exception that our approach to APIs makes the cover of a business magazine. This post shares a bit more of how we’ve grown around and beyond those seven lines.\n\nA condensed history of Stripe’s payments APIs\n\nSuccessful products tend to organically expand over time, resulting in product debt. Similar to tech debt, product debt accumulates gradually, making the product harder to understand for users and change for product teams. For API products, it’s particularly tempting to accrue product debt because it’s hard to get your users to fundamentally restructure their integration; it’s much easier to get them to add a parameter or two to their existing API requests.\n\nIn retrospect, we see clearly how our APIs have evolved—and which decisions were pivotal in shaping them. Here are the milestones that defined our payments APIs and led to the PaymentIntents API.\n\nSupporting card payments in the US (2011-2015)\n\nWe first launched the Stripe API in the US, where credit cards were—and still are—the predominant payment method. The “seven lines of code” largely sufficed, but reality was only a tiny bit more complicated. We also created Stripe.js, a JavaScript library to collect card payment details from the browser and securely store them with Stripe, represented as a Token which can later be used to create a Charge . This helped users avoid tedious PCI compliance requirements.\n\nA Token is created client-side and sent to the server. A Charge is then created server-side using that Token .\n\nThis payment flow follows a very common pattern in traditional web applications. The JavaScript client uses a publishable API key to create a Token and sends both to the server when customers submit the payment form (along with other form data about the order). The server synchronously creates a Charge using that Token and a secret API key; orders can optionally be fulfilled based on the outcome of the payment.\n\nThe Charge and the Token became foundational concepts in our payment API.\n\nAdding ACH and Bitcoin (2015)\n\nWhen we first created Charges and Tokens , they only supported credit card payments. As we expanded to more countries and types of users, we needed to add more payment methods to the API. In 2015, we added:\n\nACH debit , a common payment method in the US since the 1970s. ACH is used when moving money between US bank accounts, and supports both crediting and debiting bank accounts.\n\n, a common payment method in the US since the 1970s. ACH is used when moving money between US bank accounts, and supports both crediting and debiting bank accounts. Bitcoin, which was just gaining mindshare in the early 2010s. An increasing number of businesses were experimenting with accepting Bitcoin as a payment method.\n\nWe describe payments as “finalized” when a user has sufficient confidence the funds are guaranteed. (Of course, even finalized payments can be reversed later due to fraud or subsequent refunds.) In most cases, upon finalization, users release shipment of goods. While payments processed on card networks are initiated by the merchant and can be immediately finalized, these two payment methods are quite different from cards. Payments processed on the ACH network are finalized days later. With Bitcoin, customers (rather than the merchant) determine when a Bitcoin transaction is created. Like ACH payments, Bitcoin payments are also not finalized immediately. While the merchant will know that the customer has created the Bitcoin transaction once it is picked up by a block, it still requires 6 blocks—or about an hour—to finalize the transaction.\n\nPayment is immediately finalized Payment is finalized later No customer action required To initiate money movement Card ACH debit (days) New Customer action required To initiate money movement Bitcoin (hours) New The Charges API supported cards, ACH debit, and Bitcoin as payment methods.\n\nEach of these first three payment methods differ in how the payment is initiated and when funds are guaranteed. This made the task of creating APIs that abstract over their differences quite challenging.\n\nHere’s what we did:\n\nACH debit. Since card payments and ACH debit payments both require only static information from the customer (i.e., card number or bank account number), we expanded the Token resource to represent both card details and bank account details. A user still created a Charge from either type of Token , but we added a pending state to the Charge to represent that an ACH debit Charge isn’t immediately finalized and could still fail. Users ran their order fulfillment logic days later, when they received a webhook indicating that the Charge had succeeded.\n\nA new pending state was added to the Charge to represent payments that finalize asynchronously.\n\nBitcoin. As Bitcoin didn’t fit into our abstractions, we had to introduce a new BitcoinReceiver API to facilitate the client-side action we needed the customer to take in the online payment flow. Particular to Stripe, a “receiver” was a temporary receptacle for funds. It had a very simple state machine that described the status of the receiver: a boolean, filled , that was either true or false. Once the receiver was filled, the user could create a Charge using that BitcoinReceiver object instead of a Token object. This would virtually move the funds from the receiver to the user’s balance. If a user didn’t create the Charge within a certain time frame, the money in the receiver would be refunded to the customer. Like ACH debit Charges, Bitcoin Charges started in the pending state and succeeded asynchronously.\n\nWe introduced the BitcoinReceiver resource to represent that the customer needed to take an action to complete the payment.\n\nWith ACH debit and Bitcoin, the integration grew more complex. It now involved dealing with asynchronous payment finalization, and in Bitcoin’s case, it involved managing two state machines to complete payment: BitcoinReceiver on the client and Charge on the server.\n\nSeeking a simpler payments API (2015 - 2017)\n\nOver the next two years, we added more payment methods. Most of them were more like Bitcoin than cards—they required customer action to initiate a payment. We discovered that it wouldn’t be developer-friendly to introduce a brand new BitcoinReceiver -like resource for each of these—it would simply introduce too many new Stripe-specific concepts to reason about in the API. We aspired to design a simpler payments API and began exploring how to unify these payment methods on one integration path: the Sources API.\n\nPayment is immediately finalized Payment is finalized later No customer action required To initiate money movement Cards ACH debit\n\nSEPA direct debit New\n\nBacs debit New\n\nBECS debit New Customer action required To initiate money movement iDEAL New\n\nAlipay New\n\ngiropay New\n\nBancontact New\n\nWeChat Pay New\n\nPrzelewy24 New\n\nCards with 3D Secure New Bitcoin\n\nMultibanco New\n\nPaper checks New The Sources API was designed to be a single client-side API that could represent multiple payment methods.\n\nWe combined the two client-side abstractions we’d previously designed ( Tokens and BitcoinReceivers ) into a client-driven state machine called a Source. Upon creation, a Source could be immediately chargeable (e.g., for card payments) or pending (e.g., for payment methods that require customer action). The server-side integration remained a single HTTP request that used a secret key to create a Charge .\n\nWe combined the functionality of Tokens and receivers into a single client-side API: Sources .\n\nThe payment flow for every payment method relied on the same two API abstractions: a Source and a Charge . This seems conceptually simple at first glance, as it resembled a card integration in the U.S. However, once we understood how this flow integrated into users’ applications, we discovered many rough edges.\n\nFor example, when users added a payment method that doesn’t finalize immediately, they could no longer fulfill their customers’ orders immediately after the Charge was created. Instead, they’d have to wait until the Charge transitioned to succeeded before shipping goods. This usually involved adding a webhook integration that listens for charge.succeeded and moving fulfillment logic there.\n\nSources and Charges were still more complex for other payment methods—and integration issues could lead to lost revenue. For example, with iDEAL, the predominant payment solution in the Netherlands, the customer initiates the payment after they’re redirected to their bank’s website or mobile app. If the client-side application creates a Source and the browser then loses connectivity with the server, the next request to create a Charge wouldn’t make it through, even though the customer believes they paid. (The browser could lose connectivity for any number of reasons: the customer closes their tab after they pay on their bank’s site, the payment method requires a redirect that the customer never returns from, or the customer has a flaky internet connection.) Because the server never created a Charge , we’d refund the money associated with the Source after a few hours. This is a conversion nightmare.\n\nTo reduce the chance of this occurring, we recommended that users either poll the Stripe API from their server until the Source became chargeable or listen for the source.chargeable webhook event to create the Charge . But, if a user’s payment application goes down and they use Sources and Charges , these webhooks aren’t delivered and the server won’t create the Charge . We’ll return the customer’s money and users have to get them back on their site to pay again. Even if the user implements and maintains this best practice correctly, there’s still complexity around the different possible states of Sources and Charges and the paths and requirements for different payment method types.\n\nThere are many ways to actually create a Charge from the Source , depending on the payment method.\n\nSome Sources —like cards and bank accounts—are synchronously chargeable and can be charged immediately on the server after the online payment form is submitted, while others are asynchronous and can only be charged hours or days later. Users often built parallel integrations using both synchronous HTTP requests and event-driven webhook handlers to support each type. This means users now have multiple places where they’re creating a Charge and fulfilling their order. The code branching factor deepens for payment methods like OXXO, where the customer prints out a physical voucher and brings it to an OXXO store to pay for it in cash. Money is paid entirely out-of-band, making our best practice recommendation of listening for the source.chargeable webhook event absolutely required for these payment methods. Finally, users must track both the Charge ID and Source ID for each order. If two Sources become chargeable for the same order (e.g., the customer decides to switch their payment method mid-payment) they can ensure they don’t double-charge for the order.\n\nThis effort demands more bookkeeping and conceptual understanding from developers than “seven lines of code” did. Our users needed to grok all of these edge cases in order to build a functioning Stripe integration. Imagine the confusion caused by reasoning about these two state machines, with varying definitions of each state depending on the payment solution. Developers must manage the success, failure, and pending states of two state machines—whose states may differ across different payment methods—in order to complete a single payment.\n\nUsers must manage two different state machines that span client and server to complete a payment.\n\nLet’s refer back to the table of payment methods. You may notice that cards are the only payment method in the top left quadrant: they finalize immediately and don’t require customer action to complete a payment. This means we built support for new payment methods on top of a set of abstractions that were designed for the simplest payment method of them all: cards. Naturally, abstractions designed for cards were not going to be great at representing these more complex payment flows.\n\nPayment is immediately finalized Payment is finalized later No customer action required To initiate money movement Cards ACH debit\n\nSEPA direct debit\n\nBacs debit\n\nBECS debit Customer action required To initiate money movement iDEAL\n\nAlipay\n\ngiropay\n\nBancontact\n\nWeChat Pay\n\nPrzelewy24\n\nCards with 3D Secure Bitcoin\n\nMultibanco\n\nPaper checks Global payment methods aren’t different; cards are!\n\nIntroducing additional states and expanding on the definition of resources that were created for a specific, narrow use case resulted in a confusing integration and an overloaded set of API abstractions. It’s as if we were trying to build a spaceship by adding parts to a car until it had the functionality of a spaceship: a difficult and likely doomed proposition. Charges and Tokens were foundational in the API because they were the first APIs we had, not because they were the right abstraction for global payments. We needed to fundamentally rethink our payments abstractions.\n\nDesigning a unified payments API (late 2017 - early 2018)\n\nWe were able to start designing the APIs we wanted when we set aside further changes to Sources and Charges . It was much easier because we had a chance to learn from users over the years, and deeply understood the issues they encountered with our existing integration paths. We also accumulated payments domain expertise, having had years of experience iterating on our APIs. Taken together, our API design had a better chance to not repeat past mistakes.\n\nWe locked ourselves in a conference room for three months with the goal of designing a truly unified payments API. If successful, a developer would only need to understand a few basic concepts in order to build a payments integration. Even if they hadn’t heard of the payment method, they should be able to just add a few parameters to a few specific points in their integration. To enable this, the states and guarantees of our APIs had to be extremely predictable and consistent. There shouldn’t be an array of caveats and exceptions scattered throughout our docs.\n\nA team of five people—four engineers and a PM—walked through every payment method we supported and we could imagine supporting in the future. We iterated on an API design that would be able to model all of them. We ignored all existing abstractions and thought about the problem from first principles.\n\nWe did early work on our unified payments API in a conference room named Lynx.\n\nIt’s hard to remember now exactly what happened each day, but some rules and routines really helped us:\n\nClose laptops . When working together in the same room, we found the fastest way to be fully present and attentive was to close our computers. When we did, we felt more listened to and could more clearly and easily explain our reasoning to each other.\n\n. When working together in the same room, we found the fastest way to be fully present and attentive was to close our computers. When we did, we felt more listened to and could more clearly and easily explain our reasoning to each other. Pace your questions . Start each session with a set of questions you want to answer. Write down any new questions that arise in a working session for the next session. Try to avoid discussing them in the moment. In the time between sessions, you’ll get some distance from those questions, collect new information, and meditate more on the topic. End each session with clear answers and questions to explore in the next session.\n\n. Start each session with a set of questions you want to answer. Write down any new questions that arise in a working session for the next session. Try to avoid discussing them in the moment. In the time between sessions, you’ll get some distance from those questions, collect new information, and meditate more on the topic. End each session with clear answers and questions to explore in the next session. Use colors and shapes . Early on, lean on simple representations for complex, nascent concepts, rather than try to give them concrete names. We exhausted the available set of marker colors and drew many shapes on the whiteboard. This tack helped us avoid anchoring on specific definitions for the concepts that we were trying to shape—and helped us avoid naming bikesheds prematurely.\n\n. Early on, lean on simple representations for complex, nascent concepts, rather than try to give them concrete names. We exhausted the available set of marker colors and drew many shapes on the whiteboard. This tack helped us avoid anchoring on specific definitions for the concepts that we were trying to shape—and helped us avoid naming bikesheds prematurely. Focus on enabling real user integrations. In API design, it’s common to get caught up with pursuing perfect invariants, airtight theories, or intellectually pure solutions, but none of that is useful if it doesn’t enable a real user integration. One of our primary design tools was writing hypothetical integration guides to validate our concepts and to make sure we didn’t introduce old or new pits of failure. We wrote these for every payment method we could list—and even for some payment methods we made up, like sending cash via carrier pigeon.\n\nIn API design, it’s common to get caught up with pursuing perfect invariants, airtight theories, or intellectually pure solutions, but none of that is useful if it doesn’t enable a real user integration. One of our primary design tools was writing hypothetical integration guides to validate our concepts and to make sure we didn’t introduce old or new pits of failure. We wrote these for every payment method we could list—and even for some payment methods we made up, like sending cash via carrier pigeon. Question every assumption underpinning existing APIs . We specifically designed the first API to make card payments extremely easy, and it grew relatively organically from there. We needed to reason from first principles at every turn. Looking back, we probably could have done it even more.\n\n. We specifically designed the first API to make card payments extremely easy, and it grew relatively organically from there. We needed to reason from first principles at every turn. Looking back, we probably could have done it even more. Invite domain experts as guests. Import know-how for discussions with a specific topic in mind. Elevate the conversation with expertise.\n\nImport know-how for discussions with a specific topic in mind. Elevate the conversation with expertise. Make decisions quickly knowing you might change your mind. New observations or data would either further reinforce our initial decision or lead us to make a better choice. In every case, it was more efficient to make a decision early and avoid stasis, even if we later reversed that decision.\n\nWe frequently felt like we were brute-forcing the problem space, but the enemy of any large design project is not making decisions quickly enough because no option feels perfect.\n\nIntroducing PaymentIntents and PaymentMethods (2018)\n\nWe ended up with two new concepts: PaymentIntents and PaymentMethods. By packaging these two concepts, we finally managed to create a single integration for all payment methods.\n\nPaymentMethods, like the original Tokens , represent static information about the payment method that the customer wants to use. It includes the payment scheme and the credentials needed to move money, like card information or the customer’s name or email. For some methods, like Alipay, only the payment method name is required because the payment method itself handles collecting further information after you redirect to their site. Unlike a Source , there is no state or data specific to the particular transaction type captured on a PaymentMethod object—you can think of it as an object that specifies how to process a payment request.\n\nPaymentIntents, on the other hand, capture transaction-specific data such as how much to charge and is the stateful object that tracks the customer’s attempt to pay with various payment methods. Combine a PaymentMethod (the “how”) and a PaymentIntent (the “what”) and payment can be attempted. If one payment attempt fails, the customer can try again with a different PaymentMethod.\n\nA PaymentIntent has the following states, summarized quickly here:\n\nrequires_payment_method: Specify the PaymentMethod to use.\n\nSpecify the PaymentMethod to use. requires_confirmation: “Confirm” basically means “make money go!” Sometimes you want to pause between collecting payment method details and actually making the money go, and this (optional) state makes that possible.\n\n“Confirm” basically means “make money go!” Sometimes you want to pause between collecting payment method details and actually making the money go, and this (optional) state makes that possible. requires_action: Please perform the specified action. This can be anything from a generic redirect_to_url (self-explanatory) to a very payment-method-specific action like oxxo_display_details , which provides information for you to generate an OXXO voucher.\n\nPlease perform the specified action. This can be anything from a generic (self-explanatory) to a very payment-method-specific action like , which provides information for you to generate an OXXO voucher. processing: You’re waiting on us to process the payment.\n\nYou’re waiting on us to process the payment. succeeded: The payment has been finalized. Funds are guaranteed.\n\nThe payment has been finalized. Funds are guaranteed. failed: There’s no failed state because if a single payment attempt fails, the PaymentIntent goes back to the requires_payment_method state so that the customer can try again with a different payment method. This is convenient because the same object created server-side can be used repeatedly on the client.\n\nWith Charges and Sources , a “best practice” payments integration for cards, iDEAL, and ACH debit required managing two webhook handlers (one that is time-sensitive and in the critical path to collecting money correctly), dealing with three different times a Charge could succeed, handling two paths to failure, and dealing with two stateful objects.\n\nWith PaymentIntents and PaymentMethods, the integration is the same across all payment method types: start by creating a PaymentIntent on your server for the amount and currency to collect for an order. Pass the secret embedded on the PaymentIntent to the client. Collect the customer’s preferred payment method and confirm the PaymentIntent using the secret and payment method information. The PaymentIntent instructs what to do next when it’s in the requires_action state. Actions are standardized and predictable per payment method; for example, the 3D Secure authentication flow is managed via a set of actions. Lastly, listen for the payment_intent.succeeded webhook or wait for the PaymentIntent to enter the succeeded state to know when funds are guaranteed and when to fulfill a customer’s order. This is wholly managed by one predictable state machine. Importantly for conversion, the sole webhook handler that users must implement isn’t in the critical path to collecting money.\n\nA PaymentIntents integration.\n\nLaunching PaymentIntents and PaymentMethods (2018 - 2020)\n\nThe design of a set of APIs that would work across all payment gateway methods globally with a single integration was the hard but fun part. The implementation of a beta, production-ready version of the API was also relatively straightforward. But launching a new payment API that replaces a foundational, established API doesn’t stop at just writing the code to spec—rolling out this change took almost two years.\n\nConnecting the design to reality\n\nIntroducing a new set of abstractions to an existing public API is much harder than updating internal interfaces. No matter the size of the company, sufficient tenacity and planning can drive teams to upgrade their dependencies. However, for an API product, there’s no forcing developers to migrate, nor breaking their integration.\n\nA great API product stays out of the developer’s way for as long as possible.\n\nIf it is possible to make small changes to an existing API to accommodate new use cases, try that first so developers don’t have to rewrite their integration. In our case, we already knew from experience that just adding more parameters and states to the existing API resources wasn’t working. Even if the resource had the same name, the payment flow would look completely different.\n\nThat said, the alternative—building new, entirely independent APIs which required developers to migrate everything at once—also felt daunting. After talking to many users, we identified common patterns in their integrations. One integration created Stripe objects in the payment solution. Other integrations consumed Stripe objects for analytics, support, or reporting—potentially syncing these objects to their own database. For some users, these integrations were even owned by different teams. Given a core feature of Stripe’s APIs is that developers don’t have to touch their integration for years, we had to figure out a way to motivate users to migrate their payment flow. One way to do this was to make sure that any changes to the payment flow don’t break their other integrations.\n\nTo accomplish this, we decided to layer over the legacy APIs and create a Charge object for each payment attempted by the PaymentIntent. This way, users could migrate their payment flow to the PaymentIntents API while their analytics and reporting integrations still chugged along on an unchanged Charge resource. (This is also a good reason to not just reuse the Charge abstraction with changes to conceptually behave more like PaymentIntents. Lots of users and extensions make assumptions about what a Charge means, and changing its state machine drastically would break those assumptions.)\n\nWe didn’t like how cluttered the Charge resource had become over the last seven years, so this was not ideal. Between 2011 and 2018, the Charge resource grew from having 11 properties to 36 properties and Charge creation grew from accepting 5 parameters to 14 parameters! To make sure we don’t make the problem worse as we add more payment methods, we introduced payment_method_details, a polymorphic, typed hash on the Charge that contains payment-method-specific data. This approach helps us keep the top-level Charge resource simple, while making payment details easy to find and identify for details such as a partner reference ID or a payment-method-specific verification status:\n\n{ payment_method_details: { type: PaymentMethodType, [PaymentMethodType]: { // Payment-method-specific details about the transaction. // For cards, maybe it’s the CVC verification information. // For OXXO, maybe it’s the voucher information. } } } ~\n\nOver time, we’ve standardized this design pattern and have applied it to other resources in the API.\n\nLayering over the Charges API is just one example of a design compromise we had to make for the sake of migration. There were many other smaller challenges, but ultimately they all had some least-bad solution we could pursue, so it wasn’t too dire. The hardest part of realizing the PaymentIntent migration was not a technical challenge, but a perception challenge: The new APIs didn’t feel like “seven lines of code” anymore.\n\nKeep it simple, Stripe\n\nIn normalizing the API across all payment methods, card payments became more complicated to integrate by introducing webhook events and by flipping the order of the client and server requests in the payment flow. These choices are not intuitive for those familiar with card payments, nor are they easy to implement for developers building traditional web applications.\n\nCompared to a simple card payments integration on Charges, a PaymentIntents integration requires flipping the client and server API calls and dealing with a webhook.\n\nThis change to card payments was a challenge for one of our most important types of users: the eager developer at a startup who wants to get up and running with card payments for checkout as soon as possible. Before, their seven lines of code pasted in a terminal would result in a successful charge. This new payment processing flow relies on asynchronous events, so the magic becomes much less tangible.\n\nPaymentIntents is also objectively a harder integration for users who only care about accepting card payments in the US and Canada. We flipped the order of the client and server calls, which is difficult for traditional web applications to handle, and webhooks are often more than a little bit annoying to set up, test, and debug. (We later developed the Stripe CLI to make developing with webhooks simpler for users.)\n\nThe power-to-effort curve looks different between the Charges integration and the new PaymentIntents integration. Each incremental PaymentMethod is cheap to add to a PaymentIntents integration. However, speed is key for startups who want to get started quickly. With Charges , getting cards running was intuitive and low-effort—a compelling combination for startups.\n\nA PaymentIntents integration requires more effort up front, but each incremental payment method requires little incremental work to understand and add. On the other hand, a Charges integration is very low-effort for cards in the US and Canada, but becomes tedious and unpredictable for each subsequent payment method.\n\nOur first attempt at launching PaymentIntents without overwhelming existing users was to show both the PaymentIntents and Charges integration guides in our documentation, switching which one we showed first depending on the user’s location. The idea was that most users in the US did not need these non-card payment methods, and thus would feel overwhelmed by the idea of payments as a state machine. In reality, this branching between two completely different integrations was tremendously confusing.\n\nMany US businesses do want to go global, and folks aren’t always coding from the locale of the business they want to run. If a developer for a EU-based business ended up following the Charges integration guide, they’d eventually realize that they would have to start from scratch. This happened a few times, and was always a costly and painful experience. It was not user-centric thinking to assuage our own worries about this big API change by recommending two incompatible integration paths.\n\nOur ultimate solution to this problem was to add a convenient packaging of the API that caters to the hypothetical user that would turn away from our APIs if they had to use webhooks up front. We called the default integration the “global payments integration” and named the new integration “card payments without bank authentication.” We put the implications of this integration front and center in the documentation: with this simpler flow, you won’t be able to easily add new payment methods.\n\nThe way this conceptual packaging actually manifests in the API is a special parameter called error_on_requires_action . This parameter tells the PaymentIntent to error if further action is required to complete the payment. A user who wants a simple payment flow like Charges won’t be able handle any actions required by the PaymentIntent state machine.\n\n# Our packaging made PaymentIntents seven lines of code. curl https://api.stripe.com/v1/payment_intents \\ -u sk_test_xxx: \\ -d amount=1099 \\ -d currency=usd \\ -d confirm=true \\ -d payment_method=\"{{PAYMENT_METHOD_ID}}\" \\ -d error_on_requires_action=true ~\n\nThe parameter name makes it very clear what users are choosing. Additionally, this approach allows us to easily track how often users choose this integration path, which would not be possible if we’d just recommended that U.S. users ignore PaymentIntent states they couldn’t handle. Someday that eager developer will have the time to build out a webhooks integration or will need to add a new payment method. When that day comes, it’s clear what they need to do: remove the parameter from the integration to start handling the requires_action state. Developers using this packaging of PaymentIntents don’t have to change the core resources at play, even when they upgrade to the global integration.\n\nOur simple packaging of PaymentIntents for U.S. and Canadian card payments requires the same amount of effort to integrate as Charges .\n\nWith this packaging, we were able to provide a low-effort integration similar to Charges for users who had no interest in doing a global-payments-ready integration up front.\n\nKeeping things simple doesn’t just mean reducing the number of resources or parameters.\n\nTwo overloaded API abstractions are not simpler and are definitely not more flexible and powerful than three or four clearly-defined abstractions. Keeping things simple means making sure your APIs are consistent and predictable—and that you’re creating the right packages to gradually reveal the power of your API as your users need it. It also means not underestimating your user. It’s tempting to abstract away too much in service of “keeping things simple,” but users will often quickly discover that they need more control.\n\nAn API product is more than just the API\n\nThere has—and will always be—many lines of code propping up the vaunted “seven lines of code.” It’s reliably the case with APIs. They don’t happen without a lot of work that isn’t designing or building the actual API. Much of the effort required is unglamorous and tedious, like tracking down every piece of documentation, support article, and canned response that references the old APIs, reaching out to folks who have made community content and asking them to update it, and planning and recording many tutorials for users and user-facing teams.\n\nThere’s also the teams that appear on the periphery, but are instrumental in the success of APIs. There’s the documentation and developer products that supplement the integration experience. Stripe CLI’s launch made webhooks much less daunting. A redesign of the information architecture of our documentation made relevant guides easier to find. Stripe Samples allows developers who prefer to learn by example rather than prose to just start with some working code. A redesign of the payments view in the Stripe Dashboard allows developers to more easily debug and understand the PaymentIntent state machine.\n\nThe care, choices, and effort of Stripes past and present from across the company contributed to our most recent two-year effort to design and launch our new payments APIs. The more we grow, the more we realize that we must continue to build and rebuild deliberately and thoughtfully. These are still early days. Come join us.", "label": "non_personal"}
{"title": "Sorbet: Stripe’s type checker for Ruby", "url": "https://stripe.com/blog/sorbet-stripes-type-checker-for-ruby", "content": "By 2017, Stripe had grown to the point where hundreds of engineers had written millions of lines of code. Most of that code was—and still is—written in Ruby, which is famous for helping engineers iterate quickly (if somewhat notorious for encouraging inscrutable code). Unfortunately we were starting to see Ruby come apart at the seams: new engineers found it hard to learn the codebase, and existing engineers were scared to make sweeping changes. Everyone faced a constant tradeoff: run the fast, local tests which might not catch many breakages, or run all the tests, even the slow ones. Ruby was becoming a source of friction more than a source of productivity.\n\nWe set out to change that, with two goals in mind: make it easier to understand the code, while doubling down on what makes Ruby productive and delightful. This was the backdrop against which we decided to create and open source Sorbet, a fast, powerful type checker designed for Ruby. Sorbet statically analyzes a codebase, builds up an understanding of how each piece of code relates to every other piece, and then exposes that knowledge to the programmer via type errors, autocompletion results, documentation on hover, or jumps between definitions and usages.\n\nToday Sorbet runs over Stripe’s entire Ruby codebase, currently amounting to over 15 million lines of code spread across 150,000 files. We can't take credit for pioneering the idea of adding static types to a dynamically typed language—Microsoft and Facebook popularized the approach with TypeScript and Hack, respectively. However, we thought it was worth sharing how Sorbet has not just met but exceeded our goals in the almost four years since we first enabled it on our Ruby codebase.\n\nSorbet reinforces the delightful bits of Ruby while making engineers more productive. Not only has it made code easier to understand, it’s even helped shape and reinforce Stripe's engineering culture as we've grown. But before we dive into what makes Sorbet… Sorbet, let’s take a short step back in time to its origins at Stripe.\n\nA brief history of Sorbet inside Stripe\n\nType annotations arrived in Stripe's Ruby codebase as early as November 2016, almost a full year before work began on Sorbet. These annotations were born out of a desire to encourage engineers to write modular units with clear public interfaces. Here's an example test case from the pull request that introduced type annotations:\n\nNeither Sorbet nor any other static type checker existed to consume these type annotations yet; they existed only at runtime. The declare_method call above acted like a decorator on the def call method: it would check that the msg argument given to call was a String and that call returned a String on every invocation. Throughout the next year, these runtime-only annotations spread throughout Stripe's codebase.\n\nMonths prior we had added Flow, a static type checker for JavaScript, to our frontend codebase. Ruby developers quickly grew envious and kept asking us what it would take to get the same features for Ruby. We staffed an effort to figure out what it would take to either adopt one of the two in-progress Ruby type checkers—RDL and TypedRuby—or to build our own. RDL proved to be powerful, but too slow . TypedRuby was faster, but had bugs that would have required a near-rewrite to solve . So in November 2017, we began writing Sorbet from scratch. Six months later, in May 2018, Sorbet type checking became required in Stripe’s automated test suite. After another year of internal adoption, we released Sorbet to the world in June 2019.\n\nIn all that time a lot has changed—Sorbet has far more features today than we ever imagined back then. But there's been one constant driving force behind the project: building tools that make engineers working in Ruby more productive.\n\nSupercharged productivity in Ruby\n\nWhen we ask how Sorbet makes people more productive they tell us all sorts of things, but the most common theme is raw speed.\n\nSorbet gives near-instantaneous feedback while editing: for 80% of edits, it can finish reporting type errors in milliseconds, even in our multi-million line codebase. The longest error reporting wait times measure in seconds. Types aren't a replacement for tests, but few test suites are fast enough to run on every edit like Sorbet.\n\nBut there's more to it than just speed: Sorbet takes the toil out of understanding how code fits together.\n\nOn the day we rolled out the Sorbet-powered VS Code extension for Ruby, Justin Duke described the feeling better than anyone:\n\nHaving just spent the past few minutes clicking around VSCode like a kid on Christmas morning, I don't think it's an exaggeration to say that this might be the single largest improvement in my pay-server [Stripe's Ruby codebase] productivity since joining Stripe. Justin Duke, Satisfied Sorbet user\n\nIn a large codebase, Ruby can be uniquely hard to understand, even among other dynamically typed languages. What's worse is that it's hard to just, say, lint against the features that make Ruby hard to understand, because many of them are Ruby's most loved features. Here are some of the features that can make a Ruby codebase hard to unravel:\n\nRuby lacks import statements (like those in Python or JavaScript), which bind global names to file-scoped names. Instead, Ruby provides require statements, which merely run other Ruby code. This mechanism works kind of like #include statements in C and C++: a single require statement might hide implicit calls to hundreds of other require statements.\n\nBut this feature enables Rails’ famous “convention over configuration” approach to project layouts, which many people love about it, you don't have to import files in Rails, you can just reference the code you want to reference.\n\nRuby encourages factoring code into modules, which can then be mixed into classes or even other modules. When used well, modules can help organize code into composable, testable units.\n\nBut on the other hand, overuse of modules obscures where a method is defined behind a deep ancestor hierarchy. New Stripe engineers working in our codebase frequently struggled to find a method’s definition when it came into scope from behind multiple layers of modules.\n\nRuby embraces metaprogramming, which is when methods and objects are dynamically created by code itself, instead of directly by the programmer. Concretely, this means that while some methods are written literally like def invoices; ...; end , others are defined dynamically by calling a library function like has_many(:invoices) . Metaprogramming as a way to share code is one of the biggest reasons why projects like Rails have been so successful.\n\nUnfortunately, metaprogramming is very opaque. It prevents simple regular expression searches from surfacing method definitions. Once a definition is found, the programmer still has to trace through code to know things like what arguments the method takes.\n\nWe built Sorbet to make it easy to navigate and understand a codebase without having to give up these features people love about Ruby. The key, more than just reporting type errors quickly, is to offer a powerful editor extension, which provides ever-present answers to common questions. The answer to “where is this class defined?” is a click away, not hidden behind multiple require statements. “How am I supposed to use this method?” fades as a flick of the cursor reveals the method's types and documentation, replacing a lengthy crawl through a class's transitive mixins. Instant responses from Sorbet mean less time toiling and more time discovering.\n\nBuilding Sorbet in a way where it delivers type errors and IDE responses so fast comes from a set of design choices we made early on in its development. First, Sorbet is written in C++, not Ruby. To quote Nelson Elhage, one of the founding members of the team, \"Writing in C++ doesn't automatically make your program fast, and a program does not need to be written in C++ to be fast. However, using C++ well gives an experienced team a fairly unique set of tools to write high-performance software.\" C++ gives us great baseline performance and a lot of headroom for further improvement when we decide that it's critical to make a given component of Sorbet fast.\n\nAnother key element of why Sorbet is fast is that we deliberately chose a simple type inference algorithm. Specifically, Sorbet only does local type inference, so the result of type checking one method never affects the result of type checking another method. This inference algorithm is a pure function of the code inside a method and Sorbet's immutable indexes of what's defined where. Put this all together, and Sorbet's inference algorithm is embarrassingly parallel, scaling to as many cores as the machine has available while being able to use fast shared memory instead of copying large data structures.\n\nA bedrock for engineering values\n\nIn addition to the productivity boost, an unintentional benefit to come out of adopting Sorbet has been its cultural impact. In a fast-growing company, communicating and codifying cultural norms can be a full time job on its own! Sorbet lends concrete structure to some of Stripe's engineering norms.\n\nConsider the cultural norm “Stripe should grow more reliable over time.” Despite our best efforts, production incidents happen—our goal when an incident happens is to make sure the same one doesn't happen again. After years of using Sorbet, Stripe engineers reflexively reach for type annotations as a preventative tool when doing incident remediations.\n\nAs an aside, it's interesting to reflect on the classes of problems that simply don't happen at Stripe anymore (or if they do, they happen exceedingly rarely). For example: typos that used to manifest as NameError: uninitialized constant exceptions in production have been entirely replaced by static type errors. But even some more subtle problems are absent, like this one:\n\ndef update_invoice(invoice, paid) # ... end update_invoice('in_1KZ7eP2eZvKYlo2C3B98SLc9', true) # ??? ~\n\nDoes this method need to be passed a string invoice ID, or a full invoice object? Scanning the implementation for context clues can sometimes help, but type annotations replace guesswork with machine-checked assurances:\n\nsig {params(invoice: Invoice, paid: T::Boolean).void} def update_invoice(invoice, paid) # ... end # error: Expected `Invoice` but found `String` for argument `invoice` update_invoice('in_1KZ7eP2eZvKYlo2C3B98SLc9', true) ~\n\nView in the Sorbet Playground →\n\nThis brings up another norm: “public interfaces should have up-to-date documentation.” For this we use a clever trick about how Sorbet's strictness levels work. Sorbet activates in files with # typed: true comments at the top of the file, but only in a “best-effort” mode: type annotations aren't required and all methods behave as though their arguments were annotated with T.untyped . But by trading up to # typed: strict , Sorbet stops assuming T.untyped and instead requires signatures for all methods.\n\nTo encourage this, Stripe’s continuous integration (CI) system looks through all code changes and leaves a “Stripe code quality score” in a comment on the pull request, like this one:\n\nThe score is reported as a weighted sum of signals, where a smaller score is better. There are a lot of inputs to the score, and we hide the ones that don’t change in a given pull request, but the one relevant to Sorbet in the picture above reads, “Number of non-test files which are not strictly typed (typed below strict ).” This means both the author and reviewer get a heads up when new files aren't using # typed: strict , reminding them that at Stripe we really prefer all Ruby code to be type-annotated. After almost 4 years of Sorbet at Stripe, 85% of all non-test files opt into # typed: strict (and for that matter, over 95% of all files are # typed: true ).\n\nWe often say that most of Stripe's engineers haven't been hired yet. Tooling like Sorbet encodes lessons learned over the years and helps teach these lessons to new engineers in a hands-on environment. As we continue to grow, especially distributed around the globe, Sorbet will continue to serve as a concrete reference point for new and old coworkers to align on shared engineering values.\n\nRuby fits in alongside a handful of other languages in use at Stripe. Stripe is also deeply investing in building new product backends in Java, building delightful frontend experiences with TypeScript, and various pieces of infrastructure in Go. Stripe commits to staffing high quality development experiences across all of these languages, not just Ruby. Making strategic investments in tooling ensures engineers at Stripe write code that is safe and fast as we scale.\n\nAfter all this time, Sorbet is still gaining features, performance improvements, and bug fixes. We love that Sorbet lets us enhance Ruby's natural productivity while helping shape Stripe's code to be resilient and understandable as we grow. As we approach 5 years since Sorbet's conception, we can't wait to see where the next 5 years will lead!\n\nSorbet is written in C++ and compiles to WebAssembly, which means you can try it out in your browser. Below you’ll find a link to the Sorbet Playground.", "label": "non_personal"}
{"title": "Fast builds, secure builds. Choose two.", "url": "https://stripe.com/blog/fast-secure-builds-choose-two", "content": "From data pipelines written in Scala and Python to infrastructure defined in Terraform, Stripe engineers interact with many different programming languages daily. We rely on complex build pipelines to convert Go source code into native binaries, Java into bytecode, and TypeScript into transpiled bundles for the Stripe Dashboard. Even in interpreted languages like Ruby, our code-generation process creates hundreds of thousands of Ruby files containing everything from gRPC service definitions to GraphQL bindings.\n\nOur continuous integration (CI) system is responsible for orchestrating these build pipelines and executing the tens of thousands of test suites that our engineers depend on to validate their changes. Keeping CI performant is crucial for providing our engineers with a delightful development experience. Since our CI system also produces artifacts that ultimately process billions of dollars each day, it's vital that it meets an exceptionally high security bar. At Stripe, we lean on a combination of open-source technologies and novel engineering to deliver a CI system that meets both of these requirements.\n\nA common framework for defining builds\n\nAs our codebase grows in volume and variety, Stripe leverages Bazel to manage our build and test pipelines. Bazel provides a multi-language and multi-platform framework to define rules—recipes for how to build and test code in a specific stack. Many of the rules we use are maintained by the open-source community: rules_docker, rules_go and Java rules built directly into Bazel to name a few. Our infrastructure teams build on Bazel’s headline support for custom rulesets to define internal rulesets for Ruby, JavaScript, and Terraform. Our engineers build and test their libraries and services by using these rulesets to declare “targets” specific to their code. Each target instantiates a rule with a set of input files and other attributes. For example, a java_library rule could define a greeter target. The greeter target builds a libgreeter.jar file by invoking various “actions” defined by the rule. In this case, the java_library rule creates an action which invokes the Java compiler ( javac ).\n\nAfter engineers define their targets, Bazel is responsible for executing all the necessary actions to build and test a change. However, this execution phase is far from trivial. At Stripe’s scale, building our rapidly growing Java codebase requires executing upwards of two hundred thousand actions. Running all these actions from scratch on a single machine would take several hours, even on the largest commodity EC2 instances. Bazel offers two features to address this challenge: remote caching and remote execution. Remote caching allows Bazel to reuse outputs from an action’s earlier execution. Remote execution allows Bazel to distribute actions across multiple machines.\n\nScaling Bazel with remote caching and execution\n\nBazel’s remote caching and execution subsystems provide compelling opportunities to improve the performance and efficiency of our CI system. Our engineers consistently identify blazing fast builds as a force multiplier in their workflows. Keeping builds performant (e.g. sub-5 minutes) is core to keeping our engineers productive. Over the years, we’ve dedicated significant engineering resources to building a platform for remote caching and execution that delivers performance and efficiency wins without trading off security or reliability.\n\nTo illustrate the risks associated with a naive implementation, consider the implications of allowing any CI build to write to a remote cache. A malicious actor could then replace a business-critical binary trusted to securely handle invoice billing for Stripe customers with a corrupted version that reroutes funds to a personal bank account! Protecting ourselves from action cache poisoning requires that we only allow writes to the cache from trusted sources. A trusted source must faithfully execute actions and upload their true outputs. Fortunately, remote execution comes to the rescue by allowing Bazel to delegate action execution to a trusted source. The trusted sources are exclusively authorized to upload action results to the remote cache.\n\nCreating trusted build workers is easier said than done. Having our trusted worker run Bazel actions implies that we’re evaluating arbitrary untrusted code on our trusted machine. It’s critical that untrusted actions are prevented from writing directly to the action cache or otherwise influencing other co-tenant actions. Our initial implementation of the remote execution service used gVisor, an open-source implementation of the Linux kernel in user space. We coupled it with containerd to manage the container images in which actions execute. Our gVisor-driven sandbox ensured that we were resilient to not only privilege escalations, but also bugs in the Linux kernel. We were able to rest easy knowing that shipping malicious code to our production services would require breaching multiple strong layers of protection.\n\nWhile gVisor performed admirably for our initial workload, building our Go codebase, it faltered when faced with new workloads. JavaScript bundling, Ruby code generation and Java compilation all showed significant performance penalties in gVisor. In particular, we identified that the filesystem emulation in gVisor adds prohibitive overhead. Unlike Go compilation, which is primarily bound by user space CPU computation, common workloads in the new stacks issue thousands of filesystem syscalls. This behavior is largely attributable to how Ruby and Java import code by searching through a list of tens or hundreds of directories ( $LOAD_PATH and CLASSPATH respectively). For instance, running an empty Ruby unit test suite issues over 600,000 filesystem syscalls over the course of 5.5 seconds while searching a $LOAD_PATH with over 500 directories!\n\nReadout of execution time of a Ruby unit test file with a single no-op test, which immediately succeeds.\n\nThe search for a blazing fast sandbox\n\nWith application kernels like gVisor imposing a high overhead, and OS-level virtualization primitives like Linux containers lacking a robust enough security barrier, we started exploring hardware virtualization. Our performance goals led us towards Firecracker, a KVM-based microVM solution that features startup times in the 100s of milliseconds and substantially reduces I/O overhead. KVM allows the Linux kernel to act as a hypervisor and run virtual machines (VMs) using hardware virtualization. Our initial experimentation showed promising results, but Firecracker was far from a drop-in solution.\n\nOur most interesting challenge was providing actions with their input files. Before, with our gVisor sandbox, we’d execute actions in an OverlayFS filesystem containing a fixed container image at its base and another directory above it (the “execroot”) with the actions’ inputs, e.g. the test files to execute. Unbeknownst to the action, the execroot consisted entirely of hard links to a local “blobcache” (a directory that held all our input files). This design minimized filesystem setup overhead. For instance, consider running many JavaScript actions where each action requires the same 150K JavaScript files, comprising 2.5GiB, in its node_modules directory. Rather than repeatedly copying 2.5GiB of data for each action, we downloaded each file once into the blobcache. Then, each action received an independent node_modules directory composed of 150K hard links.\n\nHowever, Firecracker (or KVM in general) doesn't support an analogous design that depends on OverlayFS to share directories. Instead, KVM exposes a virtio-based API that only allows attaching entire block devices to the guest VM. Since hard links are only valid across the same filesystem, we’d have to directly attach the block device with the blobcache to each microVM. While that might work with a single concurrent microVM, physical block devices, especially ones receiving concurrent writes, can’t be safely mounted more than once. A naive approach of copying from the blobcache for each action would incur a steep performance penalty. We needed an alternative that would allow our remote execution service to binpack dozens of concurrent actions on a single machine.\n\nFortunately, Linux’s Logical Volume Manager (LVM) provides a compelling solution. Our remote execution service now relies on LVM to orchestrate the execution process:\n\nFirst, we continue to download action inputs into the blobcache. We concurrently boot our Firecracker microVM with empty placeholder disks and an optimized build of the Linux kernel. Then, using LVM’s snapshotting capability we create a copy-on-write snapshot of the blobcache’s logical volume. This snapshot occupies almost no physical disk space. The blobcache snapshot provides us with a logical block device that we attach to the booted Firecracker microVM using its RESTful API. With the containerd devmapper snapshotter (built on the same underlying technology that LVM snapshots abstract over), we create and attach a block device for the action’s container image. Then, we send our custom init process a gRPC request over a VM socket, instructing it to mount both block devices and execute the action. The action executes within a chroot that exposes a minimal filesystem built using OverlayFS. Finally, the blobcache snapshot serves a dual purpose, allowing the execution service access to the action’s outputs after the microVM’s termination.\n\nDiving into an ocean of opportunities\n\nThis novel sandboxing strategy is only one of the myriad techniques our remote build system leverages to improve performance. Our remote cache service responds to GetTree RPCs by returning a recursively flattened list of files and directories from a given root directory. The flattening process can be very expensive for large directories full of third-party dependencies. Since these directories rarely change, our remote cache service itself caches the flattening results in a bespoke “TreeCache.” Then, our GetTree handler walks the children of each level of the directory tree in parallel, fetching from the TreeCache when possible to short-circuit evaluation of a cached branch.\n\nIn this example, an isolated change to the src/climate/components/Badge.tsx file allows us to fetch >99.9% of the GetTree response from our TreeCache. Branches that are unchanged and thus cached are denoted with green dashes.\n\nOn the topic of large directories, we’ve started experimenting with an alternative strategy where actions depend on a SquashFS image that bundles action dependencies which don’t change often. For example, changes to a package.json (the primary input to a large node_modules directory) are few and far between. This has led to observed performance improvements across the board: the Bazel client spends less time building an action digest, our cache service spends less time in the GetTree RPC, and our execution service creates orders of magnitude fewer hard links.\n\nOur remote execution service has a couple other tricks up its sleeve. For example, when our distribution layer schedules an action on an executor, it checks if another executor is already running an identical action. If so, rather than reserving resources and executing the action itself, the second executor blocks on completion of the first execution and re-uses its results. Action merging consistently helps improve build performance and efficiency, especially for particularly lengthy actions. Since the Bazel client never checks the remote cache after starting an action, this optimization relies on our remote execution service.\n\nWe’re constantly on the lookout for new techniques to improve the performance, reliability and efficiency of our remote build services. For instance, we recently investigated NILFS, a log-structured filesystem that could rival LVM’s snapshotting performance. As our system grows, we’re exploring new strategies for load balancing in a highly heterogeneous environment, essentially solving a low-latency distributed scheduling problem. We’re eager to explore Firecracker’s snapshotting support which could help drive down latency in workloads where JVM startup is significant. For example, we could speed up Java compilation by scheduling actions on a microVM that has already started a JVM.\n\nProviding our engineers with a CI system that delivers rapid feedback on their changes and tightening the development loop is a top priority for Stripe. Our solution wouldn’t be possible without Bazel. Its primitives give our engineers and platform teams a foundation for expressing rich, domain-specific build and test pipelines. Engineers across the organization benefit from a shared vocabulary and toolkit that not only streamlines their support experience, but also provides our productivity teams with a single point of extraordinarily high leverage. In particular, features like cached build results and distributed build execution are table stakes as we strive to support thousands of engineers.\n\nRather than spreading our investment across bespoke caching and distribution models for every language’s build/test toolchain, we’ve invested deeply in implementing Bazel’s remote caching and execution APIs. Building remote caching and execution services that can delight everyone, from the Stripes testing their Subscriptions API change to the Stripes evaluating our infrastructure’s security posture, is a significant task. Our approach relies on a unique combination of technologies to meet its performance goals while balancing security. We’re far from done. Each morning, we’re invigorated by the opportunity to raise the bar of engineering productivity at Stripe.", "label": "non_personal"}
{"title": "Migrating millions of lines of code to TypeScript", "url": "https://stripe.com/blog/migrating-to-typescript", "content": "On Sunday March 6, we migrated Stripe’s largest JavaScript codebase (powering the Stripe Dashboard) from Flow to TypeScript. In a single pull request, we converted more than 3.7 million lines of code. The next day, hundreds of engineers came in to start writing TypeScript for their projects.\n\nSeriously unreal. I remember a short time ago laughing at the idea of typescript ever landing at Stripe, and then I woke up Christmas Monday morning and it was here. Mike Fix, Engineer, Stripe\n\nTypeScript is the de facto standard for JavaScript type checking, and our engineers have been overjoyed by this migration. We’re sharing our TypeScript conversion tool on GitHub to help others perform similar migrations.\n\nA brief history of JavaScript type checking at Stripe\n\nStripe has built large-scale frontend applications since 2012, including stripe.com, Stripe JS, and the Stripe Dashboard. As our company grew, we increased the quality and reliability of our products by type checking our JS code. In 2016, we were an early adopter of Flow, an optional type system for JavaScript developed at Meta (then Facebook). Since then, Flow has provided type safety for the majority of our frontend applications.\n\nExample of a generated Flow type for an API resource and associated endpoints.\n\nHowever, engineers had trouble working with Flow. The type checker’s memory usage would lock up laptops, and the in-editor integration was frequently slow and unreliable. Meanwhile TypeScript, an alternative type system developed at Microsoft, exploded in popularity thanks to its tooling and robust community. TypeScript availability became a top request among engineers at Stripe.\n\nStripe’s developer productivity team aims to provide our engineers with the most productive development environment of their careers, and delight in our tools is crucial for that. We work hard to identify the most pressing issues affecting developers; for example, we’ve built integrations into all of our development tools for reporting friction, which is quickly routed to the responsible teams and prioritized. TypeScript support was one such pressing issue and teams supporting frontend engineers began to plan out supporting TypeScript across the company.\n\nChoosing the right migration strategy\n\nOur largest frontend codebase powers the Stripe Dashboard and other user-facing products. The Dashboard codebase has tight coupling between disparate components and no cleanly factored dependency graph. An incremental migration to TypeScript would force developers to work in both languages to accomplish common tasks. We would also need an interoperability layer to sync type definitions between both languages and keep them consistent throughout the development process.\n\nIn late 2020, we formed a new horizontal JavaScript Infrastructure team: a group of engineers solely focused on elevating the experience of writing JS at Stripe. One of the team’s first challenges was to replace Flow with TypeScript without a long and uncertain migration.\n\nWe began by speaking to companies who had run similar migrations and read articles from Airtable and Zapier describing their experiences. These companies developed automated scripts to convert one language to another, ran them over their entire codebases, and merged the output as a single commit. Airtable had published their conversion script to GitHub as a source-to-source conversion tool, or “codemod,” that would parse Flow code and generate TypeScript.\n\nMigrating in this way would greatly reduce the cognitive overhead for engineers, who would not need to handle both type systems for the same product behavior. We could have a clean break between Flow and TypeScript.\n\nPlanning, preparation, and iteration\n\nWe were really impressed by the quality of Airtable’s conversion code and decided to use that as the basis for our migration efforts. Many thanks to the team at Airtable for building this out and sharing their work—the open source community benefits a ton from examples like this.\n\nWe began by copying Airtable’s codemod to Stripe’s monorepo to run against our internal code. Our JavaScript projects make heavy use of Sail, a shared design system of strictly typed React components, so that was our initial area of focus. We generated TypeScript definitions for Sail, rather than converting the code to TypeScript, as it would continue supporting applications written in Flow. To safely support both type systems, we wrote tests to verify the TypeScript definitions against any changes to the underlying Flow code. This approach would be too cumbersome for a large codebase, but thankfully the Sail component interface is explicit and quite rigid.\n\nThe core of the codemod was solid but not comprehensive: for many files, it would crash or generate imperfect output. Over several months we iterated to handle more syntactic and semantic edge cases.\n\nFor one simple example, JavaScript arrow functions can return a single expression without a return statement, such as the following:\n\nconst linesOfCode = () => 7;\n\nJavaScript object literals use braces to wrap property definitions. Because braces are also used to delineate blocks of statements, returning an object literal from an arrow function requires an additional set of parentheses to disambiguate:\n\nconst currencyMap = () => ({ca:'CAD', us:'USD'});\n\nWe noticed that the codemod was incorrectly stripping the extra parentheses from these arrow functions, but only in the case of a generic function (a function that takes a type argument), which is syntax not available in standard JavaScript:\n\n​​// bad!\n\nconst wrapper = <T>(arg: T) => {wrapped: arg};\n\nWe were able to fix this issue and add tests to prevent further regressions: There were dozens of similar syntactic fixes we made to handle the breadth of our codebases.\n\nOnce Sail was usable from TypeScript, we worked on a couple of internal applications containing hundreds of JS modules. We also added a second pass to the codemod to suppress errors in the generated code, using TypeScript’s @ts-expect-error comment to tag these errors. Rather than resolving every error ahead of time, we focused on eliminating Flow as soon as possible, tracking TypeScript error suppressions to address after the conversion.\n\nAn initial pass on the Dashboard codebase created over 97,000 error suppressions. With our iterative approach to updating the codemod, we were able to get that number down to 37,000, or about one per thousand lines of code. For comparison, the Flow code had about 5,000 error suppressions. Both Flow and TypeScript support measuring type coverage, and we were pleasantly surprised that TypeScript reported higher coverage than Flow even with these suppressions. We attribute that to an increase in the number and quality of third-party type definitions available in TypeScript, the lack of which was a large contributor to poor type coverage in Flow.\n\nAs we moved onto the Dashboard with its tens of thousands of modules, our approach created significant memory pressure on the TypeScript compiler. Our primary tool to address this was TypeScript project references: Although the Dashboard is not structured as distinct modules, we could infer a module structure and create project references based on that. This approach gave us the headroom to run TypeScript over the codebase without refactoring large chunks of application code.\n\nGoing live\n\nHundreds of engineers contribute to the Dashboard each week. Such a sweeping change would be exceptionally challenging to merge on a normal working day. Our team decided to commit to a date—March 6, a Sunday—where we would lock the Stripe monorepo and land our branch.\n\nIn the week before merging, we focused on passing a build through our CI system and deploying it to our QA environment. Although TypeScript could successfully check the project, other tools that process our source code (ESLint, Jest, Webpack, Metro) would also need updates.\n\nOne particular pain point was Jest snapshot testing: Jest generates snapshot files with a hardcoded reference to the test file that generated them. Since the codemod would generate either .ts or .tsx extensions for TypeScript files, the snapshot files would have invalid references back to their test sources. We simplified this by switching the generation to only use .tsx . This meant we could rewrite the snapshots in bulk and keep 100% of those tests passing.\n\nIn some cases we recognized that fixing the code for TypeScript compatibility would add weeks to our schedule. One set of cases was our custom ESLint rules: We had a rule to reorder imports to enforce consistency between files, but the rule was written against Babel’s Flow parser, which generated a subtly different abstract syntax tree from the TypeScript parser. In cases like this, we opted to disable some checks and do the work to restore them after the conversion.\n\nWith a passing build in hand, we reached out to product teams with user-facing functionality in the Dashboard. Although the Dashboard has extensive unit and functional testing, it has limited end-to-end test coverage. This made manual tests by product stakeholders crucial. Those tests highlighted some minor bugs, which we resolved during the final week: In one case, we were failing to load any translations for non-English Dashboard users, due to a hardcoded .js extension in the translation loading code.\n\nThis process gave us high confidence, but there is always uncertainty with a change this large: Although we had a firm grasp on our developer tooling and build processes, we were mutating every file in the codebase. Subtle errors in our conversion scripts (for example, removing an empty field from an object shared between multiple components) could cause user-facing errors, without being covered by any of our existing automated tests. These failures could manifest in a number of ways, from downstream dev tooling issues to builds that fail. We leaned on our deploy automation and ambient monitoring to make us aware of any unexpected problems, and created a Slack channel to coordinate the rollout so user-facing teams could quickly escalate any reports they would receive.\n\nOn Saturday, March 5 the team generated a new migration branch and ran our automated scripts. We then deployed that branch to QA and repeated our validation process, including the manual tests suggested by product teams. We found no new issues. We were ready for the day of the merge.\n\nEarly on the morning of Sunday, March 6, we locked the Stripe monorepo, took one more QA pass over our migration branch, and submitted the change. It merged cleanly and our automated tests passed. We kicked off the deployment to ship TypeScript into production.\n\nThanks to the care and rigor of the previous year of work, we had no unpleasant surprises as we shifted traffic to the new code. We unlocked the repository and let developers know that the Dashboard was now in TypeScript.\n\nWhen I was interviewing, I heard the migration from Flow to TypeScript was underway. I was admittedly skeptical, seeing prior teams struggle with the complexity and effort of even small codebases. The fact that I was back to normal in a few minutes [on] Monday was humbling. Eric Clemmons, Software Engineer, Stripe\n\nThe immediate response was overwhelming. Engineers were impressed by the completeness of the migration: one described it as the single biggest developer productivity boost in their time at Stripe. We were happy to have the year of work pay off with such a clear and dramatic improvement to Stripe’s codebase.\n\nTypeScript… two months later\n\nThe conversion was not perfect. Over the subsequent weeks our JS Infra team addressed issues as they arose. One example we didn’t anticipate was engineers reporting inconsistency between CI and local TypeScript runs. In TypeScript we are able to use many third-party type definitions installed from npm, and if those are updated, engineers will need to install the new versions. This was different from our Flow configuration, where dependency updates rarely changed types, so we had to educate engineers to try running yarn install as a debugging step.\n\nThere is still more work to be done: We know performance could improve further with more granular project references, and better caching could speed up our CI runs. However, the benefits have far outweighed the bumps along the road. Engineers enjoy features such as automatic dependency imports and code completion, as well as the TypeScript community’s extensive corpus of third-party type definitions and integrations. When new engineers join Stripe to write frontend code, from day one they can be successful in a language with which they’re more likely to be comfortable and familiar.\n\nWith the work on the Dashboard complete, the JS Infra team has continued to increase TypeScript’s adoption across the company. We’ve used the same tools to convert many other codebases, including all of our Payments UIs, such as Stripe Checkout. Stripe frontend engineers will soon write TypeScript for whichever project they develop.\n\nWhen we first shared the story of our migration publicly, the response was equally enthusiastic. Developers from across the industry reached out to learn more and apply the same improvements to their own codebases. To support these developers, we’re sharing our TypeScript conversion code on GitHub for teams to adapt to their own projects.\n\nAside from the particulars of JavaScript or Flow or TypeScript, our big lesson from this migration is that dramatic improvements to large codebases are possible with diligence, commitment, and optimism. We will apply that mindset to other opportunities to make our engineers more effective and hope others do the same.", "label": "non_personal"}
{"title": "How Stripe builds interactive docs with Markdoc", "url": "https://stripe.com/blog/markdoc", "content": "At Stripe, our product docs are designed to feel like an application rather than a traditional user manual. For example, we incorporate a user's own API test key into code samples, making it possible to copy and paste code that seamlessly works with the user's own account. We have client-side interactivity, like checklists and collapsible sections. We tailor the content to the individual user, conditionally displaying content based on their location or the Stripe features they use. These features result in a high-quality user experience that reduces friction and contributes to the success of developers.\n\nFor these capabilities to have the desired impact we have to make it easy for writers to use them in their content. Delivering a good user experience without compromising the authoring experience required us to develop an authoring format that enables writers to express interactivity and simple page logic without mixing code and content.\n\nOver several years, we learned how to balance interactivity, customization, and authoring productivity while undertaking a major overhaul of our documentation platform.\n\nPast is prologue\n\nTo understand how we got here it's important to understand where we started. The legacy documentation platform that we replaced was a monolithic Ruby application built with ERB templates and Sinatra routing. The content freely mixed HTML, Markdown, Ruby, and ERB helper functions.\n\nMixing code and content provided a natural way to programmatically tailor the docs to the developer, but it posed serious challenges to quality and maintainability when the body of content grew to hundreds of pages. Alongside the technical burden of maintaining code within the content, the behavior of the code can make the content itself harder to understand and manipulate safely, particularly when used by many different teams with different objectives, timetables, and areas of expertise. Content authoring effectively became software development, and with that became subject to the same technical complexity and overhead.\n\nWe wanted to introduce new content formats with significantly more interactivity and more sophisticated frontend logic, but we knew that the limitations of a code-first approach would prevent us from using these features widely. For example, our integration builder format, which was originally created as a React application with content authored in JSON, became much easier for technical writers to reproduce and maintain when it was migrated to use Markdoc for the authoring experience.\n\nDesigning Markdoc\n\nWhen we began building our current documentation platform, we wanted to simplify our authoring experience by adopting an intuitive format like Markdown. Although Markdown is significantly easier to read and reason about than ERB templates, its simplicity also imposes limitations that make it challenging to use for rich content like our product docs.\n\nMarkdown is a relatively flat format that isn't designed to express complex structure or hierarchy. It offers a small number of formatting features and provides limited control over presentation. It does not have exotic templating features like support for custom page logic, variables, conditionals, or content reuse. Markdown's enduring success and relative ubiquity are largely due to its intentionally narrow scope and the restraint exercised in its design. It is easy and enjoyable to use because it prioritizes readability and leans heavily on intuitive plain-text authoring conventions.\n\nOur custom authoring format, called Markdoc, was designed to decouple code and content while enforcing proper discipline at the boundaries. Instead of allowing each page to be treated like an open-ended application, it imposes constraints on styling and programming, providing prescriptive rails for content extensibility. It extends Markdown with custom syntax that meets the needs of our documentation platform without sacrificing Markdown's simplicity, familiarity, or ease of use for writing prose. Following the ethos and design sensibility of Markdown, Markdoc keeps the overall surface area of new features small by adding a few highly-composable primitives that can be used together to express all the functionality we need.\n\nMarkdoc provides an extensible system for defining custom tags that can be used seamlessly in Markdown content. Using the custom tag syntax, we're able to support features like conditional content, content inclusion, and variable interpolation.\n\n# This is a heading {% #section %} {% callout type=\"info\" %} This is a paragraph with *formatted* text inside the callout {% /callout %} {% if $condition %} This content shows if `$condition` is true. {% /if %} ~\n\nThe features we decided to leave out of Markdoc in order to protect content maintainability are a critical aspect of its design. For example, when deciding what built-in flow control to include in Markdoc, we deliberately chose not to include looping. We wanted to discourage writers from performing procedural content generation from inside of a document, forcing them to move it outside of the system for better encapsulation. We also decided to leave out variable assignment in order to ensure that the content is fully stateless, thus eliminating an entire class of potential bugs.\n\nI like Markdoc because it lets us still do anything we want with code in the docs without bogging down the content authoring experience. If we need some new component, designers and engineers can whip that up. So as a writer, I can work in the docs content and stay focused. Lucie Lozinski, Technical Writer\n\nReact integration\n\nMarkdoc has a modular rendering system that supports multiple output formats. Using Markdoc’s React renderer, a Markdoc document can be rendered to a React virtual DOM. Custom Markdoc tags can be configured to output React components, passing through tag attributes as React props. Markdoc also supports assigning custom React components to standard Markdown document nodes such as headings and paragraphs.\n\nDefining custom Markdoc tags that output React components makes it possible to include interactive features, like tab switchers and collapsible content sections, inside of documents. Using custom tags to express these features helps create a writer-friendly interface for the functionality.\n\nThe React ecosystem also has a wealth of useful and interesting libraries that we can incorporate into our documentation to enrich presentation. For example, we're using the React Flow library to create interactive diagrams in our documentation. We defined a set of Markdoc tags for expressing the contents of a diagram, making it easy for writers to build beautiful and consistent visual representations of APIs and technical concepts from a set of composable elements.\n\n{% diagram type=\"sequence\" description=\"Usage-based billing\" %} {% node #customer icon=\"customer\" %} Customer {% /node %} {% node #typographic icon=\"platform\" %} Typographic {% /node %} {% node #stripe icon=\"stripe\" %} Stripe {% /node %} {% edge from=\"customer\" to=\"typographic\" %} Select plan {% /edge %} {% edge from=\"typographic\" to=\"typographic\" %} Create [usage records](/docs/api/usage_records) {% /edge %} ~\n\nUnlike static images, the diagrams that are built with React Flow can easily incorporate interactivity and clickable links. They are also easier to localize and can be restyled universally.\n\nMoving our documentation frontend to React was an important goal of our platform overhaul. Stripe already used React across many parts of the user experience, including our API reference docs and user dashboard. Enabling integration and cross-pollination between those surfaces and the product docs opens up a lot of exciting opportunities for future innovation, like showing API reference overlays when the reader hovers their cursor over a function or parameter in a code example. Sharing a common set of components from Stripe's internal design pattern library helps improve cohesion.\n\nReact also offers some compelling technical advantages. The implementation of interactive frontend components in our legacy stack was split between markup ERB templates and logic written in JavaScript which made it difficult to properly encapsulate, extend, and reuse functionality—a set of problems that modern component-based frontend frameworks address in a more satisfying way.\n\nMarkdoc comes with two distinct React renderers: a renderer that dynamically builds the React virtual DOM tree on the client side, and a static renderer that transpiles the document to JavaScript code. We use the dynamic renderer in our documentation platform at Stripe, but the static renderer is useful in cases where you want to treat a piece of Markdoc content as though it is a React component or JavaScript module. For example, the static renderer makes it possible to implement a Markdoc loader for Webpack in only five lines of code.\n\nconst Markdoc = require('@markdoc/markdoc'); module.exports = function (source) { const {schema} = this.getOptions() || {}; const ast = Markdoc.parse(source); const transformed = Markdoc.transform(ast, schema); const output = Markdoc.renderers.reactStatic(transformed); return `import React from 'react'; export default ${output}`; }; ~\n\nModular rendering\n\nAlongside the React renderers, Markdoc also includes a string-based HTML renderer that can be used for conventional server-side rendering or integration with standards-based Web Components. Markdoc's modular rendering architecture makes it possible for third parties to build custom renderers for additional frameworks and systems.\n\nMarkdoc content is entirely agonistic with respect to the technology used to present the rendered document. Fully decoupling rendering from the document format gives us the flexibility to present the same content in radically different ways in the future—like incorporating it into a native mobile application or generating a print-ready output format such as a PDF. I even used Markdoc to make the slide deck for my presentation at the Write the Docs conference back in 2020. The Markdoc community has already started bringing support to other frameworks, including Vue and Svelte.\n\nEnsuring that rendering implementation details don't bleed into the content also helps to improve the authoring experience, avoiding complexity and simplifying maintainability.\n\nDocumentation as data\n\nMarkdoc's fully declarative syntax parses to an Abstract Syntax Tree (AST), a data structure that represents the content of the document. We can take advantage of the AST to perform advanced static analysis and programmatically manipulate our content.\n\nMarkdoc lets us treat our documentation like data, writing simple scripts to programmatically inspect the content. If we want to perform tasks like identifying all of the fenced code blocks that contain a specific string or all of the places where we have a heading nested inside of a callout, we can do that robustly with the AST instead of relying on text scraping and regular expressions.\n\nWe are building automated refactoring tools that use the AST, making it possible to perform complex edits across our entire body of content with a higher degree of robustness than old-fashioned find-and-replace.\n\nOne of the most important ways that we use the AST today is for validation. For every Markdoc tag and document node type, there's a schema definition specifying the names and types of the attributes it accepts, what kind of document nodes can be nested inside of it as children, and other relevant metadata. The Markdoc validator uses this information to verify the correctness of a given Markdoc document.\n\nSchemas can also include arbitrary logic that analyzes the document nodes and returns custom errors. We use this to support features like link validation, checking to make sure that every link between pages within our documentation points to a valid route. It can also be useful for enforcing certain style guidelines that relate to the document structure, like preventing authors from using the wrong heading levels in certain places.\n\nWe run the Markdoc validator in our continuous integration system to ensure correctness at build time, but we also have an internal Visual Studio Code extension that exposes validation errors in real time while the user is typing.\n\nMarkdoc makes it easy for me to build rich, interactive experiences around documentation, then surface that capability to other authors through a simple declarative interface. John O'Sullivan, Software Engineer\n\nUnder the hood\n\nMarkdoc's parser is written in JavaScript and built on top of a popular open-source Markdown library called markdown-it. Markdoc is relatively lightweight—the markdown-it library is its only direct dependency. It is intended to run in Node.js and similar server-side JavaScript environments, but it can also be bundled for use in the browser.\n\nMarkdoc uses markdown-it as a tokenizer, building its AST from the array of tokens emitted by markdown-it. Parsing logic for Markdoc's custom tag syntax is generated from a peg.js grammar and integrates with markdown-it via a plugin.\n\nMarkdoc has its own dedicated rendering architecture rather than relying on markdown-it to generate its output. Developing an independent rendering system was necessary in order to handle Markdoc's custom tags and support multiple output formats.\n\nMarkdoc rendering is performed in several phases. First, the variable resolution step converts all of the variables in the document into their corresponding values. Next, the transformation step recursively walks through the document nodes in the AST and uses the node and tag schema definitions to generate a tree of renderable document nodes—a data structure that corresponds with the shape of the rendered output. Finally, the tree of renderable document nodes is passed into the desired renderer, which emits the actual rendered output.\n\nMarkdoc document's AST can be serialized to JSON and cached for later use, improving performance by obviating the need to parse the document every time it is rendered. Our product documentation platform at Stripe maintains an in-memory cache of the AST at runtime, but we are considering moving to an architecture where we serialize the AST at build time in order to eliminate runtime Markdoc parsing entirely.\n\nWhen I first started using Markdoc at Stripe, I was delighted by how easy it was to structure docs exactly as I envisioned them. With other authoring tools, useful visual elements like collapsible sections, asides, tabs, tables, multiple-language code samples, and many more often required heavy customization or new development. With Markdoc, I have a full palette ready to use. After using Markdoc, it's hard to imagine going back to another authoring tool. Ryan Young, Technical Writer\n\nMay the source be with you\n\nOur team at Stripe spends a lot of time thinking about the authoring experience and how to get it right. In many ways, Markdoc is the embodiment of our obsession with building a better authoring experience. It's our way of bottling up everything we have learned about this topic and sharing it in a reproducible way.\n\nAfter migrating all of our content to Markdoc and seeing the advantages fully realized in production, we set out to make Markdoc available under an MIT license so that others could benefit from our efforts.\n\nWe released Markdoc to the public in May, publishing a package on npm. We also published a draft specification that formally describes the Markdoc tag syntax, with the aim of making it easier for developers to incorporate support for Markdoc tags into other Markdoc parsing libraries.\n\nMarkdoc is hardly the final word on content authoring, but we hope that our contribution to the dialog will inspire others and help elevate discussion about the importance of the authoring experience in documentation.\n\nInterested in using Markdoc at work? Let us know how we can help.", "label": "non_personal"}
{"title": "Simplifying payment methods code in our new API version", "url": "https://stripe.com/blog/dynamic-payment-methods", "content": "The landscape of payment methods is changing quickly. Buy now, pay later methods, little-known less than a decade ago, now account for more than $300 billion in transactions worldwide. Mobile wallets accounted for roughly half of global ecommerce payment transactions last year. And while we at Stripe love nothing more than keeping up with emerging payments trends, we realize most developers don’t want to constantly refactor and tweak their Stripe integration. So we’ve been thinking hard about the right set of tools to help you continuously offer the payment methods your customers want while minimizing engineering overhead.\n\nThe problem with specifying configurable properties in code\n\nWriting code for a payments interface typically requires you to specify three types of properties: the capabilities of your payments integration (e.g., whether your application can support redirects), the variable properties of a given payment (e.g., amount or currency), and numerous configurable properties (e.g., do not show Affirm for transactions less than $50). These configurable properties, which are typically hard-coded in a parameter like payment_method_types , can cause land mines that result in payment failures and errors that are hard to debug.\n\nThis is because payment methods have nuanced limitations that impact whether they can be used for a given transaction—whether it’s transaction minimums and maximums, currency limitations, merchant category restrictions, or differences in recurring payment support. Implementing a new payment method means you need to 1) know all the specific limitations for that payment method and 2) encode complex logic to hide or display that payment method based on those limitations for each transaction. Doing either of these wrong means risking payment failures and disputes, leading to lots of time and effort to ship new changes.\n\nMaking dynamic payment methods the default in the API\n\nRequiring a code change to modify configurable properties is the type of high-toil engineering work you’ve consistently told us you want to eliminate. So today, we’re changing the default behavior of the PaymentIntents and SetupIntents API. Instead of individually specifying payment_method_types in code, you can now configure payment method settings from the Dashboard—no code required. We call this integration path dynamic payment methods.\n\nWhen you use dynamic payment methods with our prebuilt UIs like the Payment Element or Checkout, Stripe handles the logic for displaying eligible payment methods in your frontend for each transaction—removing the need to know and encode specific eligibility requirements. We then dynamically order all of your eligible payment methods to maximize for conversion based on factors including the customer’s device, location, and local currency. If you want to change your payment methods configuration later, you can do so directly from the Dashboard. For example, if you want to offer different payment methods for one-time versus recurring payment flows, or if you want to hide high-risk payment methods on high-AOV transactions, you can set these rules from the Dashboard, rather than writing complex error-prone code. Your integration becomes simpler, safer, and more future-proof.\n\nBefore\n\napp.post(\"/create-payment-intent\", async (req, res) => { const { items } = req.body; const orderAmount = calculateOrderAmount(items) const orderCurrency = calculateOrderCurrency(items) let paymentMethodTypes = [\"card\"]; // Add specific payment methods based on currency and amount switch(orderCurrency) { case \"eur\": paymentMethodTypes.push(\"giropay\"); break; case \"gbp\": paymentMethodTypes.push(\"klarna\"); if (orderAmount >= 100 && orderAmount <= 100000) { paymentMethodTypes.push(\"afterpay_clearpay\"); } break; case \"usd\": paymentMethodTypes.push(\"paypal\"); break; default: } // Create PaymentIntent with payment_method_types const paymentIntent = await stripe.paymentIntents.create({ amount: orderAmount, currency: orderCurrency, payment_method_types: paymentMethodTypes, }); res.send({ clientSecret: paymentIntent.client_secret, }); }); ~\n\nAfter\n\napp.post(\"/create-payment-intent\", async (req, res) => { const { items } = req.body; const orderAmount = calculateOrderAmount(items) const orderCurrency = calculateOrderCurrency(items) // Create PaymentIntent with dynamic payment methods by default const paymentIntent = await stripe.paymentIntents.create({ amount: orderAmount, currency: orderCurrency }); res.send({ clientSecret: paymentIntent.client_secret, }); }); ~\n\nWe’ve paired this API change with tools to give you confidence that your integration is working as expected and to determine why certain payment methods aren’t available for a given transaction. From the Dashboard, you can enter a PaymentIntent ID or Checkout Session for more information on which payment methods were enabled for the transaction and why certain options were not eligible. You can also simulate which payment methods will be displayed when changing factors such as amount, currency, capture method, and future usage.\n\nAnd we’re not stopping here. Since it no longer takes a code change to configure payment methods, you can rely on Stripe to tackle all kinds of checkout optimizations that previously required nontrivial engineering effort. You can use Dashboard controls to run A/B tests on payment methods, configure payment methods for different checkout scenarios, and set custom targeting rules. To take advantage of these new features, upgrade to the latest API version today.", "label": "non_personal"}
{"title": "How we built it: Stripe Radar", "url": "https://stripe.com/blog/how-we-built-it-stripe-radar", "content": "As an engineer on Stripe’s fraud prevention team, I obsess about a single moment that lasts just a fraction of a second. It begins when someone clicks “purchase,” and it ends when their transaction is confirmed.\n\nIn that brief interval, Stripe Radar goes to work. Radar is Stripe’s fraud prevention solution. It assesses more than 1,000 characteristics of a potential transaction in order to determine the likelihood that it’s fraudulent, letting good transactions through and either blocking risky transactions or diverting them to additional security checks. It makes this decision, accurately, in less than 100 milliseconds. Out of the billions of legitimate payments made on Stripe, Radar incorrectly blocks just 0.1%.\n\nOnline payment fraud is a hard problem to solve. Any effective tool needs to be accurate, fast, and inexpensive to run for each transaction. It needs to balance blocking bad transactions against false positives (good payments that are blocked), which hurt consumers and our users’ bottom lines. The challenge is compounded by the fact that fraud is rare—on the order of 1 out of every 1,000 payments.\n\nTo identify fraudulent transactions, we rely on the breadth of the Stripe network—our biggest strength. We’ve done so by improving our machine learning (ML) architecture while enhancing the way we communicate with users about the reasons behind fraud decisions. In this post, we want to share what makes Radar so powerful and take you through some of the key decisions we’ve made—and lessons we’ve learned—over the almost seven years we’ve been building it.\n\nLesson 1: Don’t get too comfortable with your ML architecture\n\nWe started with relatively simple ML models (e.g., logistic regression) and over time have advanced to more complex ones (e.g., deep neural networks), as the Stripe network has grown and ML technology has advanced. With each architectural jump, we have observed an equivalent leap-size improvement in model performance.\n\nOur most recent architecture evolution occurred in mid-2022 when we migrated from an ensemble “Wide & Deep model,” composed of an XGBoost model and a deep neural network (DNN), to a pure DNN-only model. The result was a model that trains faster, scales better, and is more adaptable to the most cutting-edge ML techniques.\n\nThe previous architecture combined the power of memorization (the wide part, powered by XGBoost) with generalization (the deep part, powered by a DNN). It worked well, but limited the rate at which we could improve. XGBoost was incompatible at scale with more advanced ML techniques we wanted to take advantage of (e.g., transfer learning, embeddings, long training times) and also slowed the rate at which we could retrain the model because an XGBoost model is not very parallelizable—which inhibited the experimentation velocity of the many engineers who worked on the model each day.\n\nWe could have just removed the XGBoost component, but that would have caused a 1.5% drop in recall—an unacceptably large regression in performance. While XGBoost is not a deep-learning method or a cutting-edge technique these days, it still provided unique value to our model’s performance. To replace it, we looked for ways to build a DNN-only architecture that added the memorization power we’d been getting from XGBoost, without compromising the DNN’s ability to generalize.\n\nA straightforward way of improving both memorization and generalization is to increase the DNN’s size—both its depth and width. However, achieving a more-performant model wasn’t as easy as that.\n\nIncreasing the model’s size immediately improved the representational capacity of the model to learn features at both the abstract level (e.g., payment velocity and “unusual volume on a card”) and the fine-grained level (e.g., correlations between features). However, increasing depth too much ran the risk of overfitting, causing the model to memorize random noise in the features. So, in order to build a DNN-only architecture, we had to find the sweet spot that maximized a representational capacity to learn various levels while remaining resistant to overfitting.\n\nWe decided to read up on popular publications about DNN architecture and adopted a multi-branch DNN-only architecture inspired by ResNeXt. ResNeXt’s architecture adopts a “Network-in-Neuron” strategy. It splits a computation into distinct threads, or branches, where a branch can be thought of as a small network. The outputs from the branches are then summed to produce a final output. Aggregating branches has the benefit of enriching the learned features by expanding a new dimension of feature representation. It does this in a way that is more effective than the brute-force approach of merely increasing depth or width to improve accuracy.\n\nBy removing the XGBoost component of the architecture, we reduced the time to train our model by over 85% (to less than two hours). Experiments that previously required running jobs late into the night could now be completed multiple times in a single working day, a massive shift in our ability to prototype new ideas. The improvements were a good reminder to not get too comfortable with the way we were currently doing ML and to ask ourselves: If we were starting over today, what kind of model would we build?\n\nAsking those questions is allowing us to take on even more ambitious initiatives for our next year of work. These include incorporating more advanced ML techniques like transfer learning, embeddings, and multi-task learning, all of which we are actively exploring in 2023.\n\nLesson 2: Never stop searching for new ML features\n\nIn addition to evolving our model architectures, we also want to ensure our models are incorporating the richest signals. By carefully noting the common behaviors of fraud attempts, Radar has been able to compile a deep understanding of fraudulent activity and trends. This gives Radar an important advantage when put to work: Each increase in the size of Radar’s training data set creates outsized improvements in model quality, which wasn’t the case with XGBoost.\n\nOne of the biggest levers we have to make model improvements is through feature engineering. Some features could likely have an outsized impact on model performance, but first we need to identify and implement them. To do this effectively, we’ve created several processes to enable ML engineers.\n\nWe review past fraud attacks in exacting detail, building investigation reports that attempt to get into the minds of the fraudulent actors. We look for signals in the payments, like a common pattern for throwaway emails (e.g., 123test@cactuspractice.com) that might be used by fraudulent actors to quickly set up multiple accounts. We then broaden our search across the Stripe network to look for correlations in timing and signals that could connect to previous fraud attacks. Every week, the Radar team also meets to discuss new fraud trends that emerge from research into activity on the dark web.\n\nWe gather all of this information and ideate features that target the specific contours of each attack. We come up with a prioritized list, quickly implement each feature, then prototype each one to understand the impact on our model’s performance.\n\nSometimes we strike gold. Other times, even our most promising features don’t pan out. This happened once when we introduced a Boolean feature capturing whether the business was currently under a distributed fraud attack. This feature didn’t improve our model’s performance as much as we’d anticipated. As it turned out, our ML was already incorporating these patterns, even though we never expected it to. This reflects the fact that the current version of Radar is built on top of years of work by many generations of engineers.\n\nBesides developing new features, another method we explore for increasing model performance is increasing the size of our training data. With the success of ML models like ChatGPT, and large language models generally, we wanted to see if we could achieve a similar feat with Radar: Could we start with a relatively simple DNN-only architecture and get large improvements in model performance just by increasing the amount of training data?\n\nThe primary impediment to doing this was that the time to train increases linearly with the size of the training data. But thanks to the training-speed improvements we made when we switched to a DNN-only architecture, this was less of an issue.\n\nWe ran some experiments using more transaction data and got encouraging results: We made a 10x increase in training transaction data and still found significant model improvements. We’re currently working on a 100x version to generalize the results even further.\n\nIn a future post, we will dive deeper into new techniques we’re exploring to further use the power of the Stripe network and our ability to apply these insights to fight fraud, even after a payment has already occurred.\n\nLesson 3: Explanation matters as much as detection\n\nBuilding a great fraud-detection product is about more than just identifying fraud. There’s a large personal dimension to it, too. When a good transaction is flagged—or a fraudulent one gets through—our users want to know why, because false positives hurt their bottom line and frustrate their customers. Explaining fraud decisions is an area in which we’ve made a lot of investments in recent years.\n\nAnd it’s a challenge. All ML models are black boxes to an extent, and deep neural networks even more so than other types of models. It’s hard to explain to users why Radar scores transactions the way it does. This is another tradeoff we came to accept when deciding to use DNNs over simpler, more traditional ML techniques. But our engineers know the system well and have developed a range of ways to help users understand what’s going on.\n\nIn 2020 we built our risk insights feature, which lets users see which features of a transaction contributed to a transaction being declined. These can include whether the cardholder’s name matches the provided email and the number of cards previously associated with an IP address. A high number of cards may indicate suspicious behavior, such as a bad actor trying out multiple stolen credit cards. However, there may also be legitimate reasons for this, and our model evaluates this feature in the context of all our signals, understanding the correlations that may exist between them to accurately distinguish between fraudulent and good payments.\n\nRecent improvements to risk insights include displaying maps to users with the locations of purchase and shipping addresses and using Elasticsearch to quickly share related transactions, which further helps users put a specific decline in context.\n\nIn addition to providing users with insight into fraud decisions, we have been working on more sophisticated techniques for gaining deeper understanding of our ML model. This tooling includes a simple table view that displays the exact features that contributed the most to raising and lowering a transaction’s fraud score. Our engineers are actively using these solutions internally to debug support cases, and we are working on plans for sharing these insights with our users as well.\n\nExplaining Radar’s ML outcomes as clearly as possible helps users understand the relative risk of a given payment, which fraud signals may have contributed to that risk score, and how a given payment compares to others. They can then take actions to improve the quality of data they are sending (in order to generate more accurate fraud decisions) or create custom allow or block rules to tailor Radar for their specific business needs.\n\nEvolving strategies, constant focus\n\nRadar is a very different product than it was when we started. We’ve overhauled the models we use, the way we employ transaction data from the Stripe network, and the way we interact with users. Over that same period fraud patterns have changed considerably, too, from primarily stolen credit card fraud to a growing mix of traditional card fraud and high-velocity card testing attacks today.\n\nBut in the ways that matter most, the goals of the Radar team are the same. We’re still working to create an environment in which businesses and customers can transact with confidence, and we’re still focused on optimizing that brief moment we hope customers don’t even notice: the last step in a checkout, the split second we have to detect fraud before a transaction is confirmed.\n\nWe’re excited to continue innovating on ML to solve hard, important problems. If you are, too, consider joining our engineering team.", "label": "non_personal"}
{"title": "Ledger: Stripe’s system for tracking and validating money movement", "url": "https://stripe.com/blog/ledger-stripe-system-for-tracking-and-validating-money-movement", "content": "Last Black Friday to Cyber Monday, Stripe processed 300 million transactions with a total payment volume of $18.6B—and the Stripe API maintained greater than 99.999% availability. Underlying these metrics is our Global Payments and Treasury Network (GPTN) that manages the complexity of accepting payments, money storage, and money movement. Today, Stripe supports more than 135 currencies and payment methods through partnerships with local banks and financial networks in 185 countries. These entities provide different interfaces, data models, and behaviors, and Stripe continually manages this complexity so developers can quickly integrate the GPTN into their businesses.\n\nInternally, Stripe needs to guarantee that what we expect to happen during payment processing actually happens for internal customers and external auditors of our data. We built Ledger, an immutable and auditable log, as a trustworthy system of record for all of our financial data. Ledger standardizes our representation of money movement, and it serves as the scalable foundation for our automated Data Quality (DQ) Platform—guaranteeing Stripe faithfully manages money for users.\n\nMany existing systems provide primitives for accurate accounting, but the real world is imperfect, incomplete, and constantly changing. We witness basic and obvious failures like malformed reports or propagated errors from banking or network partners, and also broad macroeconomic changes such as currencies ceasing to exist or large banks collapsing overnight. While we aspire to an orderly ideal, at Stripe scale, that’s impossible—instead we built a system that keeps these imperfections manageable and bounded.\n\nLedger models internal data-producing systems with common patterns, and it relies on proactive alerting to surface issues and proposed solutions. Each day, Ledger sees five billion events and 99.99% of our dollar volume is fully ingested and verified within four days. Of that activity, 99.999% is monitored, categorized, and triaged through rich investigative tooling—while the remaining long-tail is reliably handled through manual analysis. Together, Ledger and the DQ Platform ensure over 99.9999% explainability of money movement, even as Stripe’s data volume has grown 10x.\n\nIn this blog post, we’ll share technical details on how we built this state-of-the-art money movement tracking system, and describe how teams at Stripe interact with the data quality metrics that underlie our global payments network.\n\nHow Stripe processes payments\n\nThe GPTN in part is a payment processing network consisting of customer business calls to Stripe’s API and Stripe’s interactions with a variety of banks and payment methods. There is complexity in tracking the requests Stripe makes to partners, the physical money movement between financial partners, and the reporting Stripe receives back. We make this multifaceted problem tractable by segmenting the Stripe platform into discrete services, databases, and APIs/gRPC interfaces, which lets us solve individual problems without getting overwhelmed by the broader system.\n\nThe challenge with this approach is that there is no intrinsic mechanism forcing these systems to represent or deliver data in the same way. Some might operate in real time, while others may operate on a monthly cadence with vastly different data volumes; some producers generate billions of events per day, while others may only generate a few hundred. Moreover, each system might have its own definitions of correctness or reliability. We require a mechanism that can deal with these variations and prove that these individual systems are collectively modeling our financials correctly.\n\nA simplified summary view of Stripe’s interactions with external entities\n\nHow we designed Ledger\n\nThe Stripe services mentioned above have independent responsibilities, but they collaborate to solve a large federated problem. An ideal solution provides a mental model for correctness—supported by trustworthy statistics—that easily generalizes to new use cases. Further, we want to represent all activity on the Stripe platform in a common data structure that can be analyzed by a single system.\n\nThis is the way we approach it:\n\nLedger encodes a state machine representation of producer systems, and models its behavior as a logical fund flow—the movement of balances (events) between accounts (states).\n\nLedger computes all account balances to evaluate the health of the system, grouped by various subdivisions to generate comprehensive statistics.\n\nThis approach abstracts individual differences between underlying systems and provides mathematical evidence that they are functioning correctly.\n\nLedger as a semantic data store\n\nLedger is a faithful representation of the underlying state of all payment processes on the Stripe platform. Instead of computing a derived dataset based on incoming data pipelines, Ledger models the actual work of producer systems, recording each operation as a transaction. Ledger modeling may diverge from upstream data, but we guard against these cases explicitly with data completeness checks.\n\nCombined with our other data quality metrics, we can safely rely on Ledger’s data representation to monitor external systems. If we instrument Ledger, we indirectly instrument the data-producing pipelines. And, if we identify a problem, we alert our internal users to which part of their data pipeline is broken—and exactly how they can fix it.\n\nProcessing a charge with a creation event for a pending charge, and a release event for completion\n\nInside of Ledger, we represent this activity as a movement of balances between two discrete states (creation and release), turning the above process into an observable state machine.\n\nProcessing a charge in Ledger, represented by a creation event for a pending charge and a release event for completion\n\nSystem abstraction\n\nLedger also abstracts producer systems. Instead of separately monitoring handoffs between data pipelines, we model systems as connected fund flows moving money between accounts. Because Ledger is a transaction-level system of record, we can prove that even complex multisystem pipelines with multiple stages of handoff are working correctly. We also model data consistency between otherwise disconnected systems, and we track individual transactions through their entire lifecycle. We call this tracing, and, at our scale, this totals to billions of daily transactions.\n\nUnifying separate systems with fund flows\n\nConsider an abstract end-to-end fund flow: for example, a business adding funds to its balance. This requires moving funds between banks, reconciling money movement with third-party reporting, and matching regulatory reporting with financial reporting. The fund flow spans multiple internal team boundaries, with discrete events published to different systems at different times. If we model this fund flow with logical constructs, Ledger can unify this data across separate systems and monitor its correctness.\n\nImmutability\n\nAt its core, Ledger is an immutable log of events. Transactions previously published into Ledger cannot be deleted or modified, and we can always reconstruct past state by processing all events up to that point. All constructs—balances, fund flows, data quality controls, and so on—are transformations of the static underlying structure. Ledger’s immutability ensures we can audit and reproduce any data point at any time. Immutability justifies our data quality measures by guaranteeing that we can explain and analyze the exact problematic data.\n\nHow we designed the Data Quality (DQ) Platform\n\nLedger is the foundation for our Data Quality (DQ) Platform, which unifies detection of money movement issues and response tooling. Empirically, the DQ Platform ensures reliable and timely reporting across Stripe’s key lines of business: we maintained a 99.999% readiness target, even as data volume grew 10x.\n\nTransaction-level fund flows give us powerful tools to reason about complex interconnected subcomponents. We analyze these abstractions with a set of trustworthy DQ metrics that measure the health of a fund flow. These metrics are based on a common set of questions across all fund flows. For a specific cross-section of data, evaluated at time X, we look at:\n\nClearing: Did the fund flow complete correctly?\n\nDid the fund flow complete correctly? Timeliness: Did the data arrive on time?\n\nDid the data arrive on time? Completeness: Do we have a complete representation of the underlying data system?\n\nWe then compose DQ metrics on individual fund flows to provide scoring and targeted guidance for technical experts. These measurements roll up to create a unified DQ score—a system with a 99.99% data quality score is extremely unlikely to hide major problems—turning a complex distributed analysis problem into a straightforward tabulation exercise. Technical users can likewise trust that improving DQ scores reflect true improvement in underlying system behavior and accuracy.\n\nClearing\n\nLedger is based on double-entry bookkeeping, a standard method for guaranteeing that all money in a system is fully accounted for by balancing credits and debits. Grounding our analysis in this construct gives us a mathematical proof of correctness. If you’ve never encountered this term before, a helpful explainer is “An Engineer’s Guide to Double-Entry Bookkeeping.”\n\nUsing double-entry bookkeeping to validate money movement is similar to analyzing a flow of water through a network of pipes (processes) ending in reservoirs (balance sheets). At steady state, terminal (nonclearing) reservoirs are full, and intermediate (clearing) pipes are empty. If there is water stuck in the pipes, then you have a problem—in other words, unresolved balances on the balance sheet.\n\nTraditionally, bookkeeping is purely an accounting construct, but we apply these ideas in a novel way. Rather than just tabulating cash flow in and out, we’re simultaneously modeling internal data system behaviors that may have nothing to do with physical movement of money—for example, currency conversion, report parsing, estimation, or billing analysis. We can use the same bookkeeping concepts to reason about those systems and evaluate their correctness in a much more general way.\n\nDetecting problems\n\nClearing measures the fraction of Ledger that is appropriately zeroed out at steady state. Consider an example that models two steps of a flow: charge creation (potential money movement) and release (funds becoming available). As you follow the flow, keep in mind these definitions:\n\nAccounts are buckets of money distinguished by their type (e.g., charge_unsubmitted ) and properties (e.g., id , business ).\n\nare buckets of money distinguished by their type (e.g., ) and properties (e.g., , ). Events move money between accounts (e.g., charge.creation and charge.release ).\n\nAt time T0 , the charge.creation event sets up a balance in the undisbursed account; then at T1 , charge.release completes the flow and moves the funds to the business_balance account.\n\nIt is important to note that the creation and release events are completely independent. Even if they arrive out of order, or are created by different sources, Ledger maintains accurate fund flows through the identifier for business and id . But, if the release event is never published or has the wrong id , Ledger would not clear the balance in the associated charge_undisbursed account, and it would instead hold the balance in a different instance of charge_undisbursed .\n\nExample clearing issue\n\nConsider next how a wrong value ( business: B vs. business: A ) results in two clearing accounts with nonzero balance. Instead of having one reservoir of money for business: A , we wind up with two—one for business: A and one for business: B .\n\nGeneralizing from this example, we repeat this process for every fund flow, account type, and property-based subdivision inside of Ledger. Even when we have billions of transactions, a single missing, late, or incorrect transaction immediately creates a detectable accuracy issue with a simple query—for example, “Find the clearing Accounts with nonzero balance.”\n\nTimeliness\n\nClearing prevents persistent problems, but we also need to guarantee data arrives on time for time-sensitive functions such as monthly report generation. Producers create time stamps when integrating with Ledger, and we measure the delta between when data first enters the Stripe platform and when it reaches Ledger. We set a hard threshold on the data delivery window, and we create headroom for subsequent reporting, analysis, and manipulations to guarantee 99.999% timeliness.\n\nCompleteness\n\nWe guarantee data completeness and guard against missing data from upstream systems with explicit cross-system checks alongside automated anomaly detection. For example, we ensure that every ID in a producer database has a matching Ledger event. We also run statistical modeling on data availability. We have models for every account type that use historical trends to calculate expected data arrival time and, if events do not appear, we interpret this as potentially missing data.\n\nHow teams at Stripe explore DQ metrics\n\nOn top of the DQ Platform, we built hierarchical automated alerting and rich tooling. We combine interactive metric displays with analysis and guidance. The experience for internal leaders and team members focuses on proactive feedback, simple manipulation of data, and meaningful metrics. We also provide use-case-specific context that depends on which part of the business is using it. For example, consider how we show team-level DQ metrics for our periodic financial reporting, which we call Accounting Close. Note: some details are blocked out for privacy.\n\nThe topline view is generally in a good state, but there are areas for improvement at the team level within the Payment Engineering group. For example, the 50% score for Aging Balances means that some clearing issues have persisted over time:\n\nA single team-level view of data quality metrics\n\nThis team-level view shows DQ metrics alongside a call to action including auto-generated tickets, relevant resources, and tool links—everything required for self-service. For leaders, this view provides the exact dollar impact of DQ issues.\n\nTactical views\n\nDQ scores drop when a problem is observed in Ledger. Although Ledger is a projection of underlying systems, Ledger problems are not usually problems of transcription or data modeling in Ledger. They primarily reveal real problems with system implementations, integrations, or physical money movement. In these cases, we provide tactical views to trace issues back to their root cause inside Stripe platforms or external systems.\n\nConsider an uncleared balance of a specific account type—a processing fee that must be invoiced and paid. At steady state, the invoice should be paid and the balance is zero, but over time we observe a nonclearing balance.\n\nInvestigation and attribution\n\nClicking on a point in the graph generates SQL queries in Presto (our ad-hoc SQL query engine) and surfaces relevant data: reference keys, metadata, ownership, and tips. If a Ledger user is unable to debug and publish a correction—perhaps because the root cause is related to an infrastructure or third-party incident outside their control—they can reassign ownership to the right internal stakeholders and exclude it from alerting.\n\nWhen issues are attributed to a known incident, we can retroactively analyze the impact to DQ metrics across teams to fully understand how Stripe was affected:\n\nCombined, we have the ability to measure and analyze data quality, identify root-cause problems, and flexibly interact with the underlying data constructs to manage our problem load over time. In this case, fixing problems in Ledger may involve republishing data from source systems.\n\nData correction\n\nLedger is our system of record and must remain an evergreen representation of truth. Persistent problems reduce visibility into new problems and may result in incorrect reporting or derived datasets. Because Ledger is an immutable log of events, we can’t run simple queries to mutate the state; instead, we have to revert and reprocess prior operations. If an incident occurs, we need a tool for correcting data at scale.\n\nWe built a supporting utility to create and safely execute migrations, protected by a data quality tool that generates out-of-band reports on the production impact of proposed changes. Together, these tools approximate a CI pipeline for ad-hoc data repair operations. All operations must go through a two-phase review and commit of the data—and its associated DQ impact.\n\nFewer data problems, more reliable reporting\n\nOur systems need to operate within a messy reality, but the innovations described in this blog post drive us towards a trustworthy and explainable operational model. Likewise, as businesses and mechanisms for money movement inevitably evolve, Stripe is empowered to keep pace with that change.\n\nThe DQ Platform ensures reliable and timely reporting across all Stripe business lines. The combination of clearing, timeliness, and completeness metrics ensures that internal stakeholders can make sound judgments about the correctness of underlying data systems without worrying about maintaining complex specialized knowledge.\n\nThe digital economy will continue to accelerate, and our focus is on building robust and scalable systems to power it. In the future, we want to improve timeliness to minute-level analysis and response—offering lower latency processing, which will strengthen fraud detection and increase available response time to address possible financial problems.\n\nWe are also investing in advanced enrichment capabilities that allow us to declaratively compose new datasets and reporting interfaces while guaranteeing that they meet our data quality bar. This work safely evolves the complexity of our internal systems alongside Stripe’s growth.\n\nWe’re excited to continue to solve hard, important problems. If you are too, consider joining our engineering team.", "label": "non_personal"}
{"title": "Shepherd: How Stripe adapted Chronon to scale ML feature development", "url": "https://stripe.com/blog/shepherd-how-stripe-adapted-chronon-to-scale-ml-feature-development", "content": "Machine learning (ML) is a foundation underlying nearly every facet of Stripe’s global operations, optimizing everything from backend processing to user interfaces. Applications of ML at Stripe add hundreds of millions of dollars to the internet economy each year, benefiting millions of businesses and customers worldwide. Developing and deploying ML models is a complex multistage process, and one of the hardest steps is feature engineering.\n\nBefore a feature—an input to an ML model—can be deployed into production, it typically goes through multiple iterations of ideation, prototyping, and evaluation. This is particularly challenging at Stripe’s scale, where features have to be identified among hundreds of terabytes of raw data. As an engineer on the ML Features team, my goal is to build infrastructure and tooling to streamline ML feature development. The ideal platform needs to power ML feature development across huge datasets while meeting strict latency and freshness requirements.\n\nIn 2022 we began a partnership with Airbnb to adapt and implement its platform, Chronon, as the foundation for Shepherd—our next-generation ML feature engineering platform—with a view to open sourcing it. We’ve already used it to build a new production model for fraud detection with over 200 features, and so far the Shepherd-enabled model has outperformed our previous model, blocking tens of millions of dollars of additional fraud per year. While our work building Shepherd was specific to Stripe, we are generalizing the approach by contributing optimizations and new functionality to Chronon that anyone can use.\n\nThis blog discusses the technical details of how we built Shepherd and how we are expanding the capabilities of Chronon to meet Stripe’s scale.\n\nML feature engineering at Stripe scale\n\nIn a previous blog post, we described how ML powers Stripe Radar, which allows good charges through while blocking bad ones. Fraud detection is adversarial, and Stripe needs to improve models quickly—fraud patterns change as malicious actors evolve their attacks, and Stripe needs to move even faster.\n\nML feature development is the process of defining the inputs (features) that a model uses to make its predictions. For example, a feature for a fraud prediction model could be the total number of charges processed by a business on Stripe over the last seven days.\n\nTo identify and deploy new features that would address rapidly changing fraud trends, we needed a feature engineering platform that would allow us to move quickly through the lifecycle of feature development.\n\nEffectively deploying ML models in the Stripe environment also requires meeting strict latency and feature freshness requirements.\n\nLatency: A measure of the time required to retrieve features during model inference. This is important because models such as the ones powering Radar are also used in processing payments, and the time required to retrieve features directly impacts the overall payment API latency—lower latency means faster payments and a better overall customer experience for businesses.\n\nA measure of the time required to retrieve features during model inference. This is important because models such as the ones powering Radar are also used in processing payments, and the time required to retrieve features directly impacts the overall payment API latency—lower latency means faster payments and a better overall customer experience for businesses. Feature freshness: A measure of the time required to update the value of features. This is important because Stripe needs to react quickly to changes in fraud patterns. For example, if there is an unusual spike in transactions for one business, feature values must quickly be updated to reflect the pattern so models can incorporate the new information in their predictions for other businesses.\n\nThere are trade-offs between latency and feature freshness. For example, we can improve latency at the expense of freshness by performing more precomputation when new data arrives, while we can prioritize freshness over latency by performing more of the feature computation during serving. Stripe’s strict requirements for both low latency and feature freshness across the billions of transactions we process create a unique set of constraints on our feature platform.\n\nShepherd: Stripe’s next-generation ML feature platform\n\nAs Stripe grew, so did our ambitions for applying ML to hard problems. To accelerate our feature engineering work, we evaluated several options, including revamping our existing platform, building from scratch, and implementing proprietary or open-source options. One particularly appealing option was an invitation we received from Airbnb to become early external adopters of Chronon, which Airbnb had developed to power its ML use cases.\n\nAirbnb wanted to integrate the platform with an external partner prior to open sourcing, and Chronon met all of our requirements: an intuitive Python- and SQL-based API, efficient windowed aggregations, support for online and offline computation of features, and built-in consistency monitoring. At the same time, we couldn’t just use it off-the-shelf. We knew we would need to adapt Chronon to Stripe’s unique scale, where training data can include thousands of features and billions of rows. It was going to be a significant engineering challenge, but we were confident that it was a strong foundational building block.\n\nAdapting Chronon\n\nChronon supports batch and streaming features in both online and offline contexts. To be able to use Chronon as the foundation for Shepherd, we needed to make sure the offline, online, and streaming components could all meet Stripe’s scale.\n\nML engineers use Chronon to define their features with a Python- and SQL-based API, and Chronon provides the offline, streaming, and online components to compute and serve the features. Integrating with Chronon involves setting up each of these components and providing an implementation for the key-value (KV) store used to store feature data for serving. When integrating with Chronon, we needed to make sure each of the components could meet our feature freshness and latency requirements.\n\nKV store implementation\n\nThe KV store is responsible for storing data required to serve features. Offline jobs compute and write historical feature data to the store, and streaming jobs write feature updates. To cost-efficiently scale our KV store, we split it into two implementations: a lower-cost store optimized for bulk uploads that is write-once and read-many, and a higher-cost distributed memcache-based store that is optimized for write-many and read-many. With this dual KV store implementation, we lowered the cost of storing and serving data while still meeting our latency and feature freshness requirements.\n\nStreaming jobs\n\nChronon streaming jobs consume event streams and write the events to the KV store. The events can be thought of as updates to features. The default Chronon implementation writes events into the KV store with no preaggregation. Storing individual events into the KV store would not allow us to meet our latency requirements for features with a large number of events. We needed to choose a streaming platform that could achieve low latency updates and allow us to implement a more scalable write pattern.\n\nWe chose Flink as the streaming platform because of its low latency stateful processing. Since the Chronon API is a combination of Python and Spark SQL, maintaining consistency between offline and online computation meant we needed a way to run Spark SQL expressions in Flink. Fortunately, the Spark SQL expressions used in Chronon’s feature definitions only require maps and filters. These are narrow transformations—with no shuffling of data—and can be applied to individual rows.\n\nWe implemented support for Spark SQL expressions applied to Flink rows. With Flink now powering our feature updates, we achieved p99 feature freshness of 150ms.\n\nUntiled Architecture\n\nTiled Architecture\n\nFlink-based streaming architecture allowed us to meet our feature freshness requirements; that left latency targets. To achieve those, we needed to modify how Chronon stores events in the KV store. When events are stored individually, computing features requires retrieving events for the feature and aggregating them together. If there are a large number of events for the feature, this is time-consuming and increases latency.\n\nRather than store individual events, we decided to maintain the state of preaggregated feature values in the Flink app, and periodically flush those values out to the KV store. We call each of these preaggregated values a “tile.” With tiling, computing a feature only requires retrieving and aggregating the tiles for the feature rather than all the individual events. For features with a large number of events, this is a much smaller amount of data and significantly decreases latency. We contributed both the Flink and tiling implementations back to Chronon, along with documentation on how to get started with them.\n\nMeeting Stripe’s offline requirements\n\nThe Chronon offline algorithm produces both offline training data for models and batch-only use cases. Offline jobs are also required to compute historical data used for serving GroupBys. The offline jobs are configured using the same Python- and Spark SQL-based API as the online jobs, allowing developers to define their features once and compute both online and offline features.\n\nStripe’s scale for offline jobs is larger than previous use cases of Chronon, just as it was for streaming and online components. Although the offline algorithm is designed to be robust, with support for handling skewed data, we needed to verify that it would scale to the size of Stripe’s training sets. As a first step to integrating with Chronon’s offline jobs, we performed benchmarks of training dataset generation and found the algorithm to be scalable with predictable tuning knobs.\n\nAfter verifying its scalability, we needed to integrate Chronon’s offline jobs with Stripe’s data orchestration system. We built a custom integration for scheduling and running jobs that worked with our highly customized Airflow setup. We designed the integration so users only need to mark their GroupBys as online or set an offline schedule in their Join definitions, after which the required offline jobs are automatically scheduled.\n\nWe also needed to integrate Chronon with Stripe’s data warehouse. Chronon assumes data sources are all partitioned Hive tables. Not all data sources at Stripe meet these requirements. For example, many of the data sources required for batch features are unpartitioned snapshot tables.\n\nWe built support into our Chronon integration for defining features with a wider variety of data sources, and for writing features to Stripe’s data warehouse using customized Iceberg writers. Fully integrating with our data warehouse provides feature engineers the flexibility to define features using any data source, and to consume features in downstream batch jobs for use cases including model training and batch scoring.\n\nOur implementation for more flexible data source support was Stripe-specific, but we plan to generalize the approach and contribute it to Chronon.\n\nBuilding a SEPA fraud model on Shepherd\n\nOur first use case for Shepherd was a partnership with our Local Payment Methods (LPM) team to create an updated ML model for detecting SEPA fraud. SEPA, which stands for Single Euro Payments Area, enables people and businesses to make cashless euro payments—via credit transfer and direct debit—anywhere in the European Union. The LPM team initially planned on combining new Shepherd-created features with existing features from our legacy feature platform, but found development on Shepherd so easy that they created all new features and launched a Shepherd-only model.\n\nOur new SEPA fraud model consists of over 200 features, including a combination of batch-only and streaming features. As we built the model, we also developed support for modeling delay in the offline training data so we could accurately represent the delay of batch data in training data to avoid training-serving skew—when the feature values that a model is trained on are not reflective of the feature values used to make predictions.\n\nAs part of the new SEPA fraud model, we also built monitoring and alerting for Shepherd—including integrating with Chronon’s online offline consistency monitoring. As we mentioned at the start of this post, the new model blocks tens of millions of dollars of additional fraud a year.\n\nSupporting the Chronon community\n\nAs a co-maintainer of Chronon with Airbnb, we’re excited to grow and support this open-source community while continuing to expand the capabilities of the project. We also designed the new Chronon logo, a subtle nod to the fabric of time.\n\nOver the coming months, we’ll contribute new functionality and additional optimizations to Chronon, and we’ll share more details about how teams at Stripe are adopting Shepherd.\n\nTo get started with Chronon, check out the GitHub repository, read the documentation at Chronon.ai, and drop into our community Discord channel.\n\nAnd if you’re interested in building ML infrastructure at Stripe—or developing ML features for Stripe products—consider joining our engineering team.", "label": "non_personal"}
{"title": "Test clocks: How we made it easier to test Stripe Billing integrations", "url": "https://stripe.com/blog/test-clocks-how-we-made-it-easier-to-test-stripe-billing-integrations", "content": "Stripe Billing allows businesses to manage customer relationships with recurring payments, usage triggers, and other customizable features.\n\nThese are key processes for any business, and for that reason businesses need to validate that their Stripe Billing integrations behave as they expect. But integrations are often error prone due to common misconceptions about time: days always have 24 hours (not when we change the clocks twice a year), February always has 28 days (true, except for leap years), timestamps will always be in the same format (yyyy-mm-dd hh:mm:ss is the default, but it’s not universal), system clocks are always set to the right time, and so on.\n\nBilling integrations are also difficult to test. Historically, the only way to run a test was to wait for time to pass—typically by creating test configurations with shorter subscription cycles than real production systems, or running 10-second trials to force subscriptions to cycle—and then look for any bugs that might surface in the course of normal business. Of course, doing anything that is not a perfect mirror of your production system is a shaky foundation to build on.\n\nWe sought to address these challenges with the launch of test clocks, which allow users to simulate the passage of time in Billing scenarios without waiting for actual seconds to tick by in the real world. A test clock, when associated with Customer objects, allows users to associate a time reference with each Customer and its associated Billing resources. When the test clock runs, the Subscription and Invoice objects will behave as if time has actually passed, changing states and triggering webhooks. With a test clock, users can—for example—perform the leap-year test with just a few API calls or clicks in the Dashboard.\n\nThis blog discusses the technical details of how we built test clocks in Billing, and how we updated Stripe systems to account for the different ways that time passes.\n\nConceptualizing a hybrid logical clock\n\nWe often think about time as we see it on a real-world clock or a calendar—seconds, minutes, days, months, and years pass by in steady succession. We say that time “flows,” much like a boat down a river. But we could also think of time as advancing through a sequence of events, each of which happens to occur at a particular timestamp. This combination of physical timestamps from real-world clocks with ordered and meaningful events from logical clocks creates a hybrid logical clock.\n\nThis hybrid logical clock is based on a concept of time that is useful for any system in a test environment, because it makes it easy to fast-forward to the most important future moments in a billing system. Instead of advancing through 30 days of normal clock time, which requires traversing all the seconds in between the current time and that future time, you can just advance your clock to the next “event”—the next monthly billing date.\n\nRather than flowing down the river, the boat can teleport directly to a meaningful event. As a result, the computational cost of advancing the clock is significantly reduced.\n\nBuilding a hybrid logical clock\n\nOne challenge in building a hybrid logical clock is that it’s difficult to know beforehand the total set of meaningful events, and the order in which they will occur, because any event can trigger state changes.\n\nFor example, in a scenario where customers are on a monthly recurring plan, and the business suddenly decides to credit the first 10 days free, the system needs to be flexible enough to accommodate this change in state. It is equally possible that the business may want to maintain the subscription cycle, or they may want to advance the subscription cycle by 10 days—and change the time for issuing invoices.\n\nThis needs to be taken into account when figuring out the next meaningful event. So when a user advances the time of a test clock, we repeat an “advance” function under the hood:\n\nGiven a test clock, compute the next meaningful time for all of the objects that use it. For example, this might be the next date to Invoice for each Subscription associated with the clock. If the next meaningful time is after the target time , update the frozen time of the test clock to the target time , and stop advancing. If the next meaningful time is before the target time , execute the actions that occur at the next meaningful time and update the frozen time of the test clock to the next meaningful time . Then return to step 1.\n\nIn the above diagram, the test clock starts at 19:00 (the “frozen time”) with a target time of 06:30 the next day. We start by taking action on Event 2, and set the time to 00:00—processing Event 2 also causes the new Event 2A to be scheduled for 01:30, and we process this as a normal event at 01:30. Since we haven’t reached the target time, we go through the loop again, take action on Event 3, and set the time to 03:00. In the final loop, we notice that Event 4 is past the target time, so we set the clock to 06:30, and stop advancing.\n\nSince test clocks affect only an object’s understanding of the “current” time, they execute the same exact logic that would occur in real time for each meaningful action. Test clocks can thus be used to safely and rigorously test integrations—and we use them internally to test all new features on Billing.\n\nUpdating Stripe systems to understand test clocks\n\nWe made a no-op change to our internal logic to remove dependencies on real-world time, and instead retrieve timestamps from an abstract “time provider” backed either by a real-world clock or a test clock. This approach means that there is no semantic change in the presentation of Billing objects in the API, and it allows both developers and internal systems to continue relying on existing business logic without changes in behavior.\n\nAfter we changed the basis for retrieving timestamps, we needed to ensure that time-dependent operations were not triggered by the passage of real time for objects with test clocks attached. To do that, we used a scheduling service that looks for database records meeting certain criteria, and which triggers certain events when it finds them.\n\nConsider the example of generating an Invoice for a Subscription that cycles at the start of the month. When a new month begins, the Subscription gets picked up by a part of the scheduling service that looks for Subscriptions whose billing periods have just ended, and triggers the creation of a new Invoice . Objects with an associated test clock, on the other hand, are explicitly filtered out from any database scans done by the asynchronous scheduling service. Instead, we give the test clock full control over orchestration and scheduling.\n\nUsing test clocks in Billing\n\nWith test clocks, you can confidently validate and deploy your business model in a much shorter time, allowing you to get to market faster. Test clocks allow you to safely and quickly validate integrations and can be used for any combination of scenarios within Billing: recurring subscriptions, trials that convert into paid subscriptions, prorations, renewal-payment failures, past-due subscriptions, timed discounts, subscription schedules, and so on.\n\nTo get started, simply create a test clock through the API and attach customers to it. (You can also work with test clocks in the Dashboard.) Any Billing object created under that customer will then be controlled by the test clock, and you can advance time to observe any effects on Billing objects. For more details, check out our documentation.\n\nAnd if you’re interested in building financial infrastructure at Stripe—including products such as Billing—consider joining our engineering team.", "label": "non_personal"}
{"title": "How Stripe’s document databases supported 99.999% uptime with zero-downtime data migrations", "url": "https://stripe.com/blog/how-stripes-document-databases-supported-99.999-uptime-with-zero-downtime-data-migrations", "content": "In 2023, Stripe processed $1 trillion in total payments volume, all while maintaining an uptime of 99.999%. We obsess over reliability. As engineers on the database infrastructure team, we provide a database-as-a-service (DBaaS) called DocDB as a foundation layer for our APIs.\n\nStripe’s DocDB is an extension of MongoDB Community—a popular open-source database—and consists of a set of services that we built in-house. It serves over five million queries per second from Stripe’s product applications. Our deployment is also highly customized to provide low latency and diverse access, with 10,000+ distinct query shapes over petabytes of important financial data that lives in 5,000+ collections distributed over 2,000+ database shards.\n\nWe chose to build DocDB on top of MongoDB Community because of the flexibility of its document model and its ability to handle massive volumes of real-time data at scale. MongoDB Atlas didn’t exist in 2011, so we built a self-managed cluster of MongoDB instances running in the cloud.\n\nAt the heart of DocDB is the Data Movement Platform. Built originally as a horizontal scaling solution to overcome vertical scaling limits of MongoDB compute and storage, we customized it to serve multiple purposes: merging underutilized database shards for improved utilization and efficiency, upgrading the major version of the database engine in our fleet for reliability, and transitioning databases from a multitenant arrangement to single tenancy for large users.\n\nThe Data Movement Platform enabled our transition from running a small number of database shards (each with tens of terabytes of data) to thousands of database shards (each with a fraction of the original data). It also provides client-transparent migrations with zero downtime, which makes it possible to build a highly elastic DBaaS offering. DocDB can split database shards during traffic surges and consolidate thousands of databases through bin packing when traffic is low.\n\nIn this blog post we’ll share an overview of Stripe’s database infrastructure, and discuss the design and application of the Data Movement Platform.\n\nHow we built our database infrastructure\n\nWhen Stripe launched in 2011, we chose MongoDB as our online database because it offered better developer productivity than standard relational databases. On top of MongoDB, we wanted to operate a robust database infrastructure that prioritized the reliability of our APIs, but we could not find an off-the-shelf DBaaS that met our requirements:\n\nMeeting the highest standards of availability, durability, and performance\n\nExposing a minimal set of database functions to avert self-inflicted issues due to suboptimal queries from client applications\n\nSupporting horizontal scalability with sharding\n\nOffering first-class support for multitenancy with enforced quotas\n\nProviding strong security through enforcement of authorization policies\n\nThe solution was to build DocDB—with MongoDB as the underlying storage engine—a truly elastic and scalable DBaaS, with online data migrations at its core.\n\nProduct applications at Stripe access data in their database through a fleet of database proxy servers, which we developed in-house in Go to enforce concerns of reliability, scalability, admission control, and access control. As a mechanism to horizontally scale, we made the key architectural decision to employ sharding. (If you want to learn more about database sharding, this is a helpful primer.)\n\nThousands of database shards, each housing a small chunk of the cumulative data, now underlie all of Stripe’s products. When an application sends a query to a database proxy server, it parses the query, routes it to one or more shards, combines the results from the shards, and returns them back to the application.\n\nBut how do database proxy servers know which among thousands of shards to route the query to? They rely on a chunk metadata service that maps chunks to database shards, making it easy to look up the relevant shards for a given query. In line with typical database infrastructure stacks, change events resulting from writes to the database are transported to streaming software systems, and eventually archived in an object store via the change data capture (CDC) pipeline.\n\nHigh-level overview of Stripe’s database infrastructure\n\nAt the product application level, teams at Stripe use the in-house document database control plane to provision a logical container for their data—referred to as a logical database—housing one or more DocDB collections, and each comprising documents that have a related purpose. Data in these DocDB collections is distributed across several databases (referred to as physical databases), each of which is home to a small chunk of the collection. Physical databases on DocDB live on shards deployed as replica sets that comprise a primary node and several secondary nodes with replication and automated failover.\n\nA sharded collection\n\nHow we designed the Data Movement Platform\n\nIn order to build a DBaaS offering that is horizontally scalable and highly elastic—one that can scale in and out with the needs of the product applications—we needed the ability to migrate data across database shards in a client-transparent manner with zero downtime. This is a complex distributed systems problem, one that is further compounded by the unique requirements of important financial data:\n\nData consistency and completeness: We need to ensure that the data being migrated remains consistent and complete across both the source and target shards.\n\nWe need to ensure that the data being migrated remains consistent and complete across both the source and target shards. Availability: Prolonged downtime during data migration is unacceptable, as millions of businesses count on Stripe to accept payments from their customers 24 hours a day. Our goal is to keep the key phase of the migration process shorter than the duration of a planned database primary failover—typically lasting a few seconds, and in line with the retry budget of our product applications.\n\nProlonged downtime during data migration is unacceptable, as millions of businesses count on Stripe to accept payments from their customers 24 hours a day. Our goal is to keep the key phase of the migration process shorter than the duration of a planned database primary failover—typically lasting a few seconds, and in line with the retry budget of our product applications. Granularity and adaptability: At Stripe’s scale, we need to support the migration of an arbitrary number of chunks of data from any number of sources to target shards—with no restrictions on the number of in-flight database chunk migrations in the fleet, and no restrictions on the number of migrations any given shard can participate in at any point in time. We also need to accommodate the migration of chunks of varying sizes at a high throughput, as several of our database shards contain terabytes of data.\n\nAt Stripe’s scale, we need to support the migration of an arbitrary number of chunks of data from any number of sources to target shards—with no restrictions on the number of in-flight database chunk migrations in the fleet, and no restrictions on the number of migrations any given shard can participate in at any point in time. We also need to accommodate the migration of chunks of varying sizes at a high throughput, as several of our database shards contain terabytes of data. No performance impact to source shard: When we migrate database chunks across shards, our goal is to preserve the performance and throughput of the source shard to preclude any adverse impact on performance and available throughput for user queries.\n\nTo address these requirements, we built the Data Movement Platform to manage online data migrations across database shards by invoking purpose-built services.\n\nData Movement Platform within our database infrastructure stack\n\nThe Coordinator component in the Data Movement Platform is responsible for orchestrating the various steps involved in online data migrations—it invokes the relevant services to accomplish each of the constituent steps outlined below:\n\nStep 1: Chunk migration registration\n\nFirst we register the intent to migrate database chunks from their source shards to arbitrary target shards in the chunk metadata service. Subsequently, we build indexes on the target shards for the chunks being migrated.\n\nStep 2: Bulk data import\n\nNext, we use a snapshot of the chunks on the source shards at a specific time, denoted as time T, to load the data onto one or more database shards. The service responsible for performing bulk data import accepts various data filters, and only imports the chunks of data that satisfy the filtering criteria. While this step appeared simple at first, we encountered throughput limitations when bulk loading data onto a DocDB shard. Despite attempts to address this by batching writes and adjusting DocDB engine parameters for optimal bulk data ingestion, we had little success.\n\nHowever, we achieved a significant breakthrough when we explored methods to optimize our insertion order, taking advantage of the fact that DocDB arranges its data using a B-tree data structure. By sorting the data based on the most common index attributes in the collections and inserting it in sorted order, we significantly enhanced the proximity of writes—boosting write throughput by 10x.\n\nStep 3: Async replication\n\nOnce we have imported the data onto the target shard, we begin replicating writes starting at time T from the source to the target shard for the database chunks being migrated. Our async replication systems read the mutations resulting from writes on the source shards from the CDC systems and issue writes to the target shards.\n\nThe operations log, or oplog, is a special collection on each DocDB shard that keeps a record of all the operations that mutate data in databases on that shard. We transport the oplog from every DocDB shard to Kafka, an event streaming platform, and then archive it to a cloud object storage service such as Amazon S3. (If you want to learn more about oplog, this is a helpful primer.)\n\nWe built a service to replicate mutations from one or more source DocDB shards to one or more target DocDB shards using the oplog events in Kafka and Amazon S3. We relied on the oplog events from our CDC systems to ensure that we didn’t slow user queries by consuming read throughput that would otherwise be available to user queries on the source shard, and to avoid being constrained by the size of the oplog on the source shard. We designed the service to be resilient to target shard unavailability, and to support starting, pausing, and resuming synchronization from a checkpoint at any point in time. The replication service also exposes the functionality to fetch the replication lag.\n\nMutations of the chunks under migration get replicated bidirectionally—from the source shards to the target shards and vice versa—and the replication service tags the writes it issues to avert cyclical asynchronous replication. We made this design choice to provide the flexibility to revert traffic to the source shards if any issues emerge when directing traffic to the target shards.\n\nStep 4: Correctness check\n\nAfter the replication syncs between the source and target shard, we conduct a comprehensive check for data completeness and correctness by comparing point-in-time snapshots—a deliberate design choice we made in order to avoid impacting shard throughput.\n\nStep 5: Traffic switch\n\nOnce the data in a chunk is imported from the source to the target shard—and mutations are actively replicated—a traffic switch is orchestrated by the Coordinator. In order to reroute reads and writes to the chunk of data being migrated, we need to first: stop the traffic on the source shard for a brief period of time, update the routes in the chunk metadata service, and have the proxy servers redirect reads and writes to the target shards.\n\nThe traffic switch protocol is based on the idea of versioned gating. In steady state, each proxy server annotates requests to DocDB shards with a version token number. We added a custom patch to MongoDB that allows a shard to enforce that the version token number it receives on requests from the proxy servers is newer than the version token number it knows of—and only serve requests that satisfy this criterion. To update the route for a chunk, we use the Coordinator to execute the following steps:\n\nFirst, we bump up the version token number on the source DocDB shard. The version token number is stored in a document in a special collection in DocDB, and all reads and writes on the chunk on the source shard are rejected at this point.\n\nThen, we wait for the replication service to replicate any outstanding writes on the source shard.\n\nLastly, we update the route for the chunk to point to the target shard and the version token number in the chunk metadata service.\n\nTraffic switch process\n\nUpon completion, the proxy servers fetch the updated routes for the chunk and the most up-to-date version token number from the chunk metadata service. Using the updated routes for the chunk, the proxy servers route reads and writes for the chunk to the target shard. The entire traffic switch protocol takes less than two seconds to execute, and all failed reads and writes directed to the source shard succeed on retries.\n\nStep 6: Chunk migration deregistration\n\nFinally, we conclude the migration process by marking the migration as complete in the chunk metadata service and subsequently dropping the chunk data from the source shard.\n\nApplications of the Data Movement Platform\n\nThe ability to migrate chunks of data across DocDB shards in an online manner helps us horizontally scale our database infrastructure to keep pace with the growth of Stripe. Engineers on the database infrastructure team are able to split DocDB shards for size and throughput with a click of a button, freeing up database storage and throughput headroom for product teams.\n\nIn 2023, we used the Data Movement Platform to improve the utilization of our database infrastructure. Concretely, we bin-packed thousands of underutilized databases by migrating 1.5 petabytes of data transparent to product applications, and reduced the total number of underlying DocDB shards by approximately three quarters. We also used the Data Movement Platform to upgrade our database infrastructure fleet by fork-lifting data to a later version of MongoDB in one step—without going through intermediate major and minor versions with an in-place upgrade strategy.\n\nThe database infrastructure team at Stripe is focused on building a robust and reliable foundation that scales with the growth of the internet economy. We are currently prototyping a heat management system that proactively balances data across shards based on size and throughput, and investing in shard autoscaling that dynamically responds to changes in traffic patterns.\n\nAt Stripe, we’re excited to solve hard distributed systems problems. If you are too, consider joining our engineering team.", "label": "non_personal"}
{"title": "An Insider’s Tips for Taking the Certified Backstage Associate (CBA) Exam", "url": "https://engineering.atspotify.com/2025/3/certified-backstage-associate-exam-tips", "content": "TL;DR There’s a brand new engineering certification in town: Certified Backstage Associate (CBA). Offered through The Linux Foundation, the certification shows that you have the skills and knowledge to build and manage Backstage — the open framework for internal developer portals (IDPs) that was developed at Spotify and is now used at thousands of companies around the world. If you’re interested in building your engineering career — or just want to know more about what makes Backstage such a popular platform for developer experience — here’s everything I learned from taking the CBA exam.\n\nYou’ll learn:\n\nThe value of becoming a Certified Backstage Associate\n\nHow to prepare for the proctored exam (Don’t read the questions out loud to yourself!🤫)\n\nThe four main subject areas covered by the exam itself\n\nLet’s dive in!\n\nWhy get certified in Backstage?\n\nBackstage is the framework for building internal developer portals that Spotify developed internally before open sourcing it in 2020 and then donating it to the Cloud Native Computing Foundation (CNCF) — which is also home to other great technologies, like Argo, Crossplane, Envoy, and the granddaddy of cloud native, Kubernetes.\n\nBackstage continues to prove itself as the tool of choice for improving developer experience and developer productivity for companies of all shapes and sizes — from Adobe and American Airlines, to Expedia Group, H&M, and Toyota, to startups, scale-ups, 100-year-old financial institutions, government consulting agencies, healthcare companies, and more. You’ll find the need for Backstage experts nearly everywhere you find developer teams working together, whether that’s 50 engineers or thousands of them, like at Spotify.\n\nAll that is to say, in the last five years, Backstage has grown from Spotify’s homegrown developer portal to one of the most popular platform engineering tools today. This growth means more companies than ever are looking for people with the expertise to stand up, maintain, and build out a Backstage IDP of their own within their engineering orgs.\n\nWho the CBA is for\n\nThe Linux Foundation launched the new certification program for becoming a Certified Backstage Associate last November. The program is intended for developers who have 3–6 months experience using Backstage. The CBA is ideal for site reliability engineers (SREs), DevOps engineers, platform engineers, and other software developers who want to demonstrate their Backstage proficiency. As a Certified Backstage Associate, companies will know that you have the expertise to set up, deploy, and manage a Backstage project either independently or as a team lead.\n\nWhy listen to this guy?\n\nMaybe you’re already thinking: “Wow, the CBA sounds pretty great!” Or: “I already use Backstage a lot and would love to showcase my skills.” And: “I’d love to know more about what the certification process looks like and how I should prepare for the exam.” But, also: “Who the heck is the author and why should I trust him?”\n\nWell, I’m André, aka @awanlin on GitHub. Hello! 👋 I’m an engineer on Spotify’s Backstage team, but I’ve been working with Backstage since early 2021 before I worked at Spotify. You might recognize me from answering questions in the community Discord server, as a maintainer of the Community Plugins repository, as the lead for the Documentation Special Interest Group (SIG), or as the contributor of several Backstage plugins. My early contributions to the open source project and broad Backstage knowledge are actually what led me to joining Spotify!\n\nCurrently, as a customer success engineer, I spend my days working with Spotify’s enterprise customers, making sure they’re getting the most value out of their Backstage IDP — including with Spotify Portal for Backstage, the SaaS version of Backstage that Spotify recently made available as a beta. Every day, I work with Backstage experts at other companies, helping to problem-solve technical issues and optimize developer experience through Portal, so that their devs can focus on building great features and products instead of dealing with infrastructure complexity. OK, that’s plenty about me. Onto the exam!\n\nPreparing for the proctored exam\n\nIn order to become a Certified Backstage Associate, you’ll need to complete a 90-minute, proctored, multiple-choice exam. A proctored exam means someone will be monitoring you while you are taking it. This person will not suddenly show up in your home and stand over your shoulder. But they will be actively monitoring you via webcam — and, trust me, they’re paying attention! How do I know?\n\nWell, while I was taking the CBA I ran into a challenging question. As I often do when I’m thinking through a problem, I reread the question out loud to help me better understand it. Unfortunately, the proctor thought I might be asking someone offscreen for help, and gave me a stern warning. I made sure to keep my thoughts in my head from that point on.\n\nNow, for those who are also new to taking a proctored exam remotely, let me share some other tips based on my experience, as I had never taken one until I took the CBA. Some of these may sound pretty basic, but if you’re taking the exam from home, the controlled conditions of the exam might feel a little strange there. So it’s best to do a little bit of prep ahead of time to help you get used to your new exam environment.\n\nDon’t wait until the last-minute. Make sure to give yourself plenty of time before the exam to get set up and settled, so you can avoid any surprises. Also, one less thing to worry about!\n\nCreate a zen space for yourself. Take your exam in a quiet room with an empty desk that is free of distractions. (And let family/housemates know not to disturb. Maybe put a sign on the door.) Before the exam, the proctor will ask you to pan your room with the webcam, so they can “verify that the workspace is clear of any disallowed materials”.\n\nTest the exam tools ahead of time. The day before the exam, take the time to test out your system with the tools that will be used for the exam. The exam organizers will send you detailed instructions about this well beforehand, so you should have plenty of time to familiarize yourself with the system requirements.\n\nH2O is your friend. You can have water, so make sure you have some handy (note: only “clear liquids in a clear container”). You’ll be happy you did!\n\nWith the process for the CBA exam understood, you’ll want to make sure to study and brush up on your Backstage skills. The following is a rundown of what you should know heading into the exam and links to documentation worth reading to help with that.\n\nStudy guide: What is and isn’t in the exam\n\nBackstage is a code-first framework for building an internal developer portal (IDP). To support all the various integrations you might need, it uses a plugin system. This flexibility and extensibility makes Backstage extremely powerful and allows you to quickly build a full-featured IDP that fits your org’s particular processes, practices, and tech stack. It also means that for an exam on the topic, there will be a very broad number of areas to study.\n\nThree subject areas you don’t need to know for the exam\n\nBut before we get into what will be in the exam, here are a few things that won’t be.\n\n🚫 The New Backend System (and other newer features). It’s worth mentioning that the CBA was written before what is known as the New Backend System became the default method for the backend and backend plugins — so you won’t need to study this. You also won’t be expected to know about alpha features, such as the New Frontend System and Canon, the new design library.\n\n🚫 How to build custom plugins. While a CBA should be capable of installing Backstage plugins — which involves working with and customizing TypeScript — developing custom plugins falls outside the scope of a minimally qualified CBA.\n\n🚫 The nitty-gritty of Yarn and Node.js. Regarding Yarn and Node, the exam won’t cover specifics about either of these — for example, you won’t be asked about the different features of Yarn Classic and Yarn 4. But you will have to know that Yarn is the default package manager used in Backstage and how to use it to add packages. Similarly, specific features of various Node versions will also not be covered, but knowing Backstage’s Node Version Policy would be.\n\nFour subject areas you do need to know for the exam\n\nThe four subject areas below focus on the technical implementation and features of the Backstage framework. But it’s also good to keep in mind how those technology details relate to the principles underlying the overall goals of Backstage — i.e., how the architecture and design of the framework enables anyone to build a customizable, centralized, scalable platform for collaborative software development — or, as we like to say, “a single pane of glass” for all your software development processes and infrastructure.\n\n✅ Backstage Development Workflow\n\nBackstage offers a lot of nice features that make the local development experience smooth and easy. I would go over the various aspects of your local development workflow to start:\n\nBe able to build and run Backstage locally\n\nKnow the various Backstage CLI commands and their uses\n\nUnderstand the use of TypeScript in Backstage and how to generate the typed code\n\nHave a familiarity with how to install Backstage plugins with Yarn\n\nKnow how to build a Docker container of a Backstage instance\n\n✅ Backstage Infrastructure\n\nAfter going over local development, I would work on:\n\nUnderstanding that Backstage is a framework and what its various aspects consist of\n\nKnow the configuration system used by Backstage, including use of secrets and includes\n\nBe aware of the various deployment options for Backstage\n\nHave a familiarity with the overall client-server architecture of Backstage\n\n✅ Backstage Catalog\n\nThe Catalog is the heart of Backstage — and as such, there’s a good chunk of the CBA exam that covers it. You’ll want to go over the following topics:\n\n✅ Customizing Backstage\n\nSome consider customization one of Backstage’s most powerful, yet least talked about features. Here’s some of the topics in this broad feature that I would suggest keeping in mind:\n\nKnowing the difference between frontend and backend plugins\n\nUnderstanding the general customization options for plugins and core features\n\nHow to make changes to React code and where those changes would be made\n\nUsing Material UI components — where to get them from and how to apply them\n\nOne more thing: Begin with the basics\n\nTo lead off your studying, I would take the time to refresh yourself on the basics, in addition to the subject areas above. Walk through the initial Getting Started guide and create a new Backstage instance. It might have been a long time since you’ve done this yourself and aspects of this will likely be on the exam. I would also re-familiarize yourself with the Backstage prerequisites, like Yarn and Node (see above for what will/won’t be covered). These are tools you might take for granted in your day to day — grounding yourself in them will give you a solid foundation for the rest of the exam.\n\nAnd with that, you should be able to comfortably take the Certified Backstage Associate exam! Remember, if you don’t pass the first time, you get a second chance. So, just do your best — and good luck!\n\nLet me know how you did!\n\nOnce you complete your exam, feel free to share your experience and your certification with me on LinkedIn. I would love to hear about your journey to becoming a Certified Backstage Associate! I’ll make sure to update this post with any new tips I hear!", "label": "non_personal"}
{"title": "Scaling Real-Time SignalR Applications on Heroku", "url": "https://www.heroku.com/blog/scaling-real-time-signalr-applications-on-heroku/", "content": "SignalR makes it easy to add real-time functionality to .NET web applications—things like live chat, instant notifications, or interactive dashboards. But what happens when your app starts to grow? A single server can only take you so far. At some point, you’ll need to scale out.\n\nIn this post, we’ll walk through what it takes to scale a SignalR app to run across multiple servers. We’ll start with the basics, then show you how to use Redis as a backplane and enable sticky sessions to keep WebSocket connections stable. And we’ll deploy it all to Heroku. If you’re curious about what it takes to run a real-time app across multiple dynos, this guide is for you.\n\nIntroduction to our app\n\nFor my demo application, I started with Microsoft’s tutorial project on building a real-time application using SignalR, found here. Because we’re focusing on how to scale a SignalR application, we won’t spend too much time covering how to build the original application.\n\nYou can access the code used for this demo in our GitHub repository. I’ll briefly highlight a few pieces.\n\nI used .NET 9.0 ( 9.0.203 at the time of writing). To start, I created a new web application:\n\n~$ dotnet new webapp -o SignalRChat The template \"ASP.NET Core Web App (Razor Pages)\" was created successfully. This template contains technologies from parties other than Microsoft, see https://aka.ms/aspnetcore/9.0-third-party-notices for details. Processing post-creation actions... Restoring /home/user/SignalRChat/SignalRChat.csproj: Restore succeeded\n\nThen, I installed LibMan to get the JavaScript client library for our SignalR project.\n\n~/SignalRChat$ dotnet tool install -g Microsoft.Web.LibraryManager.Cli ~/SignalRChat$ libman install @microsoft/signalr@latest \\ -p unpkg \\ -d wwwroot/js/signalr \\ --files dist/browser/signalr.js\n\nWith my dependencies in place, I created the following files:\n\nhubs/ChatHub.cs : The hub class that serves as a high-level pipeline and handles client-server communication.\n\n: The hub class that serves as a high-level pipeline and handles client-server communication. Pages/Index.cshtml : The main Razor file, combining HTML and embedded C# with Razor syntax.\n\n: The main Razor file, combining HTML and embedded C# with Razor syntax. wwwroot/js/chat.js : The chat logic for the application.\n\nLastly, I had the main application code in Program.cs :\n\nusing SignalRChat.Hubs; var builder = WebApplication.CreateBuilder(args); // Add services to the container. builder.Services.AddRazorPages(); builder.Services.AddSignalR(); var app = builder.Build(); // Configure the HTTP request pipeline. if (!app.Environment.IsDevelopment()) { app.UseExceptionHandler(\"/Error\"); // The default HSTS value is 30 days. You may want to change this for production scenarios, see https://aka.ms/aspnetcore-hsts. app.UseHsts(); } app.UseHttpsRedirection(); app.UseStaticFiles(); app.UseRouting(); app.UseAuthorization(); app.MapRazorPages(); app.MapHub(\"/chatHub\"); app.Run();\n\nYou’ll notice in this initial version that I’ve added SignalR, but I haven’t configured it to use a Redis backplane yet. We’ll iterate and get there soon.\n\nFor a sanity check, I tested my application.\n\n~/SignalRChat$ dotnet build Restore complete (0.2s) SignalRChat succeeded (3.1s) → bin/Debug/net9.0/SignalRChat.dll Build succeeded in 3.7s ~/SignalRChat$ dotnet run Using launch settings from /home/user/SignalRChat/Properties/launchSettings.json... Building... info: Microsoft.Hosting.Lifetime[14] Now listening on: http://localhost:5028 info: Microsoft.Hosting.Lifetime[0] Application started. Press Ctrl+C to shut down. info: Microsoft.Hosting.Lifetime[0] Hosting environment: Development info: Microsoft.Hosting.Lifetime[0] Content root path: /home/user/SignalRChat\n\nIn one browser, I navigated to http://localhost:5028 . Then, with a different browser, I navigated to the same page.\n\nI verified that both browsers had WebSocket connections to my running application, and I posted a message from each browser.\n\nIn real time, the messages posted in one browser were displayed in the other. My app was up and running.\n\nNow, it was time to scale.\n\nHow To scale SignalR\n\nScaling a SignalR app isn’t as simple as just adding more servers. Out of the box, each server maintains its own list of connected clients. That means if a user is connected to server A, and a message is sent through server B, that user won’t receive it—unless there’s a mechanism to synchronize messages across all servers. This is where scaling gets tricky.\n\nTo pull this off, you need two things:\n\nBackplane : The backplane handles message coordination between servers. It ensures that when one instance of your app sends a message, all other instances relay that message to their connected clients. Redis is commonly used for this purpose because it’s fast, lightweight, and supported natively by SignalR.\n\n: The backplane handles message coordination between servers. It ensures that when one instance of your app sends a message, all other instances relay that message to their connected clients. Redis is commonly used for this purpose because it’s fast, lightweight, and supported natively by SignalR. Sticky sessions: WebSockets are long-lived connections, and if your app is spread across multiple servers, you can’t have a user’s connection bouncing between them. Sticky sessions make sure all of a user’s requests are routed to the same server, which keeps WebSocket connections stable and prevents dropped connections during scale-out.\n\nBy combining these two techniques, you set your SignalR app up to handle real-time communication at scale. Let’s walk through how I did this.\n\nUsing Redis as a backplane\n\nThe first task in scaling up meant modifying my application to use Redis as a backplane. First, I added the StackExchange.Redis package for .NET.\n\n~/SignalRChat$ dotnet add package \\ Microsoft.AspNetCore.SignalR.StackExchangeRedis\n\nThen, I modified Program.cs , replacing the original builder.Services.AddSignalR(); line with the following:\n\nvar redisUrl = Environment.GetEnvironmentVariable(\"REDIS_URL\") ?? \"localhost:6379\"; if (redisUrl == \"localhost:6379\") { builder.Services.AddSignalR().AddStackExchangeRedis(redisUrl, options => { options.Configuration.ChannelPrefix = RedisChannel.Literal(\"SignalRChat\"); options.Configuration.Ssl = redisUrl.StartsWith(\"rediss://\"); options.Configuration.AbortOnConnectFail = false; }); } else { var uri = new Uri(redisUrl); var userInfoParts = uri.UserInfo.Split(':'); if (userInfoParts.Length != 2) { throw new InvalidOperationException(\"REDIS_URL is not in the expected format ('redis://user:password@host:port')\"); } var configurationOptions = new ConfigurationOptions { EndPoints = { { uri.Host, uri.Port } }, Password = userInfoParts[1], Ssl = true, }; configurationOptions.CertificateValidation += (sender, cert, chain, errors) => true; builder.Services.AddSignalR(options => { options.ClientTimeoutInterval = TimeSpan.FromSeconds(60); // default is 30 options.KeepAliveInterval = TimeSpan.FromSeconds(15); // default is 15 }).AddStackExchangeRedis(redisUrl, options => { options.Configuration = configurationOptions; }); }\n\nThe above code configures the SignalR application to use Redis, connecting via a default address ( localhost:6379 ) or through a connection string in the environment variable, REDIS_URL . Using REDIS_URL is an example of me thinking ahead, as I plan to deploy this application to Heroku with the Heroku Key-Value Store add-on.\n\nFor how to set up the Redis connection between my .NET application and my Heroku Key-Value Store add-on, I took my cues from here.\n\nWith Program.cs modified to use Redis as a backplane, I tested my application locally again.\n\n~/SignalRChat$ dotnet run\n\nThis time, with my two browser windows open, I also opened a terminal and connected to my local Redis instance, running on port 6379 . I listed the Pub/Sub channels and then subscribed to the main ChatHub channel.\n\n127.0.0.1:6379> pubsub channels 1) \"SignalRChat__Booksleeve_MasterChanged\" 2) \"SignalRChatSignalRChat.Hubs.ChatHub:internal:ack:demo_b3204c22a84c9\" 3) \"SignalRChatSignalRChat.Hubs.ChatHub:internal:return:demo_b3204c22a84c9\" 4) \"SignalRChatSignalRChat.Hubs.ChatHub:all\" 5) \"SignalRChatSignalRChat.Hubs.ChatHub:internal:groups\" 127.0.0.1:6379> subscribe SignalRChatSignalRChat.Hubs.ChatHub:all Reading messages... (press Ctrl-C to quit) 1) \"subscribe\" 2) \"SignalRChatSignalRChat.Hubs.ChatHub:all\" 3) (integer) 1\n\nIn one browser, I sent a message. Then, in the other, I sent a reply. Here’s what came across in my Redis CLI:\n\n1) \"message\" 2) \"SignalRChatSignalRChat.Hubs.ChatHub:all\" 3) \"\\x92\\x90\\x81\\xa4json\\xc4W{\\\"type\\\":1,\\\"target\\\":\\\"ReceiveMessage\\\",\\\"arguments\\\":[\\\"Chrome User\\\",\\\"This is my message.\\\"]}\\x1e\" 1) \"message\" 2) \"SignalRChatSignalRChat.Hubs.ChatHub:all\" 3) \"\\x92\\x90\\x81\\xa4json\\xc4Y{\\\"type\\\":1,\\\"target\\\":\\\"ReceiveMessage\\\",\\\"arguments\\\":[\\\"Firefox User\\\",\\\"And this is a reply.\\\"]}\\x1e\"\n\nI successfully verified that my SignalR application was using Redis as its backplane. Scaling task one of two was complete!\n\nMoving onto sticky sessions, I would need to scale. For that, I needed to deploy to Heroku.\n\nDeploying to Heroku\n\nDeploying my Redis-backed application to Heroku was straightforward. Here were the steps:\n\nStep #1: Login\n\n~/SignalRChat$ heroku login\n\nStep #2: Create app\n\n~/SignalRChat$ heroku create signalr-chat-demo Creating ⬢ signalr-chat-demo... done https://signalr-chat-demo-b49ac4212f6d.herokuapp.com/ | https://git.heroku.com/signalr-chat-demo.git\n\nStep #3: Add the Heroku Key-Value Store add-on\n\n~/SignalRChat$ heroku addons:add heroku-redis Creating heroku-redis on ⬢ signalr-chat-demo... ~$0.004/hour (max $3/month) Your add-on should be available in a few minutes. ! WARNING: Data stored in essential plans on Heroku Redis are not persisted. redis-solid-16630 is being created in the background. The app will restart when complete... Use heroku addons:info redis-solid-16630 to check creation progress Use heroku addons:docs heroku-redis to view documentation\n\nI waited a few minutes for Heroku to create my add-on. After this was completed, I had access to REDIS_URL .\n\n~/SignalRChat$ heroku config === signalr-chat-demo Config Vars REDIS_URL: rediss://:pcbcd9558e402ff2615a4484ac5ca9ac373f811e53bcb17f81ada3c243f8a11cc@ec2-52-20-254-181.compute-1.amazonaws.com:8150\n\nStep #4: Add a Procfile\n\nNext, I added a file called Procfile to my root project folder. The Procfile tells Heroku how to start up my app. It has one line:\n\nweb: cd bin/publish; ./SignalRChat --urls http://*:$PORT\n\nStep #5: Push code to Heroku\n\n~/SignalRChat$ git push heroku main … remote: -----> Building on the Heroku-24 stack remote: -----> Using buildpack: heroku/dotnet remote: -----> .NET app detected remote: -----> SDK version detection remote: Detected .NET project: `/tmp/build_ad246347/SignalRChat.csproj` remote: Inferring version requirement from `/tmp/build_ad246347/SignalRChat.csproj` remote: Detected version requirement: `^9.0` remote: Resolved .NET SDK version `9.0.203` (linux-amd64) remote: -----> SDK installation remote: Downloading SDK from https://builds.dotnet.microsoft.com/dotnet/Sdk/9.0.203/dotnet-sdk-9.0.203-linux-x64.tar.gz ... (0.7s) remote: Verifying SDK checksum remote: Installing SDK remote: -----> Publish app … remote: -----> Launching... remote: Released v4 remote: https://signalr-chat-demo-b49ac4212f6d.herokuapp.com/ deployed to Heroku remote: remote: Verifying deploy... done.\n\nStep #6: Test Heroku app\n\nIn my two browser windows, I navigated to my Heroku app URL (in my case, https://signalr-chat-demo-b49ac4212f6d.herokuapp.com/ ) and tested sending messages to the chat.\n\nI also had a terminal window open, connecting to my Heroku Key-Value Store add-on via heroku redis:cli . Just like I did when testing locally, I subscribed to the main chat channel. As I sent messages, they came across in Redis.\n\nredis:8150> subscribe SignalRChat.Hubs.ChatHub:all 1) subscribe 2) SignalRChat.Hubs.ChatHub:all 3) 2 redis:8150> 1) message 2) SignalRChat.Hubs.ChatHub:all 3) ''''json'R{\"type\":1,\"target\":\"ReceiveMessage\",\"arguments\":[\"Chrome User\",\"I'm on Heroku!\"]} redis:8150> 1) message 2) SignalRChat.Hubs.ChatHub:all 3) ''''json'M{\"type\":1,\"target\":\"ReceiveMessage\",\"arguments\":[\"Firefox User\",\"So am I!\"]}\n\nAs another sanity check, I looked in my developer tools console in my browser. Looking in the Network Inspector, I saw a stable WebSocket connection ( wss:// ) as well as the inbound and outbound connection data.\n\nI had successfully deployed to Heroku, using Redis as my backplane. I hadn’t scaled up to multiple dynos just yet, but everything was looking smooth so far.\n\nScaling with Multiple Dynos\n\nNext, I needed to scale up to use multiple dynos. With Heroku, this is simple. However, you can’t scale up with Eco or Basic dynos. So, I needed to change my dyno type to the next level up: standard-1x .\n\n~/SignalRChat$ heroku ps:type web=standard-1x Scaling dynos on signalr-chat-demo... done === Process Types Type Size Qty Cost/hour Max cost/month ──── ─────────── ─── ───────── ────────────── web Standard-1X 1 ~$0.035 $25 === Dyno Totals Type Total ─────────── ───── Standard-1X 1\n\nWith my dyno type set, I could scale up to use multiple dynos. I went with three.\n\n~/SignalRChat$ heroku ps:scale web=3 Scaling dynos... done, now running web at 3:Standard-1X\n\nMaintaining WebSocket Connections with Sticky Sessions\n\nI reloaded the application in my browser. Now, my inspector console showed an issue:\n\nHere’s the error:\n\nError: Failed to start the transport 'WebSockets': Error: WebSocket failed to connect. The connection could not be found on the server, either the endpoint may not be a SignalR endpoint, the connection ID is not present on the server, or there is a proxy blocking WebSockets. If you have multiple servers check that sticky sessions are enabled.\n\nThat’s a pretty helpful error message. Just as we had expected, our real-time SignalR application would run into issues once we scaled up to multiple dynos. What was the solution? Sticky sessions with Heroku’s session affinity feature.\n\nEnabling Heroku session affinity\n\nThis feature from Heroku works to keep all HTTP requests coming from a client consistently routed to a single dyno. It’s easy to set up, and it would solve our multi-dyno WebSocket connection issue.\n\n~/SignalRChat$ heroku features:enable http-session-affinity Enabling http-session-affinity for ⬢ signalr-chat-demo... done\n\nThat was it. With sticky sessions enabled, I was ready to test again.\n\nTesting with sticky sessions on multiple dynos\n\nI reloaded the application in both browsers. This time, my network inspector showed no errors. It looked like I had a stable WebSocket connection.\n\nReal-time chat messages were sent and received without any problems.\n\nSuccess!\n\nWrapping Up\n\nWith Redis as a backplane and sticky sessions enabled, our SignalR app scaled seamlessly across multiple dynos on Heroku. It delivered real-time messages smoothly, and the WebSocket connections remained stable even under a scaled-out setup.\n\nThe takeaway? You don’t need a complicated setup to scale SignalR, just the right combination of tooling and configuration. Whether you’re building chat apps, live dashboards, or collaborative tools, you now have a tested approach to scale real-time experiences with confidence.\n\nReady to build and deploy your own scalable SignalR application? Check out the .NET Getting Started guide for foundational knowledge. For a visual walkthrough of deploying .NET applications to Heroku, watch our Deploying .NET Applications on Heroku video.", "label": "non_personal"}
{"title": "How AI code generation works", "url": "https://github.blog/ai-and-ml/generative-ai/how-ai-code-generation-works/", "content": "Generative AI coding tools are changing software production for enterprises. Not just for their code generation abilities—from vulnerability detection and facilitating comprehension of unfamiliar codebases, to streamlining documentation and pull request descriptions, they’re fundamentally reshaping how developers approach application infrastructure, deployment, and their own work experience.\n\nWe’re now witnessing a significant turning point. As AI models get better, refusing adoption would be like “asking an office worker to use a typewriter instead of a computer,” says Albert Ziegler, principal researcher and member of the GitHub Next research and development team.\n\nIn this post, we’ll dive into the inner workings of AI code generation, exploring how it functions, its capabilities and benefits, and how developers can use it to enhance their development experience while propelling your enterprise forward in today’s competitive landscape.\n\nHow to use AI to generate code\n\nAI code generation refers to full or partial lines of code that are generated by machines instead of human developers. This emerging technology leverages advanced machine learning models, particularly large language models (LLMs), to understand and replicate the syntax, patterns, and paradigms found in human-generated code.\n\nThe AI models powering these tools, like ChatGPT and GitHub Copilot, are trained on natural language text and source code from publicly available sources that include a diverse range of code examples. This training enables them to understand the nuances of various programming languages, coding styles, and common practices. As a result, the AI can generate code suggestions that are syntactically correct and contextually relevant based on input from developers.\n\nFavored by 55% of developers, our AI-powered pair programmer, GitHub Copilot, provides contextualized coding assistance based on your organization’s codebase across dozens of programming languages, and targets developers of all experience levels. With GitHub Copilot, developers can use AI to generate code in three ways:\n\n1. Type code and AI can autocomplete the code\n\nAutocompletions are the earliest version of AI code generation. John Berryman, a senior researcher of ML on the GitHub Copilot team, explains the user experience: “I’ll be writing code and taking a pause to think. While I’m doing that, the agent itself is also thinking, looking at surrounding code and content in neighboring tabs. Then it pops up on the screen as gray ‘ghost text’ that I can reject, partially accept, or fully accept and then, if necessary, modify.”\n\nWhile every developer can reap the benefits of using AI coding tools, experienced programmers can often feel these gains even more so. “In many cases, especially for experienced programmers in a familiar environment, this suggestion speeds us up. I would have written the same thing. It’s just faster to hit ‘tab’ (thus accepting the suggestion) than it is to write out those 20 characters by myself,” says Johan Rosenkilde, principal researcher for GitHub Next.\n\nWhether developers are new or highly skilled, they’ll often have to work in less familiar languages, and code completion suggestions using GitHub Copilot can lend a helping hand. “Using GitHub Copilot for code completion has really helped speed up my learning experience,” says Berryman. “I will often accept the suggestion because it’s something I wouldn’t have written on my own since I don’t know the syntax.”\n\nUsing an AI coding tool has become an invaluable skill in itself. Why? Because the more developers practice coding with these tools, the faster they’ll get at using them.\n\nFor experienced developers in unfamiliar environments, tools like GitHub Copilot can even help jog their memories.\n\nLet’s say a developer imports a new type of library they haven’t used before, or that they don’t remember. Maybe they’re looking to figure out the standard library function or the order of the argument. In these cases, it can be helpful to make GitHub Copilot more explicitly aware of where the developer wants to go by writing a comment.\n\n“It’s quite likely that the developer might not remember the formula, but they can recognize the formula, and GitHub Copilot can remember it by being prompted,” says Rosenkilde. This is where natural language commentary comes into play: it can be a shortcut for explaining intent when the developer is struggling with the first few characters of code that they need.\n\nIf developers give specific names to their functions and variables, and write documentation, they can get better suggestions, too. That’s because GitHub Copilot can read the variable names and use them as an indicator for what that function should do.\n\nSuddenly that changes how developers write code for the better, because code with good variable and function names are more maintainable. And oftentimes the main job of a programmer is to maintain code, not write it from scratch.\n\n“When you push that code, someone is going to review it, and they will likely have a better time reviewing that code if it’s well named, if there’s even a hint of documentation in it, and so on,” says Rosenkilde. In this sense, the symbiotic relationship between the developer and the AI coding tool is not just beneficial for the developer, but for the entire team.\n\n3. Chat directly with AI\n\nWith AI chatbots, code generation can be more interactive. GitHub Copilot Chat, for example, allows developers to interact with code by asking it to explain code, improve syntax, provide ideas, generate tests, and modify existing code—making it a versatile ally in managing coding tasks.\n\nRosenkilde uses the different functionalities of GitHub Copilot:\n\n“When I want to do something and I can’t remember how to do it, I type the first few letters of it, and then I wait to see if Copilot can guess what I’m doing,” he says. “If that doesn’t work, maybe I delete those characters and I write a one liner in commentary and see whether Copilot can guess the next line. If that doesn’t work, then I go to Copilot Chat and explain in more detail what I want done.”\n\nTypically, Copilot Chat returns with something much more verbose and complete than what you get from GitHub Copilot code completion. “Namely, it describes back to you what it is you want done and how it can be accomplished. It gives you code examples, and you can respond and say, oh, I see where you’re going. But actually I meant it like this instead,” says Rosenkilde.\n\nBut using AI chatbots doesn’t mean developers should be hands off. Mistakes in reasoning could lead the AI down a path of further mistakes if left unchecked. Berryman recommends that users should interact with the chat assistant in much the same way that you would when pair programming with a human. “Go back and forth with it. Tell the assistant about the task you are working on, ask it for ideas, have it help you write code, and critique and redirect the assistant’s work in order to keep it on the right track.”\n\nThe importance of code reviews\n\nGitHub Copilot is designed to empower developers to execute their ideas. As long as there is some context for it to draw on, it will likely generate the type of code the developer wants. But this doesn’t replace code reviews between developers.\n\nCode reviews play an important role in maintaining code quality and reliability in software projects, regardless of whether AI coding tools are involved. In fact, the earlier developers can spot bugs in the code development process, the cheaper it is by orders of magnitude.\n\nOrdinary verification would be: does the code parse? Do the tests work? With AI code generation, Ziegler explains that developers should, “Scrutinize it in enough detail so that you can be sure the generated code is correct and bug-free. Because if you use tools like that in the wrong way and just accept everything, then the bugs that you introduce are going to cost you more time than you save.”\n\nRosenkilde adds, “A review with another human being is not the same as that, right? It’s a conversation between two developers about whether this change fits into the kind of software they’re building in this organization. GitHub Copilot doesn’t replace that.”\n\nThe advantages of using AI to generate code\n\nWhen developer teams use AI coding tools across the software development cycle, they experience a host of benefits, including:\n\nFaster development, more productivity\n\nAI code generation can significantly speed up the development process by automating repetitive and time-consuming tasks. This means that developers can focus on high-level architecture and problem-solving. In fact, 88% of developers reported feeling more productive when using GitHub Copilot.\n\nRosenkilde reflects on his own experience with GitHub’s AI pair programmer: “95% of the time, Copilot brings me joy and makes my day a little bit easier. And this doesn’t change the code I would have written. It doesn’t change the way I would have written it. It doesn’t change the design of my code. All it does is it makes me faster at writing that same code.” And Rosenkilde isn’t alone: 60% of developers feel more fulfilled with their jobs when using GitHub Copilot.\n\nMental load alleviated\n\nThe benefits of faster development aren’t just about speed: they’re also about alleviating the mental effort that comes with completing tedious tasks. For example, when it comes to debugging, developers have to reverse engineer what went wrong. Detecting a bug can involve digging through an endless list of potential hiding places where it might be lurking, making it repetitive and tedious work.\n\nRosenkilde explains, “Sometimes when you’re debugging, you just have to resort to creating print statements that you can’t get around. Thankfully, Copilot is brilliant at print statements.”\n\nA whopping 87% of developers reported spending less mental effort on repetitive tasks with the help of GitHub Copilot.\n\nLess context switching\n\nIn software development, context switching is when developers move between different tasks, projects, or environments, which can disrupt their workflow and decrease productivity. They also often deal with the stress of juggling multiple tasks, remembering syntax details, and managing complex code structures.\n\nWith GitHub Copilot developers can bypass several levels of context switching, staying in their IDE instead of searching on Google or jumping into external documentation.\n\n“When I’m writing natural language commentary,” says Rosenkilde, “GitHub Copilot code completion can help me. Or if I use Copilot Chat, it’s a conversation in the context that I’m in, and I don’t have to explain quite as much.”\n\nGenerating code with AI helps developers offload the responsibility of recalling every detail, allowing them to focus on higher-level thinking, problem-solving, and strategic planning.\n\nBerryman adds, “With GitHub Copilot Chat, I don’t have to restate the problem because the code never leaves my trusted environment. And I get an answer immediately. If there is a misunderstanding or follow-up questions, they are easy to communicate with.”\n\nBefore you implement any AI into your workflow, you should always review and test tools thoroughly to make sure they’re a good fit for your organization. Here are a few considerations to keep in mind.\n\nCompliance\n\nRegulatory compliance . Does the tool comply with relevant regulations in your industry?\n\n. Does the tool comply with relevant regulations in your industry? Compliance certifications. Are there attestations that demonstrate the tool’s compliance with regulations?\n\nSecurity\n\nEncryption . Is the data transmission and storage encrypted to protect sensitive information?\n\n. Is the data transmission and storage encrypted to protect sensitive information? Access controls . Are you able to implement strong authentication measures and access controls to prevent unauthorized access?\n\n. Are you able to implement strong authentication measures and access controls to prevent unauthorized access? Compliance with security standards . Is the tool compliant with industry standards?\n\n. Is the tool compliant with industry standards? Security audits. Does the tool undergo regular security audits and updates to address vulnerabilities?\n\nPrivacy\n\nData handling . Are there clear policies for handling user data and does it adhere to privacy regulations like GDPR, CCPA, etc.?\n\n. Are there clear policies for handling user data and does it adhere to privacy regulations like GDPR, CCPA, etc.? Data anonymization. Does the tool support anonymization techniques to protect user privacy?\n\nPermissioning\n\nRole-based access control . Are you able to manage permissions based on user roles and responsibilities?\n\n. Are you able to manage permissions based on user roles and responsibilities? Granular permissions . Can you control access to different features and functionalities within the tool?\n\n. Can you control access to different features and functionalities within the tool? Opt-in/Opt-out mechanisms. Can users control the use of their data and opt out if needed?\n\nPricing\n\nUnderstand the pricing model . is it based on usage, number of users, features, or other metrics?\n\n. is it based on usage, number of users, features, or other metrics? Look for transparency . Is the pricing structure clear with no hidden costs?\n\n. Is the pricing structure clear with no hidden costs? Scalability. Does the pricing scale with your usage and business growth?\n\nAdditionally, consider factors such as customer support, ease of integration with existing systems, performance, and user experience when evaluating AI coding tools. Lastly, it’s important to thoroughly assess how well the tool aligns with your organization’s specific requirements and priorities in each of these areas.\n\nVisit the GitHub Copilot Trust Center to learn more around security, privacy, and other topics.\n\nCan AI code generation be detected?\n\nThe short answer here is: maybe.\n\nLet’s first give some context to the question. It’s never really the case that a whole code base is generated with AI, because large chunks of AI-generated code are very likely to be wrong. The standard code review process is a good way to avoid this, since large swaths of completely auto-generated code would stand out to a human developer as simply not working.\n\nFor smaller amounts of AI-generated code, there is no way at the moment to detect traces of AI in code with true confidence. There are offerings that purport to classify whether content has AI-generated text, but there are limited equivalents for code, since you’d need a dedicated model to do it. Ziegler explains, “Computer generated code is good enough that it doesn’t leave any particular traces and normally has no clear tells.”\n\nAt GitHub, the Copilot team makes use of a duplicate detection filter that detects exact duplicates in code. So, if you’re writing code and it’s an exact copy of something that exists elsewhere, then it’ll flag it.\n\nIs AI code generation secure?\n\nAI code generation is not any more insecure than human generated code. A combination of testing, manual code reviews, scanning, monitoring, and feedback loops can produce the same quality of code as your human-generated code.\n\nWhen it comes to code generated by GitHub Copilot, developers can use tools like code scanning, which actively reviews your code for potential security issues in real-time and seamlessly integrates the findings into the developer workflow.\n\nUltimately, AI code generation will have vulnerabilities—but so does code written by human developers. As Ziegler explains, “It’s unclear whether computer generated code does particularly worse. So, the answer is not if you have GitHub Copilot, use a vulnerability checker. The answer is always use a vulnerability checker.”\n\nWatch this video for more tips and words of advice around secure coding best practices with AI.\n\nEmpower your enterprise with AI code generation\n\nWhile the benefits to using AI code generation tools can be significant, it’s important to note that human oversight remains crucial to ensure that the generated code aligns with project goals, coding standards, and business needs.\n\nTech leaders should embrace the use of AI code generation—not only to streamline development, but also to empower developer teams to collaborate, drive meaningful business outcomes, and deliver exceptional value to customers.\n\nLearn more or get started with the world’s most widely adopted AI developer tool.\n\nWant to learn how GitHub can help your organization do more with AI? At GitHub Galaxy 2024, we’ll explore cutting-edge research and best practices in the rapidly evolving world of AI—empowering your business to maximize productivity and innovate at scale. Register now >", "label": "non_personal"}
