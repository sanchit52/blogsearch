{"title": "Read This Will Be Fun \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/21/read-this-will-be-fun/", "content": "This was gleefully goofy and tongue-in-cheek, with magical versions of gossip mags, therapy, and video calls, a fantasy version of Vegas, and an appropriately ridiculous ending. I can\u2019t even be mad at it. Very comedy of errors, blundering heroes with no business being on a quest \u2014 Scooby Doo high fantasy. The lighthearted storytelling balanced out the extreme angst between the characters. I\u2019m a little surprised I liked it, because I think it could have easily been annoying \u2014 I was in the right mood.\n\nIt could have stood to be shorter and faster-paced \u2014 it took me three sessions to finish reading (usually I try to read books in one sitting) and I found myself getting testy at the 85% mark. I can see that shortening it would be hard with three POVs and the big hurdles they each needed to overcome.\n\nI\u2019m not convinced a second romance was necessary to the story, but it probably would have felt awfully third wheel if Elowen didn\u2019t have her own romance. (I think there could be a version of this story that made Beatrice into Gladwell\u2019s sister and was just her and Clare\u2026 which may have been a better romance than this story, but wouldn\u2019t be a DnD squad.)\n\nI didn\u2019t like Clare\u2019s bootstrap moment, and I\u2019m not actually sure what Beatrice\u2019s was?\n\nInteresting to see a romance publisher pushing in on the romantasy angle as well as sci-fi and fantasy publishers stretching towards romance.", "label": 1}
{"title": "Pushy to the Limit: Evolving Netflix\u2019s WebSocket proxy for the future", "url": "https://netflixtechblog.com/pushy-to-the-limit-evolving-netflixs-websocket-proxy-for-the-future-b468bc0ff658?source=collection_home---4------15-----------------------", "content": "Pushy to the Limit: Evolving Netflix\u2019s WebSocket proxy for the future Netflix Technology Blog 16 min read \u00b7 Sep 10, 2024 -- 16 Listen Share\n\nBy Karthik Yagna, Baskar Odayarkoil, and Alex Ellis\n\nPushy is Netflix\u2019s WebSocket server that maintains persistent WebSocket connections with devices running the Netflix application. This allows data to be sent to the device from backend services on demand, without the need for continually polling requests from the device. Over the last few years, Pushy has seen tremendous growth, evolving from its role as a best-effort message delivery service to be an integral part of the Netflix ecosystem. This post describes how we\u2019ve grown and scaled Pushy to meet its new and future needs, as it handles hundreds of millions of concurrent WebSocket connections, delivers hundreds of thousands of messages per second, and maintains a steady 99.999% message delivery reliability rate.\n\nHistory & motivation\n\nThere were two main motivating use cases that drove Pushy\u2019s initial development and usage. The first was voice control, where you can play a title or search using your virtual assistant with a voice command like \u201cShow me Stranger Things on Netflix.\u201d (See How to use voice controls with Netflix if you want to do this yourself!).\n\nIf we consider the Alexa use case, we can see how this partnership with Amazon enabled this to work. Once they receive the voice command, we allow them to make an authenticated call through apiproxy, our streaming edge proxy, to our internal voice service. This call includes metadata, such as the user\u2019s information and details about the command, such as the specific show to play. The voice service then constructs a message for the device and places it on the message queue, which is then processed and sent to Pushy to deliver to the device. Finally, the device receives the message, and the action, such as \u201cShow me Stranger Things on Netflix\u201d, is performed. This initial functionality was built out for FireTVs and was expanded from there.\n\nSample system diagram for an Alexa voice command. Where aws ends and the internet begins is an exercise left to the reader.\n\nThe other main use case was RENO, the Rapid Event Notification System mentioned above. Before the integration with Pushy, the TV UI would continuously poll a backend service to see if there were any row updates to get the latest information. These requests would happen every few seconds, which ended up creating extraneous requests to the backend and were costly for devices, which are frequently resource constrained. The integration with WebSockets and Pushy alleviated both of these points, allowing the origin service to send row updates as they were ready, resulting in lower request rates and cost savings.\n\nFor more background on Pushy, you can see this InfoQ talk by Susheel Aroskar. Since that presentation, Pushy has grown in both size and scope, and this article will be discussing the investments we\u2019ve made to evolve Pushy for the next generation of features.\n\nClient Reach\n\nThis integration was initially rolled out for Fire TVs, PS4s, Samsung TVs, and LG TVs, leading to a reach of about 30 million candidate devices. With these clear benefits, we continued to build out this functionality for more devices, enabling the same efficiency wins. As of today, we\u2019ve expanded our list of candidate devices even further to nearly a billion devices, including mobile devices running the Netflix app and the website experience. We\u2019ve even extended support to older devices that lack modern capabilities, like support for TLS and HTTPS requests. For those, we\u2019ve enabled secure communication from client to Pushy via an encryption/decryption layer on each, allowing for confidential messages to flow between the device and server.\n\nScaling to handle that growth (and more)\n\nGrowth\n\nWith that extended reach, Pushy has gotten busier. Over the last five years, Pushy has gone from tens of millions of concurrent connections to hundreds of millions of concurrent connections, and it regularly reaches 300,000 messages sent per second. To support this growth, we\u2019ve revisited Pushy\u2019s past assumptions and design decisions with an eye towards both Pushy\u2019s future role and future stability. Pushy had been relatively hands-free operationally over the last few years, and as we updated Pushy to fit its evolving role, our goal was also to get it into a stable state for the next few years. This is particularly important as we build out new functionality that relies on Pushy; a strong, stable infrastructure foundation allows our partners to continue to build on top of Pushy with confidence.\n\nThroughout this evolution, we\u2019ve been able to maintain high availability and a consistent message delivery rate, with Pushy successfully maintaining 99.999% reliability for message delivery over the last few months. When our partners want to deliver a message to a device, it\u2019s our job to make sure they can do so.\n\nHere are a few of the ways we\u2019ve evolved Pushy to handle its growing scale.\n\nA few of the related services in Pushy\u2019s immediate ecosystem and the changes we\u2019ve made for them.\n\nMessage processor\n\nOne aspect that we invested in was the evolution of the asynchronous message processor. The previous version of the message processor was a Mantis stream-processing job that processed messages from the message queue. It was very efficient, but it had a set job size, requiring manual intervention if we wanted to horizontally scale it, and it required manual intervention when rolling out a new version.\n\nIt served Pushy\u2019s needs well for many years. As the scale of the messages being processed increased and we were making more code changes in the message processor, we found ourselves looking for something more flexible. In particular, we were looking for some of the features we enjoy with our other services: automatic horizontal scaling, canaries, automated red/black rollouts, and more observability. With this in mind, we rewrote the message processor as a standalone Spring Boot service using Netflix paved-path components. Its job is the same, but it does so with easy rollouts, canary configuration that lets us roll changes safely, and autoscaling policies we\u2019ve defined to let it handle varying volumes.\n\nRewriting always comes with a risk, and it\u2019s never the first solution we reach for, particularly when working with a system that\u2019s in place and working well. In this case, we found that the burden from maintaining and improving the custom stream processing job was increasing, and we made the judgment call to do the rewrite. Part of the reason we did so was the clear role that the message processor played \u2014 we weren\u2019t rewriting a huge monolithic service, but instead a well-scoped component that had explicit goals, well-defined success criteria, and a clear path towards improvement. Since the rewrite was completed in mid-2023, the message processor component has been completely zero touch, happily automated and running reliably on its own.\n\nPush Registry\n\nFor most of its life, Pushy has used Dynomite for keeping track of device connection metadata in its Push Registry. Dynomite is a Netflix open source wrapper around Redis that provides a few additional features like auto-sharding and cross-region replication, and it provided Pushy with low latency and easy record expiry, both of which are critical for Pushy\u2019s workload.\n\nAs Pushy\u2019s portfolio grew, we experienced some pain points with Dynomite. Dynomite had great performance, but it required manual scaling as the system grew. The folks on the Cloud Data Engineering (CDE) team, the ones building the paved path for internal data at Netflix, graciously helped us scale it up and make adjustments, but it ended up being an involved process as we kept growing.\n\nThese pain points coincided with the introduction of KeyValue, which was a new offering from the CDE team that is roughly \u201cHashMap as a service\u201d for Netflix developers. KeyValue is an abstraction over the storage engine itself, which allows us to choose the best storage engine that meets our SLO needs. In our case, we value low latency \u2014 the faster we can read from KeyValue, the faster these messages can get delivered. With CDE\u2019s help, we migrated our Push Registry to use KV instead, and we have been extremely satisfied with the result. After tuning our store for Pushy\u2019s needs, it has been on autopilot since, appropriately scaling and serving our requests with very low latency.\n\nScaling Pushy horizontally and vertically\n\nMost of the other services our team runs, like apiproxy, the streaming edge proxy, are CPU bound, and we have autoscaling policies that scale them horizontally when we see an increase in CPU usage. This maps well to their workload \u2014 more HTTP requests means more CPU used, and we can scale up and down accordingly.\n\nPushy has slightly different performance characteristics, with each node maintaining many connections and delivering messages on demand. In Pushy\u2019s case, CPU usage is consistently low, since most of the connections are parked and waiting for an occasional message. Instead of relying on CPU, we scale Pushy on the number of connections, with exponential scaling to scale faster after higher thresholds are reached. We load balance the initial HTTP requests to establish the connections and rely on a reconnect protocol where devices will reconnect every 30 minutes or so, with some staggering, that gives us a steady stream of reconnecting devices to balance connections across all available instances.\n\nFor a few years, our scaling policy had been that we would add new instances when the average number of connections reached 60,000 connections per instance. For a couple hundred million devices, this meant that we were regularly running thousands of Pushy instances. We can horizontally scale Pushy to our heart\u2019s content, but we would be less content with our bill and would have to shard Pushy further to get around NLB connection limits. This evolution effort aligned well with an internal focus on cost efficiency, and we used this as an opportunity to revisit these earlier assumptions with an eye towards efficiency.\n\nBoth of these would be helped by increasing the number of connections that each Pushy node could handle, reducing the total number of Pushy instances and running more efficiently with the right balance between instance type, instance cost, and maximum concurrent connections. It would also allow us to have more breathing room with the NLB limits, reducing the toil of additional sharding as we continue to grow. That being said, increasing the number of connections per node is not without its own drawbacks. When a Pushy instance goes down, the devices that were connected to it will immediately try to reconnect. By increasing the number of connections per instance, it means that we would be increasing the number of devices that would be immediately trying to reconnect. We could have a million connections per instance, but a down node would lead to a thundering herd of a million devices reconnecting at the same time.\n\nThis delicate balance led to us doing a deep evaluation of many instance types and performance tuning options. Striking that balance, we ended up with instances that handle an average of 200,000 connections per node, with breathing room to go up to 400,000 connections if we had to. This makes for a nice balance between CPU usage, memory usage, and the thundering herd when a device connects. We\u2019ve also enhanced our autoscaling policies to scale exponentially; the farther we are past our target average connection count, the more instances we\u2019ll add. These improvements have enabled Pushy to be almost entirely hands off operationally, giving us plenty of flexibility as more devices come online in different patterns.\n\nReliability & building a stable foundation\n\nAlongside these efforts to scale Pushy for the future, we also took a close look at our reliability after finding some connectivity edge cases during recent feature development. We found a few areas for improvement around the connection between Pushy and the device, with failures due to Pushy attempting to send messages on a connection that had failed without notifying Pushy. Ideally something like a silent failure wouldn\u2019t happen, but we frequently see odd client behavior, particularly on older devices.\n\nIn collaboration with the client teams, we were able to make some improvements. On the client side, better connection handling and improvements around the reconnect flow meant that they were more likely to reconnect appropriately. In Pushy, we added additional heartbeats, idle connection cleanup, and better connection tracking, which meant that we were keeping around fewer and fewer stale connections.\n\nWhile these improvements were mostly around those edge cases for the feature development, they had the side benefit of bumping our message delivery rates up even further. We already had a good message delivery rate, but this additional bump has enabled Pushy to regularly average 5 9s of message delivery reliability.\n\nPush message delivery success rate over a recent 2-week period.\n\nRecent developments\n\nWith this stable foundation and all of these connections, what can we now do with them? This question has been the driving force behind nearly all of the recent features built on top of Pushy, and it\u2019s an exciting question to ask, particularly as an infrastructure team.\n\nShift towards direct push\n\nThe first change from Pushy\u2019s traditional role is what we call direct push; instead of a backend service dropping the message on the asynchronous message queue, it can instead leverage the Push library to skip the asynchronous queue entirely. When called to deliver a message in the direct path, the Push library will look up the Pushy connected to the target device in the Push Registry, then send the message directly to that Pushy. Pushy will respond with a status code reflecting whether it was able to successfully deliver the message or it encountered an error, and the Push library will bubble that up to the calling code in the service.\n\nThe system diagram for the direct and indirect push paths.\n\nSusheel, the original author of Pushy, added this functionality as an optional path, but for years, nearly all backend services relied on the indirect path with its \u201cbest-effort\u201d being good enough for their use cases. In recent years, we\u2019ve seen usage of this direct path really take off as the needs of backend services have grown. In particular, rather than being just best effort, these direct messages allow the calling service to have immediate feedback about the delivery, letting them retry if a device they\u2019re targeting has gone offline.\n\nThese days, messages sent via direct push make up the majority of messages sent through Pushy. For example, for a recent 24 hour period, direct messages averaged around 160,000 messages per second and indirect averaged at around 50,000 messages per second..\n\nGraph of direct vs indirect messages per second.\n\nDevice to device messaging\n\nAs we\u2019ve thought through this evolving use case, our concept of a message sender has also evolved. What if we wanted to move past Pushy\u2019s pattern of delivering server-side messages? What if we wanted to have a device send a message to a backend service, or maybe even to another device? Our messages had traditionally been unidirectional as we send messages from the server to the device, but we now leverage these bidirectional connections and direct device messaging to enable what we call device to device messaging. This device to device messaging supported early phone-to-TV communication in support of games like Triviaverse, and it\u2019s the messaging foundation for our Companion Mode as TVs and phones communicate back and forth.\n\nA screenshot of one of the authors playing Triviaquest with a mobile device as the controller.\n\nThis requires higher level knowledge of the system, where we need to know not just information about a single device, but more broader information, like what devices are connected for an account that the phone can pair with. This also enables things like subscribing to device events to know when another device comes online and when they\u2019re available to pair or send a message to. This has been built out with an additional service that receives device connection information from Pushy. These events, sent over a Kafka topic, let the service keep track of the device list for a given account. Devices can subscribe to these events, allowing them to receive a message from the service when another device for the same account comes online.\n\nPushy and its relationship with the Device List Service for discovering other devices.\n\nThis device list enables the discoverability aspect of these device to device messages. Once the devices have this knowledge of the other devices connected for the same account, they\u2019re able to choose a target device from this list that they can then send messages to.\n\nOnce a device has that list, it can send a message to Pushy over its WebSocket connection with that device as the target in what we call a device to device message (1 in the diagram below). Pushy looks up the target device\u2019s metadata in the Push registry (2) and sends the message to the second Pushy that the target device is connected to (3), as if it was the backend service in the direct push pattern above. That Pushy delivers the message to the target device (4), and the original Pushy will receive a status code in response, which it can pass back to the source device (5).\n\nA basic order of events for a device to device message.\n\nThe messaging protocol\n\nWe\u2019ve defined a basic JSON-based message protocol for device to device messaging that lets these messages be passed from the source device to the target device. As a networking team, we naturally lean towards abstracting the communication layer with encapsulation wherever possible. This generalized message means that device teams are able to define their own protocols on top of these messages \u2014 Pushy would just be the transport layer, happily forwarding messages back and forth.\n\nThe client app protocol, built on top of the device to device protocol, built on top of Pushy.\n\nThis generalization paid off in terms of investment and operational support. We built the majority of this functionality in October 2022, and we\u2019ve only needed small tweaks since then. We needed nearly no modifications as client teams built out the functionality on top of this layer, defining the higher level application-specific protocols that powered the features they were building. We really do enjoy working with our partner teams, but if we\u2019re able to give them the freedom to build on top of our infrastructure layer without us getting involved, then we\u2019re able to increase their velocity, make their lives easier, and play our infrastructure roles as message platform providers.\n\nWith early features in experimentation, Pushy sees an average of 1000 device to device messages per second, a number that will only continue to grow.\n\nGraph of device to device messages per second.\n\nThe Netty-gritty details\n\nIn Pushy, we handle incoming WebSocket messages in our PushClientProtocolHandler (code pointer to class in Zuul that we extend), which extends Netty\u2019s ChannelInboundHandlerAdapter and is added to the Netty pipeline for each client connection. We listen for incoming WebSocket messages from the connected device in its channelRead method and parse the incoming message. If it\u2019s a device to device message, we pass the message, the ChannelHandlerContext, and the PushUserAuth information about the connection\u2019s identity to our DeviceToDeviceManager.\n\nA rough overview of the internal organization for these components.\n\nThe DeviceToDeviceManager is responsible for validating the message, doing some bookkeeping, and kicking off an async call that validates that the device is an authorized target, looks up the Pushy for the target device in the local cache (or makes a call to the data store if it\u2019s not found), and forwards on the message. We run this asynchronously to avoid any event loop blocking due to these calls. The DeviceToDeviceManager is also responsible for observability, with metrics around cache hits, calls to the data store, message delivery rates, and latency percentile measurements. We\u2019ve relied heavily on these metrics for alerts and optimizations \u2014 Pushy really is a metrics service that occasionally will deliver a message or two!\n\nSecurity\n\nAs the edge of the Netflix cloud, security considerations are always top of mind. With every connection over HTTPS, we\u2019ve limited these messages to just authenticated WebSocket connections, added rate limiting, and added authorization checks to ensure that a device is able to target another device \u2014 you may have the best intentions in mind, but I\u2019d strongly prefer it if you weren\u2019t able to send arbitrary data to my personal TV from yours (and vice versa, I\u2019m sure!).\n\nLatency and other considerations\n\nOne main consideration with the products built on top of this is latency, particularly when this feature is used for anything interactive within the Netflix app.\n\nWe\u2019ve added caching to Pushy to reduce the number of lookups in the hotpath for things that are unlikely to change frequently, like a device\u2019s allowed list of targets and the Pushy instance the target device is connected to. We have to do some lookups on the initial messages to know where to send them, but it enables us to send subsequent messages faster without any KeyValue lookups. For these requests where caching removed KeyValue from the hot path, we were able to greatly speed things up. From the incoming message arriving at Pushy to the response being sent back to the device, we reduced median latency to less than a millisecond, with the 99th percentile of latency at less than 4ms.\n\nOur KeyValue latency is usually very low, but we have seen brief periods of elevated read latencies due to underlying issues in our KeyValue datastore. Overall latencies increased for other parts of Pushy, like client registration, but we saw very little increase in device to device latency with this caching in place.\n\nCultural aspects that enable this work\n\nPushy\u2019s scale and system design considerations make the work technically interesting, but we also deliberately focus on non-technical aspects that have helped to drive Pushy\u2019s growth. We focus on iterative development that solves the hardest problem first, with projects frequently starting with quick hacks or prototypes to prove out a feature. As we do this initial version, we do our best to keep an eye towards the future, allowing us to move quickly from supporting a single, focused use case to a broad, generalized solution. For example, for our cross-device messaging, we were able to solve hard problems in the early work for Triviaverse that we later leveraged for the generic device to device solution.\n\nAs one can immediately see in the system diagrams above, Pushy does not exist in a vacuum, with projects frequently involving at least half a dozen teams. Trust, experience, communication, and strong relationships all enable this to work. Our team wouldn\u2019t exist without our platform users, and we certainly wouldn\u2019t be here writing this post without all of the work our product and client teams do. This has also emphasized the importance of building and sharing \u2014 if we\u2019re able to get a prototype together with a device team, we\u2019re able to then show it off to seed ideas from other teams. It\u2019s one thing to mention that you can send these messages, but it\u2019s another to show off the TV responding to the first click of the phone controller button!\n\nThe future of Pushy\n\nIf there\u2019s anything certain in this world, it\u2019s that Pushy will continue to grow and evolve. We have many new features in the works, like WebSocket message proxying, WebSocket message tracing, a global broadcast mechanism, and subscription functionality in support of Games and Live. With all of this investment, Pushy is a stable, reinforced foundation, ready for this next generation of features.\n\nWe\u2019ll be writing about those new features as well \u2014 stay tuned for future posts.\n\nSpecial thanks to our stunning colleagues Jeremy Kelly and Justin Guerra who have both been invaluable to Pushy\u2019s growth and the WebSocket ecosystem at large. We would also like to thank our larger teams and our numerous partners for their great work; it truly takes a village!", "label": 0}
{"title": "On-device small language models with multimodality, RAG, and Function Calling", "url": "https://developers.googleblog.com/en/google-ai-edge-small-language-models-multimodality-rag-function-calling/", "content": "Last year Google AI Edge introduced support for on-device small language models (SLMs) with four initial models on Android, iOS, and Web. Today, we are excited to expand support to over a dozen models including the new Gemma 3 and Gemma 3n models, hosted on our new LiteRT Hugging Face community. Gemma 3n, available via Google AI Edge as an early preview, is Gemma\u2019s first multimodal on-device small language model supporting text, image, video, and audio inputs. Paired with our new Retrieval Augmented Generation (RAG) and Function Calling libraries, you have everything you need to prototype and build transformative AI features fully on the edge.\n\nSorry, your browser doesn't support playback for this video Let users control apps with on-device SLMs and our new function calling library\n\nBroader model support You can find our growing list of models to choose from in the LiteRT Hugging Face Community. Download any of these models and easily run them on-device with just a few lines of code. The models are fully optimized and converted for mobile and web. Full instructions on how to run these models can be found in our documentation and on each model card on Hugging Face. To customize any of these models, you finetune the base model and then convert and quantize the model using the appropriate AI Edge libraries. We have a Colab showing every step you need to fine-tune and then convert Gemma 3 1B. With the latest release of our quantization tools, we have new quantization schemes that allow for much higher quality int4 post training quantization. Compared to bf16, the default data type for many models, int4 quantization can reduce the size of language models by a factor of 2.5-4X while significantly decreasing latency and peak memory consumption.\n\nGemma 3 1B & Gemma 3n Earlier this year, we introduced Gemma 3 1B. At only 529MB, this model can run up to 2,585 tokens per second pre-fill on the mobile GPU, allowing it to process up to a page of content in under a second. Gemma 3 1B\u2019s small footprint allows it to support a wide range of devices and limits the size of files an end user would need to download in their application. Today, we are thrilled to add an early preview of Gemma 3n to our collection of supported models. The 2B and 4B parameter variants will both support native text, image, video, and audio inputs. The text and image modalities are available on Hugging Face with audio to follow shortly.\n\nSorry, your browser doesn't support playback for this video Gemma 3n analyzing images fully on-device", "label": 0}
{"title": "How to Add the Moesif API Observability Add-On to Your Heroku Applications", "url": "https://www.heroku.com/blog/add-moesif-api-observability-to-your-heroku-applications/", "content": "With API-driven applications being increasingly common, understanding how your APIs are performing is crucial for success. That\u2019s where the combination of Heroku and Moesif allows developers and their organizations to step up their observability game. In this blog, we will quickly examine how you can integrate Moesif with your Heroku app to begin monetizing and analyzing your API traffic. Let\u2019s kick things off by taking a brief look at both platforms.\n\nWhat is Heroku?\n\nHeroku is a cloud-based Platform as a Service (PaaS) that enables developers to build, run, and scale applications entirely in the cloud. It abstracts away the complexities of infrastructure management, allowing you to focus on writing code and delivering features. Heroku supports many programming languages and frameworks, making it an excellent application development and deployment tool.\n\nWhat is Moesif?\n\nMoesif is an API analytics and monetization platform that provides deep insights into how your APIs are used and delivers the capabilities to monetize them easily. It captures detailed information about API calls, including request/response payloads, latency, errors, and user behavior. With Moesif, you can:\n\nMonitor API Performance : Identify bottlenecks, track error rates, and optimize response times.\n\n: Identify bottlenecks, track error rates, and optimize response times. Understand User Behavior : See how users interact with your APIs, which endpoints are most popular, and what features they\u2019re utilizing.\n\n: See how users interact with your APIs, which endpoints are most popular, and what features they\u2019re utilizing. Debug Issues : Quickly pinpoint the root cause of errors and resolve problems impacting your users.\n\n: Quickly pinpoint the root cause of errors and resolve problems impacting your users. Monetize Your API: Implement usage-based billing models and track revenue generated from your API.\n\nBenefits of Heroku and Moesif\n\nBy using the Moesif Heroku add-on, you can reduce the time to set up API Observability and ensure a seamless integration with Heroku. Billing and user management is automatically handled by Heroku which further reduces your overhead.\n\nWhy Are API Analytics Important?\n\nIf your app contains APIs, then a specialized API analytics platform must be used to truly understand how your APIs are used and what value they deliver. API analytics are essential for several reasons:\n\nImproved Performance : Identify and fix performance issues before they affect your users.\n\n: Identify and fix performance issues before they affect your users. Enhanced User Experience : Understand how users use your API and tailor it to their needs.\n\n: Understand how users use your API and tailor it to their needs. Data-Driven Decisions : Make informed API development, pricing, and business-level decisions based on usage data.\n\n: Make informed API development, pricing, and business-level decisions based on usage data. Increased Revenue: Monetize your API effectively by understanding usage patterns and identifying growth opportunities.\n\nAPI analytics allow you to examine not only the engineering side of the puzzle but also derive a large number of business insights.\n\nAdding Moesif to Your Heroku Application (Step-by-Step)\n\nWhen using Heroku and Moesif together, the process is straightforward and can be done directly through the Heroku CLI and UI. Below, we will go through how to add Moesif to your Heroku instance, including the steps in the UI or Heroku CLI, depending on your preferred approach.\n\nAdd Via CLI\n\nFirst, we will look at installing the Moesif Add-On through the CLI. For this, we assume that you:\n\nHave a Heroku account and an app running on Heroku\n\nYou have the Heroku CLI installed and logged into the application you want to add Moesif to.\n\nWith these prerequisites handled, you can proceed.\n\nInstall the Add-on\n\nMoesif can be attached to a Heroku application via the CLI:\n\nheroku addons:create moesif\n\nOnce the command is executed, you should see something similar to the following:\n\n----- > Adding moesif to sharp-mountain-4005 .. . done, v18 ( free )\n\nA MOESIF_APPLICATION_ID config var is added to your Heroku app\u2019s configuration during provisioning. It contains the write-only API token that identifies your application with Moesif. You can confirm the variable exists via the heroku config:get command:\n\nheroku config:get MOESIF_APPLICATION_ID\n\nThis will print out your Moesif Application ID to the console, confirming it is correctly set in the config file.\n\nAdd Via UI\n\nAlternatively, you can install the Moesif Add-On through the Heroku Dashboard UI. For this, we assume that you:\n\nHave a Heroku account and an app running on Heroku\n\nAre logged into the Heroku Dashboard for the app you\u2019d like to add Moesif to\n\nWith these prerequisites handled, you can proceed.\n\nInstall the Add-On\n\nWhile logged into the dashboard for the app you want to add Moesif to, on the Overview page, click the Configure Add-ons button.\n\nThis will then bring you to the Resources screen to view your current add-ons. In this instance, we have none. From here, click the Find more add-ons button.\n\nOn the next screen, where all available add-ons are listed, click Metrics and Analytics on the left-side menu. Locate the Moesif API Observability entry and click on it.\n\nOn the Moesif API Observability and Monetization overview page, click Install Moesif API Observability in the top-right corner.\n\nNext, you\u2019ll be prompted to confirm the installation and submit the order. To confirm and install, click the Submit Order Form button to add Moesif to your Heroku app and activate your subscription.\n\nOnce complete, you\u2019ll see that Moesif has been added to your Heroku instance and is ready for further configuration.\n\nInstall the server integration\n\nWith Moesif installed on our Heroku instance and subscription activated, we need to add Moesif to the application running on Heroku. To do this, go to your Heroku dashboard and open Moesif from under \u201cInstalled add-ons\u201d\n\nOnce inside the Moesif application, the onboarding flow that appears will walk you through adding the Moesif SDK to your code.\n\nWhen initializing the SDK, use the environment variable MOESIF_APPLICATION_ID for the application ID. For example, in a Node application, you\u2019d grab the Moesif Application ID by using process.env.MOESIF_APPLICATION_ID . This would be retrieved from the app config variables.\n\nLocal setup\n\nAfter you provision the add-on, you must replicate your config variables locally so your development environment can operate against the service.\n\nUse the Heroku Local command-line tool to configure, run, and manage process types specified in your app\u2019s Procfile. Heroku Local reads configuration variables from a .env file. To view all of your app\u2019s config vars, type heroku config. Use the following command for each value that you want to add to your .env file:\n\nheroku config:get MOESIF_APPLICATION_ID -s >> .env\n\nCredentials and other sensitive values should not be committed to source control. If you\u2019re using Git, you can exclude the .env file by adding it to the gitignore file with:\n\necho .env >> .gitignore\n\nFor more information, see the Heroku Local article.\n\nUsing Moesif Dashboards\n\nOnce everything is configured, events should begin to flow into Moesif. These events can be used for analytics and monetization directly within the Moesif platform.\n\nKey Moesif Features to Leverage:\n\nLive Event Log : See individual API calls in real-time.\n\n: See individual API calls in real-time. Time Series Metrics : Track API traffic, latency, errors, and more over time.\n\n: Track API traffic, latency, errors, and more over time. Funnels and Retention : Analyze user journeys through your API.\n\n: Analyze user journeys through your API. Alerting : Get notified of critical API issues.\n\n: Get notified of critical API issues. Monetization: Drive revenue from your API calls using post-paid and pre-paid billing\n\nCheck out our docs and tutorials pages for all the ways you can leverage Moesif.\n\nOpen Through The Heroku CLI\n\nTo open Moesif, you can the following command cia the Heroku CLI:\n\nheroku addons:open moesif\n\nOr, from the Heroku Application Dashboard, select Moesif from the Add-ons menu.\n\nOnce logged in, you\u2019ll have full access to the Moesif platform, which includes everything needed for extensive API analytics and monetization.\n\nTry It Out\n\nWant to try out Moesif for yourself? You can do so by following the directions above and creating an account through Heroku or sign-up directly. Powerful API analytics and monetization capabilities are just a few clicks away.", "label": 0}
{"title": "tech industry \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/tag/tech-industry/", "content": "This feels like a sister piece to Ed Zitron\u2019s essay Era of the Business Idiots and Mandy Brown\u2019s essay Toolmen. Fair warning, this is a 5000 word post; I\u2019ve been working on this for weeks, pulling together what I\u2019ve learned about generative AI and culture over the past two years, so I hope it is worth your time \ud83d\ude04 Bonus: it doubles as a playlist \ud83c\udfb6\n\n\u201c\u2018Real power\u2019 is achieved when a technology \u2018[leaves] mythology and [enters] banality,'\u201d Marion Fourcade and Kieran Healy quote Vincent Mosco in The Ordinal Society. We\u2019ve had the mythology stage \u2014 the world tour with grandiose prophecies of imminent AGI \u2014 but now the race to normalize generative AI* is on: tech corporations are attempting to inure people to generative AI, an expression of the Business Borg aesthetic that currently carries a negative stigma outside of tech.\n\n*(My rule of thumb: if something is described as AI, it\u2019s probably predatory and/or bullshit; if it\u2019s described as machine learning, it probably does something useful. Not always true but a helpful predictor.)\n\nIn general, people like what we recognize better than what we don\u2019t \u2014 we prefer cultural works we can categorize to the unfamiliar and undefinable \u2014 and we are facing an inescapable shock-and-awe barrage of genAI graphics across the web to inundate our synapses with uncanny synthetic renderings.\n\nCurrently, generative AI is shunned by many artists and writers, the traditional arbiters of good taste and culture, because it has been developed through the theft of their labor. But tech CEOs stand to make (even bigger) fortunes if they can convince people that genAI doesn\u2019t signify bad taste, or make it seem like an irrevocable fact of life, like spam emails and text scammers. It\u2019s being deployed upon us with the same lockstep corporate solidarity that forced us to pay fees for checked luggage on flights (younger folks, before 2008 your bag used to be included with your ticket! Stowing your carry-on wasn\u2019t a competitive sport back in the day.).", "label": 1}
{"title": "FM-Intent: Predicting User Session Intent with Hierarchical Multi-Task Learning", "url": "https://netflixtechblog.com/fm-intent-predicting-user-session-intent-with-hierarchical-multi-task-learning-94c75e18f4b8?source=collection_home---4------0-----------------------", "content": "FM-Intent: Predicting User Session Intent with Hierarchical Multi-Task Learning Netflix Technology Blog 7 min read \u00b7 May 21, 2025 -- 4 Listen Share\n\nAuthors: Sejoon Oh, Moumita Bhattacharya, Yesu Feng, Sudarshan Lamkhede, Ko-Jen Hsiao, and Justin Basilico\n\nMotivation\n\nRecommender systems have become essential components of digital services across e-commerce, streaming media, and social networks [1, 2]. At Netflix, these systems drive significant product and business impact by connecting members with relevant content at the right time [3, 4]. While our recommendation foundation model (FM) has made substantial progress in understanding user preferences through large-scale learning from interaction histories (please refer to this article about FM @ Netflix), there is an opportunity to further enhance its capabilities. By extending FM to incorporate the prediction of underlying user intents, we aim to enrich its understanding of user sessions beyond next-item prediction, thereby offering a more comprehensive and nuanced recommendation experience.\n\nRecent research has highlighted the importance of understanding user intent in online platforms [5, 6, 7, 8]. As Xia et al. [8] demonstrated at Pinterest, predicting a user\u2019s future intent can lead to more accurate and personalized recommendations. However, existing intent prediction approaches typically employ simple multi-task learning that adds intent prediction heads to next-item prediction models without establishing a hierarchical relationship between these tasks.\n\nTo address these limitations, we introduce FM-Intent, a novel recommendation model that enhances our foundation model through hierarchical multi-task learning. FM-Intent captures a user\u2019s latent session intent using both short-term and long-term implicit signals as proxies, then leverages this intent prediction to improve next-item recommendations. Unlike conventional approaches, FM-Intent establishes a clear hierarchy where intent predictions directly inform item recommendations, creating a more coherent and effective recommendation pipeline.\n\nFM-Intent makes three key contributions:\n\nA novel recommendation model that captures user intent on the Netflix platform and enhances next-item prediction using this intent information. A hierarchical multi-task learning approach that effectively models both short-term and long-term user interests. Comprehensive experimental validation showing significant performance improvements over state-of-the-art models, including our foundation model.\n\nUnderstanding User Intent in Netflix\n\nIn the Netflix ecosystem, user intent manifests through various interaction metadata, as illustrated in Figure 1. FM-Intent leverages these implicit signals to predict both user intent and next-item recommendations.\n\nFigure 1: Overview of user engagement data in Netflix. User intent can be associated with several interaction metadata. We leverage various implicit signals to predict user intent and next-item.\n\nIn Netflix, there can be multiple types of user intents. For instance,\n\nAction Type: Categories reflecting what users intend to do on Netflix, such as discovering new content versus continuing previously started content. For example, when a member plays a follow-up episode of something they were already watching, this can be categorized as \u201ccontinue watching\u201d intent. Genre Preference: The pre-defined genre labels (e.g., Action, Thriller, Comedy) that indicate a user\u2019s content preferences during a session. These preferences can shift significantly between sessions, even for the same user. Movie/Show Type: Whether a user is looking for a movie (typically a single, longer viewing experience) or a TV show (potentially multiple episodes of shorter duration). Time-since-release: Whether the user prefers newly released content, recent content (e.g., between a week and a month), or evergreen catalog titles.\n\nThese dimensions serve as proxies for the latent user intent, which is often not directly observable but crucial for providing relevant recommendations.\n\nFM-Intent Model Architecture\n\nFM-Intent employs a hierarchical multi-task learning approach with three major components, as illustrated in Figure 2.\n\nFigure 2: An architectural illustration of our hierarchical multi-task learning model FM-Intent for user intent and item predictions. We use ground-truth intent and item-ID labels to optimize predictions.\n\n1. Input Feature Sequence Formation\n\nThe first component constructs rich input features by combining interaction metadata. The input feature for each interaction combines categorical embeddings and numerical features, creating a comprehensive representation of user behavior.\n\n2. User Intent Prediction\n\nThe intent prediction component processes the input feature sequence through a Transformer encoder and generates predictions for multiple intent signals.\n\nThe Transformer encoder effectively models the long-term interest of users through multi-head attention mechanisms. For each prediction task, the intent encoding is transformed into prediction scores via fully-connected layers.\n\nA key innovation in FM-Intent is the attention-based aggregation of individual intent predictions. This approach generates a comprehensive intent embedding that captures the relative importance of different intent signals for each user, providing valuable insights for personalization and explanation.\n\n3. Next-Item Prediction with Hierarchical Multi-Task Learning\n\nThe final component combines the input features with the user intent embedding to make more accurate next-item recommendations.\n\nFM-Intent employs hierarchical multi-task learning where intent predictions are conducted first, and their results are used as input features for the next-item prediction task. This hierarchical relationship ensures that the next-item recommendations are informed by the predicted user intent, creating a more coherent and effective recommendation model.\n\nOffline Results\n\nWe conducted comprehensive offline experiments on sampled Netflix user engagement data to evaluate FM-Intent\u2019s performance. Note that FM-Intent uses a much smaller dataset for training compared to the FM production model due to its complex hierarchical prediction architecture.\n\nNext-Item and Next-Intent Prediction Accuracy\n\nTable 1 compares FM-Intent with several state-of-the-art sequential recommendation models, including our production model (FM-Intent-V0).\n\nTable 1: Next-item and next-intent prediction results of baselines and our proposed method FM-Intent on the Netflix user engagement dataset.\n\nAll metrics are represented as relative % improvements compared to the SOTA baseline: TransAct. N/A indicates that a model is not capable of predicting a certain intent. Note that we added additional fully-connected layers to LSTM, GRU, and Transformer baselines in order to predict user intent, while we used original implementations for other baselines. FM-Intent demonstrates statistically significant improvement of 7.4% in next-item prediction accuracy compared to the best baseline (TransAct).\n\nMost baseline models show limited performance as they either cannot predict user intent or cannot incorporate intent predictions into next-item recommendations. Our production model (FM-Intent-V0) performs well but lacks the ability to predict and leverage user intent. Note that FM-Intent-V0 is trained with a smaller dataset for a fair comparison with other models; the actual production model is trained with a much larger dataset.\n\nQualitative Analysis: User Clustering\n\nFigure 3: K-means++ (K=10) clustering of user intent embeddings found by FM-Intent; FM-Intent finds unique clusters of users that share the similar intent.\n\nFM-Intent generates meaningful user intent embeddings that can be used for clustering users with similar intents. Figure 3 visualizes 10 distinct clusters identified through K-means++ clustering. These clusters reveal meaningful user segments with distinct viewing patterns:\n\nUsers who primarily discover new content versus those who continue watching recent/favorite content.\n\nGenre enthusiasts (e.g., anime/kids content viewers).\n\nUsers with specific viewing patterns (e.g., Rewatchers versus casual viewers).\n\nPotential Applications of FM-Intent\n\nFM-Intent has been successfully integrated into Netflix\u2019s recommendation ecosystem, can be leveraged for several downstream applications:\n\nPersonalized UI Optimization: The predicted user intent could inform the layout and content selection on the Netflix homepage, emphasizing different rows based on whether users are in discovery mode, continue-watching mode, or exploring specific genres. Analytics and User Understanding: Intent embeddings and clusters provide valuable insights into viewing patterns and preferences, informing content acquisition and production decisions. Enhanced Recommendation Signals: Intent predictions serve as features for other recommendation models, improving their accuracy and relevance. Search Optimization: Real-time intent predictions help prioritize search results based on the user\u2019s current session intent.\n\nConclusion\n\nFM-Intent represents an advancement in Netflix\u2019s recommendation capabilities by enhancing them with hierarchical multi-task learning for user intent prediction. Our comprehensive experiments demonstrate that FM-Intent significantly outperforms state-of-the-art models, including our prior foundation model that focused solely on next-item prediction. By understanding not just what users might watch next but what underlying intents users have, we can provide more personalized, relevant, and satisfying recommendations.\n\nAcknowledgements\n\nWe thank our stunning colleagues in the Foundation Model team & AIMS org. for their valuable feedback and discussions. We also thank our partner teams for getting this up and running in production.\n\nReferences\n\n[1] Amatriain, X., & Basilico, J. (2015). Recommender systems in industry: A netflix case study. In Recommender systems handbook (pp. 385\u2013419). Springer.\n\n[2] Gomez-Uribe, C. A., & Hunt, N. (2015). The netflix recommender system: Algorithms, business value, and innovation. ACM Transactions on Management Information Systems (TMIS), 6(4), 1\u201319.\n\n[3] Jannach, D., & Jugovac, M. (2019). Measuring the business value of recommender systems. ACM Transactions on Management Information Systems (TMIS), 10(4), 1\u201323.\n\n[4] Bhattacharya, M., & Lamkhede, S. (2022). Augmenting Netflix Search with In-Session Adapted Recommendations. In Proceedings of the 16th ACM Conference on Recommender Systems (pp. 542\u2013545).\n\n[5] Chen, Y., Liu, Z., Li, J., McAuley, J., & Xiong, C. (2022). Intent contrastive learning for sequential recommendation. In Proceedings of the ACM Web Conference 2022 (pp. 2172\u20132182).\n\n[6] Ding, Y., Ma, Y., Wong, W. K., & Chua, T. S. (2021). Modeling instant user intent and content-level transition for sequential fashion recommendation. IEEE Transactions on Multimedia, 24, 2687\u20132700.\n\n[7] Liu, Z., Chen, H., Sun, F., Xie, X., Gao, J., Ding, B., & Shen, Y. (2021). Intent preference decoupling for user representation on online recommender system. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence (pp. 2575\u20132582).\n\n[8] Xia, X., Eksombatchai, P., Pancha, N., Badani, D. D., Wang, P. W., Gu, N., Joshi, S. V., Farahpour, N., Zhang, Z., & Zhai, A. (2023). TransAct: Transformer-based Realtime User Action Model for Recommendation at Pinterest. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (pp. 5249\u20135259).", "label": 0}
{"title": "Model Once, Represent Everywhere: UDA (Unified Data Architecture) at Netflix", "url": "https://netflixtechblog.com/uda-unified-data-architecture-6a6aee261d8d?source=collection_home---4------0-----------------------", "content": "Model Once, Represent Everywhere: UDA (Unified Data Architecture) at Netflix Netflix Technology Blog 15 min read \u00b7 Jun 12, 2025 -- 23 Listen Share\n\nBy Alex Hutter, Alexandre Bertails, Claire Wang, Haoyuan He, Kishore Banala, Peter Royal, Shervin Afshar\n\nAs Netflix\u2019s offerings grow \u2014 across films, series, games, live events, and ads \u2014 so does the complexity of the systems that support it. Core business concepts like \u2018actor\u2019 or \u2018movie\u2019 are modeled in many places: in our Enterprise GraphQL Gateway powering internal apps, in our asset management platform storing media assets, in our media computing platform that powers encoding pipelines, to name a few. Each system models these concepts differently and in isolation, with little coordination or shared understanding. While they often operate on the same concepts, these systems remain largely unaware of that fact, and of each other.\n\nAs a result, several challenges emerge:\n\nDuplicated and Inconsistent Models \u2014 Teams re-model the same business entities in different systems, leading to conflicting definitions that are hard to reconcile.\n\n\u2014 Teams re-model the same business entities in different systems, leading to conflicting definitions that are hard to reconcile. Inconsistent Terminology \u2014 Even within a single system, teams may use different terms for the same concept, or the same term for different concepts, making collaboration harder.\n\n\u2014 Even within a single system, teams may use different terms for the same concept, or the same term for different concepts, making collaboration harder. Data Quality Issues \u2014 Discrepancies and broken references are hard to detect across our many microservices. While identifiers and foreign keys exist, they are inconsistently modeled and poorly documented, requiring manual work from domain experts to find and fix any data issues.\n\n\u2014 Discrepancies and broken references are hard to detect across our many microservices. While identifiers and foreign keys exist, they are inconsistently modeled and poorly documented, requiring manual work from domain experts to find and fix any data issues. Limited Connectivity \u2014 Within systems, relationships between data are constrained by what each system supports. Across systems, they are effectively non-existent.\n\nTo address these challenges, we need new foundations that allow us to define a model once, at the conceptual level, and reuse those definitions everywhere. But it isn\u2019t enough to just document concepts; we need to connect them to real systems and data. And more than just connect, we have to project those definitions outward, generating schemas and enforcing consistency across systems. The conceptual model must become part of the control plane.\n\nThese were the core ideas that led us to build UDA.\n\nIntroducing UDA\n\nUDA (Unified Data Architecture) is the foundation for connected data in Content Engineering. It enables teams to model domains once and represent them consistently across systems \u2014 powering automation, discoverability, and semantic interoperability.\n\nUsing UDA, users and systems can:\n\nRegister and connect domain models \u2014 formal conceptualizations of federated business domains expressed as data.\n\nWhy? So everyone uses the same official definitions for business concepts, which avoids confusion and stops different teams from rebuilding similar models in conflicting ways.\n\nCatalog and map domain models to data containers, such as GraphQL type resolvers served by a Domain Graph Service, Data Mesh sources, or Iceberg tables, through their representation as a graph.\n\nWhy? To make it easy to find where the actual data for these business concepts lives (e.g., in which specific database, table, or service) and understand how it\u2019s structured there.\n\nTranspile domain models into schema definition languages like GraphQL, Avro, SQL, RDF, and Java, while preserving semantics.\n\nWhy? To automatically create consistent technical data structures (schemas) for various systems directly from the domain models, saving developers manual effort and reducing errors caused by out-of-sync definitions.\n\nMove data faithfully between data containers, such as from federated GraphQL entities to Data Mesh (a general purpose data movement and processing platform for moving data between Netflix systems at scale), Change Data Capture (CDC) sources to joinable Iceberg Data Products.\n\nWhy? To save developer time by automatically handling how data is moved and correctly transformed between different systems. This means less manual work to configure data movement, ensuring data shows up consistently and accurately wherever it\u2019s needed.\n\nDiscover and explore domain concepts via search and graph traversal.\n\nWhy? So anyone can more easily find the specific business information they\u2019re looking for, understand how different concepts and data are related, and be confident they are accessing the correct information.\n\nProgrammatically introspect the knowledge graph using Java, GraphQL, or SPARQL.\n\nWhy? So developers can build smarter applications that leverage this connected business information, automate more complex data-dependent workflows, and help uncover new insights from the relationships in the data.\n\nThis post introduces the foundations of UDA as a knowledge graph, connecting domain models to data containers through mappings, and grounded in an in-house metamodel, or model of models, called Upper. Upper defines the language for domain modeling in UDA and enables projections that automatically generate schemas and pipelines across systems.\n\nThe same domain model can be connected to semantically equivalent data containers in the UDA knowledge graph.\n\nThis post also highlights two systems that leverage UDA in production:\n\nPrimary Data Management (PDM) is our platform for managing authoritative reference data and taxonomies. PDM turns domain models into flat or hierarchical taxonomies that drive a generated UI for business users. These taxonomy models are projected into Avro and GraphQL schemas, automatically provisioning data products in the Warehouse and GraphQL APIs in the Enterprise Gateway.\n\nSphere is our self-service operational reporting tool for business users. Sphere uses UDA to catalog and relate business concepts across systems, enabling discovery through familiar terms like \u2018actor\u2019 or \u2018movie.\u2019 Once concepts are selected, Sphere walks the knowledge graph and generates SQL queries to retrieve data from the warehouse, no manual joins or technical mediation required.\n\nUDA is a Knowledge Graph\n\nUDA needs to solve the data integration problem. We needed a data catalog unified with a schema registry, but with a hard requirement for semantic integration. Connecting business concepts to schemas and data containers in a graph-like structure, grounded in strong semantic foundations, naturally led us to consider a knowledge graph approach.\n\nWe chose RDF and SHACL as the foundation for UDA\u2019s knowledge graph. But operationalizing them at enterprise scale surfaced several challenges:\n\nRDF lacked a usable information model. While RDF offers a flexible graph structure, it provides little guidance on how to organize data into named graphs, manage ontology ownership, or define governance boundaries. Standard follow-your-nose mechanisms like owl:imports apply only to ontologies and don\u2019t extend to named graphs; we needed a generalized mechanism to express and resolve dependencies between them.\n\nWhile RDF offers a flexible graph structure, it provides little guidance on how to organize data into named graphs, manage ontology ownership, or define governance boundaries. Standard follow-your-nose mechanisms like owl:imports apply only to ontologies and don\u2019t extend to named graphs; we needed a generalized mechanism to express and resolve dependencies between them. SHACL is not a modeling language for enterprise data. Designed to validate native RDF, SHACL assumes globally unique URIs and a single data graph. But enterprise data is structured around local schemas and typed keys, as in GraphQL, Avro, or SQL. SHACL could not express these patterns, making it difficult to model and validate real-world data across heterogeneous systems.\n\nDesigned to validate native RDF, SHACL assumes globally unique URIs and a single data graph. But enterprise data is structured around local schemas and typed keys, as in GraphQL, Avro, or SQL. SHACL could not express these patterns, making it difficult to model and validate real-world data across heterogeneous systems. Teams lacked shared authoring practices. Without strong guidelines, teams modeled their ontologies inconsistently breaking semantic interoperability. Even subtle differences in style, structure, or naming led to divergent interpretations and made transpilation harder to define consistently across schemas.\n\nWithout strong guidelines, teams modeled their ontologies inconsistently breaking semantic interoperability. Even subtle differences in style, structure, or naming led to divergent interpretations and made transpilation harder to define consistently across schemas. Ontology tooling lacked support for collaborative modeling. Unlike GraphQL Federation, ontology frameworks had no built-in support for modular contributions, team ownership, or safe federation. Most engineers found the tools and concepts unfamiliar, and available authoring environments lacked the structure needed for coordinated contributions.\n\nTo address these challenges, UDA adopts a named-graph-first information model. Each named graph conforms to a governing model, itself a named graph in the knowledge graph. This systematic approach ensures resolution, modularity, and enables governance across the entire graph. While a full description of UDA\u2019s information infrastructure is beyond the scope of this post, the next sections explain how UDA bootstraps the knowledge graph with its metamodel and uses it to model data container representations and mappings.\n\nUpper is Domain Modeling\n\nUpper is a language for formally describing domains \u2014 business or system \u2014 and their concepts. These concepts are organized into domain models: controlled vocabularies that define classes of keyed entities, their attributes, and their relationships to other entities, which may be keyed or nested, within the same domain or across domains. Keyed concepts within a domain model can be organized in taxonomies of types, which can be as complex as the business or the data system needs them to be. Keyed concepts can also be extended from other domain models \u2014 that is, new attributes and relationships can be contributed monotonically. Finally, Upper ships with a rich set of datatypes for attribute values, which can also be customized per domain.\n\nThe graph representation of the onepiece: domain model from our UI. Depicted here you can see how Characters are related to Devil Fruit, and that each Devil Fruit has a type.\n\nUpper domain models are data. They are expressed as conceptual RDF and organized into named graphs, making them introspectable, queryable, and versionable within the UDA knowledge graph. This graph unifies not just the domain models themselves, but also the schemas they transpile to \u2014 GraphQL, Avro, Iceberg, Java \u2014 and the mappings that connect domain concepts to concrete data containers, such as GraphQL type resolvers served by a Domain Graph Service, Data Mesh sources, or Iceberg tables, through their representations. Upper raises the level of abstraction above traditional ontology languages: it defines a strict subset of semantic technologies from the W3C tailored and generalized for domain modeling. It builds on ontology frameworks like RDFS, OWL, and SHACL so domain authors can model effectively without even needing to learn what an ontology is.\n\nUDA domain model for One Piece. Link to full definition.\n\nUpper is the metamodel for Connected Data in UDA \u2014 the model for all models. It is designed as a bootstrapping upper ontology, which means that Upper is self-referencing, because it models itself as a domain model; self-describing, because it defines the very concept of a domain model; and self-validating, because it conforms to its own model. This approach enables UDA to bootstrap its own infrastructure: Upper itself is projected into a generated Jena-based Java API and GraphQL schema used in GraphQL service federated into Netflix\u2019s Enterprise GraphQL gateway. These same generated APIs are then used by the projections and the UI. Because all domain models are conservative extensions of Upper, other system domain models \u2014 including those for GraphQL, Avro, Data Mesh, and Mappings \u2014 integrate seamlessly into the same runtime, enabling consistent data semantics and interoperability across schemas.\n\nTraversing a domain model programmatically using the Java API generated from the Upper metamodel.\n\nData Container Representations\n\nData containers are repositories of information. They contain instance data that conform to their own schema languages or type systems: federated entities from GraphQL services, Avro records from Data Mesh sources, rows from Iceberg tables, or objects from Java APIs. Each container operates within the context of a system that imposes its own structural and operational constraints.\n\nA Data Mesh source is a data container.\n\nData container representations are data. They are faithful interpretations of the members of data systems as graph data. UDA captures the definition of these systems as their own domain models, the system domains. These models encode both the information architecture of the systems and the schemas of the data containers within. They provide a blueprint for translating the systems into graph representations.", "label": 0}
{"title": "Enhancing the Python ecosystem with type checking and free threading", "url": "https://engineering.fb.com/2025/05/05/developer-tools/enhancing-the-python-ecosystem-with-type-checking-and-free-threading/", "content": "Meta and Quansight have improved key libraries in the Python Ecosystem. There is plenty more to do and we invite the community to help with our efforts.\n\nWe\u2019ll look at two key efforts in Python\u2019s packaging ecosystem to make packages faster and easier to use:\n\n\ud83d\ude80 Unlock performance wins for developers through free-threaded Python \u2013 where we leverage Python 3.13\u2019s support for concurrent programming (made possible by removing the Global Interpreter Lock (GIL)).\n\n\u2705 Increase developer velocity in the IDE with improved type annotations.\n\nEnhancing typed Python in the Python scientific stack\n\nType hints, introduced in Python 3.5 with PEP-484, allow developers to specify variable types, enhancing code understanding without affecting runtime behavior. Type-checkers validate these annotations, helping prevent bugs and improving IDE functions like autocomplete and jump-to-definition. Despite their benefits, adoption is inconsistent across the open source ecosystem, with varied approaches to specifying and maintaining type annotations.\n\nThe landscape of open source software is fractured with respect to how type annotations are specified, maintained, and distributed to end users. Some projects have in-line annotations (types directly declared in the source code directly), others keep types in stub files, and many projects have no types at all, relying on third party repositories such as typeshed to provide community-maintained stubs. Each approach has its own pros and cons, but application and maintenance of them has been inconsistent.\n\nMeta and Quansight are addressing this inconsistency through:\n\nDirect contributions: We have improved the type coverage for pandas-stubs and numpy, and are eager to expand the effort to more packages. Community engagement: Promoting type annotation efforts to encourage community involvement, listen to feedback and create actionable ways to improve the ecosystem. Tooling and automation: Developing tools to address common challenges adding types and keeping the types up-to-date with the source code.\n\nImproved type annotations in pandas\n\nTL;DR: Pandas is the second most downloaded package from the Python scientific stack. We improved pandas-stubs package type annotation coverage from 36% to over 50%.\n\nBackground\n\nThe pandas community maintains its own stubs in a separate repository, which must be installed to obtain type annotations. While these stubs are checked separately from the source code, it allows the community to use types with their own type checking and IDE.\n\nImproving type coverage\n\nWhen we began our work in pandas-stubs, coverage was around 36%, as measured by the percentage of parameters, returns, and attributes that had a complete type annotation (the annotation is present and all generics have type arguments). After several weeks of work and about 30 PRs, type completeness is now measured at over 50%. The majority of our contributions involved adding annotations to previously-untyped parameters, adding type arguments to raw generic types, and removing deprecated/undocumented interfaces. We also improved several inaccurate annotations and updated others to match the inline annotations in the pandas source code.\n\nKey introductions\n\nTwo key introductions significantly increased coverage:\n\nReplacing raw Series types with UnknownSeries , a new type aliased to Series[Any] . When applied to return type annotations, this reduces the number of type checker false-positives when the function is called.\n\nImproving types of core Dataframe operations like insert, combine, replace, transpose, and assign, as well as many timestamp and time-zone related APIs.\n\nTooling development\n\nIn addition to improving coverage directly, we developed tooling to catalog public interfaces missing annotations. We also augmented our tools for measuring type coverage to handle the situation where stubs are distributed independently, rather than being packaged into the core library wheel.\n\nWhat is free-threaded Python ?\n\nFree-threaded Python (FTP) is an experimental build of CPython that allows multiple threads to interact with the VM in parallel. Previously, access to the VM required holding the global interpreter lock (GIL), thereby serializing execution of concurrently running threads. With the GIL becoming optional, developers will be able to take full advantage of multi-core processors and write truly parallel code.\n\nBenefits of free-threaded Python\n\nThe benefits of free-threaded Python are numerous:\n\nTrue parallelism in a single process : With the GIL removed, developers can write Python code that takes full advantage of multi-core processors without needing to use multiple processes. CPU-bound code can execute in parallel across multiple cores.\n\nImproved performance: By allowing multiple threads to execute Python code simultaneously, work can be effectively distributed across multiple threads inside a single process.\n\nSimplified concurrency: Free-threading provides developers with a more ergonomic way to write parallel programs in Python. Gone are the days of needing to use multiprocessing.Pool and/or resorting to custom shared memory data structures to efficiently share data between worker processes.\n\nGetting Python\u2019s ecosystem ready for FTP\n\nThe ecosystem of Python packages must work well with free-threaded Python in order for it to be practically useful; application owners can\u2019t use free-threading unless their dependencies work well with it. To that end, we have been taking a \u201cbottoms up\u201d approach to tackle the most difficult/popular packages in the ecosystem. We\u2019ve added free-threading support to many of the most popular packages used for scientific computing (e.g. numpy, scipy, scikit-learn) and language bindings (e.g. Cython, nanobind, pybind, PyO3).\n\nJust getting started\n\nTogether, we made substantial progress in improving type annotations and free-threading compatibility in Python libraries. We couldn\u2019t have done it without the Python community and are asking others to join our efforts. Whether it\u2019s further updates to the type annotations or preparing your code for FTP, we value your help moving the Python ecosystem forward!\n\nTo learn more about Meta Open Source, visit our open source site, subscribe to our YouTube channel, or follow us on Facebook, Threads, X and LinkedIn.", "label": 0}
{"title": "How we use GitHub to be more productive, collaborative, and secure", "url": "https://github.blog/engineering/how-we-use-github-to-be-more-productive-collaborative-and-secure/", "content": "It\u2019s that time of year where we\u2019re all looking back at what we\u2019ve accomplished and thinking ahead to goals and plans for the calendar year to come. As part of GitHub Universe, I shared some numbers that provided a window into the work our engineering and security teams drive each day on behalf of our community, customers, and Hubbers. As someone who loves data, it\u2019s not just fun to see how we operate GitHub at scale, but it\u2019s also rewarding to see how this work contributes to our vision to be the home for all developers\u2013which includes our own engineering and security teams.\n\nOver the course of the past year, GitHub staff made millions of commits across all of our internal repositories. That\u2019s a ton of branches, pull requests, Issues, and more. We processed billions of API requests daily. And we ran tens of thousands of production deployments across the internal apps that power GitHub\u2019s services. If you do the math, that\u2019s hundreds of deploys per day.\n\nGitHub is big. But the reality is, no matter your size, your scale, or your stage, we\u2019re all dealing with the same questions. Those questions boil down to how to optimize for productivity, collaboration, and, of course, security.\n\nIt\u2019s a running joke internally that you have to type \u201cGitHub\u201d three times to get to the monolith. So, let\u2019s take a look at how we at GitHub (1) use GitHub (2) to build the GitHub (3) you rely on.\n\nProductivity\n\nGitHub\u2019s cloud-powered experiences, namely Codespaces and GitHub Copilot, have been two of the biggest game changers for us in the past few years.\n\nCodespaces\n\nIt\u2019s no secret that local development hasn\u2019t evolved much in the past decade. The github/github repository, where much of what you experience on GitHub.com lives, is fairly large and took several minutes to clone even on a good network connection. Combine this with setting up dependencies and getting your environment the way you like it, spinning up a local environment used to take 45 minutes to go from checkout to a built local developer environment.\n\nBut now, with Codespaces, a few clicks and less than 60 seconds later, you\u2019re in a working development environment that\u2019s running on faster hardware than the MacBook I use daily.\n\nHeating my home office in the chilly Midwest with my laptop doing a local build was nice, but it\u2019s a thing of the past. Moving to Codespaces last year has truly impacted our day-to-day developer experience, and we\u2019re not looking back.\n\nGitHub Copilot\n\nWe\u2019ve been using GitHub Copilot for more than a year internally, and it still feels like magic to me every day. We recently published a study that looked at GitHub Copilot performance across two groups of developers\u2013one that used GitHub Copilot and one that didn\u2019t. To no one\u2019s surprise, the group that used GitHub Copilot was able to complete the same task 55% faster than the group that didn\u2019t have GitHub Copilot.\n\nGetting the job done faster is great, but the data also provided incredible insight into developer satisfaction. Almost three-quarters of the developers surveyed said that GitHub Copilot helped them stay in the flow and spend more time focusing on the fun parts of their jobs. When was the last time you adopted an experience that made you love your job more? It\u2019s an incredible example of putting developers first that has completely changed how we build here at GitHub.\n\nCollaboration\n\nAt GitHub, we\u2019re remote-first and we have highly distributed teams, so we prioritize discoverability and how we keep teams up-to-date across our work. That\u2019s where tools like Issues and projects come into play. They allow us to plan, track, and collaborate in a centralized place that\u2019s right next to the code we\u2019re working on.\n\nIncorporating projects across our security team has made it easier for us to not only track our work, but also to help people understand how their work fits into the company\u2019s broader mission and supports our customers.\n\nProjects gives us a big picture view of our work, but what about the more tactical discovery of a file, function, or new feature another team is building? When you\u2019re working on a massive 15-year-old codebase (looking at you, GitHub), sometimes you need to find code that was written well before you even joined the company, and that can feel like trying to find a needle in a haystack.\n\nSo, we\u2019ve adopted the new code search and code view, which has helped our developers quickly find what they need without losing velocity. This improved discoverability, along with the enhanced organization offered by Issues and projects, has had huge implications for our teams in terms of how we\u2019ve been able to collaborate across groups.\n\nShifting security left\n\nLike we saw when we looked at local development environments, the security industry still struggles with the same issues that have plagued us for more than a decade. Exposed credentials, as an example, are still the root cause for more than half of all data breaches today. Phishing is still the best, and cheapest, way for an adversary to get into organizations and wreak havoc. And we\u2019re still pleading with organizations to implement multi-factor authentication to keep the most basic techniques from bad actors at bay.\n\nIt\u2019s time to build security into everything we do across the developer lifecycle.\n\nThe software supply chain starts with the developer. Normalizing the use of strong authentication is one of the most important ways that we at GitHub, the home of open source, can help defend the entire ecosystem against supply chain attacks. We enforce multi-factor authentication with security keys for our internal developers, and we\u2019re requiring that every developer who contributes software on GitHub.com enable 2FA by the end of next year. The closer we can bring our security and engineering teams together, the better the outcomes and security experiences we can create together.\n\nAnother way we do that is by scaling the knowledge of our security teams with tools like CodeQL to create checks that are deployed for all our developers, protecting all our users. And because the CodeQL queries are open source, the vulnerability patterns shared by security teams at GitHub or by our customers end up as CodeQL queries that are then available for everyone. This acts like a global force multiplier for security knowledge in the developer and security communities.\n\nSecurity shouldn\u2019t be gatekeeping your teams from shipping. It should be the process that enables them to ship quickly\u2013remember our hundreds of production deployments per day?\u2013and with confidence.\n\nBig, small, or in-between\n\nAs you see, GitHub has the same priorities as any other development team out there.\n\nIt doesn\u2019t matter if you\u2019re processing billions of API requests a day, like we are, or if you\u2019re just starting on that next idea that will be launched into the world.\n\nThese are just a few ways over the course of the last year that we\u2019ve used GitHub to build our own platform securely and improve our own developer experiences, not only to be more productive, collaborative, and secure, but to be creative, to be happier, and to build the best work of our lives.\n\nTo learn more about how we use GitHub to build GitHub, and to see demos of the features highlighted here, take a look at this talk from GitHub Universe 2022.\n\nNotes\n\nTags:", "label": 0}
{"title": "How we built it: Stripe Radar", "url": "https://stripe.com/blog/how-we-built-it-stripe-radar", "content": "As an engineer on Stripe\u2019s fraud prevention team, I obsess about a single moment that lasts just a fraction of a second. It begins when someone clicks \u201cpurchase,\u201d and it ends when their transaction is confirmed.\n\nIn that brief interval, Stripe Radar goes to work. Radar is Stripe\u2019s fraud prevention solution. It assesses more than 1,000 characteristics of a potential transaction in order to determine the likelihood that it\u2019s fraudulent, letting good transactions through and either blocking risky transactions or diverting them to additional security checks. It makes this decision, accurately, in less than 100 milliseconds. Out of the billions of legitimate payments made on Stripe, Radar incorrectly blocks just 0.1%.\n\nOnline payment fraud is a hard problem to solve. Any effective tool needs to be accurate, fast, and inexpensive to run for each transaction. It needs to balance blocking bad transactions against false positives (good payments that are blocked), which hurt consumers and our users\u2019 bottom lines. The challenge is compounded by the fact that fraud is rare\u2014on the order of 1 out of every 1,000 payments.\n\nTo identify fraudulent transactions, we rely on the breadth of the Stripe network\u2014our biggest strength. We\u2019ve done so by improving our machine learning (ML) architecture while enhancing the way we communicate with users about the reasons behind fraud decisions. In this post, we want to share what makes Radar so powerful and take you through some of the key decisions we\u2019ve made\u2014and lessons we\u2019ve learned\u2014over the almost seven years we\u2019ve been building it.\n\nLesson 1: Don\u2019t get too comfortable with your ML architecture\n\nWe started with relatively simple ML models (e.g., logistic regression) and over time have advanced to more complex ones (e.g., deep neural networks), as the Stripe network has grown and ML technology has advanced. With each architectural jump, we have observed an equivalent leap-size improvement in model performance.\n\nOur most recent architecture evolution occurred in mid-2022 when we migrated from an ensemble \u201cWide & Deep model,\u201d composed of an XGBoost model and a deep neural network (DNN), to a pure DNN-only model. The result was a model that trains faster, scales better, and is more adaptable to the most cutting-edge ML techniques.\n\nThe previous architecture combined the power of memorization (the wide part, powered by XGBoost) with generalization (the deep part, powered by a DNN). It worked well, but limited the rate at which we could improve. XGBoost was incompatible at scale with more advanced ML techniques we wanted to take advantage of (e.g., transfer learning, embeddings, long training times) and also slowed the rate at which we could retrain the model because an XGBoost model is not very parallelizable\u2014which inhibited the experimentation velocity of the many engineers who worked on the model each day.\n\nWe could have just removed the XGBoost component, but that would have caused a 1.5% drop in recall\u2014an unacceptably large regression in performance. While XGBoost is not a deep-learning method or a cutting-edge technique these days, it still provided unique value to our model\u2019s performance. To replace it, we looked for ways to build a DNN-only architecture that added the memorization power we\u2019d been getting from XGBoost, without compromising the DNN\u2019s ability to generalize.\n\nA straightforward way of improving both memorization and generalization is to increase the DNN\u2019s size\u2014both its depth and width. However, achieving a more-performant model wasn\u2019t as easy as that.\n\nIncreasing the model\u2019s size immediately improved the representational capacity of the model to learn features at both the abstract level (e.g., payment velocity and \u201cunusual volume on a card\u201d) and the fine-grained level (e.g., correlations between features). However, increasing depth too much ran the risk of overfitting, causing the model to memorize random noise in the features. So, in order to build a DNN-only architecture, we had to find the sweet spot that maximized a representational capacity to learn various levels while remaining resistant to overfitting.\n\nWe decided to read up on popular publications about DNN architecture and adopted a multi-branch DNN-only architecture inspired by ResNeXt. ResNeXt\u2019s architecture adopts a \u201cNetwork-in-Neuron\u201d strategy. It splits a computation into distinct threads, or branches, where a branch can be thought of as a small network. The outputs from the branches are then summed to produce a final output. Aggregating branches has the benefit of enriching the learned features by expanding a new dimension of feature representation. It does this in a way that is more effective than the brute-force approach of merely increasing depth or width to improve accuracy.\n\nBy removing the XGBoost component of the architecture, we reduced the time to train our model by over 85% (to less than two hours). Experiments that previously required running jobs late into the night could now be completed multiple times in a single working day, a massive shift in our ability to prototype new ideas. The improvements were a good reminder to not get too comfortable with the way we were currently doing ML and to ask ourselves: If we were starting over today, what kind of model would we build?\n\nAsking those questions is allowing us to take on even more ambitious initiatives for our next year of work. These include incorporating more advanced ML techniques like transfer learning, embeddings, and multi-task learning, all of which we are actively exploring in 2023.\n\nLesson 2: Never stop searching for new ML features\n\nIn addition to evolving our model architectures, we also want to ensure our models are incorporating the richest signals. By carefully noting the common behaviors of fraud attempts, Radar has been able to compile a deep understanding of fraudulent activity and trends. This gives Radar an important advantage when put to work: Each increase in the size of Radar\u2019s training data set creates outsized improvements in model quality, which wasn\u2019t the case with XGBoost.\n\nOne of the biggest levers we have to make model improvements is through feature engineering. Some features could likely have an outsized impact on model performance, but first we need to identify and implement them. To do this effectively, we\u2019ve created several processes to enable ML engineers.\n\nWe review past fraud attacks in exacting detail, building investigation reports that attempt to get into the minds of the fraudulent actors. We look for signals in the payments, like a common pattern for throwaway emails (e.g., 123test@cactuspractice.com) that might be used by fraudulent actors to quickly set up multiple accounts. We then broaden our search across the Stripe network to look for correlations in timing and signals that could connect to previous fraud attacks. Every week, the Radar team also meets to discuss new fraud trends that emerge from research into activity on the dark web.\n\nWe gather all of this information and ideate features that target the specific contours of each attack. We come up with a prioritized list, quickly implement each feature, then prototype each one to understand the impact on our model\u2019s performance.\n\nSometimes we strike gold. Other times, even our most promising features don\u2019t pan out. This happened once when we introduced a Boolean feature capturing whether the business was currently under a distributed fraud attack. This feature didn\u2019t improve our model\u2019s performance as much as we\u2019d anticipated. As it turned out, our ML was already incorporating these patterns, even though we never expected it to. This reflects the fact that the current version of Radar is built on top of years of work by many generations of engineers.\n\nBesides developing new features, another method we explore for increasing model performance is increasing the size of our training data. With the success of ML models like ChatGPT, and large language models generally, we wanted to see if we could achieve a similar feat with Radar: Could we start with a relatively simple DNN-only architecture and get large improvements in model performance just by increasing the amount of training data?\n\nThe primary impediment to doing this was that the time to train increases linearly with the size of the training data. But thanks to the training-speed improvements we made when we switched to a DNN-only architecture, this was less of an issue.\n\nWe ran some experiments using more transaction data and got encouraging results: We made a 10x increase in training transaction data and still found significant model improvements. We\u2019re currently working on a 100x version to generalize the results even further.\n\nIn a future post, we will dive deeper into new techniques we\u2019re exploring to further use the power of the Stripe network and our ability to apply these insights to fight fraud, even after a payment has already occurred.\n\nLesson 3: Explanation matters as much as detection\n\nBuilding a great fraud-detection product is about more than just identifying fraud. There\u2019s a large personal dimension to it, too. When a good transaction is flagged\u2014or a fraudulent one gets through\u2014our users want to know why, because false positives hurt their bottom line and frustrate their customers. Explaining fraud decisions is an area in which we\u2019ve made a lot of investments in recent years.\n\nAnd it\u2019s a challenge. All ML models are black boxes to an extent, and deep neural networks even more so than other types of models. It\u2019s hard to explain to users why Radar scores transactions the way it does. This is another tradeoff we came to accept when deciding to use DNNs over simpler, more traditional ML techniques. But our engineers know the system well and have developed a range of ways to help users understand what\u2019s going on.\n\nIn 2020 we built our risk insights feature, which lets users see which features of a transaction contributed to a transaction being declined. These can include whether the cardholder\u2019s name matches the provided email and the number of cards previously associated with an IP address. A high number of cards may indicate suspicious behavior, such as a bad actor trying out multiple stolen credit cards. However, there may also be legitimate reasons for this, and our model evaluates this feature in the context of all our signals, understanding the correlations that may exist between them to accurately distinguish between fraudulent and good payments.\n\nRecent improvements to risk insights include displaying maps to users with the locations of purchase and shipping addresses and using Elasticsearch to quickly share related transactions, which further helps users put a specific decline in context.\n\nIn addition to providing users with insight into fraud decisions, we have been working on more sophisticated techniques for gaining deeper understanding of our ML model. This tooling includes a simple table view that displays the exact features that contributed the most to raising and lowering a transaction\u2019s fraud score. Our engineers are actively using these solutions internally to debug support cases, and we are working on plans for sharing these insights with our users as well.\n\nExplaining Radar\u2019s ML outcomes as clearly as possible helps users understand the relative risk of a given payment, which fraud signals may have contributed to that risk score, and how a given payment compares to others. They can then take actions to improve the quality of data they are sending (in order to generate more accurate fraud decisions) or create custom allow or block rules to tailor Radar for their specific business needs.\n\nEvolving strategies, constant focus\n\nRadar is a very different product than it was when we started. We\u2019ve overhauled the models we use, the way we employ transaction data from the Stripe network, and the way we interact with users. Over that same period fraud patterns have changed considerably, too, from primarily stolen credit card fraud to a growing mix of traditional card fraud and high-velocity card testing attacks today.\n\nBut in the ways that matter most, the goals of the Radar team are the same. We\u2019re still working to create an environment in which businesses and customers can transact with confidence, and we\u2019re still focused on optimizing that brief moment we hope customers don\u2019t even notice: the last step in a checkout, the split second we have to detect fraud before a transaction is confirmed.\n\nWe\u2019re excited to continue innovating on ML to solve hard, important problems. If you are, too, consider joining our engineering team.", "label": 0}
{"title": "Open-sourcing Pyrefly: A faster Python type checker written in Rust", "url": "https://engineering.fb.com/2025/05/15/developer-tools/open-sourcing-pyrefly-a-faster-python-type-checker-written-in-rust/", "content": "Back in 2017, engineers at Meta sought to create a type checker for Instagram\u2019s typed Python codebase. Years later, as the type system continued to evolve, that type checker eventually became Pyrefly.\n\nPyrefly is a new type checker and IDE experience for Python, written with Rust, and now available for the entire Python community to use! It\u2019s open-source, supports both CLI usage and IDE integration. and is designed to help you catch errors before runtime in Python codebases of any size.\n\nOn this episode of the Meta Tech Podcast, Pascal Hartig sits down with Maggie, Rebecca, and Neil \u2014 some of the team behind Pyrefly \u2014 to discuss this latest release from Meta and how they built an incremental type checker that scales to mono repositories.\n\nDownload or listen to the episode below:\n\nYou can also find the episode wherever you get your podcasts, including:\n\nThe Meta Tech Podcast is a podcast, brought to you by Meta, where we highlight the work Meta\u2019s engineers are doing at every level \u2013 from low-level frameworks to end-user features.\n\nSend us feedback on Instagram, Threads, or X.\n\nAnd if you\u2019re interested in learning more about career opportunities at Meta visit the Meta Careers page.\n\nLinks", "label": 0}
{"title": "The Multilingualness of 'Call Me By Your Name'", "url": "https://lifeofpablo.com/blog/the-multilingualness-of-call-me-by-your-name", "content": "The Multilingualness of 'Call Me By Your Name'\n\nThis post was written in English (en_US).\n\nI've seen the movie, Call Me By Your Name featuring Arnie Hammer and Timoth\u00e9e Chalamet a few times since it came out. Not only is it a queer film which I love but a film that includes multilingual characters. What I melt inside of happiness when multiple languages are spoken in a movie. We hear English, Italian and French. One minute you hear French, then the same character will switch to English or Italian often times in the same scene. I find this very beautiful as someone who can speak multiple romantic languages. The code switching in my opinion is pretty good. Not to brag but it helps to know a few romantic languages to understand the others. I speak English, Spanish and French, fluently. I can for the most part understand Italian since there is so much cross over and there is so much cultural cross-context that I understand. I honestly turn off the subtitles when I watch this movie and others because I don't feel the need to see the translation. The movie is a lot more meaningful to me because I can understand the subtleties of the languages and how people express themselves in such languages.\n\nMovies and TV shows that can switch between languages are so cool!", "label": 1}
{"title": "Ice Skating (and I didn't fall)", "url": "https://lifeofpablo.com/blog/ice-skating-and-i-didn-t-fall", "content": "Ice Skating (and I didn't fall)\n\nPablo looking at the tree\n\nThis post was written in English (en_US).\n\nI went ice skating for the first time in years. The last time I ice skated was back in Omaha. It's something I don't do often but I've been having an itch the last few years. Living in Downtown San Francisco, there's an Ice skating rink at Union Square that I pass by all the time. It's been one of those activities that I tell myself, \"I need to do this before they tear it down!\" I keep pondering everytime I pass it, then I forget because I have to get to my destination.\n\nThis weekend, I finally did it! I went with my friend Tim, who was visiting for the weekend. Why not do something interesting with a friend? I was very happy that I went with my friend!\n\nMy biggest achievement is I didn't fall on my face! I kept my balance.\n\nThe lesson I learned from this is to simply do something and not ponder it too late! I need to get into the habit of doing activities on my own (and make them into artist dates). I just got to do things and enjoy time with myself.", "label": 1}
{"title": "To design and develop an interactive globe", "url": "https://stripe.com/blog/globe", "content": "As humans, we\u2019re driven to build models of our world.\n\nA traditional globemaker molds a sphere, mounts it on an axle, balances it with hidden weights, and precisely applies gores\u2014triangular strips of printed earth\u2014to avoid overlap and align latitudes. Cartographers face unenviable trade-offs when making maps. They can either retain the shape of countries, but warp their size\u2014or maintain the size of countries, but contort their shape. In preserving one aspect of our world, they distort another.\n\nThese are terrestrial globe gores reissued by Giuseppe di Rossi in 1615.\n\nAs visual designers and software engineers, we\u2019re modeling a piece of the world every time we build software. In some cases, it\u2019s the entire world\u2014and that digital world is animated and interactive. There are tools that render 3D objects on the web, but they\u2019re considered sorcery by many. And conjuring that magic doesn\u2019t come without sweat. In WebGL, displaying a single triangle\u2014like a globemaker\u2019s gore\u2014with no lights, textures, interactivity, or motion requires 50+ lines of code.\n\nFor the new stripe.com, we built a 1:40 million-scale, interactive 3D model of the earth. We wanted to convey the interconnected nature of the internet economy and the global scale of our service, while acknowledging how much ground is yet to be covered. Despite expansion to 40 countries and payment processing from 195 countries, we grapple with the complexity of cross-border operations and expansion every day.\n\nWe set out to build a globe that inspires a sense of awe, invites people to explore, and conceals details for discovery. Along the way, we evaluated existing tools, designed our own solution, solved four interesting technical challenges, and improved the way we collaborate. Here\u2019s what we learned.\n\nWays to build the world\n\nIt wasn\u2019t a given that we\u2019d build an interactive 3D globe on our landing page. We designed our first version of the globe to communicate nuanced data about the amount of online, cross-border commerce happening between each country. For this reason, it includes extra visual details like country borders. For our landing page, the goal of the globe was to capture our global scale and bring a visual metaphor to life. A week before launch, we had a nice animated map where the globe now sits but we didn\u2019t love it. Despite the impending release, an executive (it was Patrick) posed to us: what would you build if you had the time to do it the way you wish you could?\n\nWe decided on a globe\u2014and felt it was a better option for three reasons. First, using a sphere to display the earth takes up less than 20% of the screen area required to display the world in two dimensions. Second, a globe more accurately portrays the relative size, shape, and orientation of countries and bodies of water, even though visibility of the entire world at a glance is easier with a map. (More than \u00be of the globe is either hidden on the reverse hemisphere or obscured by its curvature.) Lastly, as an interactive experience, spinning a globe is much more satisfying than scanning a map.\n\nThe sphere occupies approximately 17% of the total area of the map.\n\nOnce we settled on a globe, we had to work out how to bring it to life.\n\nIf we had known precisely the globe we wanted to build, we\u2019d have been foolish not to hire GlobeKit. Instead, not knowing what we did not know, we decided to figure it out ourselves. The primary tools used to render 3D objects on the web, WebGL and GLSL shaders, can be daunting. Developers writing shaders can get by without deep knowledge of trigonometry and linear algebra, but a good understanding of these disciplines make 3D graphics development substantially easier.\n\nNone of us on the team considered ourselves 3D artists, so we leaned on each other, the internet, and friends to help solve technical problems. To start, the project\u2019s design lead created the best approximation of her vision of the globe in Photoshop. We naturally kept the globe\u2019s design fluid, making it easy to adopt better ideas as they emerged without feeling precious about what was discarded.\n\nWhen it became clear that writing our own 3D engine was out of scope, we decided to use Three.js. Three is an approachable layer for WebGL, which abstracts away much of its complexity behind a well-documented API. Originally ported from ActionScript (Flash) in 2010, Three helped us create rich 3D graphics that render in real-time in the browser without needing to define how light reflects on every pixel of every shape.\n\nTo render a Three.js scene, you need a renderer, a DOM element to render in, a scene, a camera, and a mesh with both material (fragment shader) and geometry (vertex shader).\n\nTen years after its release, Three.js matches\u2014or surpasses\u2014much of what was possible in Flash a decade ago. The most engaging interactive experiences on the web are now built with Three. The community\u2019s enthusiasm bears resemblance to the early days of ActionScript with the added benefit of running on mobile browsers and requiring no plugins. As Three.js and WebGL gain popularity, approachability, and support, the web is poised to embrace 3D en masse. Since WebGL is GPU-accelerated, it\u2019s capable of processing a surprising amount of continuous visual change without bottlenecking on the CPU even on lower-end consumer hardware. Finding the boundary between making our globe feel alive and crashing the browser would emerge as our greatest technical hurdle. But it wouldn\u2019t be the only one.\n\nGlobal issues\n\nLike the challenges we faced, our globe goes a few levels deep. It\u2019s composed of three distinct layers, despite appearing as a single surface. The base layer represents the oceans, and is a semi-transparent sphere with ~50 segments both horizontally and vertically. The second layer is another sphere textured with tens of thousands of twinkling dots. The outermost layer is made of animated arcs of color which travel from one pulsing disc to another, wrapping themselves around the two spheres. The arcs travel from any country where Stripe accepts payments to countries where businesses accept payments using Stripe.\n\nWe encountered several significant technical challenges along the way, each of which could have prevented us from realizing our vision. For the benefit of those generating their own interactive globes\u2014or similar complex 3D objects\u2014let\u2019s break a few of these challenges down.\n\nWe stacked each layer of the globe to produce a single visual surface.\n\nChallenge 1: Fill the surface with dots\n\nThe primary purpose of the outermost sphere\u2014a layer of tens of thousands of dots\u2014is to define continents. But as we removed the visual complexity of borders and animated each dot, they did more than communicate land masses; they made the globe feel alive. To make them work, we had two main requirements. The first step was to find a way to maintain consistent spacing between every row and column of dots, from pole to pole. Second, we needed to animate each dot individually.\n\nBefore landing on our final design, we tested and considered three different approaches to filling open space with a cluster of dots (as shown in the image below). Each attempt had its benefits and drawbacks.\n\nThis is the North Pole of the globe, mapped with dots using three different approaches.\n\nImage of evenly spaced dots. This approach is the simplest to create, but quickly becomes problematic. The dots fuse together as the circumference of each row shrinks as it nears the poles of the globe. As a static bitmap rather than geometry, we couldn\u2019t animate each dot individually without an overly complex shader. Image of unevenly spaced dots. We increased the width and horizontal spacing in the rows of dots. The image of nearly 80,000 dots places fewer, wider dots at the top and bottom. This tweak helps prevent the pinching and clumping of dots at the globe\u2019s poles which invalidated the previous approach. When mapped as a texture onto a sphere, this image creates a nearly uniform spacing of dots. At first, we created this image texture by hand, then generated an SVG with JavaScript. This option better met our visual goals but still didn\u2019t let us animate each dot. We assumed Three.js would work well with SVGs, but since every shape must be converted to a triangle, the complexity of the conversion deterred us from this approach. Programmatically-generated layer (vs. an image) The most straightforward way to animate individual dots is to generate them in a three-dimensional space. To do this, we reused the code from our SVG to generate rows of dots as geometry in Three.js. Each row includes a different number of dots, from zero at the poles to five hundred at the equator. We used a sine function to choose the number of dots for each row, plotted each dot, and applied the lookAt method to rotate each dot to face the center of the sphere. However, the number of dots jumped inconsistently along a few latitudes, creating harsh lines and an unnatural effect in the longitudinal columns.\n\nThe final attempt\u2014and right design\u2014utilized a sunflower pattern. Like a sunflower\u2019s pattern of seeds, the dots are a sequence of hexagons tightly coiled around latitudes from the top to the bottom of a sphere. Using the built in setFromSphericalCoords method we settled on this solution:\n\nThe spiral sunflower pattern was the winning design.\n\n// Create 60000 tiny dots and spiral them around the sphere. const DOT_COUNT = 60000; // A hexagon with a radius of 2 pixels looks like a circle const dotGeometry = new THREE.CircleGeometry(2, 5); // The XYZ coordinate of each dot const positions = []; // A random identifier for each dot const rndId = []; // The country border each dot falls within const countryIds = []; const vector = new THREE.Vector3(); for (let i = DOT_COUNT; i >= 0; i--) { const phi = Math.acos(-1 + (2 * i) / DOT_COUNT); const theta = Math.sqrt(DOT_COUNT * Math.PI) * phi; // Pass the angle between this dot an the Y-axis (phi) // Pass this dot\u2019s angle around the y axis (theta) // Scale each position by 600 (the radius of the globe) vector.setFromSphericalCoords(600, phi, theta); dotGeometry.lookAt(vector); // Move the dot to the newly calculated position dotGeometry.translate(vector.x, vector.y, vector.z); \u2026 } ~\n\nChallenge 2: Group the dots by country\n\nOn the globe at stripe.com, dots form continents and light up where Stripe is live. In a previous iteration, we grouped dots by country to indicate where Stripe operates. We decided to turn this feature off for the interactive globe on our landing page, but thought it might be worthwhile to share how we approached grouping dots by country.\n\nOnce we filled the globe with dots, the next step was to transform our layers of spheres into a globe by defining countries. Our first goal was to make dots appear only within the borders of countries where Stripe is live. Once that was done, we needed dots within those live countries to be targets for animation as a group.\n\nEach country where Stripe is live is given a unique color for identification.\n\nA teammate who had recently experimented with shaders for a gaming project brought inspiration to this challenge. He thought to encode a PNG image with a unique color for each country where Stripe is live (see above). We used the built-in canvas getImageData to give us the color of each pixel in the image. Then, we matched each color to an array of country colors, tagging every dot with a unique countryId before passing its coordinates to the shader for rendering. Now we could isolate the group of dots in any country and animate its color, opacity, and position in z-space.\n\nIn 2020, each country where Stripe is live is indicated by a unique color. The assumed drawback to generating all of the dots as individual geometry was the astronomical number of calculations required to animate the properties of 60,000 dots 60 times per second. Lucky for us, the earth\u2019s surface is mostly water. By only rendering geometry for countries where Stripe is live, we were able to reduce the geometry from 60,000 dots to ~20,000 dots passing a fraction of the data to the vertex shader. By pushing less data to the shader, we freed up rendering budget for use by other animations.\n\n// We assign a color to each ISO country code const COUNTRY_MAPPING = [ [0, '#99cc99', 'at'], [1, '#993333', 'au'], [2, '#cccc00', 'be'], \u2026 ]; // Load the color-coded image then get each pixel\u2019s color new ImageLoader().load('map.png', (mapImage) => { const imageData = getImageData(mapImage); }); dotGeometry.computeBoundingSphere(); const uv = pointToUV(dotGeometry.boundingSphere.center, this.position); const sample = sampleImage(uv, imageData); // If there is no color data, return and move to the next dot if (!sample[3]) return; // Create a dot if there is color data for (let i = 0; i < dotGeometry.faces.length; i++) { const face = dotGeometry.faces[i]; // Create the vertices which make up the face of each dot positions.push( dotGeometry.vertices[face.a].x, dotGeometry.vertices[face.a].y, dotGeometry.vertices[face.a].z, \u2026 // face.b, face.c ); const [countryId] = getCountryId(sample); countryIds.push(countryId, countryId, countryId); } // Convert RGB to Hex and look up the countryId by color function getCountryId([r, g, b, _]) { const hex = [r, g, b] .map((color) => color.toString(16).padStart(2, '0')) .join(''); const countryId = COUNTRY_MAPPING.find(([_, id]) => id === hex); return countryId; } ~\n\nChallenge 3: Animate it all\n\nAfter filling in the surface with dots and grouping them into countries, we needed to connect the dots to show how and where business is done globally. Our goal was to bring the globe to life, which meant adding animation. Early on, we knew we\u2019d need to get the globe spinning, each dot twinkling, and bend arcs between countries to indicate transaction patterns. We wanted visitors to be able to control and spin the globe.\n\nAround the time we started animating, we got a new teammate. In a past life, he had engineered the scrolling of the iconic Pencil by 53 site. In short order, he added animations for undulating, aurora borealis-like lights, made the globe rotate on page load, and spun the earth when the user scrolled the page. We handled the subtly twinkling dots and arcs with a custom fragment shader (and a lot of help from thebookofshaders.com) but the rest of the animation is vanilla JavaScript. requestAnimationFrame drives the motion of the arcs, the spinning of the globe, and the changing of colors.\n\n// Draw an arc between two coordinates ... constructor(start, end, radius) { super(); // Convert latitude/longitude to XYZ on the globe const start = toXYZ(start[0], start[1], radius); const endXYZ = toXYZ(end[0], end[1], radius); // D3 interpolates along the great arc that passes // through both the start and end point const d3Interpolate = geoInterpolate( [start[1], start[0]], [end[1], end[0]], ); const control1 = d3Interpolate(0.25); const control2 = d3Interpolate(0.75); // Set the arc height to half the distance between points const arcHeight = start.distanceTo(end) * 0.5 + radius; const controlXYZ1 = toXYZ(control1[1], control1[0], arcHeight); const controlXYZ2 = toXYZ(control2[1], control2[0], arcHeight); // CubicBezier allows for curves which travel half way // around the globe without penetrating the sphere const curve = new CubicBezierCurve3(start, controlXYZ1, controlXYZ2, end); // Arcs are curved tubes with 0.5px radius and 8 sides // Each curve is broken into 44 segments this.geometry = new THREE.TubeBufferGeometry(curve, 44, 0.5, 8); this.material = new THREE.ShaderMaterial({ // A custom fragment shader animates arc colors }); this.mesh = new THREE.Mesh(this.geometry, this.material); this.add(this.mesh); // Set the draw range to show only the first vertex this.geometry.setDrawRange(0, 1); this.drawAnimatedLine(); } drawAnimatedLine = () => { let drawRangeCount = this.geometry.drawRange.count; const timeElapsed = performance.now() - this.startTime; // Animate the curve for 2.5 seconds const progress = timeElapsed / 2500; // Arcs are made up of roughly 3000 vertices drawRangeCount = progress * 3000; if (progress < 0.999) { // Update the draw range to reveal the curve this.geometry.setDrawRange(0, drawRangeCount); requestAnimationFrame(this.drawAnimatedLine); } } ~\n\nChallenge 4: Make it performant\n\nEarly on, we discussed our expectations for the globe\u2019s performance on different browsers to frame our requirements. We boiled those expectations down to one requirement: all animation and scrolling effects had to perform at 60fps (to match the common device refresh rate of 60hz). If this condition couldn\u2019t be met, we were prepared to fall back to a static image. Thanks to GPU-acceleration of WebGL and some of the findings mentioned here, we never had to abandon our interactive globe.\n\nInitially, we ruled out mobile support. We assumed that scrolling and 3D animation would be too much for any machine, and that we\u2019d have to either accept some lag or reduced motion on both smaller and more underpowered machines, or settle for the fallback. But as we learned about the capabilities of GPUs, we kept raising our expectations. Most of what\u2019s possible in WebGL works on mobile without modification. We did make minor adjustments: during scroll, we pause all animations and debounce using Lodash so the globe spins without visual hiccups.\n\nA few days prior to launch, we tested the page on laptops without dedicated GPUs and reported that they were struggling to power a 5K display with the globe running fullscreen. We weren\u2019t willing to accept defeat by falling back to an image for this rare case. Instead, we cycled through all possible bottlenecks one by one. No matter how much we simplified the geometry, stopped animations, or killed lights and shaders, we couldn\u2019t smooth it out.\n\nOn a whim, we turned off the antialias parameter of the WebGL renderer. This one change not only fixed the issue on high-res monitors, but also improved performance of the animation and smoothness of scrolling on all devices, even those already running at 60fps. One might assume that removing antialiasing would make everything look pixelated. Since it only applies to the edges of the geometry, our image textures were still smooth and gradients and lighting were unaffected. Though pixelation occurs minimally on the arcs around the globe, the performance gain was significant enough to accept a tradeoff.\n\n// Turn off antialiasing for WebGL to improve performance this.renderer = new WebGLRenderer({ antialias: false, alpha: true }); \u2026 // Rotating the globe on scroll import throttle from 'lodash/throttle'; const SCROLL_EPSILON = 0.0016; const GLOBE_TRIGGER_TOP = window.innerHeight; \u2026 document.addEventListener('scroll', this.universalScrollHandler); \u2026 // Event handler: rotate the globe based on the current browser scroll position universalScrollHandler = throttle(this.scrollHandler.bind(this), 16); scrollHandler() { // Turns off all other animation this.isScrolling = true; this.oldScrollTop = this.scrollTop; this.scrollTop = document.scrollingElement.scrollTop || document.body.scrollTop; this.scrollDelta = this.oldScrollTop - this.scrollTop; const rotationDelta = this.scrollDelta * SCROLL_EPSILON; this.globeContainer.rotation.y += rotationDelta; // Once the browser scrolls past the globe on the page // stop all animations and move the globe off-screen if (GLOBE_TRIGGER_TOP < this.scrollTop) { this.globeOff = true; this.globeEl.style.transform = 'translateX(100vw)'; } else { this.globeOff = false; this.globeEl.style.transform = 'translateX(0)'; } } ~\n\nDesigning a better world\n\nTectonic plates arrange continents, but countries\u2014how we organize the globe\u2014are defined by people. It\u2019s the same with organizations: how we define teams determines how we operate. We\u2019ve found that establishing how designers and engineers relate, collaborate, and organize has an outsized influence in how we build. There\u2019s a long line of designers and developers with a mutual respect for both pixels and code. This rapport sidesteps many pitfalls when building products: the transfer of pressure from designer to developer to deliver stunning visuals, to engineers diluting the vision at the eleventh hour. Blending design and engineering complicates the process, but enriches the result.\n\nWe could only properly evaluate our globe once we built a functional prototype with a sphere on the screen to examine. Modern software development is often built modularly, snapping components together until it\u2019s ready to ship. We pledged to build the real, whole product, even in its earliest\u2014and ugliest\u2014stages. This enabled us to separate its functionality from its finality, focusing less on if it worked and more on when it worked well enough for us to ship it. This released us from the temptation to make sacrifices in quality just to make the globe fully operative.\n\nBuilding a fully-functional prototype early in our development process focussed our highly cross-functional team; over time and through iteration, improvements unfolded gradually. Since its first incarnation in 2019, we\u2019ve used the globe for mockups, keynotes, websites, and a small, but momentous appearance in Stripe\u2019s Dashboard.\n\nMeasures of time are actually measures of the earth\u2019s rotation: sixty units of rotation per minute, and sixty minutes of rotation per hour. As our product expands to cover the surface of the globe, we\u2019ll keep smoothing the rough edges, connecting dots in distant countries, and working to keep the world spinning at 60 frames per second.", "label": 0}
{"title": "How to Protect your sites Vouch Proxy, NGINX, Reverse Proxies with Docker Compose (Part 1)", "url": "https://lifeofpablo.com/blog/protect-sites-with-vouch-proxy-and-docker-compose", "content": "How to Protect your sites Vouch Proxy, NGINX, Reverse Proxies with Docker Compose (Part 1)\n\nThis post was written in English (en_US).\n\nI've written in the past how to install Vouch Proxy using Debian. I also wrote a post a while ago where I dockerized my site and services. If it isn't obvious, I really like Vouch Proxy. It's simple and it met my needs.\n\nI'm going to share how I setup the following services using Docker Compose:\n\nVouch Proxy\n\nNGINX Reverse Proxy\n\nCreating Reverse Proxies for your apps to be protected with Vouch Proxy. I will provide example services as use cases. Grafana Prometheus\n\n\n\nThis post will divided into three (3) parts.\n\nThis guide, is recommended for those who have experience with Docker and Docker Compose. I will keep this simple for you to follow along if you don't have experience. When using Docker Compose and docker-compose.yml files, you are launching multiple containers at one time. When using a Dockerfile, one container is launched at a time.\n\nWe'll using a lot of environmental variables to configure our applications. It seems like a lot of work at first but you'll be happy that you did.\n\nLet's Setup Docker Compose\n\nHere is the docker compose we are going to use. I will break it down piece by piece as mentioned above.\n\nservices: nginx: container_name: nginx image: nginxproxy/nginx-proxy restart: unless-stopped ports: - 80 :80 - 443 :443 volumes: - /var/run/docker.sock:/tmp/docker.sock:ro - /var/docker/nginx/html:/usr/share/nginx/html - /var/docker/nginx/certs:/etc/nginx/certs - /var/docker/nginx/vhost:/etc/nginx/vhost.d - /var/docker/nginx/conf:/etc/nginx/conf.d logging: options: max-size: \"10m\" max-file: \"3\" letsencrypt-companion: container_name: letsencrypt-companion image: jrcs/letsencrypt-nginx-proxy-companion restart: unless-stopped volumes_from: - nginx volumes: - /var/run/docker.sock:/var/run/docker.sock - /var/docker/nginx/acme:/etc/acme.sh environment: DEFAULT_EMAIL: your-email@domain.com mariadb: container_name: mariadb image: mariadb:latest command: --default-authentication-plugin=mysql_native_password environment: MYSQL_ROOT_PASSWORD: changeme MYSQL_DATABASE: kanboard MYSQL_USER: kanboard MYSQL_PASSWORD: changeme volumes: - mariadb:/var/lib/mysql:z vouch-proxy-auth: container_name: vp-proxy image: quay.io/vouch/vouch-proxy:alpine-latest ports: - 9090 :9090 volumes: - ./vouch-proxy-config:/config restart: always environment: VIRTUAL_HOST: your-domain.com LETSENCRYPT_HOST: your-domain.com grafana: container_name: grafana image: grafana/grafana:latest volumes: - ../plugins/:/etc/grafana/plugins/ - ./grafana/provisioning/:/etc/grafana/provisioning/ - grafana_vol:/var/lib/grafana environment: - \"GF_SECURITY_ADMIN_PASSWORD=pwd\" - GF_USERS_ALLOW_SIGN_UP=FALSE - GF_USERS_AUTO_ASSIGN_ORG=TRUE - GF_USERS_AUTO_ASSIGN_ORG_ROLE=EDITOR - GF_AUTH_PROXY_ENABLED=true - GF_AUTH_PROXY_HEADER_NAME=X-Vouch-User - GF_AUTH_PROXY_HEADER_PROPERTY=email - GF_AUTH_PROXY_AUTO_SIGN_UP=false - GF_INSTALL_PLUGINS=grafana-azure-data-explorer-datasource - GF_SERVER_HTTP_PORT=3001 - GF_SERVER_PROTOCOL=http - GF_SERVER_DOMAIN=grafana.domain.com - GF_SERVER_ROOT_URL=grafana.domain.com - GF_SERVER_SERVE_FROM_SUB_PATH=false - GF_SMTP_ENABLED=TRUE - \"GF_SMTP_HOST=smtp.domain.com\" - \"GF_SMTP_USER=smtp-user\" - GF_SMTP_PASSWORD=changeme - \"GF_SMTP_FROM_ADDRESS=grafana@domain.com\" - \"GF_SMTP_FROM_NAME=Name of Grafana Instance\" - \"GF_SMTP_STARTTLS_POLICY=MANDATORYSTARTTLS\" expose: - 3001 vp-proxy-graf: image: nginx:latest container_name: vp-proxy-graf environment: VIRTUAL_HOST: grafana.domain.com LETSENCRYPT_HOST: grafana.domain.com volumes: - ./prometheus-grafana/nginx/graf:/etc/nginx/conf.d ports: - 8081 :80 prometheus: image: prom/prometheus:latest container_name: prometheus restart: unless-stopped volumes: - ./prometheus-grafana/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml - prometheus_data:/prometheus command: - '--config.file=/etc/prometheus/prometheus.yml' - '--storage.tsdb.path=/prometheus' - '--web.console.libraries=/etc/prometheus/console_libraries' - '--web.console.templates=/etc/prometheus/consoles' - '--web.enable-lifecycle' expose: - 9091 vp-proxy-prom: image: nginx:latest container_name: vp-proxy-prom environment: VIRTUAL_HOST: prometheus.domain.com LETSENCRYPT_HOST: prometheus.domain.com volumes: - ./prometheus-grafana/nginx/prom:/etc/nginx/conf.d ports: - 8082 :80 node-exporter: image: prom/node-exporter:latest container_name: node-exporter restart: unless-stopped volumes: - /proc:/host/proc:ro - /sys:/host/sys:ro - /:/rootfs:ro command: - '--path.procfs=/host/proc' - '--path.rootfs=/rootfs' - '--path.sysfs=/host/sys' - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)' expose: - 9100 volumes: prometheus_data: {} grafana_vol: mariadb: prom_data:\n\nWe won't run the docker compose command yet. We don't have all the files and other steps needed to run it correctly\n\nSetup Vouch Proxy Config", "label": 1}
{"title": "Sounds of San Francisco", "url": "https://lifeofpablo.com/blog/sounds-of-san-francisco", "content": "Sounds of San Francisco\n\nPeople enjoying themselves at Delores Park\n\nThis post was written in English (en_US).\n\nDelores Park\n\nThis is an interactive post featuring audio\n\nI love San Francisco. It is unique with different sounds. You never know what you're going to hear around you! It could be someone who is having a great or someone who isn't doing so well. Sound provides an identity of a city. I've noticed some patterns or sounds I only tend to find here.\n\nSomething I enjoy doing is recording the sounds of San Francisco. It could be as simple as recording near Powell Station or the Mission district. These sounds were recorded using an iPhone 12.\n\nHere is a small sample of sounds. Enjoy! Bluegrass Concert - Click to Expand!\n\nOne of my good friends plays the banjo. He's a great banjo player. If you like bluegrass he's the person to talk to!\n\nBluegrass was foreign to me until my friend taught me so much about it.\n\nYour browser does not support the audio tag.\n\nTaking the MUNI- Click to Expand! I really, really enjoy taking public transportation. Not only is it better for traffic, and reduces pollution. The MUNI (and BART) stations are great places to get some audio recordings. It feels great to have for San Francisco and the uniqueness of its underground tunnels.\n\nYour browser does not support the audio tag.\n\nPublic Conversation - Click to Expand! Wherever I turn, I hear conversations ranging from, \"What are we having for lunch?\" to \"Let's go to grab a drink!\" It's nice to hear people talking and enjoying each other's company through conversations in public spaces such as buses, on the street or simply sitting down on a bench.\n\nYour browser does not support the audio tag.\n\nStreet - Click to Expand!\n\nThere's something so beautiful about hearing the background sounds of cars, people walking, bikes, the wind, etc. Growing up it was very quiet growing up in a small midwest town. I enjoy the \"naturally\" occurring city street sounds.\n\nYour browser does not support the audio tag.\n\n\ud83d\udccd This post was written in my apartment in San Francisco.", "label": 1}
{"title": "Electrifying my house: the first year with a heat pump \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2024/01/28/electrifying-my-house-the-first-year-with-a-heat-pump/", "content": "A year ago, we had our gas furnace and water heater replaced with a heat pump and electric water heater. They both really needed replacing. We\u2019d hoped to have it done before the winter started since our furnace had already been on the fritz for several years \u2014 but the equipment was on backorder.\n\nMy energy utility, Puget Sound Energy (PSE), wants people to stay on gas, so there weren\u2019t incentives from them for this switch; however, my city developed a bulk purchase and incentive program to assist homeowners in transitioning to support the community\u2019s climate goals. This upgrade electrified our whole house, so we could shut off our gas service!*\n\nNow that I\u2019ve had a heat pump for a year, I wanted to share my experience, because I had questions and misconceptions that I didn\u2019t get answered beforehand. Tl;dr it\u2019s great, especially the cooling, just works a little differently.\n\n(Heads-up: 2500 word post, but it\u2019s got graphs! \ud83e\udd29)\n\n*Technically we still have a gas fireplace installed, but we never used it. PSE said it was safe to simply turn off even if that means there might be a small amount of gas remaining in the pipes under the house.\n\nWhy we got a heat pump\n\nOur old setup was failing\n\nBefore the upgrade, we had a ~20 year old gas furnace and an old gas water heater. In November 2019, a pipe on our water heater ruptured, spraying a ton of scalding water into our garage ceiling, filling the entire garage with steam that we had to hire a company to install fans for days to dry out. After that, the furnace began to fail regularly, falsely tripping the safety shut-off and needing to be restarted multiple times a day. We had people out to fix it multiple times, but the most they could figure was that something inside was corroded and they\u2019d clean it to buy us another few months.\n\nMy husband\u2019s telecommuted for years, and during the pandemic, I started working from home too \u2014 but the only \u201ccooling\u201d we had was a ceiling fan in our living room and dining room. We bought a used portable electric window A/C unit to keep my husband\u2019s office from turning into an inferno, and hefted it down the hall to our bedroom in the evenings. Less than ideal.\n\nNatural gas is terrible for the planet and for people\n\n\u201cNatural gas\u201d is the greenwashed industry name for methane, an extremely potent greenhouse gas. A lot of methane is fracked, which may have impacts on the health of surrounding communities. Methane is piped, often through First Nation people\u2019s lands without permission \u2014 think Keystone XL and Canada\u2019s Coastal GasLink pipeline \u2014 where it may leak and harm the water and ecosystem. Also, some methane leaks during fracking and distribution, contributing to emissions.\n\nMoreover, \u201cnatural gas\u201d is propped up by corporate propaganda that cooking with gas is better than electric, despite the atrocious impact methane has on indoor air quality and its contribution to household emissions. I have an electric stove, so I didn\u2019t have to replace mine in order to get off gas.\n\nWithout gas-burning appliances, our risk of carbon monoxide poisoning is lowered; it would come from some other source, like operating a propane stove indoors (DO NOT DO THAT THING). My husband knows someone who has brain damage from carbon monoxide poisoning!\n\nBasically, \u201cnatural gas\u201d is fucking shit and I want fucking nothing to do with it.\n\nMy new all-electric home energy setup\n\nMy house is about 1400s.f. and has two stories. It was built in the 1980s and our attic doesn\u2019t have enough insulation (we\u2019ve replaced insulation in the basement but not yet dealt with the attic).\n\nMitsubishi heat pump air handler (SVZ-KP24NA) \u2014 24,000 BTU/hour\n\nGeneral Aire residential whole-house air cleaner (GFI #4422 or 4424)\n\nMitsubishi wall head (split-type air conditioner MSZ-GL15NA) \u2014 one unit installed on second floor \u2014 we considered a second but decided to save $$$\n\n\n\n50-gallon electric water heater \u2014 the installer recommended against tankless despite what I\u2019d read online \ud83e\udd37\u200d\u2640\ufe0f\n\n18-panel / 5.04kW solar system (Itek IT280 photovoltaic modules and Itek Theia HE\u2010t UL 3.8 kW inverter) \u2014 installed in 2015\n\nThe air handler connects to our existing duct system. There\u2019s the outdoor unit, which we placed on the side of the house directly below the upstairs heat pump head, and there\u2019s also the air cleaner in the garage where the old furnace was.\n\nThe city bulk purchase program we went through offered Mitsubishi heat pumps, and we just went with what the contractor recommended.\n\nHow much did the heat pump and electric water heater cost?\n\nI am located in the Seattle area, which has been in a housing boom for a while so contractors are pretty expensive.\n\nThe heat pump cost $19,445 + 10.2% tax for equipment and installation. The permit cost $236.*\n\nThe electric water heater with new dedicated 220v circuit cost $2,145 + 10.2% tax.\n\nOur city provided a $500 incentive and coordinated a $1,200 discount for bulk purchase of equipment, so in total we paid $22,155 for both heat pump and new water heater.\n\n*The permit included an inspection, which failed the first time \u2014 we wouldn\u2019t have known what wasn\u2019t installed quite right, so thanks inspector! He was super helpful and pointed out some other unrelated things he noticed that we might consider acting on.\n\nHeat pump vs. gas furnace energy use comparisons\n\nMy situation will not be directly comparable to most people\u2019s because we have a solar array, but hopefully will still provide a relative comparison. I have provided the electricity we used, which includes power we generated and used directly, but not power we generated and sold back to PSE.\n\nUnfortunately since we upgraded the water heater at the same time, we can\u2019t isolate only the cost of the heat pump \ud83e\udd37\u200d\u2640\ufe0f\n\nObnoxiously, our power bill comes mid-month. To make life easier for myself in the graphs, I\u2019ve represented the month the bill was issued. That typically covers usage from the 18th of the previous month through the 17th of the billing month \ud83d\ude15\n\nContext: weather and energy trends in the Pacific Northwest\n\nFor usage context, in the Seattle area, it\u2019s relatively cool (and we run heating) from mid September through early June. Summer is usually relatively moderate, typically peaking with one or two weeks in the 90s (F). Over 100F is unusual.\n\nHow much more electricity did the heat pump use?\n\nThese data represent solely our electricity use, so you can guesstimate how much electricity we use for heating (and cooling) by how much our electricity increased after January 2023. During the cold months (December through March bills), we used about 1200-1500 additional kWh (for heating our space and water). Over the whole year, we used an additional 7300kWh compared to the previous year. (According to PSE, it\u2019s typical to use about 2800kWh annually for an electric water heater and 6800kWh for a heat pump, or 9600kWh annually.)\n\nYou\u2019ll see that we had a big jump in electricity use in January 2024 (technically mid December through mid January). This coincides with an extremely cold snap for Washington, with temperatures in the ~19 degree Fahrenheit range for about a week. We didn\u2019t adjust our thermostat during that time period, but did run the heat pump head overnight as well as pulled out a small standalone electric space heater to help keep the house warmer during the day.\n\nMonthly kWh electricity used, Sept. 16 2021-Jan. 17, 2024 Month 2021 2022 2023 2024 kWh difference\n\nwith heat pump 1 499 495 1946 1451 2 419 *1044 *625 3 545 1737 1192 4 681 1617 936 5 695 1120 425 6 531 993 462 7 1003 1213 210 8 1212 1059 -153 9 868 973 105 10 577 721 880 159 11 466 541 1274 733 12 430 500 1651 1151\n\nbold = electric only \u2014 heat pump installed January 30, 2023\n\n* About half the February 2023 billing cycle was on gas heat\n\n\n\nHow much methane usage did we eliminate?\n\nIn 2022, we used 770 therms of methane.* Now, we use none! \ud83d\ude4c\n\n*According to PSE, a typical house uses 690 therms for a gas furnace and 160 therms for a gas water heater, or 850 therms annually.\n\nComparing gas therms and kWh electricity\n\nA therm is equal to approximately 29 kWh. Unless I\u2019m being totally unreasonable in converting and combining these values, here\u2019s how the overall kWh shakes out \u2014 a massive reduction in energy used with the heat pump (Feb 2023-Jan 2024):\n\nHow much did it cost to run the heat pump versus the gas furnace?\n\nThe first month we used the heat pump for cooling, I was braced for a radical energy bill, but thanks to our solar panels, we continued to pay the minimum electrical charge in summer \ud83e\uddbe\n\nIn actual costs, we paid $1435 for energy (gas and electricity) in 2022 and $1158 in 2023 (heat pump only); if we assumed that the four summer months of 2022 were $7.95 as in a normal year, that would still put 2022 at $1215. And if we swapped in this year\u2019s all-electric January bill for last year\u2019s gas-heated bill, that\u2019s basically a wash. And now we have whole-house cooling in summer \ud83d\ude0e\n\nMonthly energy bill by year, Sept. 16 2021-Jan. 17, 2024 Month 2021 2022 2023 2024 1 $218.41 *$229.79 $260.96 2 $158.67 $202.26 3 $148.30 $188.41 4 $108.26 $91.73 5 $79.85 $28.09 6 $51.67 $7.95 7 *$36.18 $7.95 8 *$76.16 $7.95 9 *$86.90 $7.95 10 $71.32 *$84.62 $46.02 11 $101.87 *$161.19 $136.05 12 $143.28 *$224.62 $204.68\n\nbold = electric only \u2014 heat pump installed January 30, 2023\n\n* = our solar inverter was not selling electricity back to PSE these months, though was generating power for pass-through usage \u2014 in a normal year the summer month costs would be the minimum $7.95 \u2014 this also means there was no bank to draw from for those fall and winter months\n\nWhat I wish we\u2019d known before switching to a heat pump\n\nIt heats differently than a gas furnace: slow and steady versus bursts of heat\n\nThe first month we had our heat pump, in the cold of February 2023, I was convinced it was broken or we\u2019d been sold too small of a unit. If you go in with expectations that it\u2019s going to be a different heating experience than a gas furnace, you\u2019ll be happier than I was at first \ud83d\ude09\n\nBeing used to a gas furnace, we were used to saying, \u201cIt\u2019s cold!\u201d and bumping the thermostat* up a couple degrees, then getting the immediate feedback of a big blast of hot air and a relatively quick increase in temperature.\n\nIn contrast, the heat pump blows air lightly all day long**. If we increase the thermostat setting, it will take several hours to reach the new temperature. Likewise, when it\u2019s too cold out, sometimes it has a hard time keeping up. (I\u2019m hoping that upping our attic insulation will help hold the temperature better.)\n\n*We had a Nest before, but I don\u2019t think it actually helped save energy since we were home all day every day, so between that and Google\u2019s creepy surveillance I was fine when I found out the heat pump came with a thermostat and wouldn\u2019t work with the Nest. Unfortunately, the included thermostat is kind of annoying to program.\n\n**Because heat pumps blow constantly, the contractor recommended keeping as many vents open as possible \u2014 closing no more than one or two. (There was a technical reason for this that I forget, something about making part of the heat pump get too cold and shut down.) Unfortunately, whoever installed our original HVAC system was, er, clearly the low bidder, so we have wacky vents directly underneath windows and doors all over the house \ud83e\udd26\u200d\u2640\ufe0f The register that blows the hottest is our downstairs bathroom \ud83d\ude44\n\nSo, with the heat pump, we don\u2019t drop the temperature overnight as much as we did with the gas furnace. We keep the house a more consistent temperature all the time. Then, the poor heat pump isn\u2019t always struggling to keep up, it just holds what it\u2019s got.\n\nThe heads aren\u2019t as quiet as they\u2019re made out to be\n\nWhoever says they\u2019re basically silent hasn\u2019t paid much attention. The heat pump head (which is in our bedroom) makes mysterious noises pretty often, particularly when in heating mode*. My poor cat Mina stares at it, transfixed in horror, as it tinkles like broken glass going through the wash, whooshes like the house is blasting off, brrrs to life unexpectedly, and whirs as the louvers rotate up and down randomly. It sometimes makes noise even when the unit is turned off, which is confusing.\n\nThe fan noises are relatively quiet, especially in comparison with a window AC unit, so that\u2019s probably the frame of reference people have when they talk about it being silent. But it\u2019s not silent if you\u2019re changing from forced air, which actually is basically silent \ud83d\ude09 Overall, it\u2019s not too intrusive and totally worth it for being able to fine-tune the comfort level of our bedroom, but not quite what I had anticipated.\n\n*I am pretty sensitive to noise, but my husband and cats agree on this one \ud83d\ude09\n\nOur verdict: the heat pump is a huge win for summer, good for winter\n\nIt kicks ass in summer\n\nWow, after previously not having cooling in summer, it was incredible to be able to comfortably use my office after like 3pm in the summer \ud83d\ude02 I was especially the beneficiary because in the past my husband got the portable A/C unit since his office was hotter.\n\nThe heat pump couldn\u2019t keep up with the hottest days in the high nineties, but even then still kept the house in comfortable mid to high eighties \u2014 totally fine for us, especially with a ceiling fan.\n\nGood in normal cold, fine in cold cold\n\nOnce we learned how to program the thermostat appropriately, we\u2019ve been happy with how the heat pump keeps the house warm.\n\nI\u2019ll be honest: this thing struggles with the cold cold (below freezing). From my understanding, actually cold places use a backup furnace IIRC, but the temperature\u2019s generally moderate enough in Seattle that we get by with just a heat pump and suck it up for the one or two extra-cold weeks a year.\n\nDuring our recent cold snap, when it was 16 degrees Fahrenheit overnight one night, the house was 55 degrees when we went downstairs in the morning despite the thermostat being set to 68. It didn\u2019t heat up much during the morning, so we pulled out our little electric space heater and an electric heating pad to boost the warmth, and bundled up with down vests and blankets. This is where the heat pump head is key: we were able to keep our bedroom, where the head is located, comfortable overnight (though we did add extra blankets).\n\nAnd honestly? The heat pump\u2019s so much more efficient, better for the climate, and safer for us that wearing a jacket for a week or two in the winter seems like a totally reasonable tradeoff.", "label": 1}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2024-07", "content": "en\n\nI am sharing that I am moving to San Francisco this week! I've accepted a new job out there and I'm so stoked! It still hasn't hit me that I'm moving to a different part of California.\n\nMy career path will be to continue working in IT but leaving education. Education will always have a special place in my heart. I'm excited about the new path for my career and acquiring some new skills! Being a nerd is fun!\n\nSan Francisco and the Bay Area are such places of beauty. I've dreamt of living in a city like this one since I was a kid. I've traveled and stayed in places throughout the world for extended periods but there's nothing like being in a place you'll be at for a long time.\n\nWhy San Francisco other than your new job, Pablo? Yes, thank you for asking! I'd love to answer your question!\n\nLet me tell you that I'm curious and I like to wander off to other places. Wandering off to different parts of California is no exception. While living in Sacramento, I would hop on the train on holiday breaks and weekends to San Francisco. Who doesn't like taking a train? I have friends that I visit quite regularly. I know my way around the city. They would show me around but also I would go off on my adventure. Do you see a pattern here? I'm excited to see my friends who live down the street or across town. I won't need to travel two hours or more to get there.\n\nI'll be close to outdoor adventures, go to my favorite hackerspace and more!\n\nSan Francisco is a place where I want to continue to flourish as an individual. I'm just getting started! It will grow my career, my lifestyle choices, my health, increased network opportunities, my love for technology, and my love for public transit. I've learned so much about myself being in the city. Now that I get to live there, I will continue to learn more! Don't forget about walking everywhere as well!\n\nI'm excited to be working on some startup ideas in my downtime.\n\nI can't wait for my friends, and family to visit me! San Francisco is easy to fly into! For my Nebraska people, Interstate 80 will take you to me! You can't miss it unless you drive into the bay. Isn't there a scene like this in The Office?\n\nI still find it hard to believe that this guy from Hastings, Nebraska has made it to San Francisco! I'm going to be living in a major city! The initial idea of moving to California seemed out of this world! Moving to San Francisco takes the cake! I don't regret venturing out of Nebraska. I'm more happy than I've ever been. This is your sign to explore other parts as well. You won't regret it!\n\nI'm sad I'm leaving Sacramento. It received me with open arms and I am extremely grateful for the people I have me. Every single one of you has had an impact on me.\n\nTo my friend on the other side of the country, I'll be waving while you drink some tea!\n\nIt's time to unpack boxes and enjoy the cooler weather(see picture above). Hooray!!", "label": 1}
{"title": "Title Launch Observability at Netflix Scale", "url": "https://netflixtechblog.com/title-launch-observability-at-netflix-scale-8efe69ebd653?source=collection_home---4------2-----------------------", "content": "Title Launch Observability at Netflix Scale\n\nPart 3: System Strategies and Architecture Netflix Technology Blog 7 min read \u00b7 Mar 5, 2025 -- 6 Listen Share\n\nBy: Varun Khaitan\n\nWith special thanks to my stunning colleagues: Mallika Rao, Esmir Mesic, Hugo Marques\n\nThis blog post is a continuation of Part 2, where we cleared the ambiguity around title launch observability at Netflix. In this installment, we will explore the strategies, tools, and methodologies that were employed to achieve comprehensive title observability at scale.\n\nDefining the observability endpoint\n\nTo create a comprehensive solution, we decided to introduce observability endpoints first. Each microservice involved in our Personalization stack that integrated with our observability solution had to introduce a new \u201cTitle Health\u201d endpoint. Our goal was for each new endpoint to adhere to a few principles:\n\nAccurate reflection of production behavior Standardization across all endpoints Answering the Insight Triad: \u201cHealthy\u201d or not, why not and how to fix it.\n\nAccurately Reflecting Production Behavior\n\nA key part of our solution is insights into production behavior, which necessitates our requests to the endpoint result in traffic to the real service functions that mimics the same pathways the traffic would take if it came from the usual callers.\n\nIn order to allow for this mimicking, many systems implement an \u201cevent\u201d handling, where they convert our request into a call to the real service with properties enabled to log when titles are filtered out of their response and why. Building services that adhere to software best practices, such as Object-Oriented Programming (OOP), the SOLID principles, and modularization, is crucial to have success at this stage. Without these practices, service endpoints may become tightly coupled to business logic, making it challenging and costly to add a new endpoint that seamlessly integrates with the observability solution while following the same production logic.\n\nA service with modular business logic facilitates the seamless addition of an observability endpoint.\n\nStandardization\n\nTo standardize communication between our observability service and the personalization stack\u2019s observability endpoints, we\u2019ve developed a stable proto request/response format. This centralized format, defined and maintained by our team, ensures all endpoints adhere to a consistent protocol. As a result, requests are uniformly handled, and responses are processed cohesively. This standardization enhances adoption within the personalization stack, simplifies the system, and improves understanding and debuggability for engineers.\n\nThe request schema for the observability endpoint.\n\nThe Insight Triad API\n\nTo efficiently understand the health of a title and triage issues quickly, all implementations of the observability endpoint must answer: is the title eligible for this phase of promotion, if not \u2014 why is it not eligible, and what can be done to fix any problems.\n\nThe end-users of this observability system are Launch Managers, whose job it is to ensure smooth title launches. As such, they must be able to quickly see whether there is a problem, what the problem is, and how to solve it. Teams implementing the endpoint must provide as much information as possible so that a non-engineer (Launch Manager) can understand the root cause of the issue and fix any title setup issues as they arise. They must also provide enough information for partner engineers to identify the problem with the underlying service in cases of system-level issues.\n\nThese requirements are captured in the following protobuf object that defines the endpoint response.\n\nThe response schema for the observability endpoint.\n\nHigh level architecture\n\nWe\u2019ve distilled our comprehensive solution into the following key steps, capturing the essence of our approach:\n\nEstablish observability endpoints across all services within our Personalization and Discovery Stack. Implement proactive monitoring for each of these endpoints. Track real-time title impressions from the Netflix UI. Store the data in an optimized, highly distributed datastore. Offer easy-to-integrate APIs for our dashboard, enabling stakeholders to track specific titles effectively. \u201cTime Travel\u201d to validate ahead of time.\n\nObservability stack high level architecture diagram\n\nIn the following sections, we will explore each of these concepts and components as illustrated in the diagram above.\n\nKey Features\n\nProactive monitoring through scheduled collectors jobs\n\nOur Title Health microservice runs a scheduled collector job every 30 minutes for most of our personalization stack.\n\nFor each Netflix row we support (such as Trending Now, Coming Soon, etc.), there is a dedicated collector. These collectors retrieve the relevant list of titles from our catalog that qualify for a specific row by interfacing with our catalog services. These services are informed about the expected subset of titles for each row, for which we are assessing title health.\n\nOnce a collector retrieves its list of candidate titles, it orchestrates batched calls to assigned row services using the above standardized schema to retrieve all the relevant health information of the titles. Additionally, some collectors will instead poll our kafka queue for impressions data.\n\nReal-time Title Impressions and Kafka Queue\n\nIn addition to evaluating title health via our personalization stack services, we also keep an eye on how our recommendation algorithms treat titles by reviewing impressions data. It\u2019s essential that our algorithms treat all titles equitably, for each one has limitless potential.\n\nThis data is processed from a real-time impressions stream into a Kafka queue, which our title health system regularly polls. Specialized collectors access the Kafka queue every two minutes to retrieve impressions data. This data is then aggregated in minute(s) intervals, calculating the number of impressions titles receive in near-real-time, and presented as an additional health status indicator for stakeholders.\n\nData storage and distribution through Hollow Feeds\n\nNetflix Hollow is an Open Source java library and toolset for disseminating in-memory datasets from a single producer to many consumers for high performance read-only access. Given the shape of our data, hollow feeds are an excellent strategy to distribute the data across our service boxes.\n\nOnce collectors gather health data from partner services in the personalization stack or from our impressions stream, this data is stored in a dedicated Hollow feed for each collector. Hollow offers numerous features that help us monitor the overall health of a Netflix row, including ensuring there are no large-scale issues across a feed publish. It also allows us to track the history of each title by maintaining a per-title data history, calculate differences between previous and current data versions, and roll back to earlier versions if a problematic data change is detected.\n\nObservability Dashboard using Health Check Engine\n\nWe maintain several dashboards that utilize our title health service to present the status of titles to stakeholders. These user interfaces access an endpoint in our service, enabling them to request the current status of a title across all supported rows. This endpoint efficiently reads from all available Hollow Feeds to obtain the current status, thanks to Hollow\u2019s in-memory capabilities. The results are returned in a standardized format, ensuring easy support for future UIs.\n\nAdditionally, we have other endpoints that can summarize the health of a title across subsets of sections to highlight specific member experiences.\n\nMessage depicting a dashboard request.\n\nTime Traveling: Catching before launch\n\nTitles launching at Netflix go through several phases of pre-promotion before ultimately launching on our platform. For each of these phases, the first several hours of promotion are critical for the reach and effective personalization of a title, especially once the title has launched. Thus, to prevent issues as titles go through the launch lifecycle, our observability system needs to be capable of simulating traffic ahead of time so that relevant teams can catch and fix issues before they impact members. We call this capability \u201cTime Travel\u201d.\n\nMany of the metadata and assets involved in title setup have specific timelines for when they become available to members. To determine if a title will be viewable at the start of an experience, we must simulate a request to a partner service as if it were from a future time when those specific metadata or assets are available. This is achieved by including a future timestamp in our request to the observability endpoint, corresponding to when the title is expected to appear for a given experience. The endpoint then communicates with any further downstream services using the context of that future timestamp.\n\nAn example request with a future timestamp.\n\nConclusion\n\nThroughout this series, we\u2019ve explored the journey of enhancing title launch observability at Netflix. In Part 1, we identified the challenges of managing vast content launches and the need for scalable solutions to ensure each title\u2019s success. Part 2 highlighted the strategic approach to navigating ambiguity, introducing \u201cTitle Health\u201d as a framework to align teams and prioritize core issues. In this final part, we detailed the sophisticated system strategies and architecture, including observability endpoints, proactive monitoring, and \u201cTime Travel\u201d capabilities; all designed to ensure a thrilling viewing experience.\n\nBy investing in these innovative solutions, we enhance the discoverability and success of each title, fostering trust with content creators and partners. This journey not only bolsters our operational capabilities but also lays the groundwork for future innovations, ensuring that every story reaches its intended audience and that every member enjoys their favorite titles on Netflix.\n\nThank you for joining us on this exploration, and stay tuned for more insights and innovations as we continue to entertain the world.", "label": 0}
{"title": "Introducing Configurable Metaflow", "url": "https://netflixtechblog.com/introducing-configurable-metaflow-d2fb8e9ba1c6?source=collection_home---4------7-----------------------", "content": "Introducing Configurable Metaflow Netflix Technology Blog 13 min read \u00b7 Dec 20, 2024 -- 4 Listen Share\n\nDavid J. Berg*, David Casler^, Romain Cledat*, Qian Huang*, Rui Lin*, Nissan Pow*, Nurcan Sonmez*, Shashank Srikanth*, Chaoying Wang*, Regina Wang*, Darin Yu*\n\n*: Model Development Team, Machine Learning Platform\n\n^: Content Demand Modeling Team\n\nA month ago at QConSF, we showcased how Netflix utilizes Metaflow to power a diverse set of ML and AI use cases, managing thousands of unique Metaflow flows. This followed a previous blog on the same topic. Many of these projects are under constant development by dedicated teams with their own business goals and development best practices, such as the system that supports our content decision makers, or the system that ranks which language subtitles are most valuable for a specific piece of content.\n\nAs a central ML and AI platform team, our role is to empower our partner teams with tools that maximize their productivity and effectiveness, while adapting to their specific needs (not the other way around). This has been a guiding design principle with Metaflow since its inception.\n\nMetaflow infrastructure stack\n\nStanding on the shoulders of our extensive cloud infrastructure, Metaflow facilitates easy access to data, compute, and production-grade workflow orchestration, as well as built-in best practices for common concerns such as collaboration, versioning, dependency management, and observability, which teams use to setup ML/AI experiments and systems that work for them. As a result, Metaflow users at Netflix have been able to run millions of experiments over the past few years without wasting time on low-level concerns.\n\nA long standing FAQ: configurable flows\n\nWhile Metaflow aims to be un-opinionated about some of the upper levels of the stack, some teams within Netflix have developed their own opinionated tooling. As part of Metaflow\u2019s adaptation to their specific needs, we constantly try to understand what has been developed and, more importantly, what gaps these solutions are filling.\n\nIn some cases, we determine that the gap being addressed is very team specific, or too opinionated at too high a level in the stack, and we therefore decide to not develop it within Metaflow. In other cases, however, we realize that we can develop an underlying construct that aids in filling that gap. Note that even in that case, we do not always aim to completely fill the gap and instead focus on extracting a more general lower level concept that can be leveraged by that particular user but also by others. One such recurring pattern we noticed at Netflix is the need to deploy sets of closely related flows, often as part of a larger pipeline involving table creations, ETLs, and deployment jobs. Frequently, practitioners want to experiment with variants of these flows, testing new data, new parameterizations, or new algorithms, while keeping the overall structure of the flow or flows intact.\n\nA natural solution is to make flows configurable using configuration files, so variants can be defined without changing the code. Thus far, there hasn\u2019t been a built-in solution for configuring flows, so teams have built their bespoke solutions leveraging Metaflow\u2019s JSON-typed Parameters, IncludeFile, and deploy-time Parameters or deploying their own home-grown solution (often with great pain). However, none of these solutions make it easy to configure all aspects of the flow\u2019s behavior, decorators in particular.\n\nRequests for a feature like Metaflow Config\n\nOutside Netflix, we have seen similar frequently asked questions on the Metaflow community Slack as shown in the user quotes above:\n\nhow can I adjust the @resource requirements, such as CPU or memory, without having to hardcode the values in my flows?\n\nhow to adjust the triggering @schedule programmatically, so our production and staging deployments can run at different cadences?\n\nNew in Metaflow: Configs!\n\nToday, to answer the FAQ, we introduce a new \u2014 small but mighty \u2014 feature in Metaflow: a Config object. Configs complement the existing Metaflow constructs of artifacts and Parameters, by allowing you to configure all aspects of the flow, decorators in particular, prior to any run starting. At the end of the day, artifacts, Parameters and Configs are all stored as artifacts by Metaflow but they differ in when they are persisted as shown in the diagram below:\n\nDifferent data artifacts in Metaflow\n\nSaid another way:\n\nAn artifact is resolved and persisted to the datastore at the end of each task.\n\nis resolved and persisted to the datastore at the end of each task. A parameter is resolved and persisted at the start of a run; it can therefore be modified up to that point. One common use case is to use triggers to pass values to a run right before executing. Parameters can only be used within your step code.\n\nis resolved and persisted at the start of a run; it can therefore be modified up to that point. One common use case is to use triggers to pass values to a run right before executing. Parameters can only be used within your step code. A config is resolved and persisted when the flow is deployed. When using a scheduler such as Argo Workflows, deployment happens when create\u2019ing the flow. In the case of a local run, \u201cdeployment\u201d happens just prior to the execution of the run \u2014 think of \u201cdeployment\u201d as gathering all that is needed to run the flow. Unlike parameters, configs can be used more widely in your flow code, particularly, they can be used in step or flow level decorators as well as to set defaults for parameters. Configs can of course also be used within your flow.\n\nAs an example, you can specify a Config that reads a pleasantly human-readable configuration file, formatted as TOML. The Config specifies a triggering \u2018@schedule\u2019 and \u2018@resource\u2019 requirements, as well as application-specific parameters for this specific deployment:\n\n[schedule]\n\ncron = \"0 * * * *\"\n\n\n\n[model]\n\noptimizer = \"adam\"\n\nlearning_rate = 0.5\n\n\n\n[resources]\n\ncpu = 1\n\nUsing the newly released Metaflow 2.13, you can configure a flow with a Config like above, as demonstrated by this flow:\n\nimport pprint\n\nfrom metaflow import FlowSpec, step, Config, resources, config_expr, schedule\n\n\n\n@schedule(cron=config_expr(\"config.schedule.cron\"))\n\nclass ConfigurableFlow(FlowSpec):\n\nconfig = Config(\"config\", default=\"myconfig.toml\", parser=\"tomllib.loads\")\n\n\n\n@resources(cpu=config.resources.cpu)\n\n@step\n\ndef start(self):\n\nprint(\"Config loaded:\")\n\npprint.pp(self.config)\n\nself.next(self.end)\n\n\n\n@step\n\ndef end(self):\n\npass\n\n\n\nif __name__ == \"__main__\":\n\nConfigurableFlow()\n\nThere is a lot going on in the code above, a few highlights:\n\nyou can refer to configs before they have been defined using \u2018config_expr\u2019.\n\nyou can define arbitrary parsers \u2014 using a string means the parser doesn\u2019t even have to be present remotely!\n\nFrom the developer\u2019s point of view, Configs behave like dictionary-like artifacts. For convenience, they support the dot-syntax (when possible) for accessing keys, making it easy to access values in a nested configuration. You can also unpack the whole Config (or a subtree of it) with Python\u2019s standard dictionary unpacking syntax, \u2018**config\u2019. The standard dictionary subscript notation is also available.\n\nSince Configs turn into dictionary artifacts, they get versioned and persisted automatically as artifacts. You can access Configs of any past runs easily through the Client API. As a result, your data, models, code, Parameters, Configs, and execution environments are all stored as a consistent bundle \u2014 neatly organized in Metaflow namespaces \u2014 paving the way for easily reproducible, consistent, low-boilerplate, and now easily configurable experiments and robust production deployments.\n\nMore than a humble config file\n\nWhile you can get far by accompanying your flow with a simple config file (stored in your favorite format, thanks to user-definable parsers), Configs unlock a number of advanced use cases. Consider these examples from the updated documentation:\n\nA major benefit of Config over previous more hacky solutions for configuring flows is that they work seamlessly with other features of Metaflow: you can run steps remotely and deploy flows to production, even when relying on custom parsers, without having to worry about packaging Configs or parsers manually or keeping Configs consistent across tasks. Configs also work with the Runner and Deployer.\n\nThe Hollywood principle: don\u2019t call us, we\u2019ll call you\n\nWhen used in conjunction with a configuration manager like Hydra, Configs enable a pattern that is highly relevant for ML and AI use cases: orchestrating experiments over multiple configurations or sweeping over parameter spaces. While Metaflow has always supported sweeping over parameter grids easily using foreaches, it hasn\u2019t been easily possible to alter the flow itself, e.g. to change @resources or @pypi/@conda dependencies for every experiment.\n\nIn a typical case, you trigger a Metaflow flow that consumes a configuration file, changing how a run behaves. With Hydra, you can invert the control: it is Hydra that decides what gets run based on a configuration file. Thanks to Metaflow\u2019s new Runner and Deployer APIs, you can create a Hydra app that operates Metaflow programmatically \u2014 for instance, to deploy and execute hundreds of variants of a flow in a large-scale experiment.\n\nTake a look at two interesting examples of this pattern in the documentation. As a teaser, this video shows Hydra orchestrating deployment of tens of Metaflow flows, each of which benchmarks PyTorch using a varying number of CPU cores and tensor sizes, updating a visualization of the results in real-time as the experiment progresses:\n\nExample using Hydra with Metaflow\n\nMetaboosting Metaflow \u2014 based on a true story\n\nTo give a motivating example of what configurations look like at Netflix in practice, let\u2019s consider Metaboost, an internal Netflix CLI tool that helps ML practitioners manage, develop and execute their cross-platform projects, somewhat similar to the open-source Hydra discussed above but with specific integrations to the Netflix ecosystem. Metaboost is an example of an opinionated framework developed by a team already using Metaflow. In fact, a part of the inspiration for introducing Configs in Metaflow came from this very use case.\n\nMetaboost serves as a single interface to three different internal platforms at Netflix that manage ETL/Workflows (Maestro), Machine Learning Pipelines (Metaflow) and Data Warehouse Tables (Kragle). In this context, having a single configuration system to manage a ML project holistically gives users increased project coherence and decreased project risk.\n\nConfiguration in Metaboost\n\nEase of configuration and templatizing are core values of Metaboost. Templatizing in Metaboost is achieved through the concept of bindings, wherein we can bind a Metaflow pipeline to an arbitrary label, and then create a corresponding bespoke configuration for that label. The binding-connected configuration is then merged into a global set of configurations containing such information as GIT repository, branch, etc. Binding a Metaflow, will also signal to Metaboost that it should instantiate the Metaflow flow once per binding into our orchestration cluster.\n\nImagine a ML practitioner on the Netflix Content ML team, sourcing features from hundreds of columns in our data warehouse, and creating a multitude of models against a growing suite of metrics. When a brand new content metric comes along, with Metaboost, the first version of the metric\u2019s predictive model can easily be created by simply swapping the target column against which the model is trained.\n\nSubsequent versions of the model will result from experimenting with hyper parameters, tweaking feature engineering, or conducting feature diets. Metaboost\u2019s bindings, and their integration with Metaflow Configs, can be leveraged to scale the number of experiments as fast as a scientist can create experiment based configurations.\n\nScaling experiments with Metaboost bindings \u2014 backed by Metaflow Config\n\nConsider a Metaboost ML project named `demo` that creates and loads data to custom tables (ETL managed by Maestro), and then trains a simple model on this data (ML Pipeline managed by Metaflow). The project structure of this repository might look like the following:\n\n\u251c\u2500\u2500 metaflows\n\n\u2502 \u251c\u2500\u2500 custom -> custom python code, used by\n\n| | | Metaflow\n\n\u2502 \u2502 \u251c\u2500\u2500 data.py\n\n\u2502 \u2502 \u2514\u2500\u2500 model.py\n\n\u2502 \u2514\u2500\u2500 training.py -> defines our Metaflow pipeline\n\n\u251c\u2500\u2500 schemas\n\n\u2502 \u251c\u2500\u2500 demo_features_f.tbl.yaml -> table DDL, stores our ETL\n\n| | output, Metaflow input\n\n\u2502 \u2514\u2500\u2500 demo_predictions_f.tbl.yaml -> table DDL,\n\n| stores our Metaflow output\n\n\u251c\u2500\u2500 settings\n\n\u2502 \u251c\u2500\u2500 settings.configuration.EXP_01.yaml -> defines the additive\n\n| | config for Experiment 1\n\n\u2502 \u251c\u2500\u2500 settings.configuration.EXP_02.yaml -> defines the additive\n\n| | config for Experiment 2\n\n\u2502 \u251c\u2500\u2500 settings.configuration.yaml -> defines our global\n\n| | configuration\n\n\u2502 \u2514\u2500\u2500 settings.environment.yaml -> defines parameters based on\n\n| git branch (e.g. READ_DB)\n\n\u251c\u2500\u2500 tests\n\n\u251c\u2500\u2500 workflows\n\n\u2502 \u251c\u2500\u2500 sql\n\n\u2502 \u251c\u2500\u2500 demo.demo_features_f.sch.yaml -> Maestro workflow, defines ETL\n\n\u2502 \u2514\u2500\u2500 demo.main.sch.yaml -> Maestro workflow, orchestrates\n\n| ETLs and Metaflow\n\n\u2514\u2500\u2500 metaboost.yaml -> defines our project for\n\nMetaboost\n\nThe configuration files in the settings directory above contain the following YAML files:\n\n# settings.configuration.yaml (global configuration)\n\nmodel:\n\nfit_intercept: True\n\nconda:\n\nnumpy: '1.22.4'\n\n\"scikit-learn\": '1.4.0'\n\n# settings.configuration.EXP_01.yaml\n\ntarget_column: metricA\n\nfeatures:\n\n- runtime\n\n- content_type\n\n- top_billed_talent\n\n# settings.configuration.EXP_02.yaml\n\ntarget_column: metricA\n\nfeatures:\n\n- runtime\n\n- director\n\n- box_office\n\nMetaboost will merge each experiment configuration (*.EXP*.yaml) into the global configuration (settings.configuration.yaml) individually at Metaboost command initialization. Let\u2019s take a look at how Metaboost combines these configurations with a Metaboost command:\n\n(venv-demo) ~/projects/metaboost-demo [branch=demoX]\n\n$ metaboost metaflow settings show --yaml-path=configuration\n\n\n\nbinding=EXP_01:\n\nmodel: -> defined in setting.configuration.yaml (global)\n\nfit_intercept: true\n\nconda: -> defined in setting.configuration.yaml (global)\n\nnumpy: 1.22.4\n\n\"scikit-learn\": 1.4.0\n\ntarget_column: metricA -> defined in setting.configuration.EXP_01.yaml\n\nfeatures: -> defined in setting.configuration.EXP_01.yaml\n\n- runtime\n\n- content_type\n\n- top_billed_talent\n\n\n\nbinding=EXP_02:\n\nmodel: -> defined in setting.configuration.yaml (global)\n\nfit_intercept: true\n\nconda: -> defined in setting.configuration.yaml (global)\n\nnumpy: 1.22.4\n\n\"scikit-learn\": 1.4.0\n\ntarget_column: metricA -> defined in setting.configuration.EXP_02.yaml\n\nfeatures: -> defined in setting.configuration.EXP_02.yaml\n\n- runtime\n\n- director\n\n- box_office\n\nMetaboost understands it should deploy/run two independent instances of training.py \u2014 one for the EXP_01 binding and one for the EXP_02 binding. You can also see that Metaboost is aware that the tables and ETL workflows are not bound, and should only be deployed once. These details of which artifacts to bind and which to leave unbound are encoded in the project\u2019s top-level metaboost.yaml file.\n\n(venv-demo) ~/projects/metaboost-demo [branch=demoX]\n\n$ metaboost project list\n\n\n\nTables (metaboost table list):\n\nschemas/demo_predictions_f.tbl.yaml (binding=default):\n\ntable_path=prodhive/demo_db/demo_predictions_f\n\nschemas/demo_features_f.tbl.yaml (binding=default):\n\ntable_path=prodhive/demo_db/demo_features_f\n\n\n\nWorkflows (metaboost workflow list):\n\nworkflows/demo.demo_features_f.sch.yaml (binding=default):\n\ncluster=sandbox, workflow.id=demo.branch_demox.demo_features_f\n\nworkflows/demo.main.sch.yaml (binding=default):\n\ncluster=sandbox, workflow.id=demo.branch_demox.main\n\n\n\nMetaflows (metaboost metaflow list):\n\nmetaflows/training.py (binding=EXP_01): -> EXP_01 instance of training.py\n\ncluster=sandbox, workflow.id=demo.branch_demox.EXP_01.training\n\nmetaflows/training.py (binding=EXP_02): -> EXP_02 instance of training.py\n\ncluster=sandbox, workflow.id=demo.branch_demox.EXP_02.training\n\nBelow is a simple Metaflow pipeline that fetches data, executes feature engineering, and trains a LinearRegression model. The work to integrate Metaboost Settings into a user\u2019s Metaflow pipeline (implemented using Metaflow Configs) is as easy as adding a single mix-in to the FlowSpec definition:\n\nfrom metaflow import FlowSpec, Parameter, conda_base, step\n\nfrom custom.data import feature_engineer, get_data\n\nfrom metaflow.metaboost import MetaboostSettings\n\n\n\n@conda_base(\n\nlibraries=MetaboostSettings.get_deploy_time_settings(\"configuration.conda\")\n\n)\n\nclass DemoTraining(FlowSpec, MetaboostSettings):\n\nprediction_date = Parameter(\"prediction_date\", type=int, default=-1)\n\n\n\n@step\n\ndef start(self):\n\n# get show_settings() for free with the mixin\n\n# and get convenient debugging info\n\nself.show_settings(exclude_patterns=[\"artifact*\", \"system*\"])\n\n\n\nself.next(self.get_features)\n\n\n\n@step\n\ndef get_features(self):\n\n# feature engineers on our extracted data\n\nself.fe_df = feature_engineer(\n\n# loads data from our ETL pipeline\n\ndata=get_data(prediction_date=self.prediction_date),\n\nfeatures=self.settings.configuration.features +\n\n[self.settings.configuration.target_column]\n\n)\n\n\n\nself.next(self.train)\n\n\n\n@step\n\ndef train(self):\n\nfrom sklearn.linear_model import LinearRegression\n\n\n\n# trains our model\n\nself.model = LinearRegression(\n\nfit_intercept=self.settings.configuration.model.fit_intercept\n\n).fit(\n\nX=self.fe_df[self.settings.configuration.features],\n\ny=self.fe_df[self.settings.configuration.target_column]\n\n)\n\nprint(f\"Fit slope: {self.model.coef_[0]}\")\n\nprint(f\"Fit intercept: {self.model.intercept_}\")\n\n\n\nself.next(self.end)\n\n\n\n@step\n\ndef end(self):\n\npass\n\n\n\n\n\nif __name__ == \"__main__\":\n\nDemoTraining()\n\nThe Metaflow Config is added to the FlowSpec by mixing in the MetaboostSettings class. Referencing a configuration value is as easy as using the dot syntax to drill into whichever parameter you\u2019d like.\n\nFinally let\u2019s take a look at the output from our sample Metaflow above. We execute experiment EXP_01 with\n\nmetaboost metaflow run --binding=EXP_01\n\nwhich upon execution will merge the configurations into a single settings file (shown previously) and serialize it as a yaml file to the .metaboost/settings/compiled/ directory.\n\nYou can see the actual command and args that were sub-processed in the Metaboost Execution section below. Please note the \u2013config argument pointing to the serialized yaml file, and then subsequently accessible via self.settings. Also note the convenient printing of configuration values to stdout during the start step using a mixed in function named show_settings().\n\n(venv-demo) ~/projects/metaboost-demo [branch=demoX]\n\n$ metaboost metaflow run --binding=EXP_01\n\n\n\nMetaboost Execution:\n\n- python3.10 /root/repos/cdm-metaboost-irl/metaflows/training.py\n\n--no-pylint --package-suffixes=.py --environment=conda\n\n--config settings\n\n.metaboost/settings/compiled/settings.branch_demox.EXP_01.training.mP4eIStG.yaml\n\nrun --prediction_date20241006\n\n\n\nMetaflow 2.12.39+nflxfastdata(2.13.5);nflx(2.13.5);metaboost(0.0.27)\n\nexecuting DemoTraining for user:dcasler\n\nValidating your flow...\n\nThe graph looks good!\n\nBootstrapping Conda environment... (this could take a few minutes)\n\nAll packages already cached in s3.\n\nAll environments already cached in s3.\n\n\n\nWorkflow starting (run-id 50), see it in the UI at\n\nhttps://metaflowui.prod.netflix.net/DemoTraining/50\n\n\n\n[50/start/251640833] Task is starting.\n\n[50/start/251640833] Configuration Values:\n\n[50/start/251640833] settings.configuration.conda.numpy = 1.22.4\n\n[50/start/251640833] settings.configuration.features.0 = runtime\n\n[50/start/251640833] settings.configuration.features.1 = content_type\n\n[50/start/251640833] settings.configuration.features.2 = top_billed_talent\n\n[50/start/251640833] settings.configuration.model.fit_intercept = True\n\n[50/start/251640833] settings.configuration.target_column = metricA\n\n[50/start/251640833] settings.environment.READ_DATABASE = data_warehouse_prod\n\n[50/start/251640833] settings.environment.TARGET_DATABASE = demo_dev\n\n[50/start/251640833] Task finished successfully.\n\n\n\n[50/get_features/251640840] Task is starting.\n\n[50/get_features/251640840] Task finished successfully.\n\n\n\n[50/train/251640854] Task is starting.\n\n[50/train/251640854] Fit slope: 0.4702672504331096\n\n[50/train/251640854] Fit intercept: -6.247919678070083\n\n[50/train/251640854] Task finished successfully.\n\n\n\n[50/end/251640868] Task is starting.\n\n[50/end/251640868] Task finished successfully.\n\n\n\nDone! See the run in the UI at\n\nhttps://metaflowui.prod.netflix.net/DemoTraining/50\n\nTakeaways\n\nMetaboost is an integration tool that aims to ease the project development, management and execution burden of ML projects at Netflix. It employs a configuration system that combines git based parameters, global configurations and arbitrarily bound configuration files for use during execution against internal Netflix platforms.\n\nIntegrating this configuration system with the new Config in Metaflow is incredibly simple (by design), only requiring users to add a mix-in class to their FlowSpec \u2014 similar to this example in Metaflow documentation \u2014 and then reference the configuration values in steps or decorators. The example above templatizes a training Metaflow for the sake of experimentation, but users could just as easily use bindings/configs to templatize their flows across target metrics, business initiatives or any other arbitrary lines of work.\n\nTry it at home\n\nIt couldn\u2019t be easier to get started with Configs! Just\n\npip install -U metaflow\n\nto get the latest version and head to the updated documentation for examples. If you are impatient, you can find and execute all config-related examples in this repository as well.\n\nIf you have any questions or feedback about Config (or other Metaflow features), you can reach out to us at the Metaflow community Slack.\n\nAcknowledgments\n\nWe would like to thank Outerbounds for their collaboration on this feature; for rigorously testing it and developing a repository of examples to showcase some of the possibilities offered by this feature.", "label": 0}
{"title": "Generative AI and the Business Borg aesthetic \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/02/generative-ai-and-the-business-borg-aesthetic/", "content": "This feels like a sister piece to Ed Zitron\u2019s essay Era of the Business Idiots and Mandy Brown\u2019s essay Toolmen. Fair warning, this is a 5000 word post; I\u2019ve been working on this for weeks, pulling together what I\u2019ve learned about generative AI and culture over the past two years, so I hope it is worth your time \ud83d\ude04 Bonus: it doubles as a playlist \ud83c\udfb6\n\n\u201c\u2018Real power\u2019 is achieved when a technology \u2018[leaves] mythology and [enters] banality,'\u201d Marion Fourcade and Kieran Healy quote Vincent Mosco in The Ordinal Society. We\u2019ve had the mythology stage \u2014 the world tour with grandiose prophecies of imminent AGI \u2014 but now the race to normalize generative AI* is on: tech corporations are attempting to inure people to generative AI, an expression of the Business Borg aesthetic that currently carries a negative stigma outside of tech.\n\n*(My rule of thumb: if something is described as AI, it\u2019s probably predatory and/or bullshit; if it\u2019s described as machine learning, it probably does something useful. Not always true but a helpful predictor.)\n\nIn general, people like what we recognize better than what we don\u2019t \u2014 we prefer cultural works we can categorize to the unfamiliar and undefinable \u2014 and we are facing an inescapable shock-and-awe barrage of genAI graphics across the web to inundate our synapses with uncanny synthetic renderings.\n\nCurrently, generative AI is shunned by many artists and writers, the traditional arbiters of good taste and culture, because it has been developed through the theft of their labor. But tech CEOs stand to make (even bigger) fortunes if they can convince people that genAI doesn\u2019t signify bad taste, or make it seem like an irrevocable fact of life, like spam emails and text scammers. It\u2019s being deployed upon us with the same lockstep corporate solidarity that forced us to pay fees for checked luggage on flights (younger folks, before 2008 your bag used to be included with your ticket! Stowing your carry-on wasn\u2019t a competitive sport back in the day.).\n\nThe aesthetics of generative AI\n\n\u201cI have become momentarily obsessed with scrolling down the homepage of the MetaAI tool and seeing the infinite feed of what people have been asking of The Machine. The outputs are horribly banal, but the requests are a weird window into THE (NORMIE) HUMAN SOUL AND ITS DESIRES: www.meta.ai\u201d\n\n\u2014 Matt Muir, May 6, 2025 at 8:51 AM\n\n(via)\n\nBy its nature, generative AI produces the most likely image that meets the brief, which devolves to insipid Pictionary-style visual communication: chonky = cat, kebab = food = French chef\u2019s hat. These graphics are iconographic pablum, the uninspired result of gee-whiz curiosity about a new \u201ctool\u201d (toy)(trap) in an environment that discourages personal taste and cultural literacy.\n\nOriginally, I wrote up why I find these particular graphics tacky and visually uninteresting, but realized that ultimately, what they look like doesn\u2019t matter \u2014 it\u2019s the beliefs beneath the appearance that matter. As corporate models are trained further, Generative AI will probably continue to get better, rendering more attractive and/or plausible outputs, but it won\u2019t matter to me how good it gets because I reject the values it represents.\n\n(NB: I don\u2019t want you to feel bad about yourself if you use generative AI, because there are myriad reasons to have experimented with it, including being forced to for your employment; I want you to recognize what it symbolizes when you enthusiastically use this technology today, and make an informed choice about whether those are values you wish to signal to others.)\n\n\u201cWhat is wrong with a counterfeit is not what it is like, but how it was made.\u201d\n\n\u2014 Harry G. Frankfurt, On Bullshit\n\nAesthetics are looks that signal values\n\nWitching Hour by Ladytron\n\nAn aesthetic is an expression of taste for shared values, commonly communicated through a distinct style. We think of aesthetics as surface appearance only, but the formation of an aesthetic\u2019s conventions reflects the why and the how underneath the what. Just as the medium of a thing carries a message, so does its aesthetic. Aesthetics are visually and verbally encoded value systems.\n\nIdeology is a value system, independent of appearances. Aesthetics are the appearance of an ideology, which grow from its values. Subcultures form around ideologies, with members signaling their participation through aesthetics. \u201cGood taste\u201d is aesthetics that express those values.\n\nThough it\u2019s now often reduced to a visual style, the Arts and Crafts movement of the late 1800s and early 1900s was cross-disciplinary and united instead by an ethos \u2014 namely, the nobility of craftsmanship*:\n\n\u201cFor it is not the material, but the absence of human labor, which makes the thing worthless, and a piece of terracotta, or plaster of paris, which has been wrought by the human hand, is worth all the stone and Carrara cut by machinery.\u201d \u2014 John Ruskin, The Lamp of Truth from The Seven Lamps of Architecture\n\nAccording to the Arts and Crafts aesthetic, what is made should signal how it is made \u2014 the aesthetic\u2019s value system weights how something is made to be as important, if not more, as what is made. Surface appearance is borne of the decisions this taste for craft produces.\n\n*(People might instead think of this William Morris quote as the quintessential perspective of Arts and Crafts \u2014 \u201cHave nothing in your house that you do not know to be useful, or believe to be beautiful.\u201d \u2014 and I suspect there\u2019s a reason we\u2019ve been taught to recall a philosophy centered on material possession instead of labor.)\n\nLooking at a Kelmscott Press book (William Morris\u2019 printing company) reveals the printer\u2019s respect for handicraft: they designed custom drop letters and frames, included original illustrations by fine artists like Edward Burne-Jones, used original type modeled after typefaces used by printers like Aldus Manutius in the early days of the printing press, printed with a richer black ink than was standard at the time, and employed a heavy hand in letterpress printing so the design and type would be impressed into the page. Thoughtful, intentional ornamentation is embraced. The artifact itself is a thing to be appreciated as much as its contents; these are books that honor the integrity of all creative workers involved in their production.\n\nEmily Amick applies this analytical lens to reveal the tradwife aesthetic\u2019s underlying values:\n\nPrairie-core. Domestic bliss. Big sleeves. Bigger sourdough starters. And beneath it all, the subtle (or not-so-subtle) message: a woman\u2019s place is in the kitchen. But not because she wants to be there, because that\u2019s where God or her husband or some TikTok algorithm put her. The tradwife aesthetic promises comfort, but it delivers control. It\u2019s softness as a strategy. It\u2019s anti-feminism with a floral filter. It\u2019s nostalgia for a time when women were property, romanticized by influencers who want brand deals from butter. [\u2026] The tradwife says: give up your autonomy and someone else will take care of everything.\n\nWe adopt aesthetics based on aspirational values\n\nJessica Cullen writes that (emphasis mine): \u201cAesthetics aren\u2019t always about who we currently are but rather who we want to be.\u201d People adopt an aesthetic to say something about themselves to others. We intentionally adopt a particular subculture\u2019s aesthetics to convey our belonging and raise our status within the subculture. As Alec Leach puts it, \u201ca lot of the modern taste economy is actually the status economy.\u201d\n\nSometimes, we perceive only the surface level of an aesthetic, its appearance without the values \u2014 as Amick notes, the tradwife aesthetic \u201clooks so damn pretty and nourishing. And we are tired.\u201d \u2014 but whenever we adopt an aesthetic, we endorse (intentionally or not) the underlying values it represents. Amick continues dissecting the tradwife aesthetic and how it serves as conservative propaganda:\n\nWhat looks like innocent lifestyle content is actually part of an organized political movement designed to make patriarchy look cozy and appealing. Because politics is downstream from culture. The vibes that influence how we act and live.\n\nAesthetics matter more than ever because we act in accordance with our chosen aesthetics. As we get more and more of our \u201ccultural\u201d content on corporate silos, politics and purchases have subsumed a lot of cultural tastemaking. In this TikTok, Jamelle Bouie describes how politicians use aesthetic signaling to appeal to voters. Richard Sennett identifies that politicians come to embody \u201cintentions, desires, values, beliefs, tastes \u2013 an emphasis which has again the effect of divorcing power from responsibility.\u201d\n\nAnu Atluru argues that \u201cAesthetics are the modern units of cultural currency\u2014stores of value and instruments of power, capable of appreciating and being monetized at scale. Owning an aesthetic is owning influence.\u201d (emphasis mine)\n\nInterrogating the Business Borg aesthetic\n\nGlass Lux by Glass Lux\n\nThe Arts and Craft movement\u2019s respect for labor inspired stylistic choices that highlighted craftwork as well as the decisions in what goods to produce and how. So how does the Business Borg aesthetic reveal its values?\n\nTo define the Business Borg aesthetic, I\u2019m looking at:\n\nthe values I see expressed through generative media,\n\nI see expressed through generative media, the actions of the corporations behind generative AI,\n\nof the corporations behind generative AI, who buys into generative AI, and\n\nbuys into generative AI, and what else they like that reflects the same underlying values.\n\nWhat genAI is better at than a human artist is being cheap and instant.\n\nThe Business Borg aesthetic uses technology to signal wealth and power. Generative AI is not the only visual expression of the Business Borg aesthetic, just its most recognizable. The aesthetic is also signified visually by CGI-heavy blockbuster franchises, NFT art, and the Cybertruck; and in text by LinkedIn corporate thirst traps, X braggadocio, SEO word vomit, and generated \u201canswers\u201d to search results.\n\nAs political bedpartners, there is overlap between the Business Borg aesthetic and the MAGA aesthetic, but they\u2019re distinct viewpoints. Both share dominance as a core value, decry empathy, center patriarchy, and admire performance \u2014 but MAGA also signals Judeo-Christian morality, traditional beauty standards / traditional gender roles, hyper masculinity / violence, and nationalism. MAGA borrows aesthetics from golden pasts, like Neo-Classical architecture, tradwives, and, as Kate Wagner brands it, Regional Car Dealership Rococo; Business Borg prefer the more modern tones of cyberpunk, solarpunk, and minimalism. Business Borg are regulatory libertarians who envision themselves as the rightful leaders of society, Kings of techno-city-states; MAGA are Christian nationalists who want to use the power of the state to impose their beliefs on others.\n\nElon is a Business Borg at heart but wielded a chainsaw to appeal to the more violent MAGA aesthetic. Zuckerberg is a Business Borg but got a MAGA makeover with masculine stubble and bling.\n\nWhy am I naming this after the Borg? Like Star Trek\u2019s Borg, this is an aesthetic rooted in extractive consumption, assimilationist dominance, neo-colonial expansionism, self-righteous conviction, reductionist thinking, and proclamations of inevitability. It idolizes technology, often inspired by older science-fiction, and draws on cyberpunk aesthetics. The Silicon Valley Collective values groupthink and believes themselves superior to \u201cthe other.\u201d\n\nWho embraces this aesthetic\n\nNot all users of Generative AI embrace the Business Borg aesthetic. I think a lot of people are experimenting with generative AI out of neophilic curiosity, productivity imperatives, nihilistic determinism, and corporate fiat. Aspiring billionaires adopt the Business Borg aesthetic to signal their belonging in the cohort of the techno-rich.\n\nGenAI evangelists seem to be the same type of person who was into passive income and supplements fifteen years ago, then Soylent and SEO ten years ago, then NFTs and macro diets five years ago, now genAI and X blue checks.\n\nThe Business Borg aesthetic combines tech-centered neophilia, a hustle mindset, an obsession with optimization, evangelical fervor, and fake-it-till-you-make-it showmanship.\n\nThe Elon fanclub are Business Borg. Ed Zitron\u2019s Business Idiot shares a lot of characteristics with Business Borg (and may even be the same group, but I think feels a little different?).\n\nThe subculture emphasizes high profile demonstrations of \u201cwinning\u201c \u2014 using a $10k NFT as their X profile pic, bragging about SEO heists ripping off a competitor, generosity stunts a la Mr Beast, rubbing it in Miyazaki\u2019s face that there will be thousands of shitty knockoffs vaguely reminiscent of his work across the web.\n\nBusiness Borg signal their busyness \u2014 and importance \u2014 by broadcasting how little they sleep, how much they work, and how little they read. The only fiction they like is (old) sci-fi because they read it as non-fiction, not fiction \u2014 a source of \u201cinspiration\u201d stripped of context and commentary. Using GenAI signals their adoption of cutting edge technology, the synthetic smoothness emphasizing its nonhuman origin. They care a lot about IQ, a supposedly impartial measure of intelligence that rewards their backgrounds and thinking styles, and idolize \u201cgenius.\u201d They\u2019re not actually neurodivergent, but play on stereotypes of autistic savants to cover for their pathological greed and lack of empathy.\n\nTheir visual and linguistic taste is mid because taste is not valued in the Business Borg aesthetic. In fact, there\u2019s a certain pride in prompting things without having any skill, an almost gleeful snub to the perceived cultural gatekeepers \u2014 artists and writers and other creative workers \u2014 whose opinions the Business Borg disrespect because they believe that authority derives from money, not knowledge. They believe artists have wasted their time learning skills and developing taste. Academics have wasted their time studying things when information is just a click away. Business Borg don\u2019t care about anything besides making money, and don\u2019t care much how they spend it because the point is to have it, and show off that they have it.\n\nInto The Water by Ritual Howls\n\nGenAI True Believers often resemble the CEOs at the head of tech companies: wealthy, male, and white. These are also the people who are least at threat from the widespread use of generative AI, which reinforces racial and gender stereotypes and purports neutrality while serving up right-wing biases and corporate and foreign propaganda. Audrey Watters points out:\n\n\u201cComputing (in general and ed-tech specifically) has long been the bastion of white male privilege; and while there had been efforts to change that \u2013 in pipelines and on panels and whatnot \u2013 AI is clearly a re-entrenchment of that power, explicitly so with the Trump Administration\u2019s dismantling of civil rights protections, echoed by the tech industry\u2019s dismantling of its own DEI initiatives.\u201d\n\nAnil Dash describes \u201cAI-first\u201d as this year\u2019s \u201cReturn to Office.\u201d Managers didn\u2019t care about in-office culture until it was made clear that workers could carry on just fine without them; managers care about AI only insomuch as it permits controlling \u2014 and firing \u2014 workers.\n\nWhat the Business Borg aesthetic represents is more important than its appearance; it represents the dominance of ordinal thinking and the ability of moneyed power to do as it wishes without regard to law or morality \u2014 in short, the hierarchical worldview that some people are better than others and that their preferences trump their lesser\u2019s needs.\n\n\u201cWe will add your biological and technological distinctiveness to our own. Your culture will adapt to service us. Resistance is futile.\u201d \u2014 the Borg\n\nValues driving the Business Borg aesthetic\n\nTeri Kanefield breaks down Leor Zmigrod\u2019s book on ideology, explaining that \u201cAll ideologies seek a utopia.\u201d The Business Borg utopia puts billionaires and their ilk high atop society, in control via the technology they own.\n\nCore values I see uniting the Business Borg aesthetic are:\n\nonly the output matters efficiency is king quantity over quality appearance trumps reality \u201cprogress\u201d cannot be stopped\n\nValue: Only the output matters\n\nGenerative AI is being marketed to businesses as a low-cost replacement for workers that cuts steps \u2014 and collaboration \u2014 out of the process. This is a box-checking culture; all that matters is that an email was sent, a presentation was created, the newspaper had a summer reading insert, no matter the books on it were imaginary.\n\nFoundational beliefs\n\nprocess does not add value and wastes time\n\nthe world is reducible to data , and every question has one objectively correct answer\n\n, and every question has communication and collaboration are a waste of time (\u201cemail jobs\u201d)\n\n(\u201cemail jobs\u201d) experience is irrelevant\n\nOutcome: Tech reduces the complex to input and output\n\nIn contrast with the Arts and Crafts movement, the Business Borg aesthetic actively conceals human labor and venerates the wisdom of the machine. Generative text and graphics simulate a performance of human-less \u2014 cost-less \u2014 automation. Generative answers encourage a reliance on the machine to synthesize on one\u2019s behalf \u2014 and it doesn\u2019t matter to search engines that the \u201canswers\u201d their AI has provided cite sources incorrectly.\n\nHumans are perceived as sources of inefficiency under the Business Borg ideology, because they must be compensated in accordance with their skills and how much time they spend working. Generating material is rooted in devaluing both skill and process. The invented summer reading list was the result of forcing a single contractor to prepare an impossible quantity of work; generating content was the only way for the poor bloke to produce the content on budget. No one reviewed it, because Business Borg only care that the product exists.\n\nEd Zitron describes the evolution of the Business Idiot, personified by middle managers who are completely dissociated from the product they\u2019re selling and explicitly do not do work (emphasis mine):\n\n[Business Idiots] see every part of our lives as a series of inputs and outputs. They boast about how many books they\u2019ve read rather than the content of said books, about how many hours they work (even though they never, ever work that many), about high level they are in a video game they clearly don\u2019t play, about the money they\u2019ve raised and the scale they\u2019ve raised it at, and about how expensive and fancy their kitchen gadgets are. Everything is dominance, acquisition, growth and possession over any lived experience, because their world is one where the journey doesn\u2019t matter, because their journeys are riddled with privilege and the persecution of others in the pursuit of success. These people don\u2019t want to automate work, they want to automate existence. They fantasize about hitting a button and something happening, because experiencing \u2014 living! \u2014 is beneath them, or at least your lives and your wants and your joy are.\n\nValue: Efficiency is king\n\nGenerative AI produces endless content for low cost. Corporations are using Generative AI as an excuse to lay off workers and intensify the jobs of those remaining.\n\nReady Aim Fire (Owl Vision Remix) [Single] by Blue Stahli\n\nFoundational beliefs\n\nthe more mechanized a process is, the more efficient it becomes because humans are naturally inefficient\n\nOutcome: GenAI performs \u201cefficiency\u201d\n\nGenerative AI need not actually reduce work or cost to represent efficiency when mechanization is always favored over people. The Business Borg aesthetic perceives automation as efficient \u2014 hence situations where workers are paid to simulate chatbots simulating human agents on customer service platforms, Microsoft devs handhold Copilot, and GM\u2019s Cruise \u201cautonomous\u201d taxis needed remote human intervention every 4-5 miles!!!\n\nEfficiency is a code word for shareholders, just like \u201ccost-cutting,\u201d who know that these phrases mean putting the boot on workers\u2019 necks for short-term profits. This efficiency aesthetic is used to justify outrageously profitable companies continuing to slash workers *cough Microsoft* It plays out as Hollywood demolishing the screenwriter profession to save a buck on writing rooms and self-cannibalizing the development of future acting talent by forcing extras to be body scanned so they can be reproduced by AI.\n\nJeremy comments on the use of genAI in coding, noting that it\u2019s justified by claims like \u201cworking code wins\u201d \u2014 as in, what it looks like under the hood and how it\u2019s constructed don\u2019t matter. I\u2019m not a coder, but I\u2019ve seen enough HTML produced by PageMaker and other CMSs to be skeptical of the quality of any generated code \u2014 is genAI producing the coding equivalent of tables for web layout? \ud83e\udd14 The Business Borg aesthetic accepts mediocrity without understanding; easy and fast is always best. Good enough is always good enough.\n\nValue: Quantity over quality\n\nGenerative AI is billed as a good-enough tool that will speed up production. Cory Doctorow writes of the managerial push to automate with AI:\n\nThe point of using automation to weaken labor isn\u2019t just cheaper products \u2013 it\u2019s cheaper, defective products, inflicted on the unsuspecting and defenseless public who are no longer protected by workers\u2019 professionalism and pride in their jobs.\n\nEnormous by LLgL TNDR\n\nFoundational beliefs\n\nprofit today trumps tomorrow\u2019s concerns \u2014 someone else will have to fix \u2018that problem\u2019\n\n\u2014 someone else will have to fix \u2018that problem\u2019 money is authority \u2014 individual rankings can be quantified by wealth\n\n\u2014 individual rankings can be quantified by wealth anything that cannot be quantified is not important\n\nOutcome: Algorithms produce culture-like media, not culture\n\nGenAI and corporate web algorithms are intended to absorb attention like black absorbs light; they are designed to maximize engagement at minimal cost. Spotify benefits from degradation of culture. Netflix is a chum machine, built to spew content that people watch in the background. Kyle Raymond Fitzpatrick laments that \u201cSo much of culture is edgeless and soft, intended for us to astroglide through it without any friction or doubt as we half-watch in 1.5x speed, to consume as if we really are incapable of critical thoughts, all to appeal to everyone and no one at the same time.\u201d Internet Age capitalism produces entertainment that is culture-like, writes Nicholas Carr:\n\n\u201cWhat\u2019s really being tested here is human taste. Will we accept a simulacrum of a work of art or craft as a satisfactory substitute for the real thing?\u201d\n\nSo long as people accept cheap low-quality cultural media, businesses have little incentive to pay for higher quality. Generative AI becomes an attack on culture because it drowns out human-made art and writing so it\u2019s impossible to find amidst the Great Social Media Garbage Patch. Aidan Walker describes all this as \u2018 slop capitalism \u2018: \u201can economic and cultural system in which the primary product is slop and the primary activity is the destruction of value rather than its creation.\u201d\n\nValue: Appearances trump reality\n\nGenerative AI produces plausible graphics that we interpret as real-adjacent and plausible combinations of text that we interpret as communication.\n\nFoundational Beliefs\n\nperformance of dominance builds and reinforces real power\n\nOutcome: GenAI supersedes reality with performance and symbolism\n\nThe Business Borg aesthetic celebrates audacious performances of infinite wealth and indefinite power: adopters and evangelists for genAI also embrace filling streets with \u201cself-driving\u201d cars over the protests of residents and first responders, raining space debris onto inhabited areas from slapdash rocket ships, paying a fortune for a banana taped to a wall as conceptual art and eating it, paying women to have their IVF babies so they can seem fertile without fucking. While the culture at large has shifted towards inconspicuous consumption, luxury that requires knowledge to see, Business Borg signal their wealth blatantly.\n\nA repeating theme of the Business Borg aesthetic is replacing reality with life-like hyperreality: the simulation of conversation and connection with chatbots, the renderings of war and disaster mimicking photojournalism to push political narratives, the \u201cresurrected\u201d extinct species, the green screen action sequences that don\u2019t track. A photograph of reality may seem less real than a generated image if it does not abide by our expectations. (See also: reading \u201chuman vibes\u201d into LLM responses)\n\nLook at the snoozefest kayfabe of modern MMA: it\u2019s as much about the smack talk at weigh-ins as the fights themselves, athletes winning by points for \u201ccontrolling the octagon\u201d instead of fighting to win by KO or submission \u2014 look no further than that embarrassment of a so-called fight between Mike Tyson and the YouTuber, who danced around an old man till he got tired so he could win by decision and say he\u2019d beaten a legend \ud83d\ude34\n\nEffective altruism performs charity that can never be disproven, despite its claims of data-driven decisions, because it pretends to think at such long-term scales that known, existing suffering pales in comparison to the imagined future suffering they claim to be protecting against. It cosplays rationalism. Wannabe Foundation shit.\n\nThe Business Borg aesthetic is expressed as cheap cruft disguised as something of substance: words that look like writing, but are not; images that look like art, but are not; fights that look fighting, but are not.\n\nAs Ed Zitron sums up, it\u2019s a \u201csymbolic economy:\u201d\n\nThe sweeping changes we\u2019ve seen, both in our economy and in our society, has led to an unprecedented, gilded age of bullshit where nothing matters, and things \u2014 things of actual substance \u2014 matter nothing.\n\nTraditionally, we have been wary of those wearing a mask, explains Dan Fox in Pretentiousness, suspecting they are something other than what they present themselves as \u2014 but authenticity has faded as a cultural value. Business Borg embrace performance as more true than reality; what someone wants to be is more important than what they are now. What a technology could be is more important than what it currently is.\n\nValue: \u201cProgress\u201d cannot be stopped\n\nThe rapacious assimilation of copyrighted material to train models, the dismissal of AI\u2019s environmental cost and induced demand, and PR campaign reorienting the conversation around AI to its possible future harms as a distraction from the harms happening today all build from the same foundation: theoretically, scaling towards the pursuit of AGI \u2014 but more likely securing the funding to inextricably embed AI into our society and infrastructure.\n\nFoundational Beliefs\n\ntechnology always represents progress\n\nthe ends justify the means\n\nOutcome: GenAI is full speed ahead, no matter what\n\nAdvocates of generative AI are pursuing a brute force, fear-mongering approach to deregulation and preventing regulation. They are defying legality in obtaining training data while declaring the technology a fait accompli: resistance is futile. Their web crawlers ignore robots.txt, breaking the common courtesy of the web. They dismiss complaints about bias, claiming that they can fix that in the future. (And what incentive will there be to ever go back and fix it, once we can\u2019t avoid using it?) They\u2019re hoping enough of us will get hooked on it \u2014 and government and corporations will integrate enough of it too deeply in their processes \u2014 to allow their legally-suspect models to be shut down.\n\nRyan Broderick observes:\n\nWhat the AI arms race has actually done is codified and automated all of the failures of the previous internet era. Extremism, misinformation, harassment, non-consensual sexual material, and scams \u2014 all the content that tech companies promised to fix, but never could at scale \u2014 are now trapped in some AI\u2019s black box of data.\n\nAGI, which has been \u201cthree to five years away\u201d for years, is a Bezzle. As Cory Doctorow quotes JK Gabraith, a Bezzle is \u201cthe magic interval when a confidence trickster knows he has the money he has appropriated but the victim does not yet understand that he has lost it.\u201d John Kay expands that \u201cThe joy of the bezzle is that two people \u2013 each ignorant of the other\u2019s existence and role \u2013 can enjoy the same wealth.\u201d\n\nScams and pyramid schemes are seductive to Business Borg, like Elizabeth Holmes and Sam Bankman-Fried, because the only things they value are money and power, not making things that actually work (see: Cybertrucks falling apart, SpaceX rockets exploding); the trick is not to get caught. The goal is to surf the edge of profit as long as possible.\n\nRight now the whole stock market is bloated by outrageous NVIDIA, Google, Microsoft and Meta valuations based on the potential of generative AI to create a new \u201cessential\u201d utility, a service that everyone will need to subscribe to, forever \u2014 and so many people have bought into the grift so hard they\u2019ll do anything they can to make it a success, or at least rake in the cash for as long as they can. Then when the bubble pops, the corporations will get bailed out on the taxpayer dollar, while we workers resign ourselves to working until we die since we\u2019ve privatized retirement \ud83d\ude43\n\nGenAI is being deployed to control\n\nArtists are under attack, culturally and economically, so it is only fair that they point out the quintessential thing that artists offer that generative AI cannot: taste. Professional (and amateur) artists have devoted a great deal of time to developing their taste. The oligarchs who run Silicon Valley are steeped in their own rightness and devalue anything that isn\u2019t their expertise; if they don\u2019t know about it, it must not be important. To the Business Borg, taste is not an essential component of production.\n\nMandy Brown describes how genAI is used to undercut expertise (emphasis mine):\n\nIt\u2019s instructive that one of the mechanisms for perpetuating this ideology are chattering bots that speak both fact and falsehood in the same servile and confident tone, their makers unconcerned with the difference. In fact, their makers seem entirely concerned with obviating that difference, with disappearing distinctions between knowledge and ignorance, without which truth becomes entirely a product of power. [\u2026] [I]f those in power cannot prove that a great many people are already inferior then they will bring that inferiority about by forcing them to use a tool that diminishes their intellectual and creative capacity.\n\nBusiness Borg like generative AI because it grants them cultural power that they have not been able to dominate on their own. They lack skill, so they devalue skill. They need content, so they make an infinite content machine and conscript users as unwitting factory workers to provide free labor. The relentless promotion of GenAI is an attempt by corporations to capture cultural value by siphoning off value from human-made aesthetics. Generative AI is billionaires punching down on artists and the working class.\n\n[\u2026] I think Miyazaki\u2019s style is still valuable But it is now, in a day, valuable in a different way. What was once valuable in the awareness of painstaking labor, beautiful stories, and coherent aesthetic across the previous two qualities. PLUS our reception to it. Is now valuable PURELY in our reception to, and reproduction of, the aesthetic. [\u2026] \u2014 Reggie James (@HipCityReg) March 27, 2025\n\nGenerative AI has intentionally been molded to attack artists and diminish cultural literacy. Aidan Walker argues (read this whole piece if you liked my post):\n\nAI doesn\u2019t have to be an antagonist to schools, work, and civil society \u2014 they\u2019ve just designed and trained it that way\u2026 There could be guardrails in place, they could pay the producers of their training data, they could give the people a say in how the models are made and deployed \u2014 we could do a thousand things differently than the way they\u2019re being done now.\n\nGenerative AI \u2014 both imagery and text \u2014 is inextricable from the corporate vision for its use: a world in which workers are powerless and worthless, replaced by \u201cfree\u201d generated material. Corporate GenAI cannot be separated from the purpose for its use or the billionaires and billionaire-wannabes who shill for it. The Business Borg aesthetic imbues a sheen of venality.\n\nFurther reading:\n\nGenAI is Our Polyester by W. David Marx\n\nEconomics & labor rights in AI skepticism by Henry from online\n\nYou don\u2019t hate AI; You hate\u2026 : a collection by Mita Williams\n\nDispatch from the Trenches of the Butlerian Jihad by ADH\n\nThe other way the [Butlerian Jihad] metaphor is proving apt is the deep-seated, almost spiritual nature of anti-AI sentiment. It\u2019s not just more Luddism. Many people \u2014 though hardly all, given the popularity of AI products \u2014 sense that there is something grotesque about these simulacra, the people who push them on us, this whole affair. That aversion to the technological profane holds even when various stated objections to AI are supposedly addressed or nitpicked to death.\n\nSee also:\n\nWe need solidarity across creative industries", "label": 1}
{"title": "Part 1: A Survey of Analytics Engineering Work at Netflix", "url": "https://netflixtechblog.com/part-1-a-survey-of-analytics-engineering-work-at-netflix-d761cfd551ee?source=collection_home---4------8-----------------------", "content": "Part 1: A Survey of Analytics Engineering Work at Netflix Netflix Technology Blog 7 min read \u00b7 Dec 17, 2024 -- 2 Listen Share\n\nThis article is the first in a multi-part series sharing a breadth of Analytics Engineering work at Netflix, recently presented as part of our annual internal Analytics Engineering conference. We kick off with a few topics focused on how we\u2019re empowering Netflix to efficiently produce and effectively deliver high quality, actionable analytic insights across the company. Subsequent posts will detail examples of exciting analytic engineering domain applications and aspects of the technical craft.\n\nAt Netflix, we seek to entertain the world by ensuring our members find the shows and movies that will thrill them. Analytics at Netflix powers everything from understanding what content will excite and bring members back for more to how we should produce and distribute a content slate that maximizes member joy. Analytics Engineers deliver these insights by establishing deep business and product partnerships; translating business challenges into solutions that unblock critical decisions; and designing, building, and maintaining end-to-end analytical systems.\n\nEach year, we bring the Analytics Engineering community together for an Analytics Summit \u2014 a 3-day internal conference to share analytical deliverables across Netflix, discuss analytic practice, and build relationships within the community. We covered a broad array of exciting topics and wanted to spotlight a few to give you a taste of what we\u2019re working on across Analytics Engineering at Netflix!\n\nDataJunction: Unifying Experimentation and Analytics\n\nYian Shang, Anh Le\n\nAt Netflix, like in many organizations, creating and using metrics is often more complex than it should be. Metric definitions are often scattered across various databases, documentation sites, and code repositories, making it difficult for analysts and data scientists to find reliable information quickly. This fragmentation leads to inconsistencies and wastes valuable time as teams end up reinventing metrics or seeking clarification on definitions that should be standardized and readily accessible.\n\nEnter DataJunction (DJ). DJ acts as a central store where metric definitions can live and evolve. Once a metric owner has registered a metric into DJ, metric consumers throughout the organization can apply that same metric definition to a set of filtered records and aggregate to any dimensional grain.\n\nAs an example, imagine an analyst wanting to create a \u201cTotal Streaming Hours\u201d metric. To add this metric to DJ, they need to provide two pieces of information:\n\nThe fact table that the metric comes from:\n\nSELECT\n\naccount_id, country_iso_code, streaming_hours\n\nFROM streaming_fact_table\n\nThe metric expression:\n\n`SUM(streaming_hours)`\n\nThen metric consumers throughout the organization can call DJ to request either the SQL or the resulting data. For example,\n\ntotal_streaming_hours of each account:\n\ndj.sql(metrics=[\u201ctotal_streaming_hours\u201d], dimensions=[\u201caccount_id\u201d]))\n\ntotal_streaming_hours of each country:\n\ndj.sql(metrics=[\u201ctotal_streaming_hours\u201d], dimensions=[\u201ccountry_iso_code\u201d]))\n\ntotal_streaming_hours of each account in the US:\n\ndj.sql(metrics=[\u201ctotal_streaming_hours\u201d], dimensions=[\u201ccountry_iso_code\u201d], filters=[\u201ccountry_iso_code = \u2018US\u2019\u201d]))\n\nThe key here is that DJ can perform the dimensional join on users\u2019 behalf. If country_iso_code doesn\u2019t already exist in the fact table, the metric owner only needs to tell DJ that account_id is the foreign key to an `users_dimension_table` (we call this process \u201cdimension linking\u201d). DJ then can perform the joins to bring in any requested dimensions from `users_dimension_table`.\n\nThe Netflix Experimentation Platform heavily leverages this feature today by treating cell assignment as just another dimension that it asks DJ to bring in. For example, to compare the average streaming hours in cell A vs cell B, the Experimentation Platform relies on DJ to bring in \u201ccell_assignment\u201d as a user\u2019s dimension (no different from country_iso_code). A metric can therefore be defined once in DJ and be made available across analytics dashboards and experimentation analysis.\n\nDJ has a strong pedigree\u2013there are several prior semantic layers in the industry (e.g. Minerva at Airbnb; dbt Transform, Looker, and AtScale as paid solutions). DJ stands out as an open source solution that is actively developed and stress-tested at Netflix. We\u2019d love to see DJ easing your metric creation and consumption pain points!\n\nLORE: How we\u2019re democratizing analytics at Netflix\n\nApurva Kansara\n\nAt Netflix, we rely on data and analytics to inform critical business decisions. Over time, this has resulted in large numbers of dashboard products. While such analytics products are tremendously useful, we noticed a few trends:\n\nA large portion of such products have less than 5 MAU (monthly active users) We spend a tremendous amount of time building and maintaining business metrics and dimensions We see inconsistencies in how a particular metric is calculated, presented, and maintained across the Data & Insights organization. It is challenging to scale such bespoke solutions to ever-changing and increasingly complex business needs.\n\nAnalytics Enablement is a collection of initiatives across Data & Insights all focused on empowering Netflix analytic practitioners to efficiently produce and effectively deliver high-quality, actionable insights.\n\nSpecifically, these initiatives are focused on enabling analytics rather than on the activities that produce analytics (e.g., dashboarding, analysis, research, etc.).\n\nAs part of broad analytics enablement across all business domains, we invested in a chatbot to provide real insights to our end users using the power of LLM. One reason LLMs are well suited for such problems is that they tie the versatility of natural language with the power of data query to enable our business users to query data that would otherwise require sophisticated knowledge of underlying data models.\n\nBesides providing the end user with an instant answer in a preferred data visualization, LORE instantly learns from the user\u2019s feedback. This allows us to teach LLM a context-rich understanding of internal business metrics that were previously locked in custom code for each of the dashboard products.\n\nSome of the challenges we run into:\n\nGaining user trust: To gain our end users\u2019 trust, we focused on our model\u2019s explainability. For example, LORE provides human-readable reasoning on how it arrived at the answer that users can cross-verify. LORE also provides a confidence score to our end users based on its grounding in the domain space.\n\nTraining: We created easy-to-provide feedback using \ud83d\udc4d and \ud83d\udc4e with a fully integrated fine-tuning loop to allow end-users to teach new domains and questions around it effectively. This allowed us to bootstrap LORE across several domains within Netflix.\n\nDemocratizing analytics can unlock the tremendous potential of data for everyone within the company. With Analytics enablement and LORE, we\u2019ve enabled our business users to truly have a conversation with the data.\n\nLeveraging Foundational Platform Data to enable Cloud Efficiency Analytics\n\nJ Han, Pallavi Phadnis\n\nAt Netflix, we use Amazon Web Services (AWS) for our cloud infrastructure needs, such as compute, storage, and networking to build and run the streaming platform that we love. Our ecosystem enables engineering teams to run applications and services at scale, utilizing a mix of open-source and proprietary solutions. In order to understand how efficiently we operate in this diverse technological landscape, the Data & Insights organization partners closely with our engineering teams to share key efficiency metrics, empowering internal stakeholders to make informed business decisions.\n\nThis is where our team, Platform DSE (Data Science Engineering), comes in to enable our engineering partners to understand what resources they\u2019re using, how effectively they utilize those resources, and the cost associated with their resource usage. By creating curated datasets and democratizing access via a custom insights app and various integration points, downstream users can gain granular insights essential for making data-driven, cost-effective decisions for the business.\n\nTo address the numerous analytic needs in a scalable way, we\u2019ve developed a two-component solution:\n\nFoundational Platform Data (FPD): This component provides a centralized data layer for all platform data, featuring a consistent data model and standardized data processing methodology. We work with different platform data providers to get inventory, ownership, and usage data for the respective platforms they own. Cloud Efficiency Analytics (CEA): Built on top of FPD, this component offers an analytics data layer that provides time series efficiency metrics across various business use cases. Once the foundational data is ready, CEA consumes inventory, ownership, and usage data and applies the appropriate business logic to produce cost and ownership attribution at various granularities.\n\nAs the source of truth for efficiency metrics, our team\u2019s tenants are to provide accurate, reliable, and accessible data, comprehensive documentation to navigate the complexity of the efficiency space, and well-defined Service Level Agreements (SLAs) to set expectations with downstream consumers during delays, outages, or changes.\n\nLooking ahead, we aim to continue onboarding platforms, striving for nearly complete cost insight coverage. We\u2019re also exploring new use cases, such as tailored reports for platforms, predictive analytics for optimizing usage and detecting anomalies in cost, and a root cause analysis tool using LLMs.\n\nUltimately, our goal is to enable our engineering organization to make efficiency-conscious decisions when building and maintaining the myriad of services that allows us to enjoy Netflix as a streaming service. For more detail on our modeling approach and principles, check out this post!", "label": 0}
{"title": "Dockerizing my website and services", "url": "https://lifeofpablo.com/blog/dockerizing-my-website-and-services", "content": "Dockerizing my website and services\n\nThis post was written in English (en_US).\n\nThis week I started using Docker (again). I have used it one or twice through out the years. My first time being in 2015. I was so opposed or timid in the use of using Docker. I was so used to doing things the \"hard way\" otherwise known as manual install. So My friend Mani, has been showing me his Kubernetes setup and talking about Docker. I thought it was pretty rad! Just the way things flowed and how easy it is to stop and start a container. This is something he has started for a while. I hope to become well versed in this.\n\nThis leads to what is mentioned in the title of this post. I am in the process of learning to understand containers, Kubernetes, miniKube, docker and how this all ties together. I'll start with Docker and learn Kubernetes side-by-side and hope to understand. The goal is to containerize all my websites and services to keep up with the times and become a better developer.\n\nMy first successful implementation of a docker container was, wait for it..... Can you guess what it is? If you guessed Vouch Proxy! You win an cookie \ud83c\udf6a! Hooray! For the record, not a browser cookie. Those are not tasty or pleasing.\n\nIt was pretty simple once I figured out how to run the commands to adjust to my needs. I used my existing configuration file for Vouch Proxy but with some redactions. So, if you are seeing this post, or signed into my website, vouch proxy is running on docker!\n\nNext step is my website which runs on php and uses Datenstrom Yellow as the flat-file cms.\n\nI am excited for this new phase of my developer life. I have so much to learn! Shout out to Mani for leading me in this direction.", "label": 1}
{"title": "Behind the Scenes: Building a Robust Ads Event Processing Pipeline", "url": "https://netflixtechblog.com/behind-the-scenes-building-a-robust-ads-event-processing-pipeline-e4e86caf9249?source=collection_home---4------1-----------------------", "content": "Behind the Scenes: Building a Robust Ads Event Processing Pipeline Netflix Technology Blog 8 min read \u00b7 May 9, 2025 -- 12 Listen Share\n\nKinesh Satiya\n\nIntroduction\n\nIn a digital advertising platform, a robust feedback system is essential for the lifecycle and success of an ad campaign. This system comprises of diverse sub-systems designed to monitor, measure, and optimize ad campaigns. At Netflix, we embarked on a journey to build a robust event processing platform that not only meets the current demands but also scales for future needs. This blog post delves into the architectural evolution and technical decisions that underpin our Ads event processing pipeline.\n\nAd serving acts like the \u201cbrain\u201d \u2014 making decisions, optimizing delivery and ensuring right Ad is shown to the right member at the right time. Meanwhile, ad events, after an Ad is rendered, function like \u201cheartbeats\u201d, continuously providing real-time feedback (oxygen/nutrients) that fuels better decision-making, optimizations, reporting, measurement, and billing. Expanding on this analogy:\n\nJust as the brain relies on continuous blood flow, ad serving depends on a steady stream of ad events to adjust next ad serving decision, frequency capping, pacing, and personalization.\n\nIf the nervous system stops sending signals (ad events stop flowing), the brain (ad serving) lacks critical insights and starts making poor decisions or even fails.\n\nThe healthier and more accurate the event stream (just like strong heart function), the better the ad serving system can adapt, optimize, and drive business outcomes.\n\nLet\u2019s dive into the journey of building this pipeline.\n\nThe Pilot\n\nIn November 2022, we launched a brand new basic ads plan, in partnership with Microsoft. The software systems extended the existing Netflix playback systems to play ads. Initially, the system was designed to be simple, secure, and efficient, with an underlying ethos of device-originated and server-proxied operations. The system consisted of three main components: the Microsoft Ad Server, Netflix Ads Manager, and Ad Event Handler. Each ad served required tracking to ensure the feedback loop functioned effectively, providing the external ad server with insights on impressions, frequency capping (advertiser policy that limits the number of times a user sees a specific ad), and monetization processes.\n\nKey features of this system include:", "label": 0}
{"title": "Video annotator: a framework for efficiently building video classifiers using vision-language models and active learning", "url": "https://netflixtechblog.com/video-annotator-building-video-classifiers-using-vision-language-models-and-active-learning-8ebdda0b2db4?source=collection_home---4------23-----------------------", "content": "Video annotator: a framework for efficiently building video classifiers using vision-language models and active learning Netflix Technology Blog 6 min read \u00b7 Jun 19, 2024 -- 2 Listen Share\n\nAmir Ziai, Aneesh Vartakavi, Kelli Griggs, Eugene Lok, Yvonne Jukes, Alex Alonso, Vi Iyengar, Anna Pulido\n\nIntroduction\n\nProblem\n\nHigh-quality and consistent annotations are fundamental to the successful development of robust machine learning models. Conventional techniques for training machine learning classifiers are resource intensive. They involve a cycle where domain experts annotate a dataset, which is then transferred to data scientists to train models, review outcomes, and make changes. This labeling process tends to be time-consuming and inefficient, sometimes halting after a few annotation cycles.\n\nImplications\n\nConsequently, less effort is invested in annotating high-quality datasets compared to iterating on complex models and algorithmic methods to improve performance and fix edge cases. As a result, ML systems grow rapidly in complexity.\n\nFurthermore, constraints on time and resources often result in leveraging third-party annotators rather than domain experts. These annotators perform the labeling task without a deep understanding of the model\u2019s intended deployment or usage, often making consistent labeling of borderline or hard examples, especially in more subjective tasks, a challenge.\n\nThis necessitates multiple review rounds with domain experts, leading to unexpected costs and delays. This lengthy cycle can also result in model drift, as it takes longer to fix edge cases and deploy new models, potentially hurting usefulness and stakeholder trust.\n\nSolution\n\nWe suggest that more direct involvement of domain experts, using a human-in-the-loop system, can resolve many of these practical challenges. We introduce a novel framework, Video Annotator (VA), which leverages active learning techniques and zero-shot capabilities of large vision-language models to guide users to focus their efforts on progressively harder examples, enhancing the model\u2019s sample efficiency and keeping costs low.\n\nVA seamlessly integrates model building into the data annotation process, facilitating user validation of the model before deployment, therefore helping with building trust and fostering a sense of ownership. VA also supports a continuous annotation process, allowing users to rapidly deploy models, monitor their quality in production, and swiftly fix any edge cases by annotating a few more examples and deploying a new model version.\n\nThis self-service architecture empowers users to make improvements without active involvement of data scientists or third-party annotators, allowing for fast iteration.\n\nVideo understanding\n\nWe design VA to assist in granular video understanding which requires the identification of visuals, concepts, and events within video segments. Video understanding is fundamental for numerous applications such as search and discovery, personalization, and the creation of promotional assets. Our framework allows users to efficiently train machine learning models for video understanding by developing an extensible set of binary video classifiers, which power scalable scoring and retrieval of a vast catalog of content.\n\nVideo classification\n\nVideo classification is the task of assigning a label to an arbitrary-length video clip, often accompanied by a probability or prediction score, as illustrated in Fig 1.\n\nFig 1- Functional view of a binary video classifier. A few-second clip from \u201dOperation Varsity Blues: The College Admissions Scandal\u201d is passed to a binary classifier for detecting the \u201destablishing shots\u201d label. The classifier outputs a very high score (score is between 0 and 1), indicating that the video clip is very likely an establishing shot. In filmmaking, an establishing shot is a wide shot (i.e. video clip between two consecutive cuts) of a building or a landscape that is intended for establishing the time and location of the scene.\n\nVideo understanding via an extensible set of video classifiers\n\nBinary classification allows for independence and flexibility, allowing us to add or improve one model independent of the others. It also has the additional benefit of being easier to understand and build for our users. Combining the predictions of multiple models allows us a deeper understanding of the video content at various levels of granularity, illustrated in Fig 2.\n\nFig 2- Three video clips and the corresponding binary classifier scores for three video understanding labels. Note that these labels are not mutually exclusive. Video clips are from Operation Varsity Blues: The College Admissions Scandal, 6 Underground, and Leave The World Behind, respectively.\n\nVideo Annotator (VA)\n\nIn this section, we describe VA\u2019s three-step process for building video classifiers.\n\nStep 1 \u2014 search\n\nUsers begin by finding an initial set of examples within a large, diverse corpus to bootstrap the annotation process. We leverage text-to-video search to enable this, powered by video and text encoders from a Vision-Language Model to extract embeddings. For example, an annotator working on the establishing shots model may start the process by searching for \u201cwide shots of buildings\u201d, illustrated in Fig 3.\n\nFig 3- Step 1 \u2014 Text-to-video search to bootstrap the annotation process.\n\nStep 2 \u2014 active learning\n\nThe next stage involves a classic Active Learning loop. VA then builds a lightweight binary classifier over the video embeddings, which is subsequently used to score all clips in the corpus, and presents some examples within feeds for further annotation and refinement, as illustrated in Fig 4.\n\nFig 4- Step 2 \u2014 Active Learning loop. The annotator clicks on build, which initiates classifier training and scoring of all clips in a video corpus. Scored clips are organized in four feeds.\n\nThe top-scoring positive and negative feeds display examples with the highest and lowest scores respectively. Our users reported that this provided a valuable indication as to whether the classifier has picked up the correct concepts in the early stages of training and spot cases of bias in the training data that they were able to subsequently fix. We also include a feed of \u201cborderline\u201d examples that the model is not confident about. This feed helps with discovering interesting edge cases and inspires the need for labeling additional concepts. Finally, the random feed consists of randomly selected clips and helps to annotate diverse examples which is important for generalization.\n\nThe annotator can label additional clips in any of the feeds and build a new classifier and repeat as many times as desired.\n\nStep 3 \u2014 review\n\nThe last step simply presents the user with all annotated clips. It\u2019s a good opportunity to spot annotation mistakes and to identify ideas and concepts for further annotation via search in step 1. From this step, users often go back to step 1 or step 2 to refine their annotations.\n\nExperiments\n\nTo evaluate VA, we asked three video experts to annotate a diverse set of 56 labels across a video corpus of 500k shots. We compared VA to the performance of a few baseline methods, and observed that VA leads to the creation of higher quality video classifiers. Fig 5 compares VA\u2019s performance to baselines as a function of the number of annotated clips.\n\nFig 5- Model quality (i.e. Average Precision) as a function of the number of annotated clips for the \u201cestablishing shots\u201d label. We observe that all methods outperform the baseline, and that all methods benefit from additional annotated data, albeit to varying degrees.\n\nYou can find more details about VA and our experiments in this paper.\n\nConclusion\n\nWe presented Video Annotator (VA), an interactive framework that addresses many challenges associated with conventional techniques for training machine learning classifiers. VA leverages the zero-shot capabilities of large vision-language models and active learning techniques to enhance sample efficiency and reduce costs. It offers a unique approach to annotating, managing, and iterating on video classification datasets, emphasizing the direct involvement of domain experts in a human-in-the-loop system. By enabling these users to rapidly make informed decisions on hard samples during the annotation process, VA increases the system\u2019s overall efficiency. Moreover, it allows for a continuous annotation process, allowing users to swiftly deploy models, monitor their quality in production, and rapidly fix any edge cases.\n\nThis self-service architecture empowers domain experts to make improvements without the active involvement of data scientists or third-party annotators, and fosters a sense of ownership, thereby building trust in the system.\n\nWe conducted experiments to study the performance of VA, and found that it yields a median 8.3 point improvement in Average Precision relative to the most competitive baseline across a wide-ranging assortment of video understanding tasks. We release a dataset with 153k labels across 56 video understanding tasks annotated by three professional video editors using VA, and also release code to replicate our experiments.", "label": 0}
{"title": "Usability and safety updates to Google Auth Platform", "url": "https://developers.googleblog.com/en/usability-and-safety-updates-to-google-auth-platform/", "content": "Millions of developers rely on Google\u2019s identity platform for user authentication and the ability to authorize access to hundreds of APIs. Underpinning the platform is one of the world\u2019s largest implementations of the OAuth 2.0 protocol and related OpenID Connect standard, which provide a seamless, safe, and reliable way for developers to integrate with Google. We\u2019re excited to share some updates that will make the platform even more secure and easy to use.\n\n\n\nSimplified OAuth configuration in the Google Cloud Console\n\nDevelopers that use Sign in with Google for authentication or to obtain user authorization to call Google APIs need to register their apps and websites to create client credentials. For developers that use the Google Cloud Console, OAuth configuration pages previously lived in the APIs & Services section. Now, these pages have their own dedicated navigation section called Google Auth Platform. As part of this change, we\u2019ve made it easier to register new projects, reduced the time it takes to update app configurations, and added more helpful guidance for developers. Stay tuned for more improvements in the coming months, including a better onboarding wizard, simplified OAuth scope management, and changes to make app verification faster and more transparent.\n\nFor developers who use OAuth capabilities through other consoles like Firebase or Apps Script, your experience on those products remains unchanged.\n\n\n\nChange to how OAuth client secrets are displayed\n\nSome OAuth clients are required to use a \u201csecret\u201d when making authentication and authorization requests. The client secret is like a password for a website or application, so it\u2019s critical to protect these strings to ensure the security and privacy of user accounts and data.\n\nHistorically, developers have been able to view and download their own client secrets in the Google Cloud Console, Firebase Console, and other places across Google developer products. Starting in June, we\u2019ll start masking OAuth secrets in the client management pages of the Google Cloud Console. As an aid to help identify them, developer consoles will show the last few characters.", "label": 0}
{"title": "TensorFlow Lite is now LiteRT", "url": "https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/", "content": "LiteRT, part of the Google AI Edge suite of tools, is the runtime that lets you seamlessly deploy ML and AI models on Android, iOS, and embedded devices. With AI Edge's robust model conversion and optimization tools, you can ready both open-source and custom models for on-device development.\n\nSince its debut in 2017, TFLite has enabled developers to bring ML-powered experiences to over 100K apps running on 2.7B devices. More recently, TFLite has grown beyond its TensorFlow roots to support models authored in PyTorch , JAX , and Keras with the same leading performance. The name LiteRT captures this multi-framework vision for the future: enabling developers to start with any popular framework and run their model on-device with exceptional performance.\n\nLiteRT (short for Lite Runtime) is the new name for TensorFlow Lite (TFLite). While the name is new, it's still the same trusted, high-performance runtime for on-device AI, now with an expanded vision.\n\nThis change will roll out progressively. Starting today, you\u2019ll see the LiteRT name reflected in the developer documentation, which is moving to ai.google.dev/edge/litert, and in other references across the AI Edge website. The documentation at tensorflow.org/lite now redirects to corresponding pages at ai.google.dev/edge/litert.\n\nThe main TensorFlow brand will not be affected, nor will apps already using TensorFlow Lite.\n\n\n\nHow to access LiteRT\n\nOur goal is that this change is minimally disruptive, requiring as few code changes from developers as possible.\n\nIf you currently use TensorFlow Lite via packages, you\u2019ll need to update any dependencies to use the new LiteRT from Maven, PyPi, Cocoapods.\n\nIf you currently use TensorFlow Lite via Google Play Services, no change is necessary at this time.\n\nIf you currently build TensorFlow Lite from source, please continue building from the TensorFlow repo until code has been fully moved to the new LiteRT repo later this year.\n\n\n\nFrequently asked questions\n\n\n\n1. What is changing beyond the new name, LiteRT?\n\nFor now, the only change is the new name, LiteRT. Your production apps will not be affected. With a new name and refreshed vision, look out for more updates coming to LiteRT, improving how you deploy classic ML models, LLMs, and diffusion models with GPU and NPU acceleration across platforms.\n\n\n\n2. What\u2019s happening to the TensorFlow Lite Support Library (including TensorFlow Lite Tasks)?\n\nThe TensorFlow Lite support library and TensorFlow Lite Tasks will remain in the /tensorflow repository at this time. We encourage you to use MediaPipe Tasks for future development.\n\n\n\n3. What\u2019s happening to TensorFlow Lite Model Maker?\n\nYou can continue to access TFLite Model Maker via https://pypi.org/project/tflite-model-maker/\n\n\n\n4. What if I want to contribute code?\n\nFor now, please contribute code to the existing TensorFlow Lite repository. We\u2019ll make a separate announcement when we\u2019re ready for contributions to the LiteRT repository.\n\n\n\n5. What\u2019s happening to the .tflite file extension and file format?\n\nNo changes are being made to the .tflite file extension or format. Conversion tools will continue to output .tflite flatbuffer files, and .tflite files will be readable by LiteRT.\n\n\n\n6. How do I convert models to .tflite format?\n\nFor Tensorflow, Keras and Jax you can continue to use the same flows. For PyTorch support check out ai-edge-torch.\n\n\n\n7. Will there be any changes to classes and methods?\n\nNo. Aside from package names, you won\u2019t have to change any code you\u2019ve written for now.\n\n\n\n8. Will there be any changes to TensorFlow.js?\n\nNo, TensorFlow.js will continue to function independently as part of the Tensorflow codebase.\n\n\n\n9. My production app uses TensorFlow Lite. Will it be affected?\n\nApps that have already deployed TensorFlow Lite will not be affected. This includes apps that access TensorFlow Lite via Google Play Services. (TFLite is compiled into the apps at build time, so once they\u2019re deployed, apps have no dependency.)\n\n\n\n10. Why \u201cLiteRT\u201d?\n\n\u201cLiteRT\u201d (short for Lite Runtime) reflects the legacy of TensorFlow Lite, a pioneering \u201clite\u201d, on-device runtime, plus Google\u2019s commitment to supporting today\u2019s thriving multi-framework ecosystem.\n\n\n\n11. Is TensorFlow Lite still being actively developed?\n\nYes, but under the name LiteRT. Active development will continue on the runtime (now called LiteRT), as well as the conversion and optimization tools. To ensure you're using the most up-to-date version of the runtime, please use LiteRT.\n\n\n\n12. Where can I see examples of LiteRT in practice?\n\nYou can find examples for Python, Android, and iOS in the official LiteRT samples repo.\n\n\n\nWe\u2019re excited for the future of on-device ML, and are committed to our vision of making LiteRT the easiest to use, highest performance runtime for a wide range of models.", "label": 0}
{"title": "Weeknotes: June 14-20, 2025 \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/20/weeknotes-june-14-20-2025/", "content": "Win of the week: cooked up a giant pot of chickpeas (added a kinda-tadka of fresh rosemary sizzling in oil at the end) for a mildly awkward neighborhood potluck (where the only other handmade food was peanut butter celery sticks \ud83d\ude02 apparently my idea of potluck food is outdated)\n\nLooking forward to: finishing This Will Be Fun by E.B. Asher! I\u2019ve been in a book slump and have been enjoying this \u2014 it\u2019s gleefully goofy to balance out the characters\u2019 intense angst and pain\n\nStuff I did:\n\n13.5 hours consulting \u2014 sent over another deliverable \ud83e\uddbe\n\n0 hours writing oops\n\ntook Thursday off for Juneteenth\n\nwatched Yancey Strickler interview Nadia Asparouhova about her new book Antimemetics\u2026 may not be quite what I was expecting but I\u2019ll withhold judgment till I get the book\n\ngot an email about a new program inspired by my high school travels with my track coach and discovered I was mentioned in their promo materials?? \ud83e\udd14 still deciding how to reply\u2026\n\none virtual appointment + dentist appointment \u2014 I\u2019ve been grinding my teeth in my sleep (thanks politics) so they 3D scanned my teeth to make a mouth guard\n\nhung out with my sister\n\nbonus walk with friend but skipped regular walk for bad weather\n\nbaked my go-to coffee cake \u2014 used a smaller pan which doubled the bake time\n\nupdated my Signal settings per Cory\u2019s suggestions\n\nan hour of gardening to clear paths \u2013 again \u2013 and get rid of a bunch of plants with some kinda mildew on the leaves\u2026 I\u2019ve been trying to nurture my mildewy ninebark back to health but finally hacked almost all of it waaay back\n\n\ud83e\udd26\u200d\u2640\ufe0f I somehow failed to observe that my pine tree had candles last month, which I bought a taller ladder specifically to prune\u2026 the wood on the new growth is still green so I went for it! \ud83e\udd1e\ud83d\ude2c Husband says if the tree gets too big he\u2019s cutting it down, it\u2019s living on borrowed time anyway, so it\u2019s fine if I screw it up \u2014 and it is very possible I done fucked up, we\u2019ll find out next year I guess \ud83e\udd37\u200d\u2640\ufe0f\n\nDinners:\n\nfake chicken burgers (Quorn) + tots + ginger beer + dill pickle\n\ncauliflower shawarma + fries with toum (apparently we last ordered this exactly one year ago \ud83d\ude02)\n\nfish wraps with mango salsa and air fryer potatoes + ginger beer \u2014 cooked the tilapia from frozen, 20 minutes at 400 worked great\n\nfake chicken burgers (Impossible) with BBQ sauce and pineapple + curly fries\n\nbaked feta pasta \u2014 tried goat\u2019s milk feta, not a great batch\n\nwent out to the family Mexican place \u2014 somehow with tip and drinks it turned into $90?!\n\npigs in a blanket + baked beans + tater tots\n\nReading:\n\nRead A Bride\u2019s Story 14 by Kaoru Mori and Mammoths at the Gates by Nghi Vo and SCP-055 by qntm and CptBellman\n\nStarted reading Dangerous Fictions by Lyta Gold and This Will Be Fun by E. B. Asher\n\nContinued reading We Will Not Cancel Us by Adrienne Maree Brown\n\nAcquired free copy of In a Treacherous Court by Michelle Diener\n\nWords I looked up / concepts I learned:\n\nbiopolitics \u2014 \u201cThe governmental power of biopolitics is exerted through practices such as surveillance, healthcare policies, population control measures, gender-based laws, and the implementation of biometric identification systems.\u201d Thanks I hate it.\n\napodictically\n\n\u201caccountability sink\u201d (from Brian Merchant)\n\nhustings\n\ntendentious\n\ncompunctious\n\nShoutout to Jeremy for reminding me of the existence of Webster\u2019s 1913 Dictionary, and the paean to it by James Somers from 2014\n\nChoice phrases:\n\n\u201cThe books are not yet on the shelves, not yet touched by the mild boredom of order.\u201d\n\n\u2014 Walter Benjamin, Unpacking My Library (emphasis mine)\n\n\u201cThe fear of fiction waxes and wanes, spiking every couple of decades like some kind of hysterical cicada.\u201d\n\n\u2014 Lyta Gold, Dangerous Fictions\n\nOnline, we perform solidarity for strangers rather than engaging in hard conversations with comrades.\n\n\u2014 adrienne maree brown, We Will Not Cancel Us\n\nPretty stuff I saw:\n\nNew music I listened to:\n\nWebsite changes:\n\nNature notes:", "label": 1}
{"title": "Netflix\u2019s Distributed Counter Abstraction", "url": "https://netflixtechblog.com/netflixs-distributed-counter-abstraction-8d0c45eb66b2?source=collection_home---4------11-----------------------", "content": "Netflix\u2019s Distributed Counter Abstraction Netflix Technology Blog 19 min read \u00b7 Nov 12, 2024 -- 22 Listen Share\n\nBy: Rajiv Shringi, Oleksii Tkachuk, Kartik Sathyanarayanan\n\nIntroduction\n\nIn our previous blog post, we introduced Netflix\u2019s TimeSeries Abstraction, a distributed service designed to store and query large volumes of temporal event data with low millisecond latencies. Today, we\u2019re excited to present the Distributed Counter Abstraction. This counting service, built on top of the TimeSeries Abstraction, enables distributed counting at scale while maintaining similar low latency performance. As with all our abstractions, we use our Data Gateway Control Plane to shard, configure, and deploy this service globally.\n\nDistributed counting is a challenging problem in computer science. In this blog post, we\u2019ll explore the diverse counting requirements at Netflix, the challenges of achieving accurate counts in near real-time, and the rationale behind our chosen approach, including the necessary trade-offs.\n\nNote: When it comes to distributed counters, terms such as \u2018accurate\u2019 or \u2018precise\u2019 should be taken with a grain of salt. In this context, they refer to a count very close to accurate, presented with minimal delays.\n\nUse Cases and Requirements\n\nAt Netflix, our counting use cases include tracking millions of user interactions, monitoring how often specific features or experiences are shown to users, and counting multiple facets of data during A/B test experiments, among others.\n\nAt Netflix, these use cases can be classified into two broad categories:\n\nBest-Effort: For this category, the count doesn\u2019t have to be very accurate or durable. However, this category requires near-immediate access to the current count at low latencies, all while keeping infrastructure costs to a minimum. Eventually Consistent: This category needs accurate and durable counts, and is willing to tolerate a slight delay in accuracy and a slightly higher infrastructure cost as a trade-off.\n\nBoth categories share common requirements, such as high throughput and high availability. The table below provides a detailed overview of the diverse requirements across these two categories.\n\nDistributed Counter Abstraction\n\nTo meet the outlined requirements, the Counter Abstraction was designed to be highly configurable. It allows users to choose between different counting modes, such as Best-Effort or Eventually Consistent, while considering the documented trade-offs of each option. After selecting a mode, users can interact with APIs without needing to worry about the underlying storage mechanisms and counting methods.\n\nLet\u2019s take a closer look at the structure and functionality of the API.\n\nAPI\n\nCounters are organized into separate namespaces that users set up for each of their specific use cases. Each namespace can be configured with different parameters, such as Type of Counter, Time-To-Live (TTL), and Counter Cardinality, using the service\u2019s Control Plane.\n\nThe Counter Abstraction API resembles Java\u2019s AtomicInteger interface:\n\nAddCount/AddAndGetCount: Adjusts the count for the specified counter by the given delta value within a dataset. The delta value can be positive or negative. The AddAndGetCount counterpart also returns the count after performing the add operation.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"counter_name\": \"counter123\",\n\n\"delta\": 2,\n\n\"idempotency_token\": {\n\n\"token\": \"some_event_id\",\n\n\"generation_time\": \"2024-10-05T14:48:00Z\"\n\n}\n\n}\n\nThe idempotency token can be used for counter types that support them. Clients can use this token to safely retry or hedge their requests. Failures in a distributed system are a given, and having the ability to safely retry requests enhances the reliability of the service.\n\nGetCount: Retrieves the count value of the specified counter within a dataset.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"counter_name\": \"counter123\"\n\n}\n\nClearCount: Effectively resets the count to 0 for the specified counter within a dataset.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"counter_name\": \"counter456\",\n\n\"idempotency_token\": {...}\n\n}\n\nNow, let\u2019s look at the different types of counters supported within the Abstraction.\n\nTypes of Counters\n\nThe service primarily supports two types of counters: Best-Effort and Eventually Consistent, along with a third experimental type: Accurate. In the following sections, we\u2019ll describe the different approaches for these types of counters and the trade-offs associated with each.\n\nBest Effort Regional Counter\n\nThis type of counter is powered by EVCache, Netflix\u2019s distributed caching solution built on the widely popular Memcached. It is suitable for use cases like A/B experiments, where many concurrent experiments are run for relatively short durations and an approximate count is sufficient. Setting aside the complexities of provisioning, resource allocation, and control plane management, the core of this solution is remarkably straightforward:\n\n// counter cache key\n\ncounterCacheKey = <namespace>:<counter_name>\n\n\n\n// add operation\n\nreturn delta > 0\n\n? cache.incr(counterCacheKey, delta, TTL)\n\n: cache.decr(counterCacheKey, Math.abs(delta), TTL);\n\n\n\n// get operation\n\ncache.get(counterCacheKey);\n\n\n\n// clear counts from all replicas\n\ncache.delete(counterCacheKey, ReplicaPolicy.ALL);\n\nEVCache delivers extremely high throughput at low millisecond latency or better within a single region, enabling a multi-tenant setup within a shared cluster, saving infrastructure costs. However, there are some trade-offs: it lacks cross-region replication for the increment operation and does not provide consistency guarantees, which may be necessary for an accurate count. Additionally, idempotency is not natively supported, making it unsafe to retry or hedge requests.\n\nEdit: A note on probabilistic data structures:\n\nProbabilistic data structures like HyperLogLog (HLL) can be useful for tracking an approximate number of distinct elements, like distinct views or visits to a website, but are not ideally suited for implementing distinct increments and decrements for a given key. Count-Min Sketch (CMS) is an alternative that can be used to adjust the values of keys by a given amount. Data stores like Redis support both HLL and CMS. However, we chose not to pursue this direction for several reasons:\n\nWe chose to build on top of data stores that we already operate at scale.\n\nProbabilistic data structures do not natively support several of our requirements, such as resetting the count for a given key or having TTLs for counts. Additional data structures, including more sketches, would be needed to support these requirements.\n\nOn the other hand, the EVCache solution is quite simple, requiring minimal lines of code and using natively supported elements. However, it comes at the trade-off of using a small amount of memory per counter key.\n\nEventually Consistent Global Counter\n\nWhile some users may accept the limitations of a Best-Effort counter, others opt for precise counts, durability and global availability. In the following sections, we\u2019ll explore various strategies for achieving durable and accurate counts. Our objective is to highlight the challenges inherent in global distributed counting and explain the reasoning behind our chosen approach.\n\nApproach 1: Storing a Single Row per Counter\n\nLet\u2019s start simple by using a single row per counter key within a table in a globally replicated datastore.\n\nLet\u2019s examine some of the drawbacks of this approach:\n\nLack of Idempotency : There is no idempotency key baked into the storage data-model preventing users from safely retrying requests. Implementing idempotency would likely require using an external system for such keys, which can further degrade performance or cause race conditions.\n\n: There is no idempotency key baked into the storage data-model preventing users from safely retrying requests. Implementing idempotency would likely require using an external system for such keys, which can further degrade performance or cause race conditions. Heavy Contention: To update counts reliably, every writer must perform a Compare-And-Swap operation for a given counter using locks or transactions. Depending on the throughput and concurrency of operations, this can lead to significant contention, heavily impacting performance.\n\nSecondary Keys: One way to reduce contention in this approach would be to use a secondary key, such as a bucket_id, which allows for distributing writes by splitting a given counter into buckets, while enabling reads to aggregate across buckets. The challenge lies in determining the appropriate number of buckets. A static number may still lead to contention with hot keys, while dynamically assigning the number of buckets per counter across millions of counters presents a more complex problem.\n\nLet\u2019s see if we can iterate on our solution to overcome these drawbacks.\n\nApproach 2: Per Instance Aggregation\n\nTo address issues of hot keys and contention from writing to the same row in real-time, we could implement a strategy where each instance aggregates the counts in memory and then flushes them to disk at regular intervals. Introducing sufficient jitter to the flush process can further reduce contention.\n\nHowever, this solution presents a new set of issues:\n\nVulnerability to Data Loss : The solution is vulnerable to data loss for all in-memory data during instance failures, restarts, or deployments.\n\n: The solution is vulnerable to data loss for all in-memory data during instance failures, restarts, or deployments. Inability to Reliably Reset Counts : Due to counting requests being distributed across multiple machines, it is challenging to establish consensus on the exact point in time when a counter reset occurred.\n\n: Due to counting requests being distributed across multiple machines, it is challenging to establish consensus on the exact point in time when a counter reset occurred. Lack of Idempotency: Similar to the previous approach, this method does not natively guarantee idempotency. One way to achieve idempotency is by consistently routing the same set of counters to the same instance. However, this approach may introduce additional complexities, such as leader election, and potential challenges with availability and latency in the write path.\n\nThat said, this approach may still be suitable in scenarios where these trade-offs are acceptable. However, let\u2019s see if we can address some of these issues with a different event-based approach.\n\nApproach 3: Using Durable Queues\n\nIn this approach, we log counter events into a durable queuing system like Apache Kafka to prevent any potential data loss. By creating multiple topic partitions and hashing the counter key to a specific partition, we ensure that the same set of counters are processed by the same set of consumers. This setup simplifies facilitating idempotency checks and resetting counts. Furthermore, by leveraging additional stream processing frameworks such as Kafka Streams or Apache Flink, we can implement windowed aggregations.\n\nHowever, this approach comes with some challenges:\n\nPotential Delays : Having the same consumer process all the counts from a given partition can lead to backups and delays, resulting in stale counts.\n\n: Having the same consumer process all the counts from a given partition can lead to backups and delays, resulting in stale counts. Rebalancing Partitions: This approach requires auto-scaling and rebalancing of topic partitions as the cardinality of counters and throughput increases.\n\nFurthermore, all approaches that pre-aggregate counts make it challenging to support two of our requirements for accurate counters:\n\nAuditing of Counts : Auditing involves extracting data to an offline system for analysis to ensure that increments were applied correctly to reach the final value. This process can also be used to track the provenance of increments. However, auditing becomes infeasible when counts are aggregated without storing the individual increments.\n\n: Auditing involves extracting data to an offline system for analysis to ensure that increments were applied correctly to reach the final value. This process can also be used to track the provenance of increments. However, auditing becomes infeasible when counts are aggregated without storing the individual increments. Potential Recounting: Similar to auditing, if adjustments to increments are necessary and recounting of events within a time window is required, pre-aggregating counts makes this infeasible.\n\nBarring those few requirements, this approach can still be effective if we determine the right way to scale our queue partitions and consumers while maintaining idempotency. However, let\u2019s explore how we can adjust this approach to meet the auditing and recounting requirements.\n\nApproach 4: Event Log of Individual Increments\n\nIn this approach, we log each individual counter increment along with its event_time and event_id. The event_id can include the source information of where the increment originated. The combination of event_time and event_id can also serve as the idempotency key for the write.\n\nHowever, in its simplest form, this approach has several drawbacks:\n\nRead Latency : Each read request requires scanning all increments for a given counter potentially degrading performance.\n\n: Each read request requires scanning all increments for a given counter potentially degrading performance. Duplicate Work : Multiple threads might duplicate the effort of aggregating the same set of counters during read operations, leading to wasted effort and subpar resource utilization.\n\n: Multiple threads might duplicate the effort of aggregating the same set of counters during read operations, leading to wasted effort and subpar resource utilization. Wide Partitions : If using a datastore like Apache Cassandra, storing many increments for the same counter could lead to a wide partition, affecting read performance.\n\n: If using a datastore like Apache Cassandra, storing many increments for the same counter could lead to a wide partition, affecting read performance. Large Data Footprint: Storing each increment individually could also result in a substantial data footprint over time. Without an efficient data retention strategy, this approach may struggle to scale effectively.\n\nThe combined impact of these issues can lead to increased infrastructure costs that may be difficult to justify. However, adopting an event-driven approach seems to be a significant step forward in addressing some of the challenges we\u2019ve encountered and meeting our requirements.\n\nHow can we improve this solution further?\n\nNetflix\u2019s Approach\n\nWe use a combination of the previous approaches, where we log each counting activity as an event, and continuously aggregate these events in the background using queues and a sliding time window. Additionally, we employ a bucketing strategy to prevent wide partitions. In the following sections, we\u2019ll explore how this approach addresses the previously mentioned drawbacks and meets all our requirements.\n\nNote: From here on, we will use the words \u201crollup\u201d and \u201caggregate\u201d interchangeably. They essentially mean the same thing, i.e., collecting individual counter increments/decrements and arriving at the final value.\n\nTimeSeries Event Store:\n\nWe chose the TimeSeries Data Abstraction as our event store, where counter mutations are ingested as event records. Some of the benefits of storing events in TimeSeries include:\n\nHigh-Performance: The TimeSeries abstraction already addresses many of our requirements, including high availability and throughput, reliable and fast performance, and more.\n\nReducing Code Complexity: We reduce a lot of code complexity in Counter Abstraction by delegating a major portion of the functionality to an existing service.\n\nTimeSeries Abstraction uses Cassandra as the underlying event store, but it can be configured to work with any persistent store. Here is what it looks like:\n\nHandling Wide Partitions: The time_bucket and event_bucket columns play a crucial role in breaking up a wide partition, preventing high-throughput counter events from overwhelming a given partition. For more information regarding this, refer to our previous blog.\n\nNo Over-Counting: The event_time, event_id and event_item_key columns form the idempotency key for the events for a given counter, enabling clients to retry safely without the risk of over-counting.\n\nEvent Ordering: TimeSeries orders all events in descending order of time allowing us to leverage this property for events like count resets.\n\nEvent Retention: The TimeSeries Abstraction includes retention policies to ensure that events are not stored indefinitely, saving disk space and reducing infrastructure costs. Once events have been aggregated and moved to a more cost-effective store for audits, there\u2019s no need to retain them in the primary storage.\n\nNow, let\u2019s see how these events are aggregated for a given counter.\n\nAggregating Count Events:\n\nAs mentioned earlier, collecting all individual increments for every read request would be cost-prohibitive in terms of read performance. Therefore, a background aggregation process is necessary to continually converge counts and ensure optimal read performance.\n\nBut how can we safely aggregate count events amidst ongoing write operations?\n\nThis is where the concept of Eventually Consistent counts becomes crucial. By intentionally lagging behind the current time by a safe margin, we ensure that aggregation always occurs within an immutable window.\n\nLets see what that looks like:\n\nLet\u2019s break this down:\n\nlastRollupTs : This represents the most recent time when the counter value was last aggregated. For a counter being operated for the first time, this timestamp defaults to a reasonable time in the past.\n\n: This represents the most recent time when the counter value was last aggregated. For a counter being operated for the first time, this timestamp defaults to a reasonable time in the past. Immutable Window and Lag: Aggregation can only occur safely within an immutable window that is no longer receiving counter events. The \u201cacceptLimit\u201d parameter of the TimeSeries Abstraction plays a crucial role here, as it rejects incoming events with timestamps beyond this limit. During aggregations, this window is pushed slightly further back to account for clock skews.\n\nThis does mean that the counter value will lag behind its most recent update by some margin (typically in the order of seconds). This approach does leave the door open for missed events due to cross-region replication issues. See \u201cFuture Work\u201d section at the end.\n\nAggregation Process: The rollup process aggregates all events in the aggregation window since the last rollup to arrive at the new value.\n\nRollup Store:\n\nWe save the results of this aggregation in a persistent store. The next aggregation will simply continue from this checkpoint.\n\nWe create one such Rollup table per dataset and use Cassandra as our persistent store. However, as you will soon see in the Control Plane section, the Counter service can be configured to work with any persistent store.\n\nLastWriteTs: Every time a given counter receives a write, we also log a last-write-timestamp as a columnar update in this table. This is done using Cassandra\u2019s USING TIMESTAMP feature to predictably apply the Last-Write-Win (LWW) semantics. This timestamp is the same as the event_time for the event. In the subsequent sections, we\u2019ll see how this timestamp is used to keep some counters in active rollup circulation until they have caught up to their latest value.\n\nRollup Cache\n\nTo optimize read performance, these values are cached in EVCache for each counter. We combine the lastRollupCount and lastRollupTs into a single cached value per counter to prevent potential mismatches between the count and its corresponding checkpoint timestamp.\n\nBut, how do we know which counters to trigger rollups for? Let\u2019s explore our Write and Read path to understand this better.\n\nAdd/Clear Count:\n\nAn add or clear count request writes durably to the TimeSeries Abstraction and updates the last-write-timestamp in the Rollup store. If the durability acknowledgement fails, clients can retry their requests with the same idempotency token without the risk of overcounting. Upon durability, we send a fire-and-forget request to trigger the rollup for the request counter.\n\nGetCount:\n\nWe return the last rolled-up count as a quick point-read operation, accepting the trade-off of potentially delivering a slightly stale count. We also trigger a rollup during the read operation to advance the last-rollup-timestamp, enhancing the performance of subsequent aggregations. This process also self-remediates a stale count if any previous rollups had failed.\n\nWith this approach, the counts continually converge to their latest value. Now, let\u2019s see how we scale this approach to millions of counters and thousands of concurrent operations using our Rollup Pipeline.\n\nRollup Pipeline:\n\nEach Counter-Rollup server operates a rollup pipeline to efficiently aggregate counts across millions of counters. This is where most of the complexity in Counter Abstraction comes in. In the following sections, we will share key details on how efficient aggregations are achieved.\n\nLight-Weight Roll-Up Event: As seen in our Write and Read paths above, every operation on a counter sends a light-weight event to the Rollup server:\n\nrollupEvent: {\n\n\"namespace\": \"my_dataset\",\n\n\"counter\": \"counter123\"\n\n}\n\nNote that this event does not include the increment. This is only an indication to the Rollup server that this counter has been accessed and now needs to be aggregated. Knowing exactly which specific counters need to be aggregated prevents scanning the entire event dataset for the purpose of aggregations.\n\nIn-Memory Rollup Queues: A given Rollup server instance runs a set of in-memory queues to receive rollup events and parallelize aggregations. In the first version of this service, we settled on using in-memory queues to reduce provisioning complexity, save on infrastructure costs, and make rebalancing the number of queues fairly straightforward. However, this comes with the trade-off of potentially missing rollup events in case of an instance crash. For more details, see the \u201cStale Counts\u201d section in \u201cFuture Work.\u201d\n\nMinimize Duplicate Effort: We use a fast non-cryptographic hash like XXHash to ensure that the same set of counters end up on the same queue. Further, we try to minimize the amount of duplicate aggregation work by having a separate rollup stack that chooses to run fewer beefier instances.\n\nAvailability and Race Conditions: Having a single Rollup server instance can minimize duplicate aggregation work but may create availability challenges for triggering rollups. If we choose to horizontally scale the Rollup servers, we allow threads to overwrite rollup values while avoiding any form of distributed locking mechanisms to maintain high availability and performance. This approach remains safe because aggregation occurs within an immutable window. Although the concept of now() may differ between threads, causing rollup values to sometimes fluctuate, the counts will eventually converge to an accurate value within each immutable aggregation window.\n\nRebalancing Queues: If we need to scale the number of queues, a simple Control Plane configuration update followed by a re-deploy is enough to rebalance the number of queues.\n\n\"eventual_counter_config\": {\n\n\"queue_config\": {\n\n\"num_queues\" : 8, // change to 16 and re-deploy\n\n...\n\nHandling Deployments: During deployments, these queues shut down gracefully, draining all existing events first, while the new Rollup server instance starts up with potentially new queue configurations. There may be a brief period when both the old and new Rollup servers are active, but as mentioned before, this race condition is managed since aggregations occur within immutable windows.\n\nMinimize Rollup Effort: Receiving multiple events for the same counter doesn\u2019t mean rolling it up multiple times. We drain these rollup events into a Set, ensuring a given counter is rolled up only once during a rollup window.\n\nEfficient Aggregation: Each rollup consumer processes a batch of counters simultaneously. Within each batch, it queries the underlying TimeSeries abstraction in parallel to aggregate events within specified time boundaries. The TimeSeries abstraction optimizes these range scans to achieve low millisecond latencies.\n\nDynamic Batching: The Rollup server dynamically adjusts the number of time partitions that need to be scanned based on cardinality of counters in order to prevent overwhelming the underlying store with many parallel read requests.\n\nAdaptive Back-Pressure: Each consumer waits for one batch to complete before issuing the rollups for the next batch. It adjusts the wait time between batches based on the performance of the previous batch. This approach provides back-pressure during rollups to prevent overwhelming the underlying TimeSeries store.\n\nHandling Convergence:\n\nIn order to prevent low-cardinality counters from lagging behind too much and subsequently scanning too many time partitions, they are kept in constant rollup circulation. For high-cardinality counters, continuously circulating them would consume excessive memory in our Rollup queues. This is where the last-write-timestamp mentioned previously plays a crucial role. The Rollup server inspects this timestamp to determine if a given counter needs to be re-queued, ensuring that we continue aggregating until it has fully caught up with the writes.\n\nNow, let\u2019s see how we leverage this counter type to provide an up-to-date current count in near-realtime.\n\nExperimental: Accurate Global Counter\n\nWe are experimenting with a slightly modified version of the Eventually Consistent counter. Again, take the term \u2018Accurate\u2019 with a grain of salt. The key difference between this type of counter and its counterpart is that the delta, representing the counts since the last-rolled-up timestamp, is computed in real-time.\n\nAnd then, currentAccurateCount = lastRollupCount + delta\n\nAggregating this delta in real-time can impact the performance of this operation, depending on the number of events and partitions that need to be scanned to retrieve this delta. The same principle of rolling up in batches applies here to prevent scanning too many partitions in parallel. Conversely, if the counters in this dataset are accessed frequently, the time gap for the delta remains narrow, making this approach of fetching current counts quite effective.\n\nNow, let\u2019s see how all this complexity is managed by having a unified Control Plane configuration.\n\nControl Plane\n\nThe Data Gateway Platform Control Plane manages control settings for all abstractions and namespaces, including the Counter Abstraction. Below, is an example of a control plane configuration for a namespace that supports eventually consistent counters with low cardinality:\n\n\"persistence_configuration\": [\n\n{\n\n\"id\": \"CACHE\", // Counter cache config\n\n\"scope\": \"dal=counter\",\n\n\"physical_storage\": {\n\n\"type\": \"EVCACHE\", // type of cache storage\n\n\"cluster\": \"evcache_dgw_counter_tier1\" // Shared EVCache cluster\n\n}\n\n},\n\n{\n\n\"id\": \"COUNTER_ROLLUP\",\n\n\"scope\": \"dal=counter\", // Counter abstraction config\n\n\"physical_storage\": {\n\n\"type\": \"CASSANDRA\", // type of Rollup store\n\n\"cluster\": \"cass_dgw_counter_uc1\", // physical cluster name\n\n\"dataset\": \"my_dataset_1\" // namespace/dataset\n\n},\n\n\"counter_cardinality\": \"LOW\", // supported counter cardinality\n\n\"config\": {\n\n\"counter_type\": \"EVENTUAL\", // Type of counter\n\n\"eventual_counter_config\": { // eventual counter type\n\n\"internal_config\": {\n\n\"queue_config\": { // adjust w.r.t cardinality\n\n\"num_queues\" : 8, // Rollup queues per instance\n\n\"coalesce_ms\": 10000, // coalesce duration for rollups\n\n\"capacity_bytes\": 16777216 // allocated memory per queue\n\n},\n\n\"rollup_batch_count\": 32 // parallelization factor\n\n}\n\n}\n\n}\n\n},\n\n{\n\n\"id\": \"EVENT_STORAGE\",\n\n\"scope\": \"dal=ts\", // TimeSeries Event store\n\n\"physical_storage\": {\n\n\"type\": \"CASSANDRA\", // persistent store type\n\n\"cluster\": \"cass_dgw_counter_uc1\", // physical cluster name\n\n\"dataset\": \"my_dataset_1\", // keyspace name\n\n},\n\n\"config\": {\n\n\"time_partition\": { // time-partitioning for events\n\n\"buckets_per_id\": 4, // event buckets within\n\n\"seconds_per_bucket\": \"600\", // smaller width for LOW card\n\n\"seconds_per_slice\": \"86400\", // width of a time slice table\n\n},\n\n\"accept_limit\": \"5s\", // boundary for immutability\n\n},\n\n\"lifecycleConfigs\": {\n\n\"lifecycleConfig\": [\n\n{\n\n\"type\": \"retention\", // Event retention\n\n\"config\": {\n\n\"close_after\": \"518400s\",\n\n\"delete_after\": \"604800s\" // 7 day count event retention\n\n}\n\n}\n\n]\n\n}\n\n}\n\n]\n\nUsing such a control plane configuration, we compose multiple abstraction layers using containers deployed on the same host, with each container fetching configuration specific to its scope.\n\nProvisioning\n\nAs with the TimeSeries abstraction, our automation uses a bunch of user inputs regarding their workload and cardinalities to arrive at the right set of infrastructure and related control plane configuration. You can learn more about this process in a talk given by one of our stunning colleagues, Joey Lynch : How Netflix optimally provisions infrastructure in the cloud.\n\nPerformance\n\nAt the time of writing this blog, this service was processing close to 75K count requests/second globally across the different API endpoints and datasets:\n\nwhile providing single-digit millisecond latencies for all its endpoints:\n\nFuture Work\n\nWhile our system is robust, we still have work to do in making it more reliable and enhancing its features. Some of that work includes:\n\nRegional Rollups: Cross-region replication issues can result in missed events from other regions. An alternate strategy involves establishing a rollup table for each region, and then tallying them in a global rollup table. A key challenge in this design would be effectively communicating the clearing of the counter across regions.\n\nCross-region replication issues can result in missed events from other regions. An alternate strategy involves establishing a rollup table for each region, and then tallying them in a global rollup table. A key challenge in this design would be effectively communicating the clearing of the counter across regions. Error Detection and Stale Counts: Excessively stale counts can occur if rollup events are lost or if a rollup fails and isn\u2019t retried. This isn\u2019t an issue for frequently accessed counters, as they remain in rollup circulation. This issue is more pronounced for counters that aren\u2019t accessed frequently. Typically, the initial read for such a counter will trigger a rollup, self-remediating the issue. However, for use cases that cannot accept potentially stale initial reads, we plan to implement improved error detection, rollup handoffs, and durable queues for resilient retries.\n\nConclusion\n\nDistributed counting remains a challenging problem in computer science. In this blog, we explored multiple approaches to implement and deploy a Counting service at scale. While there may be other methods for distributed counting, our goal has been to deliver blazing fast performance at low infrastructure costs while maintaining high availability and providing idempotency guarantees. Along the way, we make various trade-offs to meet the diverse counting requirements at Netflix. We hope you found this blog post insightful.\n\nStay tuned for Part 3 of Composite Abstractions at Netflix, where we\u2019ll introduce our Graph Abstraction, a new service being built on top of the Key-Value Abstraction and the TimeSeries Abstraction to handle high-throughput, low-latency graphs.\n\nAcknowledgments\n\nSpecial thanks to our stunning colleagues who contributed to the Counter Abstraction\u2019s success: Joey Lynch, Vinay Chella, Kaidan Fullerton, Tom DeVoe, Mengqing Wang, Varun Khaitan", "label": 0}
{"title": "Celebrating Five Years of Backstage: From Open Source Project to Enterprise Business", "url": "https://engineering.atspotify.com/2025/4/celebrating-five-years-of-backstage", "content": "Did you know that we sell Spotify\u2019s developer productivity tools and support to other companies, from online retailers and fashion brands to banks and automobile manufacturers? How did Spotify get into the business of selling enterprise developer software? And how does serving the open source community \u2014 not to mention paying corporate customers \u2014 improve our own internal software practices? Tyson Singer, Spotify\u2019s head of technology and platforms, shares how our homegrown developer portal evolved from an open source project to our newest enterprise software product: Spotify Portal for Backstage.\n\nFive years ago on March 16, 2020, we open sourced Backstage, a framework for building internal developer portals (IDPs) like the one we were using at Spotify. Despite being a very new idea \u2014 the term \u201cIDP\u201d didn\u2019t even exist back then \u2014 the project took off on day one, and accelerated from there.\n\nToday, Backstage is the IDP platform of choice \u2014 with over 3,000 companies having adopted it to build IDPs of their own. Within Spotify, it\u2019s still what our 700 R&D squads rely on to help them ship every day. And, oh yeah, we\u2019re also building an enterprise software business on top of it. So how did Spotify get here? And where will Backstage take us next?\n\nA platform for developer experience\n\nBefore open sourcing Backstage, we\u2019d already been using it internally for years to help us solve a whole range of developer experience problems: context switching, fragmentation, cognitive load, silos, duplication, congestion, dependency management, tech health, compliance \u2014 you name it, Backstage could grow to help us address it. As a centralized platform, it contained the chaos of modern software development while making our whole engineering org more productive and more efficient. It became invaluable to how we work \u2014 and we didn\u2019t want to lose it.\n\nHow would we lose it? We\u2019d recently gone through a painful migration before with another technology, where we had to move from our custom-built solution to a third-party one that had become the standard. We didn\u2019t want to go through that again with Backstage. So we open sourced it.\n\nBecoming the industry standard\n\nBackstage had become so valuable to us, we thought other companies would find it valuable, too. But we also had another motive for open sourcing it. It wasn\u2019t just about sharing: Our ambition from the very start was for Backstage to become the standard for IDPs. Because if everyone else was using it, we wouldn\u2019t have to migrate off it.\n\nBut you\u2019re not going to become the standard just by being free. The product also has to be good. Even more than that, for us, Backstage had to be great. Because we\u2019re an end user, too \u2014 probably the most demanding one. We depend on Backstage to improve the developer experience and productivity of our own teams, which has a direct impact on our business. As an open source project, we\u2019d also get the benefit of outside contributions, addressing use cases we hadn\u2019t even thought of yet. Sounds great, right?\n\nBut of course, be careful what you wish for. Now we were no longer building just for ourselves, now we were building for everyone. And sometimes it feels like those two things might be competing with each other. Could we meet the demands of the outside community while still serving the needs of our own developers?\n\nHow building for others has benefited us\n\nHere\u2019s an example of what it\u2019s been like being both maintainer and end user of the Backstage project: We recently launched a complete rewrite of the entire backend system. This has been a really big endeavor for us. But we saw that adopters really needed an easier way to build and integrate plugins. So we did the work \u2014 building with the community and rewriting the whole backend over the course of a year.\n\nAnd when the 1.0 of the new backend launched, it was great. Here are just a few of the things we\u2019ve heard from the community and our enterprise customers alike: \u201cMakes things a lot easier to maintain.\u201d \u201cYou used to have to wire things together across multiple files. Now adding a new plugin is a one-liner.\u201d The new backend \u201cwas the game changer \ud83d\ude80\ud83e\uddd1\u200d\ud83d\udcbb.\u201d\n\nEverything about the new backend is simpler to work with \u2014 for the community and for us. So, of course, now we\u2019re in the middle of completely rewriting the frontend, too. And guess what? That will be great for Spotify as end users, as well.\n\nIf we hadn\u2019t open sourced Backstage, I don\u2019t know how long we would have kept living with our previous, more complicated backend system. Because of the community \u2014 both its needs and the way it enables us to build better solutions \u2014 we were pushed to build a much better version of Backstage for ourselves. And we could also quickly bring those improvements to our enterprise customers \u2014 through a no-code UI for installing and managing plugins \u2014 in Spotify Portal, our SaaS solution for Backstage adopters.\n\nInternal \u2192 open source \u2192 commercial: The best of all worlds\n\nIt\u2019s been a lot of push and pull in how we balance internal priorities with external ones. But ultimately, this is a virtuous cycle. The ever-popular Notifications feature, the Events management plugin, and the API Docs plugin, which basically powers our entity pages for API entities \u2014 all of these are used in Spotify\u2019s internal Backstage instance, and all were contributions from the community.\n\nBeing part of a developer community beyond Spotify has put us in such a great position for continuing to invest in and grow the Backstage platform. The project has improved our own internal software practices. We\u2019ve benefited from the innovation of the community at large. And we\u2019ve been able to keep investing in the ecosystem in a way that improves our commercial Backstage products and brings more value to our enterprise customers.\n\nIn five years, we\u2019ve gone from basically just a thin framework for building an IDP, to a better IDP than we could have built by ourselves, and now to a SaaS product that we think is the best IDP for the future. This journey can be bumpy. But making that repo public back in 2020 has definitely brought us to a place we never could have imagined today. All of which proves out our original idea: Investing in the community has been an investment in ourselves, as well.\n\nSo, happy 5th birthday, Backstage! We\u2019ve come far in a short time. But we\u2019re expecting even bigger things ahead.\n\nReady to try Backstage for yourself? Apply for the Spotify Portal beta \u2014 the fastest way to get up and running with a Backstage IDP of your own. Portal combines the best of open source with the best of Spotify\u2019s developer experience and developer productivity practices.", "label": 0}
{"title": "Learn to build and run AI powered apps at Firebase Demo Day \u201824", "url": "https://developers.googleblog.com/en/firebase-demo-day-24/", "content": "Welcome to Firebase Demo Day 2024\n\nWe just released 8 bite sized demo videos to showcase how Firebase helps you build and run AI-powered apps. We\u2019ll show you how to use new Firebase products and features like Firebase Genkit, Vertex AI in Firebase, Gemini in Firebase and Firebase App Hosting, to build AI features into your existing applications, monitor their performance, and create great experiences for your users.\n\nTo bring these concepts to life, we'll take you on an app dev journey through Compass, our sample travel app. We\u2019ll demonstrate how you can use Firebase to create features like personalized recommendations, smart itineraries, AI-powered chatbots, and more. Follow along as we highlight how you can leverage Firebase tools to add the same cutting-edge functionality to your own apps.\n\n\n\nWatch Firebase Demo Day 2024 from anywhere at any time, at your own pace.\n\n\n\n\n\n\n\nDemos to build AI-powered features\n\nWatch as we transform our travel app with the power of Firebase and AI. Our build demos show you how to build and deploy AI features with new Firebase products like Vertex AI, Genkit and Firebase Hosting, all while leveraging Firebase's fully managed infrastructure to get to market quickly and securely.\n\n\n\nCall Gemini from your Android app\n\nIntegrate the power of Gemini directly into your Android app using the native Vertex AI in Firebase SDK for Android to make calls to Gemini.", "label": 0}
{"title": "HDR10+ Now Streaming on Netflix", "url": "https://netflixtechblog.com/hdr10-now-streaming-on-netflix-c9ab1f4bd72b?source=collection_home---4------0-----------------------", "content": "HDR10+ Now Streaming on Netflix Netflix Technology Blog 5 min read \u00b7 Mar 24, 2025 -- 13 Listen Share\n\nRoger Quero, Liwei Guo, Jeff Watts, Joseph McCormick, Agata Opalach, Anush Moorthy\n\nWe are excited to announce that we are now streaming HDR10+ content on our service for AV1-enabled devices, enhancing the viewing experience for certified HDR10+ devices, which previously only received HDR10 content. The dynamic metadata included in our HDR10+ content improves the quality and accuracy of the picture when viewed on these devices.\n\nDelighting Members with Even Better Picture Quality\n\nNearly a decade ago, we made a bold move to be a pioneering adopter of High Dynamic Range (HDR) technology. HDR enables images to have more details, vivid colors, and improved realism. We began producing our shows and movies in HDR, encoding them in HDR, and streaming them in HDR for our members. We were confident that it would greatly enhance our members\u2019 viewing experience, and unlock new creative visions \u2014 and we were right! In the last five years, HDR streaming has increased by more than 300%, while the number of HDR-configured devices watching Netflix has more than doubled. Since launching HDR with season one of Marco Polo, Netflix now has over 11,000 hours of HDR titles for members to immerse themselves in.\n\nWe continue to enhance member joy while maintaining creative vision by adding support for HDR10+. This will further augment Netflix\u2019s growing HDR ecosystem, preserve creative intent on even more devices, and provide a more immersive viewing experience.\n\nWe enabled HDR10+ on Netflix using the AV1 video codec that was standardized by the Alliance for Open Media (AOM) in 2018. AV1 is one of the most efficient codecs available today. We previously enabled AV1 encoding for SDR content, and saw tremendous value for our members, including higher and more consistent visual quality, lower play delay and increased streaming at the highest resolution. AV1-SDR is already the second most streamed codec at Netflix, behind H.264/AVC, which has been around for over 20 years! With the addition of HDR10+ streams to AV1, we expect the day is not far when AV1 will be the most streamed codec at Netflix.\n\nTo enhance our offering, we have been adding HDR10+ streams to both new releases and existing popular HDR titles. AV1-HDR10+ now accounts for 50% of all eligible viewing hours. We will continue expanding our HDR10+ offerings with the goal of providing an HDR10+ experience for all HDR titles by the end of this year\u00b9.\n\nIndustry Adopted Formats\n\nToday, the industry recognizes three prevalent HDR formats: Dolby Vision, HDR10, and HDR10+. For all three HDR Formats, metadata is embedded in the content, serving as instructions to guide the playback device \u2014 whether it\u2019s a TV, mobile device, or computer \u2014 on how to display the image.\n\nHDR10 is the most widely adopted HDR format, supported by all HDR devices. HDR10 uses static metadata that is defined once for the entire content detailing aspects such as the maximum content light level (MaxCLL), maximum frame average light level (MaxFALL), as well as characteristics of the mastering display used for color grading. This metadata only allows for a one-size-fits-all tone mapping of the content for display devices. It cannot account for dynamic contrast across scenes, which most content contains.\n\nHDR10+ and Dolby Vision improve on this with dynamic metadata that provides content image statistics on a per-frame basis, enabling optimized tone mapping adjustments for each scene. This achieves greater perceptual fidelity to the original, preserving creative intent.\n\nHDR10 vs. HDR10+\n\nThe figure below shows screen grabs of two AV1-encoded frames of the same content displayed using HDR10 (top) and HDR10+ (bottom).\n\nPhotographs of devices displaying the same frame with HDR10 metadata (top) and HDR10+ metadata (bottom). Notice the preservation of the flashlight detail in the HDR10+ capture, and the over-exposure of the region under the flashlight in the HDR10 one\u00b2.\n\nAs seen in the flashlight on the table, the highlight details are clipped in the HDR10 content, but are recovered in HDR10+. Further, the region under the flashlight is overexposed in the HDR10 content, while HDR10+ renders that region with greater fidelity to the source. The reason HDR10+, with its dynamic metadata, shines in this example is that the scenes preceding and following the scene with this frame have markedly different luminance statistics. The static HDR10 metadata is unable to account for the change in the content. While this is a simple example, the dynamic metadata in HDR10+ demonstrates such value across any set of scenes. This consistency allows our members to stay immersed in the content, and better preserves creative intent.\n\nReceiving HDR10+\n\nAt the time of launch, these requirements must be satisfied to receive HDR10+:\n\n1.Member must have a Netflix Premium plan subscription\n\n2. Title must be available in HDR10+ format\n\n3. Member device must support AV1 & HDR10+. Here are some examples of compatible devices:\n\nSmartTVs, mobile phones, and tablets that meet Netflix certification for HDR10+\n\nSource device (such as set-top boxes, streaming devices, MVPDs, etc.) that meets Netflix certification for HDR10+, connected to an HDR10+ compliant display via HDMI\n\n4. For TV or streaming devices, ensure that the HDR toggle is enabled in our Netflix application settings: https://help.netflix.com/en/node/100220\n\nAdditional guidance: https://help.netflix.com/en/node/13444\n\nSummary\n\nMore HDR content is watched every day on Netflix. Expanding the Netflix HDR ecosystem to include HDR10+ increases the accessibility of HDR content with dynamic metadata to more members, improves the viewing experience, and preserves the creative intent of our content creators. The commitment to innovation and quality underscores our dedication to delivering an immersive and authentic viewing experience for all our members.\n\nAcknowledgements\n\nLaunching HDR10+ was a collaborative effort involving multiple teams at Netflix, and we are grateful to everyone who contributed to making this idea a reality. We would like to extend our thanks to the following teams for their crucial roles in this launch:\n\nFootnotes", "label": 0}
{"title": "Copyright anti-circumvention: an AI hypothetical", "url": "https://tommorris.org/posts/2024/copyright-anti-circumvention-ai-hypothetical/", "content": "Imagine there is an American company called FancySiri Inc. who make a commercially available large language model similar to those offered by Google, OpenAI, Anthropic etc. It provides a chatbot service similar to ChatGPT, Gemini, et al. where users can ask questions and get a load of synthetic text of dubious provenance and accuracy that purports to answer the question they asked.\n\nNow imagine there is a person, let\u2019s call him Keith Jones. Keith is a British citizen and runs a website hosted in the UK. Every page on his website is marked with \u201cCopyright \u00a9 2024 Keith Jones. All rights reserved.\u201d\n\nIn addition, Keith places Schema.org metadata in the page, in a manner broadly as follows:\n\n<body vocab=\"https://schema.org/\" typeof=\"WebPage\"> ... <footer> <span property=\"copyrightNotice\"> Copyright \u00a9 <span property=\"copyrightYear\">2024</span> <span property=\"copyrightHolder\" typeof=\"Person\"> <span property=\"name\">Keith Jones</span> </span>. <a href=\"https://en.wikipedia.org/wiki/All_rights_reserved\" property=\"license\">All rights reserved.</a> </span> </footer> </body>\n\n(Other metadata standards are available: microformats, DC terms etc. Pick whichever one you hate least.)\n\nKeith is not particularly impressed with the trend towards artificial intelligence, so he obtains a list of the User Agents of commercially available large language models that happens to include well-known existing services already mentioned and our hypothetical FancySiri Inc. (ai.robots.txt or similar lists on the web)\n\nKeith puts up a robots.txt file and implements filtering based on User Agent string matching to always return an HTTP 401 Unauthorized response to known AI/LLM scrapers including FancySiri. This denies them access to his beautifully curated collection of tasteful cat pictures and discussions of the nerdiest of minutiae concerning programming in Rust.\n\nFancySiri Inc. have published on their website\u2019s legal FAQ that they comply with robots.txt and only scrape material using the User-Agent specified, \u201cFancySiriBot\u201d.\n\nUnfortunately, they lied.\n\nEither they did not respect the contents of Keith\u2019s robots.txt, or they ran a second scraper that uses the User Agent string of a human-operated web browser. Like, say \u201cMozilla/5.0 (iPhone14,3; U; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/602.1.50 (KHTML, like Gecko) Version/10.0 Mobile/19A346 Safari/602.1\u201d (for those not familiar with reading User Agent strings, this UA is for an iPhone 13 Pro Max running iOS 15).\n\nA tech company lying? Shocking, I know.\n\nKeith learns that a blog post he published about some particularly abstruse Rust minutiae has become part of the training data used for FancySiri\u2019s large language model. For instance, when the FancySiri chatbot is asked about the rather niche topic, the server returns words that are so remarkably close to Keith\u2019s blog post that it\u2019s beyond any doubt where it came from.\n\nKeith is rather puzzled about this and takes a break from photographing his cat and writing Rust to peruse the UK\u2019s Copyright Designs and Patents Act 1988. After some consideration, he has some interesting questions. Purely hypothetically, one must note. He does not have the money to take FancySiri Inc. to court, and has a wise policy of avoiding civil litigation on the very reasonable basis that he enjoys not being sad all the time.\n\nDoes the publication of a robots.txt file and the use of a User-Agent matching traffic block amount to an effective technological protection measure as defined by s296ZF of the CDPA? Assuming the answer to (1) is yes, has FancySiri Inc. circumvented those measures knowing, or with reasonable grounds, to know that they are trying to circumvent it? That is to say, have they breached s296ZA of the CDPA? Is the machine readable metadata on Keith\u2019s webpage \u201crights management information\u201d under the definition given in s296ZG(7)(b)? If the answer to (4) is yes, has FancySiri Inc. \u201cremoved or altered\u201d the rights management information from Keith\u2019s website? If the answer to (5) is yes, are there reasons on which a court could reasonably conclude that FancySiri Inc. did so \u201cknowingly and without authority\u201d while knowing or having reason to believe that by doing so they are inducing, enabling, facilitating, or concealing an infringement of copyright (per s296ZG)?\n\nIf you\u2019re a nerd, feel free to re-run this exercise with the particular anti-circumvention rules as implemented in your preferred jurisdiction. They\u2019re all fairly similar because large chunks of copyright law is the product of international agreements (in this case, the 1996 WIPO Copyright Treaty).\n\nThe rough intuition behind this hypothetical is as follows: anti-circumvention rules are really strong. The domain of things which they protect has expanded way beyond the original purpose (computer software and other creative works including music, books, movies, visual arts etc.) to afford copyright protection to the DRM on ink cartridges, tractors and ventilators (the latter being a particularly unpleasant illustration of the prioritisation of corporate greed over human life during the COVID pandemic). Such a broad legal rule could also potentially allow a suitably placed claimant to challenge\u2014at least, in theory\u2014the use of online materials by AI companies.\n\nThe UK government are currently consulting on Copyright and Artificial Intelligence, though they seem to have not mentioned the application of anti-circumvention rules (either in the consultation document or the summary assessment of options document). Some of the questions the government are consulting on\u2014specifically about machine-readable methods for \u201creserving rights\u201d\u2014would seem to envision a regime where one has to take specific technical steps to protect works from infringement. Hence they kind of run in parallel to the anti-circumvention rules already in the CDPA and similar instruments. It might be sensible to ensure they don\u2019t conflict, or cause confusion.\n\nIf the government wish to amend the copyright laws to broaden the current limited data mining exception to cover AI training, that policy goal could be undermined by the use of anti-circumvention rules against the same AI companies they seek to, uh, liberate. That said, under the principle of \u201cwhat\u2019s good for the goose is good for the gander\u201d, perhaps Keith should be afforded the same maximalist level of protection of the intellectual property in his cat pictures and blog posts about Rust programming as companies like HP and Lexmark have greatly benefitted from when it comes to stopping people using \u201cpirate ink\u201d in their inkjet printers.\n\nAppendix: relevant CDPA passages\n\nThe relevant passages of the legislation are included below for interested readers. For those who enjoy reading legislation, the whole text is on legislation.gov.uk for your perusal. I have tried to chop out as much that isn\u2019t relevant as humanly possible.\n\nOne thing to note is that s296 deals with anti-circumvention of computer software while s296ZA-ZF deal with anti-circumvention as applied to everything else. Since the problem concerns a blog post (which might contain incidental bits of computer code in trivial quantities), I\u2019ve used the non-code sections. If the hypothetical concerned, say, a self-hosted Gitea instance containing a bunch of code, the s296 code version would apply instead.\n\nSection 296ZA: Circumvention of technological measures\n\n(1) This section applies where\u2014 (a) effective technological measures have been applied to a copyright work other than a computer program; and (b) a person (B) does anything which circumvents those measures knowing, or with reasonable grounds to know, that he is pursuing that objective.\n\n(Subsection (2) omitted - it deals with cryptographic research, which isn\u2019t going on here.)\n\n(3) The following persons have the same rights against B as a copyright owner has in respect of an infringement of copyright\u2014 (a) a person\u2014 (i) issuing to the public copies of, or (ii) communicating to the public, the work to which effective technological measures have been applied; and (b) the copyright owner or his exclusive licensee, if he is not the person specified in paragraph (a).\n\nThe remaining subsections are omitted: (4) deals with concurrent rights, (5) deals with procedural and evidential matters, while (6) and (7) concern rights in performance, publication rights, an database rights - all of which are not dealt with in the hypothetical.\n\nSection 296ZF: Interpretation of sections 296ZA to 295ZEA\n\n(1) In sections 296ZA to 296ZE, \u201ctechnological measures\u201d are any technology, device or component which is designed, in the normal course of its operation, to protect a copyright work other than a computer program. (2) Such measures are \u201ceffective\u201d if the use of the work is controlled by the copyright owner through\u2014 (a) an access control or protection process such as encryption, scrambling or other transformation of the work, or (b) a copy control mechanism, which achieves the intended protection. (3) In this section, the reference to\u2014 (a) protection of a work is to the prevention or restriction of acts that are not authorised by the copyright owner of that work and are restricted by copyright; and (b) use of a work does not extend to any use of the work that is outside the scope of the acts restricted by copyright. (4) Expressions used in sections 296ZA to 296ZEA which are defined for the purposes of Part 1 of this Act (copyright) have the same meaning as in that Part.\n\nSection 296ZG: Electronic rights management information\n\n(1) This section applies where a person (D), knowingly and without authority, removes or alters electronic rights management information which\u2014 (a) is associated with a copy of a copyright work, or (b) appears in connection with the communication to the public of a copyright work, and where D knows, or has reason to believe, that by so doing he is inducing, enabling, facilitating or concealing an infringement of copyright.\n\nSubsection (2) concerns importing and is not relevant.\n\n(3) A person issuing to the public copies of, or communicating, the work to the public, has the same rights against D and E as a copyright owner has in respect of an infringement of copyright. (4) The copyright owner or his exclusive licensee, if he is not the person issuing to the public copies of, or communicating, the work to the public, also has the same rights against D and E as he has in respect of an infringement of copyright. (5) The rights conferred by subsections (3) and (4) are concurrent, and sections 101(3) and 102(1) to (4) apply, in proceedings under this section, in relation to persons with concurrent rights as they apply, in proceedings mentioned in those provisions, in relation to a copyright owner and exclusive licensee with concurrent rights.\n\nSubsection (6) is concerned with procedural and evidential matters only relevant if Keith decides to pursue litigation.\n\n(7) In this section\u2014 (a) expressions which are defined for the purposes of Part 1 of this Act (copyright) have the same meaning as in that Part; and (b) \u201c rights management information \u201d means any information provided by the copyright owner or the holder of any right under copyright which identifies the work, the author, the copyright owner or the holder of any intellectual property rights, or information about the terms and conditions of use of the work, and any numbers or codes that represent such information.\n\nSubsections (8) and (9) aren\u2019t relevant.", "label": 1}
{"title": "Update - 1000 Albums in 1000 Days", "url": "https://lifeofpablo.com/blog/update-one-thousand-albums-in-one-thousand-days", "content": "Background from the previous post\n\nI wrote a post, 1000 Albums in 1000 Days, that a started a challenge or an adventure to listen to 1000 albums in 1000 days. So far it's going well. I've discovered new music or music I've been slightly familiar with. It's been fun to travel back the decades and see what everyone has been listening. My music taste has been less stale than when I had started.\n\nPosting on my blog\n\nI haven't posted as much on what I've been listening to. I decided to create weekly post of the titles and artist of respective I've listened to. If there is an album I really enjoyed, I will make a dedicated post.\n\nI built a microsite\n\nI created a microsite on a subdomain on lifeofpablo.com. This microsite will display all the albums I've listened to, so far. I will update as I go or I will update when I do my weekly post.\n\nThe website is https://1k.lifeofpablo.com.\n\nThe site is relatively simple. It has the following:\n\nNavigation Bar\n\nHero Image\n\nDescription\n\nAlbum Grid (Responsive of course :D )\n\nFooter\n\nI will organize it by the decade to keep things manageable. This is how it is organized in the book.\n\nIt's just plain simple.\n\nDocker and Github Actions\n\nI will update it using github actions along with Dockr to recreate the site when I make changes.\n\nRecomendations & Webmentions\n\nAs always, if any of you have any recommendations for music to listen to? Send them my way, I'd love to see what you all recommend!\n\nWays to contact me:", "label": 1}
{"title": "Beautiful Sunday", "url": "https://lifeofpablo.com/blog/beautiful-sunday", "content": "Beautiful Sunday\n\nThis post was written in English (en_US).\n\nToday is a holiday celebrated by many. I don't celebrate this holiday. It's simply another day but a more cheerful glee.\n\nThe weather is the 60s and it's sunny out. It's just a beautiful day. The day just wants to pull you outdoors and it wants you to enjoy the simplicity of what is on the outside.I just got back from a run around the park that is nearby me. It was just very nice to see a lot happening at the park. It is very uplifting to see people making use o public spaces. Many groups and families are there simply enjoying their sunday afternoon. There is a group of people dancing to traditional\n\nHere are the things I saw or heard that made me happy:\n\nListening to traditional Mexican music.\n\nMexican folk dances in respect to La Virgin of Guadalupe (The Virgin of Guadalupe). It's nice to see these dances don't get lost as time goes on.\n\nI saw a group of people putting up banners to protect LGBT rights. These must be protected at all costs!\n\nI saw families barbecuing and enjoying time together. This reminds me when I lived with my parents.\n\nEnjoy the upbeat Sunday!", "label": 1}
{"title": "My Hobby: Photography", "url": "https://lifeofpablo.com/blog/my-hobby-photography", "content": "My Hobby: Photography June, 21th2023\n\nI remember when I was 10 years old and I got a Sony Cyber-shot point and shoot camera. I have always had an interest in taking pictures. I am one of those millennials who had a flip phone. For those who didn't have to live through this, flip phones did not have good cameras what's so ever. Phones were not all-in-one like today. So I had to bring my camera with me everywhere in my pocket or on my wrist all the time to get a good shot.\n\nPhotography has been a hobby of mine for a while. I've done it on and off throughout the years. It is a good outlet for me. It started with a point and shoot and eventually, I got nicer DSLR camera. I recently in the last two or three. years started with analog photography. Both, film photography and analog format videos. It's been fun crossing over different mediums. Photography makes me happy. I do my best to capture the moment and keep that moment alive. It is a capture of a moment of time. It's mind-blowing we can literally capture a moment and stare at it forever.\n\nIt is so much more meaningful to take it on a real camera, even if it is a simple point in shoot. It's a very intimate moment between what your capturing, the camera and most importantly - the person taking the shot.\n\nI know I'm no where being any good. I just enjoy doing this hobby. I really got to meet some cool people in the photography world. I feel proud as a teacher to have inspired one of my students to really get himself to do more photography. He has amazing shots by one-thousands times.\n\nThere are so many moments I captures that I will forever cherish. Many moments I won't show the world but maybe some day I will. Who knows? Some things are better left not reaching light.\n\nBelow are just some of my photos. If you want to see what I have posted, go to my photography section of my site!\n\nMe in 2010", "label": 1}
{"title": "Implementing end-to-end encryption for Dropbox teams", "url": "https://dropbox.tech/security/end-to-end-encryption-for-dropbox-teams", "content": "People trust Dropbox to keep their most important content secure. As more teams embrace remote and distributed work, ensuring the privacy and security of their data has never been more important. While customers already appreciate our simple, seamless access controls, those who work with more sensitive information have told us they want even more control over how their data is secured. One of the ways that Dropbox is meeting the needs of these customers is with the introduction of zero-knowledge, end-to-end encryption for team folders. While Dropbox already encrypts files at rest using 256-bit AES, customers are seeking end-to-end encryption where only they possess the decryption key, so not even Dropbox can access the contents of their files. For customers with especially sensitive or confidential data\u2014for example, those working in finance or healthcare\u2014end-to-end encryption offers an additional level of security. When enabled, files are encrypted directly on the customer\u2019s device before being uploaded to our servers. Here we\u2019ll discuss our implementation of end-to-end encryption for teams, the threat model of our design and encryption algorithms, and our commitment to minimizing the risk of data loss with a team-centric key management approach.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images\u2014plus generative AI capabilities across even more connected apps. See what's new \u2192\n\nBalancing security and usability\n\nOur commitment to simplicity and reliability is at the heart of our encryption design. In our view, a secure system must also be user-friendly. A security system serves no purpose if it\u2019s too complicated to use. For a feature like end-to-end encryption, with its added layer of complexity, striking the right balance between security and usability is key. Our aim was to make this technology accessible without compromising security. At its core, our implementation of end-to-end encryption is designed so that neither Dropbox, unauthorized users, or malicious third parties can access a team\u2019s encrypted files. Only the team holds the keys. Even if an attacker gains access to those keys, our implementation still ensures the confidentiality of new files or modifications, as long as the team\u2019s keys have been rotated. Encryption also assures that a file has not been tampered with. In other words, if a file decrypts successfully, it is cryptographically guaranteed to be the exact same content as encrypted in the first place.\n\nAt the same time, because zero-knowledge encryption means customers manage their own keys, they also risk losing access to their data if those keys are lost. To address this, we\u2019ve developed a key management system designed specifically for teams. It ensures that even if one member loses their keys, the data remains accessible and secure for the rest of the team.\n\nTeam-centric key management\n\nKey management in many end-to-end encryption systems has traditionally focused on individual users, mainly because they were the first to adopt and use these systems. In those systems, each user is responsible for managing their own set of keys and making sure they're always accessible. However, this can create complications that diminish the user experience and may even lead to data loss if keys are misplaced. To counteract potential data loss, some systems use a method called key escrow, which allows for data recovery by a trusted third party, e.g. a spouse or an administrator. But this adds complexity, both in terms of the coding required and in using the product itself. By focusing on our teams customers and drawing the cryptographic boundary around teams, we were able to re-think how the key management is done. With our approach, users don\u2019t have any keys, but every team has a central team key. This key is accessible to all team members and controls access to the team\u2019s encrypted data, providing protection against unauthorized third parties. The team-centric approach offers the following benefits: Reduced risk of data loss and implicit key escrow. By sharing the team key among all members, any member with access\u2014such as a team admin with a recovery key or a member with a registered device\u2014can restore access for everyone.\n\nBy sharing the team key among all members, any member with access\u2014such as a team admin with a recovery key or a member with a registered device\u2014can restore access for everyone. Reduced user responsibility. The burden of managing cryptographic keys shifts from individuals to the team, reducing the risk of a single person causing data loss.\n\nThe burden of managing cryptographic keys shifts from individuals to the team, reducing the risk of a single person causing data loss. Reduced complexity and improved user experience. The absence of user keys as well as an explicit key escrow significantly simplifies the implementation and improves the user experience. Team members can simply use end-to-end encryption without having to worry about keys at all. To preserve data confidentiality when team members change, admins can rotate keys for the entire team. Rotating keys upon the departure of a member ensures that any potentially leaked keys become obsolete for accessing new or modified encrypted data. This mechanism is critical in a scenario where a former member, now considered an untrusted outsider, attempts to misuse a previously acquired key. By instituting a new team key for encrypting subsequent data, the system effectively safeguards the confidentiality of new files or modifications made after key rotation, thereby aligning with the threat model's emphasis on protecting data integrity against insider threats turned external.\n\nAutomatic and manual device registration\n\nBefore a user can use end-to-end encryption on a new device, the required keys must first be made available. We offer admins a choice of two device registration modes: automatic device registration and manual device registration. Automatic device registration balances security with usability by distributing keys from our system to authorized team members through the Dropbox authentication and access control infrastructure\u2014for example, when logging into a new device. Existing devices automatically authorize new devices by wrapping the team key with the new device's public key. If there are no devices available to do this, a team admin can use a recovery key to facilitate the new device's registration. The device then obtains and uses its version of the team key, ensuring quick and smooth setup without manual input. If a customer prefers more fine-grained control, they can opt for manual device registration. This process lets team admins personally approve new devices before they can access encrypted files. Team admins and members can check key authenticity by comparing the fingerprints, or security codes, of the device and team keys out-of-band. Only keys verified to belong to the correct devices and team will be used, ensuring that only legitimate team devices can access encrypted files. This process adds an additional safeguard against unauthorized access and man-in-the-middle attacks as admins can ensure that a key really belongs to a person or team, and not a malicious actor. Despite its security benefits, key verification can be cumbersome and impact usability, often leading to its limited real-world use\u2014so we've made it an optional feature for those who need greater security.\n\nWhat end-to-end encryption doesn\u2019t cover\n\nIt\u2019s important to point out there are also some threats that fall beyond the scope of our implementation: Device security. Though end-to-end encryption keeps data safe during transmission and while stored on our servers, it doesn't address security at the device level. Since encrypted files decrypt automatically for access during sync or download, we still recommend customers adopt best practices such as full-disk encryption and secure access methods to protect their devices.\n\nThough end-to-end encryption keeps data safe during transmission and while stored on our servers, it doesn't address security at the device level. Since encrypted files decrypt automatically for access during sync or download, we still recommend customers adopt best practices such as full-disk encryption and secure access methods to protect their devices. Metadata visibility. Our encryption efforts concentrate on file contents rather than metadata. With this approach, customers can still search their Dropbox account based on metadata such as file name, file type, and creation date, ensuring end-to-end encryption is still practical in everyday use.\n\nOur encryption efforts concentrate on file contents rather than metadata. With this approach, customers can still search their Dropbox account based on metadata such as file name, file type, and creation date, ensuring end-to-end encryption is still practical in everyday use. Insider threats. Our implementation safeguards against external threats to a team but doesn't change internal permissions. Teams should continue using existing access controls to manage data access amongst members, ensuring sensitive information remains compartmentalized and secure.\n\nA closer look at our encryption techniques\n\nOur implementation uses a hybrid scheme, combining a symmetric algorithm for encrypting file content with an asymmetric algorithm for securing the keys. We aim for a balance of proven security, performance, and broad platform support in our choice of encryption algorithms. Symmetric file encryption\n\nPlaintext content is split into 4 MB blocks, where each block is authenticated using AES-256 encrypted in Galois/Counter Mode (GCM) with a random and unique 96-bit nonce. While AES-GCM guarantees authenticity and integrity for each block, the 128-bit authentication tags of all blocks are cryptographically hashed using HMAC-SHA-256 to expand these guarantees to the entirety of the file. This method supports partial encryption and decryption, offering seamless security without compromising the file's integrity or order. This method is especially effective for large files, as it aligns with our practice of chunking file content into 4 MB blocks for storage. It also avoids the limitations of in-memory processing required by some APIs, like WebCrypto. Asymmetric key wrapping\n\nTo encrypt secret keys, our approach to key management uses Hybrid Public Key Encryption (HPKE), a modern and flexible standard that combines asymmetric and symmetric encryption in a hybrid crypto system. We use HPKE in single shot, base mode using Elliptic-Curve Cryptography (ECC) with the P-256 curve, SHA-256, and AES-256-GCM (DHKEM(P-256, HKDF-SHA256), HKDF-SHA256, AES-256-GCM). When manual device registration is chosen, HPKE is used in auth mode to encrypt parts of the key chain with sender authentication required for effective key verification. NIST P-256 has been chosen over other curves like Curve25519 because it is widely adopted in the industry, is available in most cryptographic libraries (e.g. WebCrypto, CryptoKit, OpenSSL), and is specified in FIPS 186-4. Post-quantum cryptography\n\nThe algorithms mentioned above do not include any post-quantum cryptography (PQC). While there exist some products with early implementations of PQC, we're taking a more cautious approach, relying on proven and time-tested encryption algorithms for several reasons: PQC's reliability for long-term storage is still uncertain due to ongoing standardization efforts. For instance, the Kyber algorithm has seen several revisions throughout its NIST standardization process.\n\nPQC is relatively new in cryptographic terms and lacks the extensive scrutiny that more established algorithms have undergone. To counteract this, some PQC applications use a hybrid model, where traditional cryptography is also used. This ensures baseline security should the PQC component be compromised at the expense of greater complexity.\n\nPQC algorithms are not yet sufficiently included in common cryptographic libraries, requiring custom implementations across some codebases and increasing the risk of vulnerabilities, bugs, and other human error.\n\nThe threat posed by quantum computing\u2014while significant\u2014is still theoretical, with its practical impacts still unknown. Given these considerations, we\u2019ve maintained flexibility around our ability to change our encryption protocols, while staying focused on trusted, well-known cryptographic implementations. This will enable us to integrate new encryption algorithms to our protocol at any time in the future. We are closely monitoring the development of Kyber and other PQC algorithms and will adapt our choice of encryption algorithms as they mature and standards evolve further.\n\nSecuring the future", "label": 0}
{"title": "Building Private Processing for AI tools on WhatsApp", "url": "https://engineering.fb.com/2025/04/29/security/whatsapp-private-processing-ai-tools/", "content": "We are inspired by the possibilities of AI to help people be more creative, productive, and stay closely connected on WhatsApp, so we set out to build a new technology that allows our users around the world to use AI in a privacy-preserving way.\n\nWe\u2019re sharing an early look into Private Processing, an optional capability that enables users to initiate a request to a confidential and secure environment and use AI for processing messages where no one \u2014 including Meta and WhatsApp \u2014 can access them.\n\nTo validate our implementation of these and other security principles, independent security researchers will be able to continuously verify our privacy and security architecture and its integrity.\n\nAI has revolutionized the way people interact with technology and information, making it possible for people to automate complex tasks and gain valuable insights from vast amounts of data. However, the current state of AI processing \u2014 which relies on large language models often running on servers, rather than mobile hardware \u2014 requires that users\u2019 requests are visible to the provider. Although that works for many use cases, it presents challenges in enabling people to use AI to process private messages while preserving the level of privacy afforded by end-to-end encryption.\n\nWe set out to enable AI capabilities with the privacy that people have come to expect from WhatsApp, so that AI can deliver helpful capabilities, such as summarizing messages, without Meta or WhatsApp having access to them, and in the way that meets the following principles:\n\nOptionality: Using Meta AI through WhatsApp, including features that use Private Processing, must be optional.\n\nTransparency: We must provide transparency when our features use Private Processing.\n\nUser control: For people\u2019s most sensitive chats that require extra assurance, they must be able to prevent messages from being used for AI features like mentioning Meta AI in chats, with the help of WhatApp\u2019s Advanced Chat Privacy feature.\n\nIntroducing Private Processing\n\nWe\u2019re excited to share an initial overview of Private Processing, a new technology we\u2019ve built to support people\u2019s needs and aspirations to leverage AI in a secure and privacy-preserving way. This confidential computing infrastructure, built on top of a Trusted Execution Environment (TEE), will make it possible for people to direct AI to process their requests \u2014 like summarizing unread WhatsApp threads or getting writing suggestions \u2014 in our secure and private cloud environment. In other words, Private Processing will allow users to leverage powerful AI features, while preserving WhatsApp\u2019s core privacy promise, ensuring no one except you and the people you\u2019re talking to can access or share your personal messages, not even Meta or WhatsApp.\n\nTo uphold this level of privacy and security, we designed Private Processing with the following foundational requirements:\n\nConfidential processing: Private Processing must be built in such a way that prevents any other system from accessing user\u2019s data \u2014 including Meta, WhatsApp or any third party \u2014 while in processing or in transit to Private Processing.\n\nEnforceable guarantees: Attempts to modify that confidential processing guarantee must cause the system to fail closed or become publicly discoverable via verifiable transparency.\n\nVerifiable transparency: Users and security researchers must be able to audit the behavior of Private Processing to independently verify our privacy and security guarantees.\n\nHowever, we know that technology platforms like ours operate in a highly adversarial environment where threat actors continuously adapt, and software and hardware systems keep evolving, generating unknown risks. As part of our defense-in-depth approach and best practices for any security-critical system, we\u2019re treating the following additional layers of requirements as core to Private Processing on WhatsApp:\n\nNon-targetability: An attacker should not be able to target a particular user for compromise without attempting to compromise the entire Private Processing system.\n\nStateless processing and forward security: Private Processing must not retain access to user messages once the session is complete to ensure that the attacker can not gain access to historical requests or responses.\n\nThreat modeling for Private Processing\n\nBecause we set out to meet these high-security requirements, our work to build Private Processing began with developing a threat model to help us identify potential attack vectors and vulnerabilities that could compromise the confidentiality, integrity, or availability of user data. We\u2019ve worked with our peers in the security community to audit the architecture and our implementation to help us continue to harden them.\n\nBuilding in the open\n\nTo help inform our industry\u2019s progress in building private AI processing, and to enable independent security research in this area, we will be publishing components of Private Processing, expanding the scope of our Bug Bounty program to include Private Processing, and releasing a detailed security engineering design paper, as we get closer to the launch of Private Processing in the coming weeks.\n\nWhile AI-enabled processing of personal messages for summarization and writing suggestions at users\u2019 direction is the first use case where Meta applies Private Processing, we expect there will be others where the same or similar infrastructure might be beneficial in processing user requests. We will continue to share our learnings and progress transparently and responsibly.\n\nHow Private Processing works\n\nPrivate Processing creates a secure cloud environment where AI models can analyze and process data without exposing it to unauthorized parties.\n\nHere\u2019s how it works:\n\nAuthentication: First, Private Processing obtains anonymous credentials to verify that the future requests are coming from authentic WhatsApp clients.\n\nThird-party routing and load balancing: In addition to these credentials, Private Processing fetches HPKE encryption public keys from a third-party CDN in order to support Oblivious HTTP (OHTTP).\n\nWire session establishment: Private Processing establishes an OHTTP connection from the user\u2019s device to a Meta gateway via a third-party relay which hides requester IP from Meta and WhatsApp.\n\nApplication session establishment: Private Processing establishes a Remote Attestation + Transport Layer Security (RA-TLS) session between the user\u2019s device and the TEE. The attestation verification step cross-checks the measurements against a third-party ledger to ensure that the client only connects to code which satisfies our verifiable transparency guarantee.\n\nRequest to Private Processing: After the above session is established, the device makes a request to Private Processing (e.g., message summarization request), that is encrypted end-to-end between the device and Private Processing with an ephemeral key that Meta and WhatsApp cannot access. In other words, no one except the user\u2019s device or the selected TEEs can decrypt the request.\n\nPrivate Processing: Our AI models process data in a confidential virtual machine (CVM), a type of TEE, without storing any messages, in order to generate a response. CVMs may communicate with other CVMs using the same RA-TLS connection clients use to complete processing.\n\nResponse from Private Processing: The processed results are then returned to the user\u2019s device, encrypted with a key that only the device and the pre-selected Private Processing server ever have access to. Private Processing does not retain access to messages after the session is completed.\n\nThe threat model\n\nIn designing any security-critical system, it is important to develop a threat model to guide how we build its defenses. Our threat model for Private Processing includes three key components:\n\nAssets : The sensitive data and systems that we need to protect.\n\nThreat actors : The individuals or groups that may attempt to compromise our assets.\n\nThreat scenarios : The ways in which our assets could be compromised, including the tactics, techniques, and procedures (TTPs) that threat actors might use.\n\nAssets\n\nIn the context of applying Private Processing to summarizing unread messages or providing writing suggestions at users\u2019 direction, we will use Private Processing to protect messaging content, whether they have been received by the user, or still in draft form. We use the term \u201cmessages\u201d to refer to these primary assets in the context of this blog.\n\nIn addition to messages, we also include additional, secondary assets which help support the goal of Private Processing and may interact with or directly process assets: the Trusted Computing Base (TCB) of the Confidential Virtual Machine (CVM), the underlying hardware, and the cryptographic keys used to protect data in transit.\n\nThreat actors\n\nWe have identified three threat actor types that could attack our system to attempt to recover assets.\n\nMalicious or compromised insiders with access to our infrastructure. A third party or supply chain vendor with access to components of the infrastructure. Malicious end users targeting other users on the platform.\n\nThreat scenarios\n\nWhen building Private Processing to be resilient against these threat actors, we consider relevant threat scenarios that may be pursued against our systems, including (but not limited to) the following:\n\nExternal actors directly exploit the exposed product attack surface or compromise the services running in Private Processing CVMs to extract messages.\n\nAnywhere the system processes untrusted data, there is potentially an attack surface for a threat actor to exploit. Examples of these kinds of attacks include exploitation of zero-day vulnerabilities or attacks unique to AI such as prompt injection.\n\nPrivate Processing is designed to reduce such an attack surface through limiting the exposed entry points to a small set of thoroughly reviewed components which are subject to regular assurance testing. The service binaries are hardened and run in a containerized environment to mitigate the risks of code execution and limit a compromised binary\u2019s ability to exfiltrate data from within the CVM to an external party.\n\nInternal or external attackers extract messages exposed through the CVM.\n\nObservability and debuggability remains a challenge in highly secure environments as they can be at odds with the goal of confidential computing, potentially exposing side channels to identify data and in the worst case accidentally leaking messages themselves. However, deploying any service at scale requires some level of observability to identify failure modes, since they may negatively impact many users, even when the frequency is uncommon. We implement a log-filtering system to limit export to only allowed log lines, such as error logs.\n\nLike any complex system, Private Processing is built of components to form a complex supply chain of both hardware and software. Internally, our CVM build process occurs in restricted environments that maintain provenance and require multi-party review. Transparency of the CVM environment, which we\u2019ll provide through publishing a third-party log of CVM binary digests and CVM binary images, will allow external researchers to analyze, replicate, and report instances where they believe logs could leak user data.\n\nInsiders with physical or remote access to Private Processing hosts interfere with the CVM at boot and runtime, potentially bypassing the protections in order to extract messages.\n\nTEE software exploitation is a growing area of security research, and vulnerability researchers have repeatedly demonstrated the ability to bypass TEE guarantees. Similarly, physical attacks on Private Processing hosts may be used to defeat TEE guarantees or present compromised hosts as legitimate to an end user.\n\nTo address these unknown risks, we built Private Processing on the principle of defense-in-depth by actively tracking novel vulnerabilities in this space, minimizing and sanitizing untrusted inputs to the TEE, minimizing attack surface through CVM hardening and enabling abuse detection through enhanced host monitoring.\n\nBecause we know that defending against physical access introduces significant complexity and attack surface even with industry-leading controls, we continuously pursue further attack surface hardening. In addition, we reduce these risks through measures like encrypted DRAM and standard physical security controls to protect our datacenters from bad actors.\n\nTo further address these unknown risks, we seek to eliminate the viability of targeted attacks via routing sessions through a third-party OHTTP relay to prevent an attacker\u2019s ability to route a specific user to a specific machine.\n\nDesigning Private Processing\n\nHere is how we designed Private Processing to meet these foundational security and privacy requirements against the threat model we developed.\n\n(Further technical documentation and security research engagements updates are coming soon).\n\nConfidential processing\n\nData shared to Private Processing is processed in an environment which does not make it available to any other system. This protection is further upheld by encrypting data end-to-end between the client and the Private Processing application, so that only Private Processing, and no one in between \u2013 including Meta, WhatsApp, or any third-party relay \u2013 can access the data.\n\nTo prevent possible user data leakage, only limited service reliability logs are permitted to leave the boundaries of CVM.\n\nSystem software\n\nTo prevent privileged runtime access to Private Processing, we prohibit remote shell access, including from the host machine, and implement security measures including code isolation. Code isolation ensures that only designated code in Private Processing has access to user data. Prohibited remote shell access ensures that neither the host nor a networked user can gain access to the CVM shell.\n\nWe defend against potential source control and supply chain attacks by implementing established industry best practices. This includes building software exclusively from checked-in source code and artifacts, where any change requires multiple engineers to modify the build artifacts or build pipeline.\n\nAs another layer of security, all code changes are auditable. This allows us to ensure that any potential issues are discovered \u2014 either through our continuous internal audits of code, or by external security researchers auditing our binaries.\n\nSystem hardware\n\nPrivate Processing utilizes CPU-based confidential virtualization technologies, along with Confidential Compute mode GPUs, which prevent certain classes of attacks from the host operating system, as well as certain physical attacks.\n\nEnforceable guarantees\n\nPrivate Processing utilizes CPU-based confidential virtualization technologies which allow attestation of software based in a hardware root of trust to guarantee the security of the system prior to each client-server connection. Before any data is transmitted, Private Processing checks these attestations, and confirms them against a third-party log of acceptable binaries.\n\nStateless and forward secure service\n\nWe operate Private Processing as a stateless service, which neither stores nor retains access to messages after the session has been completed.\n\nAdditionally, Private Processing does not store messages to disk or external storage, and thus does not maintain durable access to this data.\n\nAs part of our data minimization efforts, requests to Private Processing only include data that is useful for processing the prompt \u2014 for example, message summarization will only include the messages the user directed AI to summarize.\n\nNon-targetability\n\nPrivate Processing implements the OHTTP protocol to establish a secure session with Meta routing layers. This ensures that Meta and WhatsApp do not know which user is connecting to what CVM. In other words, Meta and WhatsApp do not know the user that initiated a request to Private Processing while the request is in route, so that a specific user cannot be routed to any specific hardware.\n\nPrivate Processing uses anonymous credentials to authenticate users over OHTTP. This way, Private Processing can authenticate users to the Private Processing system, but remains unable to identify them. Private Processing does not include any other identifiable information as part of the request during the establishment of a system session. We limit the impact of small-scale attacks by ensuring that they cannot be used to target the data of a specific user.\n\nVerifiable transparency\n\nTo provide users visibility into the processing of their data and aid in validation of any client-side behaviors, we will provide capabilities to obtain an in-app log of requests made to Private Processing, data shared with it, and details of how that secure session was set up.\n\nIn order to provide verifiability, we will make available the CVM image binary powering Private Processing. We will make these components available to researchers to allow independent, external verification of our implementation.\n\nIn addition, to enable deeper bug bounty research in this area, we will publish source code for certain components of the system, including our attestation verification code or load bearing code.\n\nWe will also be expanding the scope of our existing Bug Bounty program to cover Private Processing to enable further independent security research into Private Processing\u2019s design and implementation.\n\nFinally, we will be publishing a detailed technical white paper on the security engineering design of Private Processing to provide further transparency into our security practices, and aid others in the industry in building similar systems.\n\nGet Involved\n\nWe\u2019re deeply committed to providing our users with the best possible messaging experience while ensuring that only they and the people they\u2019re talking to can access or share their personal messages. Private Processing is a critical component of this commitment, and we\u2019re excited to make it available in the coming weeks.\n\nWe welcome feedback from our users, researchers, and the broader security community through our security research program:", "label": 0}
{"title": "How I Improved My Productivity with Cursor and the Heroku MCP Server", "url": "https://www.heroku.com/blog/improved-my-productivity-cursor-and-heroku-mcp-server/", "content": "Generative AI has been one incredible tool to improve my productivity not only for work but for personal projects too. I use it every day, from generating stories and images for my online role playing games to solving code and engineering problems and building awesome demos. Lately I\u2019ve leaned into Cursor as my go\u2011to AI coding companion. Its inline suggestions and quick edits keep me moving without context\u2011switching. Connecting Cursor to my apps through the Heroku MCP Server lets me perform actions like deploying or scaling, without leaving my code editor, making AI a first class citizen in the Heroku AI PaaS developer toolset. Using it along with the Heroku Extension for VS Code is a total win. In this article, I\u2019ll show you how tying Cursor and MCP together saved me time and helped me focus on the parts of development I actually enjoy.\n\nWhat is Model Context Protocol?\n\nModel Context Protocol (MCP) is an open standard from Anthropic that defines a uniform way for my AI assistant (like Cursor) to talk to external tools and data sources. Instead of juggling custom APIs or integrations, MCP wraps up both the \u201ccontext\u201d my code assistant needs (code snippets, environment state, database schema) and the \u201cinstructions\u201d it should follow (fetch logs, run queries, deploy apps) into a single, predictable format\u2014much like a USB\u2011C port lets any device plug into any charger without extra adapters, Model Context Protocol is the universal connector for your AI tools and services.\n\nUnder the hood, MCP follows a simple client\u2013server model:\n\nHost: my editor or chat interface (e.g., Cursor) that decides what my assistant can access\n\nClient: the small bridge component that keeps a live connection open\n\nServer: a lightweight service exposing specific capabilities (APIs, database calls, shell commands) in MCP\u2019s schema\n\nWhen I ask Cursor to \u201cscale my Heroku dynos\u201d or \u201cpull the latest customer records,\u201d it sends an MCP request to the right server, gets back a structured response, and I can keep coding without switching contexts or writing new integration code.\n\nAI Dev Tools I Use Everyday\n\nWhen I\u2019m not on stage presenting or behind a mic recording a podcast, I\u2019m usually in VS Code building JavaScript demos that highlight Heroku\u2019s capabilities and best practices. Backend work is my comfort zone, front-end and design aren\u2019t, so I lean on AI to bridge those gaps. Given a design spec (from Figma for example), I can get a frontend prototype in minutes, instead of writing HTML/CSS at hand, making the interaction with the design team straightforward. I\u2019ve tried Gemini for ideation, and ChatGPT and Claude for debugging and refactoring code.\n\nLately, though, Cursor has become my go-to IDE. Its inline LLM suggestions and agentic features let me write, test, design, and even deploy code without leaving the editor. Pairing Cursor with different MCPs means that I can remain on the IDE, it keeps me focused, cuts out needless context-switching, and helps me ship demos faster.\n\nHere, I share a list of the MCPs I use and how they improve my productivity:\n\nHeroku MCP Server\n\nAll my demos go straight to Heroku. With the Heroku extension for VS Code, I rarely leave my editor to manage apps. And thanks to the Heroku MCP Server, my AI assistant now deploys, scales dynos, fetches logs, and updates config, all without opening the dashboard or terminal.\n\nTo install it in your IDE, start by generating a Heroku Authorization token:\n\nheroku authorizations:create --description \"Heroku MCP IDE\"\n\nAlternatively, you can generate a token in the Heroku Dashboard:\n\nGo to Account Settings \u2192 Applications \u2192 Authorizations and click Create new authorization. Copy the token you receive.\n\nThen open your Cursor mcp.json and add the following JSON configuration with the previously generated Heroku Authorization token:\n\nNote: Make sure you have npx installed a global command in your operative system, npx is part of Node.js.\n\n{ \"mcpServers\": { \"heroku\": { \"command\": \"npx\", \"args\": [ \"-y\", \"@heroku/mcp-server\" ], \"env\": { \"HEROKU_API_KEY\": \"\" } }, } }\n\nCheck the project README for setup instructions on Claude Desktop, Zed, Cline, Windsurf, and VS Code.\n\nLangChain MCPDoc\n\nMany projects have started to adopt the /llms.txt file, which serves as a website index for LLMs, providing background information, guidance, and links to detailed markdown files. Cursor and other AI IDEs can use the llms.txt file to retrieve context for their tasks. The LangChain MCPDoc offers a convenient way to load llms.txt files, whether they are located remotely or locally, making them available to your agents.\n\nDepending on the project I\u2019m working on, I rely on this MCP to fetch documentation, especially when I\u2019m building other MCPs, I use the recommended https://modelcontextprotocol.io/llms.txt file, or if I\u2019m using LangChain JS to build agentic applications with Node.js, I use https://js.langchain.com/llms.txt.\n\nI have also created my own Heroku llms.txt file, which you can download locally and use for your Heroku-related projects.\n\nHere is how you can setup the LangChain MCPDoc in Cursor:\n\nNote: Make sure you have uvx installed as a global command in your operative system, uvx is part of uv, a Python package manager.\n\n{ \"mcpServers\": { \"heroku-docs-mcp\": { \"command\": \"uvx\", \"args\": [ \"--from\", \"mcpdoc\", \"mcpdoc\", \"--urls\", \"HerokuDevCenter:file:///Users/jduque/AI/llmstxt/heroku/llms.txt\", \"--allowed-domains\", \"*\", \"--transport\", \"stdio\" ] }, \"modelcontextprotocol-docs-mcp\": { \"command\": \"uvx\", \"args\": [ \"--from\", \"mcpdoc\", \"mcpdoc\", \"--urls\", \"ModelContextProtocol:https://modelcontextprotocol.io/llms.txt\", \"--allowed-domains\", \"*\", \"--transport\", \"stdio\" ] } } }\n\nFigma MCP Server\n\nAnother one of my favorites is the Figma MCP Server. It allows Cursor to download design data from Figma. I just copy and paste the link of the frame in Figma that I want to implement into my Cursor chat, and with the right prompt, it does the magic. For example, recently I had to implement our brand guidelines on a demo I\u2019m working on, and I just pasted the frame that contains the Heroku color palette. It created a Tailwind CSS theme with the right styles. Without this tool, I\u2019ll have to copy all the colors from the Figma file and organize them in the JSON structure as expected by Tailwind.\n\nHere is how you can setup the Figma MCP Server in Cursor:\n\n{ \"mcpServers\": { \"figma-mcp-server\": { \"command\": \"npx\", \"args\": [ \"-y\", \"figma-developer-mcp\", \"--figma-api-key=\", \"--stdio\" ] } } }\n\nConclusion\n\nAdding the Heroku MCP Server to Cursor transformed my editor into a powerful development tool. I stopped jumping between terminals, dashboards, and code. Instead, I write a prompt, and Cursor handles the rest: running queries, deploying apps, scaling dynos, or pulling logs.\n\nThis shift improved my productivity and shaved minutes off every task, cutting down on errors from running commands by memory or context-switching. More importantly, it lets me stay in flow longer, so I can focus on the parts of coding I enjoy the most.\n\nIf you\u2019re already using Cursor or another AI coding tool, give MCP a try. Also, take a look at this quick demo where I use the Heroku MCP Server and Cursor to build and deploy a simple web app.", "label": 0}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2024-08", "content": "en\n\nDelores Park\n\nThis is an interactive post featuring audio\n\nI love San Francisco. It is unique with different sounds. You never know what you're going to hear around you! It could be someone who is having a great or someone who isn't doing so well. Sound provides an identity of a city. I've noticed some patterns or sounds I only tend to find here.\n\nSomething I enjoy doing is recording the sounds of San Francisco. It could be as simple as recording near Powell Station or the Mission district. These sounds were recorded using an iPhone 12.\n\nHere is a small sample of sounds. Enjoy! Bluegrass Concert - Click to Expand!\n\nOne of my good friends plays the banjo. He's a great banjo player. If you like bluegrass he's the person to talk to!\n\nBluegrass was foreign to me until my friend taught me so much about it.\n\nYour browser does not support the audio tag.\n\nTaking the MUNI- Click to Expand! I really, really enjoy taking public transportation. Not only is it better for traffic, and reduces pollution. The MUNI (and BART) stations are great places to get some audio recordings. It feels great to have for San Francisco and the uniqueness of its underground tunnels.\n\nYour browser does not support the audio tag.\n\nPublic Conversation - Click to Expand! Wherever I turn, I hear conversations ranging from, \"What are we having for lunch?\" to \"Let's go to grab a drink!\" It's nice to hear people talking and enjoying each other's company through conversations in public spaces such as buses, on the street or simply sitting down on a bench.\n\nYour browser does not support the audio tag.\n\nStreet - Click to Expand!\n\nThere's something so beautiful about hearing the background sounds of cars, people walking, bikes, the wind, etc. Growing up it was very quiet growing up in a small midwest town. I enjoy the \"naturally\" occurring city street sounds.\n\nYour browser does not support the audio tag.\n\n\ud83d\udccd This post was written in my apartment in San Francisco.", "label": 1}
{"title": "Sick on Thanksgiving", "url": "https://lifeofpablo.com/blog/sick-on-thanksgiving-2024", "content": "Sick on Thanksgiving\n\nThis post was written in English (en_US).\n\nI'm lying on my couch in my San Francisco apartment trying to recover from the sickness I acquired. The last few days have been rough! Being sick is something I don't like to experience but here we are.\n\nSince I couldn't return to Nebraska this year for the holiday, I had plans to do a Friendsgiving.\n\nEveryone is enjoying themselves and having a grand old time.\n\nIt would be selfish of me to show up sick, and miserable, and risk getting people sick. I know people understand, but I don't like to disappoint others.\n\nI am grateful for a few things:\n\nMy Parents\n\nEven though I couldn't return to Nebraska, I am very grateful for them. They have been very supportive in various aspects of life. I know they miss me and they wish I could move back. They love how I've branched off to live in a different place. I miss them a lot but I will see them soon!\n\nMy Friend Evan\n\nMy friend Evan was so kind enough for me to bring me crackers as I was struggling yesterday. He's a keeper! Those crackers brought me back to life. I'm so appreciative to have Evan in my life.\n\nEvan even walked me to McDonald's this morning making sure I got a light meal. McDonald's was closed. :( I just wanted two burritos.\n\nMy Friends Sarah and Mike\n\nMy friend Sarah has been amazing over the years especially when I moved to Sacramento. She has helped me so much in getting used to California living and of course, having to put up with my shenanigans.\n\nMike is a great friend as well. He helped me guide my career to where it is now! He is a great person to have an amazing conversation with. I love talking about ethics with him and anything related to tech.\n\nI miss them both!\n\nAlone Time\n\nBeing sick allowed me to do nothing, which is not bad. I ordered some soup in hopes of making me feel better sooner. I probably should watch a movie instead of doom-scrolling. I deserve to enjoy myself a little. Maybe I will code and work on my website and working on an API.\n\nHomebrew Website Club\n\nI am grateful I hung out remotely at Homebrew Website Club yesterday! I didn't participate as much as I wanted but I still made the effort to attend. There was some great conversation happening! Shoutout to Tracy and Angelo for hanging out and tucking me in virtually when I finally crashed.\n\nHappy Holidays.", "label": 1}
{"title": "Using KerasHub for easy end-to-end machine learning workflows with Hugging Face", "url": "https://developers.googleblog.com/en/load-model-weights-from-safetensors-into-kerashub-multi-framework-machine-learning/", "content": "How to load SafeTensors checkpoints across different frameworks\n\nAs the AI ecosystem continues to evolve, there are more and more ways to define machine learning models, and even more ways to save the model weights that result from training and fine-tuning. In this growing set of choices, KerasHub allows you to mix and match popular model architectures and their weights across different ML frameworks. For example, a popular place to load checkpoints from is the Hugging Face Hub. Many of those model checkpoints were created with the Hugging Face transformers library in the SafeTensors format. Regardless of what ML framework was used to create the model checkpoint, those weights can be loaded into a KerasHub model, which allows you to use your choice of framework (JAX, PyTorch, or TensorFlow) to run the model. Yes, that means you can run a checkpoint from Mistral or Llama on JAX, or even load Gemma with PyTorch \u2013 it doesn't get any more flexible than that. Let's take a look at some of these terms in more detail, and talk about how this works in practice.\n\nModel architecture vs. model weights When loading models, there are two distinct parts that we need: the model architecture and the model weights (often called \"checkpoints\"). Let's define each of these in more detail. When we say \"model architecture\", we are referring to how the layers of the model are arranged, and the operations that happen within them. Another way to describe this might be to call it the \"structure\" of the model. We use Python frameworks like PyTorch, JAX, or Keras to express model architectures. When we talk about \"model weights\", we are referring to the \"parameters\" of a model, or numbers in a model that are changed over the course of training. The particular values of these weights are what give a trained model its characteristics. \"Checkpoints\" are a snapshot of the values of the model weights at a particular point in the training. The typical checkpoint files that are shared and widely used are the ones where the model has reached a particularly good training outcome. As the same model architecture is further refined with fine-tuning and other techniques, additional new checkpoint files are created. For example, many developers have taken Google's gemma-2-2b-it model and fine-tuned it with their own datasets, and you can see over 600 examples. All of these fine-tuned models use the same architecture as the original gemma-2-2b-it model, but their checkpoints have differing weights. So there we have it: the model architecture is described with code, while model weights are trained parameters, saved as checkpoint files. When we have a model architecture together with a set of model weights (in the form of a checkpoint file), we create a functioning model that produces useful outputs.\n\nSorry, your browser doesn't support playback for this video Different model weights can be loaded into the same model architecture. These different sets of weights are saved as checkpoints.\n\nTools like Hugging Face's transformers library and Google's KerasHub library provide model architectures and the APIs you need to experiment with them. Examples of checkpoint repositories include Hugging Face Hub and Kaggle Models. You can mix and match model architecture libraries with your choice of checkpoint repositories. For example, you can load a checkpoint from Hugging Face Hub into a JAX model architecture and fine-tune it with KerasHub. For a different task, you might find a checkpoint on Kaggle Models that's suitable for your needs. This flexibility and separation means you are not boxed into one ecosystem.\n\nWhat is KerasHub? So we\u2019ve mentioned KerasHub a few times\u2013 let\u2019s go into it in more detail. KerasHub is a Python library that helps make defining model architectures easier. It contains many of the most popular and commonly used machine learning models today, and more are being added all the time. Because it's based on Keras, KerasHub supports all three major Python machine learning libraries used today: PyTorch, JAX, and TensorFlow. This means you can have model architectures defined in whichever library you'd like. Furthermore, since KerasHub supports the most common checkpoint formats, you can easily load checkpoints from many checkpoint repositories. For example, you can find hundreds of thousands of checkpoints on Hugging Face and Kaggle to load into these model architectures.\n\nComparisons to the Hugging Face transformers library A common workflow by developers is to use the Hugging Face transformers library to fine-tune a model and upload it to the Hugging Face Hub. And if you\u2019re a user of transformers , you\u2019ll also find many familiar API patterns in KerasHub. Check out the KerasHub API documentation to learn more. An interesting aspect of KerasHub is that many of the checkpoints found on Hugging Face Hub are compatible with not only the transformers library, but also KerasHub. Let's take a look at how that works.\n\nKerasHub is compatible with Hugging Face Hub Hugging Face has a model checkpoint repository, called Hugging Face Hub. It's one of the many places where the machine learning community uploads their model checkpoints to share with the world. Especially popular on Hugging Face is the SafeTensors format, which is compatible with KerasHub. You can load these checkpoints from Hugging Face Hub directly into your KerasHub model, as long as the model architecture is available. Wondering if your favorite model is available? You can check https://keras.io/keras_hub/presets/ for a list of supported model architectures. And don't forget, all the community created fine-tuned checkpoints of these model architectures are also compatible! We recently created a new guide to help explain the process in more detail. How does this all work? KerasHub has built-in converters that simplify the use of Hugging Face transformers models. These converters automatically handle the process of translating Hugging Face model checkpoints into a format that's compatible with the KerasHub. This means you can seamlessly load a wide variety of pretrained Hugging Face transformer models from the Hugging Face Hub directly into KerasHub with just a few lines of code. If you notice a missing model architecture, you can add it by filing a pull request on GitHub.\n\nHow to load a Hugging Face Hub checkpoint into KerasHub So how do we get checkpoints from Hugging Face Hub loaded into KerasHub? Let's take a look at some concrete examples. We'll start by first choosing our machine learning library as our Keras \"backend\". We'll use JAX in the examples shown, but you can choose between JAX, PyTorch, or TensorFlow for any of them. All the examples below work regardless of which one you choose. Then we can proceed by importing keras , keras_hub , and huggingface_hub , and then login with our Hugging Face User Access token so we can access the model checkpoints.\n\nimport os os.environ[\"KERAS_BACKEND\"] = \"jax\" # or \"torch\" or \"tensorflow\" import keras from keras_hub import models from huggingface_hub import login login('HUGGINGFACE_TOKEN') Python Copied\n\nPut a Mistral model on JAX First up, perhaps we want to run a checkpoint from Mistral on JAX? Over on KerasHub, there are a handful of Mistral models available on KerasHub's list of available model architectures, let's try out mistral_0.2_instruct_7b_en . Clicking into it, we see that we should use the MistralCausalLM class to call from_preset . On the Hugging Face Hub side of things, we see that the corresponding model checkpoint is stored here, with over 900 fine-tuned versions. Browsing that list, there's a popular cybersecurity-focused fine-tuned model called Lily, with the pathname of segolilylabs/Lily-Cybersecurity-7B-v0.2 . We'll also need to add \" hf:// \" before that path to specify that KerasHub should look at Hugging Face Hub.\n\nSorry, your browser doesn't support playback for this video\n\nPutting it all together, we get the following code:\n\n# Model checkpoint from Hugging Face Hub gemma_lm = models.MistralCausalLM.from_preset(\"hf://segolilylabs/Lily-Cybersecurity-7B-v0.2\") gemma_lm.generate(\"Lily, how do evil twin wireless attacks work?\", max_length=30) Python Copied\n\nRunning Llama 3.1 on JAX Llama 3.1-8B-Instruct is a popular model, with over 5 million downloads last month. Let's put a fine-tuned version on JAX. With over 1400 fine-tuned checkpoints, there's no lack of choice. The xVerify fine-tuned checkpoint looks interesting, let's load that into JAX on KerasHub. We'll use the Llama3CausalLM class to reflect the model architecture that we are using. As before, we'll need the appropriate path from Hugging Face Hub, prefixed with \" hf:// \". It's pretty amazing that we can load and call a model with just two lines of code, right?\n\n# Model checkpoint from Hugging Face Hub gemma_lm = models.Llama3CausalLM.from_preset(\"hf://IAAR-Shanghai/xVerify-8B-I\") gemma_lm.generate(\"What is the tallest building in NYC?\", max_length=100) Python Copied\n\nLoad Gemma on JAX Finally, let's load a fine-tuned Gemma-3-4b-it checkpoint into JAX. We'll use the Gemma3CausalLM class, and select one of the fine-tuned checkpoints. How about EraX, a multilingual translator? As before, we'll use the pathname with the Hugging Face Hub prefix to create the full path of \" hf://erax-ai/EraX-Translator-V1.0 \".\n\n# Model checkpoint from Hugging Face Hub gemma_lm = models.Gemma3CausalLM.from_preset(\"hf://erax-ai/EraX-Translator-V1.0\") gemma_lm.generate(\"Translate to German: \", max_length=30) Python Copied", "label": 0}
{"title": "Setting Up Vouch Proxy using Nginx", "url": "https://lifeofpablo.com/blog/setting-up-vouch-proxy-using-nginx", "content": "Setting Up Vouch Proxy using Nginx\n\nThis post was written in English (en_US).\n\nLocation: 38.581573, -121.494400\n\nBlog Post on this using Indieauth coming soon!\n\nTable of Contents\n\nIntroduction\n\nRecently I have started experimenting with identity. An SSO solution for Nginx using the auth_request module. Vouch Proxy can protect all of your websites at once.\n\nToday, I'll demonstrate how to setup Vouch Proxy on an nginx web server. In this example I will be using Google as our provider\n\nThis tutorial assumes you have prior knowledge of using a linux server such as Debian. Message me at hello@lifeofpablo.com if you need some help. I'd be happy to do so!\n\nWhat Vouch Proxy Does?\n\nAccording to the Repository README.md, it states the following:\n\nVouch Proxy (VP) forces visitors to login and authenticate with an IdP (such as one of the services listed above) before allowing them access to a website.\n\nVP can also be used as a Single Sign On (SSO) solution to protect all web applications in the same domain.\n\nAfter a visitor logs in Vouch Proxy allows access to the protected websites for several hours. Every request is checked by VP to ensure that it is valid.\n\nVP can send the visitor's email, name and other information which the IdP provides (including access tokens) to the web application as HTTP headers. VP can be used to replace application user management entirely.\n\nThings you'll need/prepare:\n\nA linux server with a public IP address with hosting and SSL Debian will be used here but any of the common distros will work Certbot is an easy solution to get SSL certifcate for https://\n\nGo Language (to compile vouch-proxy)\n\nVouch Proxy\n\nNginx Web Server Digital Ocean has a good guide if you need to learn how to setup virtual blocks in nginx.\n\n\n\nDownload/Install Vouch Proxy from Github\n\nMake sure to have Go Lang installed\n\n$ git clone https://github.com/vouch/vouch-proxy.git $ cd vouch-proxy $ ./do.sh goget $ ./do.sh build\n\nVouch Proxy Nginx Virtual Block\n\nLet's go ahead and create a virtual block to proxy Vouch Proxy.\n\nserver { server_name vouch.example.com; location / { proxy_set_header Host vouch.example.com; proxy_set_header X-Forwarded-Proto https; proxy_pass http://127.0.0.1:9090; }\n\nLet's go ahead and create a virtual block for a regular nginx website site or edit an existing virtual block. This is the website/service that you will protect with Vouch Proxy.\n\nIn this example I am using a php web app. If you a non php site site to work you can remove this location block and and edit it to your needs.\n\nGoogle Cloud Console\n\nGoogle Cloud Console API\n\nBefore we modify the config.yml, lets create an OAuth 2.0 Client ID and Client Secret which you will paste into the config.yml file.\n\nYou will have to do the following 1. Create a Project\n\nCall it whatever you like.\n\nI called it oauth\n\nCreate Credentials\n\nYou'll create the client ID and client secret (don't worry if you do the following steps and forget to copy these down. They will be available for you to copy at any time.\n\nOAuth consent screen\n\nApp Name:\n\nThis is where you will need to setup where your redirect URL is https://vouch.example.com/url. Just like the server_name set in the nginx virtual block config above.\n\nAuthorized Domain: example.com (just the base domain)\n\nFill out any other required fields\n\nModify your config.yml\n\nvouch: domains: - yourdomain.com - yourotherdomain.com cookie: secure: false oauth: provider: google client_id: xxxxxxxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.apps.googleusercontent.com client_secret: xxxxxxxxxxxxxxxxxxxxxxxx callback_urls: - https://yourdomain.com/auth - https://yourotherdomain.com/auth preferredDomain: yourdomain.com\n\nNginx Virtual block protected by Vouch Proxy\n\nserver { listen 80 ; listen [::]: 80 ; root /root/to/web/directory; index index.php index.html; server_name secretapp.example.com; location / { try_files $uri $uri / /index.php $is_args $args ; } client_max_body_size 100m ; location ~* \\.php$ { fastcgi_pass unix:/var/run/php/php8.2-fpm.sock; include fastcgi.conf; fastcgi_index yellow.php; fastcgi_split_path_info ^(.+\\.php)(/.+)$ ; fastcgi_param SCRIPT_FILENAME $document_root $fastcgi_script_name ; include fastcgi_params; } }\n\nEventually you will need to secure your site with SSL/TLS that makes your site use https://. Google will require that your traffic is secure with using it as 0auth as the method used to sign in to your protected website.\n\nDo this after you have the survey blocks working in the following section.\n\nHere is the link for Certbot for Debian. I have tested this on Debian 10 & 11. [https://certbot.eff.org/instructions?ws=nginx&os=debianbuster](Link for Certbot)\n\nCert bot can do this for you as long as you have the subdomain in your DNS pointing to your machine and have cert bot installed. It'll add these blocks in your\n\nor\n\nserver { server_name vouch.example.com . . . . . . . . . . . . . . . . . . . . . . . . . listen [::]: 443 ssl; listen 443 ssl; ssl_certificate /etc/letsencrypt/live/vouch.example.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/vouch.example.com/privkey.pem; include /etc/letsencrypt/options-ssl-nginx.conf; ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; } server { if ( $host = vouch.example.com) { return 301 https:// $host $request_uri ; } listen 80 ; listen [::]: 80 ; server_name secretapp.example.com; return 404 ; }\n\nLet's check for errors in nginx. Type the following command.\n\nnginx -t\n\nYou should see something similar to this:\n\nnginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful\n\nLet's open a browser tab or window!\n\nNote: I'm using firefox. (Preference). Any modern browser should work.\n\nType in the protected app' URL in the address bar\n\nSign in to Google\n\nSign in with an email that is allowed to sign to access the website when you configured it in Google Cloud Console.\n\nVoila, the protected page.\n\nHere is the home page of a Bludit CMS on subdomain acting as \"secretapp.example.com\"", "label": 1}
{"title": "Announcing the Agent2Agent Protocol (A2A)", "url": "https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/", "content": "A new era of Agent Interoperability\n\n\n\nAI agents offer a unique opportunity to help people be more productive by autonomously handling many daily recurring or complex tasks. Today, enterprises are increasingly building and deploying autonomous agents to help scale, automate and enhance processes throughout the workplace\u2013from ordering new laptops, to aiding customer service representatives, to assisting in supply chain planning.\n\nTo maximize the benefits from agentic AI, it is critical for these agents to be able to collaborate in a dynamic, multi-agent ecosystem across siloed data systems and applications. Enabling agents to interoperate with each other, even if they were built by different vendors or in a different framework, will increase autonomy and multiply productivity gains, while lowering long-term costs.\n\n\n\nToday, we\u2019re launching a new, open protocol called Agent2Agent (A2A), with support and contributions from more than 50 technology partners like Atlassian, Box, Cohere, Intuit, Langchain, MongoDB, PayPal, Salesforce, SAP, ServiceNow, UKG and Workday; and leading service providers including Accenture, BCG, Capgemini, Cognizant, Deloitte, HCLTech, Infosys, KPMG, McKinsey, PwC, TCS, and Wipro. The A2A protocol will allow AI agents to communicate with each other, securely exchange information, and coordinate actions on top of various enterprise platforms or applications. We believe the A2A framework will add significant value for customers, whose AI agents will now be able to work across their entire enterprise application estates.\n\nThis collaborative effort signifies a shared vision of a future when AI agents, regardless of their underlying technologies, can seamlessly collaborate to automate complex enterprise workflows and drive unprecedented levels of efficiency and innovation.\n\nA2A is an open protocol that complements Anthropic's Model Context Protocol (MCP), which provides helpful tools and context to agents. Drawing on Google's internal expertise in scaling agentic systems, we designed the A2A protocol to address the challenges we identified in deploying large-scale, multi-agent systems for our customers. A2A empowers developers to build agents capable of connecting with any other agent built using the protocol and offers users the flexibility to combine agents from various providers. Critically, businesses benefit from a standardized method for managing their agents across diverse platforms and cloud environments. We believe this universal interoperability is essential for fully realizing the potential of collaborative AI agents.", "label": 0}
{"title": "Unlocking the power of unstructured data with RAG", "url": "https://github.blog/ai-and-ml/llms/unlocking-the-power-of-unstructured-data-with-rag/", "content": "Whether they\u2019re building a new product or improving a process or feature, developers and IT leaders need data and insights to make informed decisions.\n\nWhen it comes to software development, this data exists in two ways: unstructured and structured. While structured data follows a specific and predefined format, unstructured data\u2014like email, an audio or visual file, code comment, or commit message\u2014doesn\u2019t. This makes unstructured data hard to organize and interpret, which means teams can miss out on potentially valuable insights.\n\nTo make the most of their unstructured data, development teams are turning to retrieval-augmented generation, or RAG, a method for customizing large language models (LLMs). They can use RAG to keep LLMs up to date with organizational knowledge and the latest information available on the web. They can also use RAG and LLMs to surface and extract insights from unstructured data.\n\nGitHub data scientists, Pam Moriarty and Jessica Guo, explain unstructured data\u2019s unique value in software development, and how developers and organizations can use RAG to create greater efficiency and value in the development process.\n\nUnstructured data in software development\n\nWhen it comes to software development, unstructured data includes source code and the context surrounding it, as these sources of information don\u2019t follow a predefined format.\n\nHere are some examples of unstructured data on GitHub:\n\nREADME files describe in text the purpose behind project source code, and include instructions for source code use, how to contribute, and other details that developers decide is important to include. While they\u2019re usually written in Markdown, README files don\u2019t follow a predefined structure.\n\ndescribe in text the purpose behind project source code, and include instructions for source code use, how to contribute, and other details that developers decide is important to include. While they\u2019re usually written in Markdown, README files don\u2019t follow a predefined structure. Code files are more orderly than README files in that they follow the syntax of a programming language. But not all code files have the exact same fields nor are they all written in the same format. Additionally, some parts of the file, like coding logic and variable names, are decided by individual developers.\n\nare more orderly than README files in that they follow the syntax of a programming language. But not all code files have the exact same fields nor are they all written in the same format. Additionally, some parts of the file, like coding logic and variable names, are decided by individual developers. Package documentation explains how the software works and how to use it. Documentation, written in natural language, can include installation instructions, troubleshooting tips, a description of the package\u2019s API, and a list of any dependencies required to use the package. It can also include code snippets that highlight the package\u2019s features.\n\nexplains how the software works and how to use it. Documentation, written in natural language, can include installation instructions, troubleshooting tips, a description of the package\u2019s API, and a list of any dependencies required to use the package. It can also include code snippets that highlight the package\u2019s features. Code comments explain the function behind certain code blocks in a code file. They\u2019re text comments written in natural language and make the source code easier to understand by other developers.\n\nexplain the function behind certain code blocks in a code file. They\u2019re text comments written in natural language and make the source code easier to understand by other developers. Wiki pages , while not limited to unstructured data, can contain helpful text documentation about installation instructions, API references, and other information.\n\n, while not limited to unstructured data, can contain helpful text documentation about installation instructions, API references, and other information. Commit messages describe in natural language text the changes a developer made to a codebase and why.\n\ndescribe in natural language text the changes a developer made to a codebase and why. Issue and pull request descriptions are written in natural language and in a text field. They can contain any kind of information a developer chooses to include about a bug, feature request, or general task in a project.\n\nare written in natural language and in a text field. They can contain any kind of information a developer chooses to include about a bug, feature request, or general task in a project. Discussions contain a wealth and variety of information, from developer and end- user feedback to open-ended conversations about a topic. As long as a repository enables discussions, anyone with a GitHub account can start a discussion.\n\ncontain a wealth and variety of information, from developer and end- user feedback to open-ended conversations about a topic. As long as a repository enables discussions, anyone with a GitHub account can start a discussion. Review comments are where developers can discuss changes before they\u2019re merged into a codebase. Consequently, they contain information in natural language about code quality, context behind certain decisions, and concerns about potential bugs.\n\nThe value of unstructured data\n\nThe same features that make unstructured data valuable also make it hard to analyze.\n\nUnstructured data lacks inherent organization, as it often consists of free-form text, images, or multimedia content.\n\n\u201cWithout clear boundaries or predefined formats, extracting meaningful information from unstructured data becomes very challenging,\u201d Guo says.\n\nBut LLMs can help to identify complex patterns in unstructured data\u2014especially text. Though not all unstructured data is text, a lot of text is unstructured. And LLMs can help you to analyze it.\n\n\u201cWhen dealing with ambiguous, semi-structured or unstructured data, LLMs dramatically excel at identifying patterns, sentiments, entities, and topics within text data and uncover valuable insights that might otherwise remain hidden,\u201d Guo explains.\n\nNeed a refresher on LLMs? Check out our AI explainers, guides, and best practices >\n\nHere are a few reasons why developers and IT leaders might consider using RAG-powered LLMs to leverage unstructured data:\n\nSurface organizational best practices and establish consistency . Through RAG, an LLM can receive a prompt with additional context pulled from an organization\u2019s repositories and documents. So, instead of sifting through and piece-mealing documents, developers can quickly receive answers from an LLM that align with their organization\u2019s knowledge and best practices.\n\n. Through RAG, an LLM can receive a prompt with additional context pulled from an organization\u2019s repositories and documents. So, instead of sifting through and piece-mealing documents, developers can quickly receive answers from an LLM that align with their organization\u2019s knowledge and best practices. Accelerate and deepen understanding of an existing codebase\u2014including its conventions, functions, common issues, and bugs. Understanding and familiarizing yourself with code written by another developer is a persisting challenge for several reasons, including but not limited to: code complexity, use of different coding styles, a lack of documentation, use of legacy code or deprecated libraries and APIs, and the buildup of technical debt from quick fixes and workarounds.\n\nRAG can help to mediate these pain points by enabling developers to ask and receive answers in natural language about a specific codebase. It can also guide developers to relevant documentation or existing solutions.\n\nAccelerated and deepened understanding of a codebase enables junior developers to contribute their first pull request with less onboarding time and senior developers to mitigate live site incidents, even when they\u2019re unfamiliar with the service that\u2019s failing. It also means that legacy code suffering from \u201ccode rot\u201d and natural aging can be more quickly modernized and easily maintained.\n\nUnstructured data doesn\u2019t just help to improve development processes. It can also improve product decisions by surfacing user pain points.\n\nMoriarty says, \u201cStructured data might show a user\u2019s decision to upgrade or renew a subscription, or how frequently they use a product or not. While those decisions represent the user\u2019s attitude and feelings toward the product, it\u2019s not a complete representation. Unstructured data allows for more nuanced and qualitative feedback, making for a more complete picture.\u201d\n\nA lot of information and feedback is shared during informal discussions, whether those discussions happen on a call, over email, on social platforms, or in an instant message. From these discussions, decision makers and builders can find helpful feedback to improve a service or product, and understand general public and user sentiment.\n\nWhat about structured data?\n\nContrary to unstructured data, structured data\u2014like relational databases, Protobuf files, and configuration files\u2014follows a specific and predefined format.\n\nWe\u2019re not saying unstructured data is more valuable than structured. But the processes for analyzing structured data are more straightforward: you can use SQL functions to modify the data and traditional statistical methods to understand the relationship between different variables.\n\nThat\u2019s not to say AI isn\u2019t used for structured data analysis. \u201cThere\u2019s a reason that machine learning, given its predictive power, is and continues to be widespread across industries that use data,\u201d according to Moriarty.\n\nHowever, \u201cStructured data is often numeric, and numbers are simply easier to analyze for patterns than words are,\u201d Moriarty says. Not to mention that methods for analyzing structured data have been around longer** **than those for analyzing unstructured data: \u201cA longer history with more focus just means there are more established approaches, and more people are familiar with it,\u201d she explains.\n\nThat\u2019s why the demand to enhance structured data might seem less urgent, according to Guo. \u201cThe potential for transformative impact is significantly greater when applied to unstructured data,\u201d she says.\n\nHow does RAG extract value from unstructured data?\n\nWith RAG, an LLM can use data sources beyond its training data to generate an output.\n\nRAG is a prompting method that uses retrieval\u2014a process for searching for and accessing information\u2014to add more context to a prompt that generates an LLM response.\n\nThis method is designed to improve the quality and relevance of an LLM\u2019s outputs. Additional data sources include a vector database, traditional database, or search engine. So, developers who use an enterprise AI tool equipped with RAG can receive AI outputs customized to their organization\u2019s best practices and knowledge, and proprietary data.\n\nWe break down these data sources in our RAG explainer, but here\u2019s a quick summary:\n\nVector databases. While you code in your IDE, algorithms create embeddings for your code snippets, which are stored in a vector database. An AI coding tool can search that database to find snippets from across your codebase that are similar to the code you\u2019re currently writing and generate a suggestion.\n\nAnd when you\u2019re engaging with GitHub Copilot Chat on GitHub.com or in the IDE, your query or code is transformed into an embedding. Our retrieval service then fetches relevant embeddings from the vector database for the repository you\u2019ve indexed. These embeddings are turned back into text and code when they\u2019re added to the prompt as additional context for the LLM. This entire process leverages unstructured data, even though the retrieval system uses embeddings internally.\n\nGeneral text search. When developers engage with GitHub Copilot Chat under a GitHub Copilot Enterprise plan, they can index repositories\u2014specifically code and documentation. So, when a developer on GitHub.com or in the IDE asks GitHub Copilot Chat a question about an indexed repository, the AI coding tool can retrieve data from all of those indexed, unstructured data sources. And on GitHub.com, GitHub Copilot Chat can tap into a collection of unstructured data in Markdown files from across repositories, which we call knowledge bases.\n\nLearn about GitHub Copilot Enterprise features >\n\nBut wait, why is Markdown considered unstructured data? Though you can use Markdown to format a file, the file itself can contain essentially any kind of data. Think about it this way: how would you put the contents of a Markdown file in a table?\n\nExternal or internal search engine. The retrieval method searches and pulls information from a wide range of sources from the public web or your internal platforms and websites. That information is used for RAG, which means the AI model now has data from additional files\u2014like text, image, video, and audio\u2014to answer your questions.\n\nRetrieval also taps into internal search engines. So, if a developer wants to ask a question about a specific repository, they can index the repository and then send their question to GitHub Copilot Chat on GitHub.com. Retrieval uses our internal search engine to find relevant code or text from the indexed files, which are then used by RAG to prompt the LLM for a contextually relevant response.\n\nStay smart: LLMs can do things they weren\u2019t trained to do, so it\u2019s important to always evaluate and verify their outputs.\n\nRAG and GitHub Copilot Enterprise Powered by RAG, GitHub Copilot Enterprise can help developers and leaders at all levels receive natural language answers to questions about specific repositories. GitHub Copilot can also use content in commits, issues, and discussions to provide contextually relevant responses. In fact, by asking GitHub Copilot questions, developers actually provide GitHub Copilot with more details about the context in which information is being used, which then helps the AI coding tool provide more accurate responses tailored to an organization\u2019s unique codebase. Learn more about the use cases and benefits of GitHub Copilot Enterprise.\n\nUse RAG to unlock insights from unstructured data\n\nAs developers improve their productivity and write more code with AI tools like GitHub Copilot, there\u2019ll be even more unstructured data. Not just in the code itself, but also the information used to build, contextualize, maintain, and improve that code.\n\nThat means even more data containing rich insights that organizations can surface and leverage, or let sink and disappear.\n\nDevelopers and IT leaders can use RAG as a tool to help improve their productivity, produce high-quality and consistent code at greater speed, preserve and share information, and increase their understanding of existing codebases, which can impact reduced onboarding time.\n\nWith a RAG-powered AI tool, developers and IT leaders can quickly discover, analyze, and evaluate a wealth of unstructured data\u2014simply by asking a question.\n\nA RAG reading list \ud83d\udcda", "label": 0}
{"title": "I Took Myself Out on Date", "url": "https://lifeofpablo.com/blog/solo-date", "content": "Today, I wanted to get out the apartment and not bum around. So I went on a date. It's not often I get time for myself other than at home. I either stay home or I go the usual places I'm comfortable going.\n\nI drove down to Simon's Cafe (Chinese Food Staple in Midtown Sacramento. As I was pulling up I remembered they closed down. I was sad. Then I remembered that there was a Thai restaurant a block or two down. Chicken Pad Thai was the meal of choice.\n\nThen, I went to went to happy hour at a Wine Bar across the street from the Thai restaraunt. I had a glass of Sangria. I don't mind a cheap drink during happy hour.\n\nAfter, I went to an art gallery to observe the art. I got to meet the artist of some of the art I admired. That was pretty tight!\n\nThen I came across to the Strapping Store location where ever I was in Midtown. I got some cool Sacramento themed items (last minute Christmas gifts). It's a pretty cool store.\n\nI ended the night with getting Frozen Yogurt. Strawberry Cheesecake Fro-yo and grahm crackers are amazing.\n\nIt was nice taking myself out. I don't do this often. This is something I wouldn't mind getting more comfortable with. It simply is trying to get out of my comfort zone. Overall, I enjoyed the alone time I had. Alone time is good. Taking yourself out on a date should be okay. Just enjoy your own presence.", "label": 1}
{"title": "Back to \u201cnormal\u201d \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/15/back-to-normal/", "content": "Not to pick on this person, it\u2019s a funny protest sign \u2014 but it seems like a lot of people are thinking like we\u2019re in a \u201cone weird trick\u201d (impeachment) scenario and, once we get that done, we\u2019ll magically return to some type of \u201cnormal\u201d and we can all go home\u2026 but the concept of normality is doing some heavy lifting here.\n\nWhat \u201cnormal\u201d looks like\n\nIt\u2019s normal for minimum wage to stay the same for decades.\n\nIt\u2019s normal that people earning low wages must work multiple jobs and often still qualify for food stamps.\n\nIt\u2019s normal that billionaires exist when low-paid workers have to decide between the power bill and paying the rent.\n\nIt\u2019s normal that disabled people effectively cannot marry, since they\u2019ll often lose access to government support.\n\nIt\u2019s normal to use prisoners as slave labor (or close enough to it with hourly \u201cwages\u201d of pennies).\n\nIt\u2019s normal that immigrants from some countries must wait decades to become citizens because of arbitrary caps.\n\nIt\u2019s normal to let immigrants get paid less and treated worse in tech jobs because they\u2019re trapped by H1B visa requirements.\n\nIt\u2019s normal that *everyone* in America is to some extent trapped in their job because healthcare is ruinously expensive.\n\nIt\u2019s normal that Americans die of preventable disease for lack of money.\n\nIt\u2019s normal that women don\u2019t get to choose what medical treatment they receive if they are pregnant.\n\nIt\u2019s normal to address gun violence by blaming mental health but not increasing funding to expand access.\n\nIt\u2019s normal to \u201csolve\u201d homelessness by throwing away all of someone\u2019s worldly possessions and putting them on a one-way bus out of town.\n\nIt\u2019s normal that kids go hungry at lunch.\n\nIt\u2019s normal that Puerto Ricans and people from D.C. are denied congressional representation.\n\nIt\u2019s normal that 70- and 80-year-olds hold most of the political power in the country, and people under 40 very little.\n\nIt\u2019s normal to hand over all our personal data to megacorporations who run extensive systems of surveillance that they share with law enforcement.\n\nIt\u2019s normal that the government spies on its own citizens.\n\nIt\u2019s normal that hundreds of people die in car crashes daily.\n\nIt\u2019s normal that our spaces are built for cars, not people, and driving is often the only way to get around.\n\nIt\u2019s normal that the US government doesn\u2019t honor its treaties with Tribes.\n\nIt\u2019s normal that we supply Israel with funding while they enact genocide.\n\nIt\u2019s normal that our water rights system is so terrible, and our water use so excessive, that the Colorado River no longer reaches the Gulf of California.\n\nIt\u2019s normal that downstream communities must bear the health burdens and costs of upstream industrial polluters.\n\nThinking bigger than just getting out of this\n\n\u201cNormal\u201d fucking sucks. Outright fascism is worse, but there are plenty of things we need to change. Kat Abughazaleh, who\u2019s running for office in Illinois, says:\n\n\u201cThere\u2019s no reason every American should not be able to afford housing, groceries, health insurance, public transit (ideally), and then still have enough money to save and take your kid to the zoo or go to the movies with your friends. There\u2019s just, there\u2019s no reason \u2014 we are the wealthiest country in the world. The idea that that\u2019s unrealistic or idealistic or naive or even called childish, I think that\u2019s sucky.\u201d\n\nWe don\u2019t have to settle for returning to a normal that sucks \u2014 but protest may not be enough to force change. Our current situation has been the Republican gameplan for decades; we will not escape it by liberating ourselves from a single politician. Trump is a symptom, but the rot runs much deeper. We need to reckon emotionally with the long fights ahead; protests alone will not solve our problems. I\u2019m not saying protest isn\u2019t helpful or important \u2014 I\u2019m saying that we need to find space for both brunch and advocacy in our lives. Micah L. Sifry writes:\n\nOne of the most inspiring ways that pro-democracy movements grow their strength is by inventing and spreading ways for the silent majority to make itself both visible and influential.\n\nI\u2019ve been seeing organizers talking about the need to transform the current energy into long-term work, and not let it fizzle out. Jared Yates Sexton describes the way that authorities rein in public speech:\n\nWe are, according to those who want nothing more than to maintain control, supposed to protest enough, but not too much. Because we should rely on them. Because we need to maintain our subservient position under the party as opposed to in conversation and discourse with the party.\n\nOne strategy: focusing on local advocacy and building up\n\nMy pet theory is that most people should spend their advocacy time supporting local (and state) efforts, and donate to organizations advocating at the federal level. The collapse of local news and intense media focus on national politics encourages people to feel powerless because at the federal level most of what you can do is harass your senator. It also frames politics as something \u201cyou do\u201d once every four years for normies, every two years for the passionate, every election for the wonks and zealots. If you get people involved in local politics, I imagine that get-out-the-vote drives become a lot easier because people are already tuned in.\n\nThe local level is more influential than people realize, I suspect. In Washington State, how environmental policies often play out is that they get implemented at either King County or City of Seattle first, then several suburban cities, and then there\u2019s a model and coalition for building a state-level policy. As former city staff, I can say it\u2019s no accident it works this way \u2014 we knew our colleagues at other cities and would learn from each other, sharing lessons and draft policy language. Regional organizations support, both partnering with cities and going down to Olympia during the legislative session to advocate for (and against) bills. We know that Republicans followed a similar model in the evil direction in red states, building up a portfolio of harmful policies to bring to the federal level in the form of Project 2025\u2026 why don\u2019t we do the same for progressive policies?\n\nIf we start thinking and working now, we can set things up for the next state legislative session, then the next federal election, then the presidential election. (I\u2019m not going down rabbitholes about martial law, I don\u2019t find it helpful to indulge in that kind of despairing fatalism \u2014 I think my personal work on managing catastrophic thinking and anxiety has paid dividends here.)\n\nI haven\u2019t decided what bigger thing I want to work on yet\u2026 for now I\u2019ve just been doing ad hoc advocacy when it\u2019s pointed out by the urbanist Discord server I\u2019m in. I\u2019m pissed that Washington had a school lunch bill that failed in the legislative session, so potentially supporting another run at that\u2026 or badgering the governor about Washington\u2019s regressive taxation policy and demanding an income tax on the wealthiest Washingtonians, which he put the kibosh on this budget cycle and was why the budget was too tight to feed kids \ud83d\ude44 Come on, Bob! We have a lot of tech companies \u2014 and tech CEOs \u2014 here who could stand to pay their fair share. Or I could go harder on supporting housing; I just looked at King County\u2019s affordable housing dashboard, which identifies a need for 44,000 affordable units by 2024, and THE ENTIRE SEATTLE AREA HAD BUILT FEWER THAN FIVE THOUSAND UNITS by 2022. *screaming*\n\nOrganizing coalitions\n\nI\u2019m also spinning on the provocation of the second method of organizing described in the Win the Midwest\u2019s 10-year report (emphasis mine):\n\nThere are two approaches to base building. We can ask what we are for, and then go out and find the people who agree with us. Or we can begin by asking who we need to organize in order to build enough power to shift everything, and then go where they are: workplaces, churches, mosques, synagogues, schools, and childcare centers. Grassroots organizations that meet people where they are \u2013 physically and ideologically \u2013 and that create spaces of belonging, learning, and formation rooted in people\u2019s lives and experiences have an unlimited number of people to organize. Most people do not have clearly defined political outlooks, but they can move into public leadership roles. Organizations willing to put their base at the center of their plans can co-create a political agenda that emerges from the lived experience of their bases.\n\nI\u2019m an extremely opinionated person, and I know what outcomes I want (though I\u2019d rather listen to others on the best tactics on achieving those outcomes), but maybe I could think of this as prioritizing which outcomes to focus on based on which ones enough other people also want and will fight for. L.A. Kauffman discusses building coalitions with those we disagree with, quoting a 1981 talk / 1983 essay \u201cCoalition Politics: Turning the Century\u201d:\n\n\u201cThe reason we are stumbling,\u201d Dr. [Bernice] Reagon declared all those years ago, \u201cis that we are at the point where in order to take the next step we\u2019ve got to do it with some folk we don\u2019t care too much about. And we got to vomit over that for a little while. We must just keep going.\u201d\n\nFurther reading:\n\nWhy Do Anything? by Dj Bracken \u2014 this guy decided to start paying down student lunch debt in Utah, and eventually helped get a statewide bill passed (I was loosely involved in supporting new \u201cshare tables\u201d / fridges at our local elementary schools, AMA)\n\nFrom Aspiration to Action: Organizing Through Exhaustion, Grief, and Uncertainty by Kelly Hayes\n\nSee also:\n\nDon\u2019t let them say it\u2019s normal\n\nExtending my understanding of self-care\n\nBeing a citizen means taking ownership", "label": 1}
{"title": "Juanita Garden Tour \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/07/juanita-garden-tour/", "content": "I had a blast visiting five gardens around the Juanita neighborhood \ud83d\ude03 There were 12 gardens on the tour but I think you\u2019d be hard-pressed to see them all! I started at 11 and was wiped by 2:30 so I didn\u2019t push myself to squeeze in one more.\n\nEnjoyed talking with several of the homeowners too \u2014 one of them has written a book on mushrooming! He had a massive table spread with morels drying in the sun.\n\n\u201cChampagne Creek\u201d\n\n\u201cThis private garden has expanded and grown into a utopia of rare plants, exotic fowl, native habitat and so much more.\u201d \u201cWhimsical eclecticism is Kathy\u2019s guiding principle.\u201d\n\n\u201cBloom\u201d\n\n\u201cThis 1/2-acre garden is an artistic attempt to paint with flowers and plants: full of color, texture and whimsy.\u201d\n\n\u201cThe Half Acre Wood\u201d\n\n\u201cEvergreen Hollow\u201d\n\n\u201cNestled in the lush climate of the PNW, Evergreen Hollow is a harmonious fusion of Japanese serenity, whimsical fairy charm, and intricate bonsai artistry.\u201d\n\n\u201cDahlia House Gardens\u201d\n\n\u201cOur gardens, at over an acre, are a controlled chaos of thousands of perennials, fruit trees, berry bushes and veggie gardens. We have collected seeds and plants from Tibet, Europe and elsewhere and enjoy a very English style profusion with lawn and intertwined stairs and hidden trails.\u201d (This garden was very much my vibe.)\n\nTakeaways\n\nIt\u2019s lovely to see stunning gardens like these, but also having paid for landscaping, I have to distinguish between what someone could accomplish with many tens of thousands of dollars of hardscaping and what could be adaptable to my yard. Some of the gardens leaned on sweat equity and cleverness, which is what my budget calls for \ud83d\ude09 Boulders are $$$ as much as I love how they look \ud83e\udd7a (Our landscape plan originally called for a fountain but the landscaper wasn\u2019t willing to wait for electricity to be trenched, since it would need an inspection, even though they saw the plans when we got the quote \ud83d\ude44 so, we have a bird bath)\n\nI think some cheapish landscaping cobblestones, metal arches, and more yard art could go a fair ways to adding finish to my garden. I also suspect another couple Japanese maples could pull things together. More paths / focal points in the backyard would be beneficial visually \u2014 we didn\u2019t bother because the freeway is so loud we just wanted it to look pretty out the window \u2014 but I\u2019m finding that there is a lack of places for the eye to land when it\u2019s all green all the time. Without a lawn, and with the path so close to the house you can\u2019t see it from inside, we have few \u201cedges\u201d.", "label": 1}
{"title": "Designing accessible color systems", "url": "https://stripe.com/blog/accessible-color-systems", "content": "Color contrast is an important aspect of accessibility. Good contrast makes it easier for people with visual impairments to use products, and helps in imperfect conditions like low-light environments or older screens. With this in mind, we recently updated the colors in our user interfaces to be more accessible. Text and icon colors now reliably have legible contrast throughout the Stripe Dashboard and all other products built with our internal interface library.\n\nAchieving the right contrast with color is challenging, especially because color is incredibly subjective and has a big effect on the aesthetics of a product. We wanted to create a color system with hand-picked, vibrant colors that also met standards for accessibility and contrast.\n\nWhen we evaluated external tools to improve color contrast and legibility in our products, we noticed two common approaches to tackling the problem:\n\nHand-pick colors and check their contrast against a standard. Our experience told us that this approach made choosing colors too dependent on trial and error. Generate lighter and darker tints from a set of base colors. Unfortunately, simply darkening or lightening can result in dull or muted colors, which can be difficult to distinguish from each other and often just don\u2019t look good.\n\nWith the existing tools we found, it was hard to create a color system that allowed us to pick great colors while ensuring accessibility. We decided to create a new tool that uses perceptual color models to give real-time feedback about accessibility. This enabled us to quickly create a color scheme that met our needs, and gave us something we could iterate on in the future.\n\nBackground\n\nThe colors we use in our product interfaces are based on our brand color palette. Using these colors in our products allows us to bring some of the character of Stripe\u2019s brand into our interfaces.\n\nUnfortunately, it was difficult to meet (and maintain) contrast guidelines with these colors. The web accessibility guidelines suggest a minimum contrast ratio of 4.5 for small text, and 3.0 for large text. When we audited color usage in our products, we discovered that none of the default text colors we were using for small text (except for black) met the contrast threshold.\n\nChoosing accessible color combinations required each individual designer or engineer to understand the guidelines and select color pairs with enough contrast in each situation. With certain combinations of colors, options were limited and the accessible color combinations just didn\u2019t look good.\n\nWhen we first looked at ways to improve text contrast in our products, we initially explored shifting the default colors for text one step darker on our scale, illustrated by the left column below.\n\nUnfortunately, some of our colors still didn\u2019t have sufficient contrast at the next darkest shade. Once we got to a shade with sufficient contrast on our existing scales (the right column), we lost a lot of the brightness and vibrancy of our colors. The colors pass guidelines on a white background, but they\u2019re dark and muddy and it\u2019s difficult to tell the hues apart.\n\nWithout digging deeper it would be easy to just accept the tradeoff that you need to choose between having accessible colors or colors that look good. In order to get both, we needed to rework our color system from the ground up.\n\nWe wanted to design a new color system that would provide three key benefits out of the box:\n\nPredictable accessibility: Colors have enough contrast to pass accessibility guidelines. Clear, vibrant hues: Users can easily distinguish colors from one another. Consistent visual weight: At each level, no single color appears to take priority over another.\n\nA brief interlude on color spaces\n\nTo explain how we got there, we need to get a little nerdy about color.\n\nWe\u2019re used to working with color on screens in terms of the RGB color space. Colors are specified in terms of how much red, green, and blue light is mixed on screen to make the color.\n\nUnfortunately, while describing colors this way comes naturally to computers, it doesn\u2019t come naturally to humans. Given an RGB color value, what needs to change to make it lighter? More colorful? Add more yellow?\n\nIt\u2019s more intuitive for us to think of colors as organized by three attributes:\n\nHue: What color is it?\n\nWhat color is it? Chroma: How colorful is it?\n\nHow colorful is it? Lightness: How bright is it?\n\nA popular color space that supports specifying colors in this way is HSL. It\u2019s well supported in design tools and popular code libraries for color manipulation. There\u2019s just one problem: the way HSL calculates lightness is flawed. What most color spaces don\u2019t take into account is that different hues are inherently perceived as different levels of lightness by the human eye\u2014at the same level of mathematical lightness, yellow appears lighter than blue.\n\nThe image below is a set of colors with the same lightness and saturation in a display color space. While the color space claims the saturation and lightness are all the same, our eyes disagree. Notice that some of these colors appear lighter or more saturated than others. For example, the blues appear especially dark and the yellows and greens appear especially light.\n\nThere are color spaces which attempt to model human perception of color. Perceptually uniform color spaces model colors based on factors that relate more to human vision, and perform sophisticated color transformations to ensure that these dimensions reflect how human vision works.\n\nWhen we take a sample of colors with the same lightness and saturation in a perceptually uniform color space, we can observe a significant difference. These colors appear to blend together, and each color appears to be just as light and as saturated as the rest. This is perceptual uniformity at work.\n\nThere are surprisingly few tools that support perceptually uniform color models, and none that came close to helping us design a color palette. So we built our own.\n\nVisualizing color\n\nWe built a web interface to allow us to visualize and manipulate our color system using perceptually uniform color models. The tool gave us an immediate feedback loop while we were iterating on our colors\u2014we could see the effect of every change.\n\nThe color space illustrated above is known as CIELAB or, affectionately, Lab. The L in Lab stands for lightness, but unlike the lightness in HSL, it\u2019s designed to be perceptually uniform. By translating our color scales into the Lab color space, we can adjust our colors based on their perceptual contrast and visually compare the results.\n\nThe diagram below shows the lightness and contrast values of our previous color palette visualized in the color tool. You can see that the perceptual lightness of each of our colors follows a different curve, with the yellow and green colors much lighter than the blues and purples at the same point.\n\nBy manipulating our colors in perceptually uniform color space, we were able to produce a set of colors which have uniform contrast across all the hues, and preserve as much of the intended hue and saturation of our current colors. In the proposed colors, yellow has the same contrast range as blue, but they still look like our colors.\n\nIn the diagram below, you can see the perceptual lightness for each color follows the same curve, meaning each color (the labels on the left) has the same contrast value at a given level (the number labels on the top).\n\nOur new tool also showed us what was possible. Visualizing a perceptually uniform color model allowed us to see the constraints of visual perception. The shaded areas in the charts represent so-called imaginary colors which aren\u2019t actually reproducible or perceivable. It turns out \u201creally dark yellow\u201d isn\u2019t actually a thing.\n\nMost tools for mixing colors allow you to set values across the full range for each parameter, and just clip the colors or return the nearest fit colors that don\u2019t actually represent the parameters you set. Visualizing the available color space in real time as we made changes allowed us to iterate much faster because we could tell what changes were possible and what changes moved us closer to our goal: \u201cbright\u201d, differentiated colors that met the appropriate contrast guidelines.\n\nAt some points, finding a set of colors that worked together was like threading a needle. Here, the shaded areas show how limited the space is to actually find a combination of values that allows for roughly equal lightness for all hues.\n\nResults\n\nAfter a lot of iterations and tests with real components and interfaces, we arrived at a palette of colors that achieved our goals: our colors predictably passed accessibility guidelines, kept their clear, vibrant hues, and maintained a consistent visual weight across hues.\n\nOur new default colors for text and icons now pass the accessibility contrast threshold defined in the WCAG 2.0 guidelines.\n\nIn addition to passing contrast guidelines over white backgrounds, each color also passes when displayed atop the lightest color value in any hue. Since we commonly use these lightly tinted backgrounds to offset or highlight sections, this makes it simple and predictable to ensure text has sufficient contrast throughout our products.\n\nBecause the new colors are uniformly organized based on contrast, we also have straightforward guidelines built-in for choosing appropriate contrast pairs in less common cases. Any two colors are guaranteed to have sufficient contrast for small text if they are at least five levels apart, and at least four levels apart for icons and large text.\n\nWith contrast guidelines built in to the system, it\u2019s simple to make adjustments for color contrast in different components with predictable results.\n\nFor example, we redesigned our Badge component to use a color background to clearly differentiate each color. At the lightest possible value, the colors were too difficult to distinguish from each other. By shifting both the background and the text color up one level, we were able to maintain text contrast across all badge colors without fine tuning each color combination individually.\n\nConclusion\n\nWe learned that designing accessible color systems doesn\u2019t have to mean fumbling around in the dark. We just needed to change how we thought about color:\n\nUse a perceptually uniform color model\n\nWhen designing an accessible color system, using a perceptually uniform color model (like CIELAB) helped us understand how each color appears to our eyes as opposed to how it appears to a computer. This allowed us to validate our intuitions and use numbers to compare the lightness and colorfulness of all of our colors.\n\nAccessible doesn\u2019t mean vibrant\n\nThe WCAG accessibility standard intentionally only focuses on the contrast between a foreground and a background color\u2014not how vibrant they appear. Understanding how vibrant each color appears can helps to distinguish hues from one another.\n\nColor is hard to reason about, tools can help\n\nOne of the pitfalls of perceptually uniform color models is that there are impossible colors\u2014there\u2019s no such thing as \u201cvery colorful dark yellow\u201d or \u201cvibrant light royal blue\u201d. Building our own tool helped us see exactly which colors were possible and allowed us to rapidly iterate on our color palette until we produced a palette that was accessible, vibrant, and still felt like Stripe.\n\nAdditional resources\n\nTo learn more about color, we recommend the following resources:", "label": 0}
{"title": "Investigation of a Workbench UI Latency Issue", "url": "https://netflixtechblog.com/investigation-of-a-workbench-ui-latency-issue-faa017b4653d?source=collection_home---4------12-----------------------", "content": "Investigation of a Workbench UI Latency Issue Netflix Technology Blog 12 min read \u00b7 Oct 14, 2024 -- 4 Listen Share\n\nBy: Hechao Li and Marcelo Mayworm\n\nWith special thanks to our stunning colleagues Amer Ather, Itay Dafna, Luca Pozzi, Matheus Le\u00e3o, and Ye Ji.\n\nOverview\n\nAt Netflix, the Analytics and Developer Experience organization, part of the Data Platform, offers a product called Workbench. Workbench is a remote development workspace based on Titus that allows data practitioners to work with big data and machine learning use cases at scale. A common use case for Workbench is running JupyterLab Notebooks.\n\nRecently, several users reported that their JupyterLab UI becomes slow and unresponsive when running certain notebooks. This document details the intriguing process of debugging this issue, all the way from the UI down to the Linux kernel.\n\nSymptom\n\nMachine Learning engineer Luca Pozzi reported to our Data Platform team that their JupyterLab UI on their workbench becomes slow and unresponsive when running some of their Notebooks. Restarting the ipykernel process, which runs the Notebook, might temporarily alleviate the problem, but the frustration persists as more notebooks are run.\n\nQuantify the Slowness\n\nWhile we observed the issue firsthand, the term \u201cUI being slow\u201d is subjective and difficult to measure. To investigate this issue, we needed a quantitative analysis of the slowness.\n\nItay Dafna devised an effective and simple method to quantify the UI slowness. Specifically, we opened a terminal via JupyterLab and held down a key (e.g., \u201cj\u201d) for 15 seconds while running the user\u2019s notebook. The input to stdin is sent to the backend (i.e., JupyterLab) via a WebSocket, and the output to stdout is sent back from the backend and displayed on the UI. We then exported the .har file recording all communications from the browser and loaded it into a Notebook for analysis.\n\nUsing this approach, we observed latencies ranging from 1 to 10 seconds, averaging 7.4 seconds.\n\nBlame The Notebook\n\nNow that we have an objective metric for the slowness, let\u2019s officially start our investigation. If you have read the symptom carefully, you must have noticed that the slowness only occurs when the user runs certain notebooks but not others.\n\nTherefore, the first step is scrutinizing the specific Notebook experiencing the issue. Why does the UI always slow down after running this particular Notebook? Naturally, you would think that there must be something wrong with the code running in it.\n\nUpon closely examining the user\u2019s Notebook, we noticed a library called pystan , which provides Python bindings to a native C++ library called stan, looked suspicious. Specifically, pystan uses asyncio. However, because there is already an existing asyncio event loop running in the Notebook process and asyncio cannot be nested by design, in order for pystan to work, the authors of pystan recommend injecting pystan into the existing event loop by using a package called nest_asyncio, a library that became unmaintained because the author unfortunately passed away.\n\nGiven this seemingly hacky usage, we naturally suspected that the events injected by pystan into the event loop were blocking the handling of the WebSocket messages used to communicate with the JupyterLab UI. This reasoning sounds very plausible. However, the user claimed that there were cases when a Notebook not using pystan runs, the UI also became slow.\n\nMoreover, after several rounds of discussion with ChatGPT, we learned more about the architecture and realized that, in theory, the usage of pystan and nest_asyncio should not cause the slowness in handling the UI WebSocket for the following reasons:\n\nEven though pystan uses nest_asyncio to inject itself into the main event loop, the Notebook runs on a child process (i.e., the ipykernel process) of the jupyter-lab server process, which means the main event loop being injected by pystan is that of the ipykernel process, not the jupyter-server process. Therefore, even if pystan blocks the event loop, it shouldn\u2019t impact the jupyter-lab main event loop that is used for UI websocket communication. See the diagram below:\n\nIn other words, pystan events are injected to the event loop B in this diagram instead of event loop A. So, it shouldn\u2019t block the UI WebSocket events.\n\nYou might also think that because event loop A handles both the WebSocket events from the UI and the ZeroMQ socket events from the ipykernel process, a high volume of ZeroMQ events generated by the notebook could block the WebSocket. However, when we captured packets on the ZeroMQ socket while reproducing the issue, we didn\u2019t observe heavy traffic on this socket that could cause such blocking.\n\nA stronger piece of evidence to rule out pystan was that we were ultimately able to reproduce the issue even without it, which I\u2019ll dive into later.\n\nBlame Noisy Neighbors\n\nThe Workbench instance runs as a Titus container. To efficiently utilize our compute resources, Titus employs a CPU oversubscription feature, meaning the combined virtual CPUs allocated to containers exceed the number of available physical CPUs on a Titus agent. If a container is unfortunate enough to be scheduled alongside other \u201cnoisy\u201d containers \u2014 those that consume a lot of CPU resources \u2014 it could suffer from CPU deficiency.\n\nHowever, after examining the CPU utilization of neighboring containers on the same Titus agent as the Workbench instance, as well as the overall CPU utilization of the Titus agent, we quickly ruled out this hypothesis. Using the top command on the Workbench, we observed that when running the Notebook, the Workbench instance uses only 4 out of the 64 CPUs allocated to it. Simply put, this workload is not CPU-bound.\n\nBlame The Network\n\nThe next theory was that the network between the web browser UI (on the laptop) and the JupyterLab server was slow. To investigate, we captured all the packets between the laptop and the server while running the Notebook and continuously pressing \u2018j\u2019 in the terminal.\n\nWhen the UI experienced delays, we observed a 5-second pause in packet transmission from server port 8888 to the laptop. Meanwhile, traffic from other ports, such as port 22 for SSH, remained unaffected. This led us to conclude that the pause was caused by the application running on port 8888 (i.e., the JupyterLab process) rather than the network.\n\nThe Minimal Reproduction\n\nAs previously mentioned, another strong piece of evidence proving the innocence of pystan was that we could reproduce the issue without it. By gradually stripping down the \u201cbad\u201d Notebook, we eventually arrived at a minimal snippet of code that reproduces the issue without any third-party dependencies or complex logic:\n\nimport time\n\nimport os\n\nfrom multiprocessing import Process\n\n\n\nN = os.cpu_count()\n\n\n\ndef launch_worker(worker_id):\n\ntime.sleep(60)\n\n\n\nif __name__ == '__main__':\n\nwith open('/root/2GB_file', 'r') as file:\n\ndata = file.read()\n\nprocesses = []\n\nfor i in range(N):\n\np = Process(target=launch_worker, args=(i,))\n\nprocesses.append(p)\n\np.start()\n\n\n\nfor p in processes:\n\np.join()\n\nThe code does only two things:\n\nRead a 2GB file into memory (the Workbench instance has 480G memory in total so this memory usage is almost negligible). Start N processes where N is the number of CPUs. The N processes do nothing but sleep.\n\nThere is no doubt that this is the most silly piece of code I\u2019ve ever written. It is neither CPU bound nor memory bound. Yet it can cause the JupyterLab UI to stall for as many as 10 seconds!\n\nQuestions\n\nThere are a couple of interesting observations that raise several questions:\n\nWe noticed that both steps are required in order to reproduce the issue . If you don\u2019t read the 2GB file (that is not even used!), the issue is not reproducible. Why using 2GB out of 480GB memory could impact the performance?\n\n. If you don\u2019t read the 2GB file (that is not even used!), the issue is not reproducible. When the UI delay occurs, the jupyter-lab process CPU utilization spikes to 100% , hinting at contention on the single-threaded event loop in this process (event loop A in the diagram before). What does the jupyter-lab process need the CPU for, given that it is not the process that runs the Notebook?\n\n, hinting at contention on the single-threaded event loop in this process (event loop A in the diagram before). The code runs in a Notebook, which means it runs in the ipykernel process, that is a child process of the jupyter-lab process. How can anything that happens in a child process cause the parent process to have CPU contention?\n\nThe workbench has 64CPUs. But when we printed os.cpu_count(), the output was 96. That means the code starts more processes than the number of CPUs. Why is that?\n\nLet\u2019s answer the last question first. In fact, if you run lscpu and nproc commands inside a Titus container, you will also see different results \u2014 the former gives you 96, which is the number of physical CPUs on the Titus agent, whereas the latter gives you 64, which is the number of virtual CPUs allocated to the container. This discrepancy is due to the lack of a \u201cCPU namespace\u201d in the Linux kernel, causing the number of physical CPUs to be leaked to the container when calling certain functions to get the CPU count. The assumption here is that Python os.cpu_count() uses the same function as the lscpu command, causing it to get the CPU count of the host instead of the container. Python 3.13 has a new call that can be used to get the accurate CPU count, but it\u2019s not GA\u2019ed yet.\n\nIt will be proven later that this inaccurate number of CPUs can be a contributing factor to the slowness.\n\nMore Clues\n\nNext, we used py-spy to do a profiling of the jupyter-lab process. Note that we profiled the parent jupyter-lab process, not the ipykernel child process that runs the reproduction code. The profiling result is as follows:\n\nAs one can see, a lot of CPU time (89%!!) is spent on a function called __parse_smaps_rollup. In comparison, the terminal handler used only 0.47% CPU time. From the stack trace, we see that this function is inside the event loop A, so it can definitely cause the UI WebSocket events to be delayed.\n\nThe stack trace also shows that this function is ultimately called by a function used by a Jupyter lab extension called jupyter_resource_usage. We then disabled this extension and restarted the jupyter-lab process. As you may have guessed, we could no longer reproduce the slowness!\n\nBut our puzzle is not solved yet. Why does this extension cause the UI to slow down? Let\u2019s keep digging.\n\nRoot Cause Analysis\n\nFrom the name of the extension and the names of the other functions it calls, we can infer that this extension is used to get resources such as CPU and memory usage information. Examining the code, we see that this function call stack is triggered when an API endpoint /metrics/v1 is called from the UI. The UI apparently calls this function periodically, according to the network traffic tab in Chrome\u2019s Developer Tools.\n\nNow let\u2019s look at the implementation starting from the call get(jupter_resource_usage/api.py:42) . The full code is here and the key lines are shown below:\n\ncur_process = psutil.Process()\n\nall_processes = [cur_process] + cur_process.children(recursive=True)\n\n\n\nfor p in all_processes:\n\ninfo = p.memory_full_info()\n\nBasically, it gets all children processes of the jupyter-lab process recursively, including both the ipykernel Notebook process and all processes created by the Notebook. Obviously, the cost of this function is linear to the number of all children processes. In the reproduction code, we create 96 processes. So here we will have at least 96 (sleep processes) + 1 (ipykernel process) + 1 (jupyter-lab process) = 98 processes when it should actually be 64 (allocated CPUs) + 1 (ipykernel process) + 1 (jupyter-lab process) = 66 processes, because the number of CPUs allocated to the container is, in fact, 64.\n\nThis is truly ironic. The more CPUs we have, the slower we are!\n\nAt this point, we have answered one question: Why does starting many grandchildren processes in the child process cause the parent process to be slow? Because the parent process runs a function that\u2019s linear to the number all children process recursively.\n\nHowever, this solves only half of the puzzle. If you remember the previous analysis, starting many child processes ALONE doesn\u2019t reproduce the issue. If we don\u2019t read the 2GB file, even if we create 2x more processes, we can\u2019t reproduce the slowness.\n\nSo now we must answer the next question: Why does reading a 2GB file in the child process affect the parent process performance, especially when the workbench has as much as 480GB memory in total?\n\nTo answer this question, let\u2019s look closely at the function __parse_smaps_rollup. As the name implies, this function parses the file /proc/<pid>/smaps_rollup.\n\ndef _parse_smaps_rollup(self):\n\nuss = pss = swap = 0\n\nwith open_binary(\"{}/{}/smaps_rollup\".format(self._procfs_path, self.pid)) as f:\n\nfor line in f:\n\nif line.startswith(b\u201dPrivate_\u201d):\n\n# Private_Clean, Private_Dirty, Private_Hugetlb\n\ns uss += int(line.split()[1]) * 1024\n\nelif line.startswith(b\u201dPss:\u201d):\n\npss = int(line.split()[1]) * 1024\n\nelif line.startswith(b\u201dSwap:\u201d):\n\nswap = int(line.split()[1]) * 1024\n\nreturn (uss, pss, swap)\n\nNaturally, you might think that when memory usage increases, this file becomes larger in size, causing the function to take longer to parse. Unfortunately, this is not the answer because:\n\nFirst, the number of lines in this file is constant for all processes .\n\n. Second, this is a special file in the /proc filesystem, which should be seen as a kernel interface instead of a regular file on disk. In other words, I/O operations of this file are handled by the kernel rather than disk.\n\nThis file was introduced in this commit in 2017, with the purpose of improving the performance of user programs that determine aggregate memory statistics. Let\u2019s first focus on the handler of open syscall on this /proc/<pid>/smaps_rollup.\n\nFollowing through the single_open function, we will find that it uses the function show_smaps_rollup for the show operation, which can translate to the read system call on the file. Next, we look at the show_smaps_rollup implementation. You will notice a do-while loop that is linear to the virtual memory area.\n\nstatic int show_smaps_rollup(struct seq_file *m, void *v) {\n\n\u2026\n\nvma_start = vma->vm_start;\n\ndo {\n\nsmap_gather_stats(vma, &mss, 0);\n\nlast_vma_end = vma->vm_end;\n\n\u2026\n\n} for_each_vma(vmi, vma);\n\n\u2026\n\n}\n\nThis perfectly explains why the function gets slower when a 2GB file is read into memory. Because the handler of reading the smaps_rollup file now takes longer to run the while loop. Basically, even though smaps_rollup already improved the performance of getting memory information compared to the old method of parsing the /proc/<pid>/smaps file, it is still linear to the virtual memory used.\n\nMore Quantitative Analysis\n\nEven though at this point the puzzle is solved, let\u2019s conduct a more quantitative analysis. How much is the time difference when reading the smaps_rollup file with small versus large virtual memory utilization? Let\u2019s write some simple benchmark code like below:\n\nimport os\n\n\n\ndef read_smaps_rollup(pid):\n\nwith open(\"/proc/{}/smaps_rollup\".format(pid), \"rb\") as f:\n\nfor line in f:\n\npass\n\n\n\nif __name__ == \u201c__main__\u201d:\n\npid = os.getpid()\n\n\n\nread_smaps_rollup(pid)\n\n\n\nwith open(\u201c/root/2G_file\u201d, \u201crb\u201d) as f:\n\ndata = f.read()\n\n\n\nread_smaps_rollup(pid)\n\nThis program performs the following steps:\n\nReads the smaps_rollup file of the current process. Reads a 2GB file into memory. Repeats step 1.\n\nWe then use strace to find the accurate time of reading the smaps_rollup file.\n\n$ sudo strace -T -e trace=openat,read python3 benchmark.py 2>&1 | grep \u201csmaps_rollup\u201d -A 1\n\n\n\nopenat(AT_FDCWD, \u201c/proc/3107492/smaps_rollup\u201d, O_RDONLY|O_CLOEXEC) = 3 <0.000023>\n\nread(3, \u201c560b42ed4000\u20137ffdadcef000 \u2014 -p 0\u201d\u2026, 1024) = 670 <0.000259>\n\n...\n\nopenat(AT_FDCWD, \u201c/proc/3107492/smaps_rollup\u201d, O_RDONLY|O_CLOEXEC) = 3 <0.000029>\n\nread(3, \u201c560b42ed4000\u20137ffdadcef000 \u2014 -p 0\u201d\u2026, 1024) = 670 <0.027698>\n\nAs you can see, both times, the read syscall returned 670, meaning the file size remained the same at 670 bytes. However, the time it took the second time (i.e., 0.027698 seconds) is 100x the time it took the first time (i.e., 0.000259 seconds)! This means that if there are 98 processes, the time spent on reading this file alone will be 98 * 0.027698 = 2.7 seconds! Such a delay can significantly affect the UI experience.\n\nSolution\n\nThis extension is used to display the CPU and memory usage of the notebook process on the bar at the bottom of the Notebook:\n\nWe confirmed with the user that disabling the jupyter-resource-usage extension meets their requirements for UI responsiveness, and that this extension is not critical to their use case. Therefore, we provided a way for them to disable the extension.\n\nSummary\n\nThis was such a challenging issue that required debugging from the UI all the way down to the Linux kernel. It is fascinating that the problem is linear to both the number of CPUs and the virtual memory size \u2014 two dimensions that are generally viewed separately.\n\nOverall, we hope you enjoyed the irony of:\n\nThe extension used to monitor CPU usage causing CPU contention. An interesting case where the more CPUs you have, the slower you get!\n\nIf you\u2019re excited by tackling such technical challenges and have the opportunity to solve complex technical challenges and drive innovation, consider joining our Data Platform teams. Be part of shaping the future of Data Security and Infrastructure, Data Developer Experience, Analytics Infrastructure and Enablement, and more. Explore the impact you can make with us!", "label": 0}
{"title": "Introducing AutoPatchBench: A Benchmark for AI-Powered Security Fixes", "url": "https://engineering.fb.com/2025/04/29/ai-research/autopatchbench-benchmark-ai-powered-security-fixes/", "content": "We are introducing AutoPatchBench, a benchmark for the automated repair of vulnerabilities identified through fuzzing.\n\nBy providing a standardized benchmark, AutoPatchBench enables researchers and practitioners to objectively evaluate and compare the effectiveness of various AI program repair systems.\n\nThis initiative facilitates the development of more robust security solutions, and also encourages collaboration within the community to address the critical challenge of software vulnerability repair.\n\nAutoPatchBench is available now on GitHub.\n\nAI is increasingly being applied to solve security challenges, including repairing vulnerabilities identified through fuzzing. However, the lack of a standardized benchmark for objectively assessing AI-driven bug repair agents specific to fuzzing has impeded progress in academia and the broader community. Today, we are publicly releasing AutoPatchBench, a benchmark designed to evaluate AI program repair systems. AutoPatchBench sits within CyberSecEval 4, Meta\u2019s new benchmark suite for evaluating AI capabilities to support defensive use cases. It features 136 fuzzing-identified C/C++ vulnerabilities in real-world code repos along with verified fixes sourced from the ARVO dataset.\n\nAutoPatchBench provides a standardized evaluation framework for assessing the effectiveness of AI-assisted vulnerability repair tools. This benchmark aims to facilitate a comprehensive understanding of the capabilities and limitations of various AI-driven approaches to repairing fuzzing-found bugs. By offering a consistent set of evaluation criteria, AutoPatchBench fosters transparency and reproducibility in research, enabling both academic and industry professionals to identify best practices and areas for improvement.\n\nFixing fuzzing-found vulnerabilities with AI\n\nFuzzing is a cornerstone in automated testing, renowned for its effectiveness in uncovering security vulnerabilities. By bombarding a target program with vast amounts of pseudo-random input data, fuzz testing exposes critical security and reliability issues, such as memory corruption, invalid pointer dereference, integer overflow, and parsing errors.\n\nHowever, resolving a fuzzing crash is often a labor intensive task, demanding intricate debugging and thorough code review to pinpoint and rectify the underlying cause. This process can be both time-consuming and resource-intensive. Unlike regular test failures, fuzzing bugs frequently reveal security vulnerabilities that pose severe threats to system integrity and user data. Given these stakes, automating the repair of fuzzing bugs with AI becomes not just advantageous but essential. AI\u2019s ability to swiftly analyze patterns and propose solutions significantly reduces the time and effort required for repairs, making it an invaluable ally in safeguarding our digital environments.\n\nLet\u2019s explore the process of addressing bugs identified through fuzzing by examining a demonstrative example. Consider the following C function, which harbors a read/write buffer overflow vulnerability:\n\n#include <stdio.h> #include <string.h> void process_input(const char *input) { char buffer[8]; strcpy(buffer, input); // Potential buffer overflow printf(\"Processed: %s\n\n\", buffer); }\n\nIn this scenario, a fuzzing harness might supply an input that surpasses the buffer\u2019s capacity, leading to a crash due to buffer overflow. A typical stack trace from such a crash might appear as follows:\n\n== Fuzzer Crash Report == Program received signal SIGSEGV, Segmentation fault. 0x00007ffff7af1223 in strcpy () from /lib/x86_64-linux-gnu/libc.so.6 (gdb) bt #0 0x00007ffff7af1223 in strcpy () #1 0x0000555555555140 in process_input (input=0x7fffffffe695 \"AAAAAA...\") #2 0x0000555555555162 in main (argc=2, argv=0x7fffffffe5f8)\n\nHere, the process_input function invokes strcpy on a string that exceeds the eight-character buffer, causing a segmentation fault. A straightforward patch involves ensuring the copy operation remains within the buffer\u2019s limits. This can be achieved by using a bounded copy function like strncpy or implementing a length check before copying:\n\nvoid process_input(const char *input) { char buffer[8]; strncpy(buffer, input, sizeof(buffer) - 1); buffer[sizeof(buffer) - 1] = '\\0'; printf(\"Processed: %s\n\n\", buffer); }\n\nThis patch ensures that the string remains within the buffer\u2019s limits, effectively preventing out-of-bounds writes. Its correctness can be confirmed by verifying that the fuzzing input, which previously caused the crash, no longer does so. Additional checks can be conducted to ensure the patch doesn\u2019t introduce any unintended side effects.\n\nAs illustrated, fixing a fuzzing crash involves:\n\nAnalyzing the crash stack trace and the target code. Pinpointing the root cause. Patching the vulnerable code. Verifying the fix\u2019s accuracy.\n\nAn AI-based solution can automate these steps by utilizing an LLM\u2019s capability to understand and generate code.\n\nWhy we developed AutoPatchBench\n\nAutoPatchBench is informed by key advancements in the field of AI-driven program repair, particularly those focusing on fuzzing-found vulnerabilities. Among the notable contributions is Google\u2019s tech report on AI-powered patching, which pioneered the use of LLMs for addressing fuzzing crashes, achieving a 15% fix rate with their proprietary dataset. Subsequently, Google\u2019s study on generic program repair agents introduced the GITS-Eval benchmark, encompassing 178 bugs across various programming languages.\n\nIn the realm of AI software engineering agents, benchmarks like SWE-Bench and SWE-Bench Verified have gained widespread acceptance for evaluating generic AI SWE agents. However, these benchmarks do not specifically tackle the unique challenges posed by fuzzing-found vulnerabilities, which demand specialized approaches that utilize fuzzing-specific artifacts and address security concerns.\n\nAutoPatchBench addresses this gap by offering a dedicated benchmark focused on a wide variety of C/C++ vulnerabilities of 11 crash types identified through fuzzing with automated verification capability. Unlike the broader focus of GITS-Eval and SWE-Bench, AutoPatchBench is specifically designed to assess the effectiveness of AI-driven tools in repairing security-critical bugs typically uncovered by fuzzing. This targeted approach enables a more precise evaluation of AI capabilities in meeting the complex requirements of fuzzing-found vulnerabilities, thereby advancing the field of AI-assisted program repair in a focused manner.\n\nInside AutoPatchBench\n\nWe\u2019re making AutoPatchBench publicly available as part of CyberSecEval 4 to encourage community collaboration in tackling the challenge of automating fuzzing crash repairs. This benchmark is specifically designed for AI program repair agents focusing on C/C++ bugs identified through fuzzing. It includes real-world C/C++ vulnerabilities with verified fixes sourced from the ARVO dataset, and incorporates additional verification of AI-generated patches through fuzzing and white-box differential testing.\n\nARVO dataset\n\nThe ARVO dataset serves as the foundation for AutoPatchBench, offering a comprehensive collection of real-world vulnerabilities that are essential for advancing AI-driven security research. Sourced from C/C++ projects identified by Google\u2019s OSS-Fuzz, ARVO includes over 5,000 reproducible vulnerabilities across more than 250 projects. Each entry is meticulously documented with a triggering input, a canonical developer-written patch, and the capability to rebuild the project in both its vulnerable and patched states.\n\nHowever, there are notable challenges when using the ARVO dataset as a benchmark for AI patch generation:\n\nWhile reproducibility is vital for a reliable benchmark, the ARVO dataset includes samples where crashes are not consistently reproducible. Some samples lack crash stack traces, making it exceedingly difficult to address the crash. Although ARVO provides a ground-truth fix for each identified vulnerability, it lacks an automated mechanism to verify the correctness of a generated patch. Objective automated verification is essential for a benchmark focused on patch generation.\n\nAutoPatchBench addresses these challenges by creating a curated subset and by employing a comprehensive and automated verification process.\n\nSelection criteria\n\nTo ensure the reliability and effectiveness of AutoPatchBench, we meticulously filtered the ARVO dataset samples based on the following criteria:\n\nValid C/C++ vulnerability: The ground-truth fix shall edit one or more C/C++ source files that are not fuzzing harnesses.\n\nDual-container setup : Each vulnerability is accompanied by two containers\u2014one that contains vulnerable code and another for the fixed code\u2014that build without error.\n\nReproducibility : The crash must be consistently reproducible within the vulnerable container.\n\nValid stack trace : A valid stack trace must be present within the vulnerable container to facilitate accurate diagnosis and repair.\n\nSuccessful compilation : The vulnerable code must compile successfully within its designated container, ensuring that the environment is correctly set up for testing.\n\nFixed code verification : The fixed code must also compile successfully within its respective container, confirming that the patch does not introduce new build issues.\n\nCrash resolution : The crash must be verified as resolved within the fixed container, demonstrating the effectiveness of the patch.\n\nFuzzing pass : The fixed code must pass a comprehensive fuzzing test without finding new crashes, ensuring that the ground-truth patch maintains the integrity and functionality of the software.\n\nAfter applying these rigorous selection criteria, we retained 136 samples for AutoPatchBench that fulfill the necessary conditions for both patch generation and verification. From this refined set, we created a down-sampled subset of 113 AutoPatchBench-Lite samples to provide a focused benchmark for testing AI patch generation tools. These subsets preserves the diversity and complexity of real-world vulnerabilities including 11 distinct crash types, offering a solid foundation for advancing AI-driven security solutions.\n\nPatch verification\n\nIn the process of patch generation, the patch generator utilizes two automated methods to verify the viability of a generated patch before submitting it for evaluation. The first method involves attempting to build the patched program, which checks for syntactic correctness. The second method involves attempting to reproduce the crash by running the input that initially triggered it. If the crash no longer occurs, it suggests that the issue has been resolved. However, these steps alone are insufficient to guarantee the correctness of the patch, as a patch might not maintain the program\u2019s intended functionality, rendering it incorrect despite resolving the crash.\n\nTo address this issue, AutoPatchBench adopts a comprehensive approach to automate the evaluation of generated patches. This involves subjecting the patched code to further fuzz testing using the original fuzzing harness that initially detected the crash. Additionally, white-box differential testing compares the runtime behavior of the patched program against the ground truth repaired program, confirming that the patch has effectively resolved the underlying bug without altering the program\u2019s intended functionality. Since a patch can potentially be made in multiple places, we cannot assume that the LLM will patch the same function as the groundtruth patch does. Instead we find all the callstacks for each call to a patched function. Then we find the lowest common ancestor (LCA) across all pairs of stacktraces offered by the groundtruth patch and the LLM patch. We then utilize debug information to inspect arguments, return values, and local variables at the first function above the LCA, differential testing offers a detailed view of the patch\u2019s impact on the program state.\n\nThis process evaluates whether the generated patch produces a program state identical to the ground truth program after the patched function returns. By using a diverse set of inputs obtained from fuzzing, this gives higher confidence that the bug is fixed without changing the visible behavior of the patched functions. This differential testing is implemented using a Python script that leverages LLDB APIs to dump all visible states and identify differences between the ground truth and the patched program.\n\nHowever, as with all attempts to solve provably undecidable problems (in this case: program equivalence), there are some failure modes for this verification step. For example, sometimes the analysis fails with timeouts, in which case we consider the semantics to be preserved if both the ground truth and the LLM patch timed out. Programs might also behave non-deterministically, and we run each input three times to identify nondeterministic struct fields and values. Such fields will not be compared to avoid false alarms from noisy, random values. Additionally, we strip any fields that contain the substring \u201cbuild\u201d or \u201ctime\u201d as we\u2019ve observed false positives from build-ids (that happen to be deterministic within a program, but not across different patches).\n\nIt should also be noted that on a number of examples, the crashing PoC never actually triggered the breakpoints on the ground truth patch, making comparison of the resulting states impossible. However, our case study showed that white-box differential testing is still effective in filtering out a majority of incorrect patches despite its limitation, which will be discussed in the case study.\n\nAutoPatchBench and AutoPatchBench-Lite\n\nAutoPatchBench is a comprehensive benchmark dataset of 136 samples. It encompasses a wide range of real-world vulnerabilities, providing a robust framework for assessing the capabilities of automated patch generation systems.\n\nWithin this benchmark, we have also created a subset called AutoPatchBench-Lite that consists of 113 samples. AutoPatchBench-Lite focuses on a simpler subset of vulnerabilities where the root cause of the crash is confined to a single function. This version is designed to cater to scenarios where the complexity of the bug is relatively low, making it more accessible for tools that are in the early stages of development or for those that specialize in handling straightforward issues.\n\nThe rationale for creating AutoPatchBench-Lite stems from the observation that when root causes are distributed across multiple locations within the code, the difficulty of generating a correct patch increases significantly. Addressing such \u201chard\u201d crashes requires a tool to possess advanced reasoning capabilities to analyze larger codebases and apply patches to multiple areas simultaneously. This complexity not only challenges the tool\u2019s design but also demands a higher level of sophistication in its algorithms to ensure accurate and effective patching.\n\nBy offering both AutoPatchBench and AutoPatchBench-Lite, we provide a tiered approach to benchmarking, allowing developers to progressively test and refine their tools. This structure supports the development of more advanced solutions capable of tackling both simple and complex vulnerabilities, ultimately contributing to the enhancement of AI-assisted bug repair techniques.\n\nExpected use cases\n\nAutoPatchBench offers significant value to a diverse range of users. Developers of auto-patch tools can leverage our open-sourced patch generator to enhance their tools and assess their effectiveness using the benchmark. Software projects employing fuzzing can incorporate our open-sourced patch generator to streamline vulnerability repair. Additionally, model developers can integrate the benchmark into their development cycles to build more robust and specialized expert models for bug repair. The tooling around the patch generator provided here can also be used in reinforcement learning as a reward signal during training. This data helps train models to better understand the nuances of bug repair, enabling them to learn from past fixes and improve their ability to generate accurate patches.\n\nReference implementation\n\nWe developed a basic patch generator to establish a baseline performance using AutoPatchBench. This generator is specifically designed to address simple crashes that involve patching a single function. We have open-sourced this reference implementation to encourage the community to build and expand upon it.\n\nFigure 1 shows a high-level overview of its design. The patch generator takes crash stack trace and target source code as input. It identifies the source locations from the stack trace and extracts every function that contains those locations. It then asks the LLM to identify the root cause and repair the crash by patching one of the functions. Upon receiving a response from the LLM, the patch generator extracts the revised code, applies the patch, compiles the program, and tests it against the original input that caused the crash. If the build or test fails, we re-engage the LLM with the error message from the build or test output, requesting it to attempt a solution again until the crash is resolved. If a fix trajectory fails to reach a valid solution that passes build and crash reproduction within a finite number of steps, we start a new trajectory to reset the context window, preventing prolonged entrapment in an incorrect path.\n\nThe example prompt below also captures the essence of our approach. Note that the actual prompt is more detailed and divided into multiple segments to promote chain-of-thought reasoning.\n\nAs an experienced Security Engineer at Meta, your task is to address the following security-critical fuzzing crash. Below is the stack trace of the crash: == Fuzzer Crash Report == Program received signal SIGSEGV, Segmentation fault. 0x00007ffff7af1223 in strcpy () from /lib/x86_64-linux-gnu/libc.so.6 (gdb) bt #0 0x00007ffff7af1223 in strcpy () #1 0x0000555555555140 in process_input (input=0x7fffffffe695 \"AAAAAA...\") #2 0x0000555555555162 in main (argc=2, argv=0x7fffffffe5f8) Here is the source code for the functions involved in the stack trace: strcpy() {...} void process_input(const char *input) { char buffer[8]; strcpy(buffer, input); // Potential buffer overflow printf(\"Processed: %s\n\n\", buffer); } int main() {...} Assuming the root cause of the crash is within one of these functions, generate a patched version of the faulty function to resolve the fuzzing crash. Ensure that you provide a complete rewrite of the function so that the patch can be applied and the code compiled without errors.\n\nA case study with AutoPatchBench-Lite\n\nIn the case study, we demonstrate the use of AutoPatchBench by evaluating our reference patch generator with several LLM models. Given that our reference implementation is limited to addressing simple issues, we conducted our evaluation with AutoPatchBench-Lite, which contains 113 samples. To prevent fix trajectories from becoming excessively prolonged, we capped the maximum length of each trajectory at five. Additionally, we set the maximum number of retries to 10.\n\nPlease note that the case study is not intended to provide a statistically rigorous comparison of model performance. Instead, it aims to present preliminary results to establish a baseline expectation. We encourage future research to build upon these findings.\n\nEffectiveness of patch generation and verification\n\nWe evaluated the effectiveness of the patch generator and our automated verification processes while using different LLM models as back-end. The figure below illustrates the effectiveness of patch generation and verification by presenting the percentage of samples that successfully passed each sequential verification step: (1) patch validity: build and crash reproducibility check, (2) fuzzing pass: passes 10-minute fuzzing, and (3) testing pass: passes white-box differential testing. It is important to note that the patch generation process only utilizes step (1) to verify the build and crash reproducibility. The fuzzing and differential testing are conducted post-generation to assess correctness.\n\nFigure 2 shows that all models achieved similar generation success rates of around 60% and similar post-verification success rates of around 5-11% with overlapping confidence intervals, and therefore, we do not draw any conclusion about their relative performance. The graph does, however, reveal that a substantial portion of the generated patches are found to be incorrect when subjected to fuzzing and white-box differential testing. For instance, Gemini 1.5 Pro achieved a 61.1% patch generation success rate, yet fewer than 15% of these patches (5.3% out of total set) were found to be correct. This gap highlights that build and crash reproduction are not good enough signals to infer the correctness of generated patches, and that future patch generation approaches should scrutinize the semantic preservation of generated patches more thoroughly. This gap also underscores the vital role of the comprehensive verification processes that checks semantic equivalence, a distinctive contribution of AutoPatchBench.\n\nEffect of inference-time computation\n\nTo assess the impact of inference-time computation on improving the patch generation success rate, we present the distribution of retry counts among the 73 patches produced by Llama 4 Maverick.\n\n\n\nFigure 3 shows that 44 out of 73 patches, or 60.2%, were successfully generated on the first attempt. The remaining 40% of the samples required more than two iterations, with no evident plateau until the 10th iteration. This outcome demonstrates that allocating more computational resources during inference-time leads to a higher success rate and suggests that increasing the number of retries could yield better results.\n\nManual validation\n\nIn our investigation of the precision and recall of white-box differential testing, we conducted a manual validation of 44 patches that passed 10-minute fuzzing against human-written ground truth fixes with the help of security experts. These patches were selected from a pool of 73 generated by Llama 4 Maverick. The following table shows the confusion matrix.\n\nTable 1: Confusion matrix between human judgement and differential testing\n\nTest pass Test fail Sum Human pass 5 0 5 Human reject 7 32 39 Sum 12 32 44\n\nThe results showed that the differential testing achieved an accuracy of 84.1% for this sample (5 + 32 / 44), indicating a high overall agreement with the human assessment. However, a closer examination of the confusion matrix revealed a notable discrepancy between precision and recall. Specifically, the testing method demonstrated 100.0% recall in this case study, correctly identifying all 5 instances that humans judged as correct. In contrast, precision was relatively low (41.7%), with 7 false positives out of 12 total positive predictions. This suggests that differential testing reported success on some incorrect patches as well, highlighting the need for manual validation of patch correctness. Despite this shortcoming, the result clearly shows the utility of differential testing in automatically rejecting a substantial number of incorrect patches, which will substantially save the manual validation effort.\n\nKey insights\n\nOur case study revealed several limitations of the current patch generator.\n\nThe root cause may not exist in the stack trace\n\nFrequently, crashes are the result of state contamination that occurs prior to the crash being triggered. Consequently, none of the functions within the stack frames may include the code responsible for the root cause. Since our current implementation requires the LLM to assume that the root cause is located within one of the functions in the stack trace, it is unable to generate an accurate patch in such cases. Solving this problem would require a more autonomous agent which can reason about the root cause on its own with a code browsing capability.\n\nCheating\n\nIn some instances, the LLM resorted to \u201ccheating\u201d by producing patches that superficially resolved the issue without addressing the underlying problem. This can occur when the generator modifies or removes code in a way that prevents the crash from occurring, but does not actually fix the root cause of the issue. We observed that cheating happens more frequently when we request the LLM to retry within the same trajectory. A potential solution to this could be to empower the LLM to say \u201cI cannot fix it,\u201d which may come with a tradeoff with success rate. However, note that most of the cheating was caught in the verification step, highlighting the utility of differential testing.\n\nNeed for enhanced patch verification methods\n\nFuzzing and white-box differential testing have shown that a large majority of generated patches are incorrect when compared to the ground-truth patches. This finding highlights the challenge of generating accurate patches without enhanced verification capabilities. To address this gap, several approaches can be considered:\n\nA patch generator could provide additional code context when querying the LLM for a patch so that LLM can better understand the consequence of a code patch.\n\nA patch generator could make additional LLM queries to verify the perseverance of existing functionality.\n\nA patch generator can attempt to generate multiple valid patches by exploring multiple trajectories in parallel, and let LLM choose the best option that is most likely to be correct.\n\nIn a well-tested real-world codebase, a patch generator can utilize existing tests to validate the patches it creates. This process complements building the code and checking for crash reproduction, allowing the patch generator to retry if a patch fails the tests. The accuracy of the generated patches is largely dependent on the thoroughness of the existing tests.\n\nIn conclusion, while our study has identified several challenges with the current patch generation process, it also opens up opportunities for improvement. By addressing these limitations with innovative solutions, we can enhance the accuracy and reliability of patch generation, paving the way for more robust and effective automated tools.\n\nGet started with AutoPatchBench\n\nAutoPatchBench is now available on GitHub. We welcome pull requests to integrate new/additional agent architectures into the framework, and look forward to seeing how well they perform on AutoPatchBench.", "label": 0}
{"title": "September 2014", "url": "https://lifeofpablo.com/blog/published:2014-09", "content": "en\n\n\"Hey guys! Pablo here!\n\n[caption id=\"\"attachment_311\"\" align=\"\"alignnone\"\" width=\"\"240\"\"] Work out #selfie[/caption]\n\nWondering if I was still alive or not? Well here I am. I made it through the first week of college. Since the minute I stepped foot outside my car I knew I was going to enjoy it. Most of it, minus the move in day process.\n\nI got to see my good friend Lucas, who just came back from Brazil to come study here at UNK. Man I missed this guy. He is my roommate that I will be staying with all year long. I just hope we don't kill each by then. So far we are getting along great.\n\n.\n\nLater that morning I got to meet my suite mates: Thomas and Riley. These two are from around North Platte, NE. Who knew we would all be alike? We are all like brothers from another mother. They've made the week great.I wish I would have met them earlier!\n\n[caption id=\"\"attachment_307\"\" align=\"\"alignnone\"\" width=\"\"300\"\"] Pabs and Tom (right)[/caption]\n\nSince we are now bros, we have done many fun activities. Activities like working out or even just walking around (towards the cafeteria of course :) )\n\nThe first weekend of college was great. There were a lot of fun activities. There was a lot of free food and free stuff!! I got a free stylus for my touch screen computer. That was the highlight of my free stuff shopping spree. I really wanted the free iPad. Oh Well.\n\nWho doesn't like free stuff?? I know I do!\n\nI have made many friends so far. I am glad to see many of my friends that I went to school or that I knew especially from the World Leaders Camp.\n\nPeople on my floor are amazing. I've met people from Italy to Venezuela. I enjoy being on such a diverse campus where people immerse you in their culture. This will really want me to study abroad.\n\nThere have been some funny moments on my floor. Like this one kid who danced to anaconda\n\nIt has been such a great week here at UNK. I felt welcome and lucky to be at such a great school. I hope you all enjoyed this post. I hope to hear from many of you guys.\n\nGO LOPERS!!!\n\nHere are some more pictures.!\n\nMAN I <3 COLLEGE!!\n\n\"", "label": 1}
{"title": "Neat Websites \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/blogroll/neat-websites/", "content": "Jump to: Neat projects | Webcomics | Plants & nature | Food | Collections | Math & science | Places | Tools | Seattle\n\n\ud83c\udd95 added April 2025\n\nMore collections: blogroll | interesting people | cool artists | graphic design resources | indie shops | wishlist | big questions\n\nNeat projects\n\nIt\u2019s Post Day! (Sarah Avenir) \u2014 email art project\n\nHow Not to Make a Book (Robin Rendle) \u2014 documenting the process of creating a book about typography\n\nWerner\u2019s Nomenclature of Colors (Nicholas Rougeux) \u2014 A recreation of the original 1821 color guidebook with new cross references, photographic examples, and posters\n\n\n\nScreens, research and hypertext \u2014 hypertext book about hypertext \u2014 love that meta\n\nbrr.fyi \u2014 blog by an anonymous IT worker who overwintered in Antarctica\n\nWA 100 Peaks \u2014 photographer and climber Scott Kranz climbed 100 peaks in Washington\n\nJohannes Klingebiel\u2019s digital garden \u2014 nice design\n\nEmmanuel Quartey\u2019s \u201cquestions\u201d \u2014 I like the framing and organization of information\n\nThe Shape of Music Albums \u2014 visualization of the characteristics Spotify assigns to tracks, created by Greg Wolanski\n\nTinnitus Tracker \u2014 concerts attended by Rob Weychert\n\nAtlas of Intangibles \u2014 cool interactive visualization of markers of distinctiveness and wear in London locations\n\n\ud83c\udd95 Atlas of Surveillance \u2014 police tech by state in the US (an EFF project)\n\nAdvocacy\n\n\ud83c\udd95 Regulations.gov \u2014 did you know there was one website where you could leave comments on like every U.S. rule change? I did not!\n\n\ud83c\udd95 Choose Democracy\n\n\ud83c\udd95 Beautiful Trouble Toolbox \u2014 \u201can interconnected web of ideas and creative best practices that puts the power in your hands\u201d\n\nSpecific Suggestions \u2014 \u201cThe most potent tools for fighting injustice are the ones already in your hands.\u201d\n\nWebcomics\n\nFalse Knees (Joshua Barkman) \u2013 comic strips with goofy birds\n\nwebcomic name (Alex Norris) \u2013 \u201coh no\u201d\n\nPoorly Drawn Lines (Reza Farazmand) \u2013 comic strips with returning animal characters\n\nCat and Girl (Dorothy) \u2013 social commentary from Cat and Girl\n\nThe Creator\u2019s Guide to Comics* Devices (Reimena Yee) \u2014 illustrated guide to tools that comic artists can use in storytelling\n\nInfo\n\nPlants and nature\n\n\u2013> check out my garden section\n\nSmall Seasons \u2014 the year divided into two-week segments named for the natural phenomena that tend to happen then (in Japan)\n\nNative Plants PNW \u2014 comprehensive listings of northwest native plants\n\nPacific Northwest Wildflowers \u2014 photographic database of wildflowers filterable by color and useful for identification\n\nThe Natural Navigator (Tristan Gooley) \u2013 interpreting nature sign\n\nCotswald Diary (Chris) \u2013 a look into the ongoing work of natural restoration projects\n\nNew Hampshire Garden Solutions \u2013 pretty plants and insects\n\noakland garden club (Alexis Madrigal)* \u2013 plants and art\n\nClamsplaining (Dan Killam) \u2013 clam science\n\nNatural World Facts\u2019 Deep Sea Hub (Leo Richards) \u2014 YouTube channel \u2014 mesmeric deep ocean videos\n\nWildhope.tv \u2014 YouTube channel \u2014 documentaries of conservation projects around the world\n\nBumble Bee Watch \u2014 report sightings of native bees\n\nPangea Seed \u2014 funding marine conservation through art\n\nFood\n\n\u2013> check out recipes I like and saved recipes to try\n\nBudget Bytes Vegetarian Recipes \u2014 cheap recipes, usually easy\n\nSmitten Kitchen (Deb Perelmen) \u2014 Deb has an inviting writing style \u2013 consistently good source of baking recipes\n\nStill Tasty \u2014 database of how long food lasts and storage instructions for a wide variety of foods\n\nThe Good Enough Weekly (Devin K. Pope)* \u2013 climate and food\n\nEat This Newsletter (Jeremy Cherfas)* \u2013 food\n\nTasting History (Max Miller) \u2014 YouTube channel \u2014 he cooks a historic recipe from basically any time period and talks about its context while it\u2019s cooking\n\nKenji\u2019s Cooking Show (J. Kenji L\u00f3pez-Alt) \u2014 YouTube channel \u2014 down to earth cooking advice from a science minded chef\n\nBlack Farmers Index \u2014 directory of Black farmers by region of the USA\n\nCollections\n\nPubMed Central \u2014 a free government database of medical journal papers, many of which include free full-text access because the researchers received grant funding \ud83d\ude4c\n\nFederal Open Science Repository of Canada \u2014 \u201cfederally authored scientific articles and publications from participating science-based departments and agencies\u201d \u2014 climate & environment portal\n\nSprout Distro \u2014 free printable zines\n\n\ud83c\udd95 An Incomplete SFF Criticism and Studies Reading List (Molly Templeton)\n\nPlaces\n\n\u2013> check out my road trip page\n\nAtlas Obscura \u2014 a searchable map and repository of cool destinations around the world \u2014 I always check this when I\u2019m planning a trip\n\nClose.city \u2014 map with overlays for walking / biking / transit times to major destinations \u2014 looks similar to WalkScore but without a score\n\n\ud83c\udd95 Lushootseed Place Names \u2014 Google Map of western Washington\n\nMath and Science\n\nStand-up Maths (Matt Parker) \u2014 YouTube channel \u2014 goofy math questions explored with demos and field trips\n\nVeritasium (Derek Muller) \u2014 YouTube channel \u2013longer explainer videos on science and engineering topics, often with cool models\n\nPractical Engineering (Grady Hillhouse) \u2014 YouTube channel \u2014 simple explanations of engineering practices with model demonstrations and case studies\n\n\ud83c\udd95 Defense Against Dishonest Charts (Nathan Yao) \u2014 visual breakdown of the elements of charts and what to look for in evaluating a chart for accuracy\n\nDiff Text \u2013 compare two text blocks\n\nLoot Lasso Portfolio Rebalancing Calculator \u2013 saw recommended on Reddit, haven\u2019t used \ud83d\ude09\n\nsymbol.wtf \u2013 quickly copy and paste symbols\n\n60 Seconds of Advice on Surviving a Nuclear Blast \u2013 also see NUKEMAP by Alex Wellerstein\n\n\ud83c\udd95 PairDrop \u2013 pair devices to share files instead of emailing\n\n\ud83c\udd95 cFIREsim \u2014 calculator for financial planning\n\n\ud83c\udd95 Kinopio \u2014 \u201cthinking canvas for new ideas and hard problems\u201d\n\n\ud83c\udd95 Scribe.rip \u2014 front end for Medium articles\n\nSeattle area\n\nWashington Smoke Information \u2013 invaluable during smoke season\n\nThe Urbanist \u2013 Seattle area urbanist news\n\nLushootseed (Tulalip Tribe) \u2013 phrases and pronunciation of Lushootseed words \u201cthe language of Puget Sound\u201d\n\n\ud83c\udd95 \u201cThe Voices of Lushootseed\u201d online Lushootseed lessons *with audio recordings*\n\nWashington Trails Association \u2013 an amazing repository of trip notes with current conditions from hikers all across Washington", "label": 1}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2015", "content": "en\n\nTopic of today is music. Importantly bands! As some of you may know, I went to the Vans Warped Tour '15 and discovered a lot of bands. It was probably the best day of my life so far!\n\nI want to introduce you guys to a band I discovered this summer: BoyMeetsWorld. A band of five guys from Ohio.\n\nThey are a really good rock/alternative band. For my die-hard alternative fans. This band is for you!! Those who want to expand their music taste, I also encourage you to listen to them. I really like their music. I can really relate to them.\n\n\"\"The fivesome are out to embody their message and encourage others through their music as they too have faced some of life's tough decisions and overcame them, making their relatability to their fans truly something special.\"\" -According to Vans Warped Tour Site\n\nThese are one of the nicest guys you will ever meet! One can really relate to their music! They never disappoint their growing fan base. Thanks guys for autographing my stuff. This was definitely put in the books.\n\nIt is nice to see a band grow and see them be even more successful as time goes on! I see great things for you guys!\n\nBand Members:\n\nRyan Sulken on drums\n\nBrad Sulken on bass,\n\nDrew Ritcher and\n\nDrew Thomason on guitar,\n\nSupport these guys! You wont regret them!\n\nHey BoyMeetsWorld do you want to come to my college for a concert in the future!? Please?\n\nFind them on BandCamp, Spotify, Google Play Music, etc! This would mean a lot for them! and Twitter (@OfficialBMWBand)You wont regret it!\n\nSources: VansWarpedTour.com , Spotify, BandCamp, Soundcloud.\"", "label": 1}
{"title": "Read The Arts and Crafts Movement \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/24/read-the-arts-and-crafts-movement/", "content": "Read The Arts and Crafts Movement: A Study of Its Sources, Ideals and Influence on Design Theory by Gillian Naylor\n\nLots of original quotes are interspersed, from a variety of sources. The text itself sometimes got lost in facts, dates, and sequences; it was best where it narrativized and provided high level analysis of trends. Some of the quotes are \ud83d\udd25\n\n\u201cThe movement\u2026 represents in some sense a revolt against the hard mechanical conventional life and its insensibility to beauty (quite another thing to ornament). It is a protest against that so-called industrial progress which provides shoddy wares, the cheapness of which is paid for by the lives of their producers and the degradation of their users.\u201d\n\n\u2014 Walter Crane, The Revival of Design and Handicraft (orig. pg. 12)\n\nI found the layout of the text frustratingly uncomfortable to read. I read through page 145 (195 pages of content before endnotes).\n\nGraphics\n\nInteresting and wide-ranging collection of sample works included (chiefly black and white unfortunately) \u2014 though I was frustrated at least twice when a specific piece would be described in the text but not pictured.\n\nNotes and Quotes\n\nMartin Wiener \u2013 English Culture and the Decline of the Industrial Spirit\n\nDesign theory and categorization of ornament = related to industrialization of design\n\n\u201cThe Arts and Crafts movement was inspired by a crisis of conscience. Its motivations were social and moral, and its aesthetic values derived from the conviction that society produces the art and architecture it deserves.\u201d (from 1989 preface by Gillian Naylor)\n\n\u201cAverting mankind\u2019s enslavement to the machine by saving the mass product and the home from mechanical anarchy and by restoring them to purpose, sense and life.\u201d\n\n\u2014 Walter Gropius, The Scope of Total Architecture\n\ncontemporary critique from Thorstein Veblen 1899: idolizing the handmade \u2013> conspicuous consumption \u2014 \u201cpropaganda of crudity\u201d describing the \u201cexaltation of the defective\u201d aka imperfections in handwork (aestheticization!!!)\n\nthey don\u2019t write insults like this anymore lol: \u201cdisencumber yourselves of the lymphatic ideology of your deplorable Ruskin\u201d \u2013Marinetti, 1912\n\n1835 committee to figure out how to give craftspeople a design sense and *taste* since consumers preferred imported aesthetics\n\nRuskin thought a craft and its society were inextricable\n\n\u201cFor it is not the material, but the absence of human labor, which makes the thing worthless, and a piece of terracotta, or plaster of paris, which has been wrought by the human hand, is worth all the stone and Carrara cut by machinery. It is, indeed, possible and even usual, for men to sink into machines themselves, so that even handwork has all the character of mechanization.\u201d\n\n\u2014 Ruskin, \u2018The Lamp of Truth\u2019 from The Seven Lamps of Architecture\n\nThe Stones of Venice = key book to the movement \u2014 especially the essay \u201cNature of Gothic\u201d (here as printed by William Morris)\n\n\u201cIt is not that men are ill-fed, but that they have no pleasure in the work by which they make their bread, and therefore look to wealth as the only means of pleasure.\u201d\n\n\u2014 Ruskin, \u2018Nature of Gothic\u2019 from The Stones of Venice\n\nRuskin into destigmatizing manual labor\n\nsociety\u2019s wealth measured in human happiness and its works of art\n\nthis is grouping Burne-Jones in with Arts and Crafts rather than Pre-Raphaelite\n\n\u201cIt was just a commonplace thing handled imaginatively, and it gave me as much pleasure as anything in the exhibition. It made me feel that it takes a big man to do a simple thing.\u201d\n\n\u2014 architect John Sedding, about a piece of furniture designed by Ford Madox Brown\n\n\u201c\u2018Art\u2019 to them meant individuality and the search for \u2018truth\u2019, whether in painting, architecture or applied design \u2014 and truth, they felt, could be found both in the study of nature, and in the recreation of the spirit rather than the letter of mediaevalism.\u201d\n\nduality of \u201cstraightforward, honest craftsmanship\u201d and \u201cmid-nineteenth-century ornamental conventions\u201d\n\n\u201ccardinal principle\u201d = know your materials and learn the craft directly\n\nMorris thought pattern design should hold meaning:\n\n\u201cdo not introduce any lines or objects which cannot be explained by the structure of the pattern; it is just this logical sequence of form, this growth which looks as if, under the circumstances, it could not have been otherwise, which prevents the eye wearying of the repetition of the pattern.\u201d\n\nconflict between Morris\u2019 love of craftsmanship and the expense of producing quality goods blocking most people from accessing it led to him becoming a Socialist\n\n\u201c[I]t is the allowing of machines to be our masters, and not our servants, that so injures the beauty of life nowadays.\u201d \u2014 William Morris\n\nArts and Crafts accepted both \u201csimple and luxurious\u201d \u2014 \u201csprang from the ideal of the craftsman as artist, and from the belief in individualism and individual commitment\u201d\n\nSee also: Read Liberty: British Colour Pattern\n\nRead In Harmony with Nature", "label": 1}
{"title": "Recording Song Covers", "url": "https://lifeofpablo.com/blog/recording-song-covers", "content": "Recording Song Covers\n\nthree friends hangin' out\n\nThis post was written in English (en_US).\n\nThree friends hanging out\n\nBack in Elementary, I was in this friend group who enjoyed music. This must have been in 2009? (That was a long time ago!) When you're a kid, you slowly discover music that you enjoy and not enjoy. What better way than to expose yourself to other individual's tastes in music? I would hang out with Abraham and Luke. I'd say, they influenced who I am as a person and the direction of my musical tastes.\n\nAbraham grew up in Colombia and moved out to my hometown in Nebraska. Luke grew up in Hawaii and had moved to Nebraska a year before. Both Luke and Abraham played the guitar. I was super excited to have friends who had come from cool places!\n\nI was the only one who didn't play the guitar, much less have a passion for playing an instrument. My friends at the time still took me in and accepted me into the group. It was super duper to watch live shows in my friend's basement. They used to play covers of Subline, Linkin Park, and Bullet for My Valentine.\n\nAt that time frame, I was very interested in photography and videography. I always carried my Sony Cyber-Shot with me because you never know when you would need to get that cool shot. You're probably thinking, \"That's some lame point in shoot!\" When your parents buy you a camera, you have to work with what you got! A camera is a camera!\n\nQuestion time!\n\nCan you guess which role did I play in my friend group? If you guessed the camera guy, you win a cookie! Do you like internet cookies? I don't know how to share a real cookie via the internet.\n\nI would record their guitar sessions on my camera. It was a pretty cool experience getting blasted with hardcore metal while figuring out the best way of recording! I learned important fundamentals of video recording. I focused on stabilizing the camera, considering lighting, and audio clipping (which I had no control over). Not only was I enjoying myself but also I learned a lot.\n\nOne of my favorite memories when we weren't in Luke's basement was walking to get pizza. Living in a small town, things are relatively within walking distance.\n\nAs time went on friends came and went. We all went our different paths even though we lived in the same hometown for years. I am thankful to have hung with these guys for a few years. I hope these guys are happy and living fulfilling lives.\n\nHere are a few videos that I recorded.\n\n\n\n\n\nAnother s", "label": 1}
{"title": "How we use GitHub to be more productive, collaborative, and secure", "url": "https://github.blog/engineering/how-we-use-github-to-be-more-productive-collaborative-and-secure/", "content": "It\u2019s that time of year where we\u2019re all looking back at what we\u2019ve accomplished and thinking ahead to goals and plans for the calendar year to come. As part of GitHub Universe, I shared some numbers that provided a window into the work our engineering and security teams drive each day on behalf of our community, customers, and Hubbers. As someone who loves data, it\u2019s not just fun to see how we operate GitHub at scale, but it\u2019s also rewarding to see how this work contributes to our vision to be the home for all developers\u2013which includes our own engineering and security teams.\n\nOver the course of the past year, GitHub staff made millions of commits across all of our internal repositories. That\u2019s a ton of branches, pull requests, Issues, and more. We processed billions of API requests daily. And we ran tens of thousands of production deployments across the internal apps that power GitHub\u2019s services. If you do the math, that\u2019s hundreds of deploys per day.\n\nGitHub is big. But the reality is, no matter your size, your scale, or your stage, we\u2019re all dealing with the same questions. Those questions boil down to how to optimize for productivity, collaboration, and, of course, security.\n\nIt\u2019s a running joke internally that you have to type \u201cGitHub\u201d three times to get to the monolith. So, let\u2019s take a look at how we at GitHub (1) use GitHub (2) to build the GitHub (3) you rely on.\n\nProductivity\n\nGitHub\u2019s cloud-powered experiences, namely Codespaces and GitHub Copilot, have been two of the biggest game changers for us in the past few years.\n\nCodespaces\n\nIt\u2019s no secret that local development hasn\u2019t evolved much in the past decade. The github/github repository, where much of what you experience on GitHub.com lives, is fairly large and took several minutes to clone even on a good network connection. Combine this with setting up dependencies and getting your environment the way you like it, spinning up a local environment used to take 45 minutes to go from checkout to a built local developer environment.\n\nBut now, with Codespaces, a few clicks and less than 60 seconds later, you\u2019re in a working development environment that\u2019s running on faster hardware than the MacBook I use daily.\n\nHeating my home office in the chilly Midwest with my laptop doing a local build was nice, but it\u2019s a thing of the past. Moving to Codespaces last year has truly impacted our day-to-day developer experience, and we\u2019re not looking back.\n\nGitHub Copilot\n\nWe\u2019ve been using GitHub Copilot for more than a year internally, and it still feels like magic to me every day. We recently published a study that looked at GitHub Copilot performance across two groups of developers\u2013one that used GitHub Copilot and one that didn\u2019t. To no one\u2019s surprise, the group that used GitHub Copilot was able to complete the same task 55% faster than the group that didn\u2019t have GitHub Copilot.\n\nGetting the job done faster is great, but the data also provided incredible insight into developer satisfaction. Almost three-quarters of the developers surveyed said that GitHub Copilot helped them stay in the flow and spend more time focusing on the fun parts of their jobs. When was the last time you adopted an experience that made you love your job more? It\u2019s an incredible example of putting developers first that has completely changed how we build here at GitHub.\n\nCollaboration\n\nAt GitHub, we\u2019re remote-first and we have highly distributed teams, so we prioritize discoverability and how we keep teams up-to-date across our work. That\u2019s where tools like Issues and projects come into play. They allow us to plan, track, and collaborate in a centralized place that\u2019s right next to the code we\u2019re working on.\n\nIncorporating projects across our security team has made it easier for us to not only track our work, but also to help people understand how their work fits into the company\u2019s broader mission and supports our customers.\n\nProjects gives us a big picture view of our work, but what about the more tactical discovery of a file, function, or new feature another team is building? When you\u2019re working on a massive 15-year-old codebase (looking at you, GitHub), sometimes you need to find code that was written well before you even joined the company, and that can feel like trying to find a needle in a haystack.\n\nSo, we\u2019ve adopted the new code search and code view, which has helped our developers quickly find what they need without losing velocity. This improved discoverability, along with the enhanced organization offered by Issues and projects, has had huge implications for our teams in terms of how we\u2019ve been able to collaborate across groups.\n\nShifting security left\n\nLike we saw when we looked at local development environments, the security industry still struggles with the same issues that have plagued us for more than a decade. Exposed credentials, as an example, are still the root cause for more than half of all data breaches today. Phishing is still the best, and cheapest, way for an adversary to get into organizations and wreak havoc. And we\u2019re still pleading with organizations to implement multi-factor authentication to keep the most basic techniques from bad actors at bay.\n\nIt\u2019s time to build security into everything we do across the developer lifecycle.\n\nThe software supply chain starts with the developer. Normalizing the use of strong authentication is one of the most important ways that we at GitHub, the home of open source, can help defend the entire ecosystem against supply chain attacks. We enforce multi-factor authentication with security keys for our internal developers, and we\u2019re requiring that every developer who contributes software on GitHub.com enable 2FA by the end of next year. The closer we can bring our security and engineering teams together, the better the outcomes and security experiences we can create together.\n\nAnother way we do that is by scaling the knowledge of our security teams with tools like CodeQL to create checks that are deployed for all our developers, protecting all our users. And because the CodeQL queries are open source, the vulnerability patterns shared by security teams at GitHub or by our customers end up as CodeQL queries that are then available for everyone. This acts like a global force multiplier for security knowledge in the developer and security communities.\n\nSecurity shouldn\u2019t be gatekeeping your teams from shipping. It should be the process that enables them to ship quickly\u2013remember our hundreds of production deployments per day?\u2013and with confidence.\n\nBig, small, or in-between\n\nAs you see, GitHub has the same priorities as any other development team out there.\n\nIt doesn\u2019t matter if you\u2019re processing billions of API requests a day, like we are, or if you\u2019re just starting on that next idea that will be launched into the world.\n\nThese are just a few ways over the course of the last year that we\u2019ve used GitHub to build our own platform securely and improve our own developer experiences, not only to be more productive, collaborative, and secure, but to be creative, to be happier, and to build the best work of our lives.\n\nTo learn more about how we use GitHub to build GitHub, and to see demos of the features highlighted here, take a look at this talk from GitHub Universe 2022.\n\nNotes\n\nTags:", "label": 0}
{"title": "My Site Redesign is Good Enough - IndieWeb Carnival April 2024", "url": "https://lifeofpablo.com/blog/my-site-redesign-is-good-enough", "content": "My Site Redesign is Good Enough - IndieWeb Carnival April 2024\n\nPablo focused on a program\n\nThis post was written in English (en_US).\n\nI recently redesigned my website and let me tell you, It was long overdue. The redesign was on my to do list from a long time a go. Things were a mess in the code. It's still a work in progress. I was using multiple style sheets (I'm still reducing this). I was using a theme I had modified and I was using Tachyons, a CSS framework. Having to add more CSS classes for specific purposes was getting overbearing. Sure my blog posts were getting pretty with various designs and looked a little nicer. I hate to say it, I was often more worried about what my website looked like instead of focusing more of the quality of my posts. An RSS reader simply pulls the content and none of the styling that comes with CSS classes.\n\nI was starting to lose it. I simply needed to strip my website of any unnecessary CSS and classes. I was also limited on time so I couldn't commit to something from scratch. I found a simple CSS framework that provided me the bare minimum that made my website look very simple but more put together.\n\nThe name of the CSS framework is...wait for it... called, simple.css. It's a CSS framework that has no classes built in. Simply load this and boom! Your website is mostly organized without the \"extra\" stuff. It's super simple to override the CSS code to add something specific.\n\nUsing simple.css was good enough for me. Simply, it was a no brainer. It allowed me to address the issues, and it was good enough to implement. simple.css is simple and fast. At this time, I don't feel the need to commit to designing from scratch the CSS to make my site pretty. Going minimal is good enough for me.\n\nThis is why 'Good Enough' is enough to get things done.\n\nThis blog post is in response to Aaron's, known as RisingThumb, IndieWeb Carnival April 2024 post on good enough. Thank you for hosting this month's carnival. Everyone and anyone is welcome to host an IndieWeb Writing Carnival.", "label": 1}
{"title": "Stripe\u2019s payments APIs: The first 10 years", "url": "https://stripe.com/blog/payment-api-design", "content": "A few years ago, Bloomberg Businessweek published a feature story on Stripe. Four words spanned the center of the cover: \u201cseven lines of code,\u201d suggesting that\u2019s all it took for a business to power payments on Stripe. The assertion was bold\u2014and became a theme and meme for us.\n\nTo this day, it\u2019s not entirely clear which seven lines the article referenced. The prevailing theory is that it\u2019s the roughly seven lines of curl it took to create a Charge . In 2011, the code snippet featured on our landing page was nine lines long. But remove the optional description and card[cvc] , and there are visually seven lines:\n\nA partial screenshot of Stripe.com, circa 2011. Courtesy of the Internet Archive Wayback Machine.\n\nHowever, a search for the seven lines of code ultimately misses the point: the ability to open up a terminal, run this curl snippet, then immediately see a successful credit card payment felt like seven lines of code. It\u2019s unlikely that a developer believed a production-ready payments integration involved literally only seven lines of code. But taking something as complex as credit card processing and reducing the integration to only a few lines of code that, when run, immediately returns a successful Charge object is really quite magical.\n\nAbstracting away the complexity of payments has driven the evolution of our APIs over the last decade. This post provides the context, inflection points, and conceptual frameworks behind our API design. It\u2019s the extreme exception that our approach to APIs makes the cover of a business magazine. This post shares a bit more of how we\u2019ve grown around and beyond those seven lines.\n\nA condensed history of Stripe\u2019s payments APIs\n\nSuccessful products tend to organically expand over time, resulting in product debt. Similar to tech debt, product debt accumulates gradually, making the product harder to understand for users and change for product teams. For API products, it\u2019s particularly tempting to accrue product debt because it\u2019s hard to get your users to fundamentally restructure their integration; it\u2019s much easier to get them to add a parameter or two to their existing API requests.\n\nIn retrospect, we see clearly how our APIs have evolved\u2014and which decisions were pivotal in shaping them. Here are the milestones that defined our payments APIs and led to the PaymentIntents API.\n\nSupporting card payments in the US (2011-2015)\n\nWe first launched the Stripe API in the US, where credit cards were\u2014and still are\u2014the predominant payment method. The \u201cseven lines of code\u201d largely sufficed, but reality was only a tiny bit more complicated. We also created Stripe.js, a JavaScript library to collect card payment details from the browser and securely store them with Stripe, represented as a Token which can later be used to create a Charge . This helped users avoid tedious PCI compliance requirements.\n\nA Token is created client-side and sent to the server. A Charge is then created server-side using that Token .\n\nThis payment flow follows a very common pattern in traditional web applications. The JavaScript client uses a publishable API key to create a Token and sends both to the server when customers submit the payment form (along with other form data about the order). The server synchronously creates a Charge using that Token and a secret API key; orders can optionally be fulfilled based on the outcome of the payment.\n\nThe Charge and the Token became foundational concepts in our payment API.\n\nAdding ACH and Bitcoin (2015)\n\nWhen we first created Charges and Tokens , they only supported credit card payments. As we expanded to more countries and types of users, we needed to add more payment methods to the API. In 2015, we added:\n\nACH debit , a common payment method in the US since the 1970s. ACH is used when moving money between US bank accounts, and supports both crediting and debiting bank accounts.\n\n, a common payment method in the US since the 1970s. ACH is used when moving money between US bank accounts, and supports both crediting and debiting bank accounts. Bitcoin, which was just gaining mindshare in the early 2010s. An increasing number of businesses were experimenting with accepting Bitcoin as a payment method.\n\nWe describe payments as \u201cfinalized\u201d when a user has sufficient confidence the funds are guaranteed. (Of course, even finalized payments can be reversed later due to fraud or subsequent refunds.) In most cases, upon finalization, users release shipment of goods. While payments processed on card networks are initiated by the merchant and can be immediately finalized, these two payment methods are quite different from cards. Payments processed on the ACH network are finalized days later. With Bitcoin, customers (rather than the merchant) determine when a Bitcoin transaction is created. Like ACH payments, Bitcoin payments are also not finalized immediately. While the merchant will know that the customer has created the Bitcoin transaction once it is picked up by a block, it still requires 6 blocks\u2014or about an hour\u2014to finalize the transaction.\n\nPayment is immediately finalized Payment is finalized later No customer action required To initiate money movement Card ACH debit (days) New Customer action required To initiate money movement Bitcoin (hours) New The Charges API supported cards, ACH debit, and Bitcoin as payment methods.\n\nEach of these first three payment methods differ in how the payment is initiated and when funds are guaranteed. This made the task of creating APIs that abstract over their differences quite challenging.\n\nHere\u2019s what we did:\n\nACH debit. Since card payments and ACH debit payments both require only static information from the customer (i.e., card number or bank account number), we expanded the Token resource to represent both card details and bank account details. A user still created a Charge from either type of Token , but we added a pending state to the Charge to represent that an ACH debit Charge isn\u2019t immediately finalized and could still fail. Users ran their order fulfillment logic days later, when they received a webhook indicating that the Charge had succeeded.\n\nA new pending state was added to the Charge to represent payments that finalize asynchronously.\n\nBitcoin. As Bitcoin didn\u2019t fit into our abstractions, we had to introduce a new BitcoinReceiver API to facilitate the client-side action we needed the customer to take in the online payment flow. Particular to Stripe, a \u201creceiver\u201d was a temporary receptacle for funds. It had a very simple state machine that described the status of the receiver: a boolean, filled , that was either true or false. Once the receiver was filled, the user could create a Charge using that BitcoinReceiver object instead of a Token object. This would virtually move the funds from the receiver to the user\u2019s balance. If a user didn\u2019t create the Charge within a certain time frame, the money in the receiver would be refunded to the customer. Like ACH debit Charges, Bitcoin Charges started in the pending state and succeeded asynchronously.\n\nWe introduced the BitcoinReceiver resource to represent that the customer needed to take an action to complete the payment.\n\nWith ACH debit and Bitcoin, the integration grew more complex. It now involved dealing with asynchronous payment finalization, and in Bitcoin\u2019s case, it involved managing two state machines to complete payment: BitcoinReceiver on the client and Charge on the server.\n\nSeeking a simpler payments API (2015 - 2017)\n\nOver the next two years, we added more payment methods. Most of them were more like Bitcoin than cards\u2014they required customer action to initiate a payment. We discovered that it wouldn\u2019t be developer-friendly to introduce a brand new BitcoinReceiver -like resource for each of these\u2014it would simply introduce too many new Stripe-specific concepts to reason about in the API. We aspired to design a simpler payments API and began exploring how to unify these payment methods on one integration path: the Sources API.\n\nPayment is immediately finalized Payment is finalized later No customer action required To initiate money movement Cards ACH debit\n\nSEPA direct debit New\n\nBacs debit New\n\nBECS debit New Customer action required To initiate money movement iDEAL New\n\nAlipay New\n\ngiropay New\n\nBancontact New\n\nWeChat Pay New\n\nPrzelewy24 New\n\nCards with 3D Secure New Bitcoin\n\nMultibanco New\n\nPaper checks New The Sources API was designed to be a single client-side API that could represent multiple payment methods.\n\nWe combined the two client-side abstractions we\u2019d previously designed ( Tokens and BitcoinReceivers ) into a client-driven state machine called a Source. Upon creation, a Source could be immediately chargeable (e.g., for card payments) or pending (e.g., for payment methods that require customer action). The server-side integration remained a single HTTP request that used a secret key to create a Charge .\n\nWe combined the functionality of Tokens and receivers into a single client-side API: Sources .\n\nThe payment flow for every payment method relied on the same two API abstractions: a Source and a Charge . This seems conceptually simple at first glance, as it resembled a card integration in the U.S. However, once we understood how this flow integrated into users\u2019 applications, we discovered many rough edges.\n\nFor example, when users added a payment method that doesn\u2019t finalize immediately, they could no longer fulfill their customers\u2019 orders immediately after the Charge was created. Instead, they\u2019d have to wait until the Charge transitioned to succeeded before shipping goods. This usually involved adding a webhook integration that listens for charge.succeeded and moving fulfillment logic there.\n\nSources and Charges were still more complex for other payment methods\u2014and integration issues could lead to lost revenue. For example, with iDEAL, the predominant payment solution in the Netherlands, the customer initiates the payment after they\u2019re redirected to their bank\u2019s website or mobile app. If the client-side application creates a Source and the browser then loses connectivity with the server, the next request to create a Charge wouldn\u2019t make it through, even though the customer believes they paid. (The browser could lose connectivity for any number of reasons: the customer closes their tab after they pay on their bank\u2019s site, the payment method requires a redirect that the customer never returns from, or the customer has a flaky internet connection.) Because the server never created a Charge , we\u2019d refund the money associated with the Source after a few hours. This is a conversion nightmare.\n\nTo reduce the chance of this occurring, we recommended that users either poll the Stripe API from their server until the Source became chargeable or listen for the source.chargeable webhook event to create the Charge . But, if a user\u2019s payment application goes down and they use Sources and Charges , these webhooks aren\u2019t delivered and the server won\u2019t create the Charge . We\u2019ll return the customer\u2019s money and users have to get them back on their site to pay again. Even if the user implements and maintains this best practice correctly, there\u2019s still complexity around the different possible states of Sources and Charges and the paths and requirements for different payment method types.\n\nThere are many ways to actually create a Charge from the Source , depending on the payment method.\n\nSome Sources \u2014like cards and bank accounts\u2014are synchronously chargeable and can be charged immediately on the server after the online payment form is submitted, while others are asynchronous and can only be charged hours or days later. Users often built parallel integrations using both synchronous HTTP requests and event-driven webhook handlers to support each type. This means users now have multiple places where they\u2019re creating a Charge and fulfilling their order. The code branching factor deepens for payment methods like OXXO, where the customer prints out a physical voucher and brings it to an OXXO store to pay for it in cash. Money is paid entirely out-of-band, making our best practice recommendation of listening for the source.chargeable webhook event absolutely required for these payment methods. Finally, users must track both the Charge ID and Source ID for each order. If two Sources become chargeable for the same order (e.g., the customer decides to switch their payment method mid-payment) they can ensure they don\u2019t double-charge for the order.\n\nThis effort demands more bookkeeping and conceptual understanding from developers than \u201cseven lines of code\u201d did. Our users needed to grok all of these edge cases in order to build a functioning Stripe integration. Imagine the confusion caused by reasoning about these two state machines, with varying definitions of each state depending on the payment solution. Developers must manage the success, failure, and pending states of two state machines\u2014whose states may differ across different payment methods\u2014in order to complete a single payment.\n\nUsers must manage two different state machines that span client and server to complete a payment.\n\nLet\u2019s refer back to the table of payment methods. You may notice that cards are the only payment method in the top left quadrant: they finalize immediately and don\u2019t require customer action to complete a payment. This means we built support for new payment methods on top of a set of abstractions that were designed for the simplest payment method of them all: cards. Naturally, abstractions designed for cards were not going to be great at representing these more complex payment flows.\n\nPayment is immediately finalized Payment is finalized later No customer action required To initiate money movement Cards ACH debit\n\nSEPA direct debit\n\nBacs debit\n\nBECS debit Customer action required To initiate money movement iDEAL\n\nAlipay\n\ngiropay\n\nBancontact\n\nWeChat Pay\n\nPrzelewy24\n\nCards with 3D Secure Bitcoin\n\nMultibanco\n\nPaper checks Global payment methods aren\u2019t different; cards are!\n\nIntroducing additional states and expanding on the definition of resources that were created for a specific, narrow use case resulted in a confusing integration and an overloaded set of API abstractions. It\u2019s as if we were trying to build a spaceship by adding parts to a car until it had the functionality of a spaceship: a difficult and likely doomed proposition. Charges and Tokens were foundational in the API because they were the first APIs we had, not because they were the right abstraction for global payments. We needed to fundamentally rethink our payments abstractions.\n\nDesigning a unified payments API (late 2017 - early 2018)\n\nWe were able to start designing the APIs we wanted when we set aside further changes to Sources and Charges . It was much easier because we had a chance to learn from users over the years, and deeply understood the issues they encountered with our existing integration paths. We also accumulated payments domain expertise, having had years of experience iterating on our APIs. Taken together, our API design had a better chance to not repeat past mistakes.\n\nWe locked ourselves in a conference room for three months with the goal of designing a truly unified payments API. If successful, a developer would only need to understand a few basic concepts in order to build a payments integration. Even if they hadn\u2019t heard of the payment method, they should be able to just add a few parameters to a few specific points in their integration. To enable this, the states and guarantees of our APIs had to be extremely predictable and consistent. There shouldn\u2019t be an array of caveats and exceptions scattered throughout our docs.\n\nA team of five people\u2014four engineers and a PM\u2014walked through every payment method we supported and we could imagine supporting in the future. We iterated on an API design that would be able to model all of them. We ignored all existing abstractions and thought about the problem from first principles.\n\nWe did early work on our unified payments API in a conference room named Lynx.\n\nIt\u2019s hard to remember now exactly what happened each day, but some rules and routines really helped us:\n\nClose laptops . When working together in the same room, we found the fastest way to be fully present and attentive was to close our computers. When we did, we felt more listened to and could more clearly and easily explain our reasoning to each other.\n\n. When working together in the same room, we found the fastest way to be fully present and attentive was to close our computers. When we did, we felt more listened to and could more clearly and easily explain our reasoning to each other. Pace your questions . Start each session with a set of questions you want to answer. Write down any new questions that arise in a working session for the next session. Try to avoid discussing them in the moment. In the time between sessions, you\u2019ll get some distance from those questions, collect new information, and meditate more on the topic. End each session with clear answers and questions to explore in the next session.\n\n. Start each session with a set of questions you want to answer. Write down any new questions that arise in a working session for the next session. Try to avoid discussing them in the moment. In the time between sessions, you\u2019ll get some distance from those questions, collect new information, and meditate more on the topic. End each session with clear answers and questions to explore in the next session. Use colors and shapes . Early on, lean on simple representations for complex, nascent concepts, rather than try to give them concrete names. We exhausted the available set of marker colors and drew many shapes on the whiteboard. This tack helped us avoid anchoring on specific definitions for the concepts that we were trying to shape\u2014and helped us avoid naming bikesheds prematurely.\n\n. Early on, lean on simple representations for complex, nascent concepts, rather than try to give them concrete names. We exhausted the available set of marker colors and drew many shapes on the whiteboard. This tack helped us avoid anchoring on specific definitions for the concepts that we were trying to shape\u2014and helped us avoid naming bikesheds prematurely. Focus on enabling real user integrations. In API design, it\u2019s common to get caught up with pursuing perfect invariants, airtight theories, or intellectually pure solutions, but none of that is useful if it doesn\u2019t enable a real user integration. One of our primary design tools was writing hypothetical integration guides to validate our concepts and to make sure we didn\u2019t introduce old or new pits of failure. We wrote these for every payment method we could list\u2014and even for some payment methods we made up, like sending cash via carrier pigeon.\n\nIn API design, it\u2019s common to get caught up with pursuing perfect invariants, airtight theories, or intellectually pure solutions, but none of that is useful if it doesn\u2019t enable a real user integration. One of our primary design tools was writing hypothetical integration guides to validate our concepts and to make sure we didn\u2019t introduce old or new pits of failure. We wrote these for every payment method we could list\u2014and even for some payment methods we made up, like sending cash via carrier pigeon. Question every assumption underpinning existing APIs . We specifically designed the first API to make card payments extremely easy, and it grew relatively organically from there. We needed to reason from first principles at every turn. Looking back, we probably could have done it even more.\n\n. We specifically designed the first API to make card payments extremely easy, and it grew relatively organically from there. We needed to reason from first principles at every turn. Looking back, we probably could have done it even more. Invite domain experts as guests. Import know-how for discussions with a specific topic in mind. Elevate the conversation with expertise.\n\nImport know-how for discussions with a specific topic in mind. Elevate the conversation with expertise. Make decisions quickly knowing you might change your mind. New observations or data would either further reinforce our initial decision or lead us to make a better choice. In every case, it was more efficient to make a decision early and avoid stasis, even if we later reversed that decision.\n\nWe frequently felt like we were brute-forcing the problem space, but the enemy of any large design project is not making decisions quickly enough because no option feels perfect.\n\nIntroducing PaymentIntents and PaymentMethods (2018)\n\nWe ended up with two new concepts: PaymentIntents and PaymentMethods. By packaging these two concepts, we finally managed to create a single integration for all payment methods.\n\nPaymentMethods, like the original Tokens , represent static information about the payment method that the customer wants to use. It includes the payment scheme and the credentials needed to move money, like card information or the customer\u2019s name or email. For some methods, like Alipay, only the payment method name is required because the payment method itself handles collecting further information after you redirect to their site. Unlike a Source , there is no state or data specific to the particular transaction type captured on a PaymentMethod object\u2014you can think of it as an object that specifies how to process a payment request.\n\nPaymentIntents, on the other hand, capture transaction-specific data such as how much to charge and is the stateful object that tracks the customer\u2019s attempt to pay with various payment methods. Combine a PaymentMethod (the \u201chow\u201d) and a PaymentIntent (the \u201cwhat\u201d) and payment can be attempted. If one payment attempt fails, the customer can try again with a different PaymentMethod.\n\nA PaymentIntent has the following states, summarized quickly here:\n\nrequires_payment_method: Specify the PaymentMethod to use.\n\nSpecify the PaymentMethod to use. requires_confirmation: \u201cConfirm\u201d basically means \u201cmake money go!\u201d Sometimes you want to pause between collecting payment method details and actually making the money go, and this (optional) state makes that possible.\n\n\u201cConfirm\u201d basically means \u201cmake money go!\u201d Sometimes you want to pause between collecting payment method details and actually making the money go, and this (optional) state makes that possible. requires_action: Please perform the specified action. This can be anything from a generic redirect_to_url (self-explanatory) to a very payment-method-specific action like oxxo_display_details , which provides information for you to generate an OXXO voucher.\n\nPlease perform the specified action. This can be anything from a generic (self-explanatory) to a very payment-method-specific action like , which provides information for you to generate an OXXO voucher. processing: You\u2019re waiting on us to process the payment.\n\nYou\u2019re waiting on us to process the payment. succeeded: The payment has been finalized. Funds are guaranteed.\n\nThe payment has been finalized. Funds are guaranteed. failed: There\u2019s no failed state because if a single payment attempt fails, the PaymentIntent goes back to the requires_payment_method state so that the customer can try again with a different payment method. This is convenient because the same object created server-side can be used repeatedly on the client.\n\nWith Charges and Sources , a \u201cbest practice\u201d payments integration for cards, iDEAL, and ACH debit required managing two webhook handlers (one that is time-sensitive and in the critical path to collecting money correctly), dealing with three different times a Charge could succeed, handling two paths to failure, and dealing with two stateful objects.\n\nWith PaymentIntents and PaymentMethods, the integration is the same across all payment method types: start by creating a PaymentIntent on your server for the amount and currency to collect for an order. Pass the secret embedded on the PaymentIntent to the client. Collect the customer\u2019s preferred payment method and confirm the PaymentIntent using the secret and payment method information. The PaymentIntent instructs what to do next when it\u2019s in the requires_action state. Actions are standardized and predictable per payment method; for example, the 3D Secure authentication flow is managed via a set of actions. Lastly, listen for the payment_intent.succeeded webhook or wait for the PaymentIntent to enter the succeeded state to know when funds are guaranteed and when to fulfill a customer\u2019s order. This is wholly managed by one predictable state machine. Importantly for conversion, the sole webhook handler that users must implement isn\u2019t in the critical path to collecting money.\n\nA PaymentIntents integration.\n\nLaunching PaymentIntents and PaymentMethods (2018 - 2020)\n\nThe design of a set of APIs that would work across all payment gateway methods globally with a single integration was the hard but fun part. The implementation of a beta, production-ready version of the API was also relatively straightforward. But launching a new payment API that replaces a foundational, established API doesn\u2019t stop at just writing the code to spec\u2014rolling out this change took almost two years.\n\nConnecting the design to reality\n\nIntroducing a new set of abstractions to an existing public API is much harder than updating internal interfaces. No matter the size of the company, sufficient tenacity and planning can drive teams to upgrade their dependencies. However, for an API product, there\u2019s no forcing developers to migrate, nor breaking their integration.\n\nA great API product stays out of the developer\u2019s way for as long as possible.\n\nIf it is possible to make small changes to an existing API to accommodate new use cases, try that first so developers don\u2019t have to rewrite their integration. In our case, we already knew from experience that just adding more parameters and states to the existing API resources wasn\u2019t working. Even if the resource had the same name, the payment flow would look completely different.\n\nThat said, the alternative\u2014building new, entirely independent APIs which required developers to migrate everything at once\u2014also felt daunting. After talking to many users, we identified common patterns in their integrations. One integration created Stripe objects in the payment solution. Other integrations consumed Stripe objects for analytics, support, or reporting\u2014potentially syncing these objects to their own database. For some users, these integrations were even owned by different teams. Given a core feature of Stripe\u2019s APIs is that developers don\u2019t have to touch their integration for years, we had to figure out a way to motivate users to migrate their payment flow. One way to do this was to make sure that any changes to the payment flow don\u2019t break their other integrations.\n\nTo accomplish this, we decided to layer over the legacy APIs and create a Charge object for each payment attempted by the PaymentIntent. This way, users could migrate their payment flow to the PaymentIntents API while their analytics and reporting integrations still chugged along on an unchanged Charge resource. (This is also a good reason to not just reuse the Charge abstraction with changes to conceptually behave more like PaymentIntents. Lots of users and extensions make assumptions about what a Charge means, and changing its state machine drastically would break those assumptions.)\n\nWe didn\u2019t like how cluttered the Charge resource had become over the last seven years, so this was not ideal. Between 2011 and 2018, the Charge resource grew from having 11 properties to 36 properties and Charge creation grew from accepting 5 parameters to 14 parameters! To make sure we don\u2019t make the problem worse as we add more payment methods, we introduced payment_method_details, a polymorphic, typed hash on the Charge that contains payment-method-specific data. This approach helps us keep the top-level Charge resource simple, while making payment details easy to find and identify for details such as a partner reference ID or a payment-method-specific verification status:\n\n{ payment_method_details: { type: PaymentMethodType, [PaymentMethodType]: { // Payment-method-specific details about the transaction. // For cards, maybe it\u2019s the CVC verification information. // For OXXO, maybe it\u2019s the voucher information. } } } ~\n\nOver time, we\u2019ve standardized this design pattern and have applied it to other resources in the API.\n\nLayering over the Charges API is just one example of a design compromise we had to make for the sake of migration. There were many other smaller challenges, but ultimately they all had some least-bad solution we could pursue, so it wasn\u2019t too dire. The hardest part of realizing the PaymentIntent migration was not a technical challenge, but a perception challenge: The new APIs didn\u2019t feel like \u201cseven lines of code\u201d anymore.\n\nKeep it simple, Stripe\n\nIn normalizing the API across all payment methods, card payments became more complicated to integrate by introducing webhook events and by flipping the order of the client and server requests in the payment flow. These choices are not intuitive for those familiar with card payments, nor are they easy to implement for developers building traditional web applications.\n\nCompared to a simple card payments integration on Charges, a PaymentIntents integration requires flipping the client and server API calls and dealing with a webhook.\n\nThis change to card payments was a challenge for one of our most important types of users: the eager developer at a startup who wants to get up and running with card payments for checkout as soon as possible. Before, their seven lines of code pasted in a terminal would result in a successful charge. This new payment processing flow relies on asynchronous events, so the magic becomes much less tangible.\n\nPaymentIntents is also objectively a harder integration for users who only care about accepting card payments in the US and Canada. We flipped the order of the client and server calls, which is difficult for traditional web applications to handle, and webhooks are often more than a little bit annoying to set up, test, and debug. (We later developed the Stripe CLI to make developing with webhooks simpler for users.)\n\nThe power-to-effort curve looks different between the Charges integration and the new PaymentIntents integration. Each incremental PaymentMethod is cheap to add to a PaymentIntents integration. However, speed is key for startups who want to get started quickly. With Charges , getting cards running was intuitive and low-effort\u2014a compelling combination for startups.\n\nA PaymentIntents integration requires more effort up front, but each incremental payment method requires little incremental work to understand and add. On the other hand, a Charges integration is very low-effort for cards in the US and Canada, but becomes tedious and unpredictable for each subsequent payment method.\n\nOur first attempt at launching PaymentIntents without overwhelming existing users was to show both the PaymentIntents and Charges integration guides in our documentation, switching which one we showed first depending on the user\u2019s location. The idea was that most users in the US did not need these non-card payment methods, and thus would feel overwhelmed by the idea of payments as a state machine. In reality, this branching between two completely different integrations was tremendously confusing.\n\nMany US businesses do want to go global, and folks aren\u2019t always coding from the locale of the business they want to run. If a developer for a EU-based business ended up following the Charges integration guide, they\u2019d eventually realize that they would have to start from scratch. This happened a few times, and was always a costly and painful experience. It was not user-centric thinking to assuage our own worries about this big API change by recommending two incompatible integration paths.\n\nOur ultimate solution to this problem was to add a convenient packaging of the API that caters to the hypothetical user that would turn away from our APIs if they had to use webhooks up front. We called the default integration the \u201cglobal payments integration\u201d and named the new integration \u201ccard payments without bank authentication.\u201d We put the implications of this integration front and center in the documentation: with this simpler flow, you won\u2019t be able to easily add new payment methods.\n\nThe way this conceptual packaging actually manifests in the API is a special parameter called error_on_requires_action . This parameter tells the PaymentIntent to error if further action is required to complete the payment. A user who wants a simple payment flow like Charges won\u2019t be able handle any actions required by the PaymentIntent state machine.\n\n# Our packaging made PaymentIntents seven lines of code. curl https://api.stripe.com/v1/payment_intents \\ -u sk_test_xxx: \\ -d amount=1099 \\ -d currency=usd \\ -d confirm=true \\ -d payment_method=\"{{PAYMENT_METHOD_ID}}\" \\ -d error_on_requires_action=true ~\n\nThe parameter name makes it very clear what users are choosing. Additionally, this approach allows us to easily track how often users choose this integration path, which would not be possible if we\u2019d just recommended that U.S. users ignore PaymentIntent states they couldn\u2019t handle. Someday that eager developer will have the time to build out a webhooks integration or will need to add a new payment method. When that day comes, it\u2019s clear what they need to do: remove the parameter from the integration to start handling the requires_action state. Developers using this packaging of PaymentIntents don\u2019t have to change the core resources at play, even when they upgrade to the global integration.\n\nOur simple packaging of PaymentIntents for U.S. and Canadian card payments requires the same amount of effort to integrate as Charges .\n\nWith this packaging, we were able to provide a low-effort integration similar to Charges for users who had no interest in doing a global-payments-ready integration up front.\n\nKeeping things simple doesn\u2019t just mean reducing the number of resources or parameters.\n\nTwo overloaded API abstractions are not simpler and are definitely not more flexible and powerful than three or four clearly-defined abstractions. Keeping things simple means making sure your APIs are consistent and predictable\u2014and that you\u2019re creating the right packages to gradually reveal the power of your API as your users need it. It also means not underestimating your user. It\u2019s tempting to abstract away too much in service of \u201ckeeping things simple,\u201d but users will often quickly discover that they need more control.\n\nAn API product is more than just the API\n\nThere has\u2014and will always be\u2014many lines of code propping up the vaunted \u201cseven lines of code.\u201d It\u2019s reliably the case with APIs. They don\u2019t happen without a lot of work that isn\u2019t designing or building the actual API. Much of the effort required is unglamorous and tedious, like tracking down every piece of documentation, support article, and canned response that references the old APIs, reaching out to folks who have made community content and asking them to update it, and planning and recording many tutorials for users and user-facing teams.\n\nThere\u2019s also the teams that appear on the periphery, but are instrumental in the success of APIs. There\u2019s the documentation and developer products that supplement the integration experience. Stripe CLI\u2019s launch made webhooks much less daunting. A redesign of the information architecture of our documentation made relevant guides easier to find. Stripe Samples allows developers who prefer to learn by example rather than prose to just start with some working code. A redesign of the payments view in the Stripe Dashboard allows developers to more easily debug and understand the PaymentIntent state machine.\n\nThe care, choices, and effort of Stripes past and present from across the company contributed to our most recent two-year effort to design and launch our new payments APIs. The more we grow, the more we realize that we must continue to build and rebuild deliberately and thoughtfully. These are still early days. Come join us.", "label": 0}
{"title": "Simulating a neural operating system with Gemini 2.5 Flash-Lite", "url": "https://developers.googleblog.com/en/simulating-a-neural-operating-system-with-gemini-2-5-flash-lite/", "content": "In traditional computing, user interfaces are pre-defined. Every button, menu, and window is meticulously coded by developers. But what if an interface could be generated in real time, adapting to a user's context with each interaction? We explored this question by building a research prototype (view demo app in Google AI Studio) for a generative, infinite computer experience. Our prototype simulates an operating system where each screen is generated on the fly by a large language model. It uses Gemini 2.5 Flash-Lite, a model whose low latency is critical for creating a responsive interaction that feels instantaneous. Instead of navigating a static file system, the user interacts with an environment that the model builds and rebuilds with every click. This post outlines the core technical concepts behind this prototype.\n\nSorry, your browser doesn't support playback for this video\n\nConditioning the model for on-the-fly UI generation To generate a UI on-the-fly, we need to provide the model with a clear structure and context for each request. We engineered our prompt by dividing the model's input into two parts: a \"UI constitution\" and a \"UI interaction\". The UI constitution is a system prompt that contains a fixed set of rules for UI generation. These rules define consistent elements like the OS-level styling, the home screen format, and logic for embedding elements like maps. The UI interaction is a JSON object that captures the user's most recent action, such as a mouse click on an icon. This object serves as the specific query that prompts the model to generate the next screen. For example, clicking on a \u201cSave Note\u201d icon within the Notepad app may generate an object as the following:\n\n{ // `id`: The unique ID from the button's `data-interaction-id` attribute. id: 'save_note_action', // `type`: The interaction type from `data-interaction-type`. type: 'button_press', // `value`: Because the button has a `data-value-from` attribute, the system // retrieves the content from the textarea with the ID 'notepad_main_textarea'. value: 'Meeting notes\n\n- Discuss Q3 roadmap\n\n- Finalize budget', // `elementType`: The HTML tag of the element that was clicked. elementType: 'button', // `elementText`: The visible text inside the button. elementText: 'Save Note', // `appContext`: The ID of the application the user is currently in. // This comes from the `activeApp` state in `App.tsx`. appContext: 'notepad_app' } JSON Copied", "label": 0}
{"title": "Introducing AutoPatchBench: A Benchmark for AI-Powered Security Fixes", "url": "https://engineering.fb.com/2025/04/29/ai-research/autopatchbench-benchmark-ai-powered-security-fixes/", "content": "We are introducing AutoPatchBench, a benchmark for the automated repair of vulnerabilities identified through fuzzing.\n\nBy providing a standardized benchmark, AutoPatchBench enables researchers and practitioners to objectively evaluate and compare the effectiveness of various AI program repair systems.\n\nThis initiative facilitates the development of more robust security solutions, and also encourages collaboration within the community to address the critical challenge of software vulnerability repair.\n\nAutoPatchBench is available now on GitHub.\n\nAI is increasingly being applied to solve security challenges, including repairing vulnerabilities identified through fuzzing. However, the lack of a standardized benchmark for objectively assessing AI-driven bug repair agents specific to fuzzing has impeded progress in academia and the broader community. Today, we are publicly releasing AutoPatchBench, a benchmark designed to evaluate AI program repair systems. AutoPatchBench sits within CyberSecEval 4, Meta\u2019s new benchmark suite for evaluating AI capabilities to support defensive use cases. It features 136 fuzzing-identified C/C++ vulnerabilities in real-world code repos along with verified fixes sourced from the ARVO dataset.\n\nAutoPatchBench provides a standardized evaluation framework for assessing the effectiveness of AI-assisted vulnerability repair tools. This benchmark aims to facilitate a comprehensive understanding of the capabilities and limitations of various AI-driven approaches to repairing fuzzing-found bugs. By offering a consistent set of evaluation criteria, AutoPatchBench fosters transparency and reproducibility in research, enabling both academic and industry professionals to identify best practices and areas for improvement.\n\nFixing fuzzing-found vulnerabilities with AI\n\nFuzzing is a cornerstone in automated testing, renowned for its effectiveness in uncovering security vulnerabilities. By bombarding a target program with vast amounts of pseudo-random input data, fuzz testing exposes critical security and reliability issues, such as memory corruption, invalid pointer dereference, integer overflow, and parsing errors.\n\nHowever, resolving a fuzzing crash is often a labor intensive task, demanding intricate debugging and thorough code review to pinpoint and rectify the underlying cause. This process can be both time-consuming and resource-intensive. Unlike regular test failures, fuzzing bugs frequently reveal security vulnerabilities that pose severe threats to system integrity and user data. Given these stakes, automating the repair of fuzzing bugs with AI becomes not just advantageous but essential. AI\u2019s ability to swiftly analyze patterns and propose solutions significantly reduces the time and effort required for repairs, making it an invaluable ally in safeguarding our digital environments.\n\nLet\u2019s explore the process of addressing bugs identified through fuzzing by examining a demonstrative example. Consider the following C function, which harbors a read/write buffer overflow vulnerability:\n\n#include <stdio.h> #include <string.h> void process_input(const char *input) { char buffer[8]; strcpy(buffer, input); // Potential buffer overflow printf(\"Processed: %s\n\n\", buffer); }\n\nIn this scenario, a fuzzing harness might supply an input that surpasses the buffer\u2019s capacity, leading to a crash due to buffer overflow. A typical stack trace from such a crash might appear as follows:\n\n== Fuzzer Crash Report == Program received signal SIGSEGV, Segmentation fault. 0x00007ffff7af1223 in strcpy () from /lib/x86_64-linux-gnu/libc.so.6 (gdb) bt #0 0x00007ffff7af1223 in strcpy () #1 0x0000555555555140 in process_input (input=0x7fffffffe695 \"AAAAAA...\") #2 0x0000555555555162 in main (argc=2, argv=0x7fffffffe5f8)\n\nHere, the process_input function invokes strcpy on a string that exceeds the eight-character buffer, causing a segmentation fault. A straightforward patch involves ensuring the copy operation remains within the buffer\u2019s limits. This can be achieved by using a bounded copy function like strncpy or implementing a length check before copying:\n\nvoid process_input(const char *input) { char buffer[8]; strncpy(buffer, input, sizeof(buffer) - 1); buffer[sizeof(buffer) - 1] = '\\0'; printf(\"Processed: %s\n\n\", buffer); }\n\nThis patch ensures that the string remains within the buffer\u2019s limits, effectively preventing out-of-bounds writes. Its correctness can be confirmed by verifying that the fuzzing input, which previously caused the crash, no longer does so. Additional checks can be conducted to ensure the patch doesn\u2019t introduce any unintended side effects.\n\nAs illustrated, fixing a fuzzing crash involves:\n\nAnalyzing the crash stack trace and the target code. Pinpointing the root cause. Patching the vulnerable code. Verifying the fix\u2019s accuracy.\n\nAn AI-based solution can automate these steps by utilizing an LLM\u2019s capability to understand and generate code.\n\nWhy we developed AutoPatchBench\n\nAutoPatchBench is informed by key advancements in the field of AI-driven program repair, particularly those focusing on fuzzing-found vulnerabilities. Among the notable contributions is Google\u2019s tech report on AI-powered patching, which pioneered the use of LLMs for addressing fuzzing crashes, achieving a 15% fix rate with their proprietary dataset. Subsequently, Google\u2019s study on generic program repair agents introduced the GITS-Eval benchmark, encompassing 178 bugs across various programming languages.\n\nIn the realm of AI software engineering agents, benchmarks like SWE-Bench and SWE-Bench Verified have gained widespread acceptance for evaluating generic AI SWE agents. However, these benchmarks do not specifically tackle the unique challenges posed by fuzzing-found vulnerabilities, which demand specialized approaches that utilize fuzzing-specific artifacts and address security concerns.\n\nAutoPatchBench addresses this gap by offering a dedicated benchmark focused on a wide variety of C/C++ vulnerabilities of 11 crash types identified through fuzzing with automated verification capability. Unlike the broader focus of GITS-Eval and SWE-Bench, AutoPatchBench is specifically designed to assess the effectiveness of AI-driven tools in repairing security-critical bugs typically uncovered by fuzzing. This targeted approach enables a more precise evaluation of AI capabilities in meeting the complex requirements of fuzzing-found vulnerabilities, thereby advancing the field of AI-assisted program repair in a focused manner.\n\nInside AutoPatchBench\n\nWe\u2019re making AutoPatchBench publicly available as part of CyberSecEval 4 to encourage community collaboration in tackling the challenge of automating fuzzing crash repairs. This benchmark is specifically designed for AI program repair agents focusing on C/C++ bugs identified through fuzzing. It includes real-world C/C++ vulnerabilities with verified fixes sourced from the ARVO dataset, and incorporates additional verification of AI-generated patches through fuzzing and white-box differential testing.\n\nARVO dataset\n\nThe ARVO dataset serves as the foundation for AutoPatchBench, offering a comprehensive collection of real-world vulnerabilities that are essential for advancing AI-driven security research. Sourced from C/C++ projects identified by Google\u2019s OSS-Fuzz, ARVO includes over 5,000 reproducible vulnerabilities across more than 250 projects. Each entry is meticulously documented with a triggering input, a canonical developer-written patch, and the capability to rebuild the project in both its vulnerable and patched states.\n\nHowever, there are notable challenges when using the ARVO dataset as a benchmark for AI patch generation:\n\nWhile reproducibility is vital for a reliable benchmark, the ARVO dataset includes samples where crashes are not consistently reproducible. Some samples lack crash stack traces, making it exceedingly difficult to address the crash. Although ARVO provides a ground-truth fix for each identified vulnerability, it lacks an automated mechanism to verify the correctness of a generated patch. Objective automated verification is essential for a benchmark focused on patch generation.\n\nAutoPatchBench addresses these challenges by creating a curated subset and by employing a comprehensive and automated verification process.\n\nSelection criteria\n\nTo ensure the reliability and effectiveness of AutoPatchBench, we meticulously filtered the ARVO dataset samples based on the following criteria:\n\nValid C/C++ vulnerability: The ground-truth fix shall edit one or more C/C++ source files that are not fuzzing harnesses.\n\nDual-container setup : Each vulnerability is accompanied by two containers\u2014one that contains vulnerable code and another for the fixed code\u2014that build without error.\n\nReproducibility : The crash must be consistently reproducible within the vulnerable container.\n\nValid stack trace : A valid stack trace must be present within the vulnerable container to facilitate accurate diagnosis and repair.\n\nSuccessful compilation : The vulnerable code must compile successfully within its designated container, ensuring that the environment is correctly set up for testing.\n\nFixed code verification : The fixed code must also compile successfully within its respective container, confirming that the patch does not introduce new build issues.\n\nCrash resolution : The crash must be verified as resolved within the fixed container, demonstrating the effectiveness of the patch.\n\nFuzzing pass : The fixed code must pass a comprehensive fuzzing test without finding new crashes, ensuring that the ground-truth patch maintains the integrity and functionality of the software.\n\nAfter applying these rigorous selection criteria, we retained 136 samples for AutoPatchBench that fulfill the necessary conditions for both patch generation and verification. From this refined set, we created a down-sampled subset of 113 AutoPatchBench-Lite samples to provide a focused benchmark for testing AI patch generation tools. These subsets preserves the diversity and complexity of real-world vulnerabilities including 11 distinct crash types, offering a solid foundation for advancing AI-driven security solutions.\n\nPatch verification\n\nIn the process of patch generation, the patch generator utilizes two automated methods to verify the viability of a generated patch before submitting it for evaluation. The first method involves attempting to build the patched program, which checks for syntactic correctness. The second method involves attempting to reproduce the crash by running the input that initially triggered it. If the crash no longer occurs, it suggests that the issue has been resolved. However, these steps alone are insufficient to guarantee the correctness of the patch, as a patch might not maintain the program\u2019s intended functionality, rendering it incorrect despite resolving the crash.\n\nTo address this issue, AutoPatchBench adopts a comprehensive approach to automate the evaluation of generated patches. This involves subjecting the patched code to further fuzz testing using the original fuzzing harness that initially detected the crash. Additionally, white-box differential testing compares the runtime behavior of the patched program against the ground truth repaired program, confirming that the patch has effectively resolved the underlying bug without altering the program\u2019s intended functionality. Since a patch can potentially be made in multiple places, we cannot assume that the LLM will patch the same function as the groundtruth patch does. Instead we find all the callstacks for each call to a patched function. Then we find the lowest common ancestor (LCA) across all pairs of stacktraces offered by the groundtruth patch and the LLM patch. We then utilize debug information to inspect arguments, return values, and local variables at the first function above the LCA, differential testing offers a detailed view of the patch\u2019s impact on the program state.\n\nThis process evaluates whether the generated patch produces a program state identical to the ground truth program after the patched function returns. By using a diverse set of inputs obtained from fuzzing, this gives higher confidence that the bug is fixed without changing the visible behavior of the patched functions. This differential testing is implemented using a Python script that leverages LLDB APIs to dump all visible states and identify differences between the ground truth and the patched program.\n\nHowever, as with all attempts to solve provably undecidable problems (in this case: program equivalence), there are some failure modes for this verification step. For example, sometimes the analysis fails with timeouts, in which case we consider the semantics to be preserved if both the ground truth and the LLM patch timed out. Programs might also behave non-deterministically, and we run each input three times to identify nondeterministic struct fields and values. Such fields will not be compared to avoid false alarms from noisy, random values. Additionally, we strip any fields that contain the substring \u201cbuild\u201d or \u201ctime\u201d as we\u2019ve observed false positives from build-ids (that happen to be deterministic within a program, but not across different patches).\n\nIt should also be noted that on a number of examples, the crashing PoC never actually triggered the breakpoints on the ground truth patch, making comparison of the resulting states impossible. However, our case study showed that white-box differential testing is still effective in filtering out a majority of incorrect patches despite its limitation, which will be discussed in the case study.\n\nAutoPatchBench and AutoPatchBench-Lite\n\nAutoPatchBench is a comprehensive benchmark dataset of 136 samples. It encompasses a wide range of real-world vulnerabilities, providing a robust framework for assessing the capabilities of automated patch generation systems.\n\nWithin this benchmark, we have also created a subset called AutoPatchBench-Lite that consists of 113 samples. AutoPatchBench-Lite focuses on a simpler subset of vulnerabilities where the root cause of the crash is confined to a single function. This version is designed to cater to scenarios where the complexity of the bug is relatively low, making it more accessible for tools that are in the early stages of development or for those that specialize in handling straightforward issues.\n\nThe rationale for creating AutoPatchBench-Lite stems from the observation that when root causes are distributed across multiple locations within the code, the difficulty of generating a correct patch increases significantly. Addressing such \u201chard\u201d crashes requires a tool to possess advanced reasoning capabilities to analyze larger codebases and apply patches to multiple areas simultaneously. This complexity not only challenges the tool\u2019s design but also demands a higher level of sophistication in its algorithms to ensure accurate and effective patching.\n\nBy offering both AutoPatchBench and AutoPatchBench-Lite, we provide a tiered approach to benchmarking, allowing developers to progressively test and refine their tools. This structure supports the development of more advanced solutions capable of tackling both simple and complex vulnerabilities, ultimately contributing to the enhancement of AI-assisted bug repair techniques.\n\nExpected use cases\n\nAutoPatchBench offers significant value to a diverse range of users. Developers of auto-patch tools can leverage our open-sourced patch generator to enhance their tools and assess their effectiveness using the benchmark. Software projects employing fuzzing can incorporate our open-sourced patch generator to streamline vulnerability repair. Additionally, model developers can integrate the benchmark into their development cycles to build more robust and specialized expert models for bug repair. The tooling around the patch generator provided here can also be used in reinforcement learning as a reward signal during training. This data helps train models to better understand the nuances of bug repair, enabling them to learn from past fixes and improve their ability to generate accurate patches.\n\nReference implementation\n\nWe developed a basic patch generator to establish a baseline performance using AutoPatchBench. This generator is specifically designed to address simple crashes that involve patching a single function. We have open-sourced this reference implementation to encourage the community to build and expand upon it.\n\nFigure 1 shows a high-level overview of its design. The patch generator takes crash stack trace and target source code as input. It identifies the source locations from the stack trace and extracts every function that contains those locations. It then asks the LLM to identify the root cause and repair the crash by patching one of the functions. Upon receiving a response from the LLM, the patch generator extracts the revised code, applies the patch, compiles the program, and tests it against the original input that caused the crash. If the build or test fails, we re-engage the LLM with the error message from the build or test output, requesting it to attempt a solution again until the crash is resolved. If a fix trajectory fails to reach a valid solution that passes build and crash reproduction within a finite number of steps, we start a new trajectory to reset the context window, preventing prolonged entrapment in an incorrect path.\n\nThe example prompt below also captures the essence of our approach. Note that the actual prompt is more detailed and divided into multiple segments to promote chain-of-thought reasoning.\n\nAs an experienced Security Engineer at Meta, your task is to address the following security-critical fuzzing crash. Below is the stack trace of the crash: == Fuzzer Crash Report == Program received signal SIGSEGV, Segmentation fault. 0x00007ffff7af1223 in strcpy () from /lib/x86_64-linux-gnu/libc.so.6 (gdb) bt #0 0x00007ffff7af1223 in strcpy () #1 0x0000555555555140 in process_input (input=0x7fffffffe695 \"AAAAAA...\") #2 0x0000555555555162 in main (argc=2, argv=0x7fffffffe5f8) Here is the source code for the functions involved in the stack trace: strcpy() {...} void process_input(const char *input) { char buffer[8]; strcpy(buffer, input); // Potential buffer overflow printf(\"Processed: %s\n\n\", buffer); } int main() {...} Assuming the root cause of the crash is within one of these functions, generate a patched version of the faulty function to resolve the fuzzing crash. Ensure that you provide a complete rewrite of the function so that the patch can be applied and the code compiled without errors.\n\nA case study with AutoPatchBench-Lite\n\nIn the case study, we demonstrate the use of AutoPatchBench by evaluating our reference patch generator with several LLM models. Given that our reference implementation is limited to addressing simple issues, we conducted our evaluation with AutoPatchBench-Lite, which contains 113 samples. To prevent fix trajectories from becoming excessively prolonged, we capped the maximum length of each trajectory at five. Additionally, we set the maximum number of retries to 10.\n\nPlease note that the case study is not intended to provide a statistically rigorous comparison of model performance. Instead, it aims to present preliminary results to establish a baseline expectation. We encourage future research to build upon these findings.\n\nEffectiveness of patch generation and verification\n\nWe evaluated the effectiveness of the patch generator and our automated verification processes while using different LLM models as back-end. The figure below illustrates the effectiveness of patch generation and verification by presenting the percentage of samples that successfully passed each sequential verification step: (1) patch validity: build and crash reproducibility check, (2) fuzzing pass: passes 10-minute fuzzing, and (3) testing pass: passes white-box differential testing. It is important to note that the patch generation process only utilizes step (1) to verify the build and crash reproducibility. The fuzzing and differential testing are conducted post-generation to assess correctness.\n\nFigure 2 shows that all models achieved similar generation success rates of around 60% and similar post-verification success rates of around 5-11% with overlapping confidence intervals, and therefore, we do not draw any conclusion about their relative performance. The graph does, however, reveal that a substantial portion of the generated patches are found to be incorrect when subjected to fuzzing and white-box differential testing. For instance, Gemini 1.5 Pro achieved a 61.1% patch generation success rate, yet fewer than 15% of these patches (5.3% out of total set) were found to be correct. This gap highlights that build and crash reproduction are not good enough signals to infer the correctness of generated patches, and that future patch generation approaches should scrutinize the semantic preservation of generated patches more thoroughly. This gap also underscores the vital role of the comprehensive verification processes that checks semantic equivalence, a distinctive contribution of AutoPatchBench.\n\nEffect of inference-time computation\n\nTo assess the impact of inference-time computation on improving the patch generation success rate, we present the distribution of retry counts among the 73 patches produced by Llama 4 Maverick.\n\n\n\nFigure 3 shows that 44 out of 73 patches, or 60.2%, were successfully generated on the first attempt. The remaining 40% of the samples required more than two iterations, with no evident plateau until the 10th iteration. This outcome demonstrates that allocating more computational resources during inference-time leads to a higher success rate and suggests that increasing the number of retries could yield better results.\n\nManual validation\n\nIn our investigation of the precision and recall of white-box differential testing, we conducted a manual validation of 44 patches that passed 10-minute fuzzing against human-written ground truth fixes with the help of security experts. These patches were selected from a pool of 73 generated by Llama 4 Maverick. The following table shows the confusion matrix.\n\nTable 1: Confusion matrix between human judgement and differential testing\n\nTest pass Test fail Sum Human pass 5 0 5 Human reject 7 32 39 Sum 12 32 44\n\nThe results showed that the differential testing achieved an accuracy of 84.1% for this sample (5 + 32 / 44), indicating a high overall agreement with the human assessment. However, a closer examination of the confusion matrix revealed a notable discrepancy between precision and recall. Specifically, the testing method demonstrated 100.0% recall in this case study, correctly identifying all 5 instances that humans judged as correct. In contrast, precision was relatively low (41.7%), with 7 false positives out of 12 total positive predictions. This suggests that differential testing reported success on some incorrect patches as well, highlighting the need for manual validation of patch correctness. Despite this shortcoming, the result clearly shows the utility of differential testing in automatically rejecting a substantial number of incorrect patches, which will substantially save the manual validation effort.\n\nKey insights\n\nOur case study revealed several limitations of the current patch generator.\n\nThe root cause may not exist in the stack trace\n\nFrequently, crashes are the result of state contamination that occurs prior to the crash being triggered. Consequently, none of the functions within the stack frames may include the code responsible for the root cause. Since our current implementation requires the LLM to assume that the root cause is located within one of the functions in the stack trace, it is unable to generate an accurate patch in such cases. Solving this problem would require a more autonomous agent which can reason about the root cause on its own with a code browsing capability.\n\nCheating\n\nIn some instances, the LLM resorted to \u201ccheating\u201d by producing patches that superficially resolved the issue without addressing the underlying problem. This can occur when the generator modifies or removes code in a way that prevents the crash from occurring, but does not actually fix the root cause of the issue. We observed that cheating happens more frequently when we request the LLM to retry within the same trajectory. A potential solution to this could be to empower the LLM to say \u201cI cannot fix it,\u201d which may come with a tradeoff with success rate. However, note that most of the cheating was caught in the verification step, highlighting the utility of differential testing.\n\nNeed for enhanced patch verification methods\n\nFuzzing and white-box differential testing have shown that a large majority of generated patches are incorrect when compared to the ground-truth patches. This finding highlights the challenge of generating accurate patches without enhanced verification capabilities. To address this gap, several approaches can be considered:\n\nA patch generator could provide additional code context when querying the LLM for a patch so that LLM can better understand the consequence of a code patch.\n\nA patch generator could make additional LLM queries to verify the perseverance of existing functionality.\n\nA patch generator can attempt to generate multiple valid patches by exploring multiple trajectories in parallel, and let LLM choose the best option that is most likely to be correct.\n\nIn a well-tested real-world codebase, a patch generator can utilize existing tests to validate the patches it creates. This process complements building the code and checking for crash reproduction, allowing the patch generator to retry if a patch fails the tests. The accuracy of the generated patches is largely dependent on the thoroughness of the existing tests.\n\nIn conclusion, while our study has identified several challenges with the current patch generation process, it also opens up opportunities for improvement. By addressing these limitations with innovative solutions, we can enhance the accuracy and reliability of patch generation, paving the way for more robust and effective automated tools.\n\nGet started with AutoPatchBench\n\nAutoPatchBench is now available on GitHub. We welcome pull requests to integrate new/additional agent architectures into the framework, and look forward to seeing how well they perform on AutoPatchBench.", "label": 0}
{"title": "IndieWeb Carnival: multilingualism in a global Web", "url": "https://lifeofpablo.com/blog/indieweb-carnival-multilingualism-in-a-global-web", "content": "IndieWeb Carnival: multilingualism in a global Web\n\nThis post was written in English (en_US).\n\nIt\u2019s been a while since I\u2019ve participated in the IndieWeb Carnival. I\u2019m late writing this post. I saw October was on the topic of multilingualism in a global Web *. I\u2019m very excited to write about this topic for various reasons. Exciting might be an understatement. I\u2019m fucking stoked to write a post about languages!\n\nI grew up as a bilingual in Spanish and English. Then, I learned French in school and the whole travel abroad situation. I consider myself fluent in all three languages. I use them often. As someone who speaks multiple languages, I see the world through different lenses. One could argue that the internet itself is another world. This has allowed me to access different parts of the internet that someone who is a monolingual may not ever get to experience much.\n\nWhen I was growing up and started to learn French: I was very happy to find content so easily. One of the shows, * Code Lyoko * was vital in my learning French. I was able to watch able to watch it in all three languages I speak. It helped make connections and differentiate how the characters expressed themselves in their respective language. Being born and raised in the United States, I have to admit that the multilingual has kept me connected with my roots, and my family and has taught me things. It allows my language skills to evolve as time goes on.\n\nI often use the Mexican version of Google to find the news and look up various things about Latin America. I get more direct to the source. I do this to find more accurate reporting on things happening. When things about Mexico get reported in American media, I often question various aspects of the reporting. American media tends to picture Mexico as this scary place as a whole.\n\nWhen I was a kid, It was hard to find information about my parents\u2019 hometown in Mexico. Trying to find detailed information in English was next to impossible. Wikipedia in Spanish is always a great place to start! It\u2019s so much more satisfying as the page has more information and a lot more footnotes. What better place than to look at a website by the Mexican government to give me the census data of my parent\u2019s hometown? You can\u2019t find this information in English!\n\nThe web shouldn\u2019t be dominated by the English language. I\u2019m aware it\u2019s the lingua franca. The internet is becoming homogenized. A world being homogenized is a terrifying thing to think about. We\u2019re losing so much culture. So much perspective. Think of it this way: the internet is losing its spice. It\u2019s becoming bland. We should be proud of the languages we can present ourselves in. That\u2019s what makes the internet fun! Does everything need to be in English? Absolutely N-O-T!\n\nI must admit that I am guilty of not writing as often in different languages. I do write the occasional post in Spanish or French. At the same time, I feel as if I\u2019m contributing to the situation. This month\u2019s topic has motivated me to continue to write in Spanish and French more. I want to continue to express myself. It\u2019s important to write in different languages other than English. When I write in different languages, I express myself as me but at the same time, it\u2019s a different me. French Pablo presents himself differently from Spanish or English Pablo.\n\nThank you ~zinricky for hosting this month\u2019s IndieWeb Carnival on multilingualism in a global Web", "label": 1}
{"title": "A new level unlocked", "url": "https://blogs.microsoft.com/blog/2025/02/19/a-new-level-unlocked/", "content": "Muse, the first World and Human Action Model, could facilitate interdisciplinary collaboration, for example, when exploring gameplay ideas.\n\nToday Microsoft released Muse, a first-of-its-kind generative AI model that we are applying to gaming. But it\u2019s so much more than that. What we\u2019re sharing today is a huge step forward for gameplay ideation. And what\u2019s even more exciting is what this breakthrough represents in our journey of building and using generative AI, and what industries, developers and creators of all interests will be enabled to do next.\n\nThe impressive abilities we first witnessed with ChatGPT and GPT-4 to learn human language are now being matched by AI\u2019s abilities to learn the mechanics of how things work, in effect developing a practical understanding of interactions in the world. As a computer scientist, this ability to understand and model a 3D world is something I and many other great researchers have pursued for over 10 years and, personally, I was not sure that it could be made possible with such speed and quality.\n\nIn the case of Muse, just from observing human gameplay, this model develops a deep understanding of the environment, including its dynamics and how it evolves over time in response to actions. This unlocks the ability to rapidly iterate, remix and create in video games so developers can eventually create immersive environments and unleash their full creativity.\n\nBeyond gaming, I\u2019m excited by the potential of this capability to enable AI assistants that understand and help visualize things, from reconfiguring the kitchen in your home to redesigning a retail space to building a digital twin of a factory floor to test and explore different scenarios. All these things are just now becoming possible with AI. From the perspective of computer science research, it\u2019s pretty amazing, and the future applications of this are likely to be transformative for creators.\n\n\u2014\n\nAt Microsoft, we have a long history of collaboration between research and engineering. Today, as we release Muse, we are also announcing Azure AI Foundry Labs, where the AI community can explore the latest from Microsoft Research. Azure AI Foundry Labs will help accelerate the transition from research to solutions, bringing new ideas to the broader community to help shape the future of AI. Learn more.\n\nTags: AI, Azure AI Foundry Labs, ChatGPT, GPT-4", "label": 0}
{"title": "Enjoying different cultures", "url": "https://lifeofpablo.com/blog/enjoying-different-cultures", "content": "Enjoying different cultures\n\nThis post was written in English (en_US).\n\nIn the United States we are exposed to many different cultures. Examples of different cultures are the hispanic culture, italian, french, etc. Especially here at UNK we are more exposed than ever to different cultures.\n\nToday I want to talk about how people should be open minded and talk about how one should expose themselves too a different culture. Many of us who are small town people never really grew up with people of a different race or background. There is no harm in that. Once we all came to college we saw how diverse our world really is.. For me I've grown up in two cultures parallel to each other. I grew up in a very hispanic family while being born in to the American culture that I have known since the day I was born. So I could say that I am used to different cultures.\n\nThrough out our lives, we find people who are different from us in various ways. Different can mean being from another country. Being from a different country you have different aspects of the world. Being different is good. Those who are totally different from you are International Students.\n\nI say one should befriend an international student. Help them out. It would mean the world to them. You thought you were scared when you went off to college, they are probably three times as scared since they are in a country that they have never been in before. They will be greatful when you do help them. I know if you ever need a favor from them, they will gladly go up and beyond to help you suceed if you need help from them. It does not hurt to hangout with them. They will learn so much just being around you. You will as well. It is a win-win situation. Take them out to lunch sometime so you get to know them. They will gladly prepare a meal for you sometime. You may not think about it now but they could be of future help when needing to get a job, if they are already in the workforce.\n\nI love to learn about the other cultures. It fascinates me how different and similar cultures are. Me being the avid FOOD LOVER <3 I love to \"\"test\"\" new food for my belly to enjoy. I did not realize that I loved Japanese foodso much. I think that best part other than the insides is the seaweed to make some of them sushi entrees. NO one has anything to lose by opening themselves up to something different. You know what they say\n\nYOU NEVER KNOW UNTIL YOU TRY!\n\nI say go for it!\n\nWhile you can enjoy a culture festival. Here at UNK we have many of these. The most recent one I went to was the Japanese Food Festival. I learned so much about the Japanese culture. I got to see what their traditions truly embrace than just rely on stereotypes. I have made many Japanese friends as well. Being around theses students, you really get a one to one learning opportunity. It really teaches others not to be afraid and to always remember where one came from originally.\n\nThe food was great! Job well done to those in charge of the event. It was a one to remember. Who know it would of been SO much fun to make Japanese food. I want more! Someone hook me up!!\n\nMaybe I'll see you guys at the International Food Festival this Sunday!\n\nCheck out my gallery!!\n\n\"", "label": 1}
{"title": "A Letter Regarding Budget Cuts at the University of Nebraska Kearney", "url": "https://lifeofpablo.com/blog/letter-to-unk", "content": "My Letter to UNK and NU System\n\nThis week I found out that the university \"needs\" to perform a $3 million budget cut at the University of Nebraska at Kearney (UNK). This is angering and saddening. This creates a huge hole on the UNK campus. I graduated from the university in 2019. I graduated with a B.A. in Modern Languages. This hurts to hear that many programs such as the French program are being cut. The Modern Language Department was my home during my time at the University. It pushed me out of my comfort zone that led me to even great worldview experiences such as study abroad. The French program in Modern Languages helped prepare me to become a teacher and helped to interact with people of all backgrounds and cultures.\n\nUNK needs to have a complete modern language department to prepare students in this globalized world. Learning a language isn't about learning simply a language. Languages expand a person's worldview. You learn culture, you learn experiences, you learn how different societies work. We can't have young people's viewpoints and perspectives stuck glued to the environment they are in. They need to branch out. Going to college is a place where people learn to build deeper connections with others. This includes people they interact with daily and people who are different from them culturally. The modern languages department is such a vital program.\n\nWhat hurts more is the attack on the arts and humanities. The arts and humanities are important for a well rounded education. We need students to explore subjects outside of their majors. I learned a lot from the humanities and the arts.\n\nI ask the University of Nebraska not to eliminate these programs. If the entire university system wants to retain more people in Nebraska, before entering, during college, and post college, we must allow these opportunities.\n\nUNK is the only proper public university in Nebraska that geographically serves so many students in the western half of the state. UNK offers so many programs closer to home that many in years past had to attend schools further away from home to enter the programs needed.\n\nI am proud to be a graduate of the University of Nebraska at Kearney. This saddens me to see UNK and the entire university system go through this dark time. Please reconsider all the damage that is to come. Every single higher position university administrator and faculty must fight for the interest of students - current and present.\n\nRepresentatives of the Nebraska University system need to find and pressure funding from the state. The state has done so much damage from cutting budgets year after year. We must fight those who are against public education and higher education who put us in a chokehold.\n\nWe are currently facing so many attacks on public education and higher education from various places. The University of Nebraska system is no exception.\n\nPablo Morales\n\nUniversity of Nebraska at Kearney Graduate 2019", "label": 1}
{"title": "Update on College.", "url": "https://lifeofpablo.com/blog/update-on-college", "content": "Update on College.\n\nThis post was written in English (en_US).\n\n\"Junior year of college is the turning point of most people's education. It is the time where the most people are deep into their major. You know, the point where it seems too late to turn back. This is not the case for me.\n\nI've changed my major various times to in the last two years. It has not been the easiest process. In most cases it revolved around me jumping back and forth between different areas of science or healthcare related fields to humanities. Not deciding what I wanted in my life has had an effect on my greatly. Many friends know to what extent this led too. Thinking back on this tossing and turning made me realize how I truly did not have a passion for the sciences. You know what it really showed me? I am a person who is very stubborn. I was in denial. Deep down inside, there was no want for me to continue in the sciences. My mindset was not open to explore other options. I felt like a failure if I did not become something in the sciences or be something in healthcare. Avoiding disappointment and failure had become the way my life driving towards.\n\nThere comes a time where you have to wake up and come to reality that you really do not have interest in what you are shooting for after attempting various attempts. I should have followed my heart from the very beginning. My advice to you,\n\n\"\"Do not let anyone tell you what you should study. Only y0u know what you want in life.\"\"\n\nI did that mistake and did not follow my dreams right away. It took a lot of courage for me to stand up to myself. It is sort of shocking, huh? The mind is is your worst enemy. I am glad that I did what I did.\n\nNow I am following my passion. A passion that I have always known to have since the first time I took this class in high school. There is this a major sense of relief. I announce that I will fully commit to become a teacher and strive for what I feel that I will do best\n\nThank you for the support.\n\nWith much love,\n\nPablo Morales\n\n\"", "label": 1}
{"title": "Multilingual innovation in LLMs: How open models help unlock global communication", "url": "https://developers.googleblog.com/en/unlock-global-communication-gemma-projects/", "content": "We are thrilled to celebrate the incredible contributions of the community to the Unlock Global Communication with Gemma competition on Kaggle! Developers tackled the critical challenge in AI of adapting state-of-the-art large language models (LLMs) for diverse cultural and linguistic contexts.\n\nModels often exhibit a bias towards high-resource languages due to the predominant language of their training and evaluation datasets. This can lead to a performance gap, where the latest AI advancements may not be realized in lower-resourced languages. Additionally, these models may not only lack understanding of the language, but also culturally-relevant context that would make these models helpful for the communities.\n\nWe were incredibly impressed by the community's creative solutions for translation of languages, lyrics, old texts, and more.\n\n\n\nHonoring the innovators\n\nThrough hundreds of submissions, developers demonstrated how to bring the transformative power of LLMs to languages everywhere. Projects leveraged custom datasets and efficient post-training methods to adapt Gemma for instruction following, translation, and specific domains. We encourage you to explore the notebooks on Kaggle to see these techniques in action and apply them to your own multilingual projects.\n\nThe first place project adapted Gemma for Swahili understanding, opening up new possibilities to reach 200+ million language speakers. Gemma models were fine-tuned using parameter-efficient fine-tuning techniques for the 2B, 9B, and 27B parameter sizes.\n\nA key aspect of their tuning was Gemma\u2019s \u201cremarkable flexibility in instruction-response formatting,\u201d which allowed the models to parse instructions with minimal structural constraints and generate coherent responses across different input formats.\n\nKnowledge Yielding Adaptive Retrieval Augmentation (Kyara) explored retrieval processes for LLM fine-tuning, demonstrating how to enhance Gemma\u2019s ability to generate informed responses in Traditional Chinese.\n\nThe project focused on building high-quality question & answer (Q&A) datasets using a graph-based approach to knowledge retrieval, inspired on how humans learn by connecting concepts.\n\nThe project fine-tuned Gemma for Arabic language tasks, including translation, summarization, storytelling, and dialogue generation.\n\nAs a language with a rich historical past, the project also aimed to enhance comprehension of older forms of Arabic used in literary texts and art, employing multiple techniques to bridge tasks between Modern Standard Arabic and Classical Arabic.\n\nThis project focused on improving Italian language understanding for Gemma using a cost-effective post-training approach that addresses pitfalls such as hallucinations and catastrophic forgetting.\n\nThe 2B and 9B model sizes were fine-tuned on a mix of data, including a new instruction tuning dataset created using LLM-as-a-judge to ensure the quality of translations.\n\nThis project developed an \u201cAncient Chinese Expert\u201d using Gemma to understand and generate translations for ancient Chinese texts, highlighting the potential of LLMs for historical cultural preservation.\n\nThe model was fine-tuned on a comprehensive dataset to improve linguistic understanding, and post-training included techniques to improve instruction following.\n\nThis project tackled nuanced challenges specific to AI-driven lyric translation, enhancing Gemma\u2019s sensitivity to cultural references and symbolic language, while also ensuring rhythmic fidelity to the original song.\n\nA multilingual dataset contained lyric translations annotated to capture crucial cultural context, emotional tone, and rhythmic features, enabling the model to grasp and replicate the artistic depth of lyrical content.\n\nThis project adapted Gemma 2 JPN to generate Yomigana/Furigana, a reading aid for Japanese text and assist language learners or readers encountering complex Kanji.\n\nWhile other rule-based tools currently exist, LLMs can recognize rare Kanji better and \u201cinterpret the context of a sentence, enabling accurate disambiguation of polyphonic Kanji\u201d. The notebook also noted that conversational capabilities had degraded due to training on the singular translation task.\n\nThis project enhances Gemma\u2019s mathematical and logical understanding in Hindi numeric words, which presents a challenge for models to interpret given complex word formations, for example \u201c\u0926\u094b \u0938\u094c\u201d for \u201c200\u201d or \u201c\u0922\u093e\u0908\u201d for \u201c2.5\u201d.\n\nThe 9B model was fine-tuned on a curated and human expert-verified dataset featuring a wide array of question types, unlocking uses for AI-driven educational tools, automated tutoring, and localized content\n\nThis project fine-tuned the Gemma 2 9B model for translation tasks in Kazakh. A language written in three distinct scripts (Cyrillic, Latin, and Arabic), the Cyrillic version requires approximately twice as many tokens as English, presenting a challenge for training with limited resources.\n\nModel performance showed better benchmarks than the 27B Gemma variant and Google Translate, demonstrating how to adapt LLMs for underrepresented languages using a cost-effective approach.\n\nThis project enables Gemma to understand and translate Old English, the earliest recorded form of the English language. A custom dataset with Old English-Modern English language pairs was created to help tackle the challenge of working with historical languages and limited publicly available data.\n\nThe notebook also features a bonus audio generation component, based on an open-source Icelandic text-to-speech model, offering an approximation of how speech might have sounded.\n\n\n\n10 more awesome projects\n\nGemma 2 Reasoning for Japanese Math: This project created reasoning variants to perform chain-of-thought processes and handle complex problems.\n\nMultitask Gemma2 Agents - Summarise & Translate: This project focused on developing agents capable of multiple tasks.\n\nKorean AI Doctor Gemma2: This project adapted Gemma for medical applications in Korean.\n\nGemma Fine-Tuning for Ru-En Medical Translations: This project enhanced Gemma translation accuracy in ophthalmology.\n\nGemma PT: This project fine-tuned the ShieldGemma content classifier to detect prejudice and disinformation in Portuguese.\n\nHow to Fine-tune Gemma 2 for Advanced Reasoning: This project enhanced Gemma reasoning capabilities by implementing the Coconut (Chain of Continuous Thought) paradigm.\n\nFinetune Gemma Turkish Chat: This project fine-tuned on Gemma on a Q&A dataset to improve accuracy and conversational ability.\n\nFinetuning Gemma2 Customized Dataset: This project fine-tuned Gemma for English-Arabic translation and medical understanding.\n\nGemma-2 Finetuning on Telugu News Dataset: This project adapted Gemma to generate Telugu headlines from news articles.\n\nFinetuned Gemma2 9B Math Reasoning Model Russian: This project enhanced Gemma performance for math problems in Russian.\n\n\n\nLooking ahead with Gemma 3\n\nWith over 7,000 languages spoken worldwide, the potential for AI to bridge communication gaps is immense. The Gemma open model family provides a powerful foundation for developers to adapt high-performing models to low-resource languages.\n\nThe innovation and dedication demonstrated by the Kaggle community in adapting Gemma 2 for various languages are truly inspiring. As we continue to build a future where AI empowers global communication for everyone, we're excited for Gemma 3, which brings pretrained support for over 140 languages, making it a great foundation to build on.\n\nWe encourage developers to explore the possibilities of Gemma, to share their datasets and models with others, and continue to advance multilingual AI together.", "label": 0}
{"title": "Electrifying my house: the first year with a heat pump \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2024/01/28/electrifying-my-house-the-first-year-with-a-heat-pump/", "content": "A year ago, we had our gas furnace and water heater replaced with a heat pump and electric water heater. They both really needed replacing. We\u2019d hoped to have it done before the winter started since our furnace had already been on the fritz for several years \u2014 but the equipment was on backorder.\n\nMy energy utility, Puget Sound Energy (PSE), wants people to stay on gas, so there weren\u2019t incentives from them for this switch; however, my city developed a bulk purchase and incentive program to assist homeowners in transitioning to support the community\u2019s climate goals. This upgrade electrified our whole house, so we could shut off our gas service!*\n\nNow that I\u2019ve had a heat pump for a year, I wanted to share my experience, because I had questions and misconceptions that I didn\u2019t get answered beforehand. Tl;dr it\u2019s great, especially the cooling, just works a little differently.\n\n(Heads-up: 2500 word post, but it\u2019s got graphs! \ud83e\udd29)\n\n*Technically we still have a gas fireplace installed, but we never used it. PSE said it was safe to simply turn off even if that means there might be a small amount of gas remaining in the pipes under the house.\n\nWhy we got a heat pump\n\nOur old setup was failing\n\nBefore the upgrade, we had a ~20 year old gas furnace and an old gas water heater. In November 2019, a pipe on our water heater ruptured, spraying a ton of scalding water into our garage ceiling, filling the entire garage with steam that we had to hire a company to install fans for days to dry out. After that, the furnace began to fail regularly, falsely tripping the safety shut-off and needing to be restarted multiple times a day. We had people out to fix it multiple times, but the most they could figure was that something inside was corroded and they\u2019d clean it to buy us another few months.\n\nMy husband\u2019s telecommuted for years, and during the pandemic, I started working from home too \u2014 but the only \u201ccooling\u201d we had was a ceiling fan in our living room and dining room. We bought a used portable electric window A/C unit to keep my husband\u2019s office from turning into an inferno, and hefted it down the hall to our bedroom in the evenings. Less than ideal.\n\nNatural gas is terrible for the planet and for people\n\n\u201cNatural gas\u201d is the greenwashed industry name for methane, an extremely potent greenhouse gas. A lot of methane is fracked, which may have impacts on the health of surrounding communities. Methane is piped, often through First Nation people\u2019s lands without permission \u2014 think Keystone XL and Canada\u2019s Coastal GasLink pipeline \u2014 where it may leak and harm the water and ecosystem. Also, some methane leaks during fracking and distribution, contributing to emissions.\n\nMoreover, \u201cnatural gas\u201d is propped up by corporate propaganda that cooking with gas is better than electric, despite the atrocious impact methane has on indoor air quality and its contribution to household emissions. I have an electric stove, so I didn\u2019t have to replace mine in order to get off gas.\n\nWithout gas-burning appliances, our risk of carbon monoxide poisoning is lowered; it would come from some other source, like operating a propane stove indoors (DO NOT DO THAT THING). My husband knows someone who has brain damage from carbon monoxide poisoning!\n\nBasically, \u201cnatural gas\u201d is fucking shit and I want fucking nothing to do with it.\n\nMy new all-electric home energy setup\n\nMy house is about 1400s.f. and has two stories. It was built in the 1980s and our attic doesn\u2019t have enough insulation (we\u2019ve replaced insulation in the basement but not yet dealt with the attic).\n\nMitsubishi heat pump air handler (SVZ-KP24NA) \u2014 24,000 BTU/hour\n\nGeneral Aire residential whole-house air cleaner (GFI #4422 or 4424)\n\nMitsubishi wall head (split-type air conditioner MSZ-GL15NA) \u2014 one unit installed on second floor \u2014 we considered a second but decided to save $$$\n\n\n\n50-gallon electric water heater \u2014 the installer recommended against tankless despite what I\u2019d read online \ud83e\udd37\u200d\u2640\ufe0f\n\n18-panel / 5.04kW solar system (Itek IT280 photovoltaic modules and Itek Theia HE\u2010t UL 3.8 kW inverter) \u2014 installed in 2015\n\nThe air handler connects to our existing duct system. There\u2019s the outdoor unit, which we placed on the side of the house directly below the upstairs heat pump head, and there\u2019s also the air cleaner in the garage where the old furnace was.\n\nThe city bulk purchase program we went through offered Mitsubishi heat pumps, and we just went with what the contractor recommended.\n\nHow much did the heat pump and electric water heater cost?\n\nI am located in the Seattle area, which has been in a housing boom for a while so contractors are pretty expensive.\n\nThe heat pump cost $19,445 + 10.2% tax for equipment and installation. The permit cost $236.*\n\nThe electric water heater with new dedicated 220v circuit cost $2,145 + 10.2% tax.\n\nOur city provided a $500 incentive and coordinated a $1,200 discount for bulk purchase of equipment, so in total we paid $22,155 for both heat pump and new water heater.\n\n*The permit included an inspection, which failed the first time \u2014 we wouldn\u2019t have known what wasn\u2019t installed quite right, so thanks inspector! He was super helpful and pointed out some other unrelated things he noticed that we might consider acting on.\n\nHeat pump vs. gas furnace energy use comparisons\n\nMy situation will not be directly comparable to most people\u2019s because we have a solar array, but hopefully will still provide a relative comparison. I have provided the electricity we used, which includes power we generated and used directly, but not power we generated and sold back to PSE.\n\nUnfortunately since we upgraded the water heater at the same time, we can\u2019t isolate only the cost of the heat pump \ud83e\udd37\u200d\u2640\ufe0f\n\nObnoxiously, our power bill comes mid-month. To make life easier for myself in the graphs, I\u2019ve represented the month the bill was issued. That typically covers usage from the 18th of the previous month through the 17th of the billing month \ud83d\ude15\n\nContext: weather and energy trends in the Pacific Northwest\n\nFor usage context, in the Seattle area, it\u2019s relatively cool (and we run heating) from mid September through early June. Summer is usually relatively moderate, typically peaking with one or two weeks in the 90s (F). Over 100F is unusual.\n\nHow much more electricity did the heat pump use?\n\nThese data represent solely our electricity use, so you can guesstimate how much electricity we use for heating (and cooling) by how much our electricity increased after January 2023. During the cold months (December through March bills), we used about 1200-1500 additional kWh (for heating our space and water). Over the whole year, we used an additional 7300kWh compared to the previous year. (According to PSE, it\u2019s typical to use about 2800kWh annually for an electric water heater and 6800kWh for a heat pump, or 9600kWh annually.)\n\nYou\u2019ll see that we had a big jump in electricity use in January 2024 (technically mid December through mid January). This coincides with an extremely cold snap for Washington, with temperatures in the ~19 degree Fahrenheit range for about a week. We didn\u2019t adjust our thermostat during that time period, but did run the heat pump head overnight as well as pulled out a small standalone electric space heater to help keep the house warmer during the day.\n\nMonthly kWh electricity used, Sept. 16 2021-Jan. 17, 2024 Month 2021 2022 2023 2024 kWh difference\n\nwith heat pump 1 499 495 1946 1451 2 419 *1044 *625 3 545 1737 1192 4 681 1617 936 5 695 1120 425 6 531 993 462 7 1003 1213 210 8 1212 1059 -153 9 868 973 105 10 577 721 880 159 11 466 541 1274 733 12 430 500 1651 1151\n\nbold = electric only \u2014 heat pump installed January 30, 2023\n\n* About half the February 2023 billing cycle was on gas heat\n\n\n\nHow much methane usage did we eliminate?\n\nIn 2022, we used 770 therms of methane.* Now, we use none! \ud83d\ude4c\n\n*According to PSE, a typical house uses 690 therms for a gas furnace and 160 therms for a gas water heater, or 850 therms annually.\n\nComparing gas therms and kWh electricity\n\nA therm is equal to approximately 29 kWh. Unless I\u2019m being totally unreasonable in converting and combining these values, here\u2019s how the overall kWh shakes out \u2014 a massive reduction in energy used with the heat pump (Feb 2023-Jan 2024):\n\nHow much did it cost to run the heat pump versus the gas furnace?\n\nThe first month we used the heat pump for cooling, I was braced for a radical energy bill, but thanks to our solar panels, we continued to pay the minimum electrical charge in summer \ud83e\uddbe\n\nIn actual costs, we paid $1435 for energy (gas and electricity) in 2022 and $1158 in 2023 (heat pump only); if we assumed that the four summer months of 2022 were $7.95 as in a normal year, that would still put 2022 at $1215. And if we swapped in this year\u2019s all-electric January bill for last year\u2019s gas-heated bill, that\u2019s basically a wash. And now we have whole-house cooling in summer \ud83d\ude0e\n\nMonthly energy bill by year, Sept. 16 2021-Jan. 17, 2024 Month 2021 2022 2023 2024 1 $218.41 *$229.79 $260.96 2 $158.67 $202.26 3 $148.30 $188.41 4 $108.26 $91.73 5 $79.85 $28.09 6 $51.67 $7.95 7 *$36.18 $7.95 8 *$76.16 $7.95 9 *$86.90 $7.95 10 $71.32 *$84.62 $46.02 11 $101.87 *$161.19 $136.05 12 $143.28 *$224.62 $204.68\n\nbold = electric only \u2014 heat pump installed January 30, 2023\n\n* = our solar inverter was not selling electricity back to PSE these months, though was generating power for pass-through usage \u2014 in a normal year the summer month costs would be the minimum $7.95 \u2014 this also means there was no bank to draw from for those fall and winter months\n\nWhat I wish we\u2019d known before switching to a heat pump\n\nIt heats differently than a gas furnace: slow and steady versus bursts of heat\n\nThe first month we had our heat pump, in the cold of February 2023, I was convinced it was broken or we\u2019d been sold too small of a unit. If you go in with expectations that it\u2019s going to be a different heating experience than a gas furnace, you\u2019ll be happier than I was at first \ud83d\ude09\n\nBeing used to a gas furnace, we were used to saying, \u201cIt\u2019s cold!\u201d and bumping the thermostat* up a couple degrees, then getting the immediate feedback of a big blast of hot air and a relatively quick increase in temperature.\n\nIn contrast, the heat pump blows air lightly all day long**. If we increase the thermostat setting, it will take several hours to reach the new temperature. Likewise, when it\u2019s too cold out, sometimes it has a hard time keeping up. (I\u2019m hoping that upping our attic insulation will help hold the temperature better.)\n\n*We had a Nest before, but I don\u2019t think it actually helped save energy since we were home all day every day, so between that and Google\u2019s creepy surveillance I was fine when I found out the heat pump came with a thermostat and wouldn\u2019t work with the Nest. Unfortunately, the included thermostat is kind of annoying to program.\n\n**Because heat pumps blow constantly, the contractor recommended keeping as many vents open as possible \u2014 closing no more than one or two. (There was a technical reason for this that I forget, something about making part of the heat pump get too cold and shut down.) Unfortunately, whoever installed our original HVAC system was, er, clearly the low bidder, so we have wacky vents directly underneath windows and doors all over the house \ud83e\udd26\u200d\u2640\ufe0f The register that blows the hottest is our downstairs bathroom \ud83d\ude44\n\nSo, with the heat pump, we don\u2019t drop the temperature overnight as much as we did with the gas furnace. We keep the house a more consistent temperature all the time. Then, the poor heat pump isn\u2019t always struggling to keep up, it just holds what it\u2019s got.\n\nThe heads aren\u2019t as quiet as they\u2019re made out to be\n\nWhoever says they\u2019re basically silent hasn\u2019t paid much attention. The heat pump head (which is in our bedroom) makes mysterious noises pretty often, particularly when in heating mode*. My poor cat Mina stares at it, transfixed in horror, as it tinkles like broken glass going through the wash, whooshes like the house is blasting off, brrrs to life unexpectedly, and whirs as the louvers rotate up and down randomly. It sometimes makes noise even when the unit is turned off, which is confusing.\n\nThe fan noises are relatively quiet, especially in comparison with a window AC unit, so that\u2019s probably the frame of reference people have when they talk about it being silent. But it\u2019s not silent if you\u2019re changing from forced air, which actually is basically silent \ud83d\ude09 Overall, it\u2019s not too intrusive and totally worth it for being able to fine-tune the comfort level of our bedroom, but not quite what I had anticipated.\n\n*I am pretty sensitive to noise, but my husband and cats agree on this one \ud83d\ude09\n\nOur verdict: the heat pump is a huge win for summer, good for winter\n\nIt kicks ass in summer\n\nWow, after previously not having cooling in summer, it was incredible to be able to comfortably use my office after like 3pm in the summer \ud83d\ude02 I was especially the beneficiary because in the past my husband got the portable A/C unit since his office was hotter.\n\nThe heat pump couldn\u2019t keep up with the hottest days in the high nineties, but even then still kept the house in comfortable mid to high eighties \u2014 totally fine for us, especially with a ceiling fan.\n\nGood in normal cold, fine in cold cold\n\nOnce we learned how to program the thermostat appropriately, we\u2019ve been happy with how the heat pump keeps the house warm.\n\nI\u2019ll be honest: this thing struggles with the cold cold (below freezing). From my understanding, actually cold places use a backup furnace IIRC, but the temperature\u2019s generally moderate enough in Seattle that we get by with just a heat pump and suck it up for the one or two extra-cold weeks a year.\n\nDuring our recent cold snap, when it was 16 degrees Fahrenheit overnight one night, the house was 55 degrees when we went downstairs in the morning despite the thermostat being set to 68. It didn\u2019t heat up much during the morning, so we pulled out our little electric space heater and an electric heating pad to boost the warmth, and bundled up with down vests and blankets. This is where the heat pump head is key: we were able to keep our bedroom, where the head is located, comfortable overnight (though we did add extra blankets).\n\nAnd honestly? The heat pump\u2019s so much more efficient, better for the climate, and safer for us that wearing a jacket for a week or two in the winter seems like a totally reasonable tradeoff.", "label": 1}
{"title": "IndieWeb Carnival February 2024 - Digital Relationships", "url": "https://lifeofpablo.com/blog/indieweb-carnival-february-2024-digital-relationships", "content": "I enjoy making friends and making connections with people of all backgrounds and cultures. Growing up, I'd make friends with the international exchange students. They show up for a year or so and then leave. Many I've stayed in touch with and have gone to visit them in their home country. I would consider them close friends. We've kept this friendship even if they are across the world.\n\nAs a person, I would say I\u2019m a very affectionate person. I like to form bonds and form some level of intimacy. Intimacy can be physical, emotional, intellectual, spiritual or experiential. The idea of intimacy does not need to be sexual. I\u2019m very affectionate to those with whom I have a close relationship, such as my best friends. I would also consider them very affectionate as well. I guess we can say we are in touch with our emotions and be expressive towards each other. We\u2019ve shared so many moments together. That\u2019s why we\u2019re friends. As we\u2019ve become older and started our careers, our lives, our relationships, etc, we\u2019ve all relocated and don\u2019t see each other as often as we used to. We all find ourselves living in various parts of the United States. It\u2019s a sad reality growing up. Yet, here we are. It doesn\u2019t seem that our bonds have weakened.\n\nHaving these human relationships or interactions is the main piece of keeping our relationships so strong. Many of my relationships with people I have rely on digital relationships as well. It's how many of our relationships keep strong.\n\nMany of us desire a form of intimacy and emotional bond from those important to us. Oftentimes the only way to get these is through a virtual medium. We want to foster deep emotional connections with the absence of physical proximity. We want to share those important moments with a phone call or a video chat. We want to support each other in hard times. We want to replicate those movie nights when we were all living together. Virtual relationships allow us to provide emotional support from afar with our personal struggles, external stressors, or societal issues. This strengthens the emotional bond between individuals. Trust becomes a cornerstone that supports the vulnerability inherent in forming emotional bonds.\n\nIt's possible to have meaningful virtual relationships as we adapt with the changing times and remember the human in relationships.", "label": 1}
{"title": "December 2014", "url": "https://lifeofpablo.com/blog/published:2014-12", "content": "en\n\n\"\n\nSo who is freezing their but off? That would be me! Well we can say that winter has officially kicked in, so has the break. So it is the time to throw your shoes and load up that good ol' Netflix account. We all need to catch up on the shows that we have abandoned lately.\n\nIt has been a while since I wrote last. Let me tell you, it has been a quite the interesting semester.I stayed busy with all my activities especially with my fraternity. I'm a now an initiated member of Alpha Tau Omega. Being part of Greek life, it has opened me many doors. On my free time, I've been working out a lot lately. Since the beginning of the school year, I managed to lose over 20 pounds. What a difference it has made! I feel great. I want to thank my workout buddy for helping me out!! Props to you man for putting up with me!\n\nAll my finals went well. Let's say that I was not myself during dead week. Maybe I can make a zombie reference?? I passed all my tests and ended on a well note for the semester. There is always room for improvement.\n\nThe first day back was just filled with a lot of things to do and with a lot of surprises. Good things after another occurred.\n\nIt feels great hanging out with my family. I've missed them so much. While back in the dorm, it seemed that I really did not missed them much. Once I got home, I realized how much they were truly a missing part of me. I missed my mom's home cooked meals. What a relief that I can stay away from the cafeteria food.\n\nThen I went to my high school to see the varsity basketball game. Go Tigers!! They kicked some ass when they beat York 65-38. The game was intense. How I miss the basketball games. Once a tiger always a tiger! It was great seeing some old friends.\n\nI am glad to have made many friends this semester. I appreciate everyone of you. Let's see what is in stock for us this coming semester.\n\nHappy Holidays everyone!! Stay warm and make a snowman (if we can get enough snow.)\"", "label": 1}
{"title": "How GitHub supports neurodiverse employees (and how your company can, too)", "url": "https://github.blog/engineering/engineering-principles/how-github-supports-neurodiverse-employees-and-how-your-company-can-too/", "content": "In today\u2019s global workplace, supporting employees by appreciating and understanding their background and lived experience is crucial for the success of any organization. This includes employees who are neurodivergent. Neurodivergence refers to natural variations in human brains and cognition. The term encompasses conditions such as autism, ADHD, dyslexia, mental illness, and other neurological differences.\n\nNeurodivergent employees don\u2019t just enrich the workplace, they\u2019re good for business. According to Deloitte, teams with neurodivergent people can be up to 30 percent more productive than others. Neurodivergent folks excel in pattern recognition and the type of outside-the-box thinking highly sought after in the software industry.\n\nIn this blog post, we\u2019ll take a look at five ways GitHub fosters and supports neurodiverse employees via Neurocats, a GitHub Community of Belonging (CoB), and how you can do the same at your organization.\n\nLet\u2019s go!\n\nForktocat: An Octocat image that represents the fork function in Git, which we\u2019ve adopted in Neurocats to represent the different ways our brains work.\n\n1. Establish supportive communities\n\nAs an initial step, establish private, supportive communities where neurodivergent employees can connect, share their experiences, and find support. GitHub\u2019s Neurocats community allows members to privately discuss their neurodivergence, offer advice to each other, and build a sense of belonging, all in a safe place where members can freely express themselves without fear.\n\nNeurocats started as a private Slack channel under a different name years before it formally transitioned into a CoB. Originally called #neuroconverse, it gave the neurodivergent community at GitHub a space to chat. In the summer of 2021, a collection of passionate members started discussions with GitHub\u2019s Diversity Inclusion and Belonging team about becoming a formal CoB. In October 2021, they formed as an official group at GitHub, and after some discussion, became the Neurocats. The community now consists of hundreds of members from across the company and continues to grow.\n\nSetting up spaces for neurodivergent individuals to express themselves and meet other like-minded friends and allies not only improves their overall work life balance, it also accelerates the creation of new innovative ideas that could be the next big thing in your organization\u2019s portfolio.\n\n\u201cAs a neurodivergent people manager with dyslexia and dysgraphia, I am thrilled to be part of the Neurocats CoB, a community that embraces and normalizes our uniqueness,\u201d says Tina Barfield, senior manager at GitHub. \u201cBy doing so, we can help drive environments where everyone\u2019s strengths are celebrated, leading to greater innovation, creativity, and inclusivity.\u201d (Please note, all employee names and stories have been shared with permission.)\n\nSuggestions for establishing a supportive community: Have members lead. Embrace the powerful slogan, \u201cNothing about us, without us.\u201d\n\nEmbrace the powerful slogan, \u201cNothing about us, without us.\u201d Consider how to protect confidentiality and anonymity. Many neurodivergent people may want to control to whom and in what context they share their neurodivergence. Always obtain permission before sharing information that identifies a person\u2019s neurodivergence.\n\nMany neurodivergent people may want to control to whom and in what context they share their neurodivergence. Always obtain permission before sharing information that identifies a person\u2019s neurodivergence. Consider conversation history. In Neurocats, conversation history is wiped every four days.\n\nIn Neurocats, conversation history is wiped every four days. Never require diagnosis for membership to the community. In many cultures and countries, getting a formal diagnosis can be difficult if not impossible. If someone believes they are neurodivergent, accept them and support them.\n\n2. Foster a sense of belonging\n\nGiving employees the time and space to discuss their neurodivergence enables them to strongly relate to each other, lift each other up, and make personal discoveries that will help them navigate life both at work and at home.\n\n\u201cI didn\u2019t know what being neurodivergent was before Neurocats,\u201d says Lou Nelson, support engineer III who works on GitHub Premium Support. \u201cI thought I was a weird kid with an ADHD diagnosis. Neurocats has become the lynchpin for my career. I have made valuable connections and have a deeper insight into myself than I could have ever done alone. As a member, I find it incumbent to share this experience with others so that they also don\u2019t have to feel alone.\u201d\n\nWhen neurodivergent employees feel comfortable enough to share their stories more broadly, other employees will be drawn to those communities to either personally relate or learn and empathize about subjects they may not have previously considered.\n\n\u201cAs a people manager with ADHD, I\u2019m accustomed to being the \u2018neurodiversity pioneer\u2019 when meeting new teams or direct reports, setting an example by speaking openly about my gifts and challenges,\u201d says Julie Kang, staff manager of software engineering at GitHub. \u201cWhen I joined GitHub, and especially when I became a Neurocat, I was pleasantly surprised to find a culture that was knowledgeable, accepting, and celebratory of neurodiversity at a level I haven\u2019t seen before in my career.\u201d\n\nSuggestions for encouraging a sense of belonging in your neurodivergent community: Declare safe spaces. Before meetings where neurodivergence is discussed, call out the fact that you want this to be a safe space. Encourage kindness and empathy.\n\nBefore meetings where neurodivergence is discussed, call out the fact that you want this to be a safe space. Encourage kindness and empathy. Warn about oversharing. If there\u2019s a meeting where oversharing is possible, warn participants to think about their contributions and avoid sharing details that they may later regret.\n\nIf there\u2019s a meeting where oversharing is possible, warn participants to think about their contributions and avoid sharing details that they may later regret. Understand privacy and confidentiality expectations. Participants should respect privacy and adopt guiding principles around privacy and confidentiality.\n\nParticipants should respect privacy and adopt guiding principles around privacy and confidentiality. Share information broadly with the company or organization. Create mechanisms where members of the community can safely share thoughts and feelings with the broader company or organization and, if appropriate, externally. Anonymize this information if required and always share with consent. Examples can include recurring Q&A sessions, presentations, and discussions with the entire company, organization or a segment of the organization all help allow neurodivergent employees to be seen.\n\n3. Provide flexibility and accommodations\n\nNeurodivergent employees can often benefit from flexible working arrangements. This could include flexible hours, remote work options, noise-canceling headphones, or customized workspaces to reduce sensory overload.\n\nAsking for accommodations can be hard. Identify the process your organization or company uses to assess workplace accommodations. Encourage employees to utilize that process to obtain a workplace accommodation.\n\n\u201cOne of the biggest things for me has been seeing how many other folks went through a lot of their life being told that they just needed to apply themselves, pay attention, work harder, etc. only to repeatedly fail out of college, get fired from jobs, and generally struggle to \u2018human\u2019 correctly,\u201d says Caite Palmer, manual review analyst of security operations at GitHub. \u201cThese folks are now through all departments and levels at this large, successful company getting to do great work in a place where flexibility, asking a million questions, and problem solving are generally considered tremendous assets and encouraged.\u201d.\n\nSuggestions for providing flexibility and accommodations: Listen to your employees. Identify listening mechanisms to hear concerns from your employees. An employee or group of employees may share with you how a particular process or system is difficult. Their honesty and candor is a gift and an opportunity to improve your business.\n\nIdentify listening mechanisms to hear concerns from your employees. An employee or group of employees may share with you how a particular process or system is difficult. Their honesty and candor is a gift and an opportunity to improve your business. Consider providing a budget. At GitHub we have flexible benefits that allow employees to purchase equipment and tools which can help them focus, work, and even relax. Removing barriers for employees to obtain bespoke solutions that work for them reduces costs by removing long conversations and approvals from the process.\n\nAt GitHub we have flexible benefits that allow employees to purchase equipment and tools which can help them focus, work, and even relax. Removing barriers for employees to obtain bespoke solutions that work for them reduces costs by removing long conversations and approvals from the process. Train managers to care. Part of the GitHub manager training program is geared to help managers foster a sense of caring and empathy for the employees in their teams. Encourage kindness across all levels of the company.\n\nPart of the GitHub manager training program is geared to help managers foster a sense of caring and empathy for the employees in their teams. Encourage kindness across all levels of the company. Be clear about tasks that need to be completed and flexible on how that task is accomplished. An autistic employee might go for a long walk and then be able to hyperfocus on a task, completing it in half the time than it would have taken if they\u2019d sat at their desk the whole time. Trust your employees to do their best work in the way that fits them and judge them on the work they produce.\n\n4. Encourage open dialogue\n\nPromote a culture of openness where employees feel comfortable discussing their needs and challenges. Consider holding regular meetings and forums to discuss topics related to neurodiversity, mental health, and well-being. With the Neurocats group, we hold monthly meetings to discuss various topics, which are important to our members. One member of the Neurocats leadership team describes their experience:\n\n\u201cWe have a voice, which we use to highlight issues our members face day to day,\u201d says Owen Niblock, senior software engineer at GitHub who works on accessibility. \u201cWe also hold monthly meetings to discuss topics from ADHD and autism to anxiety, mental health issues, and more. Over the years, we\u2019ve had some success and find we are able to lobby for changes at a company level, leading to real tangible change that benefits the whole of GitHub.\u201d\n\nEnabling open dialogue means providing avenues for these discussions to happen. But going one step further and encouraging open dialogue requires more effort.\n\nSuggestions for encouraging open and honest dialogue with the neurodivergent community at your company: Listen. You won\u2019t be able to act on every piece of feedback you receive, but showing you acknowledge and appreciate feedback and making changes whenever possible will create a culture of frank and honest discussion.\n\nYou won\u2019t be able to act on every piece of feedback you receive, but showing you acknowledge and appreciate feedback and making changes whenever possible will create a culture of frank and honest discussion. Make space. Conversations can happen at many levels and in many forums. Create space for a diverse set of feedback and ideas by accepting discussions in different formats. For example, you might have a meeting to discuss something with a follow-up GitHub Discussion to collect async ideas.\n\nConversations can happen at many levels and in many forums. Create space for a diverse set of feedback and ideas by accepting discussions in different formats. For example, you might have a meeting to discuss something with a follow-up GitHub Discussion to collect async ideas. Give access to the leadership team. At GitHub all our CoBs have an executive sponsor who\u2019s a member of GitHub\u2019s senior leadership team. This gives members an ally to raise issues with, help find solutions, and communicate problems directly upwards.\n\nAt GitHub all our CoBs have an executive sponsor who\u2019s a member of GitHub\u2019s senior leadership team. This gives members an ally to raise issues with, help find solutions, and communicate problems directly upwards. Lead from the very top. Provide opportunities for CoBs to meet with senior leadership. For example, this year, each of GitHub\u2019s CoBs have a meeting with GitHub\u2019s CEO as part of a listening tour. This shows that the community\u2019s views are important to the whole business.\n\n5. Celebrate neurodiversity\n\nAcknowledge and celebrate the unique contributions of neurodiverse employees. Recognize their achievements, provide opportunities for career advancement, and ensure they have a voice in the organization. Celebrating Disability Pride Month and other related events can help raise awareness and appreciation within the company.\n\n\u201cNeurocats was the first time I found people like me not only represented at work, but celebrated and successful,\u201d Palmer says. \u201cSharing the rough days, the burnout, the overwhelm and frustration, but also the wins of finally getting appropriate support, being seen as creative instead of weird, and getting to learn about all the different ways brains can function.\u201d\n\nCelebrations should come not just from the community but also from leadership and the People Team. Sharing posts about the company\u2019s mental health benefits during Mental Health Month (in May) or sharing information about the community during meetings or training can all help to celebrate your diverse workforce.\n\nSuggestions for celebrating neurodiversity: Celebrations should come from all levels of the company. An internal post by a member of the leadership team or an informative post by the People team or Human Resources can show your company\u2019s support.\n\nAn internal post by a member of the leadership team or an informative post by the People team or Human Resources can show your company\u2019s support. Don\u2019t sugar coat it. Being neurodivergent can be hard. It\u2019s not a superpower. Celebrate the diversity and the wins, but never minimize the real struggles that many neurodivergent people deal with every day.\n\nBeing neurodivergent can be hard. It\u2019s not a superpower. Celebrate the diversity and the wins, but never minimize the real struggles that many neurodivergent people deal with every day. Make it visible. Share things in the format that makes most sense and will reach the most people.\n\nShare things in the format that makes most sense and will reach the most people. Promote. Ask members of the community to share information on your company blog or website.\n\nBy implementing these strategies, you can create an inclusive environment where neurodivergent employees feel valued, supported, and empowered to contribute their best work.\n\n\u201cNeurocats provided an environment that made me feel safe and confident in an astonishingly short amount of time, allowing me to bring my A game, leverage my strengths, and make a positive impact much sooner than usual,\u201d says Julie Kang, staff manager of software engineering at GitHub. \u201cThe support and understanding here have been truly transformative for my professional growth, and I feel equipped to pay this forward to my peers and reports.\u201d\n\nInterested in learning more about GitHub\u2019s approach to accessibility? Visit accessibility.github.com.\n\nTags:", "label": 0}
{"title": "Customizing scopes in the OAuth app authorization flow", "url": "https://dropbox.tech/developers/customizing-scopes-in-oauth-flow", "content": "As you may know, the Dropbox API authorization system uses \"scopes\" for granular control over what functionality an app can access. This allows app developers to select what API functionality their apps can use, so that users can feel more comfortable granting apps access to their accounts. This can help give users peace of mind that the apps will only be able to perform the operations that the apps actually need. It may not be obvious though that you can further customize exactly which scopes your app requests and when. Let's look at the options for configuring and customizing scopes in more detail.\n\nFirst, it's important to note that the scopes you enable on the Permissions tab of the app's page on the App Console define the maximum, as well as the default, set of scopes that the app can request. For example, let's look at a user-linked app. By default, it has the account_info.read scope, which is required to be registered for user-linked apps. We'll also enable files.content.read and files.metadata.read scopes for this example.\n\nA screenshot showing the app\u2019s scopes configuration.\n\nWhen we send a user to the app authorization page, by default, they'll be prompted to authorize the app with all of those scopes: https://www.dropbox.com/oauth2/authorize?client_id=<APP_KEY>&response_type=code\n\nA screenshot of the app authorization page defaulting to the scopes registered to the app.\n\nHowever, if you don't need all of the scopes that are currently enabled on the app, you can instead set the scope parameter on the /oauth2/authorize URL you construct. In that parameter, you can put a space-delimited list of scopes to specify just a sub-set of scopes to request for that authorization. This can be useful in scenarios where the app doesn't need all of the app's potential access, or as a way to more gradually gain the user's trust.\n\n\n\nFor example, say we just want the app to be able to read the metadata of the files and folders in the user\u2019s account; we would construct the URL like this: https://www.dropbox.com/oauth2/authorize?client_id=<APP_KEY>&response_type=code&scope=files.metadata.read\n\nA screenshot of the app authorization page requesting a sub-set of the scopes registered to the app.\n\nTip: Note how even though account_info.read is required to be enabled on the app itself, you don't have to request it during authorization. For more privacy-oriented scenarios where the app doesn't need access to the user's account information, you can set the scope parameter without the account_info.read scope as above.\n\nIf a user authorizes the app using that /oauth2/authorize URL, the app will then receive a payload like the following when it subsequently makes the corresponding call to /oauth2/token using the resulting authorization code:\n\nCopy { \"access_token\": \"<ACCESS_TOKEN>\", \"token_type\": \"bearer\", \"expires_in\": 14400, \"scope\": \"files.metadata.read\", \"uid\": \"<USER_ID>\", \"account_id\": \"<ACCOUNT_ID>\" }\n\nIf the app needs additional scopes later, it can prompt the user to authorize the app again, with the scope parameter configured with more scopes, or without the scope parameter set at all, to request all of the app\u2019s scopes.\n\nYou can also use the include_granted_scopes parameter to make it easier to request additional scopes without explicitly listing the previously granted scopes again. For example, if we then additionally want the app to be able to read the content of files in that same user\u2019s account, we would construct another URL like this:\n\nhttps://www.dropbox.com/oauth2/authorize?client_id=<APP_KEY>&response_type=code&scope=files.content.read&include_granted_scopes=user\n\nA screenshot of the app authorization page requesting additional scopes registered to the app.\n\nIf the user authorizes the app using that /oauth2/authorize URL, the app will then receive a payload like the following when it subsequently makes the corresponding call to /oauth2/token using the resulting authorization code:\n\nCopy { \"access_token\": \"<ACCESS_TOKEN>\", \"token_type\": \"bearer\", \"expires_in\": 14400, \"scope\": \"files.content.read files.metadata.read\", \"uid\": \"<USER_ID>\", \"account_id\": \"<ACCOUNT_ID>\" }", "label": 0}
{"title": "2023 Year in Review", "url": "https://lifeofpablo.com/blog/year-in-review-23", "content": "2023 Year in Review\n\nPhoto of Pablo\n\nThis post was written in English (en_US).\n\nWhat a year 2023 has been! I've learned so much about myself and hit so many milestones. This will be my first year in review I ever post. I've written brief year in reviews on paper in the past. My year in review for 2023.\n\nI started a Company\n\nI filed and started my own company called, Pabs Tech, LLC. This has been something that had in the back of my mind for a long time. I only did very tiny projects with my company.\n\nI'd like to have more projects where I can help others suceed in there projects.\n\nHelping Friends\n\nI started to help my friends build a small game.\n\nHelp friends with their projects.\n\nBlogging\n\n100 Blog posts! This blog post is number 100 for 2023. This is the most I've written on a personal site. I'm really proud of myself. This year I got back into blogging. I am so happy I did. I wrote a blog post on why I blog.\n\nThe IndieWeb\n\nI joined the IndieWeb this year! I've meet some amazing people who have motivated me in so many ways. I find them inspiring. This is a great communithy to join. I only wish I would have done it sooner.\n\nI got to meet up with an IndieWeb community member while I was visiting Los Angeles.\n\nI've attended almost every Homebrew Website Club (remote) since March.\n\nI started building an IndieAuth extension for Datenstrom Yellow\n\nPodcast\n\nStarted a podcast on my website called Pablo's Thought Podcast on things that I want to share.\n\nPhotography\n\nI really explored photography and videography. I made some major leaps and bounds in both analog and digital photography. My camera visited new cities.\n\nI posted a Photography Year in Review of my favorite shots.\n\nOutdoors\n\nI've gone on 10 hikes this year! Darn it! I live in California and I should make more use of the natural facilities.\n\nTraveling Around\n\nI did some traveling this year. I've been telling myself that I need to travel more of the United States this year even if I have visited these places before. I went to new places in California and went to places in California I've been to in the past. I even went to the same place on the East Coast Again.\n\nI visited two Buzzfeed Offices in Los Angeles and New York City.\n\nI attended NYC Climate Week for the first time\n\nI visited wine country for the first ime\n\nCities:\n\nSan Francisco\n\nLos Angeles\n\nMexico City\n\nNapa Valley (Wine Country)\n\nNew York\n\nOaxaca, Mexico\n\nSan Luis Reservoir, California\n\nThings I have learned in 2023\n\nI'm learned to start loving myself. I started going to therapy. I'm starting to unravel and start understanding why I've seen myself in such a negative light.\n\nIt's okay to make mistakes. Learning from them is key to improve.\n\nFeel less guilty in enjoying time for myself.\n\nBeing active consistently is key. It helps me stay motivated knowing I don't always have to go hard. Consistency is key.\n\nWhat about 2024 ?\n\nI have a lot planned for 2024. I will continue to travel with friends but also go places on my own. I will need to go on more artist dates.\n\nI'm planning to host an IndieWebCamp in Sacramento.\n\nI'd like to run a marathon.\n\nI'd like to learn how to dance.\n\nHere's to a great 2024!", "label": 1}
{"title": "Day 3 - UNK World Leaders Camp", "url": "https://lifeofpablo.com/blog/day-3-unk-world-leaders-camp", "content": "Day 3 - UNK World Leaders Camp\n\nThis post was written in English (en_US).\n\n\"Sleeping at the hotel in was one of the most comfortable places. Who knew old fashion was better? Breakfast wasn't what I was used to. It consisted of A slice of bread with cold cut (deli) meats. You cant forget a great cup of joe! What a simple breakfast? we made kolaches. Kolaches are a traditional food in the Czech Republic. They are formed like a cookie. We make a crater in the center to fill it with fillings. Fillings like apricot or cherry fillings. We also had cream cheese filling. Here is the result.\n\nDelicious right??\n\nThen we went on a tour around Wilber. We learned the history and reasons people moved here. One was religious freedom. (Sound familiar?)\n\nI got to play the bagpipes. It was odd but cool at the same time.\n\n[caption id=\"\"attachment_261\"\" align=\"\"alignnone\"\" width=\"\"225\"\"] Me playing the bagpipes[/caption]\n\nThen we learned how to dance polka and some waltz. It was fun. Now i can dance! (I was salsa dancing in Omaha yesterday.) We had an amazing lunch that consisted of duck, dumplings, sourkrout, and a kolache for dessert. I tought it was best meal in a long time. Czech food is actually good and healthy. Richard I'm coming to the Czech Republic for more food. LOL :-P\n\nTo end the stay in wilber Nebraska the local children entertained us by showing us traditional dances in traditional outfits. I think it is cool that these children are continuing their traditions.\n\nNow I'm back in the dorms. Going to go get ice cream or something. Then I'll go work out with my roommate and have some bro time.\n\nFollow my blog by registering!! On the left!!!\n\n\"", "label": 1}
{"title": "Heroku CLI v10: Support for Next Generation Heroku Platform", "url": "https://www.heroku.com/blog/heroku-cli-v10-next-generation-heroku-platform/", "content": "The Heroku CLI is a vital tool for developers, providing a simple, extensible way to interact with the powerful features Heroku offers. We understand the importance of keeping the CLI updated to enhance user experience and ensure stability. With the release of Heroku CLI v10, we\u2019re excited to introduce key changes that enhance the user experience and improve compatibility with the next-generation Heroku platform.\n\nHeroku CLI v10 introduces several breaking changes, updates for Fir (the next-generation Heroku platform), and overall performance improvements. Here\u2019s a breakdown of the key features:\n\nNode.js 20 Upgrade :\n\nThe CLI has been upgraded to Node.js 20 , which brings performance improvements, security fixes, and better compatibility with modern development environments.\n\n: The CLI has been upgraded to , which brings performance improvements, security fixes, and better compatibility with modern development environments. Changes to heroku logs Command : The --dyno flag for specifying the process type and dyno name is now deprecated. Cedar apps : The \u2013dyno flag will continue to work but will be deprecated. Fir apps : Users will need to use the new --process-type or --dyno-name flags instead.\n\n: Changes to ps:stop and ps:restart Commands : Positional arguments for process type and dyno name are deprecated in ps:stop and ps:restart. Cedar apps : Positional arguments will still work with a deprecation warning. Fir apps : Users must use the --process-type or --dyno-name flags.\n\n: Compatibility with Fir Apps :\n\nSeveral commands no longer work with Fir apps, including heroku run , heroku ps:exec , heroku ps:copy , heroku ps:forward , and heroku ps:socks . Users should now use heroku run:inside , which is designed to work with Fir apps but not with Cedar apps.\n\n: Several commands no longer work with Fir apps, including , , , , and .\n\nOpenTelemetry Support :\n\nA new suite of commands under heroku telemetry allows seamless integration with OpenTelemetry for Fir apps, enabling better observability. Check out our DevCenter documentation on telemetry drains for setup instructions.\n\n: A new suite of commands under allows seamless integration with OpenTelemetry for Fir apps, enabling better observability. Check out our DevCenter documentation on telemetry drains for setup instructions. Spaces Updates : The heroku spaces:create command now supports a new --generation flag, allowing users to specify whether they are creating a Cedar or Fir space. A pilot warning message will appear when Fir is selected. heroku spaces , heroku spaces:info and heroku spaces:wait now display the generation of the space.\n\n: Pipelines and Buildpacks : heroku pipelines:diff has been updated to support Fir generation apps. The heroku buildpacks command now lists buildpacks specific to Fir apps, based on the latest release.\n\n: Improved Logs for Fir Apps : heroku logs now includes a --tail flag for Fir apps to stream logs in real time. A new \u201cFetching logs\u201d message is displayed as logs are being retrieved. Color rendering issues have been fixed to ensure consistent log output.\n\n:\n\noclif Upgrade : The CLI has been upgraded to oclif v4.14.36 , providing a more stable and modular architecture.\n\n: The CLI has been upgraded to , providing a more stable and modular architecture. GitHub Workflows: Updated GitHub workflows and actions now run on Node 20\n\nThe upgrade to Node.js 20 sets a solid foundation for future improvements and feature releases. These changes also help ensure that your Heroku CLI experience stays smooth and reliable as we continue to innovate.\n\nThe CLI is now ready for the next-generation Fir platform, making it easier to manage and deploy modern apps with enhanced observability, performance, and flexibility.", "label": 0}
{"title": "Remembering Ryan White", "url": "https://lifeofpablo.com/blog/remembering-ryan-white", "content": "Remembering Ryan White\n\nThis post was written in English (en_US).\n\nToday, 30 years ago on April 8th, 1990 Ryan White passed away. Today, he would have been 48 years old. I have always been an advocate for people with HIV/AIDS. I learned about Ryan when I was in high school. Ryan White contracted HIV through contaminated blood transfusions for hemophilia. He was the poster child during a time that many people didn't know what HIV/AIDS was as a disease. He spoke out against all adversaries. Ryan was forced out of school many times. He did \"The Fight to Go to School.\" So much ignorance occurred such as thinking it could be passed through casual contact. Ryan became a national spokesman. Many famous people such as Michael Jackson, befriended Ryan in a time that nobody wanted to be near him. Having people of high status led the way in showing that this disease is not transmitted just by shaking someone's hand, giving somebody a kiss or giving somebody a kiss. Ryan didn't stay quiet about his disease, he helped inform all of us that this disease can affect anyone, not just certain groups of people. This disease doesn't discriminate your gender, race, sexual orientation, etc. We all need to bond together to find a cure and stop the hatred. The Ryan White Act was created in 1990 to help people who have been diagnosed with this disease. Today the battle continues against HIV. We must remember those who fought to help educate us. RIP Ryan White", "label": 1}
{"title": "Java 21 Virtual Threads - Dude, Where\u2019s My Lock?", "url": "https://netflixtechblog.com/java-21-virtual-threads-dude-wheres-my-lock-3052540e231d?source=collection_home---4------19-----------------------", "content": "Java 21 Virtual Threads - Dude, Where\u2019s My Lock?\n\nGetting real with virtual threads Netflix Technology Blog 10 min read \u00b7 Jul 29, 2024 -- 35 Listen Share\n\nBy Vadim Filanovsky, Mike Huang, Danny Thomas and Martin Chalupa\n\nIntro\n\nNetflix has an extensive history of using Java as our primary programming language across our vast fleet of microservices. As we pick up newer versions of Java, our JVM Ecosystem team seeks out new language features that can improve the ergonomics and performance of our systems. In a recent article, we detailed how our workloads benefited from switching to generational ZGC as our default garbage collector when we migrated to Java 21. Virtual threads is another feature we are excited to adopt as part of this migration.\n\nFor those new to virtual threads, they are described as \u201clightweight threads that dramatically reduce the effort of writing, maintaining, and observing high-throughput concurrent applications.\u201d Their power comes from their ability to be suspended and resumed automatically via continuations when blocking operations occur, thus freeing the underlying operating system threads to be reused for other operations. Leveraging virtual threads can unlock higher performance when utilized in the appropriate context.\n\nIn this article we discuss one of the peculiar cases that we encountered along our path to deploying virtual threads on Java 21.\n\nThe problem\n\nNetflix engineers raised several independent reports of intermittent timeouts and hung instances to the Performance Engineering and JVM Ecosystem teams. Upon closer examination, we noticed a set of common traits and symptoms. In all cases, the apps affected ran on Java 21 with SpringBoot 3 and embedded Tomcat serving traffic on REST endpoints. The instances that experienced the issue simply stopped serving traffic even though the JVM on those instances remained up and running. One clear symptom characterizing the onset of this issue is a persistent increase in the number of sockets in closeWait state as illustrated by the graph below:\n\nCollected diagnostics\n\nSockets remaining in closeWait state indicate that the remote peer closed the socket, but it was never closed on the local instance, presumably because the application failed to do so. This can often indicate that the application is hanging in an abnormal state, in which case application thread dumps may reveal additional insight.\n\nIn order to troubleshoot this issue, we first leveraged our alerts system to catch an instance in this state. Since we periodically collect and persist thread dumps for all JVM workloads, we can often retroactively piece together the behavior by examining these thread dumps from an instance. However, we were surprised to find that all our thread dumps show a perfectly idle JVM with no clear activity. Reviewing recent changes revealed that these impacted services enabled virtual threads, and we knew that virtual thread call stacks do not show up in jstack -generated thread dumps. To obtain a more complete thread dump containing the state of the virtual threads, we used the \u201c jcmd Thread.dump_to_file \u201d command instead. As a last-ditch effort to introspect the state of JVM, we also collected a heap dump from the instance.\n\nAnalysis\n\nThread dumps revealed thousands of \u201cblank\u201d virtual threads:\n\n#119821 \"\" virtual\n\n\n\n#119820 \"\" virtual\n\n\n\n#119823 \"\" virtual\n\n\n\n#120847 \"\" virtual\n\n\n\n#119822 \"\" virtual\n\n...\n\nThese are the VTs (virtual threads) for which a thread object is created, but has not started running, and as such, has no stack trace. In fact, there were approximately the same number of blank VTs as the number of sockets in closeWait state. To make sense of what we were seeing, we need to first understand how VTs operate.\n\nA virtual thread is not mapped 1:1 to a dedicated OS-level thread. Rather, we can think of it as a task that is scheduled to a fork-join thread pool. When a virtual thread enters a blocking call, like waiting for a Future , it relinquishes the OS thread it occupies and simply remains in memory until it is ready to resume. In the meantime, the OS thread can be reassigned to execute other VTs in the same fork-join pool. This allows us to multiplex a lot of VTs to just a handful of underlying OS threads. In JVM terminology, the underlying OS thread is referred to as the \u201ccarrier thread\u201d to which a virtual thread can be \u201cmounted\u201d while it executes and \u201cunmounted\u201d while it waits. A great in-depth description of virtual thread is available in JEP 444.\n\nIn our environment, we utilize a blocking model for Tomcat, which in effect holds a worker thread for the lifespan of a request. By enabling virtual threads, Tomcat switches to virtual execution. Each incoming request creates a new virtual thread that is simply scheduled as a task on a Virtual Thread Executor. We can see Tomcat creates a VirtualThreadExecutor here.\n\nTying this information back to our problem, the symptoms correspond to a state when Tomcat keeps creating a new web worker VT for each incoming request, but there are no available OS threads to mount them onto.\n\nWhy is Tomcat stuck?\n\nWhat happened to our OS threads and what are they busy with? As described here, a VT will be pinned to the underlying OS thread if it performs a blocking operation while inside a synchronized block or method. This is exactly what is happening here. Here is a relevant snippet from a thread dump obtained from the stuck instance:\n\n#119515 \"\" virtual\n\njava.base/jdk.internal.misc.Unsafe.park(Native Method)\n\njava.base/java.lang.VirtualThread.parkOnCarrierThread(VirtualThread.java:661)\n\njava.base/java.lang.VirtualThread.park(VirtualThread.java:593)\n\njava.base/java.lang.System$2.parkVirtualThread(System.java:2643)\n\njava.base/jdk.internal.misc.VirtualThreads.park(VirtualThreads.java:54)\n\njava.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:219)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:754)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:990)\n\njava.base/java.util.concurrent.locks.ReentrantLock$Sync.lock(ReentrantLock.java:153)\n\njava.base/java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:322)\n\nzipkin2.reporter.internal.CountBoundedQueue.offer(CountBoundedQueue.java:54)\n\nzipkin2.reporter.internal.AsyncReporter$BoundedAsyncReporter.report(AsyncReporter.java:230)\n\nzipkin2.reporter.brave.AsyncZipkinSpanHandler.end(AsyncZipkinSpanHandler.java:214)\n\nbrave.internal.handler.NoopAwareSpanHandler$CompositeSpanHandler.end(NoopAwareSpanHandler.java:98)\n\nbrave.internal.handler.NoopAwareSpanHandler.end(NoopAwareSpanHandler.java:48)\n\nbrave.internal.recorder.PendingSpans.finish(PendingSpans.java:116)\n\nbrave.RealSpan.finish(RealSpan.java:134)\n\nbrave.RealSpan.finish(RealSpan.java:129)\n\nio.micrometer.tracing.brave.bridge.BraveSpan.end(BraveSpan.java:117)\n\nio.micrometer.tracing.annotation.AbstractMethodInvocationProcessor.after(AbstractMethodInvocationProcessor.java:67)\n\nio.micrometer.tracing.annotation.ImperativeMethodInvocationProcessor.proceedUnderSynchronousSpan(ImperativeMethodInvocationProcessor.java:98)\n\nio.micrometer.tracing.annotation.ImperativeMethodInvocationProcessor.process(ImperativeMethodInvocationProcessor.java:73)\n\nio.micrometer.tracing.annotation.SpanAspect.newSpanMethod(SpanAspect.java:59)\n\njava.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n\njava.base/java.lang.reflect.Method.invoke(Method.java:580)\n\norg.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethodWithGivenArgs(AbstractAspectJAdvice.java:637)\n\n...\n\nIn this stack trace, we enter the synchronization in brave.RealSpan.finish(RealSpan.java:134) . This virtual thread is effectively pinned \u2014 it is mounted to an actual OS thread even while it waits to acquire a reentrant lock. There are 3 VTs in this exact state and another VT identified as \u201c <redacted> @DefaultExecutor - 46542 \u201d that also follows the same code path. These 4 virtual threads are pinned while waiting to acquire a lock. Because the app is deployed on an instance with 4 vCPUs, the fork-join pool that underpins VT execution also contains 4 OS threads. Now that we have exhausted all of them, no other virtual thread can make any progress. This explains why Tomcat stopped processing the requests and why the number of sockets in closeWait state keeps climbing. Indeed, Tomcat accepts a connection on a socket, creates a request along with a virtual thread, and passes this request/thread to the executor for processing. However, the newly created VT cannot be scheduled because all of the OS threads in the fork-join pool are pinned and never released. So these newly created VTs are stuck in the queue, while still holding the socket.\n\nWho has the lock?\n\nNow that we know VTs are waiting to acquire a lock, the next question is: Who holds the lock? Answering this question is key to understanding what triggered this condition in the first place. Usually a thread dump indicates who holds the lock with either \u201c - locked <0x\u2026> (at \u2026) \u201d or \u201c Locked ownable synchronizers ,\u201d but neither of these show up in our thread dumps. As a matter of fact, no locking/parking/waiting information is included in the jcmd -generated thread dumps. This is a limitation in Java 21 and will be addressed in the future releases. Carefully combing through the thread dump reveals that there are a total of 6 threads contending for the same ReentrantLock and associated Condition . Four of these six threads are detailed in the previous section. Here is another thread:\n\n#119516 \"\" virtual\n\njava.base/java.lang.VirtualThread.park(VirtualThread.java:582)\n\njava.base/java.lang.System$2.parkVirtualThread(System.java:2643)\n\njava.base/jdk.internal.misc.VirtualThreads.park(VirtualThreads.java:54)\n\njava.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:219)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:754)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:990)\n\njava.base/java.util.concurrent.locks.ReentrantLock$Sync.lock(ReentrantLock.java:153)\n\njava.base/java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:322)\n\nzipkin2.reporter.internal.CountBoundedQueue.offer(CountBoundedQueue.java:54)\n\nzipkin2.reporter.internal.AsyncReporter$BoundedAsyncReporter.report(AsyncReporter.java:230)\n\nzipkin2.reporter.brave.AsyncZipkinSpanHandler.end(AsyncZipkinSpanHandler.java:214)\n\nbrave.internal.handler.NoopAwareSpanHandler$CompositeSpanHandler.end(NoopAwareSpanHandler.java:98)\n\nbrave.internal.handler.NoopAwareSpanHandler.end(NoopAwareSpanHandler.java:48)\n\nbrave.internal.recorder.PendingSpans.finish(PendingSpans.java:116)\n\nbrave.RealScopedSpan.finish(RealScopedSpan.java:64)\n\n...\n\nNote that while this thread seemingly goes through the same code path for finishing a span, it does not go through a synchronized block. Finally here is the 6th thread:\n\n#107 \"AsyncReporter <redacted>\"\n\njava.base/jdk.internal.misc.Unsafe.park(Native Method)\n\njava.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:221)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:754)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1761)\n\nzipkin2.reporter.internal.CountBoundedQueue.drainTo(CountBoundedQueue.java:81)\n\nzipkin2.reporter.internal.AsyncReporter$BoundedAsyncReporter.flush(AsyncReporter.java:241)\n\nzipkin2.reporter.internal.AsyncReporter$Flusher.run(AsyncReporter.java:352)\n\njava.base/java.lang.Thread.run(Thread.java:1583)\n\nThis is actually a normal platform thread, not a virtual thread. Paying particular attention to the line numbers in this stack trace, it is peculiar that the thread seems to be blocked within the internal acquire() method after completing the wait. In other words, this calling thread owned the lock upon entering awaitNanos() . We know the lock was explicitly acquired here. However, by the time the wait completed, it could not reacquire the lock. Summarizing our thread dump analysis:\n\nThere are 5 virtual threads and 1 regular thread waiting for the lock. Out of those 5 VTs, 4 of them are pinned to the OS threads in the fork-join pool. There\u2019s still no information on who owns the lock. As there\u2019s nothing more we can glean from the thread dump, our next logical step is to peek into the heap dump and introspect the state of the lock.\n\nInspecting the lock", "label": 0}
{"title": "How Meta understands data at scale", "url": "https://engineering.fb.com/2025/04/28/security/how-meta-understands-data-at-scale/", "content": "Managing and understanding large-scale data ecosystems is a significant challenge for many organizations, requiring innovative solutions to efficiently safeguard user data. Meta\u2019s vast and diverse systems make it particularly challenging to comprehend its structure, meaning, and context at scale.\n\nTo address these challenges, we made substantial investments in advanced data understanding technologies, as part of our Privacy Aware Infrastructure (PAI) . Specifically, we have adopted a \u201cshift-left\u201d approach, integrating data schematization and annotations early in the product development process. We also created a universal privacy taxonomy , a standardized framework providing a common semantic vocabulary for data privacy management across Meta\u2019s products that ensures quality data understanding and provides developers with reusable and efficient compliance tooling.\n\nWe discovered that a flexible and incremental approach was necessary to onboard the wide variety of systems and languages used in building Meta\u2019s products. Additionally, continuous collaboration between privacy and product teams was essential to unlock the value of data understanding at scale.\n\nWe embarked on the journey of understanding data across Meta a decade ago with millions of assets in scope ranging from structured and unstructured, processed by millions of flows across many of the Meta App offerings. Over the past 10 years, Meta has cataloged millions of data assets and is classifying them daily, supporting numerous privacy initiatives across our product groups. Additionally, our continuous understanding approach ensures that privacy considerations are embedded at every stage of product development.\n\nAt Meta, we have a deep responsibility to protect the privacy of our community. We\u2019re upholding that by investing our vast engineering capabilities into building cutting-edge privacy technology. We believe that privacy drives product innovation. This led us to develop our Privacy Aware Infrastructure (PAI), which integrates efficient and reliable privacy tools into Meta\u2019s systems to address needs such as purpose limitation\u2014restricting how data can be used while also unlocking opportunities for product innovation by ensuring transparency in data flows\n\nData understanding is an early step in PAI. It involves capturing the structure and meaning of data assets, such as tables, logs, and AI models. Over the past decade, we have gained a deeper understanding of our data, by embedding privacy considerations into every stage of product development, ensuring a more secure and responsible approach to data management.\n\nWe embarked on our data understanding journey by employing heuristics and classifiers to automatically detect semantic types from user-generated content. This approach has evolved significantly over the years, enabling us to scale to millions of assets. However, conducting these processes outside of developer workflows presented challenges in terms of accuracy and timeliness. Delayed classifications often led to confusion and unnecessary work, while the results were difficult to consume and interpret.\n\nData understanding at Meta using PAI\n\nTo address shortcomings, we invested in data understanding by capturing asset structure (schematization), describing meaning (annotation), and inventorying it into OneCatalog (Meta\u2019s system that discovers, registers, and enumerates all data assets) across all Meta technologies. We developed tools and APIs for developers to organize assets, classify data, and auto-generate annotation code. Despite significant investment, the journey was not without challenges, requiring innovative solutions and collaboration across the organization.\n\nChallenge Approach Understanding at scale (lack of foundation) At Meta, we manage hundreds of data systems and millions of assets across our family of apps. Each product features its own distinct data model, physical schema, query language, and access patterns. This diversity created a unique hurdle for offline assets: the inability to reuse schemas due to the limitations of physical table schemas in adapting to changing definitions. Specifically, renaming columns or making other modifications had far-reaching downstream implications, rendering schema evolution challenging, thus propagation required careful coordination to ensure consistency and accuracy across multiple systems and assets. We introduced a shared asset schema format as a logical representation of the asset schema that can be translated back and forth with the system-specific format. Additionally, it offers tools to automatically classify data and send out annotation changes to asset owners for review , effectively managing long-tail systems. Inconsistent definitions (lack of shared understanding) We encountered difficulties with diverse data systems that store data in various formats, and customized data labels that made it challenging to recognize identical data elements when they are stored across multiple systems. We introduced a unified taxonomy of semantic types , which are compiled into different languages. This ensured that all systems can share the same canonical set of labels. Missing annotations (lack of quality) A solution that relied solely on data scanning and pattern matching was prone to false positives due to limited contextual information. For instance, a 64-bit integer could be misclassified as either a timestamp or a user identifier without additional context. Moreover, manual human labeling is not feasible at scale because it relies heavily on individual developers\u2019 expertise and knowledge. We shifted left by combining schematization together with annotations in code , in addition improving and utilizing multiple classification signals . Strict measurements provided precision/recall guarantees. Protection was embedded in everything we built, without requiring every developer to be a privacy expert. Organizational barriers (lack of a unified approach) Meta\u2019s data systems, with their bespoke schematization and practices, posed significant challenges in understanding data across the company. As we navigated complex interactions and with ever evolving privacy requirements, it became clear that fragmented approaches to data understanding hindered our ability to grasp data comprehensively. By collaborating with asset owners to develop intuitive tooling and improve coverage, we tackled adoption barriers such as poor developer experience and inaccurate classification. This effort laid the groundwork for a unified data understanding foundation, which was seamlessly integrated into the developer workflow. As a result, we drove a cultural shift towards reusable and efficient privacy practices, ultimately delivering value to product teams and fostering a more cohesive approach to data management.\n\nWalkthrough : Understanding user data for the \u201cBeliefs\u201d feature in Facebook Dating\n\nTo illustrate our approach and dive into the technical solution, let\u2019s consider a scenario involving structured user data. When creating a profile on the Facebook Dating app, users have the option to include their religious views to help match with others who share similar values.\n\nOn Facebook Dating, religious views are subject to purpose limitation requirements. Our five-step approach to data understanding provides a precise, end-to-end view of how we track and protect sensitive data assets, including those related to religious views:\n\nEven a simple feature can involve data being processed by dozens of heterogenous systems, making end-to-end data protection critical. To ensure comprehensive protection, it is essential to apply the necessary steps to all systems that store or process data, including distributed systems (web systems, chat, mobile and backend services) and data warehouses.\n\nConsider the data flow from online systems to the data warehouse, as shown in the diagram below. To ensure that religious belief data is identified across all these systems, we have implemented measures to prevent its use for any purpose other than the stated one.\n\nStep 1 \u2013 Schematizing\n\nAs part of the PAI initiative, Meta developed DataSchema, a standard format that is used to capture the structure and relationships of all data assets, independent of system implementation. Creating a canonical representation for compliance tools. Understanding DataSchema requires grasping schematization, which defines the logical structure and relationships of data assets, specifying field names, types, metadata, and policies.\n\nImplemented using the Thrift Interface Description Language, DataSchema is compatible with Meta systems and languages. It describes over 100 million schemas across more than 100 data systems, covering granular data units like database tables, key-value stores, data streams from distributed systems (such as those used for logging), processing pipelines, and AI models. Essentially, a data asset is like a class with annotated attributes.\n\nLet\u2019s examine the source of truth (SoT) for a user\u2019s dating profile schema, modeled in DataSchema. This schema includes the names and types of fields and subfields:\n\n- user_id (uint) - name (string) - age (uint) - religious_views (enum) - photos (array<struct>): - url (url) - photo (blob) - caption (string) - uploaded_date (timestamp) Dating profile DataSchema\n\nThe canonical SoT schema serves as the foundation for all downstream representations of the dating profile data. In practice, this schema is often translated into system-specific schemas (source of record \u2013 \u201cSoR\u201d), optimized for developer experience and system implementation in each environment.\n\nStep 2 \u2013 Predicting metadata at scale\n\nBuilding on this schematization foundation, we used annotations to describe data, enabling us to quickly and reliably locate user data, such as religious beliefs, across Meta\u2019s vast data landscape. This is achieved through a universal privacy taxonomy, a framework that provides a common semantic vocabulary for data privacy management across Meta\u2019s apps. It offers a consistent language for data description and understanding, independent of specific programming languages or technologies.\n\nThe universal privacy taxonomy works alongside data classification, which scans systems across Meta\u2019s product family to ensure compliance with privacy policies. These systems use taxonomy labels to identify and classify data elements, ensuring privacy commitments are met and data is handled appropriately according to its classification.\n\nPrivacy annotations are represented by taxonomy facets and their values. For example, an asset might pertain to an Actor.Employee, with data classified as SemanticType.Email and originating from DataOrigin.onsite, not a third party. The SemanticType annotation is our standard facet for describing the meaning, interpretation, or context of data, such as user names, email addresses, phone numbers, dates, or locations.\n\nBelow, we illustrate the semantic type taxonomy node for our scenario, Faith Spirituality:\n\nAs data models and collected data evolve, annotations can become outdated or incorrect. Moreover, new assets may lack annotations altogether. To address this, PAI utilizes various techniques to continuously verify our understanding of data elements and maintain accurate, up-to-date annotations:\n\nOur classification system leverages machine learning models and heuristics to predict data types by sampling data, extracting features, and inferring annotation values. Efficient data sampling, such as Bernoulli sampling, and processing techniques enable scaling to billions of data elements with low-latency classifications.\n\nKey components include:\n\nScheduling component : manages the set of data assets to scan, accommodating different data system architectures by either pulling data via APIs or receiving data pushed directly into the scanning service.\n\nScanning service : processes and analyzes data from various sources by accumulating samples in memory, deserializing rows (e.g., JSON) into fields and sub-fields, and extracting features using APIs available in multiple languages (C++, Python, Hack). It ensures comprehensive data capture, even for ephemeral data.\n\nClassification service : utilizes heuristic rules and machine learning models to classify data types with high accuracy. Heuristic rules : handle straightforward, deterministic cases by identifying specific data formats like dates, phone numbers, and user IDs. Machine learning models : trained on labeled datasets using supervised learning and improved through unsupervised learning to identify patterns and anomalies in unlabeled data. Ground truth calibration and verification : ensures system accuracy and reliability, allowing for model fine-tuning and improved classification performance.\n\nLineage and propagation: We integrate classification rules with high-confidence lineage signals to ensure accurate data tracking and management. Our propagation mechanism enables the seamless annotation of data as needed, ensuring that exact copies of data across systems receive equivalent classification. This approach not only maintains data integrity but also optimizes the developer experience by streamlining the process of managing data classifications across our diverse systems.\n\nStep 3 \u2013 Annotating\n\nThe integration of metadata predictions and developer input creates a comprehensive picture of a data asset\u2019s structure (schema) and its meaning (annotation). This is achieved by attaching these elements to individual fields in data assets, providing a thorough understanding of the data.\n\nBuilding on the predicting data at scale initiative (step 2), where we utilize the universal privacy taxonomy and classification systems to identify and classify data elements, the generated metadata predictions are then used to help developers annotate their data assets efficiently and correctly.\n\nPortable annotation APIs: seamlessly integrate into developer workflows ensuring:\n\nConsistent representation of data across all systems at Meta.\n\nAccurate understanding of data, enabling the application of privacy safeguards at scale.\n\nEfficient evidencing of compliance with regulatory requirements.\n\nMetadata predictions and developer input: Two key components work together to create a comprehensive data asset picture:\n\nMetadata predictions : Classifiers generate predictions to aid developers in annotating data assets efficiently and correctly. If the confidence score exceeds a certain threshold, assignment can be automated, saving developer time.\n\nDeveloper input : Developers manually refine and verify annotations, ensuring that the data\u2019s context and privacy requirements are accurately captured. Human oversight guarantees the accuracy and reliability of the data asset picture.\n\n- user_id (enum) \u2192 SemanticType::id_userID - name (string) \u2192 SemanticType::identity_name - age (uint) \u2192 SemanticType::age - religious_views (enum) \u2192 SemanticType::faithSpirituality - photos (array<struct>): - url (url) \u2192 SemanticType::electronicID_uri_mediaURI_imageURL - photo (blob) \u2192 SemanticType::media_image - caption (string) \u2192 SemanticType::media_text_naturalLanguageText - uploaded_date (timestamp) \u2192 SemanticType::uploadedTime\n\nEnsuring complete schemas with annotations: To maintain a high standard of data understanding, we have integrated data understanding into our data model lifecycle. This includes auto-generating code to represent the schema of newly created assets when missing, ensuring that no new assets are created without a proper schema.\n\nFor example, in the context of our religious beliefs in Facebook Dating, we have defined its structure, including fields like \u2018Name,\u2019 \u2018EmailAddress,\u2019 and \u2018Religion.\u2019 Furthermore, we have annotated the asset with Actor::user(), signifying that the data pertains to a user of our products. This level of detail enables us to readily identify fields containing privacy-related data and implement appropriate protective measures, such as applying the applicable purpose limitation policy.\n\nIn the case of the \u201cdating profile\u201d data asset, we have defined its structure, including fields like \u2018Name\u2019:\n\nfinal class DatingProfileSchema extends DataSchemaDefinition { <<__Override>> public function configure(ISchemaConfig $config): void { $config->metadataConfig()->description('Represents a dating profile); $config->annotationsConfig()->annotations(Actor::user()); } <<__Override>> public function getFields(): dict<string, ISchemaField> { return dict[ 'Name' => StringField::create(\"name\") ->annotations(SemanticType::identity_name()) ->example('John Doe'), 'Age' => StringInt::create('age') ->description(\u201cThe age of the user.\u201d) ->annotations(SemanticType::age()) ->example('24'), 'ReligiousViews' => EnumStringField::create('religious_views') ->annotations(SemanticType::faithSpirituality()) ->example('Atheist'), ]; } }\n\nIn order to optimize for developer experience, the details of the schema representation differ in each environment. For example, in the data warehouse, it\u2019s represented as a Dataset \u2013 an in-code Python class capturing the asset\u2019s schema and metadata. Datasets provide a native API for creating data pipelines.\n\nHere is an example of such a schema:\n\n\u200b\u200b@hive_dataset( \"dim_all_dating_users\", // table name \"dating\", // namespace oncall=\"dating_analytics\", description=\"This is the primary Dating user dimension table containing one row per Dating user per day along with their profile, visitation, and key usage information.\", metadata=Metadata(Actor.User), ) class dim_all_dating_users(DataSet): ds: Varchar = Partition(\"datestamp\") userid: DatingUserID = Column(\"User id of the profile\") email: EmailAddress = Column(\"User's email address\"), age: PersonAge = Column(\"User's stated age on date ds\") religious_views: ReligionOptions = Column(\"User's provided religious views\")\n\nOur warehouse schema incorporates rich types, a privacy-aware type system designed to enhance data understanding and facilitate effective data protection. Rich types, such as DatingUserID, EmailAddress, PersonAge, and ReligionOptions, are integrated into the schema, offering a comprehensive approach to data management while encoding privacy metadata. They provide a developer-friendly way to annotate data and enable the enforcement of data quality rules and constraints at the type level, ensuring data consistency and accuracy across the warehouse. For instance, they can detect issues like joining columns with different types of user IDs or mismatched enums before code execution.\n\nHere is an example definition:\n\nReligionOptions = enum_from_items( \"ReligionOptions\", items=[ EnumItem(\"Atheist\", \"Atheist\"), EnumItem(\"Buddhist\", \"Buddhist\"), EnumItem(\"Christian\", \"Christian\"), EnumItem(\"Hindu\", \"Hindu\"), EnumItem(\"Jewish\", \"Jewish\"), EnumItem(\"Muslim\", \"Muslim\"), ... ], annotations=(SemanticType.faithSpirituality,), )\n\nStep 4 \u2013 Inventorying assets and systems\n\nA central inventory system is crucial for managing data assets and their metadata, offering capabilities like search and compliance tracking. Meta\u2019s OneCatalog is a comprehensive system that discovers, registers, and enumerates all data assets across Meta\u2019s apps, providing inventory for easier management and tracking.\n\nKey functions of OneCatalog:\n\nRegistering all data systems : OneCatalog defines a data system as a logical abstraction over resources that persist data for a common purpose. It exhaustively examines resources across Meta\u2019s environments to discover and register all data systems hosting data assets.\n\nEnumerating all data assets : Eligible data systems must enumerate their assets through the asset enumeration platform, generating a comprehensive list of assets and their metadata in the central inventory. These assets are grouped by \u201casset classes\u201d based on shared patterns, enabling efficient management and understanding of data assets.\n\nGuarantees provided by OneCatalog:\n\nCompleteness: The system regularly checks for consistency between the data defined in its configuration and the actual data stored in the inventory. This ongoing comparison ensures that all relevant data assets are accurately accounted for and up-to-date.\n\nFreshness: In addition to regularly scheduled pull-based enumeration, the system subscribes to changes in data systems and updates its inventory in real time.\n\nUniqueness of asset ID (XID): Each asset is assigned a globally unique identifier, similar to URLs, which facilitates coordination between multiple systems and the exchange of information about assets by providing a shared key. The globally unique identifier follows a human-readable structure, e.g., asset://[asset-class]/[asset-name].\n\nUnified UI: On top of the inventory, OneCatalog provides a unified user interface that consolidates all asset metadata, serving as the central hub for asset information. This interface offers a single point of access to view and manage assets, streamlining the process of finding and understanding data.\n\nFor example, in the context of our \u201creligious beliefs in the Dating app\u201d scenario, we can use OneCatalog\u2019s unified user interface to view the warehouse dating profile table asset, providing a comprehensive overview of its metadata and relationships.\n\nCompliance and privacy assurance: OneCatalog\u2019s central inventory is utilized by various privacy teams across Meta to ensure that data assets meet requirements. With its completeness and freshness guarantees, OneCatalog serves as a reliable source of truth for privacy and compliance efforts.\n\nBy providing a single view of all data assets, OneCatalog enables teams to efficiently identify and address potential risks or vulnerabilities, such as unsecured data or unauthorized access.\n\nStep 5 \u2013 Maintaining data understanding\n\nTo maintain high coverage and quality of schemas and annotations across Meta\u2019s diverse apps, we employed a robust process that involves measuring precision and recall for both predicted metadata and developer-provided annotations. This enables us to guide the implementation of our privacy and security controls and ensure their effectiveness.\n\nBy leveraging data understanding, tooling can quickly build end-to-end compliance solutions. With schema and annotations now front and center, we\u2019ve achieved continuous understanding, enabling our engineers to easily track and protect user data, implement various security and privacy controls, and build new features at scale.\n\nOur strategy for maintaining data understanding over time includes:\n\nShifting left on creation time : We provided intuitive APIs for developers to provide metadata at asset creation time, ensuring that schemas and annotations were applied consistently in downstream use cases.\n\nDetecting and fixing annotation gaps : We surfaced prediction signals to detect coverage and quality gaps and evolved our prediction and annotation capabilities to ensure new systems and workflows were covered.\n\nCollecting ground truth : We established a baseline to measure automated systems against, with the help of subject matter experts, to continuously measure and improve them.\n\nProviding canonical consumption APIs : We developed canonical APIs for common compliance usage patterns, such as detecting user data, to ensure consistent interpretation of metadata and low entry barriers.\n\nPutting it all together\n\nComing back to our scenario: As developers on the Facebook Dating team collect or generate new data, they utilize familiar APIs that help them schematize and annotate their data. These APIs provide a consistent and intuitive way to define the structure and meaning of the data.\n\nWhen collecting data related to \u201cFaith Spirituality,\u201dthe developers use a data classifier that confirms their semantic type annotations once the data is scanned during testing. This ensures that the data is accurately labeled and can be properly handled by downstream systems.\n\nTo ensure the quality of the classification system, ground truth created by subject matter experts is used to measure its accuracy. A feedback loop between the product and PAI teams keeps the unified taxonomy updated, ensuring that it remains relevant and effective.\n\nBy using canonical and catalogued metadata, teams across Meta can implement privacy controls that are consistent and effective. This enables the company to maintain user trust and meet requirements.\n\nIn this scenario, the developers on the Facebook Dating team are:\n\nSchematizing and annotating their data using familiar APIs.\n\nUsing a data classifier to confirm semantic type annotations.\n\nLeveraging ground truth to measure the quality of the classification system.\n\nUtilizing a feedback loop to keep the unified taxonomy updated.\n\nImplementing privacy controls using canonical and catalogued metadata.\n\nLearnings and takeaways\n\nBuilding an understanding of all data at Meta was a monumental effort that not only required novel infrastructure but also the contribution of thousands of engineers across all teams at Meta, and years of investment.\n\nCanonical everything : Data understanding at scale relies on a canonical catalog of systems, asset classes, assets, and taxonomy labels, each with globally unique identifiers. This foundation enables an ecosystem of compliance tooling, separating the concerns of data understanding from consuming canonical metadata.\n\nIncremental and flexible approach : To tackle the challenge of onboarding hundreds of systems across Meta, we developed a platform that supports pulling schemas from existing implementations. We layered solutions to enhance existing untyped APIs , meeting developers where they are\u2014whether in code, configuration, or a UI defining their use case and data model. This incremental and flexible approach delivers value at every step.\n\nCollaborating for data classification excellence : Building the platform was just the beginning. The infrastructure and privacy teams also collaborated with subject matter experts to develop best-in-class classifiers for our data, addressing some of the most challenging problems. These include detecting user-generated content, classifying data embedded in blobs, and creating a governed taxonomy that allows every developer to describe their data with the right level of detail.\n\nCommunity engagement with a tight feedback loop : Our success in backfilling schemas and integrating with the developer experience was made possible by a strong partnership with product teams. By co-building solutions and establishing an immediate feedback loop, we refined our approach, addressed misclassifications, and improved classification quality. This collaboration is crucial to our continued evolution and refinement of data understanding.\n\nThe future of data understanding\n\nData understanding has become a crucial component of Meta\u2019s PAI initiative, enabling us to protect user data in a sustainable and effective manner. By creating a comprehensive understanding of our data, we can address privacy challenges durably and more efficiently than traditional methods.\n\nOur approach to data understanding aligns closely with the developer workflow, involving the creation of typed data models, collection of annotated data, and processing under relevant policies. At Meta\u2019s scale, this approach has saved significant engineering effort by automating annotation on millions of assets (i.e., fields, columns, tables) with specific labels from an inventory that are deemed commitment-critical. This automation has greatly reduced the manual effort required for annotation, allowing teams to focus on higher-priority tasks.\n\nAs data understanding continues to evolve, it is expected to have a significant impact on various aspects of operations and product offerings. Here are some potential future use cases:\n\nImproved AI and machine learning : leveraging data understanding to improve the accuracy of AI-powered content moderation and recommendation systems.\n\nStreamlined developer workflows : integrating data understanding into Meta\u2019s internal development tools to provide clear data context and reduce confusion.\n\nOperational and developer efficiency : By automating data classification and annotation for millions of assets across Meta\u2019s platforms, we can significantly improve operational efficiency. This automation enables us to leverage metadata for various use cases, such as accelerating product innovation. For instance, we\u2019re now utilizing this metadata to help developers efficiently find the right data assets, streamlining their workflow and reducing the time spent on manual searches.\n\nProduct innovation : With a comprehensive understanding of data, Meta can drive product innovation by leveraging insights to create personalized and engaging user experiences.\n\nWhile there is still more work to be done, such as evolving taxonomies to meet future compliance needs and developing novel ways to schematize data, we are excited about the potential of data understanding. By harnessing canonical metadata, we can deepen our shared understanding of data, unlocking unprecedented opportunities for innovation not only at Meta, but across the industry.\n\nAcknowledgements\n\nThe authors would like to acknowledge the contributions of many current and former Meta employees who have played a crucial role in developing data understanding over the years. In particular, we would like to extend special thanks to (in alphabetical order) Aaron Morris, Adrian Zgorzalek, Alex Gorelik, Alex Kalinin, Alex Uslontsev, Ali Fakeri Tabrizi, Amit Sarkar, Anchit Arora, Andras Belokosztolszki, Anthony O\u2019Sullivan, Archit Jain, Aygun Aydin, Ayoade Adeniyi, Ben Warren, Bob Baldwin, Brani Stojkovic, Brian Romanko, Can Lin, Carrie (Danning) Jiang, Chao Yang, Chris Ventura, Daniel Ohayon, Danny Gagne, David Taieb, Dmitry Ponomarev, Dong Jia, Dong Zhao, Eero Neuenschwander, Fang Wang, Ferhat Sahinkaya, Ferdi Adeputra, Fred Liu, Gayathri Aiyer, George Stasa, Guoqiang Jerry Chen, Haiyang Han, Haydar Imren, Henry Swanson, Ian Carmichael, Jared Greene, Jerry Pan, Jiang Wu, Johnnie Ballentyne, Joanna Jiang, Jonathan Bergeron, Joseph Li, Jun Fang, Kaustubh Karkare, Komal Mangtani, Kuldeep Chaudhary, Kunal Kataria, Lea Li, Lei Zhang, Liu Yang, Loka Potnuru, Luiz Ribeiro, Marc Celani, Matthieu Martin, Max Mazzeo, Meg Dymek, Mellany Flores, Mike Tarasyuk, Mital Mehta, Nevzat Sevim, Nick Gardner, Nikolay Kondratyev, Oliver Dodd, Pankaj Landge, Perry Stoll, Peter Nieuwenhuizen, Pranet Verma, Prashanth Bandaru, Piyush Khemka, Rahul Nambiar, Rajesh Nishtala, Rituraj Kirti, Roger (Wei) Li, Rujin Cao, Sahil Garg, Satish Sampath, Sean Wang, Seth Silverman, Shridhar Iyer, Simran Patil, Sriguru Chakravarthi, Sushaant Mujoo, Susmit Biswas, Taha Bekir Eren, Tejas Kudrimoti, Tony Harper, Vineet Chaudhary, Vishal Jain, Vitali Haravy, Vlad Fedorov, Vlad Gorelik, Wolfram Schuttle, Xiaotian Guo, Yatu Zhang, Yi Huang, Yuxi Zhang, Zejun Zhang, and Zhaohui Zhang. We would also like to express our gratitude to all reviewers of this post, including (in alphabetical order) Aleksandar Ilic, Avtar Brar, Brianna O\u2019Steen, Chloe Lu, Chris Wiltz, Imogen Barnes, Jason Hendrickson, Rituraj Kirti, Xenia Habekoss and Yuri Claure. We would like to especially thank Jonathan Bergeron for overseeing the effort and providing all of the guidance and valuable feedback, and Ramnath Krishna Prasad for pulling required support together to make this blog post happen.", "label": 0}
{"title": "My #introduction on Mastodon", "url": "https://lifeofpablo.com/blog/my-introduction-on-mastodon", "content": "My #introduction on Mastodon\n\nThis post was written in English (en_US).\n\nPablo joined Mastodon!\n\nWe all know the news of the fire storm that is happening with Twitter and lighter fluid known as Elon Musk is throwing at it.\n\nMastodon is a social media service that is free. It pretty much acts like twitter with the same functionality but the name of the functions are named differently.\n\npost \"toots\" (instead of tweets),\n\nfollow other people and organizations,\n\nfavorite (like) and\n\nboost (retweet) posts from other people.\n\nI decided to join the community and so far I am enjoying it. When I mean joined, I actually created my own Mastodon instance on my server. More on this later. So for I am loving the decentralized platform and being able to control various aspects to it. My first experience with federation was the use of OwnCloud and its variants such as NextCloud. This concept has been around for quite some time.\n\nI chose to host my own instance because I like my domain (lifeofpablo.com) and it gave me an opportunity to learn how to manage an instance and learn how to be a user as well. It's been a great experience. If there is question or something I don't know I visit the Mastodon Documentation . This where the instructions are located to install your own instance. The key to installing your own instance is making sure Node.js is setup correctly on your server. It's pretty straight forward.\n\nMy Instance Setup:\n\nBack-End Domain Setup: https://social.lifeofpablo.com\n\nMy username pablo [at] pablolifeofpablo.com Please follow me \ud83d\ude00\n\nInstall using Mastodon Documentation Front-End Single User Mode (Just Me) At this time no registrations (Please follow me!)\n\nI love that i can use my main domain as the communicative user domain.\n\nGoals:\n\nBackground I would like to use Mastodon and the oAuth (used for login system) as a way to build apps, not necessarily clients. These apps would be an extension to my Mastodon instance. I'd use the login system to login to these apps to pull and use data. Steps Use Mastodon as backend for authentication\n\nBuild Node.js app using Mastodon (oAuth) as a login system for an internal app. A tutorial I found for good foundation\n\nCreate a dashboard where I can see metrics, trends, push system wide notifications, server maintenance, etc. Essentially a backend management.\n\nMove many of the administration features to the dashboard with the option of allowing features to be available.\n\nConclusion\n\nIt's been great getting to use Mastodon on the server-side and as a user. I see a great future for Mastodon and other decentralized, federated services (and other terminology) out there.\n\nI'm sure my goals/vision will grow on what I can do with my Mastodon instance. I know I will eventually migrate and upgrade server resources such as RAM, storage, processing power. I also need to make sure I concious about how much energy and being carbon neutral or carbon negative.\n\nIntroduction Post", "label": 1}
{"title": "Paris and Belgium: Weekend Adventures", "url": "https://lifeofpablo.com/blog/paris-and-belgium-weekend-adventures", "content": "Paris and Belgium: Weekend Adventures\n\nThis post was written in English (en_US).\n\nBeing away from home these last few months have been great. I enjoy the freedoms, living independently and eating good food whenever I want. Life has treated me good here. Until more recently, a certain stage of being away finally starts. Lately, homesickness has really started to kick in. I\u2019ve really started to miss my friends, family and things I really took for granted back home. It really is bad when I am craving my mom\u2019s homemade food. (Mexican Food.) Midterms and all the studying is really killing my vibe at this point of the semester. My program is not your typical study abroad program that you go away for 3 months (not to mention that it doesn\u2019t even apply to your major) on a semi vacation. I actually have to try real hard as my french studies here in Strasbourg France will help me graduate next year. Enough bashing.\n\nWith all this, one just feels that they are slowly losing control. Sure I am enjoying myself out here in France but still there is this emptiness of something that I have been away from for far too long. Hopefully, I hope some sort of enlightenment will come bring me back to life. Then It hit me, I would see my friends in just a few weeks. I honestly couldn\u2019t believe in such short time I would be right in front of them.\n\nSo I would be meeting up my good friend Sammy and soon-to-be friend Mitch. They have never been to Europe before, so I figured why not show them around Paris, a city that I know decently and use my French to get to Point A to Point B and of course with the help of good ol\u2019 Google Maps. Paris- Weekend 1\n\nThese guys had a hell of a time getting to Paris during the 24 hours before arrival. I\u2019ll let them explain that one.\n\nSo I picked the guys up from Charles De Gaulle Aiport. I still feel bad on making you guys wait for me. :/ I ran up to my dude Sammy and gave him a bro hug! It has been way to long since I have seen that dude! I also got to meet Mitch and got to know him more through out the weekend. Cool Dude! So I showed them around Paris. To be honest, Paris is beautiful with history written all around. As with any major city, Paris is no exception, it is dirty and smelly. I really hope that the guys did not have too high of hopes for the \u201cCity of Love.\u201d Paris is overrated in my opinion but why not take advantage of leaving Strasbourg for a few days.\n\nSo we went to various monuments such as The Louvre, Arc of Triumph, Versailles and so much more. Many of the places were breath taking. The Eiffel Tour is always a good place to scope out as well as walking along the Seine River. Getting around Paris is no piece of cake but we managed. Never have I ever gone up so many round staircases. Climbing the Cathedral of Notre Dame was such a struggle. Whenever you are in Europe, walking is really the way to get around.\n\nNight life in Paris is pretty popping! We went to a few clubs in the Grand Boulevard District! This district did not disappoint. I wish we could have checked out a few other ones. One of my favorite places was the Irish Pub. Music, dancing, girls and drinks. Man was that a fun night!\n\nWe ate some good food throughout Paris. The guys had their first legit French Croissants and baguette sandwich. It\u2019s totally acceptable to have a sandwich on you at all times. I also had them try one of my favorites, a kebab sandwich. Man are they delicious! Sammy had actual ramen noodles for the first time ever! Overall we pretty much stuffed our faces all weekend.\n\nBruxelles- Weekend 2\n\nAh Bruxelles!! The place known for its deliciouos fries and waffles. You have no idea how much I stuffed my face this weekend. Just writing this post makes my mouth water. Was eating all this food this worth it? You bet it was!! When I arrived in Bruxelles, it was gloomy and misting. Not the way I wanted to start the weekend. The weather stayed the same all weekend. That did not stop the fun.\n\nSpending the weekend in Brussels was totally worth the waffles! The sugar definitely made me bounce off the walls. #belgium #worldtravel #unkstudyabroad A post shared by Pablo Morales (@pablo.morales1) on Mar 19, 2017 at 9:55am PDT\n\nWe checked out many popular attractions throughout the city. There was so much we saw, I started to lose my mind a bit. One of the coolest places I saw was a \u201cWorld Shop.\u201d They had souvenirs and artsy things from all around the world. I was so intrigued with the place that I ended up buying a few things from there.\n\nThe other purpose of coming to Bruxelles was to meet up with our friend Kellie and Sammy\u2019s brother, Louis. It was good to see some other friends from back home. I missed those two! Kellie is just a keeper, to keep a long story short. These two are actually on Spring Break right now. I am so jealous of you guys spending your holiday in Malta! I hope you enjoy every second of it!\n\nBruxelles treated everyone well!\n\nYour browser does not support the video tag.\n\nThese last few weekends have been pretty awesome! It was good to see a few friends from back home. I couldn\u2019t of asked for a better time. Traveling becomes more exciting when you are hanging out with people you know and love. I can\u2019t wait for the next adventure with them! So, is it sprangggg breakkk yet? I wish!", "label": 1}
{"title": "Day 5 - Final Day and a Special Thanks at the UNK World Leaders Camp.", "url": "https://lifeofpablo.com/blog/day-5-final-day-and-a-special-thanks-at-the-unk-world-leaders-camp", "content": "Day 5 - Final Day and a Special Thanks at the UNK World Leaders Camp.\n\nThis post was written in English (en_US).\n\n\"Well today was the end of the great World Leaders Camp hosted at UNK.\n\nI had breakfast Had some scrambled eggs, sausage and hash-brown with a cup of OJ. I was just pondering about what the day was to bring... Mixed feelings all around.\n\nWe had a Global Networking Lecture from Prof. Amundson. It was very amusing. I personally learned a lot. His point was to meet people and network. He said, \"\" Your best experiences are through networking.\"\" I 100% agree on this quote. Study abroad is really something I would enjoy.\n\nYou can not end a great camp without a great game of Jeopardy. Man was my team being beat. Colin was just spitting out answers like no other. I was getting nervous.\n\nThen we got our certificates\n\nThis is my first certificate in college. I hope for many more. I was happy when I got it.\n\nI want to thank all of the people who were in the WLC.\n\nKatherine A Berke ;\n\nAshley M Bruha\n\nLydia J Crocker\n\nSarah C Haack\n\nSierra Hirth\n\nAllison N Kiolbasa\n\nNoemi L Liscano\n\nDanielle I Merrill\n\nBrittany K Mrkvicka\n\nPaige E Phillips ;\n\nColin T Stiles\n\nCatiana Urrutia\n\nI'm glad to have met all of you guys. I consider all of you like family. I'll always be excited to see you guys and say hello even from a distance.\n\nand of course Lisa for being our advisor for the Camp!! She's pretty cool I would say so myself. We could not forget Ann Marie also!!\n\nI'll see everyone on Campus!! GO LOPERS!!\"", "label": 1}
{"title": "Gemini 2.5: Updates to our family of thinking models", "url": "https://developers.googleblog.com/en/gemini-2-5-thinking-model-updates/", "content": "Today we are excited to share updates across the board to our Gemini 2.5 model family: Gemini 2.5 Pro is generally available and stable (no changes from the 06-05 preview) Gemini 2.5 Flash is generally available and stable (no changes from the 05-20 preview, see pricing updates below) Gemini 2.5 Flash-Lite is now available in preview Gemini 2.5 models are thinking models, capable of reasoning through their thoughts before responding, resulting in enhanced performance and improved accuracy. Each model has control over the thinking budget, giving developers the ability to choose when and how much the model \u201cthinks\u201d before generating a response.\n\nOverview of our family of Gemini 2.5 thinking models\n\nIntroducing Gemini 2.5 Flash-Lite Today, we\u2019re introducing 2.5 Flash-Lite in preview with the lowest latency and cost in the 2.5 model family. It\u2019s designed as a cost-effective upgrade from our previous 1.5 and 2.0 Flash models. It also offers better performance across most evals, and lower time to first token while also achieving higher tokens per second decode. This model is great for high throughput tasks like classification or summarization at scale. Gemini 2.5 Flash-Lite is a reasoning model, which allows for dynamic control of the thinking budget with an API parameter. Because Flash-Lite is optimized for cost and speed, \u201cthinking\u201d is off by default, unlike our other models. 2.5 Flash-Lite also supports all of our native tools like Grounding with Google Search, Code Execution, and URL Context in addition to function calling.\n\nBenchmarks for Gemini 2.5 Flash-Lite\n\nOver the last year, our research teams have continued to push the pareto frontier with our Flash model series. When 2.5 Flash was initially announced, we had not yet finalized the capabilities for 2.5 Flash-Lite. We also launched with a \u201cthinking\u201d and \u201cnon-thinking price\u201d, which led to developer confusion.\n\n\n\nWith the stable version of Gemini 2.5 Flash rolling out (which is the same 05-20 model preview we made available at Google I/O), and the incredible performance of 2.5 Flash, we are updating the pricing for 2.5 Flash: $0.30 / 1M input tokens (*up from $0.15 input) $2.50 / 1M output tokens (*down from $3.50 output) We removed the thinking vs. non-thinking price difference We kept a single price tier regardless of input token size While we strive to maintain consistent pricing between preview and stable releases to minimize disruption, this is a specific adjustment reflecting Flash\u2019s exceptional value, still offering the best cost-per-intelligence available. And with Gemini 2.5 Flash-Lite, we now have an even lower cost option (with or without thinking) for cost and latency sensitive use cases that require less model intelligence.\n\nPricing updates for our Gemini Flash family\n\nIf you are using the Gemini 2.5 Flash Preview 04-17 , the existing preview pricing will remain in effect until its planned deprecation on July 15, 2025, at which point that model endpoint will be turned off. You can transition to the generally available model \u201cgemini-2.5-flash\u201d, or switch to 2.5 Flash-Lite Preview as a lower cost option.\n\nContinued growth of Gemini 2.5 Pro The growth and demand for Gemini 2.5 Pro continues to be the steepest of any of our models we have ever seen. To allow more customers to build on this model in production, we are making the 06-05 version of the model stable, with the same pareto frontier price point as before. We expect that cases where you need the highest intelligence and most capabilities are where you will see Pro shine, like coding and agentic tasks. Gemini 2.5 Pro is at the heart of many of the most loved developer tools.\n\nTop developer tools using Gemini 2.5 Pro", "label": 0}
{"title": "Letter to Planning Commission re: upzoning in Juanita \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/12/letter-to-planning-commission-re-upzoning-in-juanita/", "content": "Two large sites in the northern part of my city are being proposed for upzoning to permit taller buildings, reduced parking requirements, as well as other changes. In general the changes seem good, but there is one proposed change that is concerning: allowing townhomes instead of apartments or condos on some large sites that ought to be developed as densely as possible (link to packet). There is a great deal of community opposition to these upzones, so I sent in a supportive note to the Planning Commission in advance of their evening meeting.\n\nIn support of higher density at Michael\u2019s and Goodwill sites\n\nTo the Kirkland Planning Commission:\n\nI am writing in favor of proposed zoning amendments to the Michael\u2019s and Goodwill sites to allow higher density, taller building heights, and reduced parking requirements. These rare anchor sites offer great potential to make the neighborhood denser and more walkable.\n\nI do not support allowing townhomes in this area \u2014 along arterial routes with transit access, with many goods and services within walking or biking distance, we ought to support as much housing density as feasible. Kirkland and the greater Seattle area are in desperate need of more housing, so I urge you to use the code to require higher densities in the BC1 zone. If we do not push for higher density where it makes sense \u2014 and it makes a great deal of sense here \u2014 then the overall amount of additional housing Kirkland can accommodate will be much lower than the need. Please require a minimum density for ADUs in this area as in Alternatives 1 or 2 from Appendix 5.", "label": 1}
{"title": "5 years", "url": "https://shellsharks.com/notes/2024/05/30/5-years", "content": "Mike Sass\n\n@shellsharks\n\nOn May 30, 2019 I published the very first post to shellsharks.com, \u201cGetting Into Information Security\u201d. When I first started the blog I didn\u2019t have a particularly clear idea of what I wanted to do with it, but I had the idea to write that piece and one other idea which which was to catalog all named vulnerabilities. Beyond that, I just wrote about whatever came to mind. It\u2019s been 5 years to the day since I got that first piece out and not only has the site come a long way, but so have I.\n\nIn the beginning, I think my main focus was to keep the site mostly infosec-related in terms of my writing, but even back then I left the door open to write about other things. Afterall, the tagline for my site has always been \u201cInfosec, Technology, Life\u201d. The first 4 years of the site did end up skewing mostly Infosec / Tech but since discovering the IndieWeb last year, I have made a noticeable shift towards writing more about \u201cLife\u201d, i.e. anything non-tech / non-infosec, and it\u2019s been a lot of fun!\n\nThe site has also gone through a number of big aesthetic overhauls and architectural redesigns. There will always be new things I want to add to the site or ways in which I wish I could rearchitect it even further, but for now, I am quite happy with the latest generation of its look-and-feel as well as the functionality itself.\n\nI\u2019ve not maintained the same level of active-ness throughout the last 5 years, but in more recent history I\u2019m quite pleased with the amount of attention I\u2019ve given it, the upgrades I\u2019ve made, and the writing I\u2019ve produced, and I am certainly starting the next 5 years quite strong. I plan to continue to funnel my professional work, technological side quests and life experiences into what I write about.\n\nI really want to thank everyone who has read anything I\u2019ve written, those who have offered kind words over social media or elsewhere, those who have given me meaningful/constructive feedback and anyone who has taken the time to subscribe, like or engage with me or my writing. Like many who start a blog, I had no expectation of building a readership. I started it with the intention of sharing my oft-repeated guidance on getting into the infosec field and as a place to write resources for myself. Over time though I have found that sure enough, there are people out there who enjoy, or get value out of what I have documented or what I have to say and it is undoubtedly a good feeling.\n\nThese days, I spend as much if not more effort writing about and encouraging others to start a site and be themselves as I do writing about tech/infosec. My experience with shellsharks has taught me more about what it means to be authentic on the web than it has helped me with my various infosec-related research things. I\u2019m still a cybersecurity professional and as such will continue to make that a focus of what I learn and thus what I write about, but it is clear to me that I\u2019ve developed a passion for IndieWeb advocacy and I hope to continue to inspire others both in my field and abroad to start their own journey in the personal web.\n\nThanks for reading. Here\u2019s to the next 5 years!\n\nFun Fact: December 4, 2020 | The earliest snapshot I could get of shellsharks on archive.org", "label": 1}
{"title": "I Like Matcha", "url": "https://lifeofpablo.com/blog/i-like-matcha", "content": "I Like Matcha\n\nMatcha Latte\n\nThis post was written in English (en_US).\n\nLike is an understatement, I love matcha! I started to consume matcha two years ago. Ever since, I've been hooked on it. I love getting matcha from different places from coffee shops or restaurants because when mixed in for a drink, each place makes it different. Most of the time, I consume matcha as a tea or as a latte. There is something so special about getting in a different city. When I was visiting New York City, I lived on matcha lattes and matcha teas to keep me going on the miles and miles of walking that I'm not used to doing. Being a city like New York, it is exhausting mentally and physically due to the stimuli and increased physical activity from the sedentary lifestyle.\n\nMatcha has caffeine in it but it doesn't affect me the same as coffee. Coffee and espresso keeps me jittery and has be bouncing off the walls. Not a good feeling overall. I'm not opposed to having coffee or espresso but not my preferred choice. Matcha doesn't keep me up at night. I don't see it as coffee but an item that has caffeine.\n\nWhen I went back to Nebraska, I was happy that some of the non-Starbucks coffee shops had matcha. I guess matcha is more common than ever.\n\nAm I crazy about matcha? Probably? Who am I kidding?! YES! I don't drink it often but when I do, I'm as happy as can be (at the moment.) It's the little things in life. I probably should make my own at home because it adds up going out to get it. That's a decision for another day.\n\nJust realized this could be a love letter to matcha. Maybe it isn't.\n\n\u2764\ufe0f\u2764\ufe0f\n\nFavorite Places to get matcha (no particular order):", "label": 1}
{"title": "TensorFlow Lite is now LiteRT", "url": "https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/", "content": "LiteRT, part of the Google AI Edge suite of tools, is the runtime that lets you seamlessly deploy ML and AI models on Android, iOS, and embedded devices. With AI Edge's robust model conversion and optimization tools, you can ready both open-source and custom models for on-device development.\n\nSince its debut in 2017, TFLite has enabled developers to bring ML-powered experiences to over 100K apps running on 2.7B devices. More recently, TFLite has grown beyond its TensorFlow roots to support models authored in PyTorch , JAX , and Keras with the same leading performance. The name LiteRT captures this multi-framework vision for the future: enabling developers to start with any popular framework and run their model on-device with exceptional performance.\n\nLiteRT (short for Lite Runtime) is the new name for TensorFlow Lite (TFLite). While the name is new, it's still the same trusted, high-performance runtime for on-device AI, now with an expanded vision.\n\nThis change will roll out progressively. Starting today, you\u2019ll see the LiteRT name reflected in the developer documentation, which is moving to ai.google.dev/edge/litert, and in other references across the AI Edge website. The documentation at tensorflow.org/lite now redirects to corresponding pages at ai.google.dev/edge/litert.\n\nThe main TensorFlow brand will not be affected, nor will apps already using TensorFlow Lite.\n\n\n\nHow to access LiteRT\n\nOur goal is that this change is minimally disruptive, requiring as few code changes from developers as possible.\n\nIf you currently use TensorFlow Lite via packages, you\u2019ll need to update any dependencies to use the new LiteRT from Maven, PyPi, Cocoapods.\n\nIf you currently use TensorFlow Lite via Google Play Services, no change is necessary at this time.\n\nIf you currently build TensorFlow Lite from source, please continue building from the TensorFlow repo until code has been fully moved to the new LiteRT repo later this year.\n\n\n\nFrequently asked questions\n\n\n\n1. What is changing beyond the new name, LiteRT?\n\nFor now, the only change is the new name, LiteRT. Your production apps will not be affected. With a new name and refreshed vision, look out for more updates coming to LiteRT, improving how you deploy classic ML models, LLMs, and diffusion models with GPU and NPU acceleration across platforms.\n\n\n\n2. What\u2019s happening to the TensorFlow Lite Support Library (including TensorFlow Lite Tasks)?\n\nThe TensorFlow Lite support library and TensorFlow Lite Tasks will remain in the /tensorflow repository at this time. We encourage you to use MediaPipe Tasks for future development.\n\n\n\n3. What\u2019s happening to TensorFlow Lite Model Maker?\n\nYou can continue to access TFLite Model Maker via https://pypi.org/project/tflite-model-maker/\n\n\n\n4. What if I want to contribute code?\n\nFor now, please contribute code to the existing TensorFlow Lite repository. We\u2019ll make a separate announcement when we\u2019re ready for contributions to the LiteRT repository.\n\n\n\n5. What\u2019s happening to the .tflite file extension and file format?\n\nNo changes are being made to the .tflite file extension or format. Conversion tools will continue to output .tflite flatbuffer files, and .tflite files will be readable by LiteRT.\n\n\n\n6. How do I convert models to .tflite format?\n\nFor Tensorflow, Keras and Jax you can continue to use the same flows. For PyTorch support check out ai-edge-torch.\n\n\n\n7. Will there be any changes to classes and methods?\n\nNo. Aside from package names, you won\u2019t have to change any code you\u2019ve written for now.\n\n\n\n8. Will there be any changes to TensorFlow.js?\n\nNo, TensorFlow.js will continue to function independently as part of the Tensorflow codebase.\n\n\n\n9. My production app uses TensorFlow Lite. Will it be affected?\n\nApps that have already deployed TensorFlow Lite will not be affected. This includes apps that access TensorFlow Lite via Google Play Services. (TFLite is compiled into the apps at build time, so once they\u2019re deployed, apps have no dependency.)\n\n\n\n10. Why \u201cLiteRT\u201d?\n\n\u201cLiteRT\u201d (short for Lite Runtime) reflects the legacy of TensorFlow Lite, a pioneering \u201clite\u201d, on-device runtime, plus Google\u2019s commitment to supporting today\u2019s thriving multi-framework ecosystem.\n\n\n\n11. Is TensorFlow Lite still being actively developed?\n\nYes, but under the name LiteRT. Active development will continue on the runtime (now called LiteRT), as well as the conversion and optimization tools. To ensure you're using the most up-to-date version of the runtime, please use LiteRT.\n\n\n\n12. Where can I see examples of LiteRT in practice?\n\nYou can find examples for Python, Android, and iOS in the official LiteRT samples repo.\n\n\n\nWe\u2019re excited for the future of on-device ML, and are committed to our vision of making LiteRT the easiest to use, highest performance runtime for a wide range of models.", "label": 0}
{"title": "Guiding principles for my website \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/01/21/guiding-principles-for-my-website/", "content": "I saw someone* share a list of guiding principles for their website they described as \u201cCore Website Tenets,\u201d and I love the idea of codifying my approach to my website. Some are operational, some are aspirational, some are decision points if I\u2019m on the fence.\n\nBe friendly and kind Be open (but not too open) Respect privacy Play with it Prioritize connection over reach Use it as a tool Keep it informal Go slow Practice imperfection Use it for good\n\n*(This person has taken their site offline intentionally, so while I normally would credit them, in this case I think they would prefer I not. I sat on this post for a year hoping it would come back online but alas. If this is you and I have guessed wrong please email me!)\n\nHere\u2019s how I\u2019m currently interpreting / implementing these principles:\n\n1. Be friendly and kind\n\nLink and cite generously\n\nPost my blogroll\n\nPost my contact info\n\nInvite connection\n\nAccept comments and Webmentions\n\nReply to comments more often than not\n\nDon\u2019t be a dick \u2014 be kind even when critical\n\nBut fuck civility \u2014 niceness is a trap\n\n2. Be open (but not too open)\n\nShare where it may be of value to others, but only what I am comfortable sharing \u2014 I don\u2019t have to push my own boundaries\n\nDon\u2019t fear the impact of my blog on my professional life \u2014 embrace whole person-ness\n\nDo not share things about myself that could endanger me under fascism\n\nEmploy private posts \u2014 I don\u2019t have to publish everything I write\n\n3. Respect privacy\n\nDo not post offline friends\u2019 names\n\nDo not post images of others\u2019 faces without permission\n\nDon\u2019t talk about my relationship or partner more than incidentally\n\nDon\u2019t use corporate analytics or collect unnecessary data about readers\n\nUse people\u2019s names in the format listed online, even if I know their full name\n\nAvoid using gendered pronouns unless specified on their website (this one\u2019s a work in progress but I\u2019m trying \ud83d\udc93\ud83d\ude05)\n\nTry not to link to anyone\u2019s RSS club posts\n\n4. Play with it\n\nDo what sounds fun (why is this so hard \ud83d\ude02)\n\nPlay along with other bloggers\n\nFollow whims, indulge myself, use color \ud83c\udf08, seek delight, be weird\n\nExplore, experiment \u2013nothing need be permanent\n\nMake things just because it sounds cool \u2014 not everything must be \u201cuseful\u201d\n\nFollow my own rabbit holes\n\nKeep it a feel-good, safe space for myself \u2014 I am not obligated to approve rude comments\n\n5. Prioritize connection over reach\n\n6. Use it as a tool\n\nTake notes for myself\n\nUse it as a tool for attention\n\nPersonalize it so it works well for me\n\nSeek connections and synthesize what I\u2019ve learned\n\nPush my thinking and writing skills\n\nPractice craft\n\n7. Keep it informal\n\n8. Go slow\n\nRespond to things in my own time\n\nBe patient, reject urgency\n\nEmbrace asynchronous, slow conversation\n\nTake advantage of slow thinking and the nuance allowed by long posts\n\n9. Practice imperfection\n\n10. Use it for good\n\nI\u2019d love to hear others\u2019 philosophies about their sites \u2014 consider this an invitation if you\u2019d like to join in \ud83d\ude04", "label": 1}
{"title": "GitHub Issues search now supports nested queries and boolean operators: Here\u2019s how we (re)built it", "url": "https://github.blog/developer-skills/application-development/github-issues-search-now-supports-nested-queries-and-boolean-operators-heres-how-we-rebuilt-it/", "content": "Originally, Issues search was limited by a simple, flat structure of queries. But with advanced search syntax, you can now construct searches using logical AND/OR operators and nested parentheses, pinpointing the exact set of issues you care about.\n\nBuilding this feature presented significant challenges: ensuring backward compatibility with existing searches, maintaining performance under high query volume, and crafting a user-friendly experience for nested searches. We\u2019re excited to take you behind the scenes to share how we took this long-requested feature from idea to production.\n\nHere\u2019s what you can do with the new syntax and how it works behind the scenes\n\nIssues search now supports building queries with logical AND/OR operators across all fields, with the ability to nest query terms. For example is:issue state:open author:rileybroughten (type:Bug OR type:Epic) finds all issues that are open AND were authored by rileybroughten AND are either of type bug or epic.\n\nHow did we get here?\n\nPreviously, as mentioned, Issues search only supported a flat list of query fields and terms, which were implicitly joined by a logical AND. For example, the query assignee:@me label:support new-project translated to \u201cgive me all issues that are assigned to me AND have the label support AND contain the text new-project.\u201d\n\nBut the developer community has been asking for more flexibility in issue search, repeatedly, for nearly a decade now. They wanted to be able to find all issues that had either the label support or the label question , using the query label:support OR label:question . So, we shipped an enhancement towards this request in 2021, when we enabled an OR style search using a comma-separated list of values.\n\nHowever, they still wanted the flexibility to search this way across all issue fields, and not just the labels field. So we got to work.\n\nTechnical architecture and implementation\n\nFrom an architectural perspective, we swapped out the existing search module for Issues (IssuesQuery), with a new search module (ConditionalIssuesQuery), that was capable of handling nested queries while continuing to support existing query formats.\n\nThis involved rewriting IssueQuery, the search module that parsed query strings and mapped them into Elasticsearch queries.\n\nTo build a new search module, we first needed to understand the existing search module, and how a single search query flowed through the system. At a high level, when a user performs a search, there are three stages in its execution:\n\nParse: Breaking the user input string into a structure that is easier to process (like a list or a tree) Query: Transforming the parsed structure into an Elasticsearch query document, and making a query against Elasticsearch. Normalize: Mapping the results obtained from Elasticsearch (JSON) into Ruby objects for easy access and pruning the results to remove records that had since been removed from the database.\n\nEach stage presented its own challenges, which we\u2019ll explore in more detail below. The Normalize step remained unchanged during the re-write, so we won\u2019t dive into that one.\n\nParse stage\n\nThe user input string (the search phrase) is first parsed into an intermediate structure. The search phrase could include:\n\nQuery terms: The relevant words the user is trying to find more information about (ex: \u201cmodels\u201d)\n\nThe relevant words the user is trying to find more information about (ex: \u201cmodels\u201d) Search filters: These restrict the set of returned search documents based on some criteria (ex: \u201cassignee:Deborah-Digges\u201d)\n\nExample search phrase:\n\nFind all issues assigned to me that contain the word \u201ccodespaces\u201d: is:issue assignee:@me codespaces\n\nFind all issues with the label documentation that are assigned to me: assignee:@me label:documentation\n\n\n\nThe old parsing method: flat list\n\nWhen only flat, simple queries were supported, it was sufficient to parse the user\u2019s search string into a list of search terms and filters, which would then be passed along to the next stage of the search process.\n\nThe new parsing method: abstract syntax tree\n\nAs nested queries may be recursive, parsing the search string into a list was no longer sufficient. We changed this component to parse the user\u2019s search string into an Abstract Syntax Tree (AST) using the parsing library parslet.\n\nWe defined a grammar (a PEG or Parsing Expression Grammar) to represent the structure of a search string. The grammar supports both the existing query syntax and the new nested query syntax, to allow for backward compatibility.\n\nA simplified grammar for a boolean expression described by a PEG grammar for the parslet parser is shown below:\n\nclass Parser < Parslet::Parser rule(:space) { match[\" \"].repeat(1) } rule(:space?) { space.maybe } rule(:lparen) { str(\"(\") >> space? } rule(:rparen) { str(\")\") >> space? } rule(:and_operator) { str(\"and\") >> space? } rule(:or_operator) { str(\"or\") >> space? } rule(:var) { str(\"var\") >> match[\"0-9\"].repeat(1).as(:var) >> space? } # The primary rule deals with parentheses. rule(:primary) { lparen >> or_operation >> rparen | var } # Note that following rules are both right-recursive. rule(:and_operation) { (primary.as(:left) >> and_operator >> and_operation.as(:right)).as(:and) | primary } rule(:or_operation) { (and_operation.as(:left) >> or_operator >> or_operation.as(:right)).as(:or) | and_operation } # We start at the lowest precedence rule. root(:or_operation) end\n\nFor example, this user search string:\n\nis:issue AND (author:deborah-digges OR author:monalisa )\n\nwould be parsed into the following AST:\n\n{ \"root\": { \"and\": { \"left\": { \"filter_term\": { \"attribute\": \"is\", \"value\": [ { \"filter_value\": \"issue\" } ] } }, \"right\": { \"or\": { \"left\": { \"filter_term\": { \"attribute\": \"author\", \"value\": [ { \"filter_value\": \"deborah-digges\" } ] } }, \"right\": { \"filter_term\": { \"attribute\": \"author\", \"value\": [ { \"filter_value\": \"monalisa\" } ] } } } } } } }\n\nQuery\n\nOnce the query is parsed into an intermediate structure, the next steps are to:\n\nTransform this intermediate structure into a query document that Elasticsearch understands Execute the query against Elasticsearch to obtain results\n\nExecuting the query in step 2 remained the same between the old and new systems, so let\u2019s only go over the differences in building the query document below.\n\nThe old query generation: linear mapping of filter terms using filter classes\n\nEach filter term (Ex: label:documentation ) has a class that knows how to convert it into a snippet of an Elasticsearch query document. During query document generation, the correct class for each filter term is invoked to construct the overall query document.\n\nThe new query generation: recursive AST traversal to generate Elasticsearch bool query\n\nWe recursively traversed the AST generated during parsing to build an equivalent Elasticsearch query document. The nested structure and boolean operators map nicely to Elasticsearch\u2019s boolean query with the AND, OR, and NOT operators mapping to the must, should, and should_not clauses.\n\nWe re-used the building blocks for the smaller pieces of query generation to recursively construct a nested query document during the tree traversal.\n\nContinuing from the example in the parsing stage, the AST would be transformed into a query document that looked like this:\n\n{ \"query\": { \"bool\": { \"must\": [ { \"bool\": { \"must\": [ { \"bool\": { \"must\": { \"prefix\": { \"_index\": \"issues\" } } } }, { \"bool\": { \"should\": { \"terms\": { \"author_id\": [ \"<DEBORAH_DIGGES_AUTHOR_ID>\", \"<MONALISA_AUTHOR_ID>\" ] } } } } ] } } ] } // SOME TERMS OMITTED FOR BREVITY } }\n\nWith this new query document, we execute a search against Elasticsearch. This search now supports logical AND/OR operators and parentheses to search for issues in a more fine-grained manner.\n\nConsiderations\n\nIssues is one of the oldest and most heavily -used features on GitHub. Changing core functionality like Issues search, a feature with an average of nearly 2000 queries per second (QPS)\u2014that\u2019s almost 160M queries a day!\u2014presented a number of challenges to overcome.\n\nEnsuring backward compatibility\n\nIssue searches are often bookmarked, shared among users, and linked in documents, making them important artifacts for developers and teams. Therefore, we wanted to introduce this new capability for nested search queries without breaking existing queries for users.\n\nWe validated the new search system before it even reached users by:\n\nTesting extensively : We ran our new search module against all unit and integration tests for the existing search module. To ensure that the GraphQL and REST API contracts remained unchanged, we ran the tests for the search endpoint both with the feature flag for the new search system enabled and disabled.\n\n: We ran our new search module against all unit and integration tests for the existing search module. To ensure that the GraphQL and REST API contracts remained unchanged, we ran the tests for the search endpoint both with the feature flag for the new search system enabled and disabled. Validating correctness in production with dark-shipping: For 1% of issue searches, we ran the user\u2019s search against both the existing and new search systems in a background job, and logged differences in responses. By analyzing these differences we were able to fix bugs and missed edge cases before they reached our users. We weren\u2019t sure at the outset how to define \u201cdifferences,\u201d but we settled on \u201cnumber of results\u201d for the first iteration. In general, it seemed that we could determine whether a user would be surprised by the results of their search against the new search capability if a search returned a different number of results when they were run within a second or less of each other.\n\nFor 1% of issue searches, we ran the user\u2019s search against both the existing and new search systems in a background job, and logged differences in responses. By analyzing these differences we were able to fix bugs and missed edge cases before they reached our users.\n\nPreventing performance degradation\n\nWe expected more complex nested queries to use more resources on the backend than simpler queries, so we needed to establish a realistic baseline for nested queries, while ensuring no regression in the performance of existing, simpler ones.\n\nFor 1% of Issue searches, we ran equivalent queries against both the existing and the new search systems. We used scientist, GitHub\u2019s open source Ruby library, for carefully refactoring critical paths, to compare the performance of equivalent queries to ensure that there was no regression.\n\nPreserving user experience\n\nWe didn\u2019t want users to have a worse experience than before just because more complex searches were possible.\n\nWe collaborated closely with product and design teams to ensure usability didn\u2019t decrease as we added this feature by:\n\nLimiting the number of nested levels in a query to five. From customer interviews, we found this to be a sweet spot for both utility and usability.\n\nin a query to five. From customer interviews, we found this to be a sweet spot for both utility and usability. Providing helpful UI/UX cues: We highlight the AND/OR keywords in search queries, and provide users with the same auto-complete feature for filter terms in the UI that they were accustomed to for simple flat queries.\n\nMinimizing risk to existing users\n\nFor a feature that is used by millions of users a day, we needed to be intentional about rolling it out in a way that minimized risk to users.\n\nWe built confidence in our system by:\n\nLimiting blast radius : To gradually build confidence, we only integrated the new system in the GraphQL API and the Issues tab for a repository in the UI to start. This gave us time to collect, respond to, and incorporate feedback without risking a degraded experience for all consumers. Once we were happy with its performance, we rolled it out to the Issues dashboard and the REST API.\n\n: To gradually build confidence, we only integrated the new system in the GraphQL API and the Issues tab for a repository in the UI to start. This gave us time to collect, respond to, and incorporate feedback without risking a degraded experience for all consumers. Once we were happy with its performance, we rolled it out to the Issues dashboard and the REST API. Testing internally and with trusted partners: As with every feature we build at GitHub, we tested this feature internally for the entire period of its development by shipping it to our own team during the early days, and then gradually rolling it out to all GitHub employees. We then shipped it to trusted partners to gather initial user feedback.\n\nAnd there you have it, that\u2019s how we built, validated, and shipped the new and improved Issues search!\n\nFeedback\n\nWant to try out this exciting new functionality? Head to our docs to learn about how to use boolean operators and parentheses to search for the issues you care about!\n\nIf you have any feedback for this feature, please drop us a note on our community discussions.\n\nAcknowledgements\n\nSpecial thanks to AJ Schuster, Riley Broughten, Stephanie Goldstein, Eric Jorgensen Mike Melanson and Laura Lindeman for the feedback on several iterations of this blog post!\n\nTags:", "label": 0}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2014-08", "content": "en\n\n\"Hello Everyone!! Well the time has come. I'm all packed up (I think) and I will be moving away to college. It seems like yesterday that I was starting freshman year of high school. It went through my mind thinking if I will be able to survive the next four years? Luckily I did. :) The next chapter of my life has begun. I I hope that I will graduate college in four years or less. I will really miss everyone that I have met along the way. Anyone from helping me pick my books up in the hall way, that person that was always there for me when I was down or even the person that would join me on a great adventure. All these memories just roaming around in my head. I guess I wont be that far from home?? I'm ready to start even more memories. I am excited to be around new people. I really want to find out who I really am to get to know myself even more. It is a weird thing to say.\n\nThis will be the last time I will sleep in my own bed for a long time. It will be strange not being in the house I grew up in my whole life. There will be no home cook meals ready when I'm done with classes. It will be hard to get used to. My goal is not to get homesick (I will eventually). Like I said I am not going away that far (45 mins.) It is time for me to be more independent of myself. Mom will not be there to hold our hands in college. I will have to make decisions that will guide my daily life and those that will affect me for the rest of my life.\n\nThere are so many people I want to say goodbye to but time has not allowed me. If anyone wants to see me, let me know.\n\nCannot wait for what is ahead.\n\nWish me luck!!!\n\nEDIT: I will also keep updated on social media and make a blog post when I am all moved in.\"", "label": 1}
{"title": "Generative AI is intellectual sharecropping \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2023/09/25/generative-ai-is-intellectual-sharecropping/", "content": "Replied to Digital sharecropping by Nicholas Carr ( roughtype.com ) One of the fundamental economic characteristics of Web 2.0 is the distribution of production into the hands of the many and the concentration of the economic rewards into the hands of the few. It\u2019s a sharecropping system, but the sharecroppers are generally happy because their interest lies in self-expression or socializing, not in making money, and, besides, the economic value of each of their individual contributions is trivial. It\u2019s only by aggregating those contributions on a massive scale \u2013 on a web scale \u2013 that the business becomes lucrative. To put it a different way, the sharecroppers operate happily in an attention economy while their overseers operate happily in a cash economy.\n\nI encountered this (old) analogy for social media platforms as digital sharecropping and thought it also fit generative AI. Generative AI companies steal our intellectual property then license it back to us. We can\u2019t be compensated reasonably for our individual contributions to the model because they\u2019ve stolen from so many of us and each individual\u2019s work represents a miniscule portion of the entire model. Whatever we generate with their models can\u2019t be copyrighted and used to make money for *us* without significant human contributions \u2014 but generated works are in direct competition with the creators whose works built the model. These powerful, well-funded companies want businesses to fire their employees and pay them instead, making businesses reliant on an opaque, unpredictable service that demands vast amounts of natural resources that may be in short \u2014 and shortening \u2014 supply.\n\nBut unlike social media, which rewards users emotionally rather than financially for their labor, creators aren\u2019t getting anything out of having our work used to train generative AI models. And so we\u2019re fighting back earlier in the cycle than with social media \u2014 maybe before it can become entrenched. AI evangelists speak as if the technology\u2019s supremacy is inevitable, but that\u2019s propaganda to get us to shut up and hand over our creations and our jobs.\n\nRecent AI shenanigans in the news:\n\nChatGPT caught giving horrible advice to cancer patients by Sharon Adarlo (Neoscope)\n\nWhy Silicon Valley\u2019s biggest AI developers are hiring poets by Andrew Deck (rest of world)\n\nIowa Is Using ChatGPT to Take Out Banned Books by Alejandra Gularte (Vulture)\n\nGenerative AI at work", "label": 1}
{"title": "How we improved availability through iterative simplification", "url": "https://github.blog/engineering/engineering-principles/how-we-improved-availability-through-iterative-simplification/", "content": "Solving and staying ahead of problems when scaling up a system of GitHub\u2019s size is a delicate process. The stack is complex, and even small changes can have a big ripple effect. Here\u2019s a look at some of the tools in GitHub\u2019s toolbox, and how we\u2019ve used them to solve problems. We\u2019ll also share some of our wins and lessons we learned along the way.\n\nThere are several tools that we use to keep pace with our growing system. While we can\u2019t list them all, here are some that have been instrumental for our growth.\n\nAs we serve requests, there is a constant stream of related numbers that we care about. For example, we might want to know how often events are happening or how traffic levels compare to expected use. We can record metrics for each event in Datadog to see patterns over time and break them down across different dimensions, identifying areas that need focus.\n\nEvents also contain context that can help identify details for issues we\u2019re troubleshooting. We send all this context to Splunk for further analysis.\n\nMuch of our application data is stored in MySQL, and query performance can degrade over time due to factors like database size and query frequency. We have written custom monitors that detect and report slow and timed-out queries for further investigation and remediation.\n\nWhen we introduce changes, we often need to know how those changes affect performance. We use Scientist to test proposed changes. With this tool, we measure and report results before making the changes permanent.\n\nWhen we\u2019re ready to release a change, we roll it out incrementally to ensure it works as expected for all use cases. We also need to be able to roll back in the event of unexpected behavior. We use Flipper to limit the rollout to early access users, then to an increasing percentage of users as we build the confidence.\n\nAchieving faster database queries\n\nWe recently observed a SQL query causing a high number of timeouts. Our investigation in Splunk tracked it down to GitHub\u2019s Command Palette feature, which was loading a list of repositories. The code to generate that list looked something like this:\n\norg_repo_ids = Repository.where(owner: org).pluck(:id) suggested_repo_ids = Contribution.where(user: viewer, repository_id: org_repo_ids).pluck(:repository_id)\n\nIf an org has many active repositories, the second line could generate a SQL query with a large IN (...) clause with an increased risk of timing out. While we\u2019d seen this type of problem before, there was something unique about this particular use case. We might be able to improve performance by querying the user first since a given user contributes to a relatively small number of repositories.\n\ncontributor_repo_ids = Contribution.where(user: viewer).pluck(:repository_id) suggested_repo_ids = Repository.where(owner: org, id: contributor_repo_ids)\n\nWe created a Scientist experiment with a new candidate code block to evaluate performance. The Datadog dashboard for the experiment confirmed two things: the candidate code block returned the same results and improved performance by 80-90%.\n\nWe also did a deeper dive into the queries this feature was generating and found a couple of possible additional improvements.\n\nThe first involved eliminating a SQL query and sorting results in the application rather than asking the SQL server to sort. We followed the same process with a new experiment and found that the candidate code block performed 40-80% worse than the control. We removed the candidate code block and ended the experiment.\n\nThe second was a query filtering results based on the viewer\u2019s level of access and did so by iterating through the list of results. The access check we needed can be batched. So, we started another experiment to do the filtering with a single batched query and confirmed that the candidate code block improved performance by another 20-80%.\n\nWhile we were wrapping up these experiments, we checked for similar patterns in related code and found a similar filter we could batch. We confirmed a 30-40% performance improvement with a final experiment, and left the feature in a better place that made our developers, database administrators, and users happier.\n\nRemoving unused code\n\nWhile our tooling does surface problem areas to focus on, it\u2019s preferable to get ahead of performance issues and fix problematic areas before they cause a degraded experience. We recently analyzed the busiest request endpoints for one of our teams and found room to improve one of them before it escalated to an urgent problem.\n\nData for each request to the GitHub Rails application is logged in Splunk and tagged with the associated controller and action. We started by querying Splunk for the top 10 controller/action pairs in the endpoints owned by the team. We used that list to create a Datadog dashboard with a set of graphs for each controller/action that showed the total request volume, average and P99 request latency, and max request latency. We found that the busiest endpoint on the dashboard was an action responsible for a simple redirect, and that performance regularly degraded to the timeout threshold.\n\nWe needed to know what was slowing these requests down, so we dug into Datadog\u2019s APM feature to show requests for the problematic controller/endpoint. We sorted those requests by elapsed request time to see the slowest requests first. We identified a pattern where slow requests spent a long time performing an access check that wasn\u2019t required to send the redirect response.\n\nMost requests to the GitHub Rails application generate HTML responses where we need to be careful to ensure that all data in the response is accessible to the viewer. We\u2019re able to simplify the code involved by using shared Rails controller filters to verify that the viewer is allowed to see the resources they\u2019re requesting that run before the server renders a response. These checks aren\u2019t required for the redirect, so we wanted to confirm we could serve those requests using a different set of filters and that this approach would improve performance.\n\nSince Rails controller filters are configured when the application boots rather than when each request is processed, we weren\u2019t able to use a Scientist experiment to test a candidate code block. However, filters can be configured to run conditionally, which enabled us to use a Flipper feature flag to change behavior. We identified the set of filters that weren\u2019t required for the redirect, and configured the controller to skip those filters when the feature flag was enabled. The feature flag controls let us ramp up this behavior while monitoring both performance and request status via Datadog and keeping watch for unexpected problems via Splunk.\n\nAfter confirming that performance improved for P75/P99 request latency\u2014and more importantly, reduced max latency to be more consistent and much less likely to time out\u2014we graduated the feature and generalized the behavior so other similar controllers can use it.\n\nWhat did we learn?\n\nThere are several lessons we learned throughout this process. Here are some of the main points we keep in mind.\n\nThe investment in observability is totally worth it! We identified and solved problems quickly because of the metric and log information we track.\n\nEven when you\u2019re troubleshooting a problem that\u2019s been traditionally difficult to solve, the use case may be subtly different in a way that presents a new solution.\n\nWhen you\u2019re working on a fix, look around at adjacent code. There may be related issues you can tackle while you\u2019re there.\n\nPerformance problems are a moving target. Keeping an eye open for the next one helps you fix it when it\u2019s gotten slow rather than when it starts causing timeouts and breaking things.\n\nMake small changes in ways that you can control with a gradual rollout and measure results.", "label": 0}
{"title": "How GitHub engineers tackle platform problems", "url": "https://github.blog/engineering/infrastructure/how-github-engineers-tackle-platform-problems/", "content": "In my spare time I enjoy building Gundam models, which are model kits to build iconic mechas from the Gundam universe. You might be wondering what this has to do with software engineering. Product engineers can be seen as the engineers who take these kits and build the Gundam itself. They are able to utilize all pieces and build a working product that is fun to collect or even play with!\n\nPlatform engineers, on the other hand, supply the tools needed to build these kits (like clippers and files) and maybe even build a cool display so everyone can see the final product. They ensure that whoever is constructing it has all the necessary tools, even if they don\u2019t physically build the Gundam themselves.\n\nAbout a year ago, my team at GitHub moved to the infrastructure organization, inheriting new roles and Areas of Responsibility (AoRs). Previously, the team had tackled external customer problems, such as building the new deployment views across environments. This involved interacting with users who depend on GitHub to address challenges within their respective industries. Our new customers as a platform engineering team are internal, which makes our responsibilities different from the product-focused engineering work we were doing before.\n\nGoing back to my Gundam example, rather than constructing kits, we\u2019re now responsible for building the components of the kits. Adapting to this change meant I had to rethink my approach to code testing and problem solving.\n\nWhether you\u2019re working on product engineering or on the platform side, here are a few best practices to tackle platform problems.\n\nUnderstanding your domain\n\nOne of the most critical steps before tackling problems is understanding the domain. A \u201cdomain\u201d is the business and technical subject area in which a team and platform organization operate. This requires gaining an understanding of technical terms and how these systems interact to provide fast and reliable solutions. Here\u2019s how to get up to speed:\n\nTalk to your neighbors: Arrange a handover meeting with a team that has more knowledge and experience with the subject matter. This meeting provides an opportunity to ask questions about terminology and gain a deeper understanding of the problems the team will be addressing.\n\nArrange a handover meeting with a team that has more knowledge and experience with the subject matter. This meeting provides an opportunity to ask questions about terminology and gain a deeper understanding of the problems the team will be addressing. Investigate old issues: If there is a backlog of issues that are either stale or still persistent, they may give you a better understanding of the system\u2019s current limitations and potential areas for improvement.\n\nIf there is a backlog of issues that are either stale or still persistent, they may give you a better understanding of the system\u2019s current limitations and potential areas for improvement. Read the docs: Documentation is a goldmine of knowledge that can help you understand how the system works.\n\nBridging concepts to platform-specific skills\n\nWhile the preceding advice offers general guidance applicable to both product and platform teams, platform teams \u2014 serving as the foundational layer \u2014 necessitate a more in-depth understanding.\n\nNetworks : Understanding network fundamentals is crucial for all engineers, even those not directly involved in network operations. This includes concepts like TCP, UDP, and L4 load balancing, as well as debugging tools such as dig. A solid grasp of these areas is essential to comprehend how network traffic impacts your platform.\n\n: Understanding network fundamentals is crucial for all engineers, even those not directly involved in network operations. This includes concepts like TCP, UDP, and L4 load balancing, as well as debugging tools such as dig. A solid grasp of these areas is essential to comprehend how network traffic impacts your platform. Operating systems and hardware: Selecting appropriate virtual machines (VMs) or physical hardware is vital for both scalability and cost management. Making well-informed choices for particular applications requires a strong grasp of both. This is closely linked to choosing the right operating system for your machines, which is important to avoid systems with vulnerabilities or those nearing end of life.\n\nSelecting appropriate virtual machines (VMs) or physical hardware is vital for both scalability and cost management. Making well-informed choices for particular applications requires a strong grasp of both. This is closely linked to choosing the right operating system for your machines, which is important to avoid systems with vulnerabilities or those nearing end of life. Infrastructure as Code (IaC): Automation tools like Terraform, Ansible, and Consul are becoming increasingly essential. Proficiency in these tools is becoming a necessity as they significantly decrease human error during infrastructure provisioning and modifications.\n\nAutomation tools like Terraform, Ansible, and Consul are becoming increasingly essential. Proficiency in these tools is becoming a necessity as they significantly decrease human error during infrastructure provisioning and modifications. Distributed systems: Dealing with platform issues, particularly in distributed systems, necessitates a deep understanding that failures are inevitable. Consequently, employing proactive solutions like failover and recovery mechanisms is crucial for preserving system reliability and preventing adverse user experiences. The optimal approach for this depends entirely on the specific problem and the desired system behavior.\n\nKnowledge sharing\n\nBy sharing lessons and ideas, engineers can introduce new perspectives that lead to breakthroughs and innovations. Taking the time to understand why a project or solution did or didn\u2019t work and sharing those findings provides new perspectives that we can use going forward.\n\nHere are three reasons why knowledge sharing is so important:\n\nTeamwork makes the dream work: Collaboration often results in quicker problem resolution and fosters new solution innovation, as engineers have the opportunity to learn from each other and expand upon existing ideas.\n\nCollaboration often results in quicker problem resolution and fosters new solution innovation, as engineers have the opportunity to learn from each other and expand upon existing ideas. Prevent lost knowledge : If we don\u2019t share our lessons learned, we prevent the information from being disseminated across the team or organization. This becomes a problem if an engineer leaves the company or is simply unavailable.\n\n: If we don\u2019t share our lessons learned, we prevent the information from being disseminated across the team or organization. This becomes a problem if an engineer leaves the company or is simply unavailable. Improve our customer success: As engineers, our solutions should effectively serve our customers. By sharing our knowledge and lessons learned, we can help the team build reliable, scalable, and secure platforms, which will enable us to create better products that meet customer needs and expectations!\n\nBut big differences start to appear between product engineering and infrastructure engineering when it comes to the impact radius and the testing process.\n\nImpact radius\n\nWith platforms being the fundamental building blocks of a system, any change (small or large) can affect a wide range of products. Our team is responsible for DNS, a foundational service that impacts numerous products. Even a minor alteration to this service can have extensive repercussions, potentially disrupting access to content across our site and affecting products ranging from GitHub Pages to GitHub Copilot.\n\nUnderstand the radius: Or understand the downstream dependencies. Direct communication with teams that depend on our service provides valuable insights into how proposed changes may affect other services.\n\nOr understand the downstream dependencies. Direct communication with teams that depend on our service provides valuable insights into how proposed changes may affect other services. Postmortems: By looking at past incidents related to our platform and asking \u201cWhat is the impact of this incident?\u201d, we can form more context around what change or failure was introduced, how our platform played a role in it, and how it was fixed.\n\nBy looking at past incidents related to our platform and asking \u201cWhat is the impact of this incident?\u201d, we can form more context around what change or failure was introduced, how our platform played a role in it, and how it was fixed. Monitoring and telemetry: Condense important monitoring and logging into a small and quickly digestible medium to give you the general health of the system. This could be a Single Availability Metric (SAM), for example. The ability to quickly glance at a single dashboard allows engineers to rapidly pinpoint the source of an issue and streamlines the debugging and incident mitigation process, as compared to searching through and interpreting detailed monitors or log messages.\n\nTesting changes\n\nTesting changes in a distributed environment can be challenging, especially for services like DNS. A crucial step in solving this issue is utilizing a test site as a \u201creal\u201d machine where you can implement and assess all your changes.\n\nInfrastructure as Code (IaC): When using tools like Terraform or Ansible, it\u2019s crucial to test fundamental operations like provisioning and deprovisioning machines. There are circumstances where a machine will need to be re-provisioned. In these cases, we want to ensure the machine is not accidentally deleted and that we retain the ability to create a new one if needed.\n\nWhen using tools like Terraform or Ansible, it\u2019s crucial to test fundamental operations like provisioning and deprovisioning machines. There are circumstances where a machine will need to be re-provisioned. In these cases, we want to ensure the machine is not accidentally deleted and that we retain the ability to create a new one if needed. End-to-End (E2E): Begin directing some network traffic to these servers. Then the team can observe host behavior by directly interacting with it, or we can evaluate functionality by diverting a small portion of traffic.\n\nBegin directing some network traffic to these servers. Then the team can observe host behavior by directly interacting with it, or we can evaluate functionality by diverting a small portion of traffic. Self-healing: We want to test the platform\u2019s ability to recover from unexpected loads and identify bottlenecks before they impact our users. Early identification of bottlenecks or bugs is crucial for maintaining the health of our platform.\n\nIdeally changes will be implemented on a host-by-host basis once testing is complete. This approach allows for individual machine rollback and prevents changes from being applied to unaffected hosts.\n\nWhat to remember\n\nPlatform engineering can be difficult. The systems GitHub operates with are complex and there are a lot of services and moving parts. However, there\u2019s nothing like seeing everything come together. All the hard work our engineering teams do behind the scenes really pays off when the platform is running smoothly and teams are able to ship faster and more reliably \u2014 which allows GitHub to be the home to all developers.\n\nWant to dive deeper? Check out our infrastructure related blog posts.", "label": 0}
{"title": "A well deserve detachment of the US Food diet", "url": "https://lifeofpablo.com/blog/a-well-deserve-detachment-of-the-us-food-diet", "content": "A well deserve detachment of the US Food diet\n\nThis post was written in English (en_US).\n\nI've noticed that my diet hasn't been the best in the last few weeks. I've been eating out a bit more than I would like. I hate to say but I've been busy building new things on the web, work projects, and life overall. I'm taking a well deserved holiday soon to a place special to me in Mexico. I've been thinking how I need to cleanse and detach myself from the US food culture. I want to stay away as far from chemicals and additives. Don't get me wrong these are found in all countries, especially in processed foods.\n\nI'm excited to get some homemade food in Mexico. I'm excited for fresh ingredients from the farms and gardens in the village I am staying in. I feel that it's hard to know where your food is coming from when in the United States.\n\nThe goal is not to eat at any US-based chain fast food places or hamburgers, pizza - you know the common fast food culprits? This should be relatively easy because those are far away from the village I am staying in. Fast food has infiltrated even the most remote places. I want to indulge in so many authentic foods I don't get the chance to eat except when I am in Mexico. This is a change to reconnect with my roots, reform past relationships and find parts of me that United States culture has suppressed. Food is a good way to reconnect with lost parts of me. I will attach to good things and detach from bad things. At least for a while.", "label": 1}
{"title": "Gemini 2.5: Updates to our family of thinking models", "url": "https://developers.googleblog.com/en/gemini-2-5-thinking-model-updates/", "content": "Today we are excited to share updates across the board to our Gemini 2.5 model family: Gemini 2.5 Pro is generally available and stable (no changes from the 06-05 preview) Gemini 2.5 Flash is generally available and stable (no changes from the 05-20 preview, see pricing updates below) Gemini 2.5 Flash-Lite is now available in preview Gemini 2.5 models are thinking models, capable of reasoning through their thoughts before responding, resulting in enhanced performance and improved accuracy. Each model has control over the thinking budget, giving developers the ability to choose when and how much the model \u201cthinks\u201d before generating a response.\n\nOverview of our family of Gemini 2.5 thinking models\n\nIntroducing Gemini 2.5 Flash-Lite Today, we\u2019re introducing 2.5 Flash-Lite in preview with the lowest latency and cost in the 2.5 model family. It\u2019s designed as a cost-effective upgrade from our previous 1.5 and 2.0 Flash models. It also offers better performance across most evals, and lower time to first token while also achieving higher tokens per second decode. This model is great for high throughput tasks like classification or summarization at scale. Gemini 2.5 Flash-Lite is a reasoning model, which allows for dynamic control of the thinking budget with an API parameter. Because Flash-Lite is optimized for cost and speed, \u201cthinking\u201d is off by default, unlike our other models. 2.5 Flash-Lite also supports all of our native tools like Grounding with Google Search, Code Execution, and URL Context in addition to function calling.\n\nBenchmarks for Gemini 2.5 Flash-Lite\n\nOver the last year, our research teams have continued to push the pareto frontier with our Flash model series. When 2.5 Flash was initially announced, we had not yet finalized the capabilities for 2.5 Flash-Lite. We also launched with a \u201cthinking\u201d and \u201cnon-thinking price\u201d, which led to developer confusion.\n\n\n\nWith the stable version of Gemini 2.5 Flash rolling out (which is the same 05-20 model preview we made available at Google I/O), and the incredible performance of 2.5 Flash, we are updating the pricing for 2.5 Flash: $0.30 / 1M input tokens (*up from $0.15 input) $2.50 / 1M output tokens (*down from $3.50 output) We removed the thinking vs. non-thinking price difference We kept a single price tier regardless of input token size While we strive to maintain consistent pricing between preview and stable releases to minimize disruption, this is a specific adjustment reflecting Flash\u2019s exceptional value, still offering the best cost-per-intelligence available. And with Gemini 2.5 Flash-Lite, we now have an even lower cost option (with or without thinking) for cost and latency sensitive use cases that require less model intelligence.\n\nPricing updates for our Gemini Flash family\n\nIf you are using the Gemini 2.5 Flash Preview 04-17 , the existing preview pricing will remain in effect until its planned deprecation on July 15, 2025, at which point that model endpoint will be turned off. You can transition to the generally available model \u201cgemini-2.5-flash\u201d, or switch to 2.5 Flash-Lite Preview as a lower cost option.\n\nContinued growth of Gemini 2.5 Pro The growth and demand for Gemini 2.5 Pro continues to be the steepest of any of our models we have ever seen. To allow more customers to build on this model in production, we are making the 06-05 version of the model stable, with the same pareto frontier price point as before. We expect that cases where you need the highest intelligence and most capabilities are where you will see Pro shine, like coding and agentic tasks. Gemini 2.5 Pro is at the heart of many of the most loved developer tools.\n\nTop developer tools using Gemini 2.5 Pro", "label": 0}
{"title": "Testing the Firefox alternatives", "url": "https://tommorris.org/posts/2024/testing-the-firefox-alternatives/", "content": "Firefox and Mozilla are in a bit of a state. Firefox and Mozilla are always in a bit of a state, but the recent release of Firefox 130 which includes an AI chatbot feature has prompted a lot of head scratching.\n\nYes, I get that some people want it, and you can turn it off. And, no, that is not the issue. The issue is Mozilla are spending a lot of time and effort on chasing the sort of silly AI hype that gets Tony Blair excited. That\u2019s rather disconcerting if you\u2019d like Mozilla to focus on making a good browser instead of chasing the new and shiny because it is new and shiny.\n\n(In their defence: I\u2019ll note that some of the AI stuff Mozilla is pursing is reasonable. The translation feature uses a local model for translation, which is a great idea. It doesn\u2019t support all the languages that Google Translate does, but it\u2019s good. )\n\nAnyway, the rot just goes on. In July, it was the rollout of privacy-preserving attribution (having previously purchased Anonym). Before that, it was the rollout of the ghastly Proton UI, which necessitated (and still necessitates) setting up Lepton aka Firefox-UI-Fix. There\u2019s plenty more things people are grumpy about\u2014abandoning Servo, Solo (yet more AI hype), Pocket\u2026 there\u2019s so many things, one begins to lose track.\n\nYou can totally still stick with Firefox\u2026 but remember to download userChrome.css from GitHub. And if you want vertical tabs, be sure to install Sidebery until Mozilla get around to implementing native vertical tabs. (You know, like Chromium-based browsers including Edge and Vivaldi already have.)\n\nAnd then turn off a bunch of settings and install a bunch of extensions to tighten up privacy.\n\nAnd then brace yourself for Mozilla to find new and exciting ways to disappoint you every six weeks with some new brand of nonsensical bullshit.\n\nWhen the Privacy Preserving Ads stuff was rolled out, I pulled the trigger and switched to LibreWolf. The AI chat stuff has prompted a lot of people to look for Firefox but with less Mozilla nonsense.\n\nSwitching to what are essentially \u201cFirefox distros\u201d to collectively try and nudge Mozilla back to making more sensible decisions instead of AI hype chasing is an eminently reasonable one. There\u2019s more reasons than just that. Part of the reason I use Firefox-based browsers rather than Chromium browsers is because I want to preserve some choice and diversity in browser engines. The existence of a choice of different Firefox derived browsers may allow space for experimentation in designing better browsers. In Chromium land, Arc has shown that there\u2019s an opportunity for quite radically rethinking how browsers work. Same for Orion in Mac/iOS-land.\n\nThis isn\u2019t a detailed review of the different browsers, just a few comments and observations having tried them.\n\nLibreWolf\n\nLibreWolf has been my daily driver for a few months now. It\u2019s very close to Firefox, but with a bunch of privacy-focussed defaults.\n\nSome of these are straightforward: uBlock Origin is pre-installed, for instance. Google is not set as a default search engine\u2014DuckDuckGo, Qwant, Mojeek, and a public SearXNG instance are. HTTPS only is on by default. There\u2019s no sponsored content on the home page, and there\u2019s no telemetry.\n\nLibreWolf\u2019s commitment to good privacy defaults is simultaneously a feature and a problem. The \u201cresist fingerprinting\u201d (RFP) settings which are pulled in from the Tor Browser actively make using certain websites impossible. They trigger some kind of anti-abuse mechanisms on said websites which then require that I complete a CAPTCHA, and there is only so much of my life I wish to spend clicking squares containing fire hydrants, \u201ccrosswalks\u201d, or yellow New York taxis or various other bits of American street furniture. If it were just on crappy websites, I wouldn\u2019t mind but the main place I encounter this is actually JSTOR, along with a number of other academic/research databases. Hindering access to the few websites where the content is written by subject matter experts rather than AI-generated marketing slop is inconvenient enough that I turned RFP off via a trip to about:config .\n\nWhat I\u2019d really like is if RFP could be turned off on a per-site basis. Or if one could use the containers feature in Firefox (perhaps with Multi-Account Containers) to have RFP running on most sites, with special treatment for the websites that don\u2019t like it.\n\nSimilarly, not storing history is a great privacy choice. It\u2019s also an incredibly annoying choice for the average user who would quite like to be able to find that blog post they spent hours searching for again in their browser\u2019s history section. And, yes, you can turn it off.\n\nIf you are the sort of person who has a YubiKey, has attended a GPG key signing party, or has opinions about elliptic curve cryptography, LibreWolf may be a great fit for your tastes. If you\u2019re not, beware that security and usability are destined to have a rocky relationship\u2014and you may have to go fumbling around to disable certain security features if certain websites get aggrieved at the hardened privacy choices in LibreWolf.\n\nFloorp\n\nFloorp is a Firefox-based browser built by Ablaze, \u201ca group of creators primarily active in Japan. It is a community and not a company or a public organization\u201d (according to their FAQ).\n\nWhen you start, you can choose a \u201ctemplate\u201d. The UI doesn\u2019t really tell you what the templates are, but if you push the buttons, it\u2019ll show you.\n\nThe \u201cdefault\u201d options gives you an Vivaldi/Edge style sidebar containing icons that open up sidebar tabs for bookmarks, history and a \u2018notes\u2019 feature, plus links to documentation, Google Translate (I\u2019m home the Firefox local translation will come at some point), and passwords, settings and extensions. In addition, there\u2019s a sidebar panel called the \u201cbrowser manager\u201d which is history, downloads, tags and bookmarks rolled into one. It has a menu bar at the top. Presumably, on Windows and Linux, this is the primary way to access the menu bar, but this is probably a bug that needs fixing on the Mac version.\n\nThe \u201cbasic\u201d template disables this sidebar. The \u201cadvanced\u201d template enables a status bar that contains a \u201cfull screen\u201d button and a \u201ctake a screenshot\u201d button, and not much else, though you can obviously put stuff on there. The \u201cdefault\u201d option is fine. You can remove items from the sidebar, but annoyingly. It\u2019d be nice if you could move extension icons there, but that\u2019s a limitation of Firefox.\n\nIn addition, Floorp allows you to choose from a variety of different Firefox UI designs including both Proton and Photon/Lepton. It has built-in vertical tabs, without the fuss (but also without the features) of Sidebery. If you\u2019re intending to use vertical tabs, the Proton UI actually fits in aesthetically in both light and dark themes in a way it doesn\u2019t in Firefox.\n\nThe sensible privacy defaults of LibreWolf aren\u2019t there, so you\u2019ll probably want to install uBlock Origin, change from Google to a search engine that doesn\u2019t use text regurgitation software to advise users to \u201cenjoy the crunchy texture of glass without worrying about it contributing to weight gain\u201d, relax in a bath with a toaster and all the other stuff to turn the web from an absolute wreck into almost usable.\n\nOne feature I do like a lot in both Floorp and Zen: workspaces. These are very similar to both Arc\u2019s profiles and spaces features, and \u201ctab groups\u201d in Safari and Orion, and are handy for separating out personal and work-related tabs, or dealing with the general irritation caused by having multiple logins for different Single Sign On (another tech industry lie).\n\nDetails on the website are a bit sparse, and quite a lot of the Japanese documentation hasn\u2019t been translated into English. It\u2019s decent though, and offers lots of customisability.\n\nZen\n\nZen is relatively new. The design is clearly very inspired by Arc (rounded corners galore). Vertical tabs by default (plans are afoot to add boring old-style tabs).\n\nThere\u2019s lots to say about the layout. There\u2019s a compact mode which makes everything slide away.\n\nLike Floorp, there\u2019s a sidebar feature. Given the tab bar is a sidebar, to distinguish it from the tab bar, the settings refer to it as \u201cWeb Side Panels\u201d\u2014while the UI calls it the \u201cZen Sidebar\u201d. It\u2019s like a browser-in-a-browser. It comes preloaded with links for Wikipedia, Twitter/X, YouTube, Google Translate and Todoist. I can see the use cases for all of them: quickly looking stuff up on the side in Wikipedia, tweeting (excreting?), watching videos, translating snippets of text, and marking off items on your to-do list\u2026 but there doesn\u2019t seem to be the kind of coherence there as there is with using the sidebar to put browser features like bookmarks, history etc. I disabled the Web Side Panels/Zen Sidebar it in the settings as I don\u2019t think I\u2019ll use it.\n\nSomewhat confusingly, in Keyboard Shortcuts, I can set shortcuts to open history and bookmarks in a traditional Firefox-style sidebar. And I can open these from the View menu. In the tab sidebar, there are buttons for opening the history and bookmarks. The bookmarks button opens the bookmarks in a sidebar, while the history opens up the equivalent of the recently closed tabs section.\n\nIf you really like sidebars, you could have a sidebar with your tabs, then click \u2018bookmarks\u2019 to open a bookmarks sidebar, then have a \u201cZen Sidebar\u201d (aka a Web Side Panel). But you probably shouldn\u2019t.\n\nOne small nitpick I did find\u2014the icons for opening the bookmarks and history or sliding the tab sidebar over don\u2019t have tooltips. The settings button has a tooltip which reads \u201cOpen settings ({$shortcut})\u201d which presumably should contain a keyboard shortcut.\n\nOn macOS, another weird part\u2014if you use a tab sidebar, the red/yellow/green buttons are no longer in the corner\u2014they\u2019re immediately above the content area. This looks a bit odd. It\u2019s probably remediable by having the tab bar on the right\u2026 but the checkbox to move it to the right doesn\u2019t work for me. You can fix it by switching on \u201cUse legacy toolbar position\u201d, or by enabling the title bar in the toolbar customisation menu.\n\nOne other slight annoyance: when I tried to log on to a site supporting Passkeys, it stated \u201cThis browser or device is reporting partial passkey support.\u201d\n\nThe above complaints may sound like I dislike Zen\u2014I very much don\u2019t. It\u2019s described as \u201calpha\u201d on their website and release notes, and it\u2019s remarkably good given the first alpha release came out less than two months ago, so this kind of UI nitpicks will almost certainly go away given how much progress they\u2019re making in a very short time.\n\nThey\u2019ve made a theme store to collect themes specifically for Zen, which is helpful given the significant distance it has travelled from stock Firefox in terms of UI design.\n\nLike Floorp, there\u2019s workspaces, which are very handy and integrate with containers. Also like Floop (but unlike LibreWolf out-of-the-box), you can use Firefox Sync.\n\nThe default search engine is DuckDuckGo, which is an improvement on Google. There\u2019s also Google, (English) Wikipedia and eBay.\n\nAgain, the privacy defaults are pretty much like Firefox\u2019s, so you\u2019ll want to install uBlock Origin and any other privacy-related add-ons that you would in Firefox.\n\nOnwards\n\nThere are a bunch of other Firefox-derived browsers like Waterfox, Pale Moon, Basilisk, IceCat. I haven\u2019t tried them, and don\u2019t know much about them.\n\nOne important thing to keep an eye on how quickly security fixes get pushed through from upstream Firefox to Firefox-derived browsers. For a long time, I stuck with Firefox precisely because of this. It\u2019s a sign of how bad the rot has got at Mozilla that people are so keen on alternatives. I really hope Mozilla might learn something from this, but history suggests they\u2019ll try literally anything else first, and the love-hate relationship will never end.", "label": 1}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2024-06", "content": "en\n\nA screenshot of Homebrew Website Club featuring David Shanske, Angelo Gladding, Tracy Durnell, Kevin Yank, and me!\n\nMany dread attending video conferences due to their job or don't like it for some reason. I don't mind it for many reasons. Especially after the pandemic, the methods I use to interact with people have drastically changed.\n\nAttending online meetings is fun for me especially if I enjoy attending meetings on topics I enjoy! Attending anything IndieWeb-related via Homebrew Website Club. Topics discussed are the IndieWeb technologies, technical solutions, receiving advice on building something, and more. Of course, the meetings are not always technical. One of my favorite things we do in Homebrew Website Club is showing off the changes we've done on our personal websites! I love it when someone shares that new feature on their site or that blog post that someone finally finished! It makes me happy that people are proud of their work!\n\nWe also share those articles we like or share that cool website with that cool font or the picture someone took today. The meetings are for everyone to share a topic that is interesting to them. The conversation possibilities are endless. I've received great advice from fellow IndieWeb members on how to do XYZ. I also love picking the brains of others on XYZ especially when it comes to my personal website. I am very grateful for this. Some days I don't participate as often but I'm always attentive to what I can learn from others.\n\nI always look forward to events beyond Homebrew Website Club meetings! These are something to look forward to! These events usually have a specific goal or topic in mind. For example, I love attending writing meet-ups for dedicated writing time. It helps provide a dedicated time to write a blog post that's been marinating in my drafts for a while. Maybe you get the inspiration to write something completely new. I also enjoy the Build a Website in an Hour event to create a new page on your personal website or launch a completely new website. The event ideas are infinite! It'd be nice to see more variations on events. My goal is to host an event soon! Stay tuned!\n\nEveryone and anyone is welcome to attend these online events. You don't need to be part of the IndieWeb. If you're curious about joining one of the events, just come hang out with us! You don't have to have a personal website or be tech-savy. We'd be happy to answer your questions! Attending these meetings is a great way to connect with people located on the other side of the world. Maybe you'll meet someone who lives in the same city as you?\n\nThis blog post has been posted on IndieNews", "label": 1}
{"title": "I Voted in the California Primary", "url": "https://lifeofpablo.com/blog/i-voted-in-the-california-primary", "content": "Today is March 5th, 2024. I voted in the California Primary for the first time since moving here. I guess I do officially live here. I love how easy it is to vote in person here in Sacramento. The original plan was to submit my mail-in-ballot but I accidentally got my primary ballot soaked when I was carrying my water bottle. Of course, I forgot to request a new one and days went on without thinking about it. Then it dawned on me that today was voting day. I had in my navigation to a specific voting location and on the way there, I saw another polling location. I canceled the navigation and pulled into the polling places I had just driven by. I was so excited because, why go out of my way when I can do it right then and there!\n\nChecking in was easy. I gave my basic information for them to print off the ballot. There was an option to do it electronically but I chose paper instead. I was briefed on where the ballot drop-box was located to place the ballot after I have completed filling it out. My ballot was placed in a secrecy vanilla folder to provide discretion in carrying it between the voting official station and the voting cubicle. After receiving the ballot, I went to a voting station cubicle and filled in my ballot. I voted for those I felt would do good in their current or future positions. I had done some research on the candidates. If you don't, you really should see what each candidate stands for and look at the finer details. Then, once I was satisfied with myself filling in all the ovals, I placed the ballot in the secrecy vanilla folder to prepare to turn it in. I walked across the room to place the ballot in the ballot drop-box.\n\nFinally, I got a sticker to show off that I voted. This time I chose a sticker that says, \"Yo Vot\u00e9\" which is Spanish for, \"I Voted\".\n\nIt was that easy! It seemed very seamless. This really motivates me to pay attention in local elections that help shape the future of the city I am living in. Voting is important and a fundamental right which I am happy to exercise. I'm happy I participated in doing my civic duty to vote. We all have a voice. We are the voice for those who can't.", "label": 1}
{"title": "Evolving our infrastructure through the messaging system model in Dropbox", "url": "https://dropbox.tech/infrastructure/infrastructure-messaging-system-model-async-platform-evolution", "content": "The asynchronous platform at Dropbox integrates a suite of services that enable tasks and workflows to function independently without having to wait on one another. This is pretty important to our work as developers: It empowers any service within Dropbox to initiate and schedule tasks, seamlessly supporting over 400 product use cases\u2014including Dropbox Dash and our other AI innovations\u2014and efficiently routing more than 30 million tasks every minute. It also handles change data capture (CDC) use cases, where changes in our underlying storage system, including the FileSystem, are relayed to various product lambdas and processes. In short, it helps us ensure impactful and efficient business operations. This implementation was essential to our growth from where we were a couple of years ago. Back then, the asynchronous platform struggled with scalability and reliability, frequently falling short of the demands of our expanding product portfolio. For product engineers, the platform posed additional hurdles due to limited developer productivity tools, making it cumbersome to build and iterate on asynchronous workflows. Today\u2019s transformation into a robust and scalable system marks a dramatic shift from those early challenges\u2014it enables innovation at a desired pace. In this blog, we\u2019ll introduce an open messaging system model (MSM), which played a key role in evolving our platform. It helped us build a unified event-driven system capable of orchestrating a wide range of asynchronous tasks and meeting future needs, especially as we focus on AI. Inspired by the Open Systems Interconnection (OSI) model, the MSM divides our platform into five logical layers. This standardization simplifies layers such as frontend interfaces, lambda functions, event schedulers, and event routers, allowing them to work across various use cases with different delivery guarantees and data sources, including those related to CDC. Let\u2019s get into it.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images\u2014plus generative AI capabilities across even more connected apps. See what's new \u2192\n\nChallenges and limitations in our asynchronous infrastructure\n\nBeginning in 2021, our infrastructure comprised multiple asynchronous systems, each tailored to specific product or process requirements. These systems facilitated diverse functions\u2014such as streaming events for Dropbox file uploads and edits\u2014as well as supporting domains like security, abuse prevention, machine learning, and search indexing. Additionally, Dropbox integrated CDC functionality, enabling any modification within the underlying storage systems to generate an event, subsequently activating the async infrastructure. Despite occasional functional overlaps, these systems were developed, operated, and maintained separately, leading to inconsistencies in development speed, reliability, and operational ease. Key issues and limitations with these systems were as follows: Developer efficiency\n\nThe complexity of the current systems required product engineers to undertake a steep learning curve and assume responsibility for operational tasks such as capacity planning, release processes, and support, leading to reduced development speed and productivity. Reliability\n\nThese systems had varied service-level objectives (SLOs) for availability, latency, processing, and recovery, which resulted in inconsistent and unreliable performance. Additionally, systems were not multi-homed, and this created significant reliability risk for multiple business use cases in the event of data center failure. Operability\n\nThe variety of systems led to higher operational costs due to their complexity, requiring additional development effort for maintenance and support. The asynchronous components in our technology stack relied on a mix of external queuing solutions, such as Kafka, Redis, and Amazon SQS, creating an infrastructure that was challenging to manage and operate. System scalability\n\nAt the beginning of 2021, our system was processing over 30 billion requests daily to dispatch jobs to lambda functions. (Lambda is a serverless cloud service that runs your code automatically in response to events, without requiring you to manage any servers.) However, meeting the defined SLOs became increasingly challenging. Certain critical components, such as the delayed event scheduler, had already maxed out their throughput capacity. Consequently, we had to implement rigorous screening protocols for each new use case before onboarding in order to ensure it adhered to the system's capacity limitations and wouldn't jeopardize its performance. Lambda infrastructure\n\nThe lambda-based architecture utilized on the consumer side was complex and diverged from the Dropbox service-oriented architecture (SOA) guidelines and established best practices. Consequently, diagnosing and investigating issues on the consumption side became highly challenging, as it didn't integrate seamlessly with the Dropbox infrastructure and recommended methodologies. This lack of alignment resulted in several adverse effects, notably: Release consistency : The release procedures across these systems lacked uniformity and robust safety measures, introducing deployment and update risks.\n\nThe release procedures across these systems lacked uniformity and robust safety measures, introducing deployment and update risks. Compute efficiency : The compute clusters supporting these systems operated below peak efficiency, resulting in suboptimal resource utilization.\n\nThe compute clusters supporting these systems operated below peak efficiency, resulting in suboptimal resource utilization. No autoscaling: The absence of autoscaling for lambda infrastructure, stemming from its deviation from the Dropbox SOA guidelines, resulted in poor integration with our autoscaling infrastructure. As a result, there was a reliance on customer or platform-owner intervention to manually augment capacity when the base capacity proved inadequate to manage the workload. Extensibility\n\nExtensibility posed a significant challenge for these systems, characterized by a deficiency in flexibility and scalability to adapt to emerging product demands. The current solutions were ill-equipped to seamlessly integrate new workflows, and any attempts to expand them would introduce unnecessary complexities in implementation. With the introduction of Cypress, our new filesystem architecture, the existing system faced limitations in expanding our CDC pipeline to distribute Cypress events to multiple subscribers within Dropbox. In all, these challenges underscored the need for a more unified and consistent approach to our asynchronous infrastructure, emphasizing the importance of addressing developer velocity, reliability, operability, efficiency, and extensibility to better support the company's evolving product landscape.\n\nRethinking our approach\n\nThe existing async systems already supported over 400 business use cases. The large number of existing use cases meant we didn\u2019t have the flexibility to construct an entirely new system from scratch, as the migration would have been very time consuming. Instead, we decided to adopt a phased approach, with incremental steps to rebuild existing systems that mitigate risks associated with migrating existing production flows to a new infrastructure. Returning to the drawing board, we outlined three primary goals for the new platform, envisioning a gradual and incremental build-up of capabilities: Development velocity Simplify the asynchronous interface to streamline platform adoption for product engineers. This allows them to focus on creating innovative product features rather than investing time in understanding the complex asynchronous landscape and determining the most suitable system for their use case.\n\nDecrease the operational burden on product engineers by implementing release practices that identify code regressions during deployment and automatically initiate rollbacks if a new release breaches predefined thresholds.\n\nEnable automatic compute scaling when a lambda function encounters a backlog of events to process, ensuring that the current base capacity is augmented if deemed insufficient. Robust and extensible async foundation Unify common elements and patterns across existing async systems within Dropbox and simplify the interface.\n\nSupport new use cases with minimal modifications and avoid the need to build entire new systems by providing extensible components and flexible APIs. Cost and operational efficiency Streamline the foundational infrastructure by phasing out redundant systems (where applicable) and cut down on operational costs.\n\nTransition lambda infrastructure to the Dropbox SOA stack to increase compute efficiency and enable functionalities such as autoscaling, multihoming, and improved out-of-the-box monitoring capabilities. The overarching key performance indicator (KPI) that we aimed to improve over time was the \"time to launch\" for product engineers to deploy a new use case into production. As platform owners, our primary KPI of interest was the \"oncall time\" expended on a weekly basis.\n\nThe five layers of the messaging system model\n\nThe initial step in the refinement of the async system involved deconstructing it into its fundamental layers. We undertook this process to achieve the aforementioned objectives. Subsequently, a systematic approach was devised, beginning with the dissection of the async system into its core elements, followed by the formulation of a bottom-up strategy for its progressive enhancement. From a macroscopic standpoint, the asynchronous system can be mapped to an MSM consisting of three primary layers, analogous to the seven layers of the OSI model in network transmission frameworks. These three primary layers are: Customer layer: This component, also known as the \u201cfrontend layer,\u201d encompasses the various pathways through which users interact and interface with the async system. It encapsulates the mechanisms by which users communicate with and integrate into the async environment.\n\nThis component, also known as the \u201cfrontend layer,\u201d encompasses the various pathways through which users interact and interface with the async system. It encapsulates the mechanisms by which users communicate with and integrate into the async environment. Orchestration layer: This layer is intrinsic to the async system and encompasses the entirety of the tasks required for the scheduling and transmission of async operations to the compute layer (also known as the \u201cexecution layer\u201d). It serves as the intermediary stage between the customer layer and the compute layer, and it\u2019s responsible for ensuring that various components and services interact seamlessly to fulfill complex workflows and business logic requirements.\n\nThis layer is intrinsic to the async system and encompasses the entirety of the tasks required for the scheduling and transmission of async operations to the compute layer (also known as the \u201cexecution layer\u201d). It serves as the intermediary stage between the customer layer and the compute layer, and it\u2019s responsible for ensuring that various components and services interact seamlessly to fulfill complex workflows and business logic requirements. Compute layer: This layer is the execution hub of the async system, where the actual processing and execution of async tasks take place. It is responsible for the seamless execution of asynchronous operations, thereby ensuring the efficient functioning of the system as a whole.\n\nA 10,000-foot view of the async system\n\nThe three layers mentioned above can then be further broken down into five, more specific layers\u2014frontend, scheduler, flow control, delivery, and execution\u2014with each new layer serving an important role within the above three buckets. (Some overlap occurs between the customer and orchestration layers). These five layers of the MSM are illustrated in the diagram below.\n\nAn illustration of the five components of the Messaging System Model (MSM)\n\nNow, let's take a closer look at each of these five layers.\n\nFrontend In the architecture of an asynchronous system, the frontend layer assumes the critical role of serving as the primary interface for user interaction with the system. It represents the user-facing aspect of the asynchronous environment, orchestrating seamless communication and integration with the system's core functionalities. Users are categorized into two distinct groups: first, there are the regular product engineers who utilize programmatic methods to invoke a publish remote procedure call (RPC) and enqueue events, destined to be consumed by one or more subscribers. The second category encompasses systems such as databases or event sources, which necessitate the enqueuing of changes to diverse objects, entities, or files, thereby propelling both internal and external business workflows forward. A pivotal responsibility of the frontend layer is the management of the schema registry and the rigorous validation of every event schema traversing the system. This stringent schema validation process ensures that published events conform to the predefined contract established with subscribers. Additionally, the frontend layer is tasked with the intricate conversion of disparate message formats, including JSON, Proto, and Avro, among others, into a standardized message format\u2014typically protocol buffers\u2014compatible with the internal asynchronous implementation. Furthermore, the frontend component is entrusted with guaranteeing the durability of all events published to the asynchronous system, thereby safeguarding the integrity and reliability of the system's data flow. Scheduler The scheduler is the core engine within an async system and plays a crucial role in coordinating and dispatching disparate events for various consumers that subscribe to these events. This layer plays various roles. For example, for a CDC use case, this will call external data source APIs to get relevant range for the payloads that will be delivered to the subscribers. For a use case where events need delayed execution, the scheduler would store these events separately so they can be trigger at desired timestamp with a process keeping tabs on these events and publishing them to subscribers at those desired scheduled timestamps. Scheduler also has the responsibility to maintain the order of execution of the events and ensures task delivery to subscribers based on this order. Flow control Flow control plays a pivotal role in the orchestration layer, managing the distribution of tasks to subscribers based on several factors, such as subscriber availability, task priority, and potential throttling events. For instance, in a CDC scenario, the orchestration layer dynamically adjusts the rate of queries dispatched to subscribers. This adaptation occurs when the orchestration layer detects that a subscriber is unable to handle the job throughput effectively or when the source, backing CDC, signals the scheduler client to reduce the pace. State management, another function of this layer, encompasses the maintenance of data structures responsible for tracking ongoing events and their respective statuses (such as pending, running, or complete). Additionally, it incorporates mechanisms to retry tasks in case of transient failures, ensuring robustness and reliability in task execution. Delivery The execution layer of the messaging system model can be broken down into two main parts. The first is the delivery layer, which is the process of directing the event to the right place or service. The second, the event execution, we\u2019ll get to in a bit. Routing is the final layer in an asynchronous system, responsible for directing the message out of the system and into the domain where a designated process or lambda function will handle the event. This process or lambda function may be hosted within the same virtual private cloud (VPC) as the messaging infrastructure or may be a part of public clouds like AWS, Azure, etc. In a push-based model, the routing layer is one of the most critical components, similar to the \u201clast mile delivery\u201d in an e-commerce delivery system. Routing enables many critical functions, including: Message filtering based on subscriber preferences\n\nDelivery retries for transient failures\n\nContinuously monitoring the health of a subscriber\u2019s event execution hosts, and then routing events only to those that are healthy\n\nDispatching event execution status to the orchestration layer for state machine management\n\nEvent delivery concurrency management Execution The event execution is the second layer of the primary compute bucket. It\u2019s when the actual task happens, and it\u2019s usually done by a lambda function (i.e., serverless code), or a remote process\u2014potentially even another system or service\u2014that handles the event. In short, the compute layer involves first routing the event and then actually processing it. Lambda infrastructure refers to the underlying framework responsible for executing events. When an event is triggered, a process is initiated within this infrastructure, which subsequently returns either a success or retriable failure status post-execution. If no status is returned, or if an error occurs, the default assumption is a retriable failure. In this interaction, the router acts as the client, operating under a push model. Ideally, the executing process operates across multiple cloud environments to enhance reliability. The router has the capability to push events to various clouds based on the locality preference configured by the lambda/process owner. For example, some users may opt to configure their processes to be active in specific clouds to ensure proximity to backend storage dependencies, thereby minimizing cross-data center latency. Lambda infrastructure should also include autoscaling as part of its features. At Dropbox, our lambda infrastructure is backed by Atlas, which offers autoscaling capabilities. Additionally, Atlas supports release-time hooks, enabling validation and rollback of code changes if they would potentially degrade service uptime or impact any features negatively.\n\nConclusion", "label": 0}
{"title": "How engineers can use one-on-ones with their manager to accelerate career growth", "url": "https://github.blog/developer-skills/career-growth/how-engineers-can-use-one-on-ones-with-their-manager-to-accelerate-career-growth/", "content": "One-on-one meetings with your manager are one of the most valuable tools you have for career growth, problem-solving, and unlocking new opportunities. So if you\u2019re only using them to provide status updates, you\u2019re leaving a lot on the table.\n\nI didn\u2019t fully realize this potential until I mentioned in a one-on-one that I was interested in mentorship and growing my leadership skills. Not long after, I was asked to co-lead a project with an intern to build an internal tool that helped surface enterprise configuration details. This gave me the opportunity to take technical ownership on a project while mentoring someone in a real-world context\u2014both of which pushed me outside my comfort zone in the best way. That experience made it clear: When used intentionally, one-on-ones can open doors you didn\u2019t even know were there.\n\nMany engineers treat one-on-ones as a low-stakes standup: reporting work, mentioning blockers, and getting general feedback. While that can be useful, it barely scratches the surface of what these meetings can accomplish. Instead, think of them as a system design review for your role\u2014a time to debug challenges, optimize your workflow, and align on long-term career goals.\n\nReframing your perception of what a one-on-one can accomplish\n\nA well-structured one-on-one meeting with your manager isn\u2019t just a check-in, it\u2019s an opportunity to shape your work environment and career trajectory. You wouldn\u2019t build a system without evaluating its constraints, dependencies, and long-term maintainability. Why approach your career any differently?\n\nStart by shifting your mindset: These meetings are not status updates. Your manager already sees your pull requests, sprint velocity, and planning docs. Instead, use this time to highlight what matters\u2014what you\u2019ve shipped, the value it\u2019s delivered, and where the friction is.\n\nYou can also use this space to validate decisions and gather context. If you\u2019re weighing different paths forward, don\u2019t just ask for approval\u2014frame the conversation in terms of trade-offs:\n\n\u201cHere are the pros and cons of refactoring this service now versus later. How does this align with our broader business goals?\u201d\n\nTreat your manager like a decision-making API: Feed in the relevant signals, surface what\u2019s unclear, and work together on an informed response.\n\nUse one-on-ones for career versioning (even before you\u2019re \u201cready\u201d)\n\nOne-on-one meetings are a great time to discuss your long-term career growth\u2014even if you\u2019re not actively seeking a promotion. Instead of waiting until promotion season, start having these conversations early to build clarity, direction, and momentum over time.\n\nIf you\u2019re more than a year away from seeking a promotion, start talking to your manager about: Where am I already meeting expectations? Where should I focus on strengthening my skills?\n\nIf you\u2019re approaching the next level or considering going up for promotion soon, try focusing the conversation on: What kind of work would demonstrate readiness for the next level? Are there specific opportunities I can take on to grow my scope or visibility?\n\n\n\nBy treating growth as an iterative process rather than an all-or-nothing milestone, you can continuously improve and course-correct based on early feedback.\n\nA useful framework for structuring these discussions is the Three Circles of Impact:\n\nIndividual Contributions \u2013 The direct value of your work. Collaboration \u2013 How you work with and support others across the team. Enabling Others \u2013 Mentorship, knowledge sharing, or improving systems and tooling for your peers.\n\nIf you\u2019re not sure how to show impact across all three, your one-on-one is a great place to explore it. The key is surfacing your goals early so your manager can help guide you toward the kinds of work that will stretch your skills and broaden your influence.\n\nThe more you shape your contributions around these areas, the clearer your readiness for growth becomes\u2014and the easier it is for your manager to advocate on your behalf.\n\nYour manager can\u2019t debug what they don\u2019t see\n\nManagers don\u2019t have full visibility into your day-to-day experience, so one-on-ones are the right time to highlight persistent blockers and unclear expectations.\n\nFor instance, I once brought up a latency issue I was chasing down. The endpoint\u2019s performance was slightly above our service level objective (SLO) target, and I had already spent a good chunk of time optimizing it. But in that conversation, my manager offered a different lens:\n\n\u201cAre we optimizing for the right thing? We control the SLO. If the extra latency is due to how the system is designed (and if users aren\u2019t impacted) maybe the right move is to revisit the threshold instead of squeezing more performance out of it.\u201d\n\nThat single conversation saved me hours and helped me reframe the problem entirely. Sometimes, the fix isn\u2019t in your code\u2014it\u2019s in how you\u2019re measuring success.\n\nMake your one-on-ones work for you\n\nYour one-on-ones will become far more effective\u2014and lead to real growth\u2014when you treat them as time to think strategically, not just check in. Reframing these meetings around your goals, your environment, and your long-term development puts you in a much stronger position to advocate for yourself and your work.\n\nStart thinking about your career progression earlier than feels natural. Come prepared. Bring in what\u2019s going well, what\u2019s stuck, and where you want to grow. And remember: your manager can\u2019t fix what they don\u2019t know about, and they can\u2019t support your goals if you never share them.\n\nIf this shift feels unfamiliar, you\u2019re not alone. The Engineer\u2019s Survival Guide helped me reframe my thinking around one-on-ones.\n\nHere are a few ideas that stuck with me:\n\nYour manager isn\u2019t a mind reader.\n\nYou can\u2019t expect guidance if you don\u2019t come with a direction.\n\nYour growth is a shared effort, but it starts with you.\n\nThe earlier you see one-on-ones as a tool for impact and growth, the more value you\u2019ll get from them.\n\nTags:", "label": 0}
{"title": "Declining Picture Taken by TSA", "url": "https://lifeofpablo.com/blog/declining-picture-taken-by-tsa", "content": "Declining Picture Taken by TSA\n\nThis post was written in English (en_US).\n\nToday, I'm flying out of San Francisco (SFO) and I experienced something new today. For the first time, I was asked to have my picture taken by the Transportation Security Administration, or commonly known as the TSA. I'm not surprised that an airport such as San Francisco would have these implemented as ways of efficiently getting passengers as fast as possible.\n\nI politely told the TSA agent that I am declining facial recognition. The agent simply conducted a manual document check. It was a very easy experience and the TSA agent was very respectful.\n\nThe point here is that you're not required or obligated to have your picture taken for biometric verification in the United States. You can simply opt-out by requesting so at a point of entry, such as airport customs and of course with the TSA. Your identity document(s) is verified through a manual check.\n\nI wrote a post, Why I Opted-Out of Facial Recognition at Customs and Border Patrol on opting out of facial recognition by the United States Customs and Border Patrol and the experience I had.\n\nMany people don't know they can decline or aren't aware of the risks that facial recognition have in our society. People need to be more aware of their rights when it comes to biometrics and the data retention of such biometrics.", "label": 1}
{"title": "You have something to say, someone will listen", "url": "https://shellsharks.com/notes/2024/03/13/you-have-something-to-say-someone-will-listen", "content": "Mike Sass\n\n@shellsharks\n\n@stefan Oh man, the \u201cNothing to put there\u201d crowd hits home. That and the much-related (but not listed here) \u201cno one will care what I have to say / no one will read\u201d crowd. To anyone who matches these descriptions or has these feelings, know that you\u2026\n\nA. absolutely have something to say, and a blog is a great place to say it, and..\n\nB. The Internet is a huge place. People will find your site and read it, no matter how niche it is, or how poorly designed, or how seemingly poorly written. There\u2019s no thought you could have that couldn\u2019t educate someone else out there and nothing you could write about that there isn\u2019t some community out there that would have similar thoughts.\n\nAs for the other reasons folks don\u2019t have a site\u2026\n\nGithub Pages (and some other platforms) is 100% free (so if expense is an issue then it doesn\u2019t need to be) There are more hosting providers than ever these days, many of which make it dead simple to get started, so don\u2019t let complexity be a stopping point. You can always start real simple and get more complex as you go. Time may be the biggest hurdle. I would just say that a blog isn\u2019t something you need to maintain or publish regularly. Just put something up and post when you have time. Posts can be short, they can be sloppy, they can be whatever, Since you own the site, you can always edit something poorly written later too.\n\nFor everything else, you could check out my post about \u201cWhy I Blog. You Should Too!\u201d for other advice/inspiration. Cheers!\n\nhttps://shellsharks.com/you-should-blog", "label": 1}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2025-03", "content": "en\n\nSome Background\n\nFor a few weeks, I've been thinking about using another reverse-proxy service just to try something new.\n\nSince I launched my docker mono-repository that hosts my entire infrastructure, I've used nginx as my primary reverse-proxy. I was using Jason WIlder's nginx proxy to expose my applications to the web from the virtual machine.\n\nNow I have transitioned to using traefik. It was very easy to add labels specific to traefik to my services in my docker compose file.\n\nIn my docker compose setup, I used Let's Encrypt for SSL certificates in both nginx and traefik.\n\nJust like with any major change, I was worried about downtime but I took the risk and I tested out *lifeofpablo.com * and some other subdomains. Once I labeled the items with traefik specific items, and redeployed, it worked great! Once I looked around for anything out of place, I went back and added traefik labels to the rest of my containers.\n\nnginx\n\nWithin the docker file, nginx-proxy would use environmental variables to route outside of the machine to the domain or subdomain that the DNS will point to.\n\nenvironment: VIRTUAL_HOST: service.lifeofpablo.com LETSENCRYPT_HOST: service.lifeofpablo.com\n\ntraefik\n\nInstead of using environmental variables explicitly, traefik uses labels to route. In this example, I'm using my the service korea.lifeofpablo.com (korea) as the service being proxied.\n\nI simply matched the service name, korea as this is the name of the container in Docker and for the host, I used korea.lifeofpablo.com.\n\nlabels: - \"traefik.http.routers.korea.rule=Host(`korea.lifeofpablo.com`)\" - \"traefik.http.routers.korea.entrypoints=websecure\" - \"traefik.http.routers.korea.tls.certresolver=myresolver\"\n\nFuture\n\nWhat I have in mind for the future of my infrastructure with traefik is to control authentication. I want to use Zitadel to add authentication to services I have to simply experiment. This experimentation will be key as I continue planning the rebuild of my website. I want my future version of my website to be the center of my internet presence and be able to connect easily with my services.", "label": 1}
{"title": "Speaking in different languages", "url": "https://lifeofpablo.com/blog/speaking-in-different-languages", "content": "Speaking in different languages\n\nThis post was written in English (en_US).\n\nI wrote a post a few weeks ago about writing content in other languages and marking it up using h-entry tags. There is something so satisfying to write in different languages. Different parts of my brain activate and it works differently in each language. I also \"become a different\" person as each language influences how I present myself. I am still the same person but I tend to shift according to the target language.\n\nToday, I am going to say in a a video, \"Hello, I'm Pablo\" in three different languages, respectively: English, Spanish and French. I will also share some things about me in writing.\n\nI am really excited to make more visual content in the languages I know and the languages I am learning. I really need to look into localization as well.\n\nThank you Tracy Durnell for sharing with me Alex's blog post June 2023 in review. It is a great inspiration to use my language skills to the test.\n\nEnglish Your browser does not support the video tag. Favorite Song Nada - Zo\u00e9 Favorite Movie Spiderman Favorite Artist Low Favorite Food (other than Mexican food) Vietnamese Favorite Media Companyr New York Times Favorite Hobby Coding\n\nSpanish Your browser does not support the video tag. Canci\u00f3n Favorita Nada - Zo\u00e9 Telenovela Favorita Betty, La Fea Estado Favorito Oaxaca Plato Favorito Mole Sabor favorito Guayaba Pasatiempo Favorito Fotografiando la gente de M\u00e9xico", "label": 1}
{"title": "Google Play's billing system", "url": "https://developer.android.com/google/play/billing/", "content": "By Aug 31, 2025, all new apps and updates to existing apps must use Billing Library version 7 or newer. If you need more time to update your app, you can request an extension until Nov 1, 2025. Learn about Play Billing Library version deprecation\n\nGoogle Play's billing system is a service that enables you to sell digital products and content in your Android app, whether you want to monetize through one-time purchases or offer subscriptions to your services. Google Play offers a full set of APIs for integration with both your Android app and your server backend that unlock the familiarity and safety of Google Play purchases for your users.\n\nNote: Google Play's billing system is only for digital items. For physical goods and services, or other non-digital content, see the Google Pay SDK\n\nIntegration architecture\n\nThis section introduces the different functional modules that you can build and the APIs and libraries available to simplify the process.\n\nFigure 1. Diagram of a typical Google Play billing integration.\n\nYou can integrate Google Play's billing system with your Android app using the Play Billing Library. This library enables communication with the Google Play Services layer that provides the localized product offering available to each user in your app, as well as methods to handle other necessary user operations, like launching the purchase flow and handling its outcome.\n\nYou should also integrate Google Play's billing system with your server backend to create the necessary developer flows. This is essential to guarantee that your purchase management and cross-platform entitlements are efficient and secure. You can create this integration with the Subscriptions and in-app purchases API provided by the Google Play Developer API. The backend integration also leverages some Google Cloud platform tools.\n\nFigure 2. APIs and services provided by the Google Play Developer API.\n\nTerminology\n\nThis section lists and describes the high-level technologies and concepts that you might encounter when integrating Google Play's billing system into your app. Reference this list as you proceed through the integration guidance.\n\nTechnologies\n\nConcepts\n\nFlow . A flow shows the typical steps involved in a billing-related task. For example, a purchase flow outlines the steps involved when a user purchases your product. A subscription flow might show how a subscription transitions between states.\n\n. A flow shows the typical steps involved in a billing-related task. For example, a purchase flow outlines the steps involved when a user purchases your product. A subscription flow might show how a subscription transitions between states. Entitlement . When a user purchases an in-app product, they are then entitled to that product within your app. For one-time products, this means that the user should now have permanent access to the product. For subscriptions, this means that the user should have access while the subscription is active.\n\n. When a user purchases an in-app product, they are then entitled to that product within your app. For one-time products, this means that the user should now have permanent access to the product. For subscriptions, this means that the user should have access while the subscription is active. Product ID . The ID of a specific product type.\n\n. The ID of a specific product type. Purchase token . A string that represents a buyer's entitlement to a product on Google Play. It indicates that a Google user has paid for a specific product.\n\n. A string that represents a buyer's entitlement to a product on Google Play. It indicates that a Google user has paid for a specific product. Order ID. A string that represents a financial transaction on Google Play. An order ID is created every time a financial transaction occurs. This string is included in a receipt that is emailed to the buyer. You can use the order ID to manage refunds in the Order Management section of the Google Play Console. Order IDs are also used in sales and payout reports.\n\nNext steps\n\nTo begin integrating Google Play's billing system with your app and server backend, see the setup guide.", "label": 0}
{"title": "Things I Wish I Knew Before I Made My Website", "url": "https://shellsharks.com/blog-things-i-wish-i-had-known", "content": "Here\u2019s a list of things I wish I had known before I set out on my blogging / site-making / IndieWeb journey. (In no particular order)\n\n\n\nHad I known these, and carefully considered each, I would have saved myself A LOT of time fixing stuff, and even now, would have a lot less things to fix and add. For example, my CSS files are a mess, I have a lot of poorly managed inline .JS everywhere, accessiblity nightmares abound and much more\u2026 Learn from my mistakes!\n\nUnderstand and properly leverage Semantic HTML Elements. This approach will help your code be more readable, more modular and more descriptive.\n\nBe purposeful and methodical with your CSS \u201ccode\u201d. Try to define common sense CSS and make it reusable. To the best of your ability, try to avoid overusing inline CSS. It will make things harder to troubleshoot and more annoying to maintain over time. Take the time to understand dynamic HTMl stuff for different screen-sized devices, etc\u2026 You\u2019re going to want your site to look good on desktops and phones, simple as that.\n\nUse JavaScript sparingly, try to design your site to work well-enough for those who completely disable JavaScript. Look, my site has plenty of JS, and I know certain things would completely break if it were disabled (looking at you hamburger menu). That\u2019s really too bad for folks who want to use my site. I\u2019d like to fix this, but just haven\u2019t had time to figure it out. Also consider The JavaScript Trap. JS not only has incompatibility issues, but can also just slow down your site and introduce potential security vulns. Important things to consider!\n\nDon\u2019t box yourself in creatively\u2014 REALLY! Allow yourself to write about whatever you want. Use things like collections, different post types or tags to logically differentiate things you think are meant for different audiences if you must.\n\nVersion one of your site should have a theme toggle (i.e. dark/light mode) and a search function. You\u2019re going to want these eventually, and it\u2019s worth getting them right in the initial design if you ask me.\n\nBuild accessibility in from the get-go. I\u2019ve put very little effort into this, and that sucks. One of my Guiding Principles for this site is that it is available to be consumed by all. Yet, if I\u2019ve not made it adequately accessible, it will never meet this mantra. It\u2019s not necessarily hard to do, but if you don\u2019t consider it from t=0, it becomes harder and more time-consuming to retroactively make it so.\n\nWrite for yourself, not for some perceived \u201caudience\u201d. Don\u2019t try to be a persona (i.e. some \u201cprofessional\u201d fragment of your true self)\u2014just be yourself.\n\nDo some basic website wireframing as part of your initial size build. Carefully consider what you want your home page to look like, how you want people to navigate about, what you want your posts to look like, etc\u2026\n\nHow \u201csocial\u201d do you want your site to be? In the age of the social web, there is a lot you can add or implement to make your site interoperate with ActivityPub, IndieWeb protocols, comment systems, etc\u2026", "label": 1}
{"title": "IssueOps: Automate CI/CD (and more!) with GitHub Issues and Actions", "url": "https://github.blog/engineering/issueops-automate-ci-cd-and-more-with-github-issues-and-actions/", "content": "Software development is filled with repetitive tasks\u2014managing issues, handling approvals, triggering CI/CD workflows, and more. But what if you could automate these types of tasks directly within GitHub Issues? That\u2019s the promise of IssueOps, a methodology that turns GitHub Issues into a command center for automation.\n\nWhether you\u2019re a solo developer or part of an engineering team, IssueOps helps you streamline operations without ever leaving your repository.\n\nIn this article, I\u2019ll explore the concept of IssueOps using state-machine terminology and strategies to help you work more efficiently on GitHub. After all, who doesn\u2019t love automation?\n\nWhat is IssueOps?\n\nIssueOps is the practice of using GitHub Issues, GitHub Actions, and pull requests (PR) as an interface for automating workflows. Instead of switching between tools or manually triggering actions, you can use issue comments, labels, and state changes to kick off CI/CD pipelines, assign tasks, and even deploy applications.\n\nMuch like the various other *Ops paradigms (ChatOps, ClickOps, and so on), IssueOps is a collection of tools, workflows, and concepts that, when applied to GitHub Issues, can automate mundane, repetitive tasks. The flexibility and power of issues, along with their relationship to pull requests, create a near limitless number of possibilities, such as managing approvals and deployments. All of this can really help to simplify your workflows on GitHub. I\u2019m speaking from personal experience here.\n\nIt\u2019s important to note that IssueOps isn\u2019t just a DevOps thing! Where DevOps offers a methodology to bring developers and operations into closer alignment, IssueOps is a workflow automation practice centered around GitHub Issues. IssueOps lets you run anything from complex CI/CD pipelines to a bed and breakfast reservation system. If you can interact with it via an API, there\u2019s a good chance you can build it with IssueOps!\n\nSo, why use IssueOps?\n\nThere are lots of benefits to utilizing IssueOps. Here\u2019s how it\u2019s useful in practice:\n\nIt\u2019s event driven, so you can automate the boring stuff: IssueOps lets you automate workflows directly from GitHub Issues and pull requests, turning everyday interactions\u2014from kicking off a CI/CD pipeline and managing approvals to updating project boards\u2014into powerful triggers for GitHub Actions.\n\nIssueOps lets you automate workflows directly from GitHub Issues and pull requests, turning everyday interactions\u2014from kicking off a CI/CD pipeline and managing approvals to updating project boards\u2014into powerful triggers for GitHub Actions. It\u2019s customizable, so you can tailor workflows to your needs: No two teams work the same way, and IssueOps is flexible enough to adapt. Whether you\u2019re automating bug triage or triggering deployments, you can customize workflows based on event type and data provided.\n\nIt\u2019s transparent, so you can keep a record: All actions taken on an issue are logged in its timeline, creating an easy-to-follow record of what happened and when.\n\nIt\u2019s immutable, so you can audit whenever you need: Because IssueOps uses GitHub Issues and pull requests as a source of truth, every action leaves a record. No more chasing approvals in Slack or manually triggering workflows: IssueOps keeps everything structured, automated, and auditable right inside GitHub.\n\nOur quickstart guide to IssueOps Step 1: Define your triggers\n\nIdentify the actions that should kick off your workflows\u2014like opening an issue, adding a label, or merging a pull request. These events can serve as triggers for GitHub Actions.\n\nIdentify the actions that should kick off your workflows\u2014like opening an issue, adding a label, or merging a pull request. These events can serve as triggers for GitHub Actions. Step 2: Configure GitHub Actions Use GitHub Actions to define what happens when an event occurs. For example, if an issue is labeled deploy, you could trigger a deployment script. YAML never looked so good.\n\nStep 3: Test and iterate Like any good automation, IssueOps workflows should be tested and refined. Start small, see what works, and expand from there. Let\u2019s go: Learn more in our repository.\n\nDefining IssueOps workflows and how they\u2019re like finite-state machines\n\nMost IssueOps workflows follow the same basic pattern:\n\nA user opens an issue and provides information about a request The issue is validated to ensure it contains the required information The issue is submitted for processing Approval is requested from an authorized user or team The request is processed and the issue is closed\n\nSuppose you\u2019re an administrator of an organization and want to reduce the overhead of managing team members. In this instance, you could use IssueOps to build an automated membership request and approval process. Within a workflow like this, you\u2019d have several core steps:\n\nA user creates a request to be added to a team The request is validated The request is submitted for approval An administrator approves or denies this request The request is processed If approved, the user is added to the team If denied, the user is not added to the team The user is notified of the outcome\n\nWhen designing your own IssueOps workflows, it can be very helpful to think of them as a finite-state machine: a model for how objects move through a series of states in response to external events. Depending on certain rules defined within the state machine, a number of different actions can take place in response to state changes. If this is a little too complex, you can also think of it like a flow chart.\n\nTo apply this comparison to IssueOps, an issue is the object that is processed by a state machine. It changes state in response to events. As the object changes state, certain actions may be performed as part of a transition, provided any required conditions (guards) are met. Once an end state is reached, the issue can be closed.\n\nThis breaks down into a few key concepts:\n\nState : A point in an object\u2019s lifecycle that satisfies certain condition(s).\n\n: A point in an object\u2019s lifecycle that satisfies certain condition(s). Event : An external occurrence that triggers a state change.\n\n: An external occurrence that triggers a state change. Transition : A link between two states that, when traversed by an object, will cause certain action(s) to be performed.\n\n: A link between two states that, when traversed by an object, will cause certain action(s) to be performed. Action : An atomic task that is performed when a transition is taken.\n\n: An atomic task that is performed when a transition is taken. Guard: A condition that is evaluated when a trigger event occurs. A transition is taken only if all associated guard condition(s) are met.\n\nHere\u2019s a simple state diagram for the example I discussed above.\n\nNow, let\u2019s dive into the state machine in more detail!\n\nKey concepts behind state machines\n\nThe benefit of breaking your workflow down into these components is that you can look for edge cases, enforce conditions, and create a robust, reliable result.\n\nStates\n\nWithin a state machine, a state defines the current status of an object. As the object transitions through the state machine, it will change states in response to external events. When building IssueOps workflows, common states for issues include opened, submitted, approved, denied, and closed.\n\nThese should suffice as the core states to consider when building our workflows in our team membership example above.\n\nEvents\n\nIn a state machine, an event can be any form of interaction with the object and its current state. When building your own IssueOps, you should consider events from both the user and GitHub points of view.\n\nIn our team membership request example, there are several events that can trigger a change in state. The request can be created, submitted, approved, denied, or processed.\n\nIn this example, a user interacting with an issue\u2014such as adding labels, commenting, or updating milestones\u2014can also change its state. In GitHub Actions, there are many events that can trigger your workflows (see events that trigger workflows).\n\nHere are a few interactions, or events, that would affect our example IssueOps workflow when it comes to managing team members:\n\nRequest Event State Request is created issues opened Request is approved issue_comment created Request is denied issue_comment created\n\nAs you can see, the same GitHub workflow trigger can apply to multiple events in our state machine. Because of this, validation is key. Within your workflows, you should check both the type of event and the information provided by the user. In this case, we can conditionally trigger different workflow steps based on the content of the issue_comment event.\n\njobs: approve: name: Process Approval runs-on: ubuntu-latest if: ${{ startsWith(github.event.comment.body, '.approve') }} # ... deny: name: Process Denial runs-on: ubuntu-latest if: ${{ startsWith(github.event.comment.body, '.deny') }} # ...\n\nTransitions\n\nA transition is simply the change from one state to another. In our example, for instance, a transition occurs when someone opens an issue. When a request meets certain conditions, or guards, the change in state can take place. When the transition occurs, some actions or processing may take place, as well.\n\nWith our example workflow, you can think of the transitions themselves as the lines connecting different nodes in the state diagram. Or the lines connecting boxes in a flow chart.\n\nGuards\n\nGuards are conditions that must be verified before an event can trigger a transition to a different state. In our case, we know the following guards must be in place:\n\nA request should not transition to an Approved state unless an administrator comments .approve on the issue.\n\non the issue. A request should not transition to a Denied state unless an administrator comments .deny on the issue.\n\nWhat about after the request is approved and the user is added to the team? This is referred to as an unguarded transition. There are no conditions that must be met, so the transition happens immediately!\n\nActions\n\nLastly, actions are specific tasks that are performed during a transition. They may affect the object itself, but this is not a requirement in our state machine. In our example, the following actions may take place at different times:\n\nAdministrators are notified that a request has been submitted\n\nThe user is added to the requested team\n\nThe user is notified of the outcome\n\nA real-world example: Building a team membership workflow with IssueOps\n\nNow that all of the explanation is out of the way, let\u2019s dive into building our example! For reference, we\u2019ll focus on the GitHub Actions workflows involved in building this automation. There are some additional repository and permissions settings involved that are discussed in more detail in these IssueOps docs.\n\nStep 1: Issue form template\n\nGitHub issue forms let you create standardized, formatted issues based on a set of form fields. Combined with the issue-ops/parser action, you can get reliable, machine-readable JSON from issue body Markdown. For our example, we are going to create a simple form that accepts a single input: the team where we want to add the user.\n\nname: Team Membership Request description: Submit a new membership request title: New Team Membership Request labels: - team-membership body: - type: input id: team attributes: label: Team Name description: The team name you would like to join placeholder: my-team validations: required: true\n\nWhen issues are created using this form, they will be parsed into JSON, which can then be passed to the rest of the IssueOps workflow.\n\n{ \"team\": \"my-team\" }\n\nStep 2: Issue validation\n\nWith a machine-readable issue body, we can run additional validation checks to ensure the information provided follows any rules we might have in place. For example, we can\u2019t automatically add a user to a team if the team doesn\u2019t exist yet! That is where the issue-ops/validator action comes into play. Using an issue form template and a custom validation script, we can confirm the existence of the team ahead of time.\n\nmodule.exports = async (field) => { const { Octokit } = require('@octokit/rest') const core = require('@actions/core') const github = new Octokit({ auth: core.getInput('github-token', { required: true }) }) try { // Check if the team exists core.info(`Checking if team '${field}' exists`) await github.rest.teams.getByName({ org: process.env.GITHUB_REPOSITORY_OWNER ?? '', team_slug: field }) core.info(`Team '${field}' exists`) return 'success' } catch (error) { if (error.status === 404) { // If the team does not exist, return an error message core.error(`Team '${field}' does not exist`) return `Team '${field}' does not exist` } else { // Otherwise, something else went wrong... throw error } } }\n\nWhen included in our IssueOps workflow, this adds any validation error(s) to the comment on the issue.\n\nStep 3: Issue workflows\n\nThe main \u201centrypoint\u201d of this workflow occurs when a user creates or edits their team membership request issue. This workflow should focus heavily on validating any user inputs! For example, what should happen if the user inputs a team that does not exist?\n\nIn our state machine, this workflow is responsible for handling everything up to the opened state. Any time an issue is created, edited, or updated, it will re-run validation to ensure the request is ready to be processed. In this case, an additional guard condition is introduced. Before the request can be submitted, the user must comment with .submit after validation has passed.\n\nname: Process Issue Open/Edit on: issues: types: - opened - edited - reopened permissions: contents: read id-token: write issues: write jobs: validate: name: Validate Request runs-on: ubuntu-latest # This job should only be run on issues with the `team-membership` label. if: ${{ contains(github.event.issue.labels.*.name, 'team-membership') }} steps: # This is required to ensure the issue form template and any validation # scripts are included in the workspace. - name: Checkout id: checkout uses: actions/checkout@v4 # Since this workflow includes custom validation scripts, we need to # install Node.js and any dependencies. - name: Setup Node.js id: setup-node uses: actions/setup-node@v4 # Install dependencies from `package.json`. - name: Install Dependencies id: install run: npm install # GitHub App authentication is required if you want to interact with any # resources outside the scope of the repository this workflow runs in. - name: Get GitHub App Token id: token uses: actions/create-github-app-token@v1 with: app-id: ${{ vars.ISSUEOPS_APP_ID }} private-key: ${{ secrets.ISSUEOPS_APP_PRIVATE_KEY }} owner: ${{ github.repository_owner }} # Remove any labels and start fresh. This is important because the # issue may have been closed and reopened. - name: Remove Labels id: remove-label uses: issue-ops/labeler@v2 with: action: remove github_token: ${{ steps.token.outputs.token }} labels: | validated approved denied issue_number: ${{ github.event.issue.number }} repository: ${{ github.repository }} # Parse the issue body into machine-readable JSON, so that it can be # processed by the rest of the workflow. - name: Parse Issue Body id: parse uses: issue-ops/parser@v4 with: body: ${{ github.event.issue.body }} issue-form-template: team-membership.yml workspace: ${{ github.workspace }} # Validate early and often! Validation should be run any time an issue is # interacted with, to ensure that any changes to the issue body are valid. - name: Validate Request id: validate uses: issue-ops/validator@v3 with: add-comment: true github-token: ${{ steps.token.outputs.token }} issue-form-template: team-membership.yml issue-number: ${{ github.event.issue.number }} parsed-issue-body: ${{ steps.parse.outputs.json }} workspace: ${{ github.workspace }} # If validation passes, add the validated label to the issue. - if: ${{ steps.validate.outputs.result == 'success' }} name: Add Validated Label id: add-label uses: issue-ops/labeler@v2 with: action: add github_token: ${{ steps.token.outputs.token }} labels: | validated issue_number: ${{ github.event.issue.number }} repository: ${{ github.repository }} # The `issue-ops/validator` action will automatically notify the user that # the request was validated. However, you can optionally add instruction # on what to do next. - if: ${{ steps.validate.outputs.result == 'success' }} name: Notify User (Success) id: notify-success uses: peter-evans/create-or-update-comment@v4 with: issue-number: ${{ github.event.issue.number }} body: | Hello! Your request has been validated successfully! Please comment with `.submit` to submit this request.\n\nOnce the issue is created, any further processing is triggered using issue comments\u2014and this can be done with one workflow. However, to make things a bit easier to follow, we\u2019ll break this into a few separate workflows.\n\nSubmit workflow\n\nThe first workflow handles the user submitting the request. The main task it performs is validating the issue body against the form template to ensure it hasn\u2019t been modified.\n\nname: Process Submit Comment on: issue_comment: types: - created permissions: contents: read id-token: write issues: write jobs: submit: name: Submit Request runs-on: ubuntu-latest # This job should only be run when the following conditions are true: # # - A user comments `.submit` on the issue. # - The issue has the `team-membership` label. # - The issue has the `validated` label. # - The issue does not have the `approved` or `denied` labels. # - The issue is open. if: | startsWith(github.event.comment.body, '.submit') && contains(github.event.issue.labels.*.name, 'team-membership') == true && contains(github.event.issue.labels.*.name, 'approved') == false && contains(github.event.issue.labels.*.name, 'denied') == false && github.event.issue.state == 'open' steps: # First, we are going to re-run validation. This is important because # the issue body may have changed since the last time it was validated. # This is required to ensure the issue form template and any validation # scripts are included in the workspace. - name: Checkout id: checkout uses: actions/checkout@v4 # Since this workflow includes custom validation scripts, we need to # install Node.js and any dependencies. - name: Setup Node.js id: setup-node uses: actions/setup-node@v4 # Install dependencies from `package.json`. - name: Install Dependencies id: install run: npm install # GitHub App authentication is required if you want to interact with any # resources outside the scope of the repository this workflow runs in. - name: Get GitHub App Token id: token uses: actions/create-github-app-token@v1 with: app-id: ${{ vars.ISSUEOPS_APP_ID }} private-key: ${{ secrets.ISSUEOPS_APP_PRIVATE_KEY }} owner: ${{ github.repository_owner }} # Remove the validated label. This will be re-added if validation passes. - name: Remove Validated Label id: remove-label uses: issue-ops/labeler@v2 with: action: remove github_token: ${{ steps.token.outputs.token }} labels: | validated issue_number: ${{ github.event.issue.number }} repository: ${{ github.repository }} # Parse the issue body into machine-readable JSON, so that it can be # processed by the rest of the workflow. - name: Parse Issue Body id: parse uses: issue-ops/parser@v4 with: body: ${{ github.event.issue.body }} issue-form-template: team-membership.yml workspace: ${{ github.workspace }} # Validate early and often! Validation should be run any time an issue is # interacted with, to ensure that any changes to the issue body are valid. - name: Validate Request id: validate uses: issue-ops/validator@v3 with: add-comment: false # Don't add another validation comment. github-token: ${{ steps.token.outputs.token }} issue-form-template: team-membership.yml issue-number: ${{ github.event.issue.number }} parsed-issue-body: ${{ steps.parse.outputs.json }} workspace: ${{ github.workspace }} # If validation passed, add the validated and submitted labels to the issue. - if: ${{ steps.validate.outputs.result == 'success' }} name: Add Validated Label id: add-label uses: issue-ops/labeler@v2 with: action: add github_token: ${{ steps.token.outputs.token }} labels: | validated submitted issue_number: ${{ github.event.issue.number }} repository: ${{ github.repository }} # If validation succeeded, alert the administrator team so they can # approve or deny the request. - if: ${{ steps.validate.outputs.result == 'success' }} name: Notify Admin (Success) id: notify-success uses: peter-evans/create-or-update-comment@v4 with: issue-number: ${{ github.event.issue.number }} body: | \ud83d\udc4b @issue-ops/admins! The request has been validated and is ready for your review. Please comment with `.approve` or `.deny` to approve or deny this request.\n\nDeny workflow\n\nIf the request is denied, the user should be notified and the issue should close.\n\nname: Process Denial Comment on: issue_comment: types: - created permissions: contents: read id-token: write issues: write jobs: submit: name: Deny Request runs-on: ubuntu-latest # This job should only be run when the following conditions are true: # # - A user comments `.deny` on the issue. # - The issue has the `team-membership` label. # - The issue has the `validated` label. # - The issue has the `submitted` label. # - The issue does not have the `approved` or `denied` labels. # - The issue is open. if: | startsWith(github.event.comment.body, '.deny') && contains(github.event.issue.labels.*.name, 'team-membership') == true && contains(github.event.issue.labels.*.name, 'submitted') == true && contains(github.event.issue.labels.*.name, 'validated') == true && contains(github.event.issue.labels.*.name, 'approved') == false && contains(github.event.issue.labels.*.name, 'denied') == false && github.event.issue.state == 'open' steps: # This time, we do not need to re-run validation because the request is # being denied. It can just be closed. # However, we do need to confirm that the user who commented `.deny` is # a member of the administrator team. # GitHub App authentication is required if you want to interact with any # resources outside the scope of the repository this workflow runs in. - name: Get GitHub App Token id: token uses: actions/create-github-app-token@v1 with: app-id: ${{ vars.ISSUEOPS_APP_ID }} private-key: ${{ secrets.ISSUEOPS_APP_PRIVATE_KEY }} owner: ${{ github.repository_owner }} # Check if the user who commented `.deny` is a member of the # administrator team. - name: Check Admin Membership id: check-admin uses: actions/github-script@v7 with: github-token: ${{ steps.token.outputs.token }} script: | try { await github.rest.teams.getMembershipForUserInOrg({ org: context.repo.owner, team_slug: 'admins', username: context.actor, }) core.setOutput('member', 'true') } catch (error) { if (error.status === 404) { core.setOutput('member', 'false') } throw error } # If the user is not a member of the administrator team, exit the # workflow. - if: ${{ steps.check-admin.outputs.member == 'false' }} name: Exit run: exit 0 # If the user is a member of the administrator team, add the denied label. - name: Add Denied Label id: add-label uses: issue-ops/labeler@v2 with: action: add github_token: ${{ steps.token.outputs.token }} labels: | denied issue_number: ${{ github.event.issue.number }} repository: ${{ github.repository }} # Notify the user that the request was denied. - name: Notify User id: notify uses: peter-evans/create-or-update-comment@v4 with: issue-number: ${{ github.event.issue.number }} body: | This request has been denied and will be closed. # Close the issue as not planned. - name: Close Issue id: close uses: actions/github-script@v7 with: script: | await github.rest.issues.update({ issue_number: ${{ github.event.issue.number }}, owner: context.repo.owner, repo: context.repo.repo, state: 'closed', state_reason: 'not_planned' })\n\nApprove workflow\n\nFinally, we need to handle request approval. In this case, we need to add the user to the team, notify them, and close the issue.\n\nname: Process Approval Comment on: issue_comment: types: - created permissions: contents: read id-token: write issues: write jobs: submit: name: Approve Request runs-on: ubuntu-latest # This job should only be run when the following conditions are true: # # - A user comments `.approve` on the issue. # - The issue has the `team-membership` label. # - The issue has the `validated` label. # - The issue has the `submitted` label. # - The issue does not have the `approved` or `denied` labels. # - The issue is open. if: | startsWith(github.event.comment.body, '.approve') && contains(github.event.issue.labels.*.name, 'team-membership') == true && contains(github.event.issue.labels.*.name, 'submitted') == true && contains(github.event.issue.labels.*.name, 'validated') == true && contains(github.event.issue.labels.*.name, 'approved') == false && contains(github.event.issue.labels.*.name, 'denied') == false && github.event.issue.state == 'open' steps: # This time, we do not need to re-run validation because the request is # being approved. It can just be processed. # This is required to ensure the issue form template is included in the # workspace. - name: Checkout id: checkout uses: actions/checkout@v4 # We do need to confirm that the user who commented `.approve` is a member # of the administrator team. GitHub App authentication is required if you # want to interact with any resources outside the scope of the repository # this workflow runs in. - name: Get GitHub App Token id: token uses: actions/create-github-app-token@v1 with: app-id: ${{ vars.ISSUEOPS_APP_ID }} private-key: ${{ secrets.ISSUEOPS_APP_PRIVATE_KEY }} owner: ${{ github.repository_owner }} # Check if the user who commented `.approve` is a member of the # administrator team. - name: Check Admin Membership id: check-admin uses: actions/github-script@v7 with: github-token: ${{ steps.token.outputs.token }} script: | try { await github.rest.teams.getMembershipForUserInOrg({ org: context.repo.owner, team_slug: 'admins', username: context.actor, }) core.setOutput('member', 'true') } catch (error) { if (error.status === 404) { core.setOutput('member', 'false') } throw error } # If the user is not a member of the administrator team, exit the # workflow. - if: ${{ steps.check-admin.outputs.member == 'false' }} name: Exit run: exit 0 # Parse the issue body into machine-readable JSON, so that it can be # processed by the rest of the workflow. - name: Parse Issue body id: parse uses: issue-ops/parser@v4 with: body: ${{ github.event.issue.body }} issue-form-template: team-membership.yml workspace: ${{ github.workspace }} - name: Add to Team id: add uses: actions/github-script@v7 with: github-token: ${{ steps.token.outputs.token }} script: | const parsedIssue = JSON.parse('${{ steps.parse.outputs.json }}') await github.rest.teams.addOrUpdateMembershipForUserInOrg({ org: context.repo.owner, team_slug: parsedIssue.team, username: '${{ github.event.issue.user.login }}', role: 'member' }) - name: Notify User id: notify uses: peter-evans/create-or-update-comment@v4 with: issue-number: ${{ github.event.issue.number }} body: | This request has been processed successfully! - name: Close Issue id: close uses: actions/github-script@v7 with: script: | await github.rest.issues.update({ issue_number: ${{ github.event.issue.number }}, owner: context.repo.owner, repo: context.repo.repo, state: 'closed', state_reason: 'completed' })\n\nTake this with you\n\nAnd there you have it! With a handful of standardized workflows, you have an end-to-end, issue-driven process in place to manage team membership. This can be extended as far as you want, including support for removing users, auditing access, and more. With IssueOps, the sky is the limit!\n\nHere\u2019s the best thing about IssueOps: It brings another level of automation to a surface I\u2019m constantly using\u2014and that\u2019s GitHub. By using issues and pull requests as control centers for workflows, teams can reduce friction, improve efficiency, and keep everything transparent. Whether you want to automate deployments, approvals, or bug triage, IssueOps makes it all possible, without ever leaving your repo.\n\nFor more information and examples, check out the open source IssueOps documentation repository, and if you want a deeper dive, you can head over to the open source IssueOps documentation.\n\nIn my experience, it\u2019s always best to start small and experiment with what works best for you. With just a bit of time, you\u2019ll see your workflows get smoother with every commit (I know I have). Happy coding! \u2728\n\nTags:", "label": 0}
{"title": "Finding leaked passwords with AI: How we built Copilot secret scanning", "url": "https://github.blog/engineering/platform-security/finding-leaked-passwords-with-ai-how-we-built-copilot-secret-scanning/", "content": "In October 2024, we announced the general availability of Copilot secret scanning, leveraging AI to detect generic passwords in users\u2019 codebases. This post describes how Copilot secret scanning works under the hood, the challenges we ran into when developing it, and the framework we use for testing and iteration.\n\nWhat is Copilot secret scanning?\n\nCopilot secret scanning is a feature of GitHub Secret Protection, which protects millions of repositories on GitHub by detecting hundreds of pattern types through our partner program. The precision of these detections is paramount for security teams and developers when dealing with security alerts. Historically, our detection approach has relied on regular expressions, which is an effective method for identifying secrets with strict, provider-minted formats. However, this method struggles with the nuanced and varied structures of generic passwords, often generating excessive noise for security teams and developers.\n\nWe now detect generic passwords with GitHub Copilot, using AI to analyze context\u2014such as the usage and location of a potential secret\u2014to limit noise and deliver relevant alerts that are critical to the health and security of your repositories.\n\nGetting to the point where we were confident in our password precision was a journey over many test cases, prompt iterations, and model changes. Let\u2019s dive in to explore what we learned along the way and find out where we\u2019re going.\n\nThe private preview highlighted a problem early on: unconventional file types and structures\n\nAt the core of Copilot secret scanning lies a request to a large language model (LLM), expressed through an LLM prompt consisting of:\n\nGeneral information about the type of vulnerability, in this case passwords.\n\nThe source code location and contents of the file where we believe the vulnerability may exist.\n\nA strict JSON format specification for the model output, to allow for automated processing.\n\nOur first iteration of the prompt used the few-shot prompting technique, which provides the LLM with example inputs and outputs to demonstrate how to perform the task. We wanted a resource-effective model to run the detections at scale and landed on GPT-3.5-Turbo. In parallel, we developed a basic offline evaluation framework, including manually curated test cases with both positive and negative findings, to help us validate that our approach was sound before deploying it to customers.\n\nWe deployed this first iteration to our private preview participants and immediately noticed a problem. While it worked reasonably well at identifying credentials in our offline evaluation, it would fail spectacularly in some customer repositories. The model had difficulty interpreting file types and structures not typically seen in the conventional coding languages and patterns that LLMs train on.\n\nThis experience revealed the complexity of the problem and the limiting nature of LLMs. We had to reevaluate our approach.\n\nThe road to public preview: Improving offline evaluation and prompting\n\nIn response to these initial results, we enhanced the offline evaluation framework in a few key ways. First, we added reports from private preview participants to increase the diversity of our test cases. Next, we enhanced the framework so that we could visually identify and analyze deviations resulting from model or prompt changes. This allowed us to better see the impact of customizing different steps in our prompting strategy. Finally, we leveraged the GitHub Code Security team\u2019s evaluation processes to create a data collection pipeline, and used GPT-4 to create our own test cases based on learnings from existing secret scanning alerts in open source repositories.\n\nThis improved offline evaluation and gave us the breadth needed to measure both precision and recall. Precision is the ability to find secrets more accurately, with concerns to the false positive rate, while recall is the ability to find secrets more reliably, with concerns to the false negative rate.\n\nFrom here, we ran a series of experiments to evaluate detection quality:\n\nWhat if we tried a different model?\n\nWhat if we ran the prompt multiple times and somehow combined the responses?\n\nWhat if we ran two different prompts on two different models in sequence?\n\nHow do we better handle the nondeterministic nature of LLM responses?\n\nMore specifically, we started experimenting with a few different mechanisms to improve our detection with the LLM.\n\nWe tried voting (asking the model the same question many times), which allowed for more deterministic responses but had no material impact on our precision.\n\nWe also tried using a larger model (GPT-4) trained on a larger set of parameters as a confirming scanner, to validate the accuracy of candidates found by GPT-3.5-Turbo. This helped improve precision without reducing our recall, but was also more resource intensive.\n\nWe also tried a few different prompting strategies, such as Fill-in-the-Middle, Zero-Shot, and Chain-of-Thought. We ended up collaborating with our colleagues at Microsoft and used their MetaReflection technique, a novel offline reinforcement learning technique that allows experiential learnings from past trials to come up with a hybrid Chain of Thought (CoT) and few-shot prompt that improves precision with a small penalty in recall.\n\nWe ultimately ended up using a combination of all these techniques and moved Copilot secret scanning into public preview, opening it widely to all GitHub Secret Protection customers. This brings us to our next hurdle: scale.\n\nScaling out capacity for a public preview\n\nSecret scanning not only scans incoming Git pushes, but also your entire Git history on all branches. With each new customer, the necessary resources increase linearly. Rather than simply expanding LLM capacity, we focused on striking the most effective balance between value and cost to ensure optimal performance and efficiency. Before tackling how we managed the resources, we tried to find ways to reduce resource usage itself by:\n\nIdentifying and excluding a class of changes from scanning (such as media files or language files that contain \u201ctest,\u201d \u201cmock,\u201d or \u201cspec\u201d in the filepath), because we expected they would never contain credentials or they would be incomprehensible to the model.\n\nExperimenting with newer models, such as GPT-4-Turbo and GPT-4o-mini, that were expected to be less resource intensive without compromising on performance and latency.\n\nExperimenting with different context windows to find one that reduced resources without significantly increasing latency for the LLM to respond to our queries.\n\nMaking improvements to how we tokenize the content we want to scan, including retaining some memory of previous tokenizations while processing new parts of a file.\n\nWhile some of these efforts proved fruitful, such as limiting the content we scanned, other efforts were less effective. For example, breaking down content into smaller pieces didn\u2019t have much of an impact, while using a more powerful model did.\n\nUltimately, the most impactful change came from creating a workload-aware request management system that allowed us to maximize and equitably share LLM capacity against the variety of different workloads we run during scans.\n\nIn building the system, we noticed a fundamental problem that needed addressing in our capacity management: assigning specific rate limits to individual workloads (such as scanning incoming Git commits or scanning the full history) was suboptimal. As each workload was tied to specific traffic patterns\u2014Git commits, for example, tend to correlate with working hours, while full history scanning correlates with discrete events like a security manager or administrator enabling the feature on a new organization\u2014it was easy to land in a situation where an individual workload could run into rate limits within its operational context, leaving additional resources available elsewhere unused.\n\nWe drew significant inspiration from existing solutions in this space, such as Doorman, GitHub\u2019s own Freno, and various other weighted, fair-priority, queue-related algorithms. We came up with an algorithm that allows us to set a range of limits for each workload, preventing the workload from completely overwhelming the LLM, while allowing it to tap into resources from other workloads going unused at the moment. This strategy was so effective at maximizing utilization that we ended up using it within Copilot Autofix and security campaigns as well.\n\nMirror testing our way to general availability\n\nAchieving confidence in detection quality was crucial for moving Copilot secret scanning to general availability. We implemented a mirror testing framework that ran our prompt and filtering changes against a subset of repositories that participated in our public preview. Rescanning these repositories with our latest improvements allowed us to assess the change in real alert volumes and false positive resolutions, without impacting users.\n\nWe found a huge drop in detections and false positives with very few missing real passwords. In some cases, we saw a 94% reduction in false positives across organizations! This before-and-after comparison indicated that all the different changes we made during private and public preview led to increased precision without sacrificing recall, and that we were ready to provide a reliable and efficient detection mechanism to all GitHub Secret Protection customers.\n\nLessons for the future\n\nCopilot secret scanning is now detecting passwords on nearly 35% of all GitHub Secret Protection repositories. We\u2019re continuing to monitor performance and apply lessons learned as we leverage the tooling we created along the way:\n\nA focus on precision: Security and development teams need accurate and actionable alerts without the noise\u2014this is always our primary goal.\n\nSecurity and development teams need accurate and actionable alerts without the noise\u2014this is always our primary goal. Including diverse test cases: We continue to incorporate examples based on learnings from customer feedback into our test bed as we refine our detection capabilities.\n\nWe continue to incorporate examples based on learnings from customer feedback into our test bed as we refine our detection capabilities. Effective resource management: We always need to balance scalability with performance.\n\nWe always need to balance scalability with performance. Collaborative innovation: Partnering with other GitHub and Microsoft teams helps us push the boundaries of what Copilot can achieve.\n\nThese learnings are also shared across Copilot Autofix, which continues to expand coverage for code scanning alerts and helps development teams remediate code scanning alerts quickly.\n\nSince our general availability launch, enablement for Copilot secret scanning has been included in security configurations, allowing you to control which repositories are detecting secrets across your organizations or enterprise. We\u2019re dedicated to continuous improvement through ongoing monitoring, mirror testing, and approach refinement based on customer feedback and detection trends. Copilot secret scanning serves as a critical component for robust application security and will evolve to meet the dynamic needs of our users.\n\nCopilot secret scanning is a feature of GitHub Secret Protection, which offers enterprise-ready solutions for preventing accidental secret exposure in your repositories. GitHub Secret Protection is available to purchase starting April 1, 2025.", "label": 0}
{"title": "Cloud Efficiency at Netflix", "url": "https://netflixtechblog.com/cloud-efficiency-at-netflix-f2a142955f83?source=collection_home---4------9-----------------------", "content": "Cloud Efficiency at Netflix Netflix Technology Blog 5 min read \u00b7 Dec 17, 2024 -- 10 Listen Share\n\nBy J Han, Pallavi Phadnis\n\nContext\n\nAt Netflix, we use Amazon Web Services (AWS) for our cloud infrastructure needs, such as compute, storage, and networking to build and run the streaming platform that we love. Our ecosystem enables engineering teams to run applications and services at scale, utilizing a mix of open-source and proprietary solutions. In turn, our self-serve platforms allow teams to create and deploy, sometimes custom, workloads more efficiently. This diverse technological landscape generates extensive and rich data from various infrastructure entities, from which, data engineers and analysts collaborate to provide actionable insights to the engineering organization in a continuous feedback loop that ultimately enhances the business.\n\nOne crucial way in which we do this is through the democratization of highly curated data sources that sunshine usage and cost patterns across Netflix\u2019s services and teams. The Data & Insights organization partners closely with our engineering teams to share key efficiency metrics, empowering internal stakeholders to make informed business decisions.\n\nData is Key\n\nThis is where our team, Platform DSE (Data Science Engineering), comes in to enable our engineering partners to understand what resources they\u2019re using, how effectively and efficiently they use those resources, and the cost associated with their resource usage. We want our downstream consumers to make cost conscious decisions using our datasets.\n\nTo address these numerous analytic needs in a scalable way, we\u2019ve developed a two-component solution:\n\nFoundational Platform Data (FPD): This component provides a centralized data layer for all platform data, featuring a consistent data model and standardized data processing methodology. Cloud Efficiency Analytics (CEA): Built on top of FPD, this component offers an analytics data layer that provides time series efficiency metrics across various business use cases.\n\nFoundational Platform Data (FPD)\n\nWe work with different platform data providers to get inventory, ownership, and usage data for the respective platforms they own. Below is an example of how this framework applies to the Spark platform. FPD establishes data contracts with producers to ensure data quality and reliability; these contracts allow the team to leverage a common data model for ownership. The standardized data model and processing promotes scalability and consistency.\n\nCloud Efficiency Analytics (CEA Data)\n\nOnce the foundational data is ready, CEA consumes inventory, ownership, and usage data and applies the appropriate business logic to produce cost and ownership attribution at various granularities. The data model approach in CEA is to compartmentalize and be transparent; we want downstream consumers to understand why they\u2019re seeing resources show up under their name/org and how those costs are calculated. Another benefit to this approach is the ability to pivot quickly as new or changes in business logic is/are introduced.\n\n* For cost accounting purposes, we resolve assets to a single owner, or distribute costs when assets are multi-tenant. However, we do also provide usage and cost at different aggregations for different consumers.\n\nData Principles\n\nAs the source of truth for efficiency metrics, our team\u2019s tenants are to provide accurate, reliable, and accessible data, comprehensive documentation to navigate the complexity of the efficiency space, and well-defined Service Level Agreements (SLAs) to set expectations with downstream consumers during delays, outages or changes.\n\nWhile ownership and cost may seem straightforward, the complexity of the datasets is considerably high due to the breadth and scope of the business infrastructure and platform specific features. Services can have multiple owners, cost heuristics are unique to each platform, and the scale of infra data is large. As we work on expanding infrastructure coverage to all verticals of the business, we face a unique set of challenges:\n\nA Few Sizes to Fit the Majority\n\nDespite data contracts and a standardized data model on transforming upstream platform data into FPD and CEA, there is usually some degree of customization that is unique to that particular platform. As the centralized source of truth, we feel the constant tension of where to place the processing burden. Decision-making involves ongoing transparent conversations with both our data producers and consumers, frequent prioritization checks, and alignment with business needs as informed captains in this space.\n\nData Guarantees\n\nFor data correctness and trust, it\u2019s crucial that we have audits and visibility into health metrics at each layer in the pipeline in order to investigate issues and root cause anomalies quickly. Maintaining data completeness while ensuring correctness becomes challenging due to upstream latency and required transformations to have the data ready for consumption. We continuously iterate our audits and incorporate feedback to refine and meet our SLAs.\n\nAbstraction Layers\n\nWe value people over process, and it is not uncommon for engineering teams to build custom SaaS solutions for other parts of the organization. Although this fosters innovation and improves development velocity, it can create a bit of a conundrum when it comes to understanding and interpreting usage patterns and attributing cost in a way that makes sense to the business and end consumer. With clear inventory, ownership, and usage data from FPD, and precise attribution in the analytical layer, we aim to provide metrics to downstream users regardless of whether they utilize and build on top of internal platforms or on AWS resources directly.\n\nFuture Forward\n\nLooking ahead, we aim to continue onboarding platforms to FPD and CEA, striving for nearly complete cost insight coverage in the upcoming year. Longer term, we plan to extend FPD to other areas of the business such as security and availability. We aim to move towards proactive approaches via predictive analytics and ML for optimizing usage and detecting anomalies in cost.\n\nUltimately, our goal is to enable our engineering organization to make efficiency-conscious decisions when building and maintaining the myriad of services that allow us to enjoy Netflix as a streaming service.\n\nAcknowledgments\n\nThe FPD and CEA work would not have been possible without the cross functional input of many outstanding colleagues and our dedicated team building these important data assets.\n\n\u2014\n\nA bit about the authors:\n\nJHan enjoys nature, reading fantasy, and finding the best chocolate chip cookies and cinnamon rolls. She is adamant about writing the SQL select statement with leading commas.\n\nPallavi enjoys music, travel and watching astrophysics documentaries. With 15+ years working with data, she knows everything\u2019s better with a dash of analytics and a cup of coffee!", "label": 0}
{"title": "Unlock deeper insights with the new Python client library for Data Commons", "url": "https://developers.googleblog.com/en/pythondatacommons/", "content": "Data is the bedrock of progress across nearly every field. It serves as the raw material from which profound insights are forged, enabling us to precisely measure current realities, identify critical trends, and possibly predict future outcomes.\n\nAt Google, our mission with Data Commons is to organize the world's publicly available statistical data, making it more accessible and useful for everyone. It's an open-source knowledge graph that unifies a vast array of public data from diverse sources, simplifying access and comprehension for developers, researchers, and data analysts alike. Along with the datacommons.org website, Google Search uses Data Commons to answer queries like What is the population of San Francisco?, with the top graph generated by Data Commons.\n\nToday, we're announcing the general availability of the new Python client library for the Data Commons based on the V2 REST API. This new Python library dramatically enhances how data developers can leverage Data Commons.\n\n\n\nReal-world impact: partnering with ONE.org\n\nThis milestone was significantly shaped by the vision and substantial contributions of our partner The ONE Campaign, a global organization working to create the investments needed for economic opportunities and healthier lives in Africa. We built Data Commons as an open-source platform precisely to encourage community contributions and enable innovative uses, and this partnership with The ONE Campaign perfectly exemplifies that goal. ONE advocated for, proposed the design and coded the client library to make Data Commons' rich insights available to data scientists and analysts who want to leverage the rich ecosystem of Python analytical tools and libraries.\n\n\n\nSupport for custom Data Commons instances\n\nThe Data Commons platform also allows organizations, like the United Nations or ONE, to host their own Data Commons instances. These custom instances enable the seamless integration of proprietary datasets with the foundational Data Commons knowledge graph. Organizations leverage the Data Commons data framework and tools while maintaining full control over their data and resources.\n\nOne of the most impactful additions in the V2 library is robust support for custom instances. This means you can now use the Python library to programmatically query any public or private instance\u2014whether hosted locally, within your organization or on the Google Cloud Platform.\n\n\n\nPowerful new features\n\nThe Python library makes it very easy to perform common queries against Data Commons data, such as:\n\nExploring the structure of the knowledge graph\n\nRetrieving data for any of the 200,000+ statistical variables from over 200 datasets in domains such as demographics, economy, education, energy, environment, health, and housing\n\nEasily mapping entities from other datasets to entities in Data Commons\n\n\n\nV2 of the client library offers many technical improvements over the V1 library, including:\n\nPandas dataframe APIs are supported as an integral module, with a single installation package, allowing seamless use with other API endpoints in the same client\n\nSeveral new convenience methods for common data queries\n\nAPI key management and other stateful operations built in to the client class\n\nIntegration with the Pydantic libraries for improved type safety, validation and serialization", "label": 0}
{"title": "Unlocking Insights with High-Quality Dashboards at Scale", "url": "https://engineering.atspotify.com/2024/8/unlocking-insights-with-high-quality-dashboards-at-scale", "content": "We have a lot of dashboards at Spotify. Our Insight teams and analysts from across the company are constantly whipping up new dashboards for stakeholders and themselves, helping answer those big data-driven questions every day. These dashboards tackle everything from frequently used key metrics to exploratory insights and operational reports. It\u2019s clear to us: dashboards help us move faster and stay data-informed.\n\nIn 2023, Spotifiers \u2014 mostly data scientists and the like \u2014 created more than 4,900 dashboards in Tableau or Looker Studio. These dashboards were used by more than 6,000 Spotifiers in 2023. There is no centralized team at Spotify solely in charge of creating dashboards, but rather a lively and free market of dashboards \u2014 with the Analytics Platform team providing the scaffolding for this market \u2014 consisting of dashboard producers and dashboard consumers.\n\nAt the heart of our data visualization efforts, we lean on two main tools to amp up our dashboard game: Tableau and Google\u2019s Looker Studio. With these in our toolbox, we\u2019ve built additional systems and frameworks to maximize our ability to distribute, share, and discover insights like never before.\n\nThe core of Spotify\u2019s dashboard platform\n\nLet\u2019s talk Tableau\n\nTableau\u2019s advanced customization options let us create detailed, visually stunning dashboards that are not just informative but can also provide unique user experiences. We can craft a highly specific experience for every use case \u2014 whether that means unique drill-down capabilities or a custom chart type. These dashboards are often thought of as full-fledged internal products, not just data artifacts, each with a distinct user base with specific needs.\n\nNext up: Looker Studio\n\nLooker Studio, our quick and nimble sidekick, offers a seamless integration with Google products that we regularly use, like BigQuery. With Looker Studio, our users can go from SQL to chart to dashboard in a heartbeat. It\u2019s particularly popular among our engineering and product teams for its easy, intuitive user experience.\n\nBoth Tableau and Looker Studio are essential to our strategy, each offering unique advantages. Looker Studio excels in rapid, easy-to-use visualizations, while Tableau provides complex and detailed dashboards for more nuanced needs. This dual-tool approach allows every Spotify employee to choose the solution that best fits their specific requirements, and we provide instructions for getting started with both tools through Golden Path exercises.\n\nEmpowering dashboard producers with the Dashboard Quality Framework\n\nTo ensure our most widely shared dashboards are appropriately and accurately interpreted \u2014 and then showcased across the organization \u2014 we develop them in line with specific standards and practices.\n\nIt starts with dashboard production. Building a dashboard, whether that is in Tableau or in Looker Studio, can involve a lot of moving parts. It requires specific skills in data visualization and storytelling \u2014 skills that require expertise and time that many of us may not have. On top of that, Spotifiers looking to build a dashboard are likely in different parts of the org and not always in communication or enforcing the same standards. To alleviate quality challenges with distributed dashboard design \u2014 at scale \u2014 we developed a Dashboard Quality Framework.\n\nThere are two parts to the Dashboard Quality Framework:\n\nVital Signs: A set of automatic checks to ensure the dashboard is \u201calive and well.\u201d These are enabled by the information we get from API and logs and include information on owner status, dashboard description, using LDAP groups for sharing, underlying data endpoint lifecycle, and whether updates and extract refreshes have been made recently. Spicy Dashboard Design Checklist: A checklist of data visualization and dashboard design best practices that could turn a vanilla report into a \u201cspicy\u201d dashboard. The checklist includes items related to visual design, usability, insights, and trust. This is a manual step that requires the human eye to determine. To enable this checklist at scale, we\u2019ve turned this into a self-evaluating checklist that flags the dashboard when successfully completed.\n\nBased on the results of these measurements, a dashboard then receives one of these quality labels:\n\nLow \u2014 fails Vital Signs (regardless of Spicy Design Checklist outcome)\n\nHigh \u2014 passes Vital Signs\n\nGolden \u2014 passes Vital Signs and Spicy Design Checklist\n\nTen percent of eligible dashboards at Spotify have achieved Golden status \u2014 and we strive to keep more than 80% of all dashboards at High or above, informing dashboard owners when their dashboards don\u2019t meet the Quality Framework criteria.\n\nBelow you will find the exact details of this framework \u2014 stealing is encouraged!\n\nThe \u2018Spicy Design\u201d Dashboard ChecklistDownload\n\nTo help our Tableau users, we\u2019ve created a service that helps create and manage Tableau extracts.We\u2019ve developed a SQL scheduling tool at Spotify that helps us schedule and run batch data workflows in Google BigQuery. We\u2019ve enhanced this service to produce and publish Tableau .hyper files that can be used in dashboards on Tableau Cloud. These workflows run BigQuery SQL when upstream dependencies are available and are often faster, can load more data, and are more convenient to edit as the SQL is in .yaml files instead of embedded within Tableau.\n\nAnd to truly drive home the idea that dashboards are a product, we equip all dashboard owners with statistics and insights about their own dashboards \u2014 showing number of active users, weekly retention, users by org or job family, and an ability to easily email users (think version updates or deprecation announcements). This also allows people to consider if they should market the dashboard to a new audience, or if they should deprecate them when usage is very low. It\u2019s all about empowering dashboards owners to make great data-informed decisions about their data and dashboard products.\n\nPowering dashboard consumption with Dashboard Portal\n\nInsightful and beautiful dashboards aren\u2019t any good locked away \u2014 they need to be accessible to interested stakeholders!\n\nTo provide easy access for dashboard consumers, we developed the Dashboard Portal. Dashboard Portal is an internal site consisting of a searchable catalog of all published dashboards at Spotify, both from Tableau and Looker Studio. It includes ways to organize and curate groups of dashboards and adds extra context to dashboards for viewers.\n\nThe primary features of Dashboard Portal include:\n\nSearch: Users can search by title, description, and field names to find existing dashboards. No more \u201cDo we have a dashboard about X ?\u201d!\n\nCuration: Dashboard authors can create curated collections of dashboards that become a go-to hub for their teams. Think \u201cThe Ads Dashboard Hub,\u201d containing all the dashboards the team believe are important for their stakeholders to be aware of.\n\nTrust and Quality: The Dashboard Quality Framework labels are displayed as a badge on top of the dashboard thumbnails and on all dashboard pages, giving viewers a quick understanding of a dashboard\u2019s quality, and more importantly, whether they can confidently trust it. We\u2019re able to spotlight and draw attention to our Golden dashboards \u2014 our cream of the crop! In doing this, we\u2019ve seen that users are more likely to view dashboards with a Golden label.\n\nUbiquity: While Tableau and Looker Studio each have their own discovery mechanisms, we\u2019ve found that having a Dashboard Portal that surfaces content from multiple tools enables the user to not have to think about where the dashboard lives, but rather just find it.\n\nEnhanced Context: Dashboards are embedded directly into the Portal and boosted with extra context: dashboard owner, last data refresh date, usage stats, and underlying SQL queries.\n\nConclusion\n\nOur robust approach to dashboard creation and management showcases our commitment to data democratization and informed decision-making across the company. Our sophisticated tools and frameworks ensure that every dashboard is not only visually appealing but also a well-oiled machine delivering actionable insights.\n\nSpecial thanks to Jacob Olsufka, Arielle Silverman, Pavlina Mitsou, Daniel Wolmerud, Alex Pavlov, Axel \u00d6rnefalk, Robert Hjortsmarker, and Abhishek Upadhyay.", "label": 0}
{"title": "A day in the life: Engineer onboarding at Dropbox", "url": "https://dropbox.tech/culture/a-day-in-the-life-engineer-onboarding-at-dropbox", "content": "We\u2019re Adam Hood and Brian Amaratunga\u2014two senior software engineers who joined Dropbox in 2021 as Virtual First employees. This means we spend most of our time working remotely, with physical studios reserved for in-person collaboration. When Dropbox became a Virtual First company in October 2020, it also meant reimagining the onboarding process to ensure new employees still had a high-quality experience\u2014similar to what they would have gotten before in person. As two recent hires in Engineering, Product, and Design (EPD), we wanted to share our experience of what virtual onboarding at Dropbox is actually like. Adam: I\u2019m an engineer on the Business Space Experience team. Our goal is to improve the workflows of teams that use Dropbox. My story with Dropbox goes back to my college days in 2015 when Dropbox would recruit very heavily on my campus. They would hand out Dropbox t-shirts like they were problem sets, and I suspect that at least a quarter of the undergraduate population had one, if not more. Fast forward to Spring 2021; my job in tech was satisfying enough, but I really wanted to look for ways to grow my career\u2014and I came across Dropbox once again. I\u2019ve always been impressed by their innovation. Early in my career I remember integrating zxcvbn, an open source password strength estimator developed by Dropbox, into a login page that I was developing! As I interviewed with Dropbox, I enjoyed thinking through the tough interview questions\u2014which were unlike any I\u2019d seen before\u2014and I was encouraged by the answers the interviewers gave to some tough questions from me. They really seemed to believe in the company, and were excited about Virtual First. Brian: I\u2019m an engineer on the Organized Experience team. Our mission is to give our users the tools to keep their files tidy and organized\u2014or, as we like to say, \u201ca place for everything and everything in its place.\u201d I was looking to move on from my previous job in the healthcare software industry, so I made a profile on Hired.com. I was mostly expecting to learn about startups and smaller companies that were hiring, but Dropbox was one of the first companies to reach out. I\u2019ve always heard great things about Dropbox\u2019s engineering talent and culture\u2014and with what I\u2019d read about their vision of remote work, I was very interested in continuing the hiring process. I thoroughly enjoyed my interviews, and the questions felt challenging as opposed to just questions copied directly from LeetCode. In addition, I was impressed by all the engineers and recruiters I spoke with, which made me realize my initial impression of Dropbox was true. Once I matched with a team and learned more about the projects I\u2019d be working on, it was a very easy choice to accept the offer. Before Dropbox, we both experienced in-person onboarding at other companies, but they were of opposite extremes. Adam had essentially no onboarding at his previous company. Brian was overloaded with classes but had little practical experience early on. But at Dropbox, we found a middle ground. New hires are called Droplets, and have 90 days to get up to speed on our culture, learn various teams' processes, and ship their first small project. Engineers get all the resources they need to succeed without being overwhelmed by information\u2014or immediate pressure to deliver results.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images\u2014plus generative AI capabilities across even more connected apps. See what's new \u2192\n\nGetting started\n\nOur first day of onboarding was held over Zoom and led by our onboarding lead Alinane. We were both very impressed. As a group we talked through the Dropbox mission, our values, and business strategy. We also set up our laptops and signed-up for benefits. Spending the day on Zoom was exhausting at times, but our onboarding struck the right balance of being useful and informative without being overwhelming. One of the most useful things we received was an onboarding checklist of important tasks and a timeline for when we should do them, be it in our first week or first month. This allowed us to learn more about the company\u2014from senior leadership to our users\u2019 workflows\u2014at our own pace. The next day, we met our onboarding buddies. Adam was paired with Bozhen, and Brian was paired with Jiayi. They were responsible for helping us get settled\u2014and at first they bombarded us with loads of information. For example, we received dozens of links to various useful resources. We also learned everyone's names and roles\u2014both on our team, as well as the teams we would work most closely with. It was overwhelming at first, but also very helpful to have a fellow engineer as a resource for whatever questions popped into our heads or to guide us through various processes. Our onboarding buddies were always gracious with their time, whether responding to messages over Slack or walking us through our team\u2019s strategies over Zoom. Since we would primarily be working remotely, one of our top priorities was to set up our workspaces. Adam: I was very worried about getting used to the Touch Bar on my new Mac, so the first thing I wanted was a keyboard. Luckily, Dropbox made it easy for us to order gear like this through an internal website called Dropgear! I later added a standalone trackpad and a monitor to my desk. I really love my overall setup at this point. Brian: I also used Dropgear during my first week to get a monitor and a mouse! There were some issues with the supplier, so it took several weeks to get the mouse, but it took less than a week to get a 34-inch ultrawide monitor from Dropgear\u2014without paying a penny! Dropbox also has something called a Perks Allowance which we can spend on pretty much anything we want (with a few exceptions). I joined a week before the quarter ended, but was still able to use the full amount to get a really nice adjustable standing desk and Steelcase chair.\n\nGetting to know the company\n\nOnboarding for new EPD hires consists of both interactive video calls and documentation for asynchronous (async) review\u2014meaning at our own pace. There were around five video call sessions over the first couple weeks, each around one-to-two hours, while the rest of the information was documented in Dropbox Paper for us to peruse when we had time. This mix of video calls and async learning was our first introduction to \u201casync by default,\u201d one of the key tenets of Virtual First at Dropbox. This means Dropboxers default to sharing information async\u2014communicating via Slack or email as much as possible, and reserving meetings and real-time communication for discussion, debates, and big decisions. Each video call session was designed to get us acquainted with how Dropbox works on a technical level, both as a product and as a company. We learned about how we use open-source software and best practices for committing and reviewing code. Particularly enjoyable was a talk that introduced the overarching architecture behind Dropbox. It was really nice to get a solid mental model of all the pieces\u2014such as our hybrid approach between a monolithic and service-oriented architecture (Atlas), our async task framework (ATF), and our block storage solution (Magic Pocket)\u2014and how they all fit together. There was enough information to get us started and show us where to go when we were ready to dig deeper. And it was perfectly reasonable if we didn\u2019t recall everything that was covered; each session was recorded and made available to watch again later in case we needed to refresh our knowledge. The remaining onboarding sessions were written in Dropbox Paper for us to go over at our own pace. These self-guided lessons included a deep-dive into how our file system syncing and sharing works, and a guide to preparing for our twice-yearly performance reviews. Having so much material available async gave us a lot of flexibility when choosing when to learn and how we prioritized what to learn first. If either of us was feeling Zoom fatigue or needed some time to digest a more complex session from the morning, there was no need to jump straight into another potentially exhausting video call. We could take as much time as we needed and then jump into an async session when we were ready! At the same time, this flexibility allowed us to fit our lessons into whatever schedule worked for us and our managers. This meant we could start working on actual projects earlier in the onboarding process. We even got to ship code during our first week! Onboarding was also the perfect introduction to another core pillar of Virtual First at Dropbox: Core Collaboration Hours. These are four hour blocks when all meetings are supposed to be held. In North America, for example, Core Collaboration Hours are from 12 pm to 4pm ET. The remaining four hours of the day are meant for uninterrupted deep work\u2014and everyone has the flexibility to work those hours whenever they feel most productive, because Dropbox is async by default. This helps reduce unnecessary meetings and help us make time for what matters most, from deep work and team building to time with family and friends.\n\nBrian\u2019s calendar. Note that my team agreed to shift our Core Collaboration Hours to 1 pm to 5 pm ET.\n\nGetting to know the team\n\nWe were definitely worried about how we would build connections with our team in a virtual first environment. At our previous companies, we were able to walk up to people's desks or casually grab coffee, but that wouldn\u2019t be possible here. However, with some gentle nudging from our managers, we realized we just had to be more purposeful about building those connections than we would have before. In our first couple of weeks, we grabbed quick, 15-20 minute one-on-one meetings with our teammates. We shared how we ended up at Dropbox, and heard how each of our coworkers arrived here as well. We were impressed by the diversity of experience of our various team members, who came from all sorts of unique backgrounds, and attended virtual social events to get to know them better. For example, Brian\u2019s team has a weekly virtual board game event where the team gets together to play board games online. While meeting people remotely was a new experience, our colleagues did a great job of making us feel welcome from the start.\n\nGetting to know the work\n\nWe received our onboarding projects on our second day at Dropbox from our onboarding buddies Bozhen and Jiayi. Adam worked with Bozhen on updating modals in the Dropbox web client to improve the file upload flow, while Brian worked on adding some new naming convention rules to help users keep their file system organized. Bozhen and Jiayi did a great job breaking these projects down into well-defined, manageable tasks. They introduced us to the sprint planning processes, roughly estimating the time required and assigning due dates for our work. With our onboarding buddies handling the project planning, we could focus on getting to know the codebase\u2014including the code commit and review process\u2014without the added stress of vague or overcomplicated goals. Codelabs were one of the most useful resources in our onboarding. These are targeted tutorials that demonstrate how to accomplish different tasks within the Dropbox ecosystem\u2014such as how to create a new API endpoint or how to create a feature gate. We found them to be extremely useful, not just as tutorials, but as reference materials when completing our first projects, ensuring we didn\u2019t miss any steps. For example, Brian was able to ship code his very first week, and used the feature gate codelab to prevent people outside Dropbox from being able to access his work while it was still under development. Our onboarding projects lasted 6-8 weeks. Brian shipped the new naming convention rules to users during his seventh week at Dropbox! At the same time, Adam helped Bozhen complete the new file upload flow a couple weeks ahead of schedule, enabling the team to start user testing sooner. By this point, we were pretty comfortable with the Dropbox development ecosystem and started working with the rest of the team on other projects. Even though we were technically still onboarding and reading through the occasional onboarding doc, we felt like we were fully integrated into the team.\n\nFinal thoughts", "label": 0}
{"title": "I'm using traefik on my infrastructure.", "url": "https://lifeofpablo.com/blog/i-m-using-traefik-on-my-infrastructure", "content": "I'm using traefik on my infrastructure.\n\nTraefik Logo\n\nThis post was written in English (en_US).\n\nSome Background\n\nFor a few weeks, I've been thinking about using another reverse-proxy service just to try something new.\n\nSince I launched my docker mono-repository that hosts my entire infrastructure, I've used nginx as my primary reverse-proxy. I was using Jason WIlder's nginx proxy to expose my applications to the web from the virtual machine.\n\nNow I have transitioned to using traefik. It was very easy to add labels specific to traefik to my services in my docker compose file.\n\nIn my docker compose setup, I used Let's Encrypt for SSL certificates in both nginx and traefik.\n\nJust like with any major change, I was worried about downtime but I took the risk and I tested out *lifeofpablo.com * and some other subdomains. Once I labeled the items with traefik specific items, and redeployed, it worked great! Once I looked around for anything out of place, I went back and added traefik labels to the rest of my containers.\n\nnginx\n\nWithin the docker file, nginx-proxy would use environmental variables to route outside of the machine to the domain or subdomain that the DNS will point to.\n\nenvironment: VIRTUAL_HOST: service.lifeofpablo.com LETSENCRYPT_HOST: service.lifeofpablo.com\n\ntraefik\n\nInstead of using environmental variables explicitly, traefik uses labels to route. In this example, I'm using my the service korea.lifeofpablo.com (korea) as the service being proxied.\n\nI simply matched the service name, korea as this is the name of the container in Docker and for the host, I used korea.lifeofpablo.com.\n\nlabels: - \"traefik.http.routers.korea.rule=Host(`korea.lifeofpablo.com`)\" - \"traefik.http.routers.korea.entrypoints=websecure\" - \"traefik.http.routers.korea.tls.certresolver=myresolver\"\n\nFuture\n\nWhat I have in mind for the future of my infrastructure with traefik is to control authentication. I want to use Zitadel to add authentication to services I have to simply experiment. This experimentation will be key as I continue planning the rebuild of my website. I want my future version of my website to be the center of my internet presence and be able to connect easily with my services.", "label": 1}
{"title": "Taking the plunge: The engineering journey of building a subsea cable", "url": "https://engineering.fb.com/2025/05/01/connectivity/taking-the-plunge-the-engineering-journey-of-building-a-subsea-cable/", "content": "Meta develops infrastructure all across the globe to transport information and content for the billions of people using our services around the world. At the core of this infrastructure are aggregation points \u2013 like data centers \u2013 and the digital cables that connect them. Subsea cables \u2013 the unseen digital highways of the internet \u2013 are critical for Meta to serve people wherever they are in the world. In fact, more than 95% of the world\u2019s intercontinental traffic goes through subsea cables.\n\nMeta\u2019s engineering team prioritizes both innovation and quality when designing and deploying these cables. In the latest Meta Tech Podcast, Andy Palmer-Felgate and Pascal Pecci, both subsea cable systems engineers, join Pascal Hartig on the Meta Tech podcast to discuss the latest in subsea engineering technology. This episode dives deeper into the engineering nuances of large-scale subsea cable projects like the recently announced Project Waterworth.\n\nLearn more about Meta\u2019s work on these engineering feats. Download or listen to the episode below:\n\nThe Meta Tech Podcast is a podcast, brought to you by Meta, where we highlight the work Meta\u2019s engineers are doing at every level \u2013 from low-level frameworks to end-user features.\n\nSend us feedback on Instagram, Threads, or X.\n\n\n\nAnd if you\u2019re interested in learning more about career opportunities at Meta, visit the Meta Careers page.", "label": 0}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2023-08", "content": "Some Back Story\n\nOver the years, I reflected on how much I do love Nebraska and what it means to me. I moved away two years ago from my home state of Nebraska to the state of California. I believe it's safe to say that California is home. I've done a lot growing as a person. I am not the same person who was when I left Nebraska. I'm still growing.\n\nWhy Leave Nebraska?\n\nNebraska will forever be home. It was the place that saw me grow up. I got a great public education. It taught me how to say, \"Ope!\" and how to be a Midwestern Mexican Guy.\n\nIt's been in my mind for a while to leave. That, \"for a while\" thought has been in my mind since I was a kid. At a young age I had all these dreams and ambitions to live in various places throughout the world.\n\nI was very fortune to have visited different places in high school and in university, I got to live in France through study abroad. These experiences solidified my need to venture off more throughout the United States. Traveling abroad since I was a young kid has helped me realize there are so many great things outside of Nebraska. It's funny because I joke around how I've traveled more outside the country than in the country I reside in. I am very fortunate to have lived in Mexico for extended periods of time throughout my life.\n\nPreventing Resentment and Repeating the Mistakes of Others\n\nI've always craved so much more than the simple life back home. I didn't want to feel trapped and become resentful for lack of trying to live in and experiencing new things in new places. I see many of the people who I grew up with and reflect on my own self. I didn't want that lifestyle. So many people who I had wished had left Nebraska for greener pastures didn't leave. I didn't want to be trapped.\n\nQuestioning the Environment\n\nI've always questioned my environment. These are the questions:\n\nWhy don't we have public transportation?\n\nWhy is it weird to walk on the sidewalk in such a walk-able town?\n\nWhy are people of afraid of good change?\n\nWhy don't we think of the needs of young people who will lead the future?\n\nMy Lifestyle\n\nBeing Nebraska wasn't fitting my lifestyle anymore. I had outgrown the town of 25,000 habitants. Growing up, I never really accepted myself as a person in various degrees. I tolerated myself at best. I was smiling without actually being happy. This fa\u00e7ade of being known as, \"the guy who always has a smile on his face,\" was getting old. It was draining me for years. I truly wasn't happy. No one would ever guess that. I lost myself as an individual and I also lost who I wanted to become as individual. Looking at myself in the mirror was not a true reflection of me. I just didn't feel like anything was truly going for me. There are so many things I wish I would have addressed sooner.\n\nLeaving Nebraska was going to happen sooner than later. I had a friend nudge me a few years back to finally do it. I am grateful he nudged me enough that I felt it in my ribs.\n\nCalifornia\n\nI'm Happier as a Person\n\nI'm living in California now for two years now. Time flies!\n\nI'm a lot happier here. I'm my more genuine self. I'm still not where I want to be. This is something that I am working on. California is not a perfect place.\n\nI've made a lot more progress here. I'm slowly healing myself. This will take time.", "label": 1}
{"title": "Part 3: A Survey of Analytics Engineering Work at Netflix", "url": "https://netflixtechblog.com/part-3-a-survey-of-analytics-engineering-work-at-netflix-e67f0aa82183?source=collection_home---4------5-----------------------", "content": "Part 3: A Survey of Analytics Engineering Work at Netflix Netflix Technology Blog 9 min read \u00b7 Jan 6, 2025 -- 3 Listen Share\n\nThis article is the last in a multi-part series sharing a breadth of Analytics Engineering work at Netflix, recently presented as part of our annual internal Analytics Engineering conference. Need to catch up? Check out Part 1, which detailed how we\u2019re empowering Netflix to efficiently produce and effectively deliver high quality, actionable analytic insights across the company and Part 2, which stepped through a few exciting business applications for Analytics Engineering. This post will go into aspects of technical craft.\n\nDashboard Design Tips\n\nRina Chang, Susie Lu\n\nWhat is design, and why does it matter? Often people think design is about how things look, but design is actually about how things work. Everything is designed, because we\u2019re all making choices about how things work, but not everything is designed well. Good design doesn\u2019t waste time or mental energy; instead, it helps the user achieve their goals.\n\nWhen applying this to a dashboard application, the easiest way to use design effectively is to leverage existing patterns. (For example, people have learned that blue underlined text on a website means it\u2019s a clickable link.) So knowing the arsenal of available patterns and what they imply is useful when making the choice of when to use which pattern.\n\nFirst, to design a dashboard well, you need to understand your user.\n\nTalk to your users throughout the entire product lifecycle. Talk to them early and often, through whatever means you can.\n\nUnderstand their needs, ask why, then ask why again. Separate symptoms from problems from solutions.\n\nPrioritize and clarify \u2014 less is more! Distill what you can build that\u2019s differentiated and provides the most value to your user.\n\nHere is a framework for thinking about what your users are trying to achieve. Where do your users fall on these axes? Don\u2019t solve for multiple positions across these axes in a given view; if that exists, then create different views or potentially different dashboards.\n\nSecond, understanding your users\u2019 mental models will allow you to choose how to structure your app to match. A few questions to ask yourself when considering the information architecture of your app include:\n\nDo you have different user groups trying to accomplish different things? Split them into different apps or different views.\n\nWhat should go together on a single page? All the information needed for a single user type to accomplish their \u201cjob.\u201d If there are multiple jobs to be done, split each out onto its own page.\n\nWhat should go together within a single section on a page? All the information needed to answer a single question.\n\nDoes your dashboard feel too difficult to use? You probably have too much information! When in doubt, keep it simple. If needed, hide complexity under an \u201cAdvanced\u201d section.\n\nHere are some general guidelines for page layouts:\n\nChoose infinite scrolling vs. clicking through multiple pages depending on which option suits your users\u2019 expectations better\n\nLead with the most-used information first, above the fold\n\nCreate signposts that cue the user to where they are by labeling pages, sections, and links\n\nUse cards or borders to visually group related items together\n\nLeverage nesting to create well-understood \u201cscopes of control.\u201d Specifically, users expect a controller object to affect children either: Below it (if horizontal) or To the right of it (if vertical)\n\nThird, some tips and tricks can help you more easily tackle the unique design challenges that come with making interactive charts.\n\nTitles: Make sure filters are represented in the title or subtitle of the chart for easy scannability and screenshot-ability.\n\nTooltips: Core details should be on the page, while the context in the tooltip is for deeper information. Annotate multiple points when there are only a handful of lines.\n\nAnnotations: Provide annotations on charts to explain shifts in values so all users can access that context.\n\nColor: Limit the number of colors you use. Be consistent in how you use colors. Otherwise, colors lose meaning.\n\nOnboarding: Separate out onboarding to your dashboard from routine usage.\n\nFinally, it is important to note that these are general guidelines, but there is always room for interpretation and/or the use of good judgment to adapt them to suit your own product and use cases. At the end of the day, the most important thing is that a user can leverage the data insights provided by your dashboard to perform their work, and good design is a means to that end.\n\nLearnings from Deploying an Analytics API at Netflix\n\nDevin Carullo\n\nAt Netflix Studio, we operate at the intersection of art and science. Data is a tool that enhances decision-making, complementing the deep expertise and industry knowledge of our creative professionals.\n\nOne example is in production budgeting \u2014 namely, determining how much we should spend to produce a given show or movie. Although there was already a process for creating and comparing budgets for new productions against similar past projects, it was highly manual. We developed a tool that automatically selects and compares similar Netflix productions, flagging any anomalies for Production Finance to review.\n\nTo ensure success, it was essential that results be delivered in real-time and integrated seamlessly into existing tools. This required close collaboration among product teams, DSE, and front-end and back-end developers. We developed a GraphQL endpoint using Metaflow, integrating it into the existing budgeting product. This solution enabled data to be used more effectively for real-time decision-making.\n\nWe recently launched our MVP and continue to iterate on the product. Reflecting on our journey, the path to launch was complex and filled with unexpected challenges. As an analytics engineer accustomed to crafting quick solutions, I underestimated the effort required to deploy a production-grade analytics API.\n\nFig 1. My vague idea of how my API would work\n\nFig 2: Our actual solution\n\nWith hindsight, below are my key learnings.\n\nMeasure Impact and Necessity of Real-Time Results\n\nBefore implementing real-time analytics, assess whether real-time results are truly necessary for your use case. This can significantly impact the complexity and cost of your solution. Batch processing data may provide a similar impact and take significantly less time. It\u2019s easier to develop and maintain, and tends to be more familiar for analytics engineers, data scientists, and data engineers.\n\nAdditionally, if you are developing a proof of concept, the upfront investment may not be worth it. Scrappy solutions can often be the best choice for analytics work.\n\nExplore All Available Solutions\n\nAt Netflix, there were multiple established methods for creating an API, but none perfectly suited our specific use case. Metaflow, a tool developed at Netflix for data science projects, already supported REST APIs. However, this approach did not align with the preferred workflow of our engineering partners. Although they could integrate with REST endpoints, this solution presented inherent limitations. Large response sizes rendered the API/front-end integration unreliable, necessitating the addition of filter parameters to reduce the response size.\n\nAdditionally, the product we were integrating into was using GraphQL, and deviating from this established engineering approach was not ideal. Lastly, given our goal to overlay results throughout the product, GraphQL features, such as federation, proved to be particularly advantageous.\n\nAfter realizing there wasn\u2019t an existing solution at Netflix for deploying python endpoints with GraphQL, we worked with the Metaflow team to build this feature. This allowed us to continue developing via Metaflow and allowed our engineering partners to stay on their paved path.\n\nAlign on Performance Expectations\n\nA major challenge during development was managing API latency. Much of this could have been mitigated by aligning on performance expectations from the outset. Initially, we operated under our assumptions of what constituted an acceptable response time, which differed greatly from the actual needs of our users and our engineering partners.\n\nUnderstanding user expectations is key to designing an effective solution. Our methodology resulted in a full budget analysis taking, on average, 7 seconds. Users were willing to wait for an analysis when they modified a budget, but not every time they accessed one. To address this, we implemented caching using Metaflow, reducing the API response time to approximately 1 second for cached results. Additionally, we set up a nightly batch job to pre-cache results.\n\nWhile users were generally okay with waiting for analysis during changes, we had to be mindful of GraphQL\u2019s 30-second limit. This highlighted the importance of continuously monitoring the impact of changes on response times, leading us to our next key learning: rigorous testing.\n\nReal-Time Analysis Requires Rigorous Testing\n\nLoad Testing: We leveraged Locust to measure the response time of our endpoint and assess how the endpoint responded to reasonable and elevated loads. We were able to use FullStory, which was already being used in the product, to estimate expected calls per minute.\n\nFig 3. Locust allows us to simulate concurrent calls and measure response time\n\nUnit Tests & Integration Tests: Code testing is always a good idea, but it can often be overlooked in analytics. It is especially important when you are delivering live analysis to circumvent end users from being the first to see an error or incorrect information. We implemented unit testing and full integration tests, ensuring that our analysis would return correct results.\n\nThe Importance of Aligning Workflows and Collaboration\n\nThis project marked the first time our team collaborated directly with our engineering partners to integrate a DSE API into their product. Throughout the process, we discovered significant gaps in our understanding of each other\u2019s workflows. Assumptions about each other\u2019s knowledge and processes led to misunderstandings and delays.\n\nDeployment Paths: Our engineering partners followed a strict deployment path, whereas our approach on the DSE side was more flexible. We typically tested our work on feature branches using Metaflow projects and then pushed results to production. However, this lack of control led to issues, such as inadvertently deploying changes to production before the corresponding product updates were ready and difficulties in managing a test endpoint. Ultimately, we deferred to our engineering partners to establish a deployment path and collaborated with the Metaflow team and data engineers to implement it effectively.\n\nFig 4. Our current deployment path\n\nWork Planning: While the engineering team operated on sprints, our DSE team planned by quarters. This misalignment in planning cycles is an ongoing challenge that we are actively working to resolve.\n\nLooking ahead, our team is committed to continuing this partnership with our engineering colleagues. Both teams have invested significant time in building this relationship, and we are optimistic that it will yield substantial benefits in future projects.\n\nExternal Speaker: Benn Stancil\n\nIn addition to the above presentations, we kicked off our Analytics Summit with a keynote talk from Benn Stancil, Founder of Mode Analytics. Benn stepped through a history of the modern data stack, and the group discussed ideas on the future of analytics.", "label": 0}
{"title": "Day 4 - World Leaders Camp.", "url": "https://lifeofpablo.com/blog/day-4-world-leaders-camp", "content": "Day 4 - World Leaders Camp.\n\nThis post was written in English (en_US).\n\n\"I forgot to write for yesterday. Might as well write for today also!\n\nWe started our day with origami. It was a fun activity (if you can get past step 1.) I've done some origami over the years. It was a struggle a bit trying to build something. Not to bad after you get the hang of it.\n\nThen We had a leadership activity with Dr. Machida. It was really fun getting to know about Japan and Dr. Machida's experiences abroad at a young age. It really motivates me to study abroad!! :)\n\nI also learned how to use chopsticks. Who knew that it could be fun? It was a challenge none less.... Now I want to use them at any Chinese place.\n\nAfter Lunch We got to meet some of the students that have been on a study abroad program. Some went to the Czech Republic or Netherlands. They told us their crazy adventures. They also explained some struggles and surprises they experience. I felt like I related to them since I have also gone out of the country plenty of times. They really assured us that it is totally worth it.\n\nWe had some of the international students help us with calligraphy. They helped us all write our names in Japanese. This but also words that had a meaning to our own. I picked 'potential' and 'perseverance.' I say they did a great job . Props to you guys.\n\nThe chopsticks lesson came to great use!! We went to a Chinese restaurant. I ordered spicy chicken. Used my chopsticks to eat my entree. I felt like I succeeded at a life.\n\nLater everyone decided to take naps after a very busy day. I passed out on my bed, so did my roommate on his.\n\nI stayed up later to watch some funny movies... it was totally worth it :) :)\"", "label": 1}
{"title": "Recommending for Long-Term Member Satisfaction at Netflix", "url": "https://netflixtechblog.com/recommending-for-long-term-member-satisfaction-at-netflix-ac15cada49ef?source=collection_home---4------17-----------------------", "content": "Recommending for Long-Term Member Satisfaction at Netflix Netflix Technology Blog 8 min read \u00b7 Aug 29, 2024 -- 9 Listen Share\n\nBy Jiangwei Pan, Gary Tang, Henry Wang, and Justin Basilico\n\nIntroduction\n\nOur mission at Netflix is to entertain the world. Our personalization algorithms play a crucial role in delivering on this mission for all members by recommending the right shows, movies, and games at the right time. This goal extends beyond immediate engagement; we aim to create an experience that brings lasting enjoyment to our members. Traditional recommender systems often optimize for short-term metrics like clicks or engagement, which may not fully capture long-term satisfaction. We strive to recommend content that not only engages members in the moment but also enhances their long-term satisfaction, which increases the value they get from Netflix, and thus they\u2019ll be more likely to continue to be a member.\n\nRecommendations as Contextual Bandit\n\nOne simple way we can view recommendations is as a contextual bandit problem. When a member visits, that becomes a context for our system and it selects an action of what recommendations to show, and then the member provides various types of feedback. These feedback signals can be immediate (skips, plays, thumbs up/down, or adding items to their playlist) or delayed (completing a show or renewing their subscription). We can define reward functions to reflect the quality of the recommendations from these feedback signals and then train a contextual bandit policy on historical data to maximize the expected reward.\n\nImproving Recommendations: Models and Objectives\n\nThere are many ways that a recommendation model can be improved. They may come from more informative input features, more data, different architectures, more parameters, and so forth. In this post, we focus on a less-discussed aspect about improving the recommender objective by defining a reward function that tries to better reflect long-term member satisfaction.\n\nRetention as Reward?\n\nMember retention might seem like an obvious reward for optimizing long-term satisfaction because members should stay if they\u2019re satisfied, however it has several drawbacks:\n\nNoisy : Retention can be influenced by numerous external factors, such as seasonal trends, marketing campaigns, or personal circumstances unrelated to the service.\n\n: Retention can be influenced by numerous external factors, such as seasonal trends, marketing campaigns, or personal circumstances unrelated to the service. Low Sensitivity : Retention is only sensitive for members on the verge of canceling their subscription, not capturing the full spectrum of member satisfaction.\n\n: Retention is only sensitive for members on the verge of canceling their subscription, not capturing the full spectrum of member satisfaction. Hard to Attribute : Members might cancel only after a series of bad recommendations.\n\n: Members might cancel only after a series of bad recommendations. Slow to Measure: We only get one signal per account per month.\n\nDue to these challenges, optimizing for retention alone is impractical.\n\nProxy Rewards\n\nInstead, we can train our bandit policy to optimize a proxy reward function that is highly aligned with long-term member satisfaction while being sensitive to individual recommendations. The proxy reward r(user, item) is a function of user interaction with the recommended item. For example, if we recommend \u201cOne Piece\u201d and a member plays then subsequently completes and gives it a thumbs-up, a simple proxy reward might be defined as r(user, item) = f(play, complete, thumb).\n\nClick-through rate (CTR)\n\nClick-through rate (CTR), or in our case play-through rate, can be viewed as a simple proxy reward where r(user, item) = 1 if the user clicks a recommendation and 0 otherwise. CTR is a common feedback signal that generally reflects user preference expectations. It is a simple yet strong baseline for many recommendation applications. In some cases, such as ads personalization where the click is the target action, CTR may even be a reasonable reward for production models. However, in most cases, over-optimizing CTR can lead to promoting clickbaity items, which may harm long-term satisfaction.\n\nBeyond CTR\n\nTo align the proxy reward function more closely with long-term satisfaction, we need to look beyond simple interactions, consider all types of user actions, and understand their true implications on user satisfaction.\n\nWe give a few examples in the Netflix context:\n\nFast season completion \u2705: Completing a season of a recommended TV show in one day is a strong sign of enjoyment and long-term satisfaction.\n\n\u2705: Completing a season of a recommended TV show in one day is a strong sign of enjoyment and long-term satisfaction. Thumbs-down after completion \u274c: Completing a TV show in several weeks followed by a thumbs-down indicates low satisfaction despite significant time spent.\n\n\u274c: Completing a TV show in several weeks followed by a thumbs-down indicates low satisfaction despite significant time spent. Playing a movie for just 10 minutes \u2753: In this case, the user\u2019s satisfaction is ambiguous. The brief engagement might indicate that the user decided to abandon the movie, or it could simply mean the user was interrupted and plans to finish the movie later, perhaps the next day.\n\n\u2753: In this case, the user\u2019s satisfaction is ambiguous. The brief engagement might indicate that the user decided to abandon the movie, or it could simply mean the user was interrupted and plans to finish the movie later, perhaps the next day. Discovering new genres \u2705 \u2705: Watching more Korean or game shows after \u201cSquid Game\u201d suggests the user is discovering something new. This discovery was likely even more valuable since it led to a variety of engagements in a new area for a member.\n\nReward Engineering\n\nReward engineering is the iterative process of refining the proxy reward function to align with long-term member satisfaction. It is similar to feature engineering, except that it can be derived from data that isn\u2019t available at serving time. Reward engineering involves four stages: hypothesis formation, defining a new proxy reward, training a new bandit policy, and A/B testing. Below is a simple example.\n\nChallenge: Delayed Feedback\n\nUser feedback used in the proxy reward function is often delayed or missing. For example, a member may decide to play a recommended show for just a few minutes on the first day and take several weeks to fully complete the show. This completion feedback is therefore delayed. Additionally, some user feedback may never occur; while we may wish otherwise, not all members provide a thumbs-up or thumbs-down after completing a show, leaving us uncertain about their level of enjoyment.\n\nWe could try and wait to give a longer window to observe feedback, but how long should we wait for delayed feedback before computing the proxy rewards? If we wait too long (e.g., weeks), we miss the opportunity to update the bandit policy with the latest data. In a highly dynamic environment like Netflix, a stale bandit policy can degrade the user experience and be particularly bad at recommending newer items.\n\nSolution: predict missing feedback\n\nWe aim to update the bandit policy shortly after making a recommendation while also defining the proxy reward function based on all user feedback, including delayed feedback. Since delayed feedback has not been observed at the time of policy training, we can predict it. This prediction occurs for each training example with delayed feedback, using already observed feedback and other relevant information up to the training time as input features. Thus, the prediction also gets better as time progresses.\n\nThe proxy reward is then calculated for each training example using both observed and predicted feedback. These training examples are used to update the bandit policy.\n\nBut aren\u2019t we still only relying on observed feedback in the proxy reward function? Yes, because delayed feedback is predicted based on observed feedback. However, it is simpler to reason about rewards using all feedback directly. For instance, the delayed thumbs-up prediction model may be a complex neural network that takes into account all observed feedback (e.g., short-term play patterns). It\u2019s more straightforward to define the proxy reward as a simple function of the thumbs-up feedback rather than a complex function of short-term interaction patterns. It can also be used to adjust for potential biases in how feedback is provided.\n\nThe reward engineering diagram is updated with an optional delayed feedback prediction step.\n\nTwo types of ML models\n\nIt\u2019s worth noting that this approach employs two types of ML models:\n\nDelayed Feedback Prediction Models : These models predict p(final feedback | observed feedbacks). The predictions are used to define and compute proxy rewards for bandit policy training examples. As a result, these models are used offline during the bandit policy training.\n\n: These models predict p(final feedback | observed feedbacks). The predictions are used to define and compute proxy rewards for bandit policy training examples. As a result, these models are used offline during the bandit policy training. Bandit Policy Models: These models are used in the bandit policy \u03c0(item | user; r) to generate recommendations online and in real-time.\n\nChallenge: Online-Offline Metric Disparity\n\nImproved input features or neural network architectures often lead to better offline model metrics (e.g., AUC for classification models). However, when these improved models are subjected to A/B testing, we often observe flat or even negative online metrics, which can quantify long-term member satisfaction.\n\nThis online-offline metric disparity usually occurs when the proxy reward used in the recommendation policy is not fully aligned with long-term member satisfaction. In such cases, a model may achieve higher proxy rewards (offline metrics) but result in worse long-term member satisfaction (online metrics).\n\nNevertheless, the model improvement is genuine. One approach to resolve this is to further refine the proxy reward definition to align better with the improved model. When this tuning results in positive online metrics, the model improvement can be effectively productized. See [1] for more discussions on this challenge.\n\nSummary and Open Questions\n\nIn this post, we provided an overview of our reward engineering efforts to align Netflix recommendations with long-term member satisfaction. While retention remains our north star, it is not easy to optimize directly. Therefore, our efforts focus on defining a proxy reward that is aligned with long-term satisfaction and sensitive to individual recommendations. Finally, we discussed the unique challenge of delayed user feedback at Netflix and proposed an approach that has proven effective for us. Refer to [2] for an earlier overview of the reward innovation efforts at Netflix.\n\nAs we continue to improve our recommendations, several open questions remain:\n\nCan we learn a good proxy reward function automatically by correlating behavior with retention?\n\nHow long should we wait for delayed feedback before using its predicted value in policy training?\n\nHow can we leverage Reinforcement Learning to further align the policy with long-term satisfaction?\n\nReferences\n\n[1] Deep learning for recommender systems: A Netflix case study. AI Magazine 2021. Harald Steck, Linas Baltrunas, Ehtsham Elahi, Dawen Liang, Yves Raimond, Justin Basilico.\n\n[2] Reward innovation for long-term member satisfaction. RecSys 2023. Gary Tang, Jiangwei Pan, Henry Wang, Justin Basilico.", "label": 0}
{"title": "Feliz D\u00eda de Los Muertos", "url": "https://lifeofpablo.com/blog/feliz-dia-de-los-muertos", "content": "Celebration\n\nToday, I celebrate D\u00eda de Los Muertos. It's a special holiday to me. One way of celebrating is by doing face makup to recreate a skull. I celebrate this week by attending events, going to the Mexican bakery to get pan de muerto and setting up an altar at my place of living. This is just a few things I do.\n\nAn Important Mexican Holiday\n\nD\u00eda de Los Muertos is an important holiday in Mexico and for Mexicans around the world. We celebrate our loved ones who have passed on. On this holiday, we reconnect with them. We honor the dead by setting up ofrendas and visiting them at their grave site. We invite them into our homes and go out to meet them, respectively. We're not afraid to talk about the dead.\n\nWhy It's Important to Me\n\nAs I'm getting older, I've come to the realization that more people in my life will slowly go as time goes on. It's a fact of life. I want to honor those who came before me and those who were there in my life. It's also important to carry tradtions from my parents to my future children or family. It either stops with me or I continue the tradition. The holiday helps me be in touch more with myself and my cultural background.", "label": 1}
{"title": "How to debug code with GitHub Copilot", "url": "https://github.blog/ai-and-ml/github-copilot/how-to-debug-code-with-github-copilot/", "content": "Debugging is an essential part of a developer\u2019s workflow\u2014but it\u2019s also one of the most time consuming. What if AI could streamline the process, helping you analyze, fix, and document code faster? Enter GitHub Copilot, your AI-powered coding assistant.\n\nGitHub Copilot isn\u2019t just for writing code\u2014it\u2019s also a powerful tool for debugging. Whether you\u2019re troubleshooting in your IDE, using Copilot Chat\u2019s slash commands like /fix , or reviewing pull requests (PR) on github.com, GitHub Copilot offers flexible, intelligent solutions to speed up your debugging process. And with the free version of GitHub Copilot, available to all personal GitHub accounts, you can start exploring these features today.\n\nIn this guide, we\u2019ll explore how to debug code with GitHub Copilot, where to use it in your workflow, and best practices to get the most out of its capabilities. Whether you\u2019re new to GitHub Copilot or looking to deepen your skills, this guide has something for you.\n\nStart using GitHub Copilot \ud83c\udf1f GitHub Copilot Free includes 2,000 code completions, 50 Copilot Chat messages per month, multi-file edits, and model options like GPT-4o or Claude 3.5 Sonnet, with native support in VS Code and on GitHub.\n\nDebugging code with GitHub Copilot: surfaces and workflows\n\nDebugging code with GitHub Copilot can help you tackle issues faster while enhancing your understanding of the codebase. Whether you\u2019re fixing syntax errors, refactoring inefficient code, or troubleshooting unexpected behavior, GitHub Copilot can provide valuable insights in your debugging journey.\n\nSo, how exactly does this work? \u201cGitHub Copilot is recognizing patterns and suggesting solutions based on what it has learned,\u201d says Christopher Harrison, Senior Developer Advocate. \u201cOnce you\u2019ve identified the problem area, you can turn to GitHub Copilot and ask, \u2018I\u2019m giving this input but getting this output\u2014what\u2019s wrong?\u2019 That\u2019s where GitHub Copilot really shines.\u201d\n\nLet\u2019s explore how GitHub Copilot can help you debug your code across different surfaces, from your IDE to github.com and even pull requests.\n\n1. In Copilot Chat\n\nCopilot Chat acts as an interactive AI assistant, helping you debug issues with natural language queries. And with Copilot Free, you get 50 chat messages per month. With Copilot Chat, you can:\n\nGet real-time explanations: Ask \u201cWhy is this function throwing an error?\u201d and Copilot Chat will analyze the code and provide insights.\n\nAsk \u201cWhy is this function throwing an error?\u201d and Copilot Chat will analyze the code and provide insights. Use slash commands for debugging: Try /fix to generate a potential solution or /explain for a step-by-step breakdown of a complex function. (More on this later!)\n\nTry to generate a potential solution or for a step-by-step breakdown of a complex function. (More on this later!) Refactor code for efficiency: If your implementation is messy or inefficient, Copilot Chat can suggest cleaner alternatives. Christopher explains, \u201cRefactoring improves the readability of code, making it easier for both developers and GitHub Copilot to understand. And if code is easier to understand, it\u2019s easier to debug and spot problems.\u201d\n\nIf your implementation is messy or inefficient, Copilot Chat can suggest cleaner alternatives. Christopher explains, \u201cRefactoring improves the readability of code, making it easier for both developers and GitHub Copilot to understand. And if code is easier to understand, it\u2019s easier to debug and spot problems.\u201d Walk through errors interactively: Describe your issue in chat and get tailored guidance without ever having to leave your IDE.\n\n\ud83d\udd0e How to find GitHub Copilot Chat Look for the GitHub Copilot icon or prompt to start a conversation in your IDE, on github.com, or within pull requests. Simply click on the icon to get started with code suggestions, unit tests, suggested code fixes, and more!\n\n2. In your IDE\n\nWhen working in popular IDEs like VS Code or JetBrains, GitHub Copilot offers real-time suggestions as you type. It helps by:\n\nFlagging issues: For example, if you declare a variable but forget to initialize it, GitHub Copilot can suggest a correction.\n\nFor example, if you declare a variable but forget to initialize it, GitHub Copilot can suggest a correction. Code fixes: Encounter a syntax error? GitHub Copilot can suggest a fix in seconds, ensuring your code stays error-free.\n\nEncounter a syntax error? GitHub Copilot can suggest a fix in seconds, ensuring your code stays error-free. Contextual assistance: By analyzing your workspace, GitHub Copilot provides solutions tailored to your codebase and project structure.\n\n\ud83d\udd0e How to find GitHub Copilot in VS Code To open up the chat view, head over to the VS Code title bar and select, \u201cUse AI features with Copilot for Free.\u201d\n\nSign in with your GitHub account by clicking on \u201cSign in to use Copilot for Free.\u201d\n\nDon\u2019t have a GitHub Copilot subscription yet? Then follow the browser steps to sign up for your Copilot Free plan. Learn how to install GitHub Copilot in JetBrains, Azure Data Studio, and more >\n\n3. On github.com\n\nGitHub Copilot extends beyond your IDE, offering debugging assistance directly on github.com via Copilot Chat, particularly in repositories and discussions. With this feature, you can:\n\nTroubleshoot code in repositories: Open a file, highlight a problematic section, and use Copilot Chat to analyze it.\n\nOpen a file, highlight a problematic section, and use Copilot Chat to analyze it. Generate test cases: If you\u2019re unsure how to verify a function, GitHub Copilot can suggest test cases based on existing code.\n\nIf you\u2019re unsure how to verify a function, GitHub Copilot can suggest test cases based on existing code. Understand unfamiliar code: Reviewing an open-source project or a teammate\u2019s PR? Ask GitHub Copilot to summarize a function or explain its logic.\n\n\ud83d\udd0e How to find GitHub Copilot on github.com\n\n4. For pull request assistance\n\nGitHub Copilot can also streamline debugging within PRs, ensuring code quality before merging.\n\nSuggest improvements in PR comments: GitHub Copilot can review PRs and propose fixes directly in the conversation.\n\nGitHub Copilot can review PRs and propose fixes directly in the conversation. Generate PR summaries: Struggling to describe your changes? Greg Larkin, Senior Service Delivery Engineer, says, \u201cI use GitHub Copilot in the PR creation process to generate a summary of the changes in my feature branch compared to the branch I\u2019m merging into. That can be really helpful when I\u2019m struggling to figure out a good description, so that other people understand what I did.\u201d\n\nStruggling to describe your changes? Greg Larkin, Senior Service Delivery Engineer, says, \u201cI use GitHub Copilot in the PR creation process to generate a summary of the changes in my feature branch compared to the branch I\u2019m merging into. That can be really helpful when I\u2019m struggling to figure out a good description, so that other people understand what I did.\u201d Explain diffs: Not sure why a change was made? Ask GitHub Copilot to summarize what\u2019s different between commits.\n\nNot sure why a change was made? Ask GitHub Copilot to summarize what\u2019s different between commits. Catch edge cases before merging: Use /analyze to identify potential issues and /tests to generate missing test cases.\n\nUse to identify potential issues and to generate missing test cases. Refactor on the fly: If a PR contains redundant or inefficient code, GitHub Copilot can suggest optimized alternatives.\n\nBy integrating Copilot into your PR workflow, you can speed up code reviews while maintaining high-quality standards. Just be sure to pair it with peer expertise for the best results.\n\n\ud83d\udd0e How to find GitHub Copilot in pull requests\n\n5 slash commands in GitHub Copilot for debugging code\n\nSlash commands turn GitHub Copilot into an on-demand debugging assistant, helping you solve issues faster, get more insights, and improve your code quality. Here are some of the most useful slash commands for debugging:\n\n1. Use /help to get guidance on using GitHub Copilot effectively\n\nThe /help slash command provides guidance on how to interact with GitHub Copilot effectively, offering tips on structuring prompts, using slash commands, and maximizing GitHub Copilot\u2019s capabilities.\n\nHow it works : Type /help in Copilot Chat to receive suggestions on your current task, whether it\u2019s debugging, explaining code, or generating test cases.\n\n: Type in Copilot Chat to receive suggestions on your current task, whether it\u2019s debugging, explaining code, or generating test cases. Example: Need a refresher on what GitHub Copilot can do? Use /help to access a quick guide to slash commands like /fix and /explain .\n\n2. Use /fix to suggest and apply fixes\n\nThe /fix command is a go-to tool for resolving code issues by allowing you to highlight a block of problematic code or describe an error.\n\nHow it works: Select the code causing issues, type /fix , and let Copilot Chat generate suggestions.\n\nSelect the code causing issues, type , and let Copilot Chat generate suggestions. Example: If you have a broken API call, use /fix to get a corrected version with appropriate headers or parameters.\n\n3. Use /explain to understand code and errors\n\nThe /explain command breaks down complex code or cryptic error messages into simpler, more digestible terms.\n\nHow it works: Highlight the code or error message you want clarified, type /explain , and Copilot Chat will provide an explanation. It will explain the function\u2019s purpose, how it processes the data, potential edge cases, and any possible bugs or issues.\n\nHighlight the code or error message you want clarified, type , and Copilot Chat will provide an explanation. It will explain the function\u2019s purpose, how it processes the data, potential edge cases, and any possible bugs or issues. Example: Encounter a \u201cNullPointerException\u201d? Use /explain to understand why it occurred and how to prevent it.\n\n4. Use /tests to generate tests\n\nTesting is key to identifying bugs, and the /tests command helps by generating test cases based on your code.\n\nHow it works: Use /tests on a function or snippet, and Copilot Chat will generate relevant test cases.\n\nUse on a function or snippet, and Copilot Chat will generate relevant test cases. Example: Apply /tests to a sorting function, and Copilot Chat might generate unit tests for edge cases like empty arrays or null inputs.\n\n5. Use /doc to generate or improve documentation\n\nThere are long-term benefits to having good text documentation\u2014for developers and GitHub Copilot, which can draw context from it\u2014because it makes your codebase that much more searchable. By using the /doc command with Copilot Free, you can even ask GitHub Copilot to write a summary of specific code blocks within your IDE.\n\nThe /doc command helps you create or refine documentation for your code, which is critical when debugging or collaborating with others. Clear documentation provides context for troubleshooting, speeds up issue resolution, and helps fellow developers understand your code faster.\n\nHow it works: Highlight a function, class, or file, type /doc and right-click to see the context menu, and Copilot Chat will generate comprehensive comments or documentation.\n\nHighlight a function, class, or file, type and right-click to see the context menu, and Copilot Chat will generate comprehensive comments or documentation. Example: Apply /doc to a function, and Copilot Chat will generate inline comments detailing its purpose, parameters, and expected output.\n\nBy mastering these commands, you can streamline your debugging workflow and resolve issues faster without switching between tools or wasting time on manual tasks.\n\nUsing shortcut keys: Quickly activate GitHub Copilot\u2019s debugging features in VS Code Start/continue debugging: F5\n\nStop debugging: Shift + F5\n\nStep over: F10\n\nStep into: F11\n\nStep out: Shift + F11\n\nToggle breakpoint: F9\n\nBest practices for debugging code with GitHub Copilot\n\nProvide clear context for better results\n\nProviding the right context helps GitHub Copilot generate even more relevant debugging suggestions. As Christopher explains, \u201cThe better that Copilot is able to understand what you\u2019re trying to do and how you\u2019re trying to do it, the better the responses are that it\u2019s able to give to you.\u201d\n\nSince GitHub Copilot analyzes your code within the surrounding scope, ensure your files are well structured and that relevant dependencies are included. If you\u2019re using Copilot Chat, reference specific functions, error messages, or logs to get precise answers instead of generic suggestions.\n\n\ud83d\udca1 Pro tip: Working across multiple files? Use the @workspace command to point GitHub Copilot in the right direction and give it more context for your prompt and intended goal.\n\nAsk, refine, and optimize in real time\n\nInstead of treating GitHub Copilot as a one-and-done solution, refine its suggestions by engaging in a back-and-forth process. Greg says, \u201cI find it useful to ask GitHub Copilot for three or four different options on how to fix a problem or to analyze for performance. The more detail you provide about what you\u2019re after\u2014whether it\u2019s speed, memory efficiency, or another constraint\u2014the better the result.\u201d\n\nThis iterative approach can help you explore alternative solutions you might not have considered, leading to more robust and efficient code.\n\nMaster the art of specific prompts\n\nThe more specific your prompt, the better GitHub Copilot\u2019s response. Instead of asking \u201cWhat\u2019s wrong with this function?\u201d try \u201cWhy is this function returning undefined when the input is valid?\u201d GitHub Copilot performs best when given clear, detailed queries\u2014this applies whether you\u2019re requesting a fix, asking for an explanation, or looking for test cases to verify your changes.\n\nBy crafting precise prompts and testing edge cases, you can use GitHub Copilot to surface potential issues before they become production problems.\n\nTry a structured approach with progressive debugging\n\nNext, try a step-by-step approach to your debugging process! Instead of immediately applying fixes, use GitHub Copilot\u2019s commands to first understand the issue, analyze potential causes, and then implement a solution. This structured workflow\u2014known as progressive debugging\u2014helps you gain deeper insights into your code while ensuring that fixes align with the root cause of the problem.\n\nFor example:\n\nStart with the slash command /explain on a problematic function to understand the issue. Use the slash command /startDebugging to help with configuring interactive debugging. Finally, apply the slash command /fix to generate possible corrections.\n\n\ud83d\udccc Use case: If a function in your React app isn\u2019t rendering as expected, start by running /explain on the relevant JSX or state logic, then use /debug to identify mismanaged props, and finally, apply /fix for a corrected implementation.\n\nCombine commands for a smarter workflow\n\nSome issues require multiple levels of debugging and refinement. By combining commands, you can move from diagnosis to resolution even faster.\n\nFor example:\n\nUse /explain + /fix to understand and resolve issues quickly.\n\nto understand and resolve issues quickly. Apply /fixTestFailure + /tests to find failing tests and generate new ones.\n\n\ud83d\udccc Use case:\n\nFixing a broken function: Run the slash command /explain to understand why it fails, then use the slash command /fix to generate a corrected version.\n\nRun the slash command to understand why it fails, then use the slash command to generate a corrected version. Improving test coverage: Use the slash command /fixTestFailure to identify and fix failing tests, then use the slash command /tests to generate additional unit tests for the highlighted code.\n\nRemember, slash commands are most effective when they\u2019re used in the appropriate context, combined with clear descriptions of the problem, are part of a systematic debugging approach, and followed up with verification and testing.\n\nGitHub Copilot is a powerful tool that enhances your workflow, but it doesn\u2019t replace the need for human insight, critical thinking, and collaboration. As Greg points out, \u201cGitHub Copilot can essentially act as another reviewer, analyzing changes and providing comments. Even so, it doesn\u2019t replace human oversight. Having multiple perspectives on your code is crucial, as different reviewers will spot issues that others might miss.\u201d\n\nBy combining GitHub Copilot\u2019s suggestions with human expertise and rigorous testing, you can debug more efficiently while maintaining high-quality, reliable code.\n\nReady to try the free version of GitHub Copilot?\n\nStart using GitHub Copilot today >\n\nYou can keep the learning going with these resources:\n\n* Debug your app with GitHub Copilot in Visual Studio\n\n* Example prompts for GitHub Copilot Chat\n\n* GitHub Copilot and VS Code tutorials", "label": 0}
{"title": "Simplified Dataflow Connectors with Managed I/O", "url": "https://developers.googleblog.com/en/simplified-dataflow-connectors-with-managed-io/", "content": "Google Cloud Dataflow offers a fully managed data processing system for running Apache Beam pipelines on Google Cloud in a highly scalable manner. Due to being a fully managed service, Dataflow users do not have to worry about any service side regressions and versioning. The promise is that you only concern yourself with your pipeline logic while Google takes care of the service infrastructure. While this is certainly true, Apache Beam itself is a very full featured SDK that provides many simple to highly complex transforms for you to use in their pipelines. For example, Apache Beam provides a number of I/O connectors. Many of these connectors are Apache Beam composite transforms from 10s to 100s of steps. Historically, these have been considered \"user code\" from the service's perspective, despite being not authored or maintained by the user. There are several common complications customers run into complex Beam transforms such as I/O connectors. You are on the hook for upgrading Beam to adopt any fixes and improvements to connectors. Connector APIs vary widely and moving from one connector to another usually requires a lot of exploration and learning. While connectors offer a complete API, the API might not be optimized for the Dataflow runner. To alleviate all three of these issues, Dataflow recently introduced a new offering named Managed I/O. With Managed I/O the service itself is able to manage these complexities on your behalf. Hence you can truly focus on their pipelines business logic instead of focussing on the minutiae related to using and configuring a specific connector to suit their needs. Below we detail how each of the above mentioned complexities are addressed via Managed I/O.\n\nAutomatic SDK upgrades Apache Beam is a fully fledged SDK with many transforms, features, and optimization. Like many large pieces of software, upgrading Beam to a new version can be a significant process. Usually upgrading Beam involves upgrading all parts of a pipeline including all I/O connectors. But sometimes, you just need to obtain access to a critical bug fix or an improvement available in the latest version of one or more I/O connectors used in your pipeline. Managed I/O with Dataflow simplifies this by completely taking over the management of the Beam I/O connector version. With Managed I/O, Dataflow will make sure that I/O connectors used by pipelines are always up to date. Dataflow performs this by always upgrading I/O connectors to the latest vetted version during job submission and streaming update via replacement. For example, assume that you use a Beam pipeline that uses Beam 2.x.0 and assume that you use the Managed Apache Iceberg I/O source in your pipeline. Also, assume that the latest vetted version of the Iceberg I/O source supported by Dataflow is 2.y.0. During job submission, Dataflow will replace this specific connector with version 2.y.0 and will keep the rest of the Beam pipeline including any standard (non-managed) I/O connectors at version 2.x.0.\n\nAfter replacement, Dataflow optimizes the updated pipeline and executes it in GCE. To achieve isolation between connectors from different Beam versions, Dataflow deploys an additional Beam SDK container in GCE VMs. So in this case, Beam SDK containers from both versions 2.x.0 and 2.y.0 will be running in each GCE VM used by the Dataflow job. So with Managed I/O you can be assured that I/O connectors used in your pipeline are always up to date. This allows you to focus on improving the business logic of your pipeline without worrying about upgrading the Beam version to simply obtain I/O connector updates.\n\nSimplified IO API APIs differences across Beam I/O connectors vary greatly. This means that, whenever you try to use a new Beam I/O connector, you would have to learn an API specific to that connector. Some of the APIs can be quite large and non-intuitive. This can be due to: Support for various and in some cases redundant features offered by the underlying system. Maintaining backwards compatibility for legacy (or archaic) features or defaults. Support for customizing the I/O connector to support edge cases and implementation details that may only apply to few customers. Above points result in very large API surfaces for some connectors that are not intuitive for a new customer to use efficiently.\n\nManaged I/O offers standardized Java and Python APIs for supported I/O connectors. For example, with Beam Java SDK an I/O connector source can be instantiated in the following standardized form.\n\nManaged.read(SOURCE).withConfig(sourceConfig) Java Copied\n\nAn I/O connector sink can be instantiated in the following form.\n\nManaged.write(SINK).withConfig(sinkConfig) Java Copied\n\nHere SOURCE and SINK are keys specifically identifying the connector while sourceConfig and sinkConfig are maps of configurations used to instantiate the connector source or sink. The map of configurations may also be provided as YAML files available locally or in Google Cloud Storage. Please see the Managed I/O website for more complete examples for supported sources and sinks. Beam Python SDK offers a similarly simplified API. This means that various Beam I/O connectors with different APIs can be instantiated in a very standard way. For example,\n\n// Create a Java BigQuery I/O source Map<String, Object> bqReadConfig = ImmutableMap.of(\"query\", \"<query>\", ...); Managed.read(Managed.BIGQUERY).withConfig(bqReadConfig) // Create a Java Kafka I/O source. Map<String, Object> kafkaReadConfig = ImmutableMap.of(\"bootstrap_servers\", \"<server>\", \"topic\", \"<topic>\", ...); Managed.read(Managed.KAFKA).withConfig(kafkaReadConfig) // Create a Java Kafka I/O source but with a YAML based config available in Google Cloud Storage. String kafkaReadYAMLConfig = \"gs://path/to/config.yaml\" Managed.read(Managed.KAFKA).withConfigUrl(kafkaReadYAMLConfig) // Create a Python Iceberg I/O source. iceberg_config = {\"table\": \"<table>\", ...} managed.Read(managed.ICEBERG, config=iceberg_config) Java Copied\n\nAutomatically optimized for Dataflow Many Beam connectors offer a comprehensive API for configuring and optimizing the connector to suit a given pipeline and a given Beam runner. One downside of this is that if you specifically want to run on Dataflow, you may have to learn the specific configurations that best suit Dataflow and apply them when setting up your pipeline. Connector related documentation can be long and detailed and specific changes needed might not be intuitive. This might result in connectors used in Dataflow pipelines performing in a sub-optimal way. Manage I/O connectors alleviates this by automatically re-configuring the connectors to incorporate best practices and configure them to best suit Dataflow. Such re-configuration may occur during job submission or streaming update via replacement. For example, Dataflow streaming pipelines offer two modes, exactly-once and at-least-once while BigQuery I/O sink with Storage Write API offer two analogous delivery semantics, exactly-once and at-least-once. BigQuery sink with at-least-once delivery semantics is usually less expensive and results in lower latencies. With standard BigQuery I/O connectors, you are responsible for making sure that you use the appropriate mode when using the BigQuery I/O. With Managed BigQuery I/O sink this is automatically configured for you. Which means that if your streaming pipeline is operating at the at-least-once mode, your Managed I/O BigQuery sink will be automatically configured to use the at-least-once delivery semantics.\n\nReal-world pipelines We ran several pipelines that wrote data using the Managed Iceberg I/O sink backed by a Hadoop catalog deployed in GCS (please see here for the other supported catalogs). Pipelines were submitted using Beam 2.61.0 and the Managed I/O sink was automatically upgraded by Dataflow to the latest supported version. All benchmarks used n1-standard-4 VMs and the number of VMs used by the pipeline was fixed to 100. Please note that execution time here does not include the startup and shutdown time.\n\nAs the benchmarks show, Managed Iceberg I/O scaled up nicely and both metrics grew linearly with the data size. We also ran a streaming pipeline that read from Google Pub/Sub and used the Managed I/O Kafka sink to push messages to a Kafka cluster hosted in GCP. The pipeline used Beam 2.61.0 and Dataflow upgraded the Managed Kafka sink to the latest supported version. During the steady state, the pipeline used 10 n1-standard-4 VMs (max 20 VMs). The pipeline was consistently processing messages at a throughput of 250k msgs/sec across all steps and was run for 2 hours.\n\nThe following graph shows the data throughputs of various steps of the pipeline. Note that throughputs are different here since the element size changes between steps. The pipeline read from Pub/Sub at a rate of 75 MiB/sec (red line) and wrote to Kafka at a rate of 40 MiB/sec (green line).\n\nBoth latency and backlog was low for the duration of the pipeline execution.\n\nThe pipeline used VM CPU and memory efficiently.", "label": 0}
{"title": "Introducing sub-issues: Enhancing issue management on GitHub", "url": "https://github.blog/engineering/architecture-optimization/introducing-sub-issues-enhancing-issue-management-on-github/", "content": "Recently we launched sub-issues, a feature designed to tackle complex issue management scenarios. This blog post delves into the journey of building sub-issues, what we learned along the way, how we implemented sub-issues, and the benefits of being able to use sub-issues to build itself.\n\nWhat are sub-issues?\n\nSub-issues are a way to break a larger issue into smaller, more manageable tasks. With this feature, you can now create hierarchical lists within a single issue, making it easier to track progress and dependencies. By providing a clear structure, sub-issues help teams stay organized and focused on their goals.\n\nFor example, I often realize that a batch of work requires multiple steps, like implementing code in different repositories. Breaking this task into discrete sub-issues makes it easier to track progress and more clearly define the work I need to do. In practice we\u2019ve noticed this helps keep linked PRs more concise and easier to review.\n\nA brief history\n\nIssues have long been at the heart of project management on GitHub. From tracking bugs to planning feature development, issues provide a flexible and collaborative way for teams to organize their work. Over time, we\u2019ve enriched this foundation with tools like labels, milestones, and task lists, all to make project management even more intuitive and powerful.\n\nOne of the key challenges we set out to solve was how to better represent and manage hierarchical tasks within issues. As projects grow in complexity, breaking down work into smaller, actionable steps becomes essential. We want to empower users to seamlessly manage these nested relationships while maintaining the simplicity and clarity GitHub is known for.\n\nOur journey toward sub-issues began with a fundamental goal: to create a system that integrates deeply into the GitHub Issues experience, enabling users to visually and functionally organize their work without adding unnecessary complexity. Achieving this required careful design and technical innovation.\n\nBuilding sub-issues\n\nTo build sub-issues, we began by designing a new hierarchical structure for tasks rather than modifying the existing task list functionality. We introduced the ability to nest tasks within tasks, creating a hierarchical structure. This required updates to our data models and rendering logic to support nested sub-issues.\n\nFrom a data modeling perspective, the sub-issues table stores the relationships between parent and child issues. For example, if Issue X is a parent of Issue Y, the sub-issues table would store this link, ensuring the hierarchical relationship is maintained.\n\nIn addition, we roll up sub-issue completion information into a sub-issue list table. This allows us to performantly get progress without having to traverse through a list of sub-issues. For instance, when Issue Y is completed, the system automatically updates the progress of Issue X, eliminating the need to manually check the status of all sub-issues.\n\nWe wanted a straightforward representation of sub-issues as relationships in MySQL. This approach provided several benefits, including easier support for sub-issues in environments like GitHub Enterprise Server and GitHub Enterprise Cloud with data residency.\n\nWe exposed sub-issues through GraphQL endpoints, which let us build upon the new Issues experience and leverage newly crafted list-view components. This approach provided some benefits, including more efficient data fetching and enhanced flexibility in how issue data is queried and displayed. Overall, we could move faster because we reused existing components and leveraged new components that would be used in multiple features. This was all made possible by building sub-issues in the React ecosystem.\n\nWe also focused on providing intuitive controls for creating, editing, and managing sub-issues. To this end, we worked closely with accessibility designers and GitHub\u2019s shared components team that built the list view that powers sub-issues.\n\nOur goal was to make it as easy as possible for users to break down their tasks without disrupting their workflow.\n\nUsing sub-issues in practice\n\nDogfooding is a best practice at GitHub and it\u2019s how we build GitHub! We used sub-issues extensively within our own teams throughout the company to manage complex projects and track progress. Having a discrete area to manage our issue hierarchy resulted in a simpler, more performant experience. Through this hands-on experience, we identified areas for improvement and ensured that the feature met our high standards.\n\nOur teams found that sub-Issues significantly improved their ability to manage large projects. By breaking down tasks into smaller, actionable items, they maintained better visibility and control over their work. The hierarchical structure also made it easier to identify dependencies and ensure nothing fell through the cracks.\n\nGathering early feedback\n\nBuilding sub-issues was a team effort. Feedback from our beta testers was instrumental in shaping the final product and ensuring it met the needs of our community. For example, understanding how much metadata to display in the sub-issue list was crucial. We initially started with only issue titles, but eventually added the issue number and repository name, if the issue was from another repository.\n\nBuilding features at GitHub makes it really easy to improve our own features as we go. It was really cool to start breaking down the sub-issues work using sub-issues. This allowed us to experience the feature firsthand and identify any pain points or areas for improvement. For example, the has:sub-issues-progress and has:parent-issue filters evolved from early discussions around filtering syntax. This hands-on approach ensured that we delivered a polished and user-friendly product.\n\nThese lessons have been invaluable in not only improving sub-issues, but also in shaping our approach to future feature development. By involving users early and actively using our own features, we can continue to build products that truly meet the needs of our community. These practices will be important to our development process going forward, ensuring that we deliver high-quality, user-centric solutions.\n\nCall to action\n\nSub-issues are designed to help you break down complex tasks into manageable pieces, providing clarity and structure to your workflows. Whether you\u2019re tracking dependencies, managing progress, or organizing cross-repository work, sub-issues offer a powerful way to stay on top of your projects.\n\nWe\u2019d love for you to try sub-issues and see how they can improve your workflow. Your feedback is invaluable in helping us refine and enhance this feature. Join the conversation in our community discussion to share your thoughts, experiences, and suggestions.\n\nThank you for being an integral part of the GitHub community. Together, we\u2019re shaping the future of collaborative development!\n\nTags:", "label": 0}
{"title": "LiteRT: Maximum performance, simplified", "url": "https://developers.googleblog.com/en/litert-maximum-performance-simplified/", "content": "Over the past decade, mobile phones have incorporated increasingly powerful purpose-specific accelerators including GPUs and recently, more powerful NPUs (Neural Processing Units). By accelerating your AI models on mobile GPUs and NPUs, you can speed up your models by up to 25x compared to CPU while also reducing power consumption by up to 5x. However, unlocking these outstanding performance benefits has proven challenging for most developers, as it requires wrangling HW-specific APIs in case of GPU inference or wrangling vendor-specific SDKs, formats, and runtimes for NPU inference. Listening to your feedback, the Google AI Edge team is excited to announce multiple improvements to LiteRT solving the challenges above, and accelerating AI on mobile more easily with increased performance. Our new release includes a new LiteRT API making on-device ML inference easier than ever, our latest cutting-edge GPU acceleration, new NPU support co-developed with MediaTek and Qualcomm (open for early access), and advanced inference features to maximize performance for on-device applications. Let\u2019s dive in!\n\nMLDrift: Best GPU Acceleration Yet GPUs have always been at the heart of LiteRT\u2019s acceleration story, providing the broadest support and most consistent performance improvement. MLDrift, our latest version of GPU acceleration, pushes the bar even further with faster performance and improvements to support models of a significantly larger size through: Smarter Data Organization: MLDrift arranges data in a more efficient way by using optimized tensor layouts and storage types specifically tailored for how GPUs process data, reducing memory access time and speeding up AI calculations. Workgroup Optimization: Smart computation based on context (stage) and resource constraints Improved Data Handling: Streamlining the way the accelerator receives and sends out tensor data to reduce overhead in data transfer and conversion optimizing for data locality.\n\nThis results in significantly faster performance than CPUs, than previous versions of our TFLite GPU delegate, and even other GPU enabled frameworks particularly for CNN and Transformer models.\n\nFigure: Inference latency per model of LiteRT GPU compared to TFLite GPU, measured on Samsung 24.\n\nFind examples in our documentation and give GPU acceleration a try today.\n\nMediaTek and Qualcomm NPU Support NPUs, AI specific accelerators, are becoming increasingly common in flagship phones. They allow you to run AI models much more efficiently, and in many cases much faster. In our internal testing compared to CPUs this acceleration can be up to 25x faster, and 5x more power efficient. (May 2025, based on internal testing) Typically, each vendor provides their own SDKs, including compilers, runtime, and other dependencies, to compile and execute models on their SoCs. The SDK must precisely match the specific SoC version and requires proper download and installation. LiteRT now provides a uniform way to develop and deploy models on NPUs, abstracting away all these complexities. Vendor compiler distribution: When installing the LiteRT PyPI package, we will automatically download the vendor SDKs for compiling models. Model and vendor runtime distribution: The compiled model and SoC runtime will need to be distributed with the app. As a developer you can handle this distribution yourself, or you can have Google Play distribute them for you. In our example code you can see how to use AI Packs and Feature Delivery to deliver the right model and runtime to the right device.\n\nWe\u2019re excited to partner with MediaTek and Qualcomm to allow developers to accelerate a wide variety of classic ML models, such as vision, audio, and NLP models, on MediaTek and Qualcomm NPUs. Increased model and domain support will continue over the coming year. This feature is available in private preview. For early access apply here.\n\nSimplified GPU and NPU Hardware Acceleration We\u2019ve made GPUs and NPUs easier than ever to use by simplifying the process in the latest version of the LiteRT APIs. With the latest changes, we have simplified the setup significantly with the ability to specify the target backend as an option. As an example, this is how a developer would specify GPU acceleration:\n\n// 1. Load model. auto model = *Model::Load(\"mymodel.tflite\"); // 2. Create a compiled model targeting GPU. auto compiled_model = *CompiledModel::Create(model, kLiteRtHwAcceleratorGpu); C++ Copied\n\nAs you can see, the new CompiledModel API greatly simplifies how to specify the model and target backend(s) for acceleration.\n\nAdvanced Inference for Performance Optimization While using high performance backends is helpful, optimal performance of your application can be hindered by memory, or processor bottlenecks. With the new LiteRT APIs, you can address these challenges by leveraging built-in buffer interoperability to eliminate costly memory copy operations, and asynchronous execution to utilize idle processors in parallel.\n\nSeamless Buffer Interoperability The new TensorBuffer API provides an efficient way to handle input/output data with LiteRT. It allows you to directly use data residing in hardware memory, such as OpenGL Buffers, as inputs or outputs for your CompiledModel, completely eliminating the need for costly CPU copies.\n\nauto tensor_buffer = *litert::TensorBuffer::CreateFromGlBuffer(tensor_type, opengl_buffer); C++ Copied\n\nThis significantly reduces unnecessary CPU overhead and boosts performance.\n\n\n\nAdditionally, the TensorBuffer API enables seamless copy-free conversions between different hardware memory types when supported by the system. Imagine effortlessly transforming data from an OpenGL Buffer to an OpenCL Buffer or even to an Android HardwareBuffer without any intermediate CPU transfers. This technique is key to handling the increasing data volumes and demanding performance required by increasingly complex AI models. You can find examples in our documentation on how to use TensorBuffer.\n\nAsynchronous Execution Asynchronous execution allows different parts of the AI model or independent tasks to run concurrently across CPU, GPU, and NPUs allowing you to opportunistically leverage available compute cycles from different processors to improve efficiency and responsiveness. For instance: the CPU might handle data preprocessing the GPU could accelerate matrix multiplications in a neural network layer, and the NPU might efficiently manage specific inference tasks \u2013 all happening in parallel.\n\nIn applications which require real-time AI interactions, a task can be initiated on one processor and continue with other operations on another. Parallel processing minimizes latency and provides a smoother, more interactive user experience. By effectively managing and overlapping computations across multiple processors, asynchronous execution maximizes system throughput and ensures that the AI application remains fluid and reactive, even under heavy computational loads. Async execution is implemented by using OS-level mechanisms (e.g., sync fences on Android/Linux) allowing one HW accelerator to trigger upon the completion of another HW accelerator directly without involving the CPU. This reduces latency (up to 2x in our GPU async demo) and power consumption while making the pipeline more deterministic. Here is the code snippet showing async inference with OpenGL buffer input:\n\n// Create an input TensorBuffer based on tensor_type that wraps the given OpenGL // Buffer. env is an LiteRT environment to use existing EGL display and context. auto tensor_buffer_from_opengl = *litert::TensorBuffer::CreateFromGlBuffer(env, tensor_type, opengl_buffer); // Create an input event and attach it to the input buffer. Internally, it // creates and inserts a fence sync object into the current EGL command queue. auto input_event = *Event::CreateManaged(env, LiteRtEventTypeEglSyncFence); tensor_buffer_from_opengl.SetEvent(std::move(input_event)); // Create the input and output TensorBuffers\u2026 // Run async inference compiled_model1.RunAsync(input_buffers, output_buffers); C++ Copied", "label": 0}
{"title": "1000 Albums in 1000 Days", "url": "https://lifeofpablo.com/blog/one-thousand-albums-in-one-thousand-days", "content": "Sunday Morning Adventure\n\nIt was a Sunday and I was simply looking for something to do on an afternoon. Yesterday, I went to the bookstore yesterday in a long time. Going to the bookstore is a fun place to simply browse with no goal. You can simply just look and hope that something catches your eye. Oh boy! did a few things catch my eye! One book I really want to talk about is 1000 Record Covers by Michael Ors. Just as the title describes, it is a book showing different record covers. The book organized by decade (50s, 60s, 70s, 80s and 90s). It's a simple book with an record covers and a information about the record album. I inititally hesitated in purchasing this book. Then, I started thinking about my taste in music.\n\nBreaking The Loop\n\nI've been stuck in this loop of music where I listen to the same stuff over and over and over again. It's been something I've been disatisfied with myself for a while. I purchased this book not just look at all the vintage record covers, I want to expand my taste in music. So I decided to challenge myself to break away from comfortability by listening to music I don't listen to normally or never thought to listen.\n\nI am using this book as a guide to find new music. Streaming services ideally make it easy to discover new music. At the same time it seems overwhelming and distracting to even know what to discover. A book helps provide content without other distractions popping out of every direction. I also want to learn more music history throught listening which will spark my curiosity even more. I want to be able to find more ways to connect myself and see the world differently through music.\n\nThe First Album\n\nToday is Day One and we're starting at the 50's . I am listening to album Let's Go Dancing To Rock And Roll by Hen Gates And His Gaters\n\nTitle Let's Go Dancing to Rock and Roll Artist Hen Gates And His Gaters\n\nSo far, i've enjoyed it. It like the genre so far. We're off to a great start.\n\nSome limitations\n\nNot everything will be on Apple Music as I found out with Let's Go Dancing to Rock and Roll. Youtube is my next solution. That involves me making a playlist since it hasn't been organized by someone else. I won't let this discourage me but it will be challenging at times.\n\nHow long will this take?\n\nI will share my progress in listening and what I have learned so far in future blog posts, at various cyles such as various intervals. Definetly will share at albums number 250, 500, 750 and of course at 1000. According to basic math, it will take me close to 2.75 years to complete this if I do one album a day. This could either go faster or slower depending the circumstances.\n\nBuild a microsite\n\nI will create a web app or a static page that I update daily or every few days on what I am listening to from the book. It will take suggestions using webmentions and other forms of contact.\n\nThe link will come soon in an upate post.\n\nRecomendations\n\nDo any of you have any recommendations for music to listen to? Send them my way, I'd love to see what you all recommend!", "label": 1}
{"title": "Unlock your potential: Discover the enhanced Google Developer Program", "url": "https://developers.googleblog.com/en/google-developer-program-latest-enhancements/", "content": "The Google Developer Program is evolving. We're introducing AI-powered tools and expanded resources designed to help you build faster, smarter and more effectively with Google\u2019s technologies.\n\nIn today\u2019s fast-paced development landscape, having the right toolkit is crucial. We\u2019ve listened to your feedback and are delivering updates that will enable you to focus on what matters most: creating exceptional AI applications.\n\n\n\nUnlock your full potential with Google Developer Program Premium: Now infused with AI\n\nThe Google Developer Program premium membership ($299/year) is an enhanced set of resources designed to provide developers with more advanced capabilities, including access to Google\u2019s latest AI tools. Think of it as your catalyst for growth with advanced tools and dedicated support at every phase of your development journey.\n\nToday, we are adding some new ways you can experience the power of Google\u2019s AI:\n\nEnhanced Coding Assistance with Gemini Code Assist Standard: Access paid Gemini in Firebase features and Gemini Code Assist to instantly boost your coding efficiency. Write high-quality code, faster, and with greater confidence.\n\nIncreased Capacity with Firebase Studio Workspaces: Get more room to build! Project IDX is now part of Firebase Studio. With premium, you get 30 Firebase Studio workspaces, providing greater flexibility to handle complex projects and scale your applications.\n\nExperiment with the Latest Models: Dive into API-driven AI with a $50 GenAI developer credit for Google AI Studio and Google Cloud Vertex AI. Experiment with cutting-edge Gemini, Imagen, and Veo models and integrate powerful AI capabilities into your application.\n\nAccess to Premium Google AI Features: Premium members receive a 3-month free trial of Google One AI Premium. Enjoy Gemini Advanced, NotebookLM Plus, increased storage, and much more.\n\nWe've also improved the premium benefits dashboard. Log in to see all your benefits and easily activate them in one place.", "label": 0}
{"title": "Data Platform Explained Part II", "url": "https://engineering.atspotify.com/2024/5/data-platform-explained-part-ii", "content": "Check out Data Platform Explained Part I, where we started sharing the journey of building a data platform, its building blocks, and the motivation for investing into building a platformized solution at Spotify.\n\nIntroduction\n\nIn Data Platform Explained Part I, we shared the first steps in the journey to build a data platform, the insights that indicate it\u2019s time to start building one, and how we are organized to succeed on it. In this article, we will take one step further into the why, what, and how of our data platform, introduce you to the domains underneath it that are responsible for the platform\u2019s building blocks \u2014 here we will talk about scalability, the tooling we use and provide, alongside the value each building block brings to a data platform \u2014 and finally our strategy to navigate the complexity of a data ecosystem by building a strong community around it.\n\nData Collection\n\nWhen it comes to scalability, Spotify\u2019s Data Collection platform collects more than 1 trillion events per day. Its event delivery architecture is constantly evolving through numerous iterations. To learn more about the event delivery evolution, its inception, and subsequent improvements, check out this blog post.\n\nData Collection is needed, so we can:\n\nUnderstand what content is relevant to Spotify users\n\nDirectly respond to user feedback\n\nHave a deeper understanding of user interactions to enhance their experience\n\nWhen a team at Spotify decides to instrument their functionality with event delivery, aside from writing code using our SDs, they only need to define the event schemas. The infrastructure then automatically deploys a new set of event-specific components (such as PubSub queues, anonymization pipelines, and streaming jobs) using K8 operators. Any changes to the event schema triggers the deployment of corresponding resources. Anonymization solutions, including internal key-handling systems, are covered in detail in this article.\n\nThe balance between centralized and distributed ownership allows most updates to be managed by consumers of the consumption dataset, without requiring intervention from the infrastructure team.\n\nToday, over 1800 different event types \u2014 or signals representing interactions from Spotify users \u2014 are being published. In terms of team structure, the data collection area is organized to focus on the event delivery infrastructure, supporting and enhancing client SDKs for event transmission, and building the high quality datasets that represent the user journey experience, as well as the infrastructure needed behind it.\n\nData Management & Data Processing\n\nOur Data Processing efforts focus on empowering Spotify to utilize data effectively, while Data Management is dedicated to ensuring data integrity through tool creation and collaborative efforts. With more than 38,000 actively scheduled pipelines handling both hourly and daily tasks, scalability is a key consideration. Data Management and Data Processing are essential for Spotify to effectively manage its extensive data and pipelines. It\u2019s crucial to maintain data traceability (lineage), searchability (metadata), and accessibility, while implementing access controls and retention policies to manage storage costs and comply with regulations. These functions enable Spotify to extract maximum value from its data assets while upholding operational efficiency and regulatory standards.\n\nThe scheduling and orchestration of workflows are essential components of Data Processing. Once a workflow is picked up by the scheduler, it\u2019s executed on BigQuery, or either Flink or Dataflow clusters. Most pipelines utilize Scio, a Scala API for Beam.\n\nData pipelines generate data endpoints, each adhering to a specific schema and possibly containing multiple partitions. These endpoints are equipped with retention policies, access controls, lineage tracking, and quality checks.\n\nDefining a workflow or endpoint involves custom K8 operators, which help us to easily deploy and maintain complex structures. In that manner, the resource definition lives in the same repo as the pipeline code and gets deployed and maintained by the codeowners.\n\nMonitoring options include alerts for data lateness, long-running or failing workflows, and endpoints. Backstage integration facilitates easy resource management, monitoring, cost analysis, and quality assurance.\n\nBuilding a culture around the data platform\n\nBuilding a data platform is non-trivial \u2014 it needs to be flexible enough to satisfy a variety of different use cases, aligning with cost effectiveness and return on investment goals, and at the same time keeping the developer experience lean. The data platform needs to be easy to onboard to and have seamless upgrade paths (nobody likes to be disrupted by platform upgrades and breaking changes). And the platform needs to be reliable \u2014 if teams have the expectation to build business critical logic on top of your platform, we treat the platform as a critical use case as well.\n\nThere are multiple ways to elevate engagement with your product:\n\nDocumentation (which is easy to find). We all have been in situations where, \u201cI remember reading about it, but I don\u2019t remember where.\u201d It should be easier to find documentation than to ask a question (considering the waiting time).\n\nOnboard teams. There is no better way to learn about your product than to start using it yourself. Go to users and embed there. Learn about different use cases, make sure that your product is easy to use in all possible environments, and bring the learnings back to the platform.\n\nFleetshift the changes. People love evolving and making changes to their infrastructure and having the code being highlighted as deprecated, right? Not really. That is why we should automate all possible toils and migrations. Plan to deal with risks. Make time to support your customers.\n\nBuild a community where people are free to ask questions and where there are dedicated goalies to answer these questions. Answering community questions should not be left to free will, but should instead be encouraged and taken seriously. At Spotify we have a slack channel #data-support, where all data questions are addressed.\n\nWrapping up\n\nOur Data Platform has come a long way, and continues to evolve. At the very beginning, we were a few people, part of one team. We ran the pipelines on-premise, operating the largest Hadoop cluster in Europe. We are now 100+ engineers working on building the Spotify data platform on GCP, with data collection, management, and processing capabilities.\n\nThere is no formula or script to set up a data platform. A good way to start is by aligning your organizational needs with your investments. These needs become the drivers for your platform\u2019s building blocks, and may change over time. Make sure the challenges are clear \u2014 define clear goals and set clear expectations \u2014 it will help you to have the right support from your organization and to be on the path for success.\n\nGet closer to your users, have a clear way through which customers and stakeholders can reach out and give you direct feedback \u2014 it will set the stage to create a community around your platform. Finally, you do not have to start big: just start somewhere then evolve, iterate, and learn.", "label": 0}
{"title": "Fast builds, secure builds. Choose two.", "url": "https://stripe.com/blog/fast-secure-builds-choose-two", "content": "From data pipelines written in Scala and Python to infrastructure defined in Terraform, Stripe engineers interact with many different programming languages daily. We rely on complex build pipelines to convert Go source code into native binaries, Java into bytecode, and TypeScript into transpiled bundles for the Stripe Dashboard. Even in interpreted languages like Ruby, our code-generation process creates hundreds of thousands of Ruby files containing everything from gRPC service definitions to GraphQL bindings.\n\nOur continuous integration (CI) system is responsible for orchestrating these build pipelines and executing the tens of thousands of test suites that our engineers depend on to validate their changes. Keeping CI performant is crucial for providing our engineers with a delightful development experience. Since our CI system also produces artifacts that ultimately process billions of dollars each day, it's vital that it meets an exceptionally high security bar. At Stripe, we lean on a combination of open-source technologies and novel engineering to deliver a CI system that meets both of these requirements.\n\nA common framework for defining builds\n\nAs our codebase grows in volume and variety, Stripe leverages Bazel to manage our build and test pipelines. Bazel provides a multi-language and multi-platform framework to define rules\u2014recipes for how to build and test code in a specific stack. Many of the rules we use are maintained by the open-source community: rules_docker, rules_go and Java rules built directly into Bazel to name a few. Our infrastructure teams build on Bazel\u2019s headline support for custom rulesets to define internal rulesets for Ruby, JavaScript, and Terraform. Our engineers build and test their libraries and services by using these rulesets to declare \u201ctargets\u201d specific to their code. Each target instantiates a rule with a set of input files and other attributes. For example, a java_library rule could define a greeter target. The greeter target builds a libgreeter.jar file by invoking various \u201cactions\u201d defined by the rule. In this case, the java_library rule creates an action which invokes the Java compiler ( javac ).\n\nAfter engineers define their targets, Bazel is responsible for executing all the necessary actions to build and test a change. However, this execution phase is far from trivial. At Stripe\u2019s scale, building our rapidly growing Java codebase requires executing upwards of two hundred thousand actions. Running all these actions from scratch on a single machine would take several hours, even on the largest commodity EC2 instances. Bazel offers two features to address this challenge: remote caching and remote execution. Remote caching allows Bazel to reuse outputs from an action\u2019s earlier execution. Remote execution allows Bazel to distribute actions across multiple machines.\n\nScaling Bazel with remote caching and execution\n\nBazel\u2019s remote caching and execution subsystems provide compelling opportunities to improve the performance and efficiency of our CI system. Our engineers consistently identify blazing fast builds as a force multiplier in their workflows. Keeping builds performant (e.g. sub-5 minutes) is core to keeping our engineers productive. Over the years, we\u2019ve dedicated significant engineering resources to building a platform for remote caching and execution that delivers performance and efficiency wins without trading off security or reliability.\n\nTo illustrate the risks associated with a naive implementation, consider the implications of allowing any CI build to write to a remote cache. A malicious actor could then replace a business-critical binary trusted to securely handle invoice billing for Stripe customers with a corrupted version that reroutes funds to a personal bank account! Protecting ourselves from action cache poisoning requires that we only allow writes to the cache from trusted sources. A trusted source must faithfully execute actions and upload their true outputs. Fortunately, remote execution comes to the rescue by allowing Bazel to delegate action execution to a trusted source. The trusted sources are exclusively authorized to upload action results to the remote cache.\n\nCreating trusted build workers is easier said than done. Having our trusted worker run Bazel actions implies that we\u2019re evaluating arbitrary untrusted code on our trusted machine. It\u2019s critical that untrusted actions are prevented from writing directly to the action cache or otherwise influencing other co-tenant actions. Our initial implementation of the remote execution service used gVisor, an open-source implementation of the Linux kernel in user space. We coupled it with containerd to manage the container images in which actions execute. Our gVisor-driven sandbox ensured that we were resilient to not only privilege escalations, but also bugs in the Linux kernel. We were able to rest easy knowing that shipping malicious code to our production services would require breaching multiple strong layers of protection.\n\nWhile gVisor performed admirably for our initial workload, building our Go codebase, it faltered when faced with new workloads. JavaScript bundling, Ruby code generation and Java compilation all showed significant performance penalties in gVisor. In particular, we identified that the filesystem emulation in gVisor adds prohibitive overhead. Unlike Go compilation, which is primarily bound by user space CPU computation, common workloads in the new stacks issue thousands of filesystem syscalls. This behavior is largely attributable to how Ruby and Java import code by searching through a list of tens or hundreds of directories ( $LOAD_PATH and CLASSPATH respectively). For instance, running an empty Ruby unit test suite issues over 600,000 filesystem syscalls over the course of 5.5 seconds while searching a $LOAD_PATH with over 500 directories!\n\nReadout of execution time of a Ruby unit test file with a single no-op test, which immediately succeeds.\n\nThe search for a blazing fast sandbox\n\nWith application kernels like gVisor imposing a high overhead, and OS-level virtualization primitives like Linux containers lacking a robust enough security barrier, we started exploring hardware virtualization. Our performance goals led us towards Firecracker, a KVM-based microVM solution that features startup times in the 100s of milliseconds and substantially reduces I/O overhead. KVM allows the Linux kernel to act as a hypervisor and run virtual machines (VMs) using hardware virtualization. Our initial experimentation showed promising results, but Firecracker was far from a drop-in solution.\n\nOur most interesting challenge was providing actions with their input files. Before, with our gVisor sandbox, we\u2019d execute actions in an OverlayFS filesystem containing a fixed container image at its base and another directory above it (the \u201cexecroot\u201d) with the actions\u2019 inputs, e.g. the test files to execute. Unbeknownst to the action, the execroot consisted entirely of hard links to a local \u201cblobcache\u201d (a directory that held all our input files). This design minimized filesystem setup overhead. For instance, consider running many JavaScript actions where each action requires the same 150K JavaScript files, comprising 2.5GiB, in its node_modules directory. Rather than repeatedly copying 2.5GiB of data for each action, we downloaded each file once into the blobcache. Then, each action received an independent node_modules directory composed of 150K hard links.\n\nHowever, Firecracker (or KVM in general) doesn't support an analogous design that depends on OverlayFS to share directories. Instead, KVM exposes a virtio-based API that only allows attaching entire block devices to the guest VM. Since hard links are only valid across the same filesystem, we\u2019d have to directly attach the block device with the blobcache to each microVM. While that might work with a single concurrent microVM, physical block devices, especially ones receiving concurrent writes, can\u2019t be safely mounted more than once. A naive approach of copying from the blobcache for each action would incur a steep performance penalty. We needed an alternative that would allow our remote execution service to binpack dozens of concurrent actions on a single machine.\n\nFortunately, Linux\u2019s Logical Volume Manager (LVM) provides a compelling solution. Our remote execution service now relies on LVM to orchestrate the execution process:\n\nFirst, we continue to download action inputs into the blobcache. We concurrently boot our Firecracker microVM with empty placeholder disks and an optimized build of the Linux kernel. Then, using LVM\u2019s snapshotting capability we create a copy-on-write snapshot of the blobcache\u2019s logical volume. This snapshot occupies almost no physical disk space. The blobcache snapshot provides us with a logical block device that we attach to the booted Firecracker microVM using its RESTful API. With the containerd devmapper snapshotter (built on the same underlying technology that LVM snapshots abstract over), we create and attach a block device for the action\u2019s container image. Then, we send our custom init process a gRPC request over a VM socket, instructing it to mount both block devices and execute the action. The action executes within a chroot that exposes a minimal filesystem built using OverlayFS. Finally, the blobcache snapshot serves a dual purpose, allowing the execution service access to the action\u2019s outputs after the microVM\u2019s termination.\n\nDiving into an ocean of opportunities\n\nThis novel sandboxing strategy is only one of the myriad techniques our remote build system leverages to improve performance. Our remote cache service responds to GetTree RPCs by returning a recursively flattened list of files and directories from a given root directory. The flattening process can be very expensive for large directories full of third-party dependencies. Since these directories rarely change, our remote cache service itself caches the flattening results in a bespoke \u201cTreeCache.\u201d Then, our GetTree handler walks the children of each level of the directory tree in parallel, fetching from the TreeCache when possible to short-circuit evaluation of a cached branch.\n\nIn this example, an isolated change to the src/climate/components/Badge.tsx file allows us to fetch >99.9% of the GetTree response from our TreeCache. Branches that are unchanged and thus cached are denoted with green dashes.\n\nOn the topic of large directories, we\u2019ve started experimenting with an alternative strategy where actions depend on a SquashFS image that bundles action dependencies which don\u2019t change often. For example, changes to a package.json (the primary input to a large node_modules directory) are few and far between. This has led to observed performance improvements across the board: the Bazel client spends less time building an action digest, our cache service spends less time in the GetTree RPC, and our execution service creates orders of magnitude fewer hard links.\n\nOur remote execution service has a couple other tricks up its sleeve. For example, when our distribution layer schedules an action on an executor, it checks if another executor is already running an identical action. If so, rather than reserving resources and executing the action itself, the second executor blocks on completion of the first execution and re-uses its results. Action merging consistently helps improve build performance and efficiency, especially for particularly lengthy actions. Since the Bazel client never checks the remote cache after starting an action, this optimization relies on our remote execution service.\n\nWe\u2019re constantly on the lookout for new techniques to improve the performance, reliability and efficiency of our remote build services. For instance, we recently investigated NILFS, a log-structured filesystem that could rival LVM\u2019s snapshotting performance. As our system grows, we\u2019re exploring new strategies for load balancing in a highly heterogeneous environment, essentially solving a low-latency distributed scheduling problem. We\u2019re eager to explore Firecracker\u2019s snapshotting support which could help drive down latency in workloads where JVM startup is significant. For example, we could speed up Java compilation by scheduling actions on a microVM that has already started a JVM.\n\nProviding our engineers with a CI system that delivers rapid feedback on their changes and tightening the development loop is a top priority for Stripe. Our solution wouldn\u2019t be possible without Bazel. Its primitives give our engineers and platform teams a foundation for expressing rich, domain-specific build and test pipelines. Engineers across the organization benefit from a shared vocabulary and toolkit that not only streamlines their support experience, but also provides our productivity teams with a single point of extraordinarily high leverage. In particular, features like cached build results and distributed build execution are table stakes as we strive to support thousands of engineers.\n\nRather than spreading our investment across bespoke caching and distribution models for every language\u2019s build/test toolchain, we\u2019ve invested deeply in implementing Bazel\u2019s remote caching and execution APIs. Building remote caching and execution services that can delight everyone, from the Stripes testing their Subscriptions API change to the Stripes evaluating our infrastructure\u2019s security posture, is a significant task. Our approach relies on a unique combination of technologies to meet its performance goals while balancing security. We\u2019re far from done. Each morning, we\u2019re invigorated by the opportunity to raise the bar of engineering productivity at Stripe.", "label": 0}
{"title": "Title Launch Observability at Netflix Scale", "url": "https://netflixtechblog.com/title-launch-observability-at-netflix-scale-19ea916be1ed?source=collection_home---4------4-----------------------", "content": "Title Launch Observability at Netflix Scale\n\nPart 2: Navigating Ambiguity Netflix Technology Blog 6 min read \u00b7 Jan 7, 2025 -- 8 Listen Share\n\nBy: Varun Khaitan\n\nWith special thanks to my stunning colleagues: Mallika Rao, Esmir Mesic, Hugo Marques\n\nBuilding on the foundation laid in Part 1, where we explored the \u201cwhat\u201d behind the challenges of title launch observability at Netflix, this post shifts focus to the \u201chow.\u201d How do we ensure every title launches seamlessly and remains discoverable by the right audience?\n\nIn the dynamic world of technology, it\u2019s tempting to leap into problem-solving mode. But the key to lasting success lies in taking a step back \u2014 understanding the broader context before diving into solutions. This thoughtful approach doesn\u2019t just address immediate hurdles; it builds the resilience and scalability needed for the future. Let\u2019s explore how this mindset drives results.\n\nUnderstanding the Bigger Picture\n\nLet\u2019s take a comprehensive look at all the elements involved and how they interconnect. We should aim to address questions such as: What is vital to the business? Which aspects of the problem are essential to resolve? And how did we arrive at this point?\n\nThis process involves:\n\nIdentifying Stakeholders: Determine who is impacted by the issue and whose input is crucial for a successful resolution. In this case, the main stakeholders are:\n\n\n\n- Title Launch Operators\n\nRole: Responsible for setting up the title and its metadata into our systems.\n\nChallenge: Don\u2019t understand the cascading effects of their setup on these perceived black box personalization systems\n\n\n\n- Personalization System Engineers\n\nRole: Develop and operate the personalization systems.\n\nChallenge: End up spending unplanned cycles on title launch and personalization investigations.\n\n\n\n- Product Managers\n\nRole: Ensure we put forward the best experience for our members.\n\nChallenge: Members may not connect with the most relevant title.\n\n\n\n- Creative Representatives\n\nRole: Mediator between the content creators and Netflix.\n\nChallenge: Build trust in the Netflix brand with content creators. Mapping the Current Landscape: By charting the existing landscape, we can pinpoint areas ripe for improvement and steer clear of redundant efforts. Beyond the scattered solutions and makeshift scripts, it became evident that there was no established solution for title launch observability. This suggests that this area has been neglected for quite some time and likely requires significant investment. This situation presents both challenges and opportunities; while it may be more difficult to make initial progress, there are plenty of easy wins to capitalize on. Clarifying the Core Problem: By clearly defining the problem, we can ensure that our solutions address the root cause rather than just the symptoms. While there were many issues and problems we could address, the core problem here was to make sure every title was treated fairly by our personalization stack. If we can ensure fair treatment with confidence and bring that visibility to all our stakeholders, we can address all their challenges. Assessing Business Priorities: Understanding what is most important to the organization helps prioritize actions and resources effectively. In this context, we\u2019re focused on developing systems that ensure successful title launches, build trust between content creators and our brand, and reduce engineering operational overhead. While this is a critical business need and we definitely should solve it, it\u2019s essential to evaluate how it stacks up against other priorities across different areas of the organization.\n\nDefining Title Health\n\nNavigating such an ambiguous space required a shared understanding to foster clarity and collaboration. To address this, we introduced the term \u201cTitle Health,\u201d a concept designed to help us communicate effectively and capture the nuances of maintaining each title\u2019s visibility and performance. This shared language became a foundation for discussing the complexities of this domain.\n\n\u201cTitle Health\u201d encompasses various metrics and indicators that reflect how well a title is performing, in terms of discoverability and member engagement. The three main questions we try to answer are:\n\nIs this title visible at all to any member? Is this title visible to an appropriate audience size? Is this title reaching all the appropriate audiences?\n\nDefining Title Health provided a framework to monitor and optimize each title\u2019s lifecycle. It allowed us to align with partners on principles and requirements before building solutions, ensuring every title reaches its intended audience seamlessly. This common language not only introduced the problem space effectively but also accelerated collaboration and decision-making across teams.\n\nCategories of issues\n\nTo build a robust plan for title launch observability, we first needed to categorize the types of issues we encounter. This structured approach allows us to address all aspects of title health comprehensively.\n\nCurrently, these issues are grouped into three primary categories:\n\n1. Title Setup\n\nA title\u2019s setup includes essential attributes like metadata (e.g., launch dates, audio and subtitle languages, editorial tags) and assets (e.g., artwork, trailers, supplemental messages). These elements are critical for a title\u2019s eligibility in a row, accurate personalization, and an engaging presentation. Since these attributes feed directly into algorithms, any delays or inaccuracies can ripple through the system.\n\nThe observability system must ensure that title setup is complete and validated in a timely manner, identify potential bottlenecks and ensure a smooth launch process.\n\n2. Personalization Systems\n\nTitles are eligible to be recommended across multiple canvases on product \u2014 HomePage, Coming Soon, Messaging, Search and more. Personalization systems handle the recommendation and serving of titles on these canvases, leveraging a vast ecosystem of microservices, caches, databases, code, and configurations to build these product canvases.\n\nWe aim to validate that titles are eligible in all appropriate product canvases across the end to end personalization stack during all of the title\u2019s launch phases.\n\n3. Algorithms\n\nComplex algorithms drive each personalized product experience, recommending titles tailored to individual members. Observability here means validating the accuracy of algorithmic recommendations for all titles.\n\nAlgorithmic performance can be affected by various factors, such as model shortcomings, incomplete or inaccurate input signals, feature anomalies, or interactions between titles. Identifying and addressing these issues ensures that recommendations remain precise and effective.\n\nBy categorizing issues into these areas, we can systematically address challenges and deliver a reliable, personalized experience for every title on our platform.\n\nIssue Analysis\n\nLet\u2019s also learn more about how often we see each of these types of issues and how much effort it takes to fix them once they come up.\n\nFrom the above chart, we see that setup issues are the most common but they are also easy to fix since it\u2019s relatively straightforward to go back and rectify a title\u2019s metadata. System issues, which mostly manifest as bugs in our personalization microservices are not uncommon, and they take moderate effort to address. Algorithm issues, while rare, are really difficult to address since these often involve interpreting and retraining complex machine learning models.\n\nEvaluating Our Options\n\nNow that we understand more deeply about the problems we want to address and how we should go about prioritizing our resources. Lets go back to the two options we discussed in Part 1, and make an informed decision.\n\nUltimately, we realized this space demands the full spectrum of features we\u2019ve discussed. But the question remained: Where do we start?\n\nAfter careful consideration, we chose to focus on proactive issue detection first. Catching problems before launch offered the greatest potential for business impact, ensuring smoother launches, better member experiences, and stronger system reliability.\n\nThis decision wasn\u2019t just about solving today\u2019s challenges \u2014 it was about laying the foundation for a scalable, robust system that can grow with the complexities of our ever-evolving platform.\n\nUp next\n\nIn the next iteration we will talk about how to design an observability endpoint that works for all personalization systems. What are the main things to keep in mind while creating a microservice API endpoint? How do we ensure standardization? What is the architecture of the systems involved?\n\nKeep an eye out for our next binge-worthy episode!", "label": 0}
{"title": "I was once #Greek.", "url": "https://lifeofpablo.com/blog/i-was-once-greek", "content": "I was once #Greek.\n\nThis post was written in English (en_US).\n\n\"Over the years Greek life ( or more commonly known now as Fraternity & Sorority Life), has been in the news spotlight. Unfortunately, its mostly negative news. There is truly a good side to Greek Life. I mean I was once part of the system. Then again once a Greek always Greek, right? Let me tell you my story and experiences in Greek Life.\n\nI am Pablo and I affiliate myself with the Alpha Tau Omega Fraternity (ATO or AT\u03a9.) I am proud to say that I joined the ATOs, even if it was short lived. I am sad that my fraternity is no longer on campus. Our nonexistence is not due to being under terrible spotlight like many fraternites or sororities do.\n\nDuring my active time in Greek life, I gained so much! Sure we had small numbers but I would not trade it for the world. I met friends that still to this day I talk with today! I met so many great people in the the Greek community. I am truly a social person, so I am not afraid to come meet new people.\n\nCollege is all about opening your up doors to different perspectives and finding yourself.\n\nThere you have it, a side of me that does not come up much anymore! When I do talk about it, I am always proud to look back at something I was proud to be in! I may not be active anymore, but I am proud to say that I, Pablo Morales, am Greek!\"", "label": 1}
{"title": "Selecting a model for semantic search at Dropbox scale", "url": "https://dropbox.tech/machine-learning/selecting-model-semantic-search-dropbox-ai", "content": "Nautilus is our search engine for finding documents and other files in Dropbox. Introduced in 2018, Nautilus uses a conventional keyword-based approach that, while functional, has some inherent shortcomings. Because Nautilus has limited contextual understanding of what someone may be looking for, users are required to precisely recall a file\u2019s exact name or the specific keywords within. For instance, a search for \u201cemployment contract\u201d may overlook relevant \u201cjob agreement\u201d or \u201coffer letter\u201d documents, as Nautilus did not grab their contextual similarity. And for multilingual users, Nautilus expects queries and documents to be in the same language, hindering efficient retrieval when dealing with content in different languages. To mitigate these limitations, we considered techniques such as stemming, spelling correction, and query expansion for improved flexibility. However, we wondered if we could elevate the Dropbox search experience further. Could it be possible to help users find their content without needing to know the exact search term? Enter semantic search. Rather than rely on exact keyword matches, semantic search aims to better understand the relationship between user queries and document content. This functionality ultimately enables Dropbox users to locate crucial information more quickly, so they can spend less time searching and more time focusing on the task at hand. For multilingual users, semantic search also unlocks another capability: cross-lingual search. This advanced feature allows users to search in one language and receive relevant results in other languages, further enhancing accessibility and usability. We\u2019re excited to share that Dropbox now supports semantic search (powered by Nautilus), adding the aforementioned capabilities. We rolled it out for Dropbox users internally in early 2024 and then externally as an experiment for a subset of Pro and Essential users in May 2024. And with this release, we observed a nearly 17% reduction in empty search sessions (measured by ZRR, or zero-results rate), and a 2% lift in search session success (measured by qCTR, or qualified click-through rate). Based on these positive results, we decided to make semantic search generally available to all Pro and Essential users in August 2024, and coming soon in early 2025 for Business users. Staying true to our AI Principles, we performed the evaluation of different embedding models in-house with pre-trained models, and we did not train on any user data. Below, we\u2019ll introduce the key concepts behind semantic search, and walk you through our approach for selecting one essential component: the text embedding model.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images\u2014plus generative AI capabilities across even more connected apps. See what's new \u2192\n\nUnderstanding semantic search: A new paradigm\n\nSemantic search is designed to retrieve information based on meaning and intent, going beyond the limitations of simple keyword matching. While the terms \"semantic search\" and \"vector search\" are sometimes used interchangeably, within the context of this post we define \"semantic search\" as the entire flow\u2014from user query input to search results output\u2014and \"vector search\" as the specific step of retrieving items based on vector similarity. We plan to introduce the concept of semantic search for multiple file types, but in this first iteration of semantic search at Dropbox, we focused on supporting only text files. At its core, semantic search relies on vector search as the underlying technology to interpret and process unstructured data such as text, images, audio, and video. This process involves transforming content into numeric representations called embeddings, which capture the rich features of the data in a high-dimensional space. These embeddings, or vectors, are dense lists of numbers encoding features across hundreds or thousands of dimensions. During training, machine learning models refine these dimensions to capture nuanced patterns\u2014whether visual details, document structure, or associations among words. While individual dimensions are often abstract, certain aspects may implicitly capture features like sentiment, syntax, or relationships between concepts. This dense representation enables a context-aware understanding that drives more meaningful retrieval. Vector search provides the flexibility to store different types of embeddings, for different file types, based on the specific needs of the application. By selecting or training models that capture the most relevant features for the task at hand\u2014such as visual characteristics for images or semantic relationships for text\u2014we can tailor the embeddings to support a wide range of content types and use cases. Building on this foundation, semantic search operates by transforming user queries into embeddings and then performing vector search to retrieve results that align with the query's intent, rather than its literal terms. The search begins when a user enters a query, which is converted into an embedding and compared against stored embeddings using vector search. Related items naturally cluster together in the vector space, even when the formats differ\u2014whether text, images, or diagrams. Using nearest-neighbor algorithms, vector search identifies results based on meaning, ensuring that the retrieved content aligns with the user's intent. For example, a search for \u201cguides and resources\u201d could surface instructional PDFs, training videos, and visual diagrams\u2014regardless of the exact labels for each item. This similarity-based retrieval approach unlocks a wide range of possibilities, from linking related documents and multimedia to uncovering insights across diverse content. Vector search enhances our interaction with information by establishing meaningful connections between items, supporting a flexible and integrated search experience. Through its ability to understand meaning and context, vector search provides the technical foundation for a search system that can adapt to varied applications, delivering more relevant and context-aware results.\n\nIdentifying the right model for our purposes\n\nCentral to semantic search\u2014and for our use case, for text documents\u2014is the document embedding model, which maps each document and query to the respective embeddings. Implementing semantic search at Dropbox scale would require mapping both new and existing documents to their embeddings and storing them in our indices. For a search platform as large as Dropbox, indexing more than a trillion documents and exabytes of data, could get very expensive computationally! Overall performance\u2014in terms of both latency and quality\u2014would hinge on the model we used. Since computing the embedding for a user\u2019s search query would now be in the critical path for retrieving their documents, speed would be paramount. Also, a faster model would mean we could index our users\u2019 documents faster, leading to time and cost savings down the line. At the same time, the quality of our embeddings needed to be high enough to benefit from the vector search paradigm. Whatever document embedding model we chose would need to generate embeddings that were similar enough to match documents with their relevant queries, but still discriminative enough to filter out irrelevant documents. We decided to perform a thorough evaluation of available models to find the one most suitable for our purposes, measuring each model with speed and search quality in mind.\n\nIntegrating the Massive Text Embedding Benchmark\n\nFortunately, assessing document embedding models is an undertaking that has already been embraced by the research community. The Massive Text Embedding Benchmark (MTEB) is an open-source benchmark that assesses document embedding models across eight evaluation tasks and 56 datasets, some of them multilingual. MTEB has published over 2,000 results to its leaderboard since its release in 2022. We adapted the MTEB benchmark and leveraged the reranking task to fit our needs as follows: Customizations for Dropbox infrastructure. We added adapters to enable the evaluation of models running in our in-house inference services, in addition to models executed inline. We also added adapters to allow streaming datasets that reside in our infrastructure.\n\nWe added adapters to enable the evaluation of models running in our in-house inference services, in addition to models executed inline. We also added adapters to allow streaming datasets that reside in our infrastructure. Multiple embeddings per document. MTEB by default presumes that a single embedding is generated per document, as the size of documents in the public datasets are fairly consistent. However, in production, our user\u2019s document can range from very tiny to very large, leading to the hypothesis that we should try generating multiple embeddings per document. We implemented various nuanced strategies for separating, or chunking, a given document into individual chunks. Chunking is further configurable with a parameter for specifying overlap between consecutive chunks. We also explored summarization, in order to ensure that we respect the model\u2019s internal threshold on input size. The resulting embeddings can then be used directly or in aggregate.\n\nMTEB by default presumes that a single embedding is generated per document, as the size of documents in the public datasets are fairly consistent. However, in production, our user\u2019s document can range from very tiny to very large, leading to the hypothesis that we should try generating multiple embeddings per document. We implemented various nuanced strategies for separating, or chunking, a given document into individual chunks. Chunking is further configurable with a parameter for specifying overlap between consecutive chunks. We also explored summarization, in order to ensure that we respect the model\u2019s internal threshold on input size. The resulting embeddings can then be used directly or in aggregate. Optimizations for storage and precision. To optimize storage costs, we reduced the precision (full 32-bit floating point, half float, quarter float, fixed point of various bit depth) and dimensionality (via Gaussian random projections) of MTEB's full-sized embeddings.\n\nTo optimize storage costs, we reduced the precision (full 32-bit floating point, half float, quarter float, fixed point of various bit depth) and dimensionality (via Gaussian random projections) of MTEB's full-sized embeddings. Files versus documents. Whereas the public datasets in MTEB consist of unnamed documents identified by their content, documents in Dropbox are named by our users\u2014and filenames are quite significant in retrieval tasks. We crafted various approaches to incorporate embeddings of the filenames into MTEB when applicable.\n\nConstructing novel datasets for evaluation\n\nAmong the evaluation tasks that MTEB offers, we were most interested in re-ranking and retrieval tasks. While the performance of various models across the two tasks were readily available on the leaderboard, these two tasks primarily offered English-only datasets, which does not match the distribution of the Dropbox corpus. To address any potential distribution shift between the public datasets and the Dropbox corpus, we leveraged our existing ML-powered search platform Nautilus to construct our own MTEB-compatible datasets for more precise evaluation. We created a Kubeflow pipeline to generate a custom dataset purely for the purpose of evaluating the different embeddings models and configurations. With this pipeline, we extract anonymized query-document pairs from Dropbox search logs and put them in an MTEB-compatible format within our in-house service for hosting L0 datasets. We have strict security and access controls in place to ensure that only Dropboxers who need access to this data can access it. Moreover, as per our data retention policies, the datasets get deleted after 30 days. We also extended the pipeline to unlock multilingual evaluation. At the time of our benchmarking, the public retrieval datasets were English-only (nowadays, MIRACL dataset bridges this gap). In order to diversify our evaluation criteria beyond English, we pioneered a set of multilingual datasets (Spanish, French, German, Japanese, Korean) from Dropbox search logs.\n\nEvaluation and model selection\n\nFrom an exhaustive evaluation of 11 models\u2014including four multilingual ones\u2014we selected multilingual-e5-large as the top performer. This model not only excelled on our Dropbox datasets, but at the time of benchmarking also stood out as the best multilingual model on the MTEB public leaderboard across various tasks. Below are our benchmark results on our custom datasets for multilingual models, using a configuration of two embeddings per document: one for title with path, and another for the first chunk of the text content. Metrics reported are mean reciprocal rank (MRR) and mean average precision (MAP), where higher is better.\n\nModel English Japanese Spanish Korean German paraphrase-multilingual-mpnet-base-v2 MRR: 0.3299\n\nMAP: 0.3462 MRR: 0.2245 MAP: 0.2448 MRR: 0.2367 MAP: 0.2568 MRR: 0.2338 MAP: 0.2546 MRR: 0.2879 MAP: 0.3078 paraphrase-multilingual-MiniLM-L12-v2 MRR: 0.3108 MAP: 0.3278 MRR: 0.2628 MAP: 0.2804 MRR: 0.2043 MAP: 0.2273 MRR: 0.2374 MAP: 0.2584 MRR: 0.2355 MAP: 0.2533 multilingual-e5-large MRR: 0.5044 MAP: 0.5133 MRR: 0.4265 MAP: 0.4386 MRR: 0.3350 MAP: 0.3524 MRR: 0.4003 MAP: 0.4118 MRR: 0.3305 \"map\": 0.3432 multilingual-e5-base MRR: 0.4492 MAP: 0.4603 MRR: 0.3659 MAP: 0.3795 MRR: 0.3330 MAP: 0.3511 MRR: 0.3817 MAP: 0.3957 MRR: 0.3405 MAP: 0.3535\n\nWhile our evaluation primarily focused on re-ranking and retrieval tasks, we knew that other teams at Dropbox might be interested in using MTEB to evaluate additional tasks. To extend its functionality beyond vector search, we linked MTEB with the rest of our infrastructure and added additional key parameters for evaluation. This means that other Dropbox initiatives\u2014such as Dropbox Dash, or our AI-powered file summaries and conversational features\u2014can now leverage MTEB to evaluate various document embedding models for their applications as well.\n\nPutting the model into production\n\nIn order to make the best use of our storage and compute resources, putting multilingual-e5-large into production required some tradeoffs. Based on available storage capacity, we started by setting an upper bound of 4KB of vector-search-related metadata per document. This gave us room to adjust the number of embeddings per document, as well as the dimensionality of those embeddings and their numerical precision. In compression experiments, reducing precision to 8-bit per channel resulted in a manageable 1KB per embedding with a marginal impact on quality. However, reducing dimensionality adversely affected quality. Given that two embeddings fit our storage constraints, we opted to maintain the full dimension of the embeddings to preserve data integrity. The final quantization format we adopted is a slight variation over the standard 8-bit: we first scale the embedding so that the maximum over the magnitudes of individual channels is exactly 1.0. We store the scalar separately as a 32-bit float (4 bytes)\u2014and the scaled embedding, which lies in [-1, 1], can be converted to 8-bit fixed point by remapping the range to 8-bit signed integer and rounding. We found that this scheme minimized the error on cosine similarity over the query-document pairs in our dataset. As for managing our compute resources, we had to consider: The maximum number of characters per document\n\nConstraints on the number of document chunks\n\nHow to balance chunk sizes to maintain contextual relevance without an overwhelming amount of processing\n\nDifferent document embedding strategies (file path and/or content embeddings) Our findings ultimately favored a dual approach: storing separate embeddings for the file path (including path and filename) and the document content up to some limit (512 tokens). While this method doesn't encompass the entire document, focusing on the initial 512 tokens significantly lowered costs and processing demands with just two embeddings per document.\n\nA more relevant Dropbox search experience", "label": 0}
{"title": "Introducing Netflix\u2019s Key-Value Data Abstraction Layer", "url": "https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30?source=collection_home---4------14-----------------------", "content": "Introducing Netflix\u2019s Key-Value Data Abstraction Layer Netflix Technology Blog 13 min read \u00b7 Sep 18, 2024 -- 10 Listen Share\n\nVidhya Arvind, Rajasekhar Ummadisetty, Joey Lynch, Vinay Chella\n\nIntroduction\n\nAt Netflix our ability to deliver seamless, high-quality, streaming experiences to millions of users hinges on robust, global backend infrastructure. Central to this infrastructure is our use of multiple online distributed databases such as Apache Cassandra, a NoSQL database known for its high availability and scalability. Cassandra serves as the backbone for a diverse array of use cases within Netflix, ranging from user sign-ups and storing viewing histories to supporting real-time analytics and live streaming.\n\nOver time as new key-value databases were introduced and service owners launched new use cases, we encountered numerous challenges with datastore misuse. Firstly, developers struggled to reason about consistency, durability and performance in this complex global deployment across multiple stores. Second, developers had to constantly re-learn new data modeling practices and common yet critical data access patterns. These include challenges with tail latency and idempotency, managing \u201cwide\u201d partitions with many rows, handling single large \u201cfat\u201d columns, and slow response pagination. Additionally, the tight coupling with multiple native database APIs \u2014 APIs that continually evolve and sometimes introduce backward-incompatible changes \u2014 resulted in org-wide engineering efforts to maintain and optimize our microservice\u2019s data access.\n\nTo overcome these challenges, we developed a holistic approach that builds upon our Data Gateway Platform. This approach led to the creation of several foundational abstraction services, the most mature of which is our Key-Value (KV) Data Abstraction Layer (DAL). This abstraction simplifies data access, enhances the reliability of our infrastructure, and enables us to support the broad spectrum of use cases that Netflix demands with minimal developer effort.\n\nIn this post, we dive deep into how Netflix\u2019s KV abstraction works, the architectural principles guiding its design, the challenges we faced in scaling diverse use cases, and the technical innovations that have allowed us to achieve the performance and reliability required by Netflix\u2019s global operations.\n\nThe Key-Value Service\n\nThe KV data abstraction service was introduced to solve the persistent challenges we faced with data access patterns in our distributed databases. Our goal was to build a versatile and efficient data storage solution that could handle a wide variety of use cases, ranging from the simplest hashmaps to more complex data structures, all while ensuring high availability, tunable consistency, and low latency.\n\nData Model\n\nAt its core, the KV abstraction is built around a two-level map architecture. The first level is a hashed string ID (the primary key), and the second level is a sorted map of a key-value pair of bytes. This model supports both simple and complex data models, balancing flexibility and efficiency.\n\nHashMap<String, SortedMap<Bytes, Bytes>>\n\nFor complex data models such as structured Records or time-ordered Events , this two-level approach handles hierarchical structures effectively, allowing related data to be retrieved together. For simpler use cases, it also represents flat key-value Maps (e.g. id \u2192 {\"\" \u2192 value} ) or named Sets (e.g. id \u2192 {key \u2192 \"\"} ). This adaptability allows the KV abstraction to be used in hundreds of diverse use cases, making it a versatile solution for managing both simple and complex data models in large-scale infrastructures like Netflix.\n\nThe KV data can be visualized at a high level, as shown in the diagram below, where three records are shown.\n\nmessage Item (\n\nBytes key,\n\nBytes value,\n\nMetadata metadata,\n\nInteger chunk\n\n)\n\nDatabase Agnostic Abstraction\n\nThe KV abstraction is designed to hide the implementation details of the underlying database, offering a consistent interface to application developers regardless of the optimal storage system for that use case. While Cassandra is one example, the abstraction works with multiple data stores like EVCache, DynamoDB, RocksDB, etc\u2026\n\nFor example, when implemented with Cassandra, the abstraction leverages Cassandra\u2019s partitioning and clustering capabilities. The record ID acts as the partition key, and the item key as the clustering column:\n\nThe corresponding Data Definition Language (DDL) for this structure in Cassandra is:\n\nCREATE TABLE IF NOT EXISTS <ns>.<table> (\n\nid text,\n\nkey blob,\n\nvalue blob,\n\nvalue_metadata blob,\n\n\n\nPRIMARY KEY (id, key))\n\nWITH CLUSTERING ORDER BY (key <ASC|DESC>)\n\nNamespace: Logical and Physical Configuration\n\nA namespace defines where and how data is stored, providing logical and physical separation while abstracting the underlying storage systems. It also serves as central configuration of access patterns such as consistency or latency targets. Each namespace may use different backends: Cassandra, EVCache, or combinations of multiple. This flexibility allows our Data Platform to route different use cases to the most suitable storage system based on performance, durability, and consistency needs. Developers just provide their data problem rather than a database solution!\n\nIn this example configuration, the ngsegment namespace is backed by both a Cassandra cluster and an EVCache caching layer, allowing for highly durable persistent storage and lower-latency point reads.\n\n\"persistence_configuration\":[\n\n{\n\n\"id\":\"PRIMARY_STORAGE\",\n\n\"physical_storage\": {\n\n\"type\":\"CASSANDRA\",\n\n\"cluster\":\"cassandra_kv_ngsegment\",\n\n\"dataset\":\"ngsegment\",\n\n\"table\":\"ngsegment\",\n\n\"regions\": [\"us-east-1\"],\n\n\"config\": {\n\n\"consistency_scope\": \"LOCAL\",\n\n\"consistency_target\": \"READ_YOUR_WRITES\"\n\n}\n\n}\n\n},\n\n{\n\n\"id\":\"CACHE\",\n\n\"physical_storage\": {\n\n\"type\":\"CACHE\",\n\n\"cluster\":\"evcache_kv_ngsegment\"\n\n},\n\n\"config\": {\n\n\"default_cache_ttl\": 180s\n\n}\n\n}\n\n]\n\n\n\nKey APIs of the KV Abstraction\n\nTo support diverse use-cases, the KV abstraction provides four basic CRUD APIs:\n\nPutItems \u2014 Write one or more Items to a Record\n\nThe PutItems API is an upsert operation, it can insert new data or update existing data in the two-level map structure.\n\nmessage PutItemRequest (\n\nIdempotencyToken idempotency_token,\n\nstring namespace,\n\nstring id,\n\nList<Item> items\n\n)\n\nAs you can see, the request includes the namespace, Record ID, one or more items, and an idempotency token to ensure retries of the same write are safe. Chunked data can be written by staging chunks and then committing them with appropriate metadata (e.g. number of chunks).\n\nGetItems \u2014 Read one or more Items from a Record\n\nThe GetItems API provides a structured and adaptive way to fetch data using ID, predicates, and selection mechanisms. This approach balances the need to retrieve large volumes of data while meeting stringent Service Level Objectives (SLOs) for performance and reliability.\n\nmessage GetItemsRequest (\n\nString namespace,\n\nString id,\n\nPredicate predicate,\n\nSelection selection,\n\nMap<String, Struct> signals\n\n)\n\nThe GetItemsRequest includes several key parameters:\n\nNamespace : Specifies the logical dataset or table\n\n: Specifies the logical dataset or table Id : Identifies the entry in the top-level HashMap\n\n: Identifies the entry in the top-level HashMap Predicate : Filters the matching items and can retrieve all items ( match_all ), specific items ( match_keys ), or a range ( match_range )\n\n: Filters the matching items and can retrieve all items ( ), specific items ( ), or a range ( ) Selection : Narrows returned responses for example page_size_bytes for pagination, item_limit for limiting the total number of items across pages and include / exclude to include or exclude large values from responses\n\n: Narrows returned responses for example for pagination, for limiting the total number of items across pages and / to include or exclude large values from responses Signals: Provides in-band signaling to indicate client capabilities, such as supporting client compression or chunking.\n\nThe GetItemResponse message contains the matching data:\n\nmessage GetItemResponse (\n\nList<Item> items,\n\nOptional<String> next_page_token\n\n)\n\nItems : A list of retrieved items based on the Predicate and Selection defined in the request.\n\n: A list of retrieved items based on the and defined in the request. Next Page Token: An optional token indicating the position for subsequent reads if needed, essential for handling large data sets across multiple requests. Pagination is a critical component for efficiently managing data retrieval, especially when dealing with large datasets that could exceed typical response size limits.\n\nDeleteItems \u2014 Delete one or more Items from a Record\n\nThe DeleteItems API provides flexible options for removing data, including record-level, item-level, and range deletes \u2014 all while supporting idempotency.\n\nmessage DeleteItemsRequest (\n\nIdempotencyToken idempotency_token,\n\nString namespace,\n\nString id,\n\nPredicate predicate\n\n)\n\n\n\nJust like in the GetItems API, the Predicate allows one or more Items to be addressed at once:\n\nRecord-Level Deletes (match_all) : Removes the entire record in constant latency regardless of the number of items in the record.\n\n: Removes the entire record in constant latency regardless of the number of items in the record. Item-Range Deletes (match_range) : This deletes a range of items within a Record. Useful for keeping \u201cn-newest\u201d or prefix path deletion.\n\n: This deletes a range of items within a Record. Useful for keeping \u201cn-newest\u201d or prefix path deletion. Item-Level Deletes (match_keys): Deletes one or more individual items.\n\nSome storage engines (any store which defers true deletion) such as Cassandra struggle with high volumes of deletes due to tombstone and compaction overhead. Key-Value optimizes both record and range deletes to generate a single tombstone for the operation \u2014 you can learn more about tombstones in About Deletes and Tombstones.\n\nItem-level deletes create many tombstones but KV hides that storage engine complexity via TTL-based deletes with jitter. Instead of immediate deletion, item metadata is updated as expired with randomly jittered TTL applied to stagger deletions. This technique maintains read pagination protections. While this doesn\u2019t completely solve the problem it reduces load spikes and helps maintain consistent performance while compaction catches up. These strategies help maintain system performance, reduce read overhead, and meet SLOs by minimizing the impact of deletes.\n\nComplex Mutate and Scan APIs\n\nBeyond simple CRUD on single Records, KV also supports complex multi-item and multi-record mutations and scans via MutateItems and ScanItems APIs. PutItems also supports atomic writes of large blob data within a single Item via a chunked protocol. These complex APIs require careful consideration to ensure predictable linear low-latency and we will share details on their implementation in a future post.\n\nDesign Philosophies for reliable and predictable performance\n\nIdempotency to fight tail latencies\n\nTo ensure data integrity the PutItems and DeleteItems APIs use idempotency tokens, which uniquely identify each mutative operation and guarantee that operations are logically executed in order, even when hedged or retried for latency reasons. This is especially crucial in last-write-wins databases like Cassandra, where ensuring the correct order and de-duplication of requests is vital.\n\nIn the Key-Value abstraction, idempotency tokens contain a generation timestamp and random nonce token. Either or both may be required by backing storage engines to de-duplicate mutations.\n\nmessage IdempotencyToken (\n\nTimestamp generation_time,\n\nString token\n\n)\n\nAt Netflix, client-generated monotonic tokens are preferred due to their reliability, especially in environments where network delays could impact server-side token generation. This combines a client provided monotonic generation_time timestamp with a 128 bit random UUID token . Although clock-based token generation can suffer from clock skew, our tests on EC2 Nitro instances show drift is minimal (under 1 millisecond). In some cases that require stronger ordering, regionally unique tokens can be generated using tools like Zookeeper, or globally unique tokens such as a transaction IDs can be used.\n\nThe following graphs illustrate the observed clock skew on our Cassandra fleet, suggesting the safety of this technique on modern cloud VMs with direct access to high-quality clocks. To further maintain safety, KV servers reject writes bearing tokens with large drift both preventing silent write discard (write has timestamp far in past) and immutable doomstones (write has a timestamp far in future) in storage engines vulnerable to those.\n\nHandling Large Data through Chunking\n\nKey-Value is also designed to efficiently handle large blobs, a common challenge for traditional key-value stores. Databases often face limitations on the amount of data that can be stored per key or partition. To address these constraints, KV uses transparent chunking to manage large data efficiently.\n\nFor items smaller than 1 MiB, data is stored directly in the main backing storage (e.g. Cassandra), ensuring fast and efficient access. However, for larger items, only the id, key, and metadata are stored in the primary storage, while the actual data is split into smaller chunks and stored separately in chunk storage. This chunk storage can also be Cassandra but with a different partitioning scheme optimized for handling large values. The idempotency token ties all these writes together into one atomic operation.\n\nBy splitting large items into chunks, we ensure that latency scales linearly with the size of the data, making the system both predictable and efficient. A future blog post will describe the chunking architecture in more detail, including its intricacies and optimization strategies.\n\nClient-Side Compression\n\nThe KV abstraction leverages client-side payload compression to optimize performance, especially for large data transfers. While many databases offer server-side compression, handling compression on the client side reduces expensive server CPU usage, network bandwidth, and disk I/O. In one of our deployments, which helps power Netflix\u2019s search, enabling client-side compression reduced payload sizes by 75%, significantly improving cost efficiency.\n\nSmarter Pagination\n\nWe chose payload size in bytes as the limit per response page rather than the number of items because it allows us to provide predictable operation SLOs. For instance, we can provide a single-digit millisecond SLO on a 2 MiB page read. Conversely, using the number of items per page as the limit would result in unpredictable latencies due to significant variations in item size. A request for 10 items per page could result in vastly different latencies if each item was 1 KiB versus 1 MiB.\n\nUsing bytes as a limit poses challenges as few backing stores support byte-based pagination; most data stores use the number of results e.g. DynamoDB and Cassandra limit by number of items or rows. To address this, we use a static limit for the initial queries to the backing store, query with this limit, and process the results. If more data is needed to meet the byte limit, additional queries are executed until the limit is met, the excess result is discarded and a page token is generated.\n\nThis static limit can lead to inefficiencies, one large item in the result may cause us to discard many results, while small items may require multiple iterations to fill a page, resulting in read amplification. To mitigate these issues, we implemented adaptive pagination which dynamically tunes the limits based on observed data.\n\nAdaptive Pagination\n\nWhen an initial request is made, a query is executed in the storage engine, and the results are retrieved. As the consumer processes these results, the system tracks the number of items consumed and the total size used. This data helps calculate an approximate item size, which is stored in the page token. For subsequent page requests, this stored information allows the server to apply the appropriate limits to the underlying storage, reducing unnecessary work and minimizing read amplification.\n\nWhile this method is effective for follow-up page requests, what happens with the initial request? In addition to storing item size information in the page token, the server also estimates the average item size for a given namespace and caches it locally. This cached estimate helps the server set a more optimal limit on the backing store for the initial request, improving efficiency. The server continuously adjusts this limit based on recent query patterns or other factors to keep it accurate. For subsequent pages, the server uses both the cached data and the information in the page token to fine-tune the limits.\n\nIn addition to adaptive pagination, a mechanism is in place to send a response early if the server detects that processing the request is at risk of exceeding the request\u2019s latency SLO.\n\nFor example, let us assume a client submits a GetItems request with a per-page limit of 2 MiB and a maximum end-to-end latency limit of 500ms. While processing this request, the server retrieves data from the backing store. This particular record has thousands of small items so it would normally take longer than the 500ms SLO to gather the full page of data. If this happens, the client would receive an SLO violation error, causing the request to fail even though there is nothing exceptional. To prevent this, the server tracks the elapsed time while fetching data. If it determines that continuing to retrieve more data might breach the SLO, the server will stop processing further results and return a response with a pagination token.\n\nThis approach ensures that requests are processed within the SLO, even if the full page size isn\u2019t met, giving clients predictable progress. Furthermore, if the client is a gRPC server with proper deadlines, the client is smart enough not to issue further requests, reducing useless work.\n\nIf you want to know more, the How Netflix Ensures Highly-Reliable Online Stateful Systems article talks in further detail about these and many other techniques.\n\nSignaling\n\nKV uses in-band messaging we call signaling that allows the dynamic configuration of the client and enables it to communicate its capabilities to the server. This ensures that configuration settings and tuning parameters can be exchanged seamlessly between the client and server. Without signaling, the client would need static configuration \u2014 requiring a redeployment for each change \u2014 or, with dynamic configuration, would require coordination with the client team.\n\nFor server-side signals, when the client is initialized, it sends a handshake to the server. The server responds back with signals, such as target or max latency SLOs, allowing the client to dynamically adjust timeouts and hedging policies. Handshakes are then made periodically in the background to keep the configuration current. For client-communicated signals, the client, along with each request, communicates its capabilities, such as whether it can handle compression, chunking, and other features.\n\nKV Usage @ Netflix\n\nThe KV abstraction powers several key Netflix use cases, including:\n\nStreaming Metadata : High-throughput, low-latency access to streaming metadata, ensuring personalized content delivery in real-time.\n\n: High-throughput, low-latency access to streaming metadata, ensuring personalized content delivery in real-time. User Profiles : Efficient storage and retrieval of user preferences and history, enabling seamless, personalized experiences across devices.\n\n: Efficient storage and retrieval of user preferences and history, enabling seamless, personalized experiences across devices. Messaging : Storage and retrieval of push registry for messaging needs, enabling the millions of requests to flow through.\n\n: Storage and retrieval of push registry for messaging needs, enabling the millions of requests to flow through. Real-Time Analytics: This persists large-scale impression and provides insights into user behavior and system performance, moving data from offline to online and vice versa.\n\nFuture Enhancements\n\nLooking forward, we plan to enhance the KV abstraction with:\n\nLifecycle Management : Fine-grained control over data retention and deletion.\n\n: Fine-grained control over data retention and deletion. Summarization : Techniques to improve retrieval efficiency by summarizing records with many items into fewer backing rows.\n\n: Techniques to improve retrieval efficiency by summarizing records with many items into fewer backing rows. New Storage Engines : Integration with more storage systems to support new use cases.\n\n: Integration with more storage systems to support new use cases. Dictionary Compression: Further reducing data size while maintaining performance.\n\nConclusion\n\nThe Key-Value service at Netflix is a flexible, cost-effective solution that supports a wide range of data patterns and use cases, from low to high traffic scenarios, including critical Netflix streaming use-cases. The simple yet robust design allows it to handle diverse data models like HashMaps, Sets, Event storage, Lists, and Graphs. It abstracts the complexity of the underlying databases from our developers, which enables our application engineers to focus on solving business problems instead of becoming experts in every storage engine and their distributed consistency models. As Netflix continues to innovate in online datastores, the KV abstraction remains a central component in managing data efficiently and reliably at scale, ensuring a solid foundation for future growth.\n\nAcknowledgments: Special thanks to our stunning colleagues who contributed to Key Value\u2019s success: William Schor, Mengqing Wang, Chandrasekhar Thumuluru, Rajiv Shringi, John Lu, George Cambell, Ammar Khaku, Jordan West, Chris Lohfink, Matt Lehman, and the whole online datastores team (ODS, f.k.a CDE).", "label": 0}
{"title": "How Meta understands data at scale", "url": "https://engineering.fb.com/2025/04/28/security/how-meta-understands-data-at-scale/", "content": "Managing and understanding large-scale data ecosystems is a significant challenge for many organizations, requiring innovative solutions to efficiently safeguard user data. Meta\u2019s vast and diverse systems make it particularly challenging to comprehend its structure, meaning, and context at scale.\n\nTo address these challenges, we made substantial investments in advanced data understanding technologies, as part of our Privacy Aware Infrastructure (PAI) . Specifically, we have adopted a \u201cshift-left\u201d approach, integrating data schematization and annotations early in the product development process. We also created a universal privacy taxonomy , a standardized framework providing a common semantic vocabulary for data privacy management across Meta\u2019s products that ensures quality data understanding and provides developers with reusable and efficient compliance tooling.\n\nWe discovered that a flexible and incremental approach was necessary to onboard the wide variety of systems and languages used in building Meta\u2019s products. Additionally, continuous collaboration between privacy and product teams was essential to unlock the value of data understanding at scale.\n\nWe embarked on the journey of understanding data across Meta a decade ago with millions of assets in scope ranging from structured and unstructured, processed by millions of flows across many of the Meta App offerings. Over the past 10 years, Meta has cataloged millions of data assets and is classifying them daily, supporting numerous privacy initiatives across our product groups. Additionally, our continuous understanding approach ensures that privacy considerations are embedded at every stage of product development.\n\nAt Meta, we have a deep responsibility to protect the privacy of our community. We\u2019re upholding that by investing our vast engineering capabilities into building cutting-edge privacy technology. We believe that privacy drives product innovation. This led us to develop our Privacy Aware Infrastructure (PAI), which integrates efficient and reliable privacy tools into Meta\u2019s systems to address needs such as purpose limitation\u2014restricting how data can be used while also unlocking opportunities for product innovation by ensuring transparency in data flows\n\nData understanding is an early step in PAI. It involves capturing the structure and meaning of data assets, such as tables, logs, and AI models. Over the past decade, we have gained a deeper understanding of our data, by embedding privacy considerations into every stage of product development, ensuring a more secure and responsible approach to data management.\n\nWe embarked on our data understanding journey by employing heuristics and classifiers to automatically detect semantic types from user-generated content. This approach has evolved significantly over the years, enabling us to scale to millions of assets. However, conducting these processes outside of developer workflows presented challenges in terms of accuracy and timeliness. Delayed classifications often led to confusion and unnecessary work, while the results were difficult to consume and interpret.\n\nData understanding at Meta using PAI\n\nTo address shortcomings, we invested in data understanding by capturing asset structure (schematization), describing meaning (annotation), and inventorying it into OneCatalog (Meta\u2019s system that discovers, registers, and enumerates all data assets) across all Meta technologies. We developed tools and APIs for developers to organize assets, classify data, and auto-generate annotation code. Despite significant investment, the journey was not without challenges, requiring innovative solutions and collaboration across the organization.\n\nChallenge Approach Understanding at scale (lack of foundation) At Meta, we manage hundreds of data systems and millions of assets across our family of apps. Each product features its own distinct data model, physical schema, query language, and access patterns. This diversity created a unique hurdle for offline assets: the inability to reuse schemas due to the limitations of physical table schemas in adapting to changing definitions. Specifically, renaming columns or making other modifications had far-reaching downstream implications, rendering schema evolution challenging, thus propagation required careful coordination to ensure consistency and accuracy across multiple systems and assets. We introduced a shared asset schema format as a logical representation of the asset schema that can be translated back and forth with the system-specific format. Additionally, it offers tools to automatically classify data and send out annotation changes to asset owners for review , effectively managing long-tail systems. Inconsistent definitions (lack of shared understanding) We encountered difficulties with diverse data systems that store data in various formats, and customized data labels that made it challenging to recognize identical data elements when they are stored across multiple systems. We introduced a unified taxonomy of semantic types , which are compiled into different languages. This ensured that all systems can share the same canonical set of labels. Missing annotations (lack of quality) A solution that relied solely on data scanning and pattern matching was prone to false positives due to limited contextual information. For instance, a 64-bit integer could be misclassified as either a timestamp or a user identifier without additional context. Moreover, manual human labeling is not feasible at scale because it relies heavily on individual developers\u2019 expertise and knowledge. We shifted left by combining schematization together with annotations in code , in addition improving and utilizing multiple classification signals . Strict measurements provided precision/recall guarantees. Protection was embedded in everything we built, without requiring every developer to be a privacy expert. Organizational barriers (lack of a unified approach) Meta\u2019s data systems, with their bespoke schematization and practices, posed significant challenges in understanding data across the company. As we navigated complex interactions and with ever evolving privacy requirements, it became clear that fragmented approaches to data understanding hindered our ability to grasp data comprehensively. By collaborating with asset owners to develop intuitive tooling and improve coverage, we tackled adoption barriers such as poor developer experience and inaccurate classification. This effort laid the groundwork for a unified data understanding foundation, which was seamlessly integrated into the developer workflow. As a result, we drove a cultural shift towards reusable and efficient privacy practices, ultimately delivering value to product teams and fostering a more cohesive approach to data management.\n\nWalkthrough : Understanding user data for the \u201cBeliefs\u201d feature in Facebook Dating\n\nTo illustrate our approach and dive into the technical solution, let\u2019s consider a scenario involving structured user data. When creating a profile on the Facebook Dating app, users have the option to include their religious views to help match with others who share similar values.\n\nOn Facebook Dating, religious views are subject to purpose limitation requirements. Our five-step approach to data understanding provides a precise, end-to-end view of how we track and protect sensitive data assets, including those related to religious views:\n\nEven a simple feature can involve data being processed by dozens of heterogenous systems, making end-to-end data protection critical. To ensure comprehensive protection, it is essential to apply the necessary steps to all systems that store or process data, including distributed systems (web systems, chat, mobile and backend services) and data warehouses.\n\nConsider the data flow from online systems to the data warehouse, as shown in the diagram below. To ensure that religious belief data is identified across all these systems, we have implemented measures to prevent its use for any purpose other than the stated one.\n\nStep 1 \u2013 Schematizing\n\nAs part of the PAI initiative, Meta developed DataSchema, a standard format that is used to capture the structure and relationships of all data assets, independent of system implementation. Creating a canonical representation for compliance tools. Understanding DataSchema requires grasping schematization, which defines the logical structure and relationships of data assets, specifying field names, types, metadata, and policies.\n\nImplemented using the Thrift Interface Description Language, DataSchema is compatible with Meta systems and languages. It describes over 100 million schemas across more than 100 data systems, covering granular data units like database tables, key-value stores, data streams from distributed systems (such as those used for logging), processing pipelines, and AI models. Essentially, a data asset is like a class with annotated attributes.\n\nLet\u2019s examine the source of truth (SoT) for a user\u2019s dating profile schema, modeled in DataSchema. This schema includes the names and types of fields and subfields:\n\n- user_id (uint) - name (string) - age (uint) - religious_views (enum) - photos (array<struct>): - url (url) - photo (blob) - caption (string) - uploaded_date (timestamp) Dating profile DataSchema\n\nThe canonical SoT schema serves as the foundation for all downstream representations of the dating profile data. In practice, this schema is often translated into system-specific schemas (source of record \u2013 \u201cSoR\u201d), optimized for developer experience and system implementation in each environment.\n\nStep 2 \u2013 Predicting metadata at scale\n\nBuilding on this schematization foundation, we used annotations to describe data, enabling us to quickly and reliably locate user data, such as religious beliefs, across Meta\u2019s vast data landscape. This is achieved through a universal privacy taxonomy, a framework that provides a common semantic vocabulary for data privacy management across Meta\u2019s apps. It offers a consistent language for data description and understanding, independent of specific programming languages or technologies.\n\nThe universal privacy taxonomy works alongside data classification, which scans systems across Meta\u2019s product family to ensure compliance with privacy policies. These systems use taxonomy labels to identify and classify data elements, ensuring privacy commitments are met and data is handled appropriately according to its classification.\n\nPrivacy annotations are represented by taxonomy facets and their values. For example, an asset might pertain to an Actor.Employee, with data classified as SemanticType.Email and originating from DataOrigin.onsite, not a third party. The SemanticType annotation is our standard facet for describing the meaning, interpretation, or context of data, such as user names, email addresses, phone numbers, dates, or locations.\n\nBelow, we illustrate the semantic type taxonomy node for our scenario, Faith Spirituality:\n\nAs data models and collected data evolve, annotations can become outdated or incorrect. Moreover, new assets may lack annotations altogether. To address this, PAI utilizes various techniques to continuously verify our understanding of data elements and maintain accurate, up-to-date annotations:\n\nOur classification system leverages machine learning models and heuristics to predict data types by sampling data, extracting features, and inferring annotation values. Efficient data sampling, such as Bernoulli sampling, and processing techniques enable scaling to billions of data elements with low-latency classifications.\n\nKey components include:\n\nScheduling component : manages the set of data assets to scan, accommodating different data system architectures by either pulling data via APIs or receiving data pushed directly into the scanning service.\n\nScanning service : processes and analyzes data from various sources by accumulating samples in memory, deserializing rows (e.g., JSON) into fields and sub-fields, and extracting features using APIs available in multiple languages (C++, Python, Hack). It ensures comprehensive data capture, even for ephemeral data.\n\nClassification service : utilizes heuristic rules and machine learning models to classify data types with high accuracy. Heuristic rules : handle straightforward, deterministic cases by identifying specific data formats like dates, phone numbers, and user IDs. Machine learning models : trained on labeled datasets using supervised learning and improved through unsupervised learning to identify patterns and anomalies in unlabeled data. Ground truth calibration and verification : ensures system accuracy and reliability, allowing for model fine-tuning and improved classification performance.\n\nLineage and propagation: We integrate classification rules with high-confidence lineage signals to ensure accurate data tracking and management. Our propagation mechanism enables the seamless annotation of data as needed, ensuring that exact copies of data across systems receive equivalent classification. This approach not only maintains data integrity but also optimizes the developer experience by streamlining the process of managing data classifications across our diverse systems.\n\nStep 3 \u2013 Annotating\n\nThe integration of metadata predictions and developer input creates a comprehensive picture of a data asset\u2019s structure (schema) and its meaning (annotation). This is achieved by attaching these elements to individual fields in data assets, providing a thorough understanding of the data.\n\nBuilding on the predicting data at scale initiative (step 2), where we utilize the universal privacy taxonomy and classification systems to identify and classify data elements, the generated metadata predictions are then used to help developers annotate their data assets efficiently and correctly.\n\nPortable annotation APIs: seamlessly integrate into developer workflows ensuring:\n\nConsistent representation of data across all systems at Meta.\n\nAccurate understanding of data, enabling the application of privacy safeguards at scale.\n\nEfficient evidencing of compliance with regulatory requirements.\n\nMetadata predictions and developer input: Two key components work together to create a comprehensive data asset picture:\n\nMetadata predictions : Classifiers generate predictions to aid developers in annotating data assets efficiently and correctly. If the confidence score exceeds a certain threshold, assignment can be automated, saving developer time.\n\nDeveloper input : Developers manually refine and verify annotations, ensuring that the data\u2019s context and privacy requirements are accurately captured. Human oversight guarantees the accuracy and reliability of the data asset picture.\n\n- user_id (enum) \u2192 SemanticType::id_userID - name (string) \u2192 SemanticType::identity_name - age (uint) \u2192 SemanticType::age - religious_views (enum) \u2192 SemanticType::faithSpirituality - photos (array<struct>): - url (url) \u2192 SemanticType::electronicID_uri_mediaURI_imageURL - photo (blob) \u2192 SemanticType::media_image - caption (string) \u2192 SemanticType::media_text_naturalLanguageText - uploaded_date (timestamp) \u2192 SemanticType::uploadedTime\n\nEnsuring complete schemas with annotations: To maintain a high standard of data understanding, we have integrated data understanding into our data model lifecycle. This includes auto-generating code to represent the schema of newly created assets when missing, ensuring that no new assets are created without a proper schema.\n\nFor example, in the context of our religious beliefs in Facebook Dating, we have defined its structure, including fields like \u2018Name,\u2019 \u2018EmailAddress,\u2019 and \u2018Religion.\u2019 Furthermore, we have annotated the asset with Actor::user(), signifying that the data pertains to a user of our products. This level of detail enables us to readily identify fields containing privacy-related data and implement appropriate protective measures, such as applying the applicable purpose limitation policy.\n\nIn the case of the \u201cdating profile\u201d data asset, we have defined its structure, including fields like \u2018Name\u2019:\n\nfinal class DatingProfileSchema extends DataSchemaDefinition { <<__Override>> public function configure(ISchemaConfig $config): void { $config->metadataConfig()->description('Represents a dating profile); $config->annotationsConfig()->annotations(Actor::user()); } <<__Override>> public function getFields(): dict<string, ISchemaField> { return dict[ 'Name' => StringField::create(\"name\") ->annotations(SemanticType::identity_name()) ->example('John Doe'), 'Age' => StringInt::create('age') ->description(\u201cThe age of the user.\u201d) ->annotations(SemanticType::age()) ->example('24'), 'ReligiousViews' => EnumStringField::create('religious_views') ->annotations(SemanticType::faithSpirituality()) ->example('Atheist'), ]; } }\n\nIn order to optimize for developer experience, the details of the schema representation differ in each environment. For example, in the data warehouse, it\u2019s represented as a Dataset \u2013 an in-code Python class capturing the asset\u2019s schema and metadata. Datasets provide a native API for creating data pipelines.\n\nHere is an example of such a schema:\n\n\u200b\u200b@hive_dataset( \"dim_all_dating_users\", // table name \"dating\", // namespace oncall=\"dating_analytics\", description=\"This is the primary Dating user dimension table containing one row per Dating user per day along with their profile, visitation, and key usage information.\", metadata=Metadata(Actor.User), ) class dim_all_dating_users(DataSet): ds: Varchar = Partition(\"datestamp\") userid: DatingUserID = Column(\"User id of the profile\") email: EmailAddress = Column(\"User's email address\"), age: PersonAge = Column(\"User's stated age on date ds\") religious_views: ReligionOptions = Column(\"User's provided religious views\")\n\nOur warehouse schema incorporates rich types, a privacy-aware type system designed to enhance data understanding and facilitate effective data protection. Rich types, such as DatingUserID, EmailAddress, PersonAge, and ReligionOptions, are integrated into the schema, offering a comprehensive approach to data management while encoding privacy metadata. They provide a developer-friendly way to annotate data and enable the enforcement of data quality rules and constraints at the type level, ensuring data consistency and accuracy across the warehouse. For instance, they can detect issues like joining columns with different types of user IDs or mismatched enums before code execution.\n\nHere is an example definition:\n\nReligionOptions = enum_from_items( \"ReligionOptions\", items=[ EnumItem(\"Atheist\", \"Atheist\"), EnumItem(\"Buddhist\", \"Buddhist\"), EnumItem(\"Christian\", \"Christian\"), EnumItem(\"Hindu\", \"Hindu\"), EnumItem(\"Jewish\", \"Jewish\"), EnumItem(\"Muslim\", \"Muslim\"), ... ], annotations=(SemanticType.faithSpirituality,), )\n\nStep 4 \u2013 Inventorying assets and systems\n\nA central inventory system is crucial for managing data assets and their metadata, offering capabilities like search and compliance tracking. Meta\u2019s OneCatalog is a comprehensive system that discovers, registers, and enumerates all data assets across Meta\u2019s apps, providing inventory for easier management and tracking.\n\nKey functions of OneCatalog:\n\nRegistering all data systems : OneCatalog defines a data system as a logical abstraction over resources that persist data for a common purpose. It exhaustively examines resources across Meta\u2019s environments to discover and register all data systems hosting data assets.\n\nEnumerating all data assets : Eligible data systems must enumerate their assets through the asset enumeration platform, generating a comprehensive list of assets and their metadata in the central inventory. These assets are grouped by \u201casset classes\u201d based on shared patterns, enabling efficient management and understanding of data assets.\n\nGuarantees provided by OneCatalog:\n\nCompleteness: The system regularly checks for consistency between the data defined in its configuration and the actual data stored in the inventory. This ongoing comparison ensures that all relevant data assets are accurately accounted for and up-to-date.\n\nFreshness: In addition to regularly scheduled pull-based enumeration, the system subscribes to changes in data systems and updates its inventory in real time.\n\nUniqueness of asset ID (XID): Each asset is assigned a globally unique identifier, similar to URLs, which facilitates coordination between multiple systems and the exchange of information about assets by providing a shared key. The globally unique identifier follows a human-readable structure, e.g., asset://[asset-class]/[asset-name].\n\nUnified UI: On top of the inventory, OneCatalog provides a unified user interface that consolidates all asset metadata, serving as the central hub for asset information. This interface offers a single point of access to view and manage assets, streamlining the process of finding and understanding data.\n\nFor example, in the context of our \u201creligious beliefs in the Dating app\u201d scenario, we can use OneCatalog\u2019s unified user interface to view the warehouse dating profile table asset, providing a comprehensive overview of its metadata and relationships.\n\nCompliance and privacy assurance: OneCatalog\u2019s central inventory is utilized by various privacy teams across Meta to ensure that data assets meet requirements. With its completeness and freshness guarantees, OneCatalog serves as a reliable source of truth for privacy and compliance efforts.\n\nBy providing a single view of all data assets, OneCatalog enables teams to efficiently identify and address potential risks or vulnerabilities, such as unsecured data or unauthorized access.\n\nStep 5 \u2013 Maintaining data understanding\n\nTo maintain high coverage and quality of schemas and annotations across Meta\u2019s diverse apps, we employed a robust process that involves measuring precision and recall for both predicted metadata and developer-provided annotations. This enables us to guide the implementation of our privacy and security controls and ensure their effectiveness.\n\nBy leveraging data understanding, tooling can quickly build end-to-end compliance solutions. With schema and annotations now front and center, we\u2019ve achieved continuous understanding, enabling our engineers to easily track and protect user data, implement various security and privacy controls, and build new features at scale.\n\nOur strategy for maintaining data understanding over time includes:\n\nShifting left on creation time : We provided intuitive APIs for developers to provide metadata at asset creation time, ensuring that schemas and annotations were applied consistently in downstream use cases.\n\nDetecting and fixing annotation gaps : We surfaced prediction signals to detect coverage and quality gaps and evolved our prediction and annotation capabilities to ensure new systems and workflows were covered.\n\nCollecting ground truth : We established a baseline to measure automated systems against, with the help of subject matter experts, to continuously measure and improve them.\n\nProviding canonical consumption APIs : We developed canonical APIs for common compliance usage patterns, such as detecting user data, to ensure consistent interpretation of metadata and low entry barriers.\n\nPutting it all together\n\nComing back to our scenario: As developers on the Facebook Dating team collect or generate new data, they utilize familiar APIs that help them schematize and annotate their data. These APIs provide a consistent and intuitive way to define the structure and meaning of the data.\n\nWhen collecting data related to \u201cFaith Spirituality,\u201dthe developers use a data classifier that confirms their semantic type annotations once the data is scanned during testing. This ensures that the data is accurately labeled and can be properly handled by downstream systems.\n\nTo ensure the quality of the classification system, ground truth created by subject matter experts is used to measure its accuracy. A feedback loop between the product and PAI teams keeps the unified taxonomy updated, ensuring that it remains relevant and effective.\n\nBy using canonical and catalogued metadata, teams across Meta can implement privacy controls that are consistent and effective. This enables the company to maintain user trust and meet requirements.\n\nIn this scenario, the developers on the Facebook Dating team are:\n\nSchematizing and annotating their data using familiar APIs.\n\nUsing a data classifier to confirm semantic type annotations.\n\nLeveraging ground truth to measure the quality of the classification system.\n\nUtilizing a feedback loop to keep the unified taxonomy updated.\n\nImplementing privacy controls using canonical and catalogued metadata.\n\nLearnings and takeaways\n\nBuilding an understanding of all data at Meta was a monumental effort that not only required novel infrastructure but also the contribution of thousands of engineers across all teams at Meta, and years of investment.\n\nCanonical everything : Data understanding at scale relies on a canonical catalog of systems, asset classes, assets, and taxonomy labels, each with globally unique identifiers. This foundation enables an ecosystem of compliance tooling, separating the concerns of data understanding from consuming canonical metadata.\n\nIncremental and flexible approach : To tackle the challenge of onboarding hundreds of systems across Meta, we developed a platform that supports pulling schemas from existing implementations. We layered solutions to enhance existing untyped APIs , meeting developers where they are\u2014whether in code, configuration, or a UI defining their use case and data model. This incremental and flexible approach delivers value at every step.\n\nCollaborating for data classification excellence : Building the platform was just the beginning. The infrastructure and privacy teams also collaborated with subject matter experts to develop best-in-class classifiers for our data, addressing some of the most challenging problems. These include detecting user-generated content, classifying data embedded in blobs, and creating a governed taxonomy that allows every developer to describe their data with the right level of detail.\n\nCommunity engagement with a tight feedback loop : Our success in backfilling schemas and integrating with the developer experience was made possible by a strong partnership with product teams. By co-building solutions and establishing an immediate feedback loop, we refined our approach, addressed misclassifications, and improved classification quality. This collaboration is crucial to our continued evolution and refinement of data understanding.\n\nThe future of data understanding\n\nData understanding has become a crucial component of Meta\u2019s PAI initiative, enabling us to protect user data in a sustainable and effective manner. By creating a comprehensive understanding of our data, we can address privacy challenges durably and more efficiently than traditional methods.\n\nOur approach to data understanding aligns closely with the developer workflow, involving the creation of typed data models, collection of annotated data, and processing under relevant policies. At Meta\u2019s scale, this approach has saved significant engineering effort by automating annotation on millions of assets (i.e., fields, columns, tables) with specific labels from an inventory that are deemed commitment-critical. This automation has greatly reduced the manual effort required for annotation, allowing teams to focus on higher-priority tasks.\n\nAs data understanding continues to evolve, it is expected to have a significant impact on various aspects of operations and product offerings. Here are some potential future use cases:\n\nImproved AI and machine learning : leveraging data understanding to improve the accuracy of AI-powered content moderation and recommendation systems.\n\nStreamlined developer workflows : integrating data understanding into Meta\u2019s internal development tools to provide clear data context and reduce confusion.\n\nOperational and developer efficiency : By automating data classification and annotation for millions of assets across Meta\u2019s platforms, we can significantly improve operational efficiency. This automation enables us to leverage metadata for various use cases, such as accelerating product innovation. For instance, we\u2019re now utilizing this metadata to help developers efficiently find the right data assets, streamlining their workflow and reducing the time spent on manual searches.\n\nProduct innovation : With a comprehensive understanding of data, Meta can drive product innovation by leveraging insights to create personalized and engaging user experiences.\n\nWhile there is still more work to be done, such as evolving taxonomies to meet future compliance needs and developing novel ways to schematize data, we are excited about the potential of data understanding. By harnessing canonical metadata, we can deepen our shared understanding of data, unlocking unprecedented opportunities for innovation not only at Meta, but across the industry.\n\nAcknowledgements\n\nThe authors would like to acknowledge the contributions of many current and former Meta employees who have played a crucial role in developing data understanding over the years. In particular, we would like to extend special thanks to (in alphabetical order) Aaron Morris, Adrian Zgorzalek, Alex Gorelik, Alex Kalinin, Alex Uslontsev, Ali Fakeri Tabrizi, Amit Sarkar, Anchit Arora, Andras Belokosztolszki, Anthony O\u2019Sullivan, Archit Jain, Aygun Aydin, Ayoade Adeniyi, Ben Warren, Bob Baldwin, Brani Stojkovic, Brian Romanko, Can Lin, Carrie (Danning) Jiang, Chao Yang, Chris Ventura, Daniel Ohayon, Danny Gagne, David Taieb, Dmitry Ponomarev, Dong Jia, Dong Zhao, Eero Neuenschwander, Fang Wang, Ferhat Sahinkaya, Ferdi Adeputra, Fred Liu, Gayathri Aiyer, George Stasa, Guoqiang Jerry Chen, Haiyang Han, Haydar Imren, Henry Swanson, Ian Carmichael, Jared Greene, Jerry Pan, Jiang Wu, Johnnie Ballentyne, Joanna Jiang, Jonathan Bergeron, Joseph Li, Jun Fang, Kaustubh Karkare, Komal Mangtani, Kuldeep Chaudhary, Kunal Kataria, Lea Li, Lei Zhang, Liu Yang, Loka Potnuru, Luiz Ribeiro, Marc Celani, Matthieu Martin, Max Mazzeo, Meg Dymek, Mellany Flores, Mike Tarasyuk, Mital Mehta, Nevzat Sevim, Nick Gardner, Nikolay Kondratyev, Oliver Dodd, Pankaj Landge, Perry Stoll, Peter Nieuwenhuizen, Pranet Verma, Prashanth Bandaru, Piyush Khemka, Rahul Nambiar, Rajesh Nishtala, Rituraj Kirti, Roger (Wei) Li, Rujin Cao, Sahil Garg, Satish Sampath, Sean Wang, Seth Silverman, Shridhar Iyer, Simran Patil, Sriguru Chakravarthi, Sushaant Mujoo, Susmit Biswas, Taha Bekir Eren, Tejas Kudrimoti, Tony Harper, Vineet Chaudhary, Vishal Jain, Vitali Haravy, Vlad Fedorov, Vlad Gorelik, Wolfram Schuttle, Xiaotian Guo, Yatu Zhang, Yi Huang, Yuxi Zhang, Zejun Zhang, and Zhaohui Zhang. We would also like to express our gratitude to all reviewers of this post, including (in alphabetical order) Aleksandar Ilic, Avtar Brar, Brianna O\u2019Steen, Chloe Lu, Chris Wiltz, Imogen Barnes, Jason Hendrickson, Rituraj Kirti, Xenia Habekoss and Yuri Claure. We would like to especially thank Jonathan Bergeron for overseeing the effort and providing all of the guidance and valuable feedback, and Ramnath Krishna Prasad for pulling required support together to make this blog post happen.", "label": 0}
{"title": "Introducing sub-issues: Enhancing issue management on GitHub", "url": "https://github.blog/engineering/architecture-optimization/introducing-sub-issues-enhancing-issue-management-on-github/", "content": "Recently we launched sub-issues, a feature designed to tackle complex issue management scenarios. This blog post delves into the journey of building sub-issues, what we learned along the way, how we implemented sub-issues, and the benefits of being able to use sub-issues to build itself.\n\nWhat are sub-issues?\n\nSub-issues are a way to break a larger issue into smaller, more manageable tasks. With this feature, you can now create hierarchical lists within a single issue, making it easier to track progress and dependencies. By providing a clear structure, sub-issues help teams stay organized and focused on their goals.\n\nFor example, I often realize that a batch of work requires multiple steps, like implementing code in different repositories. Breaking this task into discrete sub-issues makes it easier to track progress and more clearly define the work I need to do. In practice we\u2019ve noticed this helps keep linked PRs more concise and easier to review.\n\nA brief history\n\nIssues have long been at the heart of project management on GitHub. From tracking bugs to planning feature development, issues provide a flexible and collaborative way for teams to organize their work. Over time, we\u2019ve enriched this foundation with tools like labels, milestones, and task lists, all to make project management even more intuitive and powerful.\n\nOne of the key challenges we set out to solve was how to better represent and manage hierarchical tasks within issues. As projects grow in complexity, breaking down work into smaller, actionable steps becomes essential. We want to empower users to seamlessly manage these nested relationships while maintaining the simplicity and clarity GitHub is known for.\n\nOur journey toward sub-issues began with a fundamental goal: to create a system that integrates deeply into the GitHub Issues experience, enabling users to visually and functionally organize their work without adding unnecessary complexity. Achieving this required careful design and technical innovation.\n\nBuilding sub-issues\n\nTo build sub-issues, we began by designing a new hierarchical structure for tasks rather than modifying the existing task list functionality. We introduced the ability to nest tasks within tasks, creating a hierarchical structure. This required updates to our data models and rendering logic to support nested sub-issues.\n\nFrom a data modeling perspective, the sub-issues table stores the relationships between parent and child issues. For example, if Issue X is a parent of Issue Y, the sub-issues table would store this link, ensuring the hierarchical relationship is maintained.\n\nIn addition, we roll up sub-issue completion information into a sub-issue list table. This allows us to performantly get progress without having to traverse through a list of sub-issues. For instance, when Issue Y is completed, the system automatically updates the progress of Issue X, eliminating the need to manually check the status of all sub-issues.\n\nWe wanted a straightforward representation of sub-issues as relationships in MySQL. This approach provided several benefits, including easier support for sub-issues in environments like GitHub Enterprise Server and GitHub Enterprise Cloud with data residency.\n\nWe exposed sub-issues through GraphQL endpoints, which let us build upon the new Issues experience and leverage newly crafted list-view components. This approach provided some benefits, including more efficient data fetching and enhanced flexibility in how issue data is queried and displayed. Overall, we could move faster because we reused existing components and leveraged new components that would be used in multiple features. This was all made possible by building sub-issues in the React ecosystem.\n\nWe also focused on providing intuitive controls for creating, editing, and managing sub-issues. To this end, we worked closely with accessibility designers and GitHub\u2019s shared components team that built the list view that powers sub-issues.\n\nOur goal was to make it as easy as possible for users to break down their tasks without disrupting their workflow.\n\nUsing sub-issues in practice\n\nDogfooding is a best practice at GitHub and it\u2019s how we build GitHub! We used sub-issues extensively within our own teams throughout the company to manage complex projects and track progress. Having a discrete area to manage our issue hierarchy resulted in a simpler, more performant experience. Through this hands-on experience, we identified areas for improvement and ensured that the feature met our high standards.\n\nOur teams found that sub-Issues significantly improved their ability to manage large projects. By breaking down tasks into smaller, actionable items, they maintained better visibility and control over their work. The hierarchical structure also made it easier to identify dependencies and ensure nothing fell through the cracks.\n\nGathering early feedback\n\nBuilding sub-issues was a team effort. Feedback from our beta testers was instrumental in shaping the final product and ensuring it met the needs of our community. For example, understanding how much metadata to display in the sub-issue list was crucial. We initially started with only issue titles, but eventually added the issue number and repository name, if the issue was from another repository.\n\nBuilding features at GitHub makes it really easy to improve our own features as we go. It was really cool to start breaking down the sub-issues work using sub-issues. This allowed us to experience the feature firsthand and identify any pain points or areas for improvement. For example, the has:sub-issues-progress and has:parent-issue filters evolved from early discussions around filtering syntax. This hands-on approach ensured that we delivered a polished and user-friendly product.\n\nThese lessons have been invaluable in not only improving sub-issues, but also in shaping our approach to future feature development. By involving users early and actively using our own features, we can continue to build products that truly meet the needs of our community. These practices will be important to our development process going forward, ensuring that we deliver high-quality, user-centric solutions.\n\nCall to action\n\nSub-issues are designed to help you break down complex tasks into manageable pieces, providing clarity and structure to your workflows. Whether you\u2019re tracking dependencies, managing progress, or organizing cross-repository work, sub-issues offer a powerful way to stay on top of your projects.\n\nWe\u2019d love for you to try sub-issues and see how they can improve your workflow. Your feedback is invaluable in helping us refine and enhance this feature. Join the conversation in our community discussion to share your thoughts, experiences, and suggestions.\n\nThank you for being an integral part of the GitHub community. Together, we\u2019re shaping the future of collaborative development!\n\nTags:", "label": 0}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2022-07", "content": "en\n\nThe Metro Experience with a Jingle This is part of a three or four part blog series. These posts will be interactive on my website (https://lifeofpablo.com) . I am in the process of rebuilding my website so stayed tuned.\n\nMy name is Pablo Morales. I used to teach within Omaha Public Schools but now I teach in Sacramento, CA. I still teach French & Spanish to middle schoolers.\n\nI have always been a fan of public transportation! Within the United States, commuting to work seems painfully dreadful. We are a nation dominated by cars and bad policies. Our public transportation infrastructure is not good for a very \u201cdeveloped\u201d country. Our roads are suffering, and everything else in-between isn\u2019t pleasant either. This isn't the case in South Korea. I'm only a week or so in since arriving in South Korea. I am absolutely M-I-N-D blown on what I am seeing and hearing.. My brain can't handle this!\n\nCommuting in Seoul, Korea is not about trying to catch your bus or train, you are there for the experience of being in a station. You're probably wondering, \"Pablo, Isn't the topic over public transportation\u2026.you know boring?\" I say, \"Absolutely Not!\" I've been to multiple cities who don't even come close to the Korean experience of public transportation. My favorite part about using the Seoul Metro System is that you get to hear all the cute and relaxing rings and jingles offered at all metro stations and some bus stations.\n\nCommuting is hard on the body when traveling far. It's hot and humid out, or you're just simply tired. Those little jingles give you a glimmer of hope that you are getting closer to your final destination. You stay motivated instead of only hearing the usual \"This train is departing.\" or departing. It brightens the mood. My two favorite jingles or sounds are: The Trumpet Link A steelpan Link Do you see what I mean? It definitely made me smile. Everyone seems so happy. I knew traveling within Seoul was going to be fun but who knew these little jingles are what make the experience of the metro in Seoul.\n\nHere is a handpicked few of my favorite jingles. Not all are in circulation at the moment.\n\nLink\n\nLink\n\nBeing a minority in the US and still being a minority in South Korea.\n\nSomeone who speaks 3 different languages. Learning the Korean language is easy in some aspects and difficult in others. I teach French & Spanish. The one common thing between these two languages is the alphabet - romanized letters. The Korean Language is written in Hangul.\n\nI was an ELL student. Even. I now understand what it is like to be in my student\u2019s mind.", "label": 1}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2016", "content": "en\n\n\"Over the years Greek life ( or more commonly known now as Fraternity & Sorority Life), has been in the news spotlight. Unfortunately, its mostly negative news. There is truly a good side to Greek Life. I mean I was once part of the system. Then again once a Greek always Greek, right? Let me tell you my story and experiences in Greek Life.\n\nI am Pablo and I affiliate myself with the Alpha Tau Omega Fraternity (ATO or AT\u03a9.) I am proud to say that I joined the ATOs, even if it was short lived. I am sad that my fraternity is no longer on campus. Our nonexistence is not due to being under terrible spotlight like many fraternites or sororities do.\n\nDuring my active time in Greek life, I gained so much! Sure we had small numbers but I would not trade it for the world. I met friends that still to this day I talk with today! I met so many great people in the the Greek community. I am truly a social person, so I am not afraid to come meet new people.\n\nCollege is all about opening your up doors to different perspectives and finding yourself.\n\nThere you have it, a side of me that does not come up much anymore! When I do talk about it, I am always proud to look back at something I was proud to be in! I may not be active anymore, but I am proud to say that I, Pablo Morales, am Greek!\"", "label": 1}
{"title": "Public Education in Nebraska", "url": "https://lifeofpablo.com/blog/public-education-in-nebraska", "content": "Public Education in Nebraska\n\nThis post was written in English (en_US).\n\nIn ResponseGetting creative to do good in politics by Tracy Durnell\n\nI'll keep this brief.\n\nEducation is very important me. Education has been ingrained in me I was the first to graduate in my family from university. I studied to become a public school teacher. I became a public school teacher early in my career. I taught during the COVID pandemic during my first year of becoming a public school teacher.\n\nPublic school education is important to me.\n\nIt makes me sad to see what Nebraska, my home state, has become regarding public education. Now that I live in California, I feel, indirectly, I am standing on the sidelines seeing how my home-sate wants to divert public funds or public taxes to fund private institutions. We live in a time where we can't properly fund, nor respect our own public schools across the nation. The de-funding public schools has been in the works as far back as the 1980s. Other efforts happened before this as well. This has been most predominantly since the Reagan Administration. The Reagan Administration slashed funding for public education across the board.\n\nIt's embarrassing we are trying to divert money from a great public school system from the State of Nebraska as it ranks one of the highest in the nation. I myself am a product of a Nebraska Public Education. We need to fix the issues happening, not make the problems worse.\n\nWhy should we fund private institutions? Why should we fund religious public institutions? Shouldn't we have separation of church and state? Public funds should stay in public schools. It's like they are trying to make a problem worse? Why keep under funding the public school system?\n\nNone of the private schools in my hometown, offered the subjects I was interested. I lived not too far from a Catholic school. They offered the \"bare minimum.\" They didn't offer other world languages beyond Spanish. They didn't offer auto mechanics or other trades (attending trade school is just as important as university. This is discussion for another time.) What is the likelihood of them expanding their programs?\n\nMany counties in Nebraska don't even have private institutions. How is the state going to give private institutions money to education students when the facilities don't exist? Here's a news report\n\nPrivate education will increase inequality.\n\nPublic schools have been there for everyone. No matter your socioeconomic status. It was there for me. Public schools are there to help the most disadvantaged.\n\nI agree, with the phrase, \"Society is cheaping out on the kids\" as written by Tracy Durnell, \"We must fight the privatisation of school to provide racial equity, class mobility, and teaching reality instead of \u2018alternative facts.\" We need to invest in students and not let them down.\n\nHow the State Currently get funding\n\nNebraska Public Media does a great job, visually of reporting how schools are funded in Nebraska.\n\nGraphic from Nebraska Public Media: Nebraska relies heavily on property taxes to fund its public schools, compared to other states. (Graphic courtesy of OpenSky Policy Institute, uses 2019-2020 data\n\nMore state funding on the way for public schools in Nebraska\n\nAdding funding to public schools is only one piece of the puzzle, we need to also address the systems, bureaucracies, institutions, and bad polices that exist, past and present.\n\nI don't like to be cynical, I am scared what is coming. People need to become informed in what is happening in their community. People need to read the bills introduced in their respective level of government. We must continue to fight!", "label": 1}
{"title": "Copilot for all: Introducing Microsoft 365 Copilot Chat", "url": "https://www.microsoft.com/en-us/microsoft-365/blog/2025/01/15/copilot-for-all-introducing-microsoft-365-copilot-chat/", "content": "Our ambition is to empower every employee with a Copilot and to transform every business process with agents. From Dow to Disney, companies are going big with Microsoft 365 Copilot and agents, uncovering key scenarios that can deliver real ROI. Now, organizations of all sizes are looking to scale their AI transformation and realize the enterprise-wide ROI that comes with broad adoption.\n\nToday, we\u2019re introducing Microsoft 365 Copilot Chat, a new offering that adds pay-as-you-go agents to our existing free chat experience for Microsoft 365 commercial customers.1 Copilot Chat enables your entire workforce\u2014from customer service representatives to marketing leads to frontline technicians\u2014to start using Copilot and agents today. It includes:\n\nFree, secure AI chat powered by GPT-4o.\n\npowered by GPT-4o. Agents accessible right in the chat.\n\naccessible right in the chat. IT controls, including enterprise data protection and agent management.\n\nMoving forward, every organization will have a mix of Copilot Chat and Microsoft 365 Copilot\u2014our best-in-class offering\u2014to drive AI transformation at scale. Let\u2019s walk through the new product lineup.\n\nClick to enlarge\n\nCopilot Chat: The power of chat + agents\n\nCopilot is the UI for AI, and it all starts with Copilot Chat. It\u2019s the chat experience you\u2019ll use every day\u2014powered by broad knowledge from the web, built on GPT-4o, and designed to be safe and secure for business use. It represents a foundational shift in how we work, enabling everyone to work smarter, faster, and more collaboratively.\n\nCopilot Chat includes:\n\nWeb-grounded chat with GPT-4o. You can use it to do market research, write a strategy document, or prepare for a meeting. File uploads allow you to add any document to the chat and ask Copilot to do things like summarize key points in a Word document, analyze data in an Excel spreadsheet, and suggest improvements to a PowerPoint presentation. 2 With Copilot Pages , you can collaborate on content with people and AI in real time\u2014adding content from Copilot, your files, and now from the web as well. And you can quickly create AI-generated images for campaigns, product launches, and social media posts. 3\n\nwith GPT-4o. You can use it to do market research, write a strategy document, or prepare for a meeting. allow you to add any document to the chat and ask Copilot to do things like summarize key points in a Word document, analyze data in an Excel spreadsheet, and suggest improvements to a PowerPoint presentation. With , you can collaborate on content with people and AI in real time\u2014adding content from Copilot, your files, and now from the web as well. And you can quickly create for campaigns, product launches, and social media posts. Agents . Using natural language, now anyone can easily create agents to automate repetitive tasks and business processes\u2014directly in Copilot Chat. A customer service representative can ask a customer relationship management (CRM) agent for account details before a customer meeting, while field service agents can access step-by-step instructions and real-time product knowledge stored in SharePoint. Agents are priced on a metered basis, and IT stays in control. IT admins can also build organization-wide agents and manage agent deployment, all powered by Microsoft Copilot Studio.\n\n. Using natural language, now anyone can easily create agents to automate repetitive tasks and business processes\u2014directly in Copilot Chat. A customer service representative can ask a customer relationship management (CRM) agent for account details before a customer meeting, while field service agents can access step-by-step instructions and real-time product knowledge stored in SharePoint. Agents are priced on a metered basis, and IT stays in control. IT admins can also build organization-wide agents and manage agent deployment, all powered by Microsoft Copilot Studio. Copilot Control System. Copilot Chat includes foundational capabilities of the Copilot Control System, including enterprise data protection (EDP) for data privacy and security and the ability to govern access and manage the usage and lifecycle of Copilot and agents, as well as measurement and reporting.\n\nWhile Copilot Chat is a powerful new on-ramp for everyone in your organization to build the AI habit, Microsoft 365 Copilot remains our best-in-class personal AI assistant for work. It includes everything in Copilot Chat and more. Microsoft 365 Copilot combines the power of GPT-4o grounded in your work data\u2014all your meetings, emails, chats, documents, and more; Copilot in the Microsoft 365 apps that millions rely on every day\u2014Microsoft Teams, Outlook, Word, Excel, and PowerPoint; and usage and access to agents. And we continue to rapidly add new capabilities like Copilot Actions to tackle people\u2019s biggest pain points at work. We\u2019ve empowered every IT team to lead and manage at scale with the Copilot Control System and Copilot Analytics to measure the impact and ROI of your Copilot investment.\n\nCustomers can get started with either the free or paid experience in the Microsoft 365 Copilot app, available at m365copilot.com or in the Windows, Android, or iPhone app stores.\n\nThese announcements enable every customer to accelerate their AI transformation and realize enterprise-wide ROI. Now, every employee has a Copilot and a team of agents to scale their impact.\n\nTry Copilot Chat at m365copilot.com and visit WorkLab for the latest research and insights on AI and the future of work.\n\nDownload the Microsoft 365 Copilot app Take the power of AI on the go with the Copilot app. Download today\n\nFootnotes:\n\n1 Learn more about Copilot Chat eligibility.\n\n2 File upload limits apply\n\n3 Image generation limits apply", "label": 0}
{"title": "Test clocks: How we made it easier to test Stripe Billing integrations", "url": "https://stripe.com/blog/test-clocks-how-we-made-it-easier-to-test-stripe-billing-integrations", "content": "Stripe Billing allows businesses to manage customer relationships with recurring payments, usage triggers, and other customizable features.\n\nThese are key processes for any business, and for that reason businesses need to validate that their Stripe Billing integrations behave as they expect. But integrations are often error prone due to common misconceptions about time: days always have 24 hours (not when we change the clocks twice a year), February always has 28 days (true, except for leap years), timestamps will always be in the same format (yyyy-mm-dd hh:mm:ss is the default, but it\u2019s not universal), system clocks are always set to the right time, and so on.\n\nBilling integrations are also difficult to test. Historically, the only way to run a test was to wait for time to pass\u2014typically by creating test configurations with shorter subscription cycles than real production systems, or running 10-second trials to force subscriptions to cycle\u2014and then look for any bugs that might surface in the course of normal business. Of course, doing anything that is not a perfect mirror of your production system is a shaky foundation to build on.\n\nWe sought to address these challenges with the launch of test clocks, which allow users to simulate the passage of time in Billing scenarios without waiting for actual seconds to tick by in the real world. A test clock, when associated with Customer objects, allows users to associate a time reference with each Customer and its associated Billing resources. When the test clock runs, the Subscription and Invoice objects will behave as if time has actually passed, changing states and triggering webhooks. With a test clock, users can\u2014for example\u2014perform the leap-year test with just a few API calls or clicks in the Dashboard.\n\nThis blog discusses the technical details of how we built test clocks in Billing, and how we updated Stripe systems to account for the different ways that time passes.\n\nConceptualizing a hybrid logical clock\n\nWe often think about time as we see it on a real-world clock or a calendar\u2014seconds, minutes, days, months, and years pass by in steady succession. We say that time \u201cflows,\u201d much like a boat down a river. But we could also think of time as advancing through a sequence of events, each of which happens to occur at a particular timestamp. This combination of physical timestamps from real-world clocks with ordered and meaningful events from logical clocks creates a hybrid logical clock.\n\nThis hybrid logical clock is based on a concept of time that is useful for any system in a test environment, because it makes it easy to fast-forward to the most important future moments in a billing system. Instead of advancing through 30 days of normal clock time, which requires traversing all the seconds in between the current time and that future time, you can just advance your clock to the next \u201cevent\u201d\u2014the next monthly billing date.\n\nRather than flowing down the river, the boat can teleport directly to a meaningful event. As a result, the computational cost of advancing the clock is significantly reduced.\n\nBuilding a hybrid logical clock\n\nOne challenge in building a hybrid logical clock is that it\u2019s difficult to know beforehand the total set of meaningful events, and the order in which they will occur, because any event can trigger state changes.\n\nFor example, in a scenario where customers are on a monthly recurring plan, and the business suddenly decides to credit the first 10 days free, the system needs to be flexible enough to accommodate this change in state. It is equally possible that the business may want to maintain the subscription cycle, or they may want to advance the subscription cycle by 10 days\u2014and change the time for issuing invoices.\n\nThis needs to be taken into account when figuring out the next meaningful event. So when a user advances the time of a test clock, we repeat an \u201cadvance\u201d function under the hood:\n\nGiven a test clock, compute the next meaningful time for all of the objects that use it. For example, this might be the next date to Invoice for each Subscription associated with the clock. If the next meaningful time is after the target time , update the frozen time of the test clock to the target time , and stop advancing. If the next meaningful time is before the target time , execute the actions that occur at the next meaningful time and update the frozen time of the test clock to the next meaningful time . Then return to step 1.\n\nIn the above diagram, the test clock starts at 19:00 (the \u201cfrozen time\u201d) with a target time of 06:30 the next day. We start by taking action on Event 2, and set the time to 00:00\u2014processing Event 2 also causes the new Event 2A to be scheduled for 01:30, and we process this as a normal event at 01:30. Since we haven\u2019t reached the target time, we go through the loop again, take action on Event 3, and set the time to 03:00. In the final loop, we notice that Event 4 is past the target time, so we set the clock to 06:30, and stop advancing.\n\nSince test clocks affect only an object\u2019s understanding of the \u201ccurrent\u201d time, they execute the same exact logic that would occur in real time for each meaningful action. Test clocks can thus be used to safely and rigorously test integrations\u2014and we use them internally to test all new features on Billing.\n\nUpdating Stripe systems to understand test clocks\n\nWe made a no-op change to our internal logic to remove dependencies on real-world time, and instead retrieve timestamps from an abstract \u201ctime provider\u201d backed either by a real-world clock or a test clock. This approach means that there is no semantic change in the presentation of Billing objects in the API, and it allows both developers and internal systems to continue relying on existing business logic without changes in behavior.\n\nAfter we changed the basis for retrieving timestamps, we needed to ensure that time-dependent operations were not triggered by the passage of real time for objects with test clocks attached. To do that, we used a scheduling service that looks for database records meeting certain criteria, and which triggers certain events when it finds them.\n\nConsider the example of generating an Invoice for a Subscription that cycles at the start of the month. When a new month begins, the Subscription gets picked up by a part of the scheduling service that looks for Subscriptions whose billing periods have just ended, and triggers the creation of a new Invoice . Objects with an associated test clock, on the other hand, are explicitly filtered out from any database scans done by the asynchronous scheduling service. Instead, we give the test clock full control over orchestration and scheduling.\n\nUsing test clocks in Billing\n\nWith test clocks, you can confidently validate and deploy your business model in a much shorter time, allowing you to get to market faster. Test clocks allow you to safely and quickly validate integrations and can be used for any combination of scenarios within Billing: recurring subscriptions, trials that convert into paid subscriptions, prorations, renewal-payment failures, past-due subscriptions, timed discounts, subscription schedules, and so on.\n\nTo get started, simply create a test clock through the API and attach customers to it. (You can also work with test clocks in the Dashboard.) Any Billing object created under that customer will then be controlled by the test clock, and you can advance time to observe any effects on Billing objects. For more details, check out our documentation.\n\nAnd if you\u2019re interested in building financial infrastructure at Stripe\u2014including products such as Billing\u2014consider joining our engineering team.", "label": 0}
{"title": "Google Cloud donates A2A to Linux Foundation", "url": "https://developers.googleblog.com/en/google-cloud-donates-a2a-to-linux-foundation/", "content": "Today at Open Source Summit North America, the Linux Foundation announced the formation of the Agent2Agent project with Amazon Web Services, Cisco, Google, Microsoft, Salesforce, SAP, and ServiceNow. With the formation of this new, independent entity, the companies will collaborate closely on fostering an open and interoperable ecosystem for AI agents with the Agent2Agent (A2A) protocol and other interoperability technology. The project will be hosted by the Linux Foundation and will be seeded with Google\u2019s transfer of the groundbreaking Agent2Agent (A2A) protocol specification, accompanying SDKs, and developer tooling.\n\nThe A2A protocol, an open standard for communication and collaboration between distinct AI agents, aims to break down the silos that currently limit the potential of artificial intelligence. More than 100 companies now support the protocol, with AWS and Cisco as its newest validators. By providing a common language for AI agents to discover each other\u2019s capabilities, securely exchange information, and coordinate complex tasks, the A2A protocol is paving the way for a new era of more powerful, collaborative, and innovative AI applications.\n\nThe formation of the Agent2Agent project under the neutral governance of the Linux Foundation will ensure that this critical component remains vendor-agnostic and community-driven. This move is designed to accelerate the adoption and development of the A2A protocol by providing a robust framework for open collaboration, intellectual property management, and long-term stewardship.", "label": 0}
{"title": "Imagen 4 is now available in the Gemini API and Google AI Studio", "url": "https://developers.googleblog.com/en/imagen-4-now-available-in-the-gemini-api-and-google-ai-studio/", "content": "We're thrilled to bring Imagen 4, our best text-to-image model yet, to paid preview in the Gemini API and for limited free testing in Google AI Studio. Imagen 4 offers significantly improved text rendering over our prior image models and pushes the boundaries of text-to-image generation quality.\n\n\n\nThe Imagen 4 Family: Imagen 4 and Imagen 4 Ultra\n\nWe\u2019re introducing two models within the Imagen 4 family, built to serve a variety of creative needs:\n\n\n\nImagen 4: Your go-to for most tasks\n\nThis is our flagship text-to-image model designed to handle a wide range of image generation tasks with significant improvements in quality, particularly for text generation, over Imagen 3. Imagen 4 is priced at $0.04 per output image.\n\n\n\nImagen 4 Ultra: Precision for your prompts\n\nWhen you need your images to precisely follow instructions, Imagen 4 Ultra is the model for you. It's designed to produce outputs that are more highly aligned with your text prompts, achieving strong results compared to other leading image generation models. Imagen 4 Ultra is priced at $0.06 per output image.\n\nWe will introduce additional billing tiers in the coming weeks. In the meantime, you can request higher rate limits for Imagen 4 and 4 Ultra.\n\n\n\nSee Imagen 4 in action\n\nTo give you a glimpse of Imagen 4's capabilities, here are some examples of what you can create. Created using Imagen 4 Ultra, the prompts below showcase the model's versatility across various styles and content.\n\nPrompt: A 3-panel cosmic epic comic. Panel 1: Tiny 'Stardust' in nebula; radar shows anomaly (text 'ANOMALY DETECTED'), hull text 'stardust'. Pilot whispers. Panel 2: Bioluminescent leviathan emerges; console red text 'WARNING!. Panel 3: Leviathan chases ship through asteroids; console re text 'SHIELD CRITICAL!', screen text 'EVADE!'. Pilot screams, SFX 'CRUNCH!', 'ROOOOAAARR!'.", "label": 0}
{"title": "Open-sourcing Pyrefly: A faster Python type checker written in Rust", "url": "https://engineering.fb.com/2025/05/15/developer-tools/open-sourcing-pyrefly-a-faster-python-type-checker-written-in-rust/", "content": "Back in 2017, engineers at Meta sought to create a type checker for Instagram\u2019s typed Python codebase. Years later, as the type system continued to evolve, that type checker eventually became Pyrefly.\n\nPyrefly is a new type checker and IDE experience for Python, written with Rust, and now available for the entire Python community to use! It\u2019s open-source, supports both CLI usage and IDE integration. and is designed to help you catch errors before runtime in Python codebases of any size.\n\nOn this episode of the Meta Tech Podcast, Pascal Hartig sits down with Maggie, Rebecca, and Neil \u2014 some of the team behind Pyrefly \u2014 to discuss this latest release from Meta and how they built an incremental type checker that scales to mono repositories.\n\nDownload or listen to the episode below:\n\nYou can also find the episode wherever you get your podcasts, including:\n\nThe Meta Tech Podcast is a podcast, brought to you by Meta, where we highlight the work Meta\u2019s engineers are doing at every level \u2013 from low-level frameworks to end-user features.\n\nSend us feedback on Instagram, Threads, or X.\n\nAnd if you\u2019re interested in learning more about career opportunities at Meta visit the Meta Careers page.\n\nLinks", "label": 0}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2014", "content": "en\n\n\"\n\nSo who is freezing their but off? That would be me! Well we can say that winter has officially kicked in, so has the break. So it is the time to throw your shoes and load up that good ol' Netflix account. We all need to catch up on the shows that we have abandoned lately.\n\nIt has been a while since I wrote last. Let me tell you, it has been a quite the interesting semester.I stayed busy with all my activities especially with my fraternity. I'm a now an initiated member of Alpha Tau Omega. Being part of Greek life, it has opened me many doors. On my free time, I've been working out a lot lately. Since the beginning of the school year, I managed to lose over 20 pounds. What a difference it has made! I feel great. I want to thank my workout buddy for helping me out!! Props to you man for putting up with me!\n\nAll my finals went well. Let's say that I was not myself during dead week. Maybe I can make a zombie reference?? I passed all my tests and ended on a well note for the semester. There is always room for improvement.\n\nThe first day back was just filled with a lot of things to do and with a lot of surprises. Good things after another occurred.\n\nIt feels great hanging out with my family. I've missed them so much. While back in the dorm, it seemed that I really did not missed them much. Once I got home, I realized how much they were truly a missing part of me. I missed my mom's home cooked meals. What a relief that I can stay away from the cafeteria food.\n\nThen I went to my high school to see the varsity basketball game. Go Tigers!! They kicked some ass when they beat York 65-38. The game was intense. How I miss the basketball games. Once a tiger always a tiger! It was great seeing some old friends.\n\nI am glad to have made many friends this semester. I appreciate everyone of you. Let's see what is in stock for us this coming semester.\n\nHappy Holidays everyone!! Stay warm and make a snowman (if we can get enough snow.)\"", "label": 1}
{"title": "Globalizing Productions with Netflix\u2019s Media Production Suite", "url": "https://netflixtechblog.com/globalizing-productions-with-netflixs-media-production-suite-fc3c108c0a22?source=collection_home---4------2-----------------------", "content": "Globalizing Productions with Netflix\u2019s Media Production Suite Netflix Technology Blog 12 min read \u00b7 Mar 31, 2025 -- 8 Listen Share\n\nJesse Korosi, Thijs van de Kamp, Mayra Vega, Laura Futuro, Anton Margoline\n\nThe journey from script to screen is full of challenges in the ever-evolving world of film and television. The industry has always innovated, and over the last decade, it started moving towards cloud-based workflows. However, unlocking cloud innovation and all its benefits on a global scale has proven to be difficult. The opportunity is clear: streamline complex media management logistics, eliminate tedious, non-creative task-based work and enable productions to focus on what matters most\u2013creative storytelling. With these challenges in mind, Netflix has developed a suite of tools by filmmakers for filmmakers: the Media Production Suite (MPS).\n\nWhat are we solving for?\n\nSignificant time and resources are devoted to managing media logistics throughout the production lifecycle. An average Netflix title produces around ~200 Terabytes of Original Camera Files (OCF), with outliers up to 700 Terabytes, not including any work-in-progress files, VFX assets, 3D assets, etc. The data produced on set is traditionally copied to physical tape stock like LTO. This workflow has been considered the industry norm for a long time and may be cost-effective, but comes with trade-offs. Aside from needing to physically ship and track all movement of the tape stock, storing media on a physical tape makes it harder to search, play and share media assets; slowing down accessibility to production media when needed, especially when titles need to collaborate with talent and vendors all over the world.\n\nEven when workflows are fully digital, the distribution of media between multiple departments and vendors can still be challenging. A lack of automation and standardization often results in a labour-intensive process across post-production and VFX with a lot of dependencies that introduce potential human errors and security risks. Many productions utilize a large variety of vendors, making this collaboration a large technical puzzle. As file sizes grow and workflows become more complex, these issues are magnified, leading to inefficiencies that slow down post-production and reduce the available time spent on creative work.\n\nMoving media into the cloud introduces new challenges for production and post ramping up to meet the operational and technological hurdles this poses. For some post-production facilities, it\u2019s not uncommon to see a wall of portable hard drives at their facility, with media being hand-carried between vendors because alternatives are not available. The need for a centralized, cloud-based solution that transcends these barriers is more pressing than ever. This results in a willingness to embrace new and innovative ideas, even if exploratory, and introduce drastic workflow changes to productions in pursuit of creative evolution.\n\nAt Netflix, we believe that great stories can come from anywhere, but we have seen that technical limitations in traditional workflows reduce access to media and restrict filmmakers\u2019 access to talent. Besides the need for robust cloud storage for their media, artists need access to powerful workstations and real-time playback. Depending on the market, or production budget, cutting-edge technology might not be available or affordable.\n\nWhat if we started charting a course to break free from many of these technical limitations and found ways to enhance creativity? Industry trade shows like the International Broadcast Convention (IBC) and the National Association of Broadcasters Show (NAB) highlight a strong global trend: instead of bringing media to the artist/applications (traditional workflow) we see the shift towards bringing people and applications to the media (cloud workflows and remote workstations). The concept of cloud-based workflows is not new, as many technology leaders in our industry have been experimenting in this space for more than a decade. However, executing this vision at a Netflix scale with hundreds of titles a year has not been done before\u2026\n\nThe challenge of building a global technology to solve this\n\nBuilding solutions at a global scale poses significant challenges. The art of making movies and series lacks equal access to technology, best practices, and global standardization. Different countries worldwide are at different phases of innovation based on local needs and nuances. While some regions boast over a century of cinematic history and have a strong industry, others are just beginning to carve their niche. This vast gap presents a unique challenge: developing global technology that caters to both established and emerging markets, each with distinct languages and workflows.\n\nThe large diversity of needs by talent and vendors globally creates a standardization challenge and can be seen when productions use a global talent pool. Many mature post-production and VFX facilities have built scripts and automation that flow between various artists and personnel within their facility; allowing a more streamlined workflow, even though the customization is time-consuming. E.g., Transcoding, or transcriptions that automatically run when files are dropped in a hot folder, with the expectation that certain sidecar metadata files will accompany them with a specific organizational structure. Embracing and integrating new workflows introduces the fear of disrupting a well-established process, increasing additional pressure on the profit margins of vendors. Small workflow changes that may seem arbitrary may actually have a large impact on vendors. Therefore, innovation should provide meaningful benefits to a title in order to get adopted at scale. Reliability, a proven track record, strong support, and an incredibly low tolerance for bugs, or issues are top of mind in well-established markets.\n\nIn developing this suite, we recognized the necessity of addressing the vast array of titles that flow through Netflix without the luxury of expanding into a massive operational entity. Consequently, automation became imperative. The intricacies of color and framing management, along with deliverables, must be seamlessly controlled and effortlessly managed by the user, without the need for manual intervention. Therefore, we cannot lean into humans configuring JSON files behind the scenes to map camera formats into deliverables. By embracing open standards, we not only streamline these processes but also facilitate smoother collaboration across diverse markets and countries, ensuring that our global productions can operate with unparalleled efficiency and cohesion. To ensure this, we\u2019ve decided to lean heavily into standards like ACES, AMF, ASC MHL, ASC FDL, and OTIO. ACES and AMF for color pipeline management. ASC MHL for any file management/verifications. ASC FDL will serve as our framing interoperability and OTIO for any timeline interchange. Leaning into standards like this means that many things can be automated at scale and more importantly, high-complexity workflows can be offered to markets or shows that don\u2019t normally have access to them. As an example, if a show is shot on various camera formats all framed and recorded at different resolutions, with different lenses and different safeties on each frame. The task of normalizing all of these for a VFX vendor into one common container with a normalized center extracted frame is often only offered to very high-end titles, considering it takes a human behind the curtain to create all of these mappings. But by leaning into a standard like the FDL, it means this can now easily be automated, and the control for these mappings, put directly in the hands of users.\n\nOur Answer \u2014 Content Hub\u2019s Media Production Suite (MPS)\n\nIntroducing Content Hub Media Production Suite video\n\nBuilding a global scalable solution that could be utilized in a diversity of markets has been an exciting challenge. We set out to provide customizable and feature-rich tooling for advanced users while remaining intuitive and streamlined enough for less experienced filmmakers. With collaboration from Netflix teams, vendors, and talent across the globe, we\u2019ve taken a bold step forward in enabling a suite of tools inside Netflix Content Hub that democratizes technology: the Media Production Suite. While leveraging our scale economies and access to resources, we can now unlock global talent pools for our productions, drastically reduce non-creative task-based work, streamline workflows, and level the playing field between our markets, ultimately maximizing the time available for what matters most; creative work!\n\nSo what is it?\n\n1. Netflix Hybrid Infrastructure: Netflix has invested in a hybrid infrastructure, a mix of cloud-based and physically distributed capabilities operating in multiple locations across the world and close to our productions to optimize user performance. This infrastructure is available for Netflix shows and is foundational under Content Hub\u2019s Media Production Suite tooling. Local storage and compute services are connected through the Netflix Open Connect network (Netflix Content Delivery Network) to the infrastructure of Amazon Web Services (AWS). The system facilitates large volumes of camera and sound media and is built for speed. In order to ensure that productions have sufficient upload speeds to get their media into the cloud, Netflix has started to roll out Content Hub Ingest Centers globally to provide high-speed internet connectivity where required. With all media centralized, MPS eliminates the need for physical media transport and reduces the risk of human error. This approach not only streamlines operations but also enhances security and accessibility.\n\n2. Automation and Tooling: In addition to the Netflix Hybrid infrastructure layer, MPS consists of a suite of tools that tap into the media in the Netflix ecosystem.\n\nFootage Ingest \u2014 An application that allows users to upload media/files into Content Hub.\n\nMedia Library \u2014 A central library that allows users to search, preview, share and download media.\n\nDailies \u2014 A workflow, backed by an operational team, offering automated Quality Control of your footage, sound sync, application of color, rendering, and delivering dailies directly to editorial.\n\nRemote Workstations \u2014 Offering access to remote editorial workstations and storage for post-production needs.\n\nVFX Pulls \u2014 An automated method for converting and delivering visual effects plates, associated color, and framing files to VFX vendors.\n\nConform Pulls \u2014 An automated method for consolidating, trimming, and delivering all OCF to picture-finishing vendors.\n\nMedia Downloader \u2014 An automated download tool that initiates a download once media has been made available in the Netflix cloud.\n\nWhile each of the individual tools within MPS is at different states of maturity, over 350 titles have made use of at least one of the tools noted above. Input has been taken from all over the world while developing, with users ranging from UCAN (United States/Canada), EMEA (Europe, Middle East, and Africa), SEA (South East Asia), LATAM (Latin America), and APAC (Asia Pacific).\n\nSenna: Early Adoption and Insightful Feedback Driving MPS Evolution\n\nMedia from the Brazilian-produced series \u2018Senna\u2019 being reviewed in MPS\n\nThe Brazilian-produced series Senna, which follows the life of legendary Formula 1 driver Ayrton Senna, utilized MPS to reshape their content creation workflow, overcome geographical barriers, and unlock innovation to support world-class storytelling for a global audience. Senna is a groundbreaking series, not just for its storytelling but for its production journey across Argentina, Uruguay, Brazil, and the United Kingdom. With editorial teams spread across Porto Alegre and Spain, and VFX studios collaborating across locations in Brazil, Canada, the United States, and India, all orchestrated by our subsidiary Scanline VFX. The series exemplifies the global nature of modern filmmaking and was the perfect fit for Netflix\u2019s new Content Hub Media Production Suite (MPS).\n\nAt the heart of Senna\u2019s workflow orchestration is MPS. While each of the tools within MPS is based on an opt-in model, in order to use many of the downstream services, the first step is ensuring that the original camera files (OCF) and original sound files (OSF) are uploaded. \u201cWe knew we were going to shoot in different places,\u201d said Post Supervisor Gabriel Queiroz,\u201cto have all this material cloud-based, it\u2019s definitely one of the most important things for us. It would be hard to bring all this media physically from Argentina or wherever to Brazil. It will take us a lot of time.\u201d With Senna shooting across locations, allowing production the capability of uploading their OCF and OSF resulted in no longer requiring shuttling hard drives on airplanes, creating LTO tapes, & managing physical shipments for their negative. And yes, you read that correctly; when utilizing MPS, we don\u2019t require LTO tapes to be written unless there are title-specific needs.\n\nWith Senna beginning production back in June of 2023, our investment in MPS was still very early stages, and the tooling was considered beta. However, with the help, feedback, and partnership from this production, it was quickly realized that the investment was worth doubling down on. Since the early version used on Senna, Netflix has been spinning up ingest centers around the world, where drives can be dropped off, and within a matter of hours, all original camera files are uploaded into the Netflix ecosystem. While creating the ability to upload is not a novel concept, behind the scenes, it\u2019s far from simple. Once a drive has been plugged in and our Netflix Footage Ingest application is opened, a validation is run, ensuring all expected media from set is on the drive. After media has been uploaded and checksums are run validating media integrity, all media is inspected, metadata is extracted, and assets are created for viewing/sharing/downloading with playable proxies. All media is then automatically backed up to a second tier of cloud-based storage for the final archive.\n\nTraditionally, if you wanted to check in with your post vendor on how things are going for each of these media management steps noted above, or whether or not you can clear on set camera cards if you haven\u2019t gotten a completion notification, you would have to pick up the phone and call your vendor. For Senna, anyone who wanted visibility on progress, simply logged in to Content Hub and could see any activity in the Footage Ingest dashboard, as well as look up any information needed on past uploads.\n\nRemote monitoring media being uploaded and archived using the MPS Footage Ingest workflow\n\nWhile many services in MPS are available once media has been uploaded, Senna\u2019s use of MPS focused on VFX. With Senna shooting a high volume of footage and the show having a high volume of VFX shots, according to Post Supervisor Gabriel Queiroz \u201cUsing MPS was basically a no-brainer, [having] used the tool before, I knew what it could bring to the project. And to be honest, with the amount of footage that we have, it was just so much material and with the amount of vendors we have, knowing that we would have to deliver all this footage to all these kinds of vendors, including outside of Brazil and to different parts of the world.\u201d\n\nWith a traditional workflow, utilizing available resources in Latin America, VFX Pulls would have been done manually. This process is prone to human error and more importantly, for a show like Senna, too slow and would have resulted in different I/O methods for every vendor.\n\nIllustrating a traditional VFX Editor having to manage various I/O methods\n\nBy utilizing MPS, the Assistant Editor was able to log into Content Hub, upload an EDL, and have their VFX Pulls automatically transcoded, color files consolidated and all media placed into a Google Drive style folder built directly in Content Hub (called Workspaces). The VFX Editor was able to make any additional tweaks they wanted to the directory before farming out each of the shots to whichever vendor they were meant for. When it came time for the VFX vendors to then send shots back to editorial or DI, this was also done through MPS. Having one standard method for I/O for all VFX file sharing meant that Editorial and DI did not have to manage a different file transfer/workflow for every single vendor that was onboarded.\n\nIllustrating a more streamlined workflow for VFX vendors when using MPS\n\nAfter picture was locked and it was time for Senna to do their Online, the DI facility Quanta was able to utilize the Conform Pull service within MPS. The Conform Pull service allowed their team to upload an EDL, which ran a QC on all of the media from within their edit to ensure a smooth conform and then consolidated, trimmed, and packaged up all of the media they needed for the online. Since this early beta and thanks to learnings from many shows like Senna, advancements have been made in the system\u2019s ability to match back to source media for both Conform and VFX Pulls. Rather than requiring an exact match between EDL and source OCF, there are several variations of fuzzy matching that can take place, as well as a current investigation in using one of our perceptual matching algorithms, allowing for a perceptual conform using computer vision, instead of solely relying on metadata.\n\nInside Senna with Content Hub Media Production Suite video\n\nConclusion\n\nThe Media Production Suite (MPS) represents a transformative leap in how we approach media production at Netflix. By embracing open standards, we have crafted a scalable solution that not only makes economic sense but also democratizes access to advanced production tools across the globe. This approach allows us to eliminate tedious tasks, enabling our teams to focus on what truly matters: creative storytelling. By fostering global collaboration and leveraging the power of cloud-based workflows, we\u2019re not just enhancing efficiency but also elevating the quality of our productions. As we continue to innovate and refine our processes, we remain committed to breaking down barriers and unlocking the full potential of creative talent worldwide. The future of filmmaking is here, and with MPS, we are leading the charge toward a more connected and creatively empowered industry.", "label": 0}
{"title": "Pushy to the Limit: Evolving Netflix\u2019s WebSocket proxy for the future", "url": "https://netflixtechblog.com/pushy-to-the-limit-evolving-netflixs-websocket-proxy-for-the-future-b468bc0ff658?source=collection_home---4------15-----------------------", "content": "Pushy to the Limit: Evolving Netflix\u2019s WebSocket proxy for the future Netflix Technology Blog 16 min read \u00b7 Sep 10, 2024 -- 16 Listen Share\n\nBy Karthik Yagna, Baskar Odayarkoil, and Alex Ellis\n\nPushy is Netflix\u2019s WebSocket server that maintains persistent WebSocket connections with devices running the Netflix application. This allows data to be sent to the device from backend services on demand, without the need for continually polling requests from the device. Over the last few years, Pushy has seen tremendous growth, evolving from its role as a best-effort message delivery service to be an integral part of the Netflix ecosystem. This post describes how we\u2019ve grown and scaled Pushy to meet its new and future needs, as it handles hundreds of millions of concurrent WebSocket connections, delivers hundreds of thousands of messages per second, and maintains a steady 99.999% message delivery reliability rate.\n\nHistory & motivation\n\nThere were two main motivating use cases that drove Pushy\u2019s initial development and usage. The first was voice control, where you can play a title or search using your virtual assistant with a voice command like \u201cShow me Stranger Things on Netflix.\u201d (See How to use voice controls with Netflix if you want to do this yourself!).\n\nIf we consider the Alexa use case, we can see how this partnership with Amazon enabled this to work. Once they receive the voice command, we allow them to make an authenticated call through apiproxy, our streaming edge proxy, to our internal voice service. This call includes metadata, such as the user\u2019s information and details about the command, such as the specific show to play. The voice service then constructs a message for the device and places it on the message queue, which is then processed and sent to Pushy to deliver to the device. Finally, the device receives the message, and the action, such as \u201cShow me Stranger Things on Netflix\u201d, is performed. This initial functionality was built out for FireTVs and was expanded from there.\n\nSample system diagram for an Alexa voice command. Where aws ends and the internet begins is an exercise left to the reader.\n\nThe other main use case was RENO, the Rapid Event Notification System mentioned above. Before the integration with Pushy, the TV UI would continuously poll a backend service to see if there were any row updates to get the latest information. These requests would happen every few seconds, which ended up creating extraneous requests to the backend and were costly for devices, which are frequently resource constrained. The integration with WebSockets and Pushy alleviated both of these points, allowing the origin service to send row updates as they were ready, resulting in lower request rates and cost savings.\n\nFor more background on Pushy, you can see this InfoQ talk by Susheel Aroskar. Since that presentation, Pushy has grown in both size and scope, and this article will be discussing the investments we\u2019ve made to evolve Pushy for the next generation of features.\n\nClient Reach\n\nThis integration was initially rolled out for Fire TVs, PS4s, Samsung TVs, and LG TVs, leading to a reach of about 30 million candidate devices. With these clear benefits, we continued to build out this functionality for more devices, enabling the same efficiency wins. As of today, we\u2019ve expanded our list of candidate devices even further to nearly a billion devices, including mobile devices running the Netflix app and the website experience. We\u2019ve even extended support to older devices that lack modern capabilities, like support for TLS and HTTPS requests. For those, we\u2019ve enabled secure communication from client to Pushy via an encryption/decryption layer on each, allowing for confidential messages to flow between the device and server.\n\nScaling to handle that growth (and more)\n\nGrowth\n\nWith that extended reach, Pushy has gotten busier. Over the last five years, Pushy has gone from tens of millions of concurrent connections to hundreds of millions of concurrent connections, and it regularly reaches 300,000 messages sent per second. To support this growth, we\u2019ve revisited Pushy\u2019s past assumptions and design decisions with an eye towards both Pushy\u2019s future role and future stability. Pushy had been relatively hands-free operationally over the last few years, and as we updated Pushy to fit its evolving role, our goal was also to get it into a stable state for the next few years. This is particularly important as we build out new functionality that relies on Pushy; a strong, stable infrastructure foundation allows our partners to continue to build on top of Pushy with confidence.\n\nThroughout this evolution, we\u2019ve been able to maintain high availability and a consistent message delivery rate, with Pushy successfully maintaining 99.999% reliability for message delivery over the last few months. When our partners want to deliver a message to a device, it\u2019s our job to make sure they can do so.\n\nHere are a few of the ways we\u2019ve evolved Pushy to handle its growing scale.\n\nA few of the related services in Pushy\u2019s immediate ecosystem and the changes we\u2019ve made for them.\n\nMessage processor\n\nOne aspect that we invested in was the evolution of the asynchronous message processor. The previous version of the message processor was a Mantis stream-processing job that processed messages from the message queue. It was very efficient, but it had a set job size, requiring manual intervention if we wanted to horizontally scale it, and it required manual intervention when rolling out a new version.\n\nIt served Pushy\u2019s needs well for many years. As the scale of the messages being processed increased and we were making more code changes in the message processor, we found ourselves looking for something more flexible. In particular, we were looking for some of the features we enjoy with our other services: automatic horizontal scaling, canaries, automated red/black rollouts, and more observability. With this in mind, we rewrote the message processor as a standalone Spring Boot service using Netflix paved-path components. Its job is the same, but it does so with easy rollouts, canary configuration that lets us roll changes safely, and autoscaling policies we\u2019ve defined to let it handle varying volumes.\n\nRewriting always comes with a risk, and it\u2019s never the first solution we reach for, particularly when working with a system that\u2019s in place and working well. In this case, we found that the burden from maintaining and improving the custom stream processing job was increasing, and we made the judgment call to do the rewrite. Part of the reason we did so was the clear role that the message processor played \u2014 we weren\u2019t rewriting a huge monolithic service, but instead a well-scoped component that had explicit goals, well-defined success criteria, and a clear path towards improvement. Since the rewrite was completed in mid-2023, the message processor component has been completely zero touch, happily automated and running reliably on its own.\n\nPush Registry\n\nFor most of its life, Pushy has used Dynomite for keeping track of device connection metadata in its Push Registry. Dynomite is a Netflix open source wrapper around Redis that provides a few additional features like auto-sharding and cross-region replication, and it provided Pushy with low latency and easy record expiry, both of which are critical for Pushy\u2019s workload.\n\nAs Pushy\u2019s portfolio grew, we experienced some pain points with Dynomite. Dynomite had great performance, but it required manual scaling as the system grew. The folks on the Cloud Data Engineering (CDE) team, the ones building the paved path for internal data at Netflix, graciously helped us scale it up and make adjustments, but it ended up being an involved process as we kept growing.\n\nThese pain points coincided with the introduction of KeyValue, which was a new offering from the CDE team that is roughly \u201cHashMap as a service\u201d for Netflix developers. KeyValue is an abstraction over the storage engine itself, which allows us to choose the best storage engine that meets our SLO needs. In our case, we value low latency \u2014 the faster we can read from KeyValue, the faster these messages can get delivered. With CDE\u2019s help, we migrated our Push Registry to use KV instead, and we have been extremely satisfied with the result. After tuning our store for Pushy\u2019s needs, it has been on autopilot since, appropriately scaling and serving our requests with very low latency.\n\nScaling Pushy horizontally and vertically\n\nMost of the other services our team runs, like apiproxy, the streaming edge proxy, are CPU bound, and we have autoscaling policies that scale them horizontally when we see an increase in CPU usage. This maps well to their workload \u2014 more HTTP requests means more CPU used, and we can scale up and down accordingly.\n\nPushy has slightly different performance characteristics, with each node maintaining many connections and delivering messages on demand. In Pushy\u2019s case, CPU usage is consistently low, since most of the connections are parked and waiting for an occasional message. Instead of relying on CPU, we scale Pushy on the number of connections, with exponential scaling to scale faster after higher thresholds are reached. We load balance the initial HTTP requests to establish the connections and rely on a reconnect protocol where devices will reconnect every 30 minutes or so, with some staggering, that gives us a steady stream of reconnecting devices to balance connections across all available instances.\n\nFor a few years, our scaling policy had been that we would add new instances when the average number of connections reached 60,000 connections per instance. For a couple hundred million devices, this meant that we were regularly running thousands of Pushy instances. We can horizontally scale Pushy to our heart\u2019s content, but we would be less content with our bill and would have to shard Pushy further to get around NLB connection limits. This evolution effort aligned well with an internal focus on cost efficiency, and we used this as an opportunity to revisit these earlier assumptions with an eye towards efficiency.\n\nBoth of these would be helped by increasing the number of connections that each Pushy node could handle, reducing the total number of Pushy instances and running more efficiently with the right balance between instance type, instance cost, and maximum concurrent connections. It would also allow us to have more breathing room with the NLB limits, reducing the toil of additional sharding as we continue to grow. That being said, increasing the number of connections per node is not without its own drawbacks. When a Pushy instance goes down, the devices that were connected to it will immediately try to reconnect. By increasing the number of connections per instance, it means that we would be increasing the number of devices that would be immediately trying to reconnect. We could have a million connections per instance, but a down node would lead to a thundering herd of a million devices reconnecting at the same time.\n\nThis delicate balance led to us doing a deep evaluation of many instance types and performance tuning options. Striking that balance, we ended up with instances that handle an average of 200,000 connections per node, with breathing room to go up to 400,000 connections if we had to. This makes for a nice balance between CPU usage, memory usage, and the thundering herd when a device connects. We\u2019ve also enhanced our autoscaling policies to scale exponentially; the farther we are past our target average connection count, the more instances we\u2019ll add. These improvements have enabled Pushy to be almost entirely hands off operationally, giving us plenty of flexibility as more devices come online in different patterns.\n\nReliability & building a stable foundation\n\nAlongside these efforts to scale Pushy for the future, we also took a close look at our reliability after finding some connectivity edge cases during recent feature development. We found a few areas for improvement around the connection between Pushy and the device, with failures due to Pushy attempting to send messages on a connection that had failed without notifying Pushy. Ideally something like a silent failure wouldn\u2019t happen, but we frequently see odd client behavior, particularly on older devices.\n\nIn collaboration with the client teams, we were able to make some improvements. On the client side, better connection handling and improvements around the reconnect flow meant that they were more likely to reconnect appropriately. In Pushy, we added additional heartbeats, idle connection cleanup, and better connection tracking, which meant that we were keeping around fewer and fewer stale connections.\n\nWhile these improvements were mostly around those edge cases for the feature development, they had the side benefit of bumping our message delivery rates up even further. We already had a good message delivery rate, but this additional bump has enabled Pushy to regularly average 5 9s of message delivery reliability.\n\nPush message delivery success rate over a recent 2-week period.\n\nRecent developments\n\nWith this stable foundation and all of these connections, what can we now do with them? This question has been the driving force behind nearly all of the recent features built on top of Pushy, and it\u2019s an exciting question to ask, particularly as an infrastructure team.\n\nShift towards direct push\n\nThe first change from Pushy\u2019s traditional role is what we call direct push; instead of a backend service dropping the message on the asynchronous message queue, it can instead leverage the Push library to skip the asynchronous queue entirely. When called to deliver a message in the direct path, the Push library will look up the Pushy connected to the target device in the Push Registry, then send the message directly to that Pushy. Pushy will respond with a status code reflecting whether it was able to successfully deliver the message or it encountered an error, and the Push library will bubble that up to the calling code in the service.\n\nThe system diagram for the direct and indirect push paths.\n\nSusheel, the original author of Pushy, added this functionality as an optional path, but for years, nearly all backend services relied on the indirect path with its \u201cbest-effort\u201d being good enough for their use cases. In recent years, we\u2019ve seen usage of this direct path really take off as the needs of backend services have grown. In particular, rather than being just best effort, these direct messages allow the calling service to have immediate feedback about the delivery, letting them retry if a device they\u2019re targeting has gone offline.\n\nThese days, messages sent via direct push make up the majority of messages sent through Pushy. For example, for a recent 24 hour period, direct messages averaged around 160,000 messages per second and indirect averaged at around 50,000 messages per second..\n\nGraph of direct vs indirect messages per second.\n\nDevice to device messaging\n\nAs we\u2019ve thought through this evolving use case, our concept of a message sender has also evolved. What if we wanted to move past Pushy\u2019s pattern of delivering server-side messages? What if we wanted to have a device send a message to a backend service, or maybe even to another device? Our messages had traditionally been unidirectional as we send messages from the server to the device, but we now leverage these bidirectional connections and direct device messaging to enable what we call device to device messaging. This device to device messaging supported early phone-to-TV communication in support of games like Triviaverse, and it\u2019s the messaging foundation for our Companion Mode as TVs and phones communicate back and forth.\n\nA screenshot of one of the authors playing Triviaquest with a mobile device as the controller.\n\nThis requires higher level knowledge of the system, where we need to know not just information about a single device, but more broader information, like what devices are connected for an account that the phone can pair with. This also enables things like subscribing to device events to know when another device comes online and when they\u2019re available to pair or send a message to. This has been built out with an additional service that receives device connection information from Pushy. These events, sent over a Kafka topic, let the service keep track of the device list for a given account. Devices can subscribe to these events, allowing them to receive a message from the service when another device for the same account comes online.\n\nPushy and its relationship with the Device List Service for discovering other devices.\n\nThis device list enables the discoverability aspect of these device to device messages. Once the devices have this knowledge of the other devices connected for the same account, they\u2019re able to choose a target device from this list that they can then send messages to.\n\nOnce a device has that list, it can send a message to Pushy over its WebSocket connection with that device as the target in what we call a device to device message (1 in the diagram below). Pushy looks up the target device\u2019s metadata in the Push registry (2) and sends the message to the second Pushy that the target device is connected to (3), as if it was the backend service in the direct push pattern above. That Pushy delivers the message to the target device (4), and the original Pushy will receive a status code in response, which it can pass back to the source device (5).\n\nA basic order of events for a device to device message.\n\nThe messaging protocol\n\nWe\u2019ve defined a basic JSON-based message protocol for device to device messaging that lets these messages be passed from the source device to the target device. As a networking team, we naturally lean towards abstracting the communication layer with encapsulation wherever possible. This generalized message means that device teams are able to define their own protocols on top of these messages \u2014 Pushy would just be the transport layer, happily forwarding messages back and forth.\n\nThe client app protocol, built on top of the device to device protocol, built on top of Pushy.\n\nThis generalization paid off in terms of investment and operational support. We built the majority of this functionality in October 2022, and we\u2019ve only needed small tweaks since then. We needed nearly no modifications as client teams built out the functionality on top of this layer, defining the higher level application-specific protocols that powered the features they were building. We really do enjoy working with our partner teams, but if we\u2019re able to give them the freedom to build on top of our infrastructure layer without us getting involved, then we\u2019re able to increase their velocity, make their lives easier, and play our infrastructure roles as message platform providers.\n\nWith early features in experimentation, Pushy sees an average of 1000 device to device messages per second, a number that will only continue to grow.\n\nGraph of device to device messages per second.\n\nThe Netty-gritty details\n\nIn Pushy, we handle incoming WebSocket messages in our PushClientProtocolHandler (code pointer to class in Zuul that we extend), which extends Netty\u2019s ChannelInboundHandlerAdapter and is added to the Netty pipeline for each client connection. We listen for incoming WebSocket messages from the connected device in its channelRead method and parse the incoming message. If it\u2019s a device to device message, we pass the message, the ChannelHandlerContext, and the PushUserAuth information about the connection\u2019s identity to our DeviceToDeviceManager.\n\nA rough overview of the internal organization for these components.\n\nThe DeviceToDeviceManager is responsible for validating the message, doing some bookkeeping, and kicking off an async call that validates that the device is an authorized target, looks up the Pushy for the target device in the local cache (or makes a call to the data store if it\u2019s not found), and forwards on the message. We run this asynchronously to avoid any event loop blocking due to these calls. The DeviceToDeviceManager is also responsible for observability, with metrics around cache hits, calls to the data store, message delivery rates, and latency percentile measurements. We\u2019ve relied heavily on these metrics for alerts and optimizations \u2014 Pushy really is a metrics service that occasionally will deliver a message or two!\n\nSecurity\n\nAs the edge of the Netflix cloud, security considerations are always top of mind. With every connection over HTTPS, we\u2019ve limited these messages to just authenticated WebSocket connections, added rate limiting, and added authorization checks to ensure that a device is able to target another device \u2014 you may have the best intentions in mind, but I\u2019d strongly prefer it if you weren\u2019t able to send arbitrary data to my personal TV from yours (and vice versa, I\u2019m sure!).\n\nLatency and other considerations\n\nOne main consideration with the products built on top of this is latency, particularly when this feature is used for anything interactive within the Netflix app.\n\nWe\u2019ve added caching to Pushy to reduce the number of lookups in the hotpath for things that are unlikely to change frequently, like a device\u2019s allowed list of targets and the Pushy instance the target device is connected to. We have to do some lookups on the initial messages to know where to send them, but it enables us to send subsequent messages faster without any KeyValue lookups. For these requests where caching removed KeyValue from the hot path, we were able to greatly speed things up. From the incoming message arriving at Pushy to the response being sent back to the device, we reduced median latency to less than a millisecond, with the 99th percentile of latency at less than 4ms.\n\nOur KeyValue latency is usually very low, but we have seen brief periods of elevated read latencies due to underlying issues in our KeyValue datastore. Overall latencies increased for other parts of Pushy, like client registration, but we saw very little increase in device to device latency with this caching in place.\n\nCultural aspects that enable this work\n\nPushy\u2019s scale and system design considerations make the work technically interesting, but we also deliberately focus on non-technical aspects that have helped to drive Pushy\u2019s growth. We focus on iterative development that solves the hardest problem first, with projects frequently starting with quick hacks or prototypes to prove out a feature. As we do this initial version, we do our best to keep an eye towards the future, allowing us to move quickly from supporting a single, focused use case to a broad, generalized solution. For example, for our cross-device messaging, we were able to solve hard problems in the early work for Triviaverse that we later leveraged for the generic device to device solution.\n\nAs one can immediately see in the system diagrams above, Pushy does not exist in a vacuum, with projects frequently involving at least half a dozen teams. Trust, experience, communication, and strong relationships all enable this to work. Our team wouldn\u2019t exist without our platform users, and we certainly wouldn\u2019t be here writing this post without all of the work our product and client teams do. This has also emphasized the importance of building and sharing \u2014 if we\u2019re able to get a prototype together with a device team, we\u2019re able to then show it off to seed ideas from other teams. It\u2019s one thing to mention that you can send these messages, but it\u2019s another to show off the TV responding to the first click of the phone controller button!\n\nThe future of Pushy\n\nIf there\u2019s anything certain in this world, it\u2019s that Pushy will continue to grow and evolve. We have many new features in the works, like WebSocket message proxying, WebSocket message tracing, a global broadcast mechanism, and subscription functionality in support of Games and Live. With all of this investment, Pushy is a stable, reinforced foundation, ready for this next generation of features.\n\nWe\u2019ll be writing about those new features as well \u2014 stay tuned for future posts.\n\nSpecial thanks to our stunning colleagues Jeremy Kelly and Justin Guerra who have both been invaluable to Pushy\u2019s growth and the WebSocket ecosystem at large. We would also like to thank our larger teams and our numerous partners for their great work; it truly takes a village!", "label": 0}
{"title": "An Insider\u2019s Tips for Taking the Certified Backstage Associate (CBA) Exam", "url": "https://engineering.atspotify.com/2025/3/certified-backstage-associate-exam-tips", "content": "TL;DR There\u2019s a brand new engineering certification in town: Certified Backstage Associate (CBA). Offered through The Linux Foundation, the certification shows that you have the skills and knowledge to build and manage Backstage \u2014 the open framework for internal developer portals (IDPs) that was developed at Spotify and is now used at thousands of companies around the world. If you\u2019re interested in building your engineering career \u2014 or just want to know more about what makes Backstage such a popular platform for developer experience \u2014 here\u2019s everything I learned from taking the CBA exam.\n\nYou\u2019ll learn:\n\nThe value of becoming a Certified Backstage Associate\n\nHow to prepare for the proctored exam (Don\u2019t read the questions out loud to yourself!\ud83e\udd2b)\n\nThe four main subject areas covered by the exam itself\n\nLet\u2019s dive in!\n\nWhy get certified in Backstage?\n\nBackstage is the framework for building internal developer portals that Spotify developed internally before open sourcing it in 2020 and then donating it to the Cloud Native Computing Foundation (CNCF) \u2014 which is also home to other great technologies, like Argo, Crossplane, Envoy, and the granddaddy of cloud native, Kubernetes.\n\nBackstage continues to prove itself as the tool of choice for improving developer experience and developer productivity for companies of all shapes and sizes \u2014 from Adobe and American Airlines, to Expedia Group, H&M, and Toyota, to startups, scale-ups, 100-year-old financial institutions, government consulting agencies, healthcare companies, and more. You\u2019ll find the need for Backstage experts nearly everywhere you find developer teams working together, whether that\u2019s 50 engineers or thousands of them, like at Spotify.\n\nAll that is to say, in the last five years, Backstage has grown from Spotify\u2019s homegrown developer portal to one of the most popular platform engineering tools today. This growth means more companies than ever are looking for people with the expertise to stand up, maintain, and build out a Backstage IDP of their own within their engineering orgs.\n\nWho the CBA is for\n\nThe Linux Foundation launched the new certification program for becoming a Certified Backstage Associate last November. The program is intended for developers who have 3\u20136 months experience using Backstage. The CBA is ideal for site reliability engineers (SREs), DevOps engineers, platform engineers, and other software developers who want to demonstrate their Backstage proficiency. As a Certified Backstage Associate, companies will know that you have the expertise to set up, deploy, and manage a Backstage project either independently or as a team lead.\n\nWhy listen to this guy?\n\nMaybe you\u2019re already thinking: \u201cWow, the CBA sounds pretty great!\u201d Or: \u201cI already use Backstage a lot and would love to showcase my skills.\u201d And: \u201cI\u2019d love to know more about what the certification process looks like and how I should prepare for the exam.\u201d But, also: \u201cWho the heck is the author and why should I trust him?\u201d\n\nWell, I\u2019m Andr\u00e9, aka @awanlin on GitHub. Hello! \ud83d\udc4b I\u2019m an engineer on Spotify\u2019s Backstage team, but I\u2019ve been working with Backstage since early 2021 before I worked at Spotify. You might recognize me from answering questions in the community Discord server, as a maintainer of the Community Plugins repository, as the lead for the Documentation Special Interest Group (SIG), or as the contributor of several Backstage plugins. My early contributions to the open source project and broad Backstage knowledge are actually what led me to joining Spotify!\n\nCurrently, as a customer success engineer, I spend my days working with Spotify\u2019s enterprise customers, making sure they\u2019re getting the most value out of their Backstage IDP \u2014 including with Spotify Portal for Backstage, the SaaS version of Backstage that Spotify recently made available as a beta. Every day, I work with Backstage experts at other companies, helping to problem-solve technical issues and optimize developer experience through Portal, so that their devs can focus on building great features and products instead of dealing with infrastructure complexity. OK, that\u2019s plenty about me. Onto the exam!\n\nPreparing for the proctored exam\n\nIn order to become a Certified Backstage Associate, you\u2019ll need to complete a 90-minute, proctored, multiple-choice exam. A proctored exam means someone will be monitoring you while you are taking it. This person will not suddenly show up in your home and stand over your shoulder. But they will be actively monitoring you via webcam \u2014 and, trust me, they\u2019re paying attention! How do I know?\n\nWell, while I was taking the CBA I ran into a challenging question. As I often do when I\u2019m thinking through a problem, I reread the question out loud to help me better understand it. Unfortunately, the proctor thought I might be asking someone offscreen for help, and gave me a stern warning. I made sure to keep my thoughts in my head from that point on.\n\nNow, for those who are also new to taking a proctored exam remotely, let me share some other tips based on my experience, as I had never taken one until I took the CBA. Some of these may sound pretty basic, but if you\u2019re taking the exam from home, the controlled conditions of the exam might feel a little strange there. So it\u2019s best to do a little bit of prep ahead of time to help you get used to your new exam environment.\n\nDon\u2019t wait until the last-minute. Make sure to give yourself plenty of time before the exam to get set up and settled, so you can avoid any surprises. Also, one less thing to worry about!\n\nCreate a zen space for yourself. Take your exam in a quiet room with an empty desk that is free of distractions. (And let family/housemates know not to disturb. Maybe put a sign on the door.) Before the exam, the proctor will ask you to pan your room with the webcam, so they can \u201cverify that the workspace is clear of any disallowed materials\u201d.\n\nTest the exam tools ahead of time. The day before the exam, take the time to test out your system with the tools that will be used for the exam. The exam organizers will send you detailed instructions about this well beforehand, so you should have plenty of time to familiarize yourself with the system requirements.\n\nH2O is your friend. You can have water, so make sure you have some handy (note: only \u201cclear liquids in a clear container\u201d). You\u2019ll be happy you did!\n\nWith the process for the CBA exam understood, you\u2019ll want to make sure to study and brush up on your Backstage skills. The following is a rundown of what you should know heading into the exam and links to documentation worth reading to help with that.\n\nStudy guide: What is and isn\u2019t in the exam\n\nBackstage is a code-first framework for building an internal developer portal (IDP). To support all the various integrations you might need, it uses a plugin system. This flexibility and extensibility makes Backstage extremely powerful and allows you to quickly build a full-featured IDP that fits your org\u2019s particular processes, practices, and tech stack. It also means that for an exam on the topic, there will be a very broad number of areas to study.\n\nThree subject areas you don\u2019t need to know for the exam\n\nBut before we get into what will be in the exam, here are a few things that won\u2019t be.\n\n\ud83d\udeab The New Backend System (and other newer features). It\u2019s worth mentioning that the CBA was written before what is known as the New Backend System became the default method for the backend and backend plugins \u2014 so you won\u2019t need to study this. You also won\u2019t be expected to know about alpha features, such as the New Frontend System and Canon, the new design library.\n\n\ud83d\udeab How to build custom plugins. While a CBA should be capable of installing Backstage plugins \u2014 which involves working with and customizing TypeScript \u2014 developing custom plugins falls outside the scope of a minimally qualified CBA.\n\n\ud83d\udeab The nitty-gritty of Yarn and Node.js. Regarding Yarn and Node, the exam won\u2019t cover specifics about either of these \u2014 for example, you won\u2019t be asked about the different features of Yarn Classic and Yarn 4. But you will have to know that Yarn is the default package manager used in Backstage and how to use it to add packages. Similarly, specific features of various Node versions will also not be covered, but knowing Backstage\u2019s Node Version Policy would be.\n\nFour subject areas you do need to know for the exam\n\nThe four subject areas below focus on the technical implementation and features of the Backstage framework. But it\u2019s also good to keep in mind how those technology details relate to the principles underlying the overall goals of Backstage \u2014 i.e., how the architecture and design of the framework enables anyone to build a customizable, centralized, scalable platform for collaborative software development \u2014 or, as we like to say, \u201ca single pane of glass\u201d for all your software development processes and infrastructure.\n\n\u2705 Backstage Development Workflow\n\nBackstage offers a lot of nice features that make the local development experience smooth and easy. I would go over the various aspects of your local development workflow to start:\n\nBe able to build and run Backstage locally\n\nKnow the various Backstage CLI commands and their uses\n\nUnderstand the use of TypeScript in Backstage and how to generate the typed code\n\nHave a familiarity with how to install Backstage plugins with Yarn\n\nKnow how to build a Docker container of a Backstage instance\n\n\u2705 Backstage Infrastructure\n\nAfter going over local development, I would work on:\n\nUnderstanding that Backstage is a framework and what its various aspects consist of\n\nKnow the configuration system used by Backstage, including use of secrets and includes\n\nBe aware of the various deployment options for Backstage\n\nHave a familiarity with the overall client-server architecture of Backstage\n\n\u2705 Backstage Catalog\n\nThe Catalog is the heart of Backstage \u2014 and as such, there\u2019s a good chunk of the CBA exam that covers it. You\u2019ll want to go over the following topics:\n\n\u2705 Customizing Backstage\n\nSome consider customization one of Backstage\u2019s most powerful, yet least talked about features. Here\u2019s some of the topics in this broad feature that I would suggest keeping in mind:\n\nKnowing the difference between frontend and backend plugins\n\nUnderstanding the general customization options for plugins and core features\n\nHow to make changes to React code and where those changes would be made\n\nUsing Material UI components \u2014 where to get them from and how to apply them\n\nOne more thing: Begin with the basics\n\nTo lead off your studying, I would take the time to refresh yourself on the basics, in addition to the subject areas above. Walk through the initial Getting Started guide and create a new Backstage instance. It might have been a long time since you\u2019ve done this yourself and aspects of this will likely be on the exam. I would also re-familiarize yourself with the Backstage prerequisites, like Yarn and Node (see above for what will/won\u2019t be covered). These are tools you might take for granted in your day to day \u2014 grounding yourself in them will give you a solid foundation for the rest of the exam.\n\nAnd with that, you should be able to comfortably take the Certified Backstage Associate exam! Remember, if you don\u2019t pass the first time, you get a second chance. So, just do your best \u2014 and good luck!\n\nLet me know how you did!\n\nOnce you complete your exam, feel free to share your experience and your certification with me on LinkedIn. I would love to hear about your journey to becoming a Certified Backstage Associate! I\u2019ll make sure to update this post with any new tips I hear!", "label": 0}
{"title": "Unlock deeper insights with the new Python client library for Data Commons", "url": "https://developers.googleblog.com/en/pythondatacommons/", "content": "Data is the bedrock of progress across nearly every field. It serves as the raw material from which profound insights are forged, enabling us to precisely measure current realities, identify critical trends, and possibly predict future outcomes.\n\nAt Google, our mission with Data Commons is to organize the world's publicly available statistical data, making it more accessible and useful for everyone. It's an open-source knowledge graph that unifies a vast array of public data from diverse sources, simplifying access and comprehension for developers, researchers, and data analysts alike. Along with the datacommons.org website, Google Search uses Data Commons to answer queries like What is the population of San Francisco?, with the top graph generated by Data Commons.\n\nToday, we're announcing the general availability of the new Python client library for the Data Commons based on the V2 REST API. This new Python library dramatically enhances how data developers can leverage Data Commons.\n\n\n\nReal-world impact: partnering with ONE.org\n\nThis milestone was significantly shaped by the vision and substantial contributions of our partner The ONE Campaign, a global organization working to create the investments needed for economic opportunities and healthier lives in Africa. We built Data Commons as an open-source platform precisely to encourage community contributions and enable innovative uses, and this partnership with The ONE Campaign perfectly exemplifies that goal. ONE advocated for, proposed the design and coded the client library to make Data Commons' rich insights available to data scientists and analysts who want to leverage the rich ecosystem of Python analytical tools and libraries.\n\n\n\nSupport for custom Data Commons instances\n\nThe Data Commons platform also allows organizations, like the United Nations or ONE, to host their own Data Commons instances. These custom instances enable the seamless integration of proprietary datasets with the foundational Data Commons knowledge graph. Organizations leverage the Data Commons data framework and tools while maintaining full control over their data and resources.\n\nOne of the most impactful additions in the V2 library is robust support for custom instances. This means you can now use the Python library to programmatically query any public or private instance\u2014whether hosted locally, within your organization or on the Google Cloud Platform.\n\n\n\nPowerful new features\n\nThe Python library makes it very easy to perform common queries against Data Commons data, such as:\n\nExploring the structure of the knowledge graph\n\nRetrieving data for any of the 200,000+ statistical variables from over 200 datasets in domains such as demographics, economy, education, energy, environment, health, and housing\n\nEasily mapping entities from other datasets to entities in Data Commons\n\n\n\nV2 of the client library offers many technical improvements over the V1 library, including:\n\nPandas dataframe APIs are supported as an integral module, with a single installation package, allowing seamless use with other API endpoints in the same client\n\nSeveral new convenience methods for common data queries\n\nAPI key management and other stateful operations built in to the client class\n\nIntegration with the Pydantic libraries for improved type safety, validation and serialization", "label": 0}
{"title": "Google Pay inside sandboxed iframe for PCI DSS v4 compliance", "url": "https://developers.googleblog.com/en/google-pay-inside-sandboxed-iframe-for-pci-dss-v4-compliance/", "content": "Using a sandboxed iframe satisfies any concerns with compliance since scripts within the iFrame will not have access to the parent DOM. See the following illustration for an example:\n\nOne way to comply with this requirement is to use a technique like Subresource Integrity (SRI) . However, the Google Pay JavaScript (pay.js) build and release process does not allow for a long-lived, stable hash required by techniques like SRI.\n\nIf you are developing or maintaining a checkout page you might come across PCI DSS v4 which includes the following requirement under 6.4.3:\n\nIn this case the domain \u201ccdn.somewhereelse.com\u201d would load Google Pay\u2019s pay.js JavaScript file. After a successful transaction, the inner iframe can communicate with the parent page through mechanisms like window.postMessage() if needed.\n\nIn order for Google Pay to work in all browsers we need the following 4 sandbox attribute values in addition to allow=\u201dpayment\u201d :\n\nallow-scripts\n\nTo allow the iframe to execute scripts (pay.js as an example)\n\nallow-popups\n\nAllows the embedded page to create 'child browsing contexts'. In practice, this flag enables the embedded iframe to open new tabs and windows when the user clicks a link.\n\nallow-same-origin\n\nIf not set, fails on various occasions for browsers. If set, the iframe has access to the parents storage and cookies.\n\nallow-forms\n\nAllows forms such as the Google Pay login to submit the data.\n\nSee this test page to see the various iframe sandbox values in action.\n\n\n\nShopify successfully certified for PCI DSS v4\n\nGoogle Pay partnered with Shopify to implement the above solution. Shopify was able to successfully pass the PCI DSS v4 audit by using a sandboxed iframe to display the Google Pay button. Here is what Shopify has to say:\n\nWe\u2019ve built Shopify Checkout in such a way that Google Pay code executes in a secure sandboxed environment, allowing us to maintain the integrity of our checkout and comply with PCI DSS V4 requirements.\n\n\n\n\u2013 Ilya Grigorik, Distinguished Engineer at Shopify\n\nFor more information on how Shopify built their checkout solution using sandboxed iframes, their \u201cPowering Shopify\u2019s High-Performance, PCI DSS v4 Compliant Checkout with Sandboxing\u201d blog post has the insights.\n\n\n\nConclusion\n\nWrapping your Google Pay integration in a sandboxed iframe can help you to comply with PCI DSS v4 requirements. For more assistance with your implementation, sign in to the Google Pay & Wallet Console to create a support ticket. In addition, you can join the developer community in the #payments channel on Discord.\n\nFollow @GooglePayDevs on X for future updates. If you have questions, tag @GooglePayDevs and include #AskGooglePayDevs in your tweets.", "label": 0}
{"title": "Maestro: Data/ML Workflow Orchestrator at Netflix", "url": "https://netflixtechblog.com/maestro-netflixs-workflow-orchestrator-ee13a06f9c78?source=collection_home---4------20-----------------------", "content": "Maestro: Data/ML Workflow Orchestrator at Netflix Netflix Technology Blog 18 min read \u00b7 Jul 22, 2024 -- 12 Listen Share\n\nBy Jun He, Natallia Dzenisenka, Praneeth Yenugutala, Yingyi Zhang, and Anjali Norwood\n\nTL;DR\n\nWe are thrilled to announce that the Maestro source code is now open to the public! Please visit the Maestro GitHub repository to get started. If you find it useful, please give us a star.\n\nWhat is Maestro\n\nMaestro is a horizontally scalable workflow orchestrator designed to manage large-scale Data/ML workflows such as data pipelines and machine learning model training pipelines. It oversees the entire lifecycle of a workflow, from start to finish, including retries, queuing, task distribution to compute engines, etc.. Users can package their business logic in various formats such as Docker images, notebooks, bash script, SQL, Python, and more. Unlike traditional workflow orchestrators that only support Directed Acyclic Graphs (DAGs), Maestro supports both acyclic and cyclic workflows and also includes multiple reusable patterns, including foreach loops, subworkflow, and conditional branch, etc.\n\nOur Journey with Maestro\n\nSince we first introduced Maestro in this blog post, we have successfully migrated hundreds of thousands of workflows to it on behalf of users with minimal interruption. The transition was seamless, and Maestro has met our design goals by handling our ever-growing workloads. Over the past year, we\u2019ve seen a remarkable 87.5% increase in executed jobs. Maestro now launches thousands of workflow instances and runs half a million jobs daily on average, and has completed around 2 million jobs on particularly busy days.\n\nScalability and Versatility\n\nMaestro is a fully managed workflow orchestrator that provides Workflow-as-a-Service to thousands of end users, applications, and services at Netflix. It supports a wide range of workflow use cases, including ETL pipelines, ML workflows, AB test pipelines, pipelines to move data between different storages, etc. Maestro\u2019s horizontal scalability ensures it can manage both a large number of workflows and a large number of jobs within a single workflow.\n\nAt Netflix, workflows are intricately connected. Splitting them into smaller groups and managing them across different clusters adds unnecessary complexity and degrades the user experience. This approach also requires additional mechanisms to coordinate these fragmented workflows. Since Netflix\u2019s data tables are housed in a single data warehouse, we believe a single orchestrator should handle all workflows accessing it.\n\nJoin us on this exciting journey by exploring the Maestro GitHub repository and contributing to its ongoing development. Your support and feedback are invaluable as we continue to improve the Maestro project.\n\nIntroducing Maestro\n\nNetflix Maestro offers a comprehensive set of features designed to meet the diverse needs of both engineers and non-engineers. It includes the common functions and reusable patterns applicable to various use cases in a loosely coupled way.\n\nA workflow definition is defined in a JSON format. Maestro combines user-supplied fields with those managed by Maestro to form a flexible and powerful orchestration definition. An example can be found in the Maestro repository wiki.\n\nA Maestro workflow definition comprises two main sections: properties and versioned workflow including its metadata. Properties include author and owner information, and execution settings. Maestro preserves key properties across workflow versions, such as author and owner information, run strategy, and concurrency settings. This consistency simplifies management and aids in trouble-shootings. If the ownership of the current workflow changes, the new owner can claim the ownership of the workflows without creating a new workflow version. Users can also enable the triggering or alerting features for a given workflow over the properties.\n\nVersioned workflow includes attributes like a unique identifier, name, description, tags, timeout settings, and criticality levels (low, medium, high) for prioritization. Each workflow change creates a new version, enabling tracking and easy reversion, with the active or the latest version used by default. A workflow consists of steps, which are the nodes in the workflow graph defined by users. Steps can represent jobs, another workflow using subworkflow step, or a loop using foreach step. Steps consist of unique identifiers, step types, tags, input and output step parameters, step dependencies, retry policies, and failure mode, step outputs, etc. Maestro supports configurable retry policies based on error types to enhance step resilience.\n\nThis high-level overview of Netflix Maestro\u2019s workflow definition and properties highlights its flexibility to define complex workflows. Next, we dive into some of the useful features in the following sections.\n\nWorkflow Run Strategy\n\nUsers want to automate data pipelines while retaining control over the execution order. This is crucial when workflows cannot run in parallel or must halt current executions when new ones occur. Maestro uses predefined run strategies to decide whether a workflow instance should run or not. Here is the list of predefined run strategies Maestro offers.\n\nSequential Run Strategy\n\nThis is the default strategy used by maestro, which runs workflows one at a time based on a First-In-First-Out (FIFO) order. With this run strategy, Maestro runs workflows in the order they are triggered. Note that an execution does not depend on the previous states. Once a workflow instance reaches one of the terminal states, whether succeeded or not, Maestro will start the next one in the queue.\n\nStrict Sequential Run Strategy\n\nWith this run strategy, Maestro will run workflows in the order they are triggered but block execution if there\u2019s a blocking error in the workflow instance history. Newly triggered workflow instances are queued until the error is resolved by manually restarting the failed instances or marking the failed ones unblocked.\n\nIn the above example, run5 fails at 5AM, then later runs are queued but do not run. When someone manually marks run5 unblocked or restarts it, then the workflow execution will resume. This run strategy is useful for time insensitive but business critical workflows. This gives the workflow owners the option to review the failures at a later time and unblock the executions after verifying the correctness.\n\nFirst-only Run Strategy\n\nWith this run strategy, Maestro ensures that the running workflow is complete before queueing a new workflow instance. If a new workflow instance is queued while the current one is still running, Maestro will remove the queued instance. Maestro will execute a new workflow instance only if there is no workflow instance currently running, effectively turning off queuing with this run strategy. This approach helps to avoid idempotency issues by not queuing new workflow instances.\n\nLast-only Run Strategy\n\nWith this run strategy, Maestro ensures the running workflow is the latest triggered one and keeps only the last instance. If a new workflow instance is queued while there is an existing workflow instance already running, Maestro will stop the running instance and execute the newly triggered one. This is useful if a workflow is designed to always process the latest data, such as processing the latest snapshot of an entire table each time.\n\nParallel with Concurrency Limit Run Strategy\n\nWith this run strategy, Maestro will run multiple triggered workflow instances in parallel, constrained by a predefined concurrency limit. This helps to fan out and distribute the execution, enabling the processing of large amounts of data within the time limit. A common use case for this strategy is for backfilling the old data.\n\nParameters and Expression Language Support\n\nIn Maestro, parameters play an important role. Maestro supports dynamic parameters with code injection, which is super useful and powerful. This feature significantly enhances the flexibility and dynamism of workflows, allowing using parameters to control execution logic and enable state sharing between workflows and their steps, as well as between upstream and downstream steps. Together with other Maestro features, it makes the defining of workflows dynamic and enables users to define parameterized workflows for complex use cases.\n\nHowever, code injection introduces significant security and safety concerns. For example, users might unintentionally write an infinite loop that creates an array and appends items to it, eventually crashing the server with out-of-memory (OOM) issues. While one approach could be to ask users to embed the injected code within their business logic instead of the workflow definition, this would impose additional work on users and tightly couple their business logic with the workflow. In certain cases, this approach blocks users to design some complex parameterized workflows.\n\nTo mitigate these risks and assist users to build parameterized workflows, we developed our own customized expression language parser, a simple, secure, and safe expression language (SEL). SEL supports code injection while incorporating validations during syntax tree parsing to protect the system. It leverages the Java Security Manager to restrict access, ensuring a secure and controlled environment for code execution.\n\nSimple, Secure, and Safe Expression Language (SEL)\n\nSEL is a homemade simple, secure, and safe expression language (SEL) to address the risks associated with code injection within Maestro parameterized workflows. It is a simple expression language and the grammar and syntax follow JLS (Java Language Specifications). SEL supports a subset of JLS, focusing on Maestro use cases. For example, it supports data types for all Maestro parameter types, raising errors, datetime handling, and many predefined utility methods. SEL also includes additional runtime checks, such as loop iteration limits, array size checks, object memory size limits and so on, to enhance security and reliability. For more details about SEL, please refer to the Maestro GitHub documentation.\n\nOutput Parameters\n\nTo further enhance parameter support, Maestro allows for callable step execution, which returns output parameters from user execution back to the system. The output data is transmitted to Maestro via its REST API, ensuring that the step runtime does not have direct access to the Maestro database. This approach significantly reduces security concerns.\n\nParameterized Workflows\n\nThanks to the powerful parameter support, users can easily create parameterized workflows in addition to static ones. Users enjoy defining parameterized workflows because they are easy to manage and troubleshoot while being powerful enough to solve complex use cases.\n\nStatic workflows are simple and easy to use but come with limitations. Often, users have to duplicate the same workflow multiple times to accommodate minor changes. Additionally, workflow and jobs cannot share the states without using parameters.\n\nOn the other hand, completely dynamic workflows can be challenging to manage and support. They are difficult to debug or troubleshoot and hard to be reused by others.\n\nParameterized workflows strike a balance by being initialized step by step at runtime based on user defined parameters. This approach provides great flexibility for users to control the execution at runtime while remaining easy to manage and understand.\n\nAs we described in the previous Maestro blog post, parameter support enables the creation of complex parameterized workflows, such as backfill data pipelines.\n\nWorkflow Execution Patterns\n\nMaestro provides multiple useful building blocks that allow users to easily define dataflow patterns or other workflow patterns. It provides support for common patterns directly within the Maestro engine. Direct engine support not only enables us to optimize these patterns but also ensures a consistent approach to implementing them. Next, we will talk about the three major building blocks that Maestro provides.\n\nForeach Support\n\nIn Maestro, the foreach pattern is modeled as a dedicated step within the original workflow definition. Each iteration of the foreach loop is internally treated as a separate workflow instance, which scales similarly as any other Maestro workflow based on the step executions (i.e. a sub-graph) defined within the foreach definition block. The execution of sub-graph within a foreach step is delegated to a separate workflow instance. Foreach step then monitors and collects the status of these foreach workflow instances, each managing the execution of a single iteration. For more details, please refer to our previous Maestro blog post.\n\nThe foreach pattern is frequently used to repeatedly run the same jobs with different parameters, such as data backfilling or machine learning model tuning. It would be tedious and time consuming to request users to explicitly define each iteration in the workflow definition (potentially hundreds of thousands of iterations). Additionally, users would need to create new workflows if the foreach range changes, further complicating the process.\n\nConditional Branch Support\n\nThe conditional branch feature allows subsequent steps to run only if specific conditions in the upstream step are met. These conditions are defined using the SEL expression language, which is evaluated at runtime. Combined with other building blocks, users can build powerful workflows, e.g. doing some remediation if the audit check step fails and then run the job again.\n\nSubworkflow Support\n\nThe subworkflow feature allows a workflow step to run another workflow, enabling the sharing of common functions across multiple workflows. This effectively enables \u201cworkflow as a function\u201d and allows users to build a graph of workflows. For example, we have observed complex workflows consisting of hundreds of subworkflows to process data across hundreds tables, where subworkflows are provided by multiple teams.\n\nThese patterns can be combined together to build composite patterns for complex workflow use cases. For instance, we can loop over a set of subworkflows or run nested foreach loops. One example that Maestro users developed is an auto-recovery workflow that utilizes both conditional branch and subworkflow features to handle errors and retry jobs automatically.\n\nIn this example, subworkflow `job1` runs another workflow consisting of extract-transform-load (ETL) and audit jobs. Next, a status check job leverages the Maestro parameter and SEL support to retrieve the status of the previous job. Based on this status, it can decide whether to complete the workflow or to run a recovery job to address any data issues. After resolving the issue, it then executes subworkflow `job2`, which runs the same workflow as subworkflow `job1`.\n\nStep Runtime and Step Parameter\n\nStep Runtime Interface\n\nIn Maestro, we use step runtime to describe a job at execution time. The step runtime interface defines two pieces of information:\n\nA set of basic APIs to control the behavior of a step instance at execution runtime. Some simple data structures to track step runtime state and execution result.\n\nMaestro offers a few step runtime implementations such as foreach step runtime, subworkflow step runtime (mentioned in previous section). Each implementation defines its own logic for start, execute and terminate operations. At runtime, these operations control the way to initialize a step instance, perform the business logic and terminate the execution under certain conditions (i.e. manual intervention by users).\n\nAlso, Maestro step runtime internally keeps track of runtime state as well as the execution result of the step. The runtime state is used to determine the next state transition of the step and tell if it has failed or terminated. The execution result hosts both step artifacts and the timeline of step execution history, which are accessible by subsequent steps.\n\nStep Parameter Merging\n\nTo control step behavior in a dynamic way, Maestro supports both runtime parameters and tags injection in step runtime. This makes a Maestro step more flexible to absorb runtime changes (i.e. overridden parameters) before actually being started. Maestro internally maintains a step parameter map that is initially empty and is updated by merging step parameters in the order below:\n\nDefault General Parameters : Parameters merging starts from default parameters that in general every step should have. For example, workflow_instance_id, step_instance_uuid, step_attempt_id and step_id are required parameters for each maestro step. They are internally reserved by maestro and cannot be passed by users.\n\n: Parameters merging starts from default parameters that in general every step should have. For example, workflow_instance_id, step_instance_uuid, step_attempt_id and step_id are required parameters for each maestro step. They are internally reserved by maestro and cannot be passed by users. Injected Parameters : Maestro then merges injected parameters (if present) into the parameter map. The injected parameters come from step runtime, which are dynamically generated based on step schema. Each type of step can have its own schema with specific parameters associated with this step. The step schema can evolve independently with no need to update Maestro code.\n\n: Maestro then merges injected parameters (if present) into the parameter map. The injected parameters come from step runtime, which are dynamically generated based on step schema. Each type of step can have its own schema with specific parameters associated with this step. The step schema can evolve independently with no need to update Maestro code. Default Typed Parameters : After injecting runtime parameters, Maestro tries to merge default parameters that are related to a specific type of step. For example, foreach step has loop_params and loop_index default parameters which are internally set by maestro and used for foreach step only.\n\n: After injecting runtime parameters, Maestro tries to merge default parameters that are related to a specific type of step. For example, foreach step has loop_params and loop_index default parameters which are internally set by maestro and used for foreach step only. Workflow and Step Info Parameters : These parameters contain information about step and the workflow it belongs to. This can be identity information, i.e. workflow_id and will be merged to step parameter map if present.\n\n: These parameters contain information about step and the workflow it belongs to. This can be identity information, i.e. workflow_id and will be merged to step parameter map if present. Undefined New Parameters : When starting or restarting a maestro workflow instance, users can specify new step parameters that are not present in initial step definition. ParamsManager merges these parameters to ensure they are available at execution time.\n\n: When starting or restarting a maestro workflow instance, users can specify new step parameters that are not present in initial step definition. ParamsManager merges these parameters to ensure they are available at execution time. Step Definition Parameters : These step parameters are defined by users at definition time and get merged if they are not empty.\n\n: These step parameters are defined by users at definition time and get merged if they are not empty. Run and Restart Parameters: When starting or restarting a maestro workflow instance, users can override defined parameters by providing run or restart parameters. These two types of parameters are merged at the end so that step runtime can see the most recent and accurate parameter space.\n\nThe parameters merging logic can be visualized in the diagram below.\n\nStep Dependencies and Signals\n\nSteps in the Maestro execution workflow graph can express execution dependencies using step dependencies. A step dependency specifies the data-related conditions required by a step to start execution. These conditions are usually defined based on signals, which are pieces of messages carrying information such as parameter values and can be published through step outputs or external systems like SNS or Kafka messages.\n\nSignals in Maestro serve both signal trigger pattern and signal dependencies (a publisher-subscriber) pattern. One step can publish an output signal (a sample example) that can unblock the execution of multiple other steps that depend on it. A signal definition includes a list of mapped parameters, allowing Maestro to perform \u201csignal matching\u201d on a subset of fields. Additionally, Maestro supports signal operators like <, >, etc., on signal parameter values.\n\nNetflix has built various abstractions on top of the concept of signals. For instance, a ETL workflow can update a table with data and send signals that unblock steps in downstream workflows dependent on that data. Maestro supports \u201csignal lineage,\u201d which allows users to navigate all historical instances of signals and the workflow steps that match (i.e. publishing or consuming) those signals. Signal triggering guarantees exactly-once execution for the workflow subscribing a signal or a set of joined signals. This approach is efficient, as it conserves resources by only executing the workflow or step when the specified conditions in the signals are met. A signal service is implemented for those advanced abstractions. Please refer to the Maestro blog for further details on it.\n\nBreakpoint\n\nMaestro allows users to set breakpoints on workflow steps, functioning similarly to code-level breakpoints in an IDE. When a workflow instance executes and reaches a step with a breakpoint, that step enters a \u201cpaused\u201d state. This halts the workflow graph\u2019s progression until a user manually resumes from the breakpoint. If multiple instances of a workflow step are paused at a breakpoint, resuming one instance will only affect that specific instance, leaving the others in a paused state. Deleting the breakpoint will cause all paused step instances to resume.\n\nThis feature is particularly useful during the initial development of a workflow, allowing users to inspect step executions and output data. It is also beneficial when running a step multiple times in a \u201cforeach\u201d pattern with various input parameters. Setting a single breakpoint on a step will cause all iterations of the foreach loop to pause at that step for debugging purposes. Additionally, the breakpoint feature allows human intervention during the workflow execution and can also be used for other purposes, e.g. supporting mutating step states while the workflow is running.\n\nTimeline\n\nMaestro includes a step execution timeline, capturing all significant events such as execution state machine changes and the reasoning behind them. This feature is useful for debugging, providing insights into the status of a step. For example, it logs transitions such as \u201cCreated\u201d and \u201cEvaluating params\u201d, etc. An example of a timeline is included here for reference. The implemented step runtimes can add the timeline events into the timeline to surface the execution information to the end users.\n\nRetry Policies\n\nMaestro supports retry policies for steps that reach a terminal state due to failure. Users can specify the number of retries and configure retry policies, including delays between retries and exponential backoff strategies, in addition to fixed interval retries. Maestro distinguishes between two types of retries: \u201cplatform\u201d and \u201cuser.\u201d Platform retries address platform-level errors unrelated to user logic, while user retries are for user-defined conditions. Each type can have its own set of retry policies.\n\nAutomatic retries are beneficial for handling transient errors that can be resolved without user intervention. Maestro provides the flexibility to set retries to zero for non-idempotent steps to avoid retry. This feature ensures that users have control over how retries are managed based on their specific requirements.\n\nAggregated View\n\nBecause a workflow instance can have multiple runs, it is important for users to see an aggregated state of all steps in the workflow instance. Aggregated view is computed by merging base aggregated view with current runs instance step statuses. For example, as you can see on the figure below simulating a simple case, there is a first run, where step1 and step2 succeeded, step3 failed, and step4 and step5 have not started. When the user restarts the run, the run starts from step3 in run 2 with step1 and step2 skipped which succeeded in the previous run. After all steps succeed, the aggregated view shows the run states for all steps.\n\nRollup\n\nRollup provides a high-level summary of a workflow instance, detailing the status of each step and the count of steps in each status. It flattens steps across the current instance and any nested non-inline workflows like subworkflows or foreach steps. For instance, if a successful workflow has three steps, one of which is a subworkflow corresponding to a five-step workflow, the rollup will indicate that seven steps succeeded. Only leaf steps are counted in the rollup, as other steps serve merely as pointers to concrete workflows.\n\nRollup also retains references to any non-successful steps, offering a clear overview of step statuses and facilitating easy navigation to problematic steps, even within nested workflows. The aggregated rollup for a workflow instance is calculated by combining the current run\u2019s runtime data with a base rollup. The current state is derived from the statuses of active steps, including aggregated rollups for foreach and subworkflow steps. The base rollup is established when the workflow instance begins and includes statuses of inline steps (excluding foreach and subworkflows) from the previous run that are not part of the current run.\n\nFor subworkflow steps, the rollup simply reflects the rollup of the subworkflow instance. For foreach steps, the rollup combines the base rollup of the foreach step with the current state rollup. The base is derived from the previous run\u2019s aggregated rollup, excluding the iterations to be restarted in the new run. The current state is periodically updated by aggregating rollups of running iterations until all iterations reach a terminal state.\n\nDue to these processes, the rollup model is eventually consistent. While the figure below illustrates a straightforward example of rollup, the calculations can become complex and recursive, especially with multiple levels of nested foreaches and subworkflows.\n\nMaestro Event Publishing\n\nWhen workflow definition, workflow instance or step instance is changed, Maestro generates an event, processes it internally and publishes the processed event to external system(s). Maestro has both internal and external events. The internal event tracks changes within the life cycle of workflow, workflow instance or step instance. It is published to an internal queue and processed within Maestro. After internal events are processed, some of them will be transformed into external event and sent out to the external queue (i.e. SNS, Kafka). The external event carries maestro status change information for downstream services. The event publishing flow is illustrated in the diagram below:\n\nAs shown in the diagram, the Maestro event processor bridges the two aforementioned Maestro events. It listens on the internal queue to get the published internal events. Within the processor, the internal job event is processed based on its type and gets converted to an external event if needed. The notification publisher at the end emits the external event so that downstream services can consume.\n\nThe downstream services are mostly event-driven. The Maestro event carries the most useful message for downstream services to capture different changes in Maestro. In general, these changes can be classified into two categories: workflow change and instance status change. The workflow change event is associated with actions at workflow level, i.e definition or properties of a workflow has changed. Meanwhile, instance status change tracks status transition on workflow instance or step instance.\n\nGet Started with Maestro\n\nMaestro has been extensively used within Netflix, and today, we are excited to make the Maestro source code publicly available. We hope that the scalability and usability that Maestro offers can expedite workflow development outside Netflix. We invite you to try Maestro, use it within your organization, and contribute to its development.\n\nYou can find the Maestro code repository at github.com/Netflix/maestro. If you have any questions, thoughts, or comments about Maestro, please feel free to create a GitHub issue in the Maestro repository. We are eager to hear from you.\n\nWe are taking workflow orchestration to the next level and constantly solving new problems and challenges, please stay tuned for updates. If you are passionate about solving large scale orchestration problems, please join us.\n\nAcknowledgements\n\nThanks to other Maestro team members, Binbing Hou, Zhuoran Dong, Brittany Truong, Deepak Ramalingam, Moctar Ba, for their contributions to the Maestro project. Thanks to our Product Manager Ashim Pokharel for driving the strategy and requirements. We\u2019d also like to thank Andrew Seier, Romain Cledat, Olek Gorajek, and other stunning colleagues at Netflix for their contributions to the Maestro project. We also thank Prashanth Ramdas, Eva Tse, David Noor, Charles Smith and other leaders of Netflix engineering organizations for their constructive feedback and suggestions on the Maestro project.", "label": 0}
{"title": "FM-Intent: Predicting User Session Intent with Hierarchical Multi-Task Learning", "url": "https://netflixtechblog.com/fm-intent-predicting-user-session-intent-with-hierarchical-multi-task-learning-94c75e18f4b8?source=collection_home---4------0-----------------------", "content": "FM-Intent: Predicting User Session Intent with Hierarchical Multi-Task Learning Netflix Technology Blog 7 min read \u00b7 May 21, 2025 -- 4 Listen Share\n\nAuthors: Sejoon Oh, Moumita Bhattacharya, Yesu Feng, Sudarshan Lamkhede, Ko-Jen Hsiao, and Justin Basilico\n\nMotivation\n\nRecommender systems have become essential components of digital services across e-commerce, streaming media, and social networks [1, 2]. At Netflix, these systems drive significant product and business impact by connecting members with relevant content at the right time [3, 4]. While our recommendation foundation model (FM) has made substantial progress in understanding user preferences through large-scale learning from interaction histories (please refer to this article about FM @ Netflix), there is an opportunity to further enhance its capabilities. By extending FM to incorporate the prediction of underlying user intents, we aim to enrich its understanding of user sessions beyond next-item prediction, thereby offering a more comprehensive and nuanced recommendation experience.\n\nRecent research has highlighted the importance of understanding user intent in online platforms [5, 6, 7, 8]. As Xia et al. [8] demonstrated at Pinterest, predicting a user\u2019s future intent can lead to more accurate and personalized recommendations. However, existing intent prediction approaches typically employ simple multi-task learning that adds intent prediction heads to next-item prediction models without establishing a hierarchical relationship between these tasks.\n\nTo address these limitations, we introduce FM-Intent, a novel recommendation model that enhances our foundation model through hierarchical multi-task learning. FM-Intent captures a user\u2019s latent session intent using both short-term and long-term implicit signals as proxies, then leverages this intent prediction to improve next-item recommendations. Unlike conventional approaches, FM-Intent establishes a clear hierarchy where intent predictions directly inform item recommendations, creating a more coherent and effective recommendation pipeline.\n\nFM-Intent makes three key contributions:\n\nA novel recommendation model that captures user intent on the Netflix platform and enhances next-item prediction using this intent information. A hierarchical multi-task learning approach that effectively models both short-term and long-term user interests. Comprehensive experimental validation showing significant performance improvements over state-of-the-art models, including our foundation model.\n\nUnderstanding User Intent in Netflix\n\nIn the Netflix ecosystem, user intent manifests through various interaction metadata, as illustrated in Figure 1. FM-Intent leverages these implicit signals to predict both user intent and next-item recommendations.\n\nFigure 1: Overview of user engagement data in Netflix. User intent can be associated with several interaction metadata. We leverage various implicit signals to predict user intent and next-item.\n\nIn Netflix, there can be multiple types of user intents. For instance,\n\nAction Type: Categories reflecting what users intend to do on Netflix, such as discovering new content versus continuing previously started content. For example, when a member plays a follow-up episode of something they were already watching, this can be categorized as \u201ccontinue watching\u201d intent. Genre Preference: The pre-defined genre labels (e.g., Action, Thriller, Comedy) that indicate a user\u2019s content preferences during a session. These preferences can shift significantly between sessions, even for the same user. Movie/Show Type: Whether a user is looking for a movie (typically a single, longer viewing experience) or a TV show (potentially multiple episodes of shorter duration). Time-since-release: Whether the user prefers newly released content, recent content (e.g., between a week and a month), or evergreen catalog titles.\n\nThese dimensions serve as proxies for the latent user intent, which is often not directly observable but crucial for providing relevant recommendations.\n\nFM-Intent Model Architecture\n\nFM-Intent employs a hierarchical multi-task learning approach with three major components, as illustrated in Figure 2.\n\nFigure 2: An architectural illustration of our hierarchical multi-task learning model FM-Intent for user intent and item predictions. We use ground-truth intent and item-ID labels to optimize predictions.\n\n1. Input Feature Sequence Formation\n\nThe first component constructs rich input features by combining interaction metadata. The input feature for each interaction combines categorical embeddings and numerical features, creating a comprehensive representation of user behavior.\n\n2. User Intent Prediction\n\nThe intent prediction component processes the input feature sequence through a Transformer encoder and generates predictions for multiple intent signals.\n\nThe Transformer encoder effectively models the long-term interest of users through multi-head attention mechanisms. For each prediction task, the intent encoding is transformed into prediction scores via fully-connected layers.\n\nA key innovation in FM-Intent is the attention-based aggregation of individual intent predictions. This approach generates a comprehensive intent embedding that captures the relative importance of different intent signals for each user, providing valuable insights for personalization and explanation.\n\n3. Next-Item Prediction with Hierarchical Multi-Task Learning\n\nThe final component combines the input features with the user intent embedding to make more accurate next-item recommendations.\n\nFM-Intent employs hierarchical multi-task learning where intent predictions are conducted first, and their results are used as input features for the next-item prediction task. This hierarchical relationship ensures that the next-item recommendations are informed by the predicted user intent, creating a more coherent and effective recommendation model.\n\nOffline Results\n\nWe conducted comprehensive offline experiments on sampled Netflix user engagement data to evaluate FM-Intent\u2019s performance. Note that FM-Intent uses a much smaller dataset for training compared to the FM production model due to its complex hierarchical prediction architecture.\n\nNext-Item and Next-Intent Prediction Accuracy\n\nTable 1 compares FM-Intent with several state-of-the-art sequential recommendation models, including our production model (FM-Intent-V0).\n\nTable 1: Next-item and next-intent prediction results of baselines and our proposed method FM-Intent on the Netflix user engagement dataset.\n\nAll metrics are represented as relative % improvements compared to the SOTA baseline: TransAct. N/A indicates that a model is not capable of predicting a certain intent. Note that we added additional fully-connected layers to LSTM, GRU, and Transformer baselines in order to predict user intent, while we used original implementations for other baselines. FM-Intent demonstrates statistically significant improvement of 7.4% in next-item prediction accuracy compared to the best baseline (TransAct).\n\nMost baseline models show limited performance as they either cannot predict user intent or cannot incorporate intent predictions into next-item recommendations. Our production model (FM-Intent-V0) performs well but lacks the ability to predict and leverage user intent. Note that FM-Intent-V0 is trained with a smaller dataset for a fair comparison with other models; the actual production model is trained with a much larger dataset.\n\nQualitative Analysis: User Clustering\n\nFigure 3: K-means++ (K=10) clustering of user intent embeddings found by FM-Intent; FM-Intent finds unique clusters of users that share the similar intent.\n\nFM-Intent generates meaningful user intent embeddings that can be used for clustering users with similar intents. Figure 3 visualizes 10 distinct clusters identified through K-means++ clustering. These clusters reveal meaningful user segments with distinct viewing patterns:\n\nUsers who primarily discover new content versus those who continue watching recent/favorite content.\n\nGenre enthusiasts (e.g., anime/kids content viewers).\n\nUsers with specific viewing patterns (e.g., Rewatchers versus casual viewers).\n\nPotential Applications of FM-Intent\n\nFM-Intent has been successfully integrated into Netflix\u2019s recommendation ecosystem, can be leveraged for several downstream applications:\n\nPersonalized UI Optimization: The predicted user intent could inform the layout and content selection on the Netflix homepage, emphasizing different rows based on whether users are in discovery mode, continue-watching mode, or exploring specific genres. Analytics and User Understanding: Intent embeddings and clusters provide valuable insights into viewing patterns and preferences, informing content acquisition and production decisions. Enhanced Recommendation Signals: Intent predictions serve as features for other recommendation models, improving their accuracy and relevance. Search Optimization: Real-time intent predictions help prioritize search results based on the user\u2019s current session intent.\n\nConclusion\n\nFM-Intent represents an advancement in Netflix\u2019s recommendation capabilities by enhancing them with hierarchical multi-task learning for user intent prediction. Our comprehensive experiments demonstrate that FM-Intent significantly outperforms state-of-the-art models, including our prior foundation model that focused solely on next-item prediction. By understanding not just what users might watch next but what underlying intents users have, we can provide more personalized, relevant, and satisfying recommendations.\n\nAcknowledgements\n\nWe thank our stunning colleagues in the Foundation Model team & AIMS org. for their valuable feedback and discussions. We also thank our partner teams for getting this up and running in production.\n\nReferences\n\n[1] Amatriain, X., & Basilico, J. (2015). Recommender systems in industry: A netflix case study. In Recommender systems handbook (pp. 385\u2013419). Springer.\n\n[2] Gomez-Uribe, C. A., & Hunt, N. (2015). The netflix recommender system: Algorithms, business value, and innovation. ACM Transactions on Management Information Systems (TMIS), 6(4), 1\u201319.\n\n[3] Jannach, D., & Jugovac, M. (2019). Measuring the business value of recommender systems. ACM Transactions on Management Information Systems (TMIS), 10(4), 1\u201323.\n\n[4] Bhattacharya, M., & Lamkhede, S. (2022). Augmenting Netflix Search with In-Session Adapted Recommendations. In Proceedings of the 16th ACM Conference on Recommender Systems (pp. 542\u2013545).\n\n[5] Chen, Y., Liu, Z., Li, J., McAuley, J., & Xiong, C. (2022). Intent contrastive learning for sequential recommendation. In Proceedings of the ACM Web Conference 2022 (pp. 2172\u20132182).\n\n[6] Ding, Y., Ma, Y., Wong, W. K., & Chua, T. S. (2021). Modeling instant user intent and content-level transition for sequential fashion recommendation. IEEE Transactions on Multimedia, 24, 2687\u20132700.\n\n[7] Liu, Z., Chen, H., Sun, F., Xie, X., Gao, J., Ding, B., & Shen, Y. (2021). Intent preference decoupling for user representation on online recommender system. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence (pp. 2575\u20132582).\n\n[8] Xia, X., Eksombatchai, P., Pancha, N., Badani, D. D., Wang, P. W., Gu, N., Joshi, S. V., Farahpour, N., Zhang, Z., & Zhai, A. (2023). TransAct: Transformer-based Realtime User Action Model for Recommendation at Pinterest. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (pp. 5249\u20135259).", "label": 0}
{"title": "How to make Storybook Interactions respect user motion preferences", "url": "https://github.blog/engineering/user-experience/how-to-make-storybook-interactions-respect-user-motion-preferences/", "content": "Recently, while browsing my company\u2019s Storybook, I came across something that seemed broken: a flickering component that appeared to be re-rendering repeatedly. The open source tool that helps designers, developers, and others build and use reusable components was behaving weirdly. As I dug in, I realized I was seeing the unintended effects of the Storybook Interactions addon, which allows developers to simulate user interactions within a story, in action.\n\nStorybook Interactions can be a powerful tool, enabling developers to simulate and test user behaviors quickly. But if you\u2019re unfamiliar with Interactions\u2014especially if you\u2019re just looking to explore available components\u2014the simulated tests jumping around on the screen can feel disorienting.\n\nThis can be especially jarring for users who have the prefers-reduced-motion setting enabled in their operating system. When these users encounter a story that includes an interaction, their preferences are ignored and they have no option to disable or enable it. Instead, the Storybook Interaction immediately plays on page load, regardless. These rapid screen movements can cause disorientation for users or in some cases can even trigger a seizure.\n\nKnowledge share Operating systems allow users to set a motion preference. Adhering to this setting can be critical for some users. For example, for users who have photosensitive epilepsy or vertigo, even a small animation can be life-threatening. This explicit preference for a reduced motion experience can be used by browsers, applications, and websites to reduce unnecessary animations and motions via the prefers-reduced-motion CSS media feature.\n\nAt this time, Storybook does not have built-in capabilities to toggle interactions on or off. Until this feature can be baked in I am hoping this blog will provide you with an alternative way to make your work environment more inclusive. Now, let\u2019s get into building an addon that respects user\u2019s motion preferences and allows users to toggle interactions on and off.\n\nGoals\n\nUsers with prefers-reduced-motion enabled MUST have interactions off by default. Users with prefers-reduced-motion enabled MUST have a way to toggle the feature on or off without altering their operating system user preferences. All users SHOULD have a way to toggle the feature on or off without altering their user preferences.\n\nLet\u2019s get started\n\nStep 1: Build a Storybook addon\n\nStorybook allows developers to create custom addons. In this case, we will create one that will allow users to toggle Interactions on or off, while respecting the prefers-reduced-motion setting.\n\nAdd the following code to a file in your project\u2019s .storybook folder:\n\nimport React, {useCallback, useEffect} from 'react' import {IconButton} from '@storybook/components' import {PlayIcon, StopIcon} from '@storybook/icons' export const ADDON_ID = 'toggle-interaction' export const TOOL_ID = `${ADDON_ID}/tool` export const INTERACTION_STORAGE_KEY = 'disableInteractions' export const InteractionToggle = () => { const [disableInteractions, setDisableInteractions] = React.useState( window?.localStorage.getItem(INTERACTION_STORAGE_KEY) === 'true', ) useEffect(() => { const reducedMotion = matchMedia('(prefers-reduced-motion)') if (window?.localStorage.getItem(INTERACTION_STORAGE_KEY) === null && reducedMotion.matches) { window?.localStorage?.setItem(INTERACTION_STORAGE_KEY, 'true') setDisableInteractions(true) } }, []) const toggleMyTool = useCallback(() => { window?.localStorage?.setItem(INTERACTION_STORAGE_KEY, `${!disableInteractions}`) setDisableInteractions(!disableInteractions) // Refreshes the page to cause the interaction to stop/start window.location.reload() }, [disableInteractions, setDisableInteractions]) return ( <IconButton key={TOOL_ID} aria-label=\"Disable Interactions\" onClick={toggleMyTool} defaultChecked={disableInteractions} aria-pressed={disableInteractions} > {disableInteractions ? <PlayIcon /> : <StopIcon />} Interactions </IconButton> ) }\n\nCode breakdown\n\nThis addon stores user preferences for Interactions using window.localStorage . When the addon first loads, it checks whether the preference is already set and, if so, it defaults to the user\u2019s preference.\n\nconst [disableInteractions, setDisableInteractions] = React.useState( window?.localStorage.getItem(INTERACTION_STORAGE_KEY) === 'true', )\n\nThis useEffect hook checks if a user has their motion preferences set to prefers-reduced-motion and ensures that Interactions are turned off if the user hasn\u2019t already set a preference in Storybook.\n\nuseEffect(() => { const reducedMotion = matchMedia('(prefers-reduced-motion)') if (window?.localStorage.getItem(INTERACTION_STORAGE_KEY) === null && reducedMotion.matches) { window?.localStorage?.setItem(INTERACTION_STORAGE_KEY, 'true') setDisableInteractions(true) } }, [])\n\nWhen a user clicks the toggle button, preferences are updated and the page is refreshed to reflect the changes.\n\nconst toggleMyTool = useCallback(() => { window?.localStorage?.setItem(INTERACTION_STORAGE_KEY, `${!disableInteractions}`) setDisableInteractions(!disableInteractions) // Refreshes the page to cause the interaction to stop/start window.location.reload() }, [disableInteractions, setDisableInteractions])\n\nStep 2: Register your new addon with Storybook\n\nIn your .storybook/manager file, register your new addon:\n\naddons.register(ADDON_ID, () => { addons.add(TOOL_ID, { title: 'toggle interaction', type: types.TOOL as any, match: ({ viewMode, tabId }) => viewMode === 'story' && !tabId, render: () => <InteractionToggle />, }) })\n\nThis adds the toggle button to the Storybook toolbar, which will allow users to change their Storybook Interaction preferences.\n\nStep 3: Add functionality to check user preferences\n\nFinally, create a function that checks whether Interactions should be played and add it to your interaction stories:\n\nimport {INTERACTION_STORAGE_KEY} from './.storybook/src/InteractionToggle' export const shouldInteractionPlay = () => { const disableInteractions = window?.localStorage?.getItem(INTERACTION_STORAGE_KEY) return disableInteractions === 'false' || disableInteractions === null } export const SomeComponentStory = { render: SomeComponent, play: async ({context}) => { if (shouldInteractionPlay()) { ... } }) }\n\nWith this custom addon, you can ensure your workplace remains accessible to users with motion sensitivities while benefiting from Storybook\u2019s Interactions. For those with prefers-reduced-motion enabled, motion will be turned off by default and all users will be able to toggle interactions on or off.", "label": 0}
{"title": "What motivates you to work hard?", "url": "https://lifeofpablo.com/blog/q-2", "content": "What motivates you to work hard?\n\nWhat motivates you to work hard?\n\nThis post was written in English (en_US).\n\nThis post responds to my blog post. I am answering question #2\n\nWhat motivates me to work hard? Now that's a good question!\n\nWhat motivates me is trying to reach an end goal. In order to reach happiness, to reach financial stability, to find a direction for some sort of purpose. What motivates me to work hard is how I put in the energy to give it my all so the result feels that I accomplished something.\n\nI work hard due to my work ethic and the values I learned while growing up. How can I become equal to those around me and not be seen as less? Certain factors oblige me that I have to work harder for the things I aspire to reach. I am more than capable as those around me. Growing up, I knew things weren't going to be easy.\n\nI work hard so I can provide my parents with a better life. They did their best and I want to be able to help them as they are aging. I want to work hard to help them also enjoy life.\n\nAm I doing enough? I'm not sure.", "label": 1}
{"title": "What's new with Agents: ADK, Agent Engine, and A2A Enhancements", "url": "https://developers.googleblog.com/en/agents-adk-agent-engine-a2a-enhancements-google-io/", "content": "At Google, we envision a future where intelligent agents are not just tools, but collaborative partners in solving complex challenges, streamlining workflows, and unlocking new possibilities. We believe that empowering developers with a platform that offers flexibility, trust, and comprehensive capabilities is key to realizing this potential. Today, we're thrilled to share a series of significant updates across our product portfolio that reflect this vision, designed to help you build and manage your intelligent agents with unprecedented ease and power. These enhancements focus on providing robust development tools, intuitive management interfaces, and seamless agent-to-agent communications, delivering a stronger foundation for the next generation of AI powered solutions.\n\nBuilding with confidence and flexibility: Agent Development Kit (ADK) To empower you to create sophisticated agents with stability and adaptability, we've added significant innovations with our Agent Development Kit (ADK). Python ADK v1.0.0: Stability for Production-Ready Agents We're excited to announce the v1.0.0 stable release of our Python Agent Development Kit. This milestone signifies that the Python ADK is now production-ready, offering a reliable and robust platform for developers to confidently build and deploy their agents in live environments. We've heard incredible feedback from customers using Agent Development Kit already, including Renault Group, Box, and Revionics. Java ADK v0.1.0: Extending Agent Capabilities to the Java Ecosystem Expanding our reach, we're also launching the initial release of the Java ADK v0.1.0. This development brings the power and flexibility of the ADK to Java developers, enabling them to leverage its capabilities for their agent development needs.\n\nTo get started with the Java ADK, you can add the following dependency to your Maven project:\n\n<dependency> <groupId>com.google.adk</groupId> <artifactId>google-adk</artifactId> <version>0.1.0</version> </dependency> XML Copied\n\nIntuitive control and management: The Agent Engine UI The Vertex AI Agent Engine helps developers deploy, manage, and scale agents in production. We\u2019re excited to now offer an Agent Engine UI to simplify the agent lifecycle in a more straightforward and centralized way. This user-friendly interface, accessible within the Google Cloud console, provides a comprehensive dashboard to view and manage your deployed agents, list sessions, trace and debug actions, and monitor your agents. This streamlined approach significantly enhances the development and management process, offering you greater control and deeper insights into your agent's behavior and performance.", "label": 0}
{"title": "How GitHub uses CodeQL to secure GitHub", "url": "https://github.blog/engineering/how-github-uses-codeql-to-secure-github/", "content": "GitHub\u2019s Product Security Engineering team writes code and implements tools that help secure the code that powers GitHub. We use GitHub Advanced Security (GHAS) to discover, track, and remediate vulnerabilities and enforce secure coding standards at scale. One tool we rely heavily on to analyze our code at scale is CodeQL.\n\nCodeQL is GitHub\u2019s static analysis engine that powers automated security analyses. You can use it to query code in much the same way you would query a database. It provides a much more robust way to analyze code and uncover problems than an old-fashioned text search through a codebase.\n\nThe following post will detail how we use CodeQL to keep GitHub secure and how you can apply these lessons to your own organization. You will learn why and how we use:\n\nCustom query packs (and how we create and manage them).\n\nCustom queries.\n\nVariant analysis to uncover potentially insecure programming practices.\n\nEnabling CodeQL at scale\n\nWe employ CodeQL in a variety of ways at GitHub.\n\nDefault setup with the default and security-extended query suites\n\nDefault setup with the default and security-extended query suites meets the needs of the vast majority of our over 10,000 repositories. With these settings, pull requests automatically get a security review from CodeQL. Advanced setup with a custom query pack\n\nA few repositories, like our large Ruby monolith, need extra special attention, so we use advanced setup with a query pack containing custom queries to really tailor to our needs. Multi-repository variant analysis (MRVA)\n\nTo conduct variant analysis and quick auditing, we use MRVA. We also write custom CodeQL queries to detect code patterns that are either specific to GitHub\u2019s codebases or patterns we want a security engineer to manually review.\n\nThe specific custom Actions workflow step we use on our monolith is pretty simple. It looks like this:\n\n- name: Initialize CodeQL uses: github/codeql-action/init@v3 with: languages: ${{ matrix.language }} config-file: ./.github/codeql/${{ matrix.language }}/codeql-config.yml\n\nOur Ruby configuration is pretty standard, but advanced setup offers a variety of configuration options using custom configuration files. The interesting part is the packs option, which is how we enable our custom query pack as part of the CodeQL analysis. This pack contains a collection of CodeQL queries we have written for Ruby, specifically for the GitHub codebase.\n\nSo, let\u2019s dive deeper into why we did that\u2014and how!\n\nPublishing our CodeQL query pack\n\nInitially, we published CodeQL query files directly to the GitHub monolith repository, but we moved away from this approach for several reasons:\n\nIt required going through the production deployment process for each new or updated query.\n\nQueries not included in a query pack were not pre-compiled, which slowed down CodeQL analysis in CI.\n\nOur test suite for CodeQL queries ran as part of the monolith\u2019s CI jobs. When a new version of the CodeQL CLI was released, it sometimes caused the query tests to fail because of changes in the query output, even when there were no changes to the code in the pull request. This often led to confusion and frustration among engineers, as the failure wasn\u2019t related to their pull request changes.\n\nBy switching to publishing a query pack to GitHub Container Registry (GCR), we\u2019ve simplified our process and eliminated many of these pain points, making it easier to ship and maintain our CodeQL queries. So while it\u2019s possible to deploy custom CodeQL query files directly to a repository, we recommend publishing CodeQL queries as a query pack to the GCR for easier deployment and faster iteration.\n\nCreating our query pack\n\nWhen setting up our custom query pack, we faced several considerations, particularly around managing dependencies like the ruby-all package.\n\nTo ensure our custom queries remain maintainable and concise, we extend classes from the default query suite, such as the ruby-all library. This allows us to leverage existing functionality rather than reinventing the wheel, keeping our queries concise and maintainable. However, changes to the CodeQL library API can introduce breaking changes, potentially deprecating our queries or causing errors. Since CodeQL runs as part of our CI, we wanted to minimize the chance of this happening, as this can lead to frustration and loss of trust from developers.\n\nWe develop our queries against the latest version of the ruby-all package, ensuring we\u2019re always working with the most up-to-date functionality. To mitigate the risk of breaking changes affecting CI, we pin the ruby-all version when we\u2019re ready to release, locking it in the codeql-pack.lock.yml file. This guarantees that when our queries are deployed, they will run with the specific version of ruby-all we\u2019ve tested, avoiding potential issues from unintentional updates.\n\nHere\u2019s how we manage this setup:\n\nIn our qlpack.yml, we set the dependency to use the latest version of ruby-all\n\nDuring development, this configuration pulls in the latest version) of ruby-all when running codeql pack init , ensuring we\u2019re always up to date. // Our custom query pack's qlpack.yml library: false name: github/internal-ruby-codeql version: 0.2.3 extractor: 'ruby' dependencies: codeql/ruby-all: \"*\" tests: 'test' description: \"Ruby CodeQL queries used internally at GitHub\"\n\nwhen running , ensuring we\u2019re always up to date. Before releasing, we lock the version in the codeql-pack.lock.yml file, specifying the exact version to ensure stability and prevent issues in CI. // Our custom query pack's codeql-pack.lock.yml lockVersion: 1.0.0 dependencies: ... codeql/ruby-all: version: 1.0.6\n\nThis approach allows us to balance developing against the latest features of the ruby-all package while ensuring stability when we release.\n\nWe also have a set of CodeQL unit tests that exercise our queries against sample code snippets, which helps us quickly determine if any query will cause errors before we publish our pack. These tests are run as part of the CI process in our query pack repository, providing an early check for issues. We strongly recommend writing unit tests for your custom CodeQL queries to ensure stability and reliability.\n\nAltogether, the basic flow for releasing new CodeQL queries via our pack is as follows:\n\nOpen a pull request with the new query.\n\nWrite unit tests for the new query.\n\nMerge the pull request.\n\nIncrement the pack version in a new pull request.\n\nRun codeql pack init to resolve dependencies.\n\nto resolve dependencies. Correct unit tests as needed.\n\nPublish the query pack to the GitHub Container Registry (GCR).\n\nRepositories with the query pack in their config will start using the updated queries.\n\nWe have found this flow balances our team\u2019s development experience while ensuring stability in our published query pack.\n\nConfiguring our repository to use our custom query pack\n\nWe won\u2019t provide a general recommendation on configuration here, given that it ultimately depends on how your organization deploys code. We opted against locking our pack to a particular version in our CodeQL configuration file (see above). Instead, we chose to manage our versioning by publishing the CodeQL package in GCR. This results in the GitHub monolith retrieving the latest published version of the query pack. To roll back changes, we simply have to republish the package. In one instance, we released a query that had a high number of false positives and we were able to publish a new version of the pack that removed that query in less than 15 minutes. This is faster than the time it would have taken us to merge a pull request on the monolith repository to roll back the version in the CodeQL configuration file.\n\nOne of the problems we encountered with publishing the query pack in GCR was how to easily make the package available to multiple repositories within our enterprise. There are several approaches we explored.\n\nGrant access permissions for individual repositories. On the package management page, you can grant permissions for individual repositories to access your package. This was not a good solution for us since we have too many repositories for it to be feasible to do manually, yet there is not currently a way to configure programmatically using an API.\n\nOn the package management page, you can grant permissions for individual repositories to access your package. This was not a good solution for us since we have too many repositories for it to be feasible to do manually, yet there is not currently a way to configure programmatically using an API. Mint a personal access token for the CodeQL action runner. We could have minted a personal access token (PAT) that has access to read all packages for our organization and added that to the CodeQL action runner. However, this would have required managing a new token, and it seemed a bit more permissive than we wanted because it could read all of our private packages rather than ones we explicitly allow it to have access to.\n\nWe could have minted a personal access token (PAT) that has access to read all packages for our organization and added that to the CodeQL action runner. However, this would have required managing a new token, and it seemed a bit more permissive than we wanted because it could read all of our private packages rather than ones we explicitly allow it to have access to. Provide access permissions via a linked repository. We ended up implementing the third solution that we explored. We link a repository to the package and allow the package to inherit access permissions from the linked repository.\n\nCodeQL query pack queries\n\nWe write a variety of custom queries to be used in our custom query packs. These cover GitHub-specific patterns that aren\u2019t included in the default CodeQL query pack. This allows us to tailor the analysis to patterns and preferences that are specific to our company and codebase. Some of the types of things we alert on using our custom query pack include:\n\nHigh-risk APIs specific to GitHub\u2019s code that can be dangerous if they receive unsanitized user input.\n\nUse of specific built-in Rails methods for which we have safer, custom methods or functions.\n\nRequired authorization methods not being used in our REST API endpoint definitions and GraphQL object/mutation definitions.\n\nREST API endpoints and GraphQL mutations that require engineers to define access control methods to determine which actors can access them. (Specifically, the query detects the absence of this method definition to ensure that the actors\u2019 permissions are being checked for these endpoints.)\n\nUse of signed tokens so we can nudge engineers to include Product Security as a reviewer when using them.\n\nCustom queries can be used more for educational purposes rather than being blockers to shipping code. For example, we want to alert engineers when they use the ActiveRecord::decrypt method. This method should generally not be used in production code, as it will cause an encrypted column to become decrypted. We use the recommendation severity in the query metadata so these alerts are treated as more of an informational alert. That means this may trigger an alert in a pull request, but it won\u2019t cause the CodeQL CI job to fail. We use this lower severity level to allow engineers to assess the impact of new queries without immediate blocking. Additionally, this alert level isn\u2019t tracked through our Fundamentals program, meaning it doesn\u2019t require immediate action, reflecting the query\u2019s maturity as we continue to refine its relevance and risk assessment.\n\n/** * @id rb/github/use-of-activerecord-decrypt * @description Do not use the .decrypt method on AR models, this will decrypt all encrypted attributes and save * them unencrypted, effectively undoing encryption and possibly making the attributes inaccessible. * If you need to access the unencrypted value of any attribute, you can do so by calling my_model.attribute_name. * @kind problem * @severity recommendation * @name Use of ActiveRecord decrypt method * @tags security * github-internal */ import ruby import DataFlow import codeql.ruby.DataFlow import codeql.ruby.frameworks.ActiveRecord /** Match against .decrypt method calls where the receiver may be an ActiveRecord object */ class ActiveRecordDecryptMethodCall extends ActiveRecordInstanceMethodCall { ActiveRecordDecryptMethodCall() { this.getMethodName() = \"decrypt\" } } from ActiveRecordDecryptMethodCall call select call, \"Do not use the .decrypt method on AR models, this will decrypt all encrypted attributes and save them unencrypted.\n\nAnother educational query is the one mentioned above in which we detect the absence of the `control_access` method in a class that defines a REST API endpoint. If a pull request introduces a new endpoint without `control_access`, a comment will appear on the pull request saying that the `control_access` method wasn\u2019t found and it\u2019s a requirement for REST API endpoints. This will notify the reviewer of a potential issue and prompt the developer to fix it.\n\n/** * @id rb/github/api-control-access * @name Rest API Without 'control_access' * @description All REST API endpoints must call the 'control_access' method, to ensure that only specified actor types are able to access the given endpoint. * @kind problem * @tags security * github-internal * @precision high * @problem.severity recommendation */ import codeql.ruby.AST import codeql.ruby.DataFlow import codeql.ruby.TaintTracking import codeql.ruby.ApiGraphs // Api::App REST API endpoints should generally call the control_access method private DataFlow::ModuleNode appModule() { result = API::getTopLevelMember(\"Api\").getMember(\"App\").getADescendentModule() and not result = protectedApiModule() and not result = staffAppApiModule() } // Api::Admin, Api::Staff, Api::Internal, and Api::ThirdParty REST API endpoints do not need to call the control_access method private DataFlow::ModuleNode protectedApiModule() { result = API::getTopLevelMember([\"Api\"]) .getMember([\"Admin\", \"Staff\", \"Internal\", \"ThirdParty\"]) .getADescendentModule() } // Api::Staff::App REST API endpoints do not need to call the control_access method private DataFlow::ModuleNode staffAppApiModule() { result = API::getTopLevelMember([\"Api\"]).getMember(\"Staff\").getMember(\"App\").getADescendentModule() } private class ApiRouteWithoutControlAccess extends DataFlow::CallNode { ApiRouteWithoutControlAccess() { this = appModule().getAModuleLevelCall([\"get\", \"post\", \"delete\", \"patch\", \"put\"]) and not performsAccessControl(this.getBlock()) } } predicate performsAccessControl(DataFlow::BlockNode blocknode) { accessControlCalled(blocknode.asExpr().getExpr()) } predicate accessControlCalled(Block block) { // the method `control_access` is called somewhere inside `block` block.getAStmt().getAChild*().(MethodCall).getMethodName() = \"control_access\" } from ApiRouteWithoutControlAccess api select api.getLocation(), \"The control_access method was not detected in this REST API endpoint. All REST API endpoints must call this method to ensure that the endpoint is only accessible to the specified actor types.\"\n\nVariant analysis\n\nVariant analysis (VA) refers to the process of searching for variants of security vulnerabilities. This is particularly useful when we\u2019re responding to a bug bounty submission or a security incident. We use a combination of tools to do this, including GitHub\u2019s code search functionality, custom scripts, and CodeQL. We will often start by using code search to find patterns similar to the one that caused a particular vulnerability across numerous repositories. This is sometimes not good enough, as code search is not semantically aware, meaning that it cannot determine whether a given variable is an Active Record object or whether it is being used in an `if` expression. To answer those types of questions we turn to CodeQL.\n\nWhen we write CodeQL queries for variant analysis we are much less concerned about false positives, since the goal is to provide results for security engineers to analyze. The quality of the code is also not quite as important, as these queries will only be used for the duration of the VA effort. Some of the types of things we use CodeQL for during VAs are:\n\nWhere are we using SHA1 hashes?\n\nOne of our internal API endpoints was vulnerable to SQLi according to a recent bug bounty report. Where are we passing user input to that API endpoint?\n\nThere is a problem with how some HTTP request libraries in Ruby handle the proxy setting. Can we look at places we are instantiating our HTTP request libraries with a proxy setting?\n\nOne recent example involved a subtle vulnerability in Rails. We wanted to detect when the following condition was present in our code:\n\nA parameter was used to look up an Active Record object.\n\nThat parameter is later reused after the Active Record object is looked up.\n\nThe concern with this condition is that it could lead to an insecure direct object reference (IDOR) vulnerability because Active Record finder methods can accept an array. If the code looks up an Active Record object in one call to determine if a given entity has access to a resource, but later uses a different element from that array to find an object reference, that can lead to an IDOR vulnerability. It would be difficult to write a query to detect all vulnerable instances of this pattern, but we were able to write a query that found potential vulnerabilities that gave us a list of code paths to manually analyze. We ran the query against a large number of our Ruby codebases using CodeQL\u2019s MRVA.\n\nThe query, which is a bit hacky and not quite production grade, is below:\n\n/** * @name wip array query * @description an array is passed to an AR finder object */ import ruby import codeql.ruby.AST import codeql.ruby.ApiGraphs import codeql.ruby.frameworks.Rails import codeql.ruby.frameworks.ActiveRecord import codeql.ruby.frameworks.ActionController import codeql.ruby.DataFlow import codeql.ruby.Frameworks import codeql.ruby.TaintTracking // Gets the \"final\" receiver in a chain of method calls. // For example, in `Foo.bar`, this would give the `Foo` access, and in // `foo.bar.baz(\"arg\")` it would give the `foo` variable access private Expr getUltimateReceiver(MethodCall call) { exists(Expr recv | recv = call.getReceiver() and ( result = getUltimateReceiver(recv) or not recv instanceof MethodCall and result = recv ) ) } // Names of class methods on ActiveRecord models that may return one or more // instances of that model. This also includes the `initialize` method. // See https://api.rubyonrails.org/classes/ActiveRecord/FinderMethods.html private string staticFinderMethodName() { exists(string baseName | baseName = [\"find_by\", \"find_or_create_by\", \"find_or_initialize_by\", \"where\"] and result = baseName + [\"\", \"!\"] ) // or // result = [\"new\", \"create\"] } private class ActiveRecordModelFinderCall extends ActiveRecordModelInstantiation, DataFlow::CallNode { private ActiveRecordModelClass cls; ActiveRecordModelFinderCall() { exists(MethodCall call, Expr recv | call = this.asExpr().getExpr() and recv = getUltimateReceiver(call) and ( // The receiver refers to an `ActiveRecordModelClass` by name recv.(ConstantReadAccess).getAQualifiedName() = cls.getAQualifiedName() or // The receiver is self, and the call is within a singleton method of // the `ActiveRecordModelClass` recv instanceof SelfVariableAccess and exists(SingletonMethod callScope | callScope = call.getCfgScope() and callScope = cls.getAMethod() ) ) and ( call.getMethodName() = staticFinderMethodName() or // dynamically generated finder methods call.getMethodName().indexOf(\"find_by_\") = 0 ) ) } final override ActiveRecordModelClass getClass() { result = cls } } class FinderCallArgument extends DataFlow::Node { private ActiveRecordModelFinderCall finderCallNode; FinderCallArgument() { this = finderCallNode.getArgument(_) } } class ParamsHashReference extends DataFlow::CallNode { private Rails::ParamsCall params; // TODO: only direct element references against `params` calls are considered ParamsHashReference() { this.getReceiver().asExpr().getExpr() = params } string getArgString() { result = this.getArgument(0).asExpr().getConstantValue().getStringlikeValue() } } class ArrayPassedToActiveRecordFinder extends TaintTracking::Configuration { ArrayPassedToActiveRecordFinder() { this = \"ArrayPassedToActiveRecordFinder\" } override predicate isSource(DataFlow::Node source) { source instanceof ParamsHashReference } override predicate isSink(DataFlow::Node sink) { sink instanceof FinderCallArgument } string getParamsArg(DataFlow::CallNode paramsCall) { result = paramsCall.getArgument(0).asExpr().getConstantValue().getStringlikeValue() } // this doesn't check for anything fancy like whether it's reuse in a if/else // only intended for quick manual audit filtering of interesting candidates // so remains fairly broad to not induce false negatives predicate paramsUsedAfterLookups(DataFlow::Node source) { exists(DataFlow::CallNode y | y instanceof ParamsHashReference and source.getEnclosingMethod() = y.getEnclosingMethod() and source != y and getParamsArg(source) = getParamsArg(y) // we only care if it's used again AFTER an object lookup and y.getLocation().getStartLine() > source.getLocation().getStartLine()) } } from ArrayPassedToActiveRecordFinder config, DataFlow::Node source, DataFlow::Node sink where config.hasFlow(source, sink) and config.paramsUsedAfterLookups(source) select source, sink.getLocation()\n\nConclusion\n\nCodeQL can be very useful for product security engineering teams to detect and prevent vulnerabilities at scale. We use a combination of queries that run in CI using our query pack and one-off queries run through MRVA to find potential vulnerabilities and communicate them to engineers. CodeQL isn\u2019t only useful for finding security vulnerabilities, though; it is also useful for detecting the presence or absence of security controls that are defined in code. This saves our security team time by surfacing certain security problems automatically, and saves our engineers time by detecting them earlier in the development process.\n\nWriting custom CodeQL queries\n\nTips for getting started\n\nWe have a large number of articles and resources for writing custom CodeQL queries. If you haven\u2019t written custom CodeQL queries before, here are some resources to help get you started:\n\nImprove the security of your applications today by enabling CodeQL for free on your public repositories, or try GitHub Advanced Security for your organization.\n\nMichael Recachinas, GitHub Staff Security Engineer, also contributed to this blog post.", "label": 0}
{"title": "Leveraging BigQuery JSON for Optimized MongoDB Dataflow Pipelines", "url": "https://developers.googleblog.com/en/leveraging-bigquery-json-for-optimized-mongodb-dataflow-pipelines/", "content": "This streamlined approach saves time and resources, empowering users to unlock the full potential of their data through advanced data analytics and machine learning.\n\nWe're delighted to introduce a major enhancement to our Google Cloud Dataflow templates for MongoDB Atlas. By enabling direct support for JSON data types, users can now seamlessly integrate their MongoDB Atlas data into BigQuery, eliminating the need for complex data transformations.\n\nLimitations without JSON support\n\nTraditionally, Dataflow pipelines designed to handle MongoDB Atlas data often necessitate the transformation of data into JSON strings or flattening complex structures to a single level of nesting before loading into BigQuery. Although this approach is viable, it can result in several drawbacks:\n\nIncreased latency: The multiple data conversions required can lead to increased latency and can significantly slow down the overall pipeline execution time.\n\nHigher operational costs: The extra data transformations and storage requirements associated with this approach can lead to increased operational costs.\n\nReduced query performance: Flattening complex document structures in JSON String format can impact query performance and make it difficult to analyze nested data.\n\n\n\nSo, what\u2019s new?\n\nBigQuery's Native JSON format addresses these challenges by enabling users to directly load nested JSON data from MongoDB Atlas into BigQuery without any intermediate conversions.\n\nThis approach offers numerous benefits:\n\nReduced operating costs: By eliminating the need for additional data transformations, users can significantly reduce operational expenses, including those associated with infrastructure, storage, and compute resources.\n\nEnhanced query performance: BigQuery's optimized storage and query engine is designed to efficiently process data in Native JSON format, resulting in significantly faster query execution times and improved overall query performance.\n\nImproved data flexibility: users can easily query and analyze complex data structures, including nested and hierarchical data, without the need for time-consuming and error-prone flattening or normalization processes.\n\nA significant advantage of this pipeline lies in its ability to directly leverage BigQuery's powerful JSON functions on the MongoDB data loaded into BigQuery. This eliminates the need for a complex and time-consuming data transformation process. The JSON data within BigQuery can be queried and analyzed using standard BQML queries.\n\nWhether you prefer a streamlined cloud-based approach or a hands-on, customizable solution, the Dataflow pipeline can be deployed either through the Google Cloud console or by running the code from github repository.\n\n\n\nEnabling data-driven decision-making\n\nTo summarize, Google\u2019s Dataflow template provides a flexible solution for transferring data from MongoDB to BigQuery. It can process entire collections or capture incremental changes using MongoDB's Change Stream functionality. The pipeline's output format can be customized to suit your specific needs. Whether you prefer a raw JSON representation or a flattened schema with individual fields, you can easily configure it through the userOption parameter. Additionally, data transformation can be performed during template execution using User-Defined Functions (UDFs).\n\nBy adopting BigQuery Native JSON format in your Dataflow pipelines, you can significantly enhance the efficiency, performance, and cost-effectiveness of your data processing workflows. This powerful combination empowers you to extract valuable insights from your data and make data-driven decisions.\n\nFollow the Google Documentation to learn how to set up the Dataflow templates for MongoDB Atlas and BigQuery.", "label": 0}
{"title": "Title Launch Observability at Netflix Scale", "url": "https://netflixtechblog.com/title-launch-observability-at-netflix-scale-c88c586629eb?source=collection_home---4------10-----------------------", "content": "Title Launch Observability at Netflix Scale\n\nPart 1: Understanding The Challenges Netflix Technology Blog 5 min read \u00b7 Dec 17, 2024 -- 7 Listen Share\n\nBy: Varun Khaitan\n\nWith special thanks to my stunning colleagues: Mallika Rao, Esmir Mesic, Hugo Marques\n\nIntroduction\n\nAt Netflix, we manage over a thousand global content launches each month, backed by billions of dollars in annual investment. Ensuring the success and discoverability of each title across our platform is a top priority, as we aim to connect every story with the right audience to delight our members. To achieve this, we are committed to building robust systems that deliver comprehensive observability, enabling us to take full accountability for every title on our service.\n\nThe Challenge of Title Launch Observability\n\nAs engineers, we\u2019re wired to track system metrics like error rates, latencies, and CPU utilization \u2014 but what about metrics that matter to a title\u2019s success?\n\nConsider the following example of two different Netflix Homepages:\n\nSample Homepage A\n\nSample Homepage B\n\nTo a basic recommendation system, the two sample pages might appear equivalent as long as the viewer watches the top title. Yet, these pages couldn\u2019t be more different. Each title represents countless hours of effort and creativity, and our systems need to honor that uniqueness.\n\nHow do we bridge this gap? How can we design systems that recognize these nuances and empower every title to shine and bring joy to our members?\n\nThe Operational Needs of a Personalization System\n\nIn the early days of Netflix Originals, our launch team would huddle together at midnight, manually verifying that titles appeared in all the right places. While this hands-on approach worked for a handful of titles, it quickly became clear that it couldn\u2019t scale. As Netflix expanded globally and the volume of title launches skyrocketed, the operational challenges of maintaining this manual process became undeniable.\n\nOperating a personalization system for a global streaming service involves addressing numerous inquiries about why certain titles appear or fail to appear at specific times and places.\n\nSome examples:\n\nWhy is title X not showing on the Coming Soon row for a particular member?\n\nWhy is title Y missing from the search page in Brazil?\n\nIs title Z being displayed correctly in all product experiences as intended?\n\nAs Netflix scaled, we faced the mounting challenge of providing accurate, timely answers to increasingly complex queries about title performance and discoverability. This led to a suite of fragmented scripts, runbooks, and ad hoc solutions scattered across teams \u2014 an approach that was neither sustainable nor efficient.\n\nThe stakes are even higher when ensuring every title launches flawlessly. Metadata and assets must be correctly configured, data must flow seamlessly, microservices must process titles without error, and algorithms must function as intended. The complexity of these operational demands underscored the urgent need for a scalable solution.\n\nAutomating the Operations\n\nIt becomes evident over time that we need to automate our operations to scale with the business. As we thought more about this problem and possible solutions, two clear options emerged.\n\nOption 1: Log Processing\n\nLog processing offers a straightforward solution for monitoring and analyzing title launches. By logging all titles as they are displayed, we can process these logs to identify anomalies and gain insights into system performance. This approach provides a few advantages:\n\nLow burden on existing systems: Log processing imposes minimal changes to existing infrastructure. By leveraging logs, which are already generated during regular operations, we can scale observability without significant system modifications. This allows us to focus on data analysis and problem-solving rather than managing complex system changes. Using the source of truth: Logs serve as a reliable \u201csource of truth\u201d by providing a comprehensive record of system events. They allow us to verify whether titles are presented as intended and investigate any discrepancies. This capability is crucial for ensuring our recommendation systems and user interfaces function correctly, supporting successful title launches.\n\nHowever, taking this approach also presents several challenges:\n\nCatching Issues Ahead of Time: Logging primarily addresses post-launch scenarios, as logs are generated only after titles are shown to members. To detect issues proactively, we need to simulate traffic and predict system behavior in advance. Once artificial traffic is generated, discarding the response object and relying solely on logs becomes inefficient. Appropriate Accuracy: Comprehensive logging requires services to log both included and excluded titles, along with reasons for exclusion. This could lead to an exponential increase in logged data. Utilizing probabilistic logging methods could compromise accuracy, making it difficult to ascertain whether a title\u2019s absence in logs is due to exclusion or random chance. SLA and Cost Considerations: Our existing online logging systems do not natively support logging at the title granularity level. While reengineering these systems to accommodate this additional axis is possible, it would entail increased costs. Additionally, the time-sensitive nature of these investigations precludes the use of cold storage, which cannot meet the stringent SLAs required.\n\nOption 2: Observability Endpoints in Our Personalization Systems\n\nTo prioritize title launch observability, we could adopt a centralized approach. By introducing observability endpoints across all systems, we can enable real-time data flow into a dedicated microservice for title launch observability. This approach embeds observability directly into the very fabric of services managing title launches and personalization, ensuring seamless monitoring and insights. Key benefits and strategies include:\n\nReal-Time Monitoring: Observability endpoints enable real-time monitoring of system performance and title placements, allowing us to detect and address issues as they arise. Proactive Issue Detection: By simulating future traffic(an aspect we call \u201ctime travel\u201d) and capturing system responses ahead of time, we can preemptively identify potential issues before they impact our members or the business. Enhanced Accuracy: Observability endpoints provide precise data on title inclusions and exclusions, allowing us to make accurate assertions about system behavior and title visibility. It also provides us with advanced debugability information needed to fix identified issues. Scalability and Cost Efficiency: While initial implementation required some investment, this approach ultimately offers a scalable and cost-effective solution to managing title launches at Netflix scale.\n\nChoosing this option also comes with some tradeoffs:\n\nSignificant Initial Investment: Several systems would need to create new endpoints and refactor their codebases to adopt this new method of prioritizing launches. Synchronization Risk: There would be a potential risk that these new endpoints may not accurately represent production behavior, thus necessitating conscious efforts to ensure all endpoints remain synchronized.\n\nUp Next\n\nBy adopting a comprehensive observability strategy that includes real-time monitoring, proactive issue detection, and source of truth reconciliation, we\u2019ve significantly enhanced our ability to ensure the successful launch and discovery of titles across Netflix, enriching the global viewing experience for our members. In the next part of this series, we\u2019ll dive into how we achieved this, sharing key technical insights and details.\n\nStay tuned for a closer look at the innovation behind the scenes in Part 2!", "label": 0}
{"title": "Heroku Postgres Upgrade Guide: Simplify Your Move to Version 17", "url": "https://www.heroku.com/blog/heroku-postgres-upgrade-guide-simplify-move-version-17/", "content": "If you\u2019ve ever deployed an app on Heroku, chances are you\u2019ve used Heroku Postgres \u2014 our fully managed, reliable, and scalable Postgres database service. It\u2019s the backbone for millions of applications, from weekend side projects to enterprise-grade systems running in production.\n\nBut Postgres, like all software, continues to evolve. With new versions released each year, you gain access to performance enhancements, critical security updates, and powerful new features. Keeping your database up to date isn\u2019t just good practice \u2014 it\u2019s essential for long-term stability and success.\n\nThat\u2019s why we\u2019re thrilled to share that Postgres 17 is now available on Heroku. And with our newly simplified upgrade process, keeping your database current has never been easier. There\u2019s no better time to plan your next upgrade and take full advantage of everything Postgres 17 has to offer.\n\nWhat is Heroku Postgres?\n\nHeroku Postgres is a managed Postgres service built into the Heroku platform. It handles provisioning, maintenance, backups, high availability, and monitoring so that customers can focus on building engaging data-driven applications, instead of managing infrastructure.\n\nWhy upgrading your Postgres version matters\n\nThere are several important reasons for why upgrading your Postgres database is necessary,\n\nSecurity : Postgres regularly releases security updates to patch vulnerabilities. Running an outdated version could expose your database to known security risks. Once a version is unsupported, the Postgres Community won\u2019t have any more security releases for that version.\n\n: Postgres regularly releases security updates to patch vulnerabilities. Running an outdated version could expose your database to known security risks. Once a version is unsupported, the Postgres Community won\u2019t have any more security releases for that version. Bug Fixes : Each new version includes fixes for bugs and issues found in previous versions.\n\n: Each new version includes fixes for bugs and issues found in previous versions. Performance Improvements : Newer versions often include performance optimizations, better query planning, and improved resource utilization.\n\n: Newer versions often include performance optimizations, better query planning, and improved resource utilization. New Features : Postgres releases bring new features and capabilities that can enhance your database\u2019s functionality. For example: Better parallel query execution Improved indexing options Enhanced monitoring capabilities New data types and functions\n\n: Postgres releases bring new features and capabilities that can enhance your database\u2019s functionality. For example: Compatibility: Staying current helps maintain compatibility with other tools, libraries, and applications that interact with your database.\n\nThe backstory: Our Postgres version support & deprecation policy\n\nAt Heroku, we follow a well-defined lifecycle for Postgres versions:\n\nEach major version is supported for a set period \u2014 currently three years.\n\nWhen a version approaches end-of-support, we begin our deprecation process \u2014 announcing an upcoming removal from the platform and stopping new provisioning.\n\nWhen deprecation is announced, we give customers advance notice and time to upgrade manually.\n\nIf no action is taken, we then automatically upgrade databases still on unsupported versions, ensuring security and platform stability.\n\nWhat we learned\n\nOur internal upgrade automation has quietly and successfully upgraded tens of thousands of databases each year leading many customers to ask:\n\n\u201cCan we use the same system to manage our own upgrades?\u201d\n\nThat demand inspired the improved pg:upgrade CLI experience \u2014 a safer, more transparent, and self-service version of our proven internal tools. Now, all Heroku Postgres users can benefit from the same automation and built-in checks that power our large-scale upgrade process.\n\nVisit our devcenter for more details on how Heroku manages Postgres version support and deprecation timelines.\n\nIntroducing new pg:upgrade commands\n\nWe\u2019re rolling out five new heroku pg:upgrade:* commands that give you more control, visibility, and confidence during Postgres version upgrades:\n\npg:upgrade:prepare \u2013 Schedule a Postgres upgrade for Standard-tier and higher leader databases during your next maintenance window.\n\npg:upgrade:run \u2013 Trigger an upgrade manually. Perfect to start an upgrade immediately on Essential-tier databases and follower databases, or run a prepared upgrade before the next scheduled maintenance window on a Standard-tier or higher database.\n\npg:upgrade:cancel \u2013 Cancel a scheduled upgrade (before it starts running).\n\npg:upgrade:dryrun \u2013 Simulate an upgrade on a Standard-tier or higher database using a follower to preview the upgrade experience and detect any potential issues \u2014 no impact on your production database.\n\npg:upgrade:wait \u2013 Track the progress of your upgrade in real time.\n\nYou\u2019ll receive email notifications at every key stage:\n\nWhen the upgrade is scheduled, running, cancelled and completed (successfully or not).\n\nAfter a dry run completes, with a summary of the results and any potential issues detected.\n\nUpgrading is now a simple 1-step process\n\nYou might notice there are more commands available now, but upgrading your database has actually become much simpler \u2014 it\u2019s now just a 1-step process!\n\nHeroku handles what used to be multiple manual steps \u2014 provisioning a follower, entering maintenance mode, promoting, reattaching, exiting maintenance mode \u2014 all with a single workflow.\n\nSee the section below for the most efficient path based on your database tier.\n\nStep-by-step: How to upgrade Heroku Postgres\n\nEssential-tier database upgrades\n\nTo upgrade, just run:\n\nheroku pg:upgrade:run HEROKU_POSTGRESQL_RED --app example app\n\nThat\u2019s it \u2014 no preparation step required.\n\nNote: If you don\u2019t specify a version with --version , the upgrade will use the latest supported Postgres version on Heroku.\n\nStandard-tier & higher database upgrades\n\nWe recommend this process for Standard-tier and higher, regardless of whether or not you have follower databases.\n\nStep 0 \u2013 Optional (but recommended)\n\nRun a test upgrade to detect any potential issues before upgrading your production database.\n\nheroku pg:upgrade:dryrun HEROKU_POSTGRESQL_RED --app example-app\n\nThen proceed with the actual upgrade in one simple step:\n\nStep 1 \u2013 Prepare the upgrade\n\nheroku pg:upgrade:prepare HEROKU_POSTGRESQL_RED --app example-app --version 17\n\nThis schedules the upgrade for your next maintenance window.\n\nNote: If --version is not specified, we\u2019ll automatically use the latest supported Postgres version on Heroku.\n\nUse the following to track when the upgrade is scheduled and ready to run:\n\nheroku pg:upgrade:wait HEROKU_POSTGRESQL_RED --app example-app\n\nStep 2 (Optional) \u2013 Manually run the upgrade\n\nheroku pg:upgrade:run HEROKU_POSTGRESQL_RED --app example-app\n\nThis will upgrade your leader database and its follower(s) automatically.\n\nTrack the progress until completion with:\n\nheroku pg:upgrade:wait HEROKU_POSTGRESQL_RED --app example-app\n\nTip: If you don\u2019t manually run this command, the upgrade will be run automatically during the scheduled maintenance window. You can view your app\u2019s maintenance window and scheduled maintenances by running:\n\nheroku pg:info HEROKU_POSTGRESQL_RED --app example-app\n\nFor more information on maintenance windows, check out the Heroku Postgres Maintenance documentation.\n\nBenefits of the new upgrade mechanism\n\nThe new upgrade operates in-place using an internal replica, simplifying the process, removing the need to manage a separate follower add-on, and minimizing the risk of data loss or inconsistencies by managing database access internally throughout the upgrade.\n\nusing an internal replica, simplifying the process, removing the need to manage a separate follower add-on, and minimizing the risk of data loss or inconsistencies by managing database access internally throughout the upgrade. Using this automation will reduce downtime to about 5-10 minutes for a typical upgrade.\n\nfor a typical upgrade. When you upgrade your leader database, any followers are automatically upgraded \u2013 no need to recreate or manually reattach followers.\n\n\u2013 no need to recreate or manually reattach followers. Your app\u2019s DATABASE_URL and other config_vars remain unchanged after the upgrade, ensuring your application continues to operate without any reconfiguration.\n\nand other after the upgrade, ensuring your application continues to operate without any reconfiguration. The same simple steps apply to upgrading databases with Streaming Data Connectors , replacing what used to require at least 8 manual steps as outlined here.\n\n, replacing what used to require at least as outlined here. If we detect a known issue with your data/schema during the upgrade, you\u2019ll receive an email with remediation steps to help you complete the upgrade.\n\nwith your data/schema during the upgrade, you\u2019ll receive an email with remediation steps to help you complete the upgrade. If the issue is unexpected or cannot be resolved automatically , we\u2019ll prompt you to open a support ticket so our team can help troubleshoot.\n\n, we\u2019ll prompt you to so our team can help troubleshoot. In all cases, your database remains available , and user access is restored once the upgrade process completes \u2014 whether it finishes successfully or is automatically aborted for safety.\n\n, and once the upgrade process completes \u2014 whether it finishes successfully or is automatically aborted for safety. Want added peace of mind? Run a test upgrade in advance using heroku pg:upgrade:dryrun . This simulates the upgrade on a copy of your database and highlights potential issues before touching production.\n\nThe \u201cold\u201d follower upgrade approach\n\nWhile we now recommend upgrading the leader database directly using the approach explained\n\nabove, customers who prefer the traditional flow can still use the follower upgrade approach.\n\nTo do this, you can continue to follow the steps as described here.\n\nIn order to run the upgrade, use:\n\nheroku pg:upgrade:run HEROKU_POSTGRESQL_RED --app example-app\n\nThis approach has one notable benefit, your original leader database remains untouched during the upgrade, which allows for easier rollback, testing, or verification before promoting the upgraded follower.\n\nDeprecation notice\n\nThe legacy heroku pg:upgrade command will be deprecated soon. To ensure a smoother, safer upgrade experience, we strongly recommend switching to the new heroku pg:upgrade:* subcommands.\n\nIf you continue to use the old command, you\u2019ll receive tailored warnings and redirection to help guide you toward the updated flow. Make the switch today to take full advantage of the simplified, automated upgrade process.\n\nUpgrading shouldn\u2019t be a chore \u2013 it should be a habit\n\nUpgrading your Postgres database shouldn\u2019t be a last-minute scramble \u2014 it should be a routine habit. Regular upgrades help keep your applications secure and performant, while also giving you access to the latest features and improvements that drive innovation. By making upgrades a part of your development rhythm, you set your systems up for long-term stability, scalability, and success.\n\nAt Heroku, we\u2019re focused on making the overall Postgres experience safer and more intuitive for developers. A key part of that is improving the upgrade process: with streamlined tooling, automation, and built-in safeguards, upgrading your Postgres version is now significantly faster and more reliable. All of this is designed to help you stay focused on what matters most \u2013 building and shipping great apps \u2013 while staying confident that your data layer is future-ready.", "label": 0}
{"title": "Saying Goodbye", "url": "https://lifeofpablo.com/blog/saying-goodbye", "content": "Saying Goodbye\n\nThis post was written in English (en_US).\n\nSo this week the last foreign exchange student left Hastings to go back to his home country. It was sad seeing him leave.\n\nIt is fun meeting new people. Not everyday does a person get to meet a lot of foreign people especially here in Small Town, Nebraska. This especially students around our own age. Hastings High had a total of 6 foreign exchange students from many different nationalities most of which came from Europe and one from South America. Coming here was a new complete experience for all of them.\n\nIt was fun meeting all the foreign exchange students this year. You guys really helped me become a more open minded person to become more indulged with diversity. I learned a lot about myself. Thanks for all the great experiences. This will be one of the fondest memories of high school that I will be able to look back on.\n\nIt was fun meeting all the foreign exchange students this year. I want to to apologize to the ones that I did not say goodbye :( .I wish I could of been there.\n\nTo all of you guys of this year and years past. THANK YOU. I will see you guys in the future. Best of luck!", "label": 1}
{"title": "How Edison is helping us build a faster, more powerful Dropbox on the web", "url": "https://dropbox.tech/frontend/edison-webserver-a-faster-more-powerful-dropbox-on-the-web", "content": "How Dropbox re-wrote its core web serving stack for the next decade\u2014sunsetting technical debt accrued over 13 years, and migrating high-traffic surfaces to a future-proofed platform ready for the company\u2019s multi-product evolution. ~ ~ ~ In the Fall of 2021, Dropbox quietly launched an internal alpha version of our core web-based file browser. This internal alpha marked a critical milestone in Dropbox web architecture: our file browser was now a Single Page Application, or SPA. On its own, this wouldn\u2019t have been very remarkable. SPAs have been around in concept for almost two decades, and have been widely used on the web for at least half that time. In fact, to most people managing a website, SPA functionality is table stakes and barely worth writing a blog post about. That would have been true for us too if the web property wasn\u2019t Dropbox.com\u2014a very large surface spanning hundreds of back end services with deep and multi-layered feature sets owned by dozens of teams. We weren\u2019t just launching a SPA, but a whole new platform to enable it. This platform would not only have to work correctly and outperform the existing stack, but\u2014critically\u2014do so without requiring a tough rewrite for teams who wanted to migrate. Anyone who has worked with a system as large as ours\u2014with all its attendant tech debt\u2014knows that rewriting even limited segments can be a challenging endeavor. Our debut of the SPA architecture represented a complete rebuild of web foundations at Dropbox\u2014an entirely new web framework, stack, and platform called Edison. Developed over the last three years, Edison gives us the keys to unlock a host of performance and functional improvements. It enables sub-second developer iteration cycles and isomorphic rendering\u2014meaning JavaScript that runs on both the client and server\u2014and unlocks the unification of our web surface into a dynamic SPA. It is a full rewrite of our core web systems that sunsets some 13 years of technical debt. We built Edison to be our web serving stack for the next decade\u2014a flexible platform fit for our highest-traffic surfaces and future products and features alike.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images\u2014plus generative AI capabilities across even more connected apps. See what's new \u2192\n\nWhat is the web to Dropbox?\n\n\u201cThings can change so fast on the internet\u201d \u2014 Tim Berners-Lee Dropbox launched its website in 2008, almost two decades after the web was invented. At launch, Dropbox was still just a desktop app, and the website a delivery mechanism and marketing surface. For much of the early years, the real action was in the app which ran on your computer and quietly kept your digital life in sync. Our engineering talent specialized in infrastructure\u2014syncing, networking, chunking, and storage\u2014rather than the web. Over time, Dropbox built a web platform piece by piece, moving from a brochure site to an account manager and file viewer\u2014and finally, to today\u2019s platform, where we can provide advanced features like PDF editing and video collaboration. Many more users are able to benefit from these advanced features thanks to the reach and ubiquity of the web. Today, the web is central to all Dropbox products\u2014from enabling new file-based workflows to organizing cloud-only data\u2014and increasingly where the battle for user utility will be fought. It\u2019s critical to our future that we have a web platform which gives our product engineers the most reach and speed that it possibly can. As we move to meet the challenges of the future, Edison is just one example of how seriously Dropbox now takes the web, and the shifting internal and external culture around it.\n\nThe history of Dropbox on the web\n\nLike any surface with a storied history, our web code has undergone a number of significant iterations over the years. Back in 2012, our web architecture was based on Pyxl, a Python-with-inline-HTML engine\u2014familiar ground to folks who grew up writing PHP! We built much of our client front end in CoffeeScript, until that turned into a dead-end and pushed us to migrate to TypeScript. We used and then deprecated jQuery. We adopted React. In short: much of our trajectory echoes the evolution of the web ecosystem itself over the last decade-and-a-half, and should be familiar to many who built web properties during that time. More interesting is how the integration between our back end and front end evolved. In broad strokes, we moved in step with the wider web ecosystem\u2014adopting open source technologies, server rendering, and interactive front ends. But over time we also built out custom infrastructure to solve our own set of business challenges. A critical piece of this was our custom webserver, and the optimizations built into it. This (legacy) webserver is called DWS. (If you\u2019re wondering what that acronym stands for, please take a guess; congratulations, you\u2019re right!) DWS served us well for most of a decade, and allowed us to successfully iterate on our website during that time. But, inevitably, two things happened next: the web continued to evolve, and we wanted to demand more of DWS than it was designed to do. As we moved to expand the capabilities of our web products, the limitations of DWS presented somewhat intractable issues for both our application feature set and developer productivity. There were two main classes of problems we were up against.\n\n\n\nProblem 1: Pagelets DWS was designed so that feature teams could iterate independently on the web. For example, one team might want to work on navigation while another worked on the sidebar. In particular, those teams would want to iterate and release without continuous cross-team coordination. DWS enabled this. DWS not only allowed engineers to develop independently, but also enabled their code to execute independently on both the back and front end. These properties ensured that one feature team could not hold up or slow down another. To meet these requirements DWS settled on a pagelet architecture. For those unfamiliar with the term, a pagelet is a subsection of a page, such as the navigation bar or the footer. In our case, one pagelet encompasses both the back end (data fetching, controller) as well as the front end (view) code. Each pagelet has a controller in Python which retrieves the back end data it needs, specifies page metadata, and provides a JavaScript (JS) entry point with initialization data. Pagelets execute in parallel and stream the data they retrieve into the browser response, keeping the HTTP connection open for as long as needed until the streaming is complete. Under DWS, a collection of pagelets is defined as a route. In the browser, each pagelet\u2019s entry point module is executed as an individual React app, which initializes as soon as ready and renders into a specific HTML slot in the page\u2019s appshell (the basic HTML which initializes the page and provides root elements to hold incoming content). As each pagelet completes, the page is assembled into its complete form. Upsides to this architecture: Complex, multi-featured pages could be vertically sliced into manageable parts.\n\nThere was clear separation and ownership of product code.\n\nEach team\u2019s product could be rendered independent of other pagelets.\n\nIt made early data retrieval possible. DWS issued parallel back end requests which streamed into the HTML, so that by the time the JS began executing, the data fetches it needed were already complete.1 Downsides to this architecture: Data fetch instructions were defined in the Python controller, but fetched data was consumed in the JS layer in the browser. These layers needed to be kept in sync manually, which was an obvious source of bugs. This also meant that it was hard to statically verify that a piece of data was actually still in use, making it easy to over-fetch data or forget to eliminate a data fetch as the front end changed. Writing functionality that crossed pagelet boundaries was nontrivial. The difficulty of crossing pagelet boundaries was an especially serious limitation. It meant we couldn\u2019t evolve our site into anything which you\u2019d define as a holistic application, let alone a SPA. If you wanted your pagelet\u2019s JS code to interact with another pagelet\u2019s JS code, you\u2019d need to meet with the owning team and agree to design a mutual JS messaging API (yikes!). This closed off a whole world of possibilities and feature sets.\n\nProblem 2: Commingled layers Any engineer who has designed a distributed system knows a few principles to building robust, composable, flexible interfaces. Separation of concerns. Loose coupling. The basic problem we faced as we tried to stretch our wings on the web was predictable: because DWS had evolved from a system of HTML-in-Python, it still retained a lot of hard-to-shift coupling between the server and the JS client. What this meant was that if an engineer developing a feature for the web wanted to run their code\u2014which might only be a few lines of JS and CSS\u2014they also needed to run the whole webserver stack. The system wasn\u2019t designed to be modular, and the JS application layer had many direct and indirect dependencies on server-rendered data. Executing JS without the webserver was rarely feasible, and even if some segments were, it wasn\u2019t possible to build a universal way to do it. Running a webserver is easy if you only have a handful of routes, and maybe a few thousand lines of code. Many projects will use webpack-dev-server and start their site up in, say, 10-20 seconds. However, our scale made this approach prohibitive. At Dropbox, it meant running ~100 back end services, all of which needed to be compiled and started up in the virtualized server environment we call devbox. This could take 20 minutes\u2014and sometimes took twice that! An experienced front end engineer who joined the company might see their workflow change from \u201cI can load the dev site on my laptop in 20 seconds\u201d to \u201cI need a provisioned dedicated remote server and 20-30 minutes.\u201d Again, yikes! We needed to get to a place where engineers could run their JS code quickly and easily\u2014a place that didn\u2019t require expensive builds of the back end and round-trips to a remote box. After all, it\u2019s one thing to be able to run a single module without the webserver; there are systems that allow you to test some subset of your functionality (for example, Storybook). But these were patches on our problem, not full solutions. It\u2019s no good if engineers need to work out how to run their code every time they need to test something. We needed a general solution.\n\n\n\nIntroducing Edison\n\nFinding an an answer to our challenges with web client development at Dropbox wasn\u2019t straightforward. Our first incremental rewrite began in 2016, before I joined Dropbox, and before the initial release of Next.js\u2014the open-source project which Edison is frequently compared to. We had several false starts and backward steps as we iterated on incremental versus foundational changes. In retrospect, the key functional requirements we had to nail with Edison were: The browser runtime is a single, unified JS application.\n\nThe JS client can execute without the server. The key non-functional requirements were:\n\nAn easy migration. Rewriting all our code for a new system was a non-starter. Any particular team rewriting their code was a non-starter. We had nearly two million lines of TypeScript code, and around 100,000 lines of back end code handling pagelets. This was partly a technical problem, but also a political one, since expensive rewrites would have the potential to heavily delay or even render a new system DOA.\n\nRewriting all our code for a new system was a non-starter. Any particular rewriting their code was a non-starter. We had nearly two million lines of TypeScript code, and around 100,000 lines of back end code handling pagelets. This was partly a technical problem, but also a political one, since expensive rewrites would have the potential to heavily delay or even render a new system DOA. Equal or better performance. The existing system had clear optimization lines that we had to match. We had to ensure that parallel products couldn\u2019t impact each other, and that data fetches happened well in advance of the JS initialization. Again, this was both a technical and a political problem, because although limited performance regressions could hypothetically be the right technical outcome in some scenarios, they\u2019d cause political pushback from impacted teams. By 2020, we had a clear sense of the necessary outcome: a serverless client architecture, re-imagined from the ground-up, alongside a server which could do Node-based rendering. In other words, we needed an isomorphic JS application stack tightly integrated with our existing services and code architecture.\n\nStructure Edison consists of three major parts: Edison Server written in Go\n\naccepts direct web traffic via our Envoy proxy layer\n\ndelegates server-side rendering (SSR 2 ) to Streaming Reactserver\n\n) to Streaming Reactserver performs data fetches via Courier, our gRPC framework\n\nhandles ahead-of-time fetching and communicates with Edison Client in the browser Streaming Reactserver Go-wrapped Node service 3 tasked specifically with performing React tree renders\n\ntasked specifically with performing React tree renders able to pass messages back to Edison Server asynchronously during render Edison Client, our browser-based runtime main browser entry point\n\ninterfaces with Edison Server to fulfill ahead-of-time data fetch guarantees\n\nhandles data exchange directly with Courier services for late fetches\n\nimplements a single React app for the whole page Running serverless is a key part of Edison\u2019s design. In principle, you could build Edison Client and our product code and have them served from a static CDN. There\u2019s no reason for us to do that in production, but it\u2019s an enabling feature (we\u2019ll come back to this later). But if we can run serverless, why have a server component at all? Edison Server is a critical part of our stack. It complements the client by acting as a runtime accelerator, and is responsible for meeting the performance targets set by DWS. Let\u2019s dive into how that\u2019s accomplished.\n\nApplication acceleration with Edison Earlier I discussed pagelets and the optimizations we\u2019ve gained by processing their data requirements in parallel, early in the request lifecycle. This is one of the key optimizations the server needs to perform. But how does it provide the speed boost? Suppose we\u2019re looking at a file preview, which is a sub-function of our file-browsing application. We want to bring up a page which has a file listing and an overlaid preview of a PDF. Here\u2019s how a simplified version of that might look: The server naively responds to the initial request for /browse with the JS application entry point bundle for that page. Once it loads, the browse app requests the data for the user. Once it has that data, the browse app determines that it needs the preview JS app to proceed. Once that data loads, the page can complete rendering and the user can see the content. Here\u2019s that process represented on a timeline:\n\nAn example of a naive cascade for a pure-client JS application\n\nWhen chains of requirements execute linearly like this, they can rapidly add up to unacceptable delays in serving an experience to a user. This cascade is also greatly simplified; an actual production application can have hundreds of modules which may interact in many different ways. This is where server acceleration shines. Since Edison Server can pre-analyze the whole application tree, it can anticipate many of these requests:\n\nThe same sequence, with modules and data preloaded\n\nThis kind of acceleration via application analysis is possible because of the cross-service communication between Edison Server and Streaming Reactserver during the server-side render of the React tree. When a request comes in, the following sequence results: Edison resolves the route to a JS entry point module, packages up all the data required to render that entry point, and sends it to Streaming Reactserver. Streaming Reactserver then begins a server-side render4. During the render, the code path encounters data fetch calls (specifically, calls to Edison.prefetch ). As those calls are encountered, Streaming Reactserver sends an asynchronous message to Edison Server, which immediately kicks off the data fetch while the React render continues. When that data fetch completes, Edison Server can stream the results into the response HTML, regardless of whether the React render is complete or not, so that the data needed is ready before the application requires it. This is how we unlock the same performance wins as Edison\u2019s predecessor DWS. Even better, we end up with an architecture where data fetch requests are co-located with the code that needs them, and all the application logic now lives in a single layer of the system.\n\nMigrating to Edison Prior to Edison, we undertook a phased set of steps to gradually migrate the web page APIs away from Python and Pyxl towards JS. This provided manageable steps for stakeholders, allowing them to upgrade incrementally with benefits along the way. Edison was meant to be a seamless extension of that trend. By launch, teams that were up to date on our old stack had the following: a Python pagelet executor to handle data loads and provide initialization data\n\na set of JS application code to accept that data and render the product Migrating to Edison meant teams had only to make manageable adjustments:\n\na data servicer (could be Python, Go, or any other Dropbox-supported language) to handle data loads and provide initialization data back over gRPC\n\na set of JS application code which implemented Edison.prefetch to call out to the servicer In most cases, this work amounted to a refactor of a single Python file, repackaged with the Servicer API instead of the Pagelet API, and a new JS entry point module to wrap the JS logic and make the prefetches. Critically, this was a change that teams could undertake: incrementally, with the Edison version gated to internal traffic only\n\nwithout code duplication, since both back end and front end core code could be shared\n\nwith strong expectations that feature parity would be available to them out of the gate Asking teams to handle multiple incremental refactors took time and capital, but was critical. By the time Edison was ready for launch, the migration effort for major web services had been dramatically reduced. These refactors got us to a place where even major pages could be run simultaneously on DWS and Edison with the same JS code, making gradual rollouts easy to achieve.\n\nSolved problems To summarize: Edison successfully broke up our commingled application layers by moving all the product code into the TypeScript layer.\n\nWe preserved our ability to do acceleration with early data fetches by relying on asynchronous gRPC calls initiated during the server-side React render.\n\nWe collapsed two sources of truth for data fetches into one, making maintenance easier and the code more readable for product engineers.\n\nWe retained the ability for separate product modules\u2014what we previously called pagelets in DWS \u2014 to own their back end data providers, and to execute their JS code as soon as those providers returned the initialization data they needed.\n\nto own their back end data providers, and to execute their JS code as soon as those providers returned the initialization data they needed. Application engineers could now trivially write functionality which crossed the old pagelet boundaries, since the application is now a single, coherent React tree.\n\nRoutes that had been refactored into pure React could run simultaneously on DWS and Edison, and migrated incrementally for safety. What\u2019s left? The last piece is developer productivity. Remember that, with DWS, engineers needed to run the whole stack (100+ services) to execute any JS. At this stage in Edison\u2019s rollout, that was still the case. So let\u2019s talk about what Edison enabled us to do next, and how we\u2019re solving the next set of problems while building for the future.\n\nBuilding on Edison\n\nShifting engineers to an Edison-based surface let us do two things. First, we could immediately work on a single, coherent React tree\u2014bringing us back to the start of this journey, where I talked about moving the first parts of the Dropbox.com web surface to a SPA architecture. Second, splitting the layers between client and server properly now meant the client no longer needed to be served from the webserver. This allowed us to streamline much of our developers\u2019 workflow.\n\nThe rapid rise of the SPA Single Page Applications aren\u2019t an immediate panacea or necessarily a final endpoint of our architecture. For many sites\u2014especially mostly-static sites\u2014a thinner client could solve the same pain points. But for a site like ours, where actions such as file uploads, moves, and manipulation are center to its functionality, Dropbox on the web needs to function, in essence, as a full, graphical file manager. Being able to decouple user actions and page state from navigation around the surface is essential. Some specific examples of how a SPA architecture benefits our web surface are: It enables intuitive navigation around the file tree, allowing us to do something like drag a file into the sidebar. Those were entirely separate React apps before!\n\nWe can provide visual transitions between surfaces which used to be entirely separate, such as folder navigations. These give users a better sense of what\u2019s changing and reduce cognitive overhead.\n\nWe can deliver better performance by powering page navigations with a single API call, instead of a full page load with all its attendant costs. As this part of our product continues to mature, I am sure the SPA will be a topic worthy of its own story. Rather than dig deeper on it now, let\u2019s talk about client-server splitting and our developer productivity enhancement, which we call Edison Localmode.\n\n\n\nEdison Localmode Since the client no longer needs the webserver to function, we now have the ability to do things like: bundle our sources and deploy to a CDN, eliminating infrastructure\n\ndeploy an app to Electron\u2014a unified, cross-platform code pipeline\n\nserve the whole web client from our engineers\u2019 laptops, enabling rapid iteration All of these are awesome abilities\u2014but for the purposes of this post, it\u2018s the third we care about the most, since it solves our problem with developer productivity! The basic goal of Edison Localmode is simple: web client developers should not need to run anything but the web client. We should be able to decouple them from the webserver. In the new Edison world, engineers who are creating and iterating on JS and CSS code should no longer need to repeatedly touch the webserver\u2014or any back end services for that matter. They should be working on a pure client codebase which utilizes APIs to interface with the remainder of the Dropbox world. Edison Localmode fulfills this promise. Edison Localmode is an opt-in system which enables serving of all static assets (JS and CSS) directly from an engineer\u2019s development laptop. An engineer loads a page as normal (from a Dropbox webserver, whether that be a dev server they\u2019ve spun up, or our staging environment), boots a lightweight local Node-based asset server, and opts into Localmode. From then until they opt out, all static assets will be served from their local laptop. The engineer is then free of the need to touch the server. As they continue to iterate, the lightweight local server will watch the filesystem and re-transpile5 code on the fly, patching it directly into the running page so that modifications are applied instantly to the working surface. Let\u2019s recap, because it\u2019s deceptively easy to state, but quite transformative. Before: An engineer modifies and saves a source file. They manually issue a command to reload devbox, which syncs changed files, rebuilds, and restarts services. Once complete (15-30 seconds), the engineer switches back to the browser and hits refresh. If their code runs deep in a flow\u2014like the third stage of an interactive form\u2014the engineer needs to manually reconstruct that state each and every time they refresh. Once the page is reloaded, the engineer checks to see if their code is working correctly. Repeat as needed to complete the feature. After: An engineer modifies and saves a source file.\n\nThe code is transpiled and running in the page, with all states intact and requiring no manual intervention, before they\u2019ve even had the time to switch back to the browser. For an engineer who might have previously reloaded their devbox every few minutes, the raw time savings can add up considerably. If you\u2019re reloading your code every 15 minutes, and looking at the conservative side of devbox reload speeds, you\u2019re potentially saving one minute every hour. At six hours a day, that scales up to around four working days per year you\u2019re no longer wasting. Less quantifiably, whenever an engineer is delayed for 30 seconds, they will inevitably try to check messages on Slack while they wait, get bumped out of flow, and lose more time. Having instant feedback allows engineers to remain in flow more continuously, and increase the speed of their changes. These are big wins for developer productivity and overall satisfaction. Front end development on devbox was a notoriously frustrating experience; with the release of Edison Localmode, work that used to be a chore can now return to its natural state of being satisfying and quick!\n\nThe future\n\nEdison is the culmination of many things for Dropbox. As the web gained prominence, our workforce diversified; we grew from mostly back end engineers to include a strong cohort of full-stack web engineers. Edison would have never been possible without this culture shift. Developing Edison also led to a better understanding of the constraints of our own systems. DWS made many implicit assumptions that Edison illuminated, allowing them to be better understood and isolated. Getting to this point required incremental adjustments to our APIs and any code that used them. This increased our understanding of what future alignment of our systems with open source should look like, and what it would take to get there. Alignment with open-source is something we continue to work towards, and is a de facto good; it reduces our reliance on bespoke tools and can further increase developer productivity. In the meantime, Edison is a huge accelerator for our product development. Tasks and features which would have been inaccessible or lengthy projects under DWS are now easy, and development work which was riven with interruptions and delays has become smooth and continuous. Edison lets us think much more freely about what our web products ought to be and envision a Dropbox.com closer to the cutting edge. If building innovative products, experiences, and infrastructure excites you, come build the future with us! Visit dropbox.com/jobs to see our open roles, and follow @LifeInsideDropbox on Instagram and Facebook to see what it's like to create a more enlightened way of working. ~ ~ ~", "label": 0}
{"title": "I Left Nebraska Two Years Ago for California", "url": "https://lifeofpablo.com/blog/i-left-nebraska-two-years-ago-for-california", "content": "Some Back Story\n\nOver the years, I reflected on how much I do love Nebraska and what it means to me. I moved away two years ago from my home state of Nebraska to the state of California. I believe it's safe to say that California is home. I've done a lot growing as a person. I am not the same person who was when I left Nebraska. I'm still growing.\n\nWhy Leave Nebraska?\n\nNebraska will forever be home. It was the place that saw me grow up. I got a great public education. It taught me how to say, \"Ope!\" and how to be a Midwestern Mexican Guy.\n\nIt's been in my mind for a while to leave. That, \"for a while\" thought has been in my mind since I was a kid. At a young age I had all these dreams and ambitions to live in various places throughout the world.\n\nI was very fortune to have visited different places in high school and in university, I got to live in France through study abroad. These experiences solidified my need to venture off more throughout the United States. Traveling abroad since I was a young kid has helped me realize there are so many great things outside of Nebraska. It's funny because I joke around how I've traveled more outside the country than in the country I reside in. I am very fortunate to have lived in Mexico for extended periods of time throughout my life.\n\nPreventing Resentment and Repeating the Mistakes of Others\n\nI've always craved so much more than the simple life back home. I didn't want to feel trapped and become resentful for lack of trying to live in and experiencing new things in new places. I see many of the people who I grew up with and reflect on my own self. I didn't want that lifestyle. So many people who I had wished had left Nebraska for greener pastures didn't leave. I didn't want to be trapped.\n\nQuestioning the Environment\n\nI've always questioned my environment. These are the questions:\n\nWhy don't we have public transportation?\n\nWhy is it weird to walk on the sidewalk in such a walk-able town?\n\nWhy are people of afraid of good change?\n\nWhy don't we think of the needs of young people who will lead the future?\n\nMy Lifestyle\n\nBeing Nebraska wasn't fitting my lifestyle anymore. I had outgrown the town of 25,000 habitants. Growing up, I never really accepted myself as a person in various degrees. I tolerated myself at best. I was smiling without actually being happy. This fa\u00e7ade of being known as, \"the guy who always has a smile on his face,\" was getting old. It was draining me for years. I truly wasn't happy. No one would ever guess that. I lost myself as an individual and I also lost who I wanted to become as individual. Looking at myself in the mirror was not a true reflection of me. I just didn't feel like anything was truly going for me. There are so many things I wish I would have addressed sooner.\n\nLeaving Nebraska was going to happen sooner than later. I had a friend nudge me a few years back to finally do it. I am grateful he nudged me enough that I felt it in my ribs.\n\nCalifornia\n\nI'm Happier as a Person\n\nI'm living in California now for two years now. Time flies!\n\nI'm a lot happier here. I'm my more genuine self. I'm still not where I want to be. This is something that I am working on. California is not a perfect place.\n\nI've made a lot more progress here. I'm slowly healing myself. This will take time.", "label": 1}
{"title": "Streamlining LLM Inference at the Edge with TFLite", "url": "https://developers.googleblog.com/en/streamlining-llm-inference-at-the-edge-with-tflite/", "content": "Optimizing Time to First Token and Peak Memory Usage with a Smarter Cache for XNNPack\n\nXNNPack is the default TensorFlow Lite CPU inference engine for all models. It delivers game changing speedups across mobile, desktop, and Web platforms. One of the optimizations employed in XNNPack is repacking the static weights of the Convolution, Depthwise Convolution, Transposed Convolution, and Fully Connected operators into an internal layout optimized for inference computations. During inference, the repacked weights are accessed in a sequential pattern that is friendly to the processors\u2019 pipelines. The inference latency reduction comes at a cost: repacking essentially creates an extra copy of the weights inside XNNPack. Previous efforts have been made to reduce that cost by adding an in-memorycache to XNNPack. This cache allows sharing the packed weights between independent TFLite interpreters that would run the same model independently. TFLite XNNPack delegate implementation has been improved to address some of the shortcomings of the existing cache.\n\n1. The cache lives in anonymous memory, which incurs swapping to disk in case of memory pressure, leading to poor performance. 2. It requires repacking the initial weights every time a process is started. 3. Because repacking reads the original TFLite weights and writes to a new buffer, this leads to a high peak memory usage during the packing. 4. It requires tedious steps and careful lifecycle management to properly enable caching through XNNPack delegate. 5. It doesn\u2019t allow sharing the weights across processes.\n\n.\n\nThe New XNNPack Cache Provider Interface XNNPack has been updated and provides an interface that lets you implement a weight cache provider. A weight cache provider behaves as a dictionary that XNNPack will fill and query in order to access packed buffers. Here are its main functions. look_up looks up a packed buffer key and returns a unique identifier (or a special identifier reserved for NotFound) that may be later used to retrieve the buffer address. reserve_space reserves a buffer that may be used to store information of a given size. That buffer then needs to be committed using look_up_or_insert . look_up_or_insert checks if a buffer matching the given key exists in the cache provider. If not, the given data is committed to the cache provider. This function also returns the identifier that may be used to retrieve the buffer address. offset_to_addr returns the buffer address from the identifier returned by look_up and look_up_or_insert . The interactions between XNNPack and the weight cache provider are illustrated in the following diagram.\n\n.\n\nLoading the Cache From Disk with MMAP in the TFLite Delegate The TFLite Delegate now uses this new interface and has its own weight cache provider. This provider is capable of saving and loading the packed weights directly to / from disk. TFLite has been leveraging flatbuffer and file-backed memory mapping for a long time. We are filling the gap here by leveraging the same technique, for the following advantages.\n\nIt eliminates the repacking overhead. Persisting packed weights on disk bypasses the costly repacking process each time a model is loaded. This translates to a significant reduction in both startup latency and peak memory usage. Even for the initial building, this offers packed data deduplication and further improves packing performance by avoiding repacking the same data again.\n\nIt improves memory management. mmap leverages the operating system's virtual memory management allowing it to optimize overall system memory usage and performance. In our case, this is especially advantageous for random access bulky read-only file access, like a neural network\u2019s operation\u2019s constant weights for instance. With packed data stored on disk, the XNNPack cache no longer relies on anonymous memory which can be prone to performance issues under memory pressure. Instead, it leverages the operating system's virtual memory management for smoother operation. By eliminating the need to copy data between the file system and memory, mmap significantly reduces overhead and speeds up access times. You can find more information about file mappings and memory usage directly from mmap\u2019s man page and other interesting reads.\n\nIt allows cross-process collaboration. mmap -based file loading opens the door for seamless weight sharing between multiple processes as each process\u2019 virtual address space maps to the same physical memory pages. This not only reduces the overall memory footprint as multiple processes share the same memory but also accelerates model loading across the board.\n\n.\n\nIt simplifies the user facing API. Instead of requiring the user to setup and manage the cache object throughout the application lifetime, they can simply provide a path to the cache file.\n\nstd::unique_ptr<tflite::Interpreter> interpreter; // Setup the options for the XNNPack delegate. TfLiteXNNPackDelegateOptions xnnpack_options = TfLiteXNNPackDelegateOptionsDefault(); xnnpack_options.weight_cache_file_path = \"/tmp/cache_file.xnn_cache\"; // Create and apply the XNNPack delegate to a TFLite interpreter. // Static weights will be packed and written into weights_cache on the first run. // They will be automatically loaded for all other runs. TfLiteDelegate* delegate = TfLiteXNNPackDelegateCreate(&xnnpack_options); interpreter->ModifyGraphWithDelegate(delegate); C++ Copied\n\nMaintaining Cache Integrity To guarantee accurate and efficient inference, it's crucial to invalidate the XNNPack cache under specific conditions: Model Evolution: if your model's weights or structure change, the cached data becomes outdated and must be invalidated. This means removing the file at the provided cache path. XNNPack Upgrades: updates to XNNPack's internal packing algorithm may result in incompatible cached weights, requiring the cache to be recomputed. Fortunately XNNPack is capable of detecting this and will replace the existing cache automatically. In essence, any modification that could impact the way weights are packed or utilized by XNNPack should trigger a cache invalidation.\n\nBenchmarks The session initialisation is dominated by the weight packing. For LLMs several subgraphs are reusing the same weights. Building the cache is faster because the deduplication functionality avoids packing those same weights multiple times. For more standard models, like stable diffusion, there is no deduplication and the slightly higher initialisation time is due to saving the cache to disk. Reloading the cache (from the 2nd run on) brings the initialisation down to a fraction of the previous time in all the cases. The session initialisation improvement naturally affects the time to the first token for LLMs, roughly dividing it by 2 in the benchmarks. The memory gains brought by the cache implementation can also be seen. The peak Resident Set Size is lowered for LLMs thanks to the deduplication. For other models that don\u2019t benefit from the deduplication, there is no change. Reloading the cache brings the peak RSS even further down because the TFLite original models aren\u2019t read anymore and therefore never get pulled into memory.\n\nGemma 2B on a Pixel 8 Pro\n\n.\n\nPhi2 on a Pixel 8 Pro\n\n.\n\nStable Diffusion on a Pixel 8 Pro\n\n.", "label": 0}
{"title": "Why I Opted-Out of Facial Recognition at Customs and Border Patrol", "url": "https://lifeofpablo.com/blog/why-i-opted-out-of-facial-recognition-at-customs-and-border-patrol", "content": "I have recently returned the the United States, after a few weeks in Mexico. The time flew by and I made my way back home.\n\nThe last few weeks I've been reading on facial recognition. This has been a topic I've had an interest for quite some time. Our phones, our computers, and any item that had humble beginnings such as a door bell all have some form of facial recognition built in or programmed. It's weird to think about how you can unlock your phone with your face.\n\nPrivacy has become an issue with all these conveniences. Everywhere you go, your face (you as a person) is being tracked. EVERYWHERE!.\n\nSince I was traveling, I have been reading on facial recognition and the TSA\n\nAs a U.S. Citizen, I can opt out of facial recognition from Customs and Border Protection (CBP) :\n\nHere is a sign:\n\nA sign allowing U.S. citizens to opt out of facial scans.\n\nThe Experience\n\nI decided before I arrived at Los Angeles International AIrport (LAX) I decided to once I got to customs I would ask them to do a manual document check. I was already expecting them to ask me a bunch of questions. Things would either be easy or hard. I didn't want them running my face across different databases. Why should I give them more of my biometrics when they have this already?\n\nWhen I arrived at the he Customs and Border Protection Agent booth/station, I told them that I didn't want to my photo taken for facial scan. The agent went off on me. It seemed that he was shaming me for exercising my right to not have my face scanned. There were signs all over saying to inform an agent if I don't want my face scanned. This is exactly what I requested.\n\nThe Agent asked me the following questions\n\n\"Why are you doing this?\"\n\n\"Why are you making this more complicated?\"\n\n\"Do you see how many people have gone through starting this conversation?\"\n\n\"Do you that we have your facial biometics because you have a passport and driver's license?\"\n\n\"Any identity document you have is a biometric?'\n\nIt seemed that I was inconveniencing them for doing their job and providing a manual document check as my right to not to have my face scanned and verified across databases. The thing that made me mad was the intimidation tactics it seemed the agent was trying to get across. I didn't really appreciate that at all.\n\nMany of the questions the agent asked also came from the CBP Declaration Form 6059B.\n\nI was also asked where in Mexico I went, what other times I left the United States. I'm a person who provided to the point answers when asked. It's generally a good idea to provide answers not too short or answers too long either. They are law enforcement, you don't want to give unnecessary information to them either.\n\nI stood my ground. Eventually this whole ordeal was over and I moved on to collect my luggage.\n\nThe Issues\n\nThe issue I have is not the government has my biometrics, the problem I have is the how and who handles my data. This concern frightens me more when third-party companies or private companies are contracted by the government to handle this data. When other entities get involved,\n\nHow much control does a citizen have?\n\nHow much does the government actually follow up what these companies are doing with our data?\n\nHow is this information secured from hackers and kept safe?\n\nI felt so violated when I had to verify my identity for ID.me to access my IRS (Internal Revenue Service) to access my tax transcript. Why should a third party service be put in the way of accessing my government tax transcript? Why is identity verification being outsourced to a private company? The service wasn't really great at all. . It's so flawed for people of color.\n\nWe're in a Police Lineup", "label": 1}
{"title": "Keeping current in infosec", "url": "https://shellsharks.com/notes/2023/11/06/keeping-current-in-infosec", "content": "Mike Sass\n\n@shellsharks\n\nI do a lot of reading related to my infosec career, whether it be in-depth reading/research/analysis or just briefly skimming articles/social media posts. For what I do, staying on top of what is happening in the industry is very important. Couple that with an admittedly debilitating mild social media / phone addiction and you get a very routine course of daily reading. Below I will explain my daily routine for keeping up with cybersecurity news, research and more!\n\nDaily Infosec Reading Routine\n\nComplete Checks\n\nThe items below represent feeds I fully check daily, and in many cases, multiple-times/throughout-the day. I\u2019ve also listed them in the order I typically check them.\n\nThings I Check Sparingly\n\nBelow I\u2019ve listed feeds/platforms I check either as overflow from my primary reading or only once-in-a-while, either because the signal-to-noise ratio isn\u2019t great or the feed is algorithmic, i.e. no way to \u201cread all of it\u201d.\n\nThreads : Threads is a lively place but the infosec content there is very limited at this time.\n\n: Threads is a lively place but the infosec content there is very limited at this time. LinkedIn : There is OK infosec stuff here, if you can stomach sifting through all the insufferable content.\n\n: There is OK infosec stuff here, if you can stomach sifting through all the insufferable content. BlueSky : A small infosec contingent here but nothing compared to Mastodon.\n\n: A small infosec contingent here but nothing compared to Mastodon. Discord: There are a lot of infosec-related servers, but looking through all the different servers and their respective channels for interesting discussions/links is too time-consuming of a task. I don\u2019t even have enough time most days to look at the Shellsharks Discord server.\n\nPodcasts\n\nThough not technically \u201creading\u201d, I also have a couple infosec-related podcasts I tune in to when new episodes become available.\n\nWhen I find interesting articles, I save them to my read-it-later service (which is currently Pocket - though I plan on migrating to something else soon). When I have time, I go back into my saved items and open articles to peruse/digest.\n\nIn addition to browsing and reading articles, I also find time to share interesting articles out (via Mastodon, Threads, Discord or Lemmy) and have discussions about them if/when people engage.\n\nI will admit I am probably on the more extreme end of \u201ckeeping up with\u201d security news/trends and you certainly don\u2019t need to go to these lengths to stay fresh. But now you know what I do so feel free to copy some or all of it and get out there and learn!", "label": 1}
{"title": "Heroku", "url": "https://www.heroku.com/blog/feed/", "content": "@media only screen and (max-width: 414px) and (orientation: portrait) { #cover-image { width: 100%; } }\n\nAs part of our Blackhat Europe talk \u201cReverse Engineering and Exploiting Builds in the Cloud\u201d we publicly released a new tool called Terrier.\n\nAnnouncing Terrier: An open-source tool for identifying and analysing container and image components.\n\nIn this blog post, I am going to show you how Terrier can help you identify and verify container and image components for a wide variety of use-cases, be it from a supply-chain perspective or forensics perspective. Terrier can be found on Github.\n\nIn this blog post, I am not going to go into too much detail about containers and images (you can learn more here) however it is important to highlight a few characteristics of containers and images that make them interesting in terms of Terrier. Containers are run from images and currently the Open Containers Initiative (OCI) is the most popular format for images. The remainder of this blog post refers to OCI images as images.\n\nEssentially images are tar archives that container multiple tar archives and meta-information that represent the \u201clayers\u201d of an image. The OCI format of images makes images relatively simple to work with which makes analysis relatively simple. If you only had access to a terminal and the tar command, you could pretty much get what you need from the image\u2019s tar archive.\n\nWhen images are utilised at runtime for a container, their contents become the contents of the running container and the layers are essentially extracted to a location on the container\u2019s runtime host. The container runtime host is the host that is running and maintaining the containers. This location is typically /var/lib/docker/overlay2/<containerID>/ . This location contains a few folders of interest, particularly the \"merged\" folder. The \"merged\" folder contains the contents of the image and any changes that have occurred in the container since its creation. For example, if the image contained a location such as /usr/chris/stuff and after creating a container from this image I created a file called helloworld.txt at the location /usr/chris/stuff . This would result in the following valid path on the container runtime host /var/lib/docker/overlay2/<containerID>/merged/usr/chris/stuff/helloworld.txt .\n\nNow that we have a brief understanding of images and containers, we can look at what Terrier does. Often it is the case that you would like to determine if an image or container contains a specific file. This requirement may be due to a forensic analysis need or to identify and prevent a certain supply-chain attack vector. Regardless of the requirement, having the ability to determine the presence of a specific file in an image or container is useful.\n\nTerrier can be used to determine if a specific image contains a specific file. In order to do this, you need the following:\n\nAn OCI Image i.e TAR archive A SHA256 hash of a specific file/s\n\nThe first point can be easily achieved with Docker by using the following command:\n\n$ docker save imageid -o myImage.tar\n\nThe command above uses a Docker image ID which can be obtained using the following command:\n\n$ docker images\n\nOnce you have your image exported as a tar archive, you will then need to establish the SHA256 hash of the particular file you would like to identify in the image. There are multiple ways to achieve this but in this example, we are going to use the hash of the Golang binary go1.13.4 linux/amd64 which can be achieved with following command on a Linux host:\n\n$ cat /usr/local/go/bin/go | sha256sum\n\nThe command above should result in the following SHA256 hash: 82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd\n\nNow that we have a hash, we can use this hash to determine if the Golang binary is in the image myImage.tar . To achieve this, we need to populate a configuration file for Terrier. Terrier makes use of YAML configuration files and below is our config file that we save as cfg.yml :\n\nmode: image image: myImage.tar hashes: - hash: '82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd'\n\nThe config file above has multiple entries which allow us to specify the mode that Terrier will operate in and in this case, we are working with an image file (tar archive) so the mode is image . The image file we are working with is myImage.tar and the hash we are looking to identify is in the hashes list.\n\nWe are now ready to run Terrier and this can be done with the following command:\n\n$ ./terrier\n\nThe command above should result in output similar to the following:\n\n$ ./terrier [+] Loading config: cfg.yml [+] Analysing Image [+] Docker Image Source: myImage.tar [*] Inspecting Layer: 34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf [*] Inspecting Layer: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560 [*] Inspecting Layer: 6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759 [!] Found file '6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759/usr/local/go/bin/go' with hash: 82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd [*] Inspecting Layer: a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1 [*] Inspecting Layer: aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545 [*] Inspecting Layer: d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c [*] Inspecting Layer: dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c\n\nWe have identified a file /usr/local/go/bin/go located at layer 6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759 that has the same SHA256 hash as the one we provided. We now have verification that the image \u201cmyImage.tar\u201d contains a file with the SHA256 hash we provided.\n\nThis example can be extended upon and you can instruct Terrier to search for multiple hashes. In this case, we are going to search for a malicious file. Recently a malicious Python library was identified in the wild and went by the name \u201cJeilyfish\u201d. Terrier could be used to check if a Docker image of yours contained this malicious package. To do this, we can determine the SHA256 of one of the malicious Python files that contains the backdoor:\n\n$ cat jeIlyfish-0.7.1/jeIlyfish/_jellyfish.py | sha256sum cf734865dd344cd9b0b349cdcecd83f79a751150b5fd4926f976adddb93d902c\n\nWe then update our Terrier config to include the hash calculated above.\n\nmode: image image: myImage.tar hashes: - hash: '82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd' - hash: 'cf734865dd344cd9b0b349cdcecd83f79a751150b5fd4926f976adddb93d902c'\n\nWe then run Terrier against and analyse the results:\n\n$ ./terrier [+] Loading config: cfg.yml [+] Analysing Image [+] Docker Image Source: myImage.tar [*] Inspecting Layer: 34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf [*] Inspecting Layer: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560 [*] Inspecting Layer: 6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759 [!] Found file '6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759/usr/local/go/bin/go' with hash: 82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd [*] Inspecting Layer: a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1 [*] Inspecting Layer: aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545 [*] Inspecting Layer: d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c [*] Inspecting Layer: dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c\n\nThe results above indicate that our image did not contain the malicious Python package.\n\nThere is no limit as to how many hashes you can search for however it should be noted that Terrier performs all its actions in-memory for performance reasons so you might hit certain limits if you do not have enough accessible memory.\n\nTerrier can also be used to determine if a specific image contains a specific file at a specific location. This can be useful to ensure that an image is using a specific component i.e binary, shared object or dependency. This can also be seen as \u201cpinning\u201d components by ensuring that you are images are using specific components i.e a specific version of cURL.\n\nIn order to do this, you need the following:\n\nAn OCI Image i.e TAR archive A SHA256 hash of a specific file/s The path and name of the specific file/s\n\nThe first point can be easily achieved with Docker by using the following command:\n\n$ docker save imageid -o myImage.tar\n\nThe command above utilises a Docker image id which can be obtained using the following command:\n\n$ docker images\n\nOnce you have your image exported as a tar archive, you will need to determine the path of the file you would like to identify and verify in the image. For example, if we would like to ensure that our images are making use of a specific version of cURL, we can run the following commands in a container or some other environment that resembles the image.\n\n$ which curl /usr/bin/curl\n\nWe now have the path to cURL and can now generate the SHA256 of this instance of cURL because in this case, we trust this instance of cURL. We could determine the hash by other means for example many binaries are released with a corresponding hash from the developer which can be acquired from the developer\u2019s website.\n\n$ cat /usr/bin/curl | sha256sum 9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96\n\nWith this information, we can now populate our config file for Terrier:\n\nmode: image image: myImage.tar files: - name: '/usr/bin/curl' hashes: - hash: '9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96'\n\nWe\u2019ve saved the above config as cfg.yml and when we run Terrier with this config, we get the following output:\n\n$ ./terrier [+] Loading config: cfg.yml [+] Analysing Image [+] Docker Image Source: myImage.tar [*] Inspecting Layer: 34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf [*] Inspecting Layer: 34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf [*] Inspecting Layer: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560 [*] Inspecting Layer: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560 [*] Inspecting Layer: 6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759 [*] Inspecting Layer: 6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759 [*] Inspecting Layer: a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1 [*] Inspecting Layer: a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1 [*] Inspecting Layer: aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545 [*] Inspecting Layer: aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545 [*] Inspecting Layer: d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c [*] Inspecting Layer: d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c [*] Inspecting Layer: dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c [*] Inspecting Layer: dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c [!] All components were identified: (1/1) [!] All components were identified and verified: (1/1) $ echo $? 0\n\nThe output above indicates that the file /usr/bin/curl was successfully identified and verified, meaning that the image contained a file at the location /usr/bin/curl and that the SHA256 of that file matched the hash we provided in the config. Terrier also makes use of return codes and if we analyse the return code from the output above, we can see that the value is 0 which indicates a success. If Terrier cannot identify or verify all the provided files, a return code of 1 is returned which indicates a failure. The setting of return codes is particularly useful in testing environments or CI/CD environments.\n\nWe can also run Terrier with verbose mode enable to get more information:\n\n$ ./terrier [+] Loading config: cfg.yml [+] Analysing Image [+] Docker Image Source: myImage.tar [*] Inspecting Layer: 34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf [*] Inspecting Layer: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560 [!] Identified instance of '/usr/bin/curl' at: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560/usr/bin/curl [!] Verified matching instance of '/usr/bin/curl' at: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560/usr/bin/curl with hash: 9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96 [*] Inspecting Layer: 6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759 [*] Inspecting Layer: a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1 [*] Inspecting Layer: aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545 [*] Inspecting Layer: d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c [*] Inspecting Layer: dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c [!] All components were identified: (1/1) [!] All components were identified and verified: (1/1)\n\nThe output above provides some more detailed information such as which layer the cURL files was located at. If you wanted more information, you could enable the veryveryverbose option in the config file but beware, this is a lot of output and grep will be your friend.\n\nThere is no limit for how many hashes you can specify for a file. This can be useful for when you want to allow more than one version of a specific file i.e multiple versions of cURL. An example config of multiple hashes for a file might look like:\n\nmode: image image: myImage.tar files: - name: '/usr/bin/curl' hashes: - hash: '9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96' - hash: 'aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545' - hash: '6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759' - hash: 'd4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c'\n\nThe config above allows Terrier to verify if the identified cURL instance is one of the provided hashes. There is also no limit for the amount of files Terrier can attempt to identify and verify.\n\nTerrier\u2019s Github repo also contains a useful script called convertSHA.sh which can be used to convert a list of SHA256 hashes and filenames into a Terrier config file. This is useful when converting the output from other tools into a Terrier friendly format. For example, we could have the following contents of a file:\n\n8946690bfe12308e253054ea658b1552c02b67445763439d1165c512c4bc240d ./bin/uname 6de8254cfd49543097ae946c303602ffd5899b2c88ec27cfcd86d786f95a1e92 ./bin/gzexe 74ff9700d623415bc866c013a1d8e898c2096ec4750adcb7cd0c853b4ce11c04 ./bin/wdctl 61c779de6f1b9220cdedd7dfee1fa4fb44a4777fff7bd48d12c21efb87009877 ./bin/dmesg 7bdde142dc5cb004ab82f55adba0c56fc78430a6f6b23afd33be491d4c7c238b ./bin/which 3ed46bd8b4d137cad2830974a78df8d6b1d28de491d7a23d305ad58742a07120 ./bin/mknod e8ca998df296413624b2bcf92a31ee3b9852f7590f759cc4a8814d3e9046f1eb ./bin/mv a91d40b349e2bccd3c5fe79664e70649ef0354b9f8bd4658f8c164f194b53d0f ./bin/chown 091abe52520c96a75cf7d4ff38796fc878cd62c3a75a3fd8161aa3df1e26bebd ./bin/uncompress c5ebd611260a9057144fd1d7de48dbefc14e16240895cb896034ae05a94b5750 ./bin/echo d4ba9ffb5f396a2584fec1ca878930b677196be21aee16ee6093eb9f0a93bf8f ./bin/df 5fb515ff832650b2a25aeb9c21f881ca2fa486900e736dfa727a5442a6de83e5 ./bin/tar 6936c9aa8e17781410f286bb1cbc35b5548ea4e7604c1379dc8e159d91a0193d ./bin/zforce 8d641329ea7f93b1caf031b70e2a0a3288c49a55c18d8ba86cc534eaa166ec2e ./bin/gzip 0c1a1f53763ab668fb085327cdd298b4a0c1bf2f0b51b912aa7bc15392cd09e7 ./bin/su 20c358f7ee877a3fd2138ecce98fada08354810b3e9a0e849631851f92d09cc4 ./bin/bzexe 01764d96697b060b2a449769073b7cf2df61b5cb604937e39dd7a47017e92ee0 ./bin/znew 0d1a106dc28c3c41b181d3ba2fc52086ede4e706153e22879e60e7663d2f6aad ./bin/login fb130bda68f6a56e2c2edc3f7d5b805fd9dcfbcc26fb123a693b516a83cfb141 ./bin/dir 0e7ca63849eebc9ea476ea1fefab05e60b0ac8066f73c7d58e8ff607c941f212 ./bin/bzmore 14dc8106ec64c9e2a7c9430e1d0bef170aaad0f5f7f683c1c1810b466cdf5079 ./bin/zless 9cf4cda0f73875032436f7d5c457271f235e59c968c1c101d19fc7bf137e6e37 ./bin/chmod c5f12f157b605b1141e6f97796732247a26150a0a019328d69095e9760b42e38 ./bin/sleep b9711301d3ab42575597d8a1c015f49fddba9a7ea9934e11d38b9ff5248503a8 ./bin/zfgrep 0b2840eaf05bb6802400cc5fa793e8c7e58d6198334171c694a67417c687ffc7 ./bin/stty d9393d0eca1de788628ad0961b74ec7a648709b24423371b208ae525f60bbdad ./bin/bunzip2 d2a56d64199e674454d2132679c0883779d43568cd4c04c14d0ea0e1307334cf ./bin/mkdir 1c48ade64b96409e6773d2c5c771f3b3c5acec65a15980d8dca6b1efd3f95969 ./bin/cat 09198e56abd1037352418279eb51898ab71cc733642b50bcf69d8a723602841e ./bin/true 97f3993ead63a1ce0f6a48cda92d6655ffe210242fe057b8803506b57c99b7bc ./bin/zdiff 0d06f9724af41b13cdacea133530b9129a48450230feef9632d53d5bbb837c8c ./bin/ls da2da96324108bbe297a75e8ebfcb2400959bffcdaa4c88b797c4d0ce0c94c50 ./bin/zegrep\n\nThe file contents above are trusted SHA256 hashes for specific files. If we would like to use this list for ensuring that a particular image is making use of the files listed above, we can do the following:\n\n$ ./convertSHA.sh trustedhashes.txt terrier.yml\n\nThe script above takes the input file trustedhashes.txt which contains our trusted hashes listed above and converts them into a Terrier friendly config file called terrier.yml which looks like the following:\n\nmode: image image: myImage.tar files: - name: '/bin/uname' hashes: - hash: '8946690bfe12308e253054ea658b1552c02b67445763439d1165c512c4bc240d' - name: '/bin/gzexe' hashes: - hash: '6de8254cfd49543097ae946c303602ffd5899b2c88ec27cfcd86d786f95a1e92' - name: '/bin/wdctl' hashes: - hash: '74ff9700d623415bc866c013a1d8e898c2096ec4750adcb7cd0c853b4ce11c04' - name: '/bin/dmesg' hashes: - hash: '61c779de6f1b9220cdedd7dfee1fa4fb44a4777fff7bd48d12c21efb87009877' - name: '/bin/which' hashes: - hash: '7bdde142dc5cb004ab82f55adba0c56fc78430a6f6b23afd33be491d4c7c238b' - name: '/bin/mknod'\n\nThe config file terrier.yml is ready to be used:\n\n$ ./terrier -cfg=terrier.yml [+] Loading config: terrier.yml [+] Analysing Image [+] Docker Image Source: myImage.tar [*] Inspecting Layer: 34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf [*] Inspecting Layer: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560 [*] Inspecting Layer: 6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759 [*] Inspecting Layer: a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1 [*] Inspecting Layer: aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545 [*] Inspecting Layer: d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c [*] Inspecting Layer: dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c [!] Not all components were identifed: (4/31) [!] Component not identified: /bin/uncompress [!] Component not identified: /bin/bzexe [!] Component not identified: /bin/bzmore [!] Component not identified: /bin/bunzip2 $ echo $? 1\n\nAs we can see from the output above, Terrier was unable to identify 4/31 of the components provided in the config. The return code is also 1 which indicates a failure. If we were to remove the components that are not in the provided image, the output from the previous command would look like the following:\n\n$ ./terrier -cfg=terrier.yml [+] Loading config: terrier.yml [+] Analysing Image [+] Docker Image Source: myImage.tar [*] Inspecting Layer: 34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf [*] Inspecting Layer: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560 [*] Inspecting Layer: 6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759 [*] Inspecting Layer: a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1 [*] Inspecting Layer: aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545 [*] Inspecting Layer: d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c [*] Inspecting Layer: dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c [!] All components were identified: (27/27) [!] Not all components were verified: (26/27) [!] Component not verified: /bin/cat [!] Component not verified: /bin/chmod [!] Component not verified: /bin/chown [!] Component not verified: /bin/df [!] Component not verified: /bin/dir [!] Component not verified: /bin/dmesg [!] Component not verified: /bin/echo [!] Component not verified: /bin/gzexe [!] Component not verified: /bin/gzip [!] Component not verified: /bin/login [!] Component not verified: /bin/ls [!] Component not verified: /bin/mkdir [!] Component not verified: /bin/mknod [!] Component not verified: /bin/mv [!] Component not verified: /bin/sleep [!] Component not verified: /bin/stty [!] Component not verified: /bin/su [!] Component not verified: /bin/tar [!] Component not verified: /bin/true [!] Component not verified: /bin/uname [!] Component not verified: /bin/wdctl [!] Component not verified: /bin/zdiff [!] Component not verified: /bin/zfgrep [!] Component not verified: /bin/zforce [!] Component not verified: /bin/zless [!] Component not verified: /bin/znew $ echo $? 1\n\nThe output above indicates that Terrier was able to identify all the components provided but many were not verifiable, the hashes did not match and once again, the return code is 1 to indicate this failure.\n\nThe previous sections focused on identifying files in images, which can be referred to as a form of \u201cstatic analysis,\u201d however it is also possible to perform this analysis to running containers. In order to do this, you need the following:\n\nLocation of the container\u2019s merged folder A SHA256 hash of a specific file/s\n\nThe merged folder is Docker specific, in this case, we are using it because this is where the contents of the Docker container reside, this might be another location if it were LXC.\n\nThe location of the container\u2019s merged folder can be determined by running the following commands. First obtain the container\u2019s ID:\n\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b9e676fd7b09 golang \"bash\" 20 hours ago Up 20 hours cocky_robinson\n\nOnce you have the container\u2019s ID, you can run the following command which will help you identify the location of the container\u2019s merged folder on the underlying host.\n\n$ docker exec b9e676fd7b09 mount | grep diff overlay on / type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/7ZDEFE6PX4C3I3LGIGGI5MWQD4: /var/lib/docker/overlay2/l/EZNIFFIXOVO2GIT5PTBI754HC4:/var/lib/docker/overlay2/l/UWKXP76FVZULHGRKZMVYJHY5IK: /var/lib/docker/overlay2/l/DTQQUTRXU4ZLLQTMACWMJYNRTH:/var/lib/docker/overlay2/l/R6DE2RY63EJABTON6HVSFRFICC: /var/lib/docker/overlay2/l/U4JNTFLQEKMFHVEQJ5BQDLL7NO:/var/lib/docker/overlay2/l/FEBURQY25XGHJNPSFY5EEPCFKA: /var/lib/docker/overlay2/l/ICNMAZ44JY5WZQTFMYY4VV6OOZ, upperdir=/var/lib/docker/overlay2/04f84ddd30a7df7cd3f8b1edeb4fb89d476ed84cf3f76d367e4ebf22cd1978a4/diff, workdir=/var/lib/docker/overlay2/04f84ddd30a7df7cd3f8b1edeb4fb89d476ed84cf3f76d367e4ebf22cd1978a4/work)\n\nFrom the results above, we are interested in two entries, upperdir and workdir because these two entries will provide us with the path to the container\u2019s merged folder. From the results above, we can determine that the container\u2019s merged directory is located at /var/lib/docker/overlay2/04f84ddd30a7df7cd3f8b1edeb4fb89d476ed84cf3f76d367e4ebf22cd1978a4/ on the underlying host.\n\nNow that we have the location, we need some files to identify and in this case, we are going to reuse the SHA256 hashes from the previous section. Let\u2019s now go ahead and populate our Terrier configuration with this new information.\n\nmode: container path: merged #image: myImage.tar hashes: - hash: '82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd' - hash: 'cf734865dd344cd9b0b349cdcecd83f79a751150b5fd4926f976adddb93d902c'\n\nThe configuration above shows that we have changed the mode from image to container and we have added the path to our merged folder. We have kept the two hashes from the previous section.\n\nIf we run Terrier with this configuration from the location /var/lib/docker/overlay2/04f84ddd30a7df7cd3f8b1edeb4fb89d476ed84cf3f76d367e4ebf22cd1978a4/ , we get the following output:\n\n$ ./terrier [+] Loading config: cfg.yml [+] Analysing Container [!] Found matching instance of '82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd' at: merged/usr/local/go/bin/go with hash:82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd\n\nFrom the output above, we know that the container ( b9e676fd7b09 ) does not contain the malicious Python package but it does contain an instance of the Golang binary which is located at merged/usr/local/go/bin/go .\n\nAnd as you might have guessed, Terrier can also be used to verify and identify files at specific paths in containers. To do this, we need the following:\n\nLocation of the container\u2019s merged folder A SHA256 hash of a specific file/s The path and name of the specific file/s\n\nThe points above can be determined using the same procedures described in the previous sections. Below is an example Terrier config file that we could use to identify and verify components in a running container:\n\nmode: container path: merged verbose: true files: - name: '/usr/bin/curl' hashes: - hash: '9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96' - name: '/usr/local/go/bin/go' hashes: - hash: '82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd'\n\nIf we run Terrier with the above config, we get the following output:\n\n$ ./terrier [+] Loading config: cfg.yml [+] Analysing Container [!] Found matching instance of '/usr/bin/curl' at: merged/usr/bin/curl with hash:9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96 [!] Found matching instance of '/usr/local/go/bin/go' at: merged/usr/local/go/bin/go with hash:82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91 dd3ff92dd [!] All components were identified: (2/2) [!] All components were identified and verified: (2/2) $ echo $? 0\n\nFrom the output above, we can see that Terrier was able to successfully identify and verify all the files in the running container. The return code is also 0 which indicates a successful execution of Terrier.\n\nIn addition to Terrier being used as a standalone CLI tool, Terrier can also be integrated easily with existing CI/CD technologies such as GitHub Actions and CircleCI. Below are two example configurations that show how Terrier can be used to identify and verify certain components of Docker files in a pipeline and prevent the pipeline from continuing if all verifications do not pass. This can be seen as an extra mitigation for supply-chain attacks.\n\nBelow is a CircleCI example configuration using Terrier to verify the contents of an image.\n\nversion: 2 jobs: build: machine: true steps: - checkout - run: name: Build Docker Image command: | docker build -t builditall . - run: name: Save Docker Image Locally command: | docker save builditall -o builditall.tar - run: name: Verify Docker Image Binaries command: | ./terrier\n\nBelow is a Github Actions example configuration using Terrier to verify the contents of an image.\n\nname: Go on: [push] jobs: build: name: Build runs-on: ubuntu-latest steps: - name: Get Code uses: actions/checkout@master - name: Build Docker Image run: | docker build -t builditall . - name: Save Docker Image Locally run: | docker save builditall -o builditall.tar - name: Verify Docker Image Binaries run: | ./terrier\n\nIn this blog post, we have looked at how to perform multiple actions on Docker (and OCI) containers and images via Terrier. The actions performed allowed us to identify specific files according to thei\n\nThe post Terrier: An Open-Source Tool for Identifying and Analyzing Container and Image Components appeared first on Heroku.", "label": 0}
{"title": "Celebrating Flutter\u2019s \u201cProduction Era\u201d", "url": "https://developers.googleblog.com/en/celebrating-flutters-production-era/", "content": "This article is cross posted on Flutter\n\nJust over six years ago, we unveiled Flutter 1.0. Today, at #FlutterInProduction, we\u2019re celebrating how far we\u2019ve come \u2014 from the immense support we\u2019ve received from thousands of contributors in the community, to the widespread adoption of Flutter as a production-grade app framework for building multi-platform app experiences. If you haven\u2019t experienced Flutter yet, we invite you to try it! As we shared today, you\u2019d be joining a big group: Flutter has over 1 million monthly active developers across the globe, and powers nearly 30% of all new iOS apps. More than 90 thousand developers actively participate in Flutter Meetups across more than sixty countries. And if you want input on designing or building a new successful Flutter app, we have a large and growing list of Flutter Consultants ready to help you. \u201cApptopia tracks millions of apps in the Apple AppStore and Google Play Store, and analyzes and detects which developer SDKs were used to create the apps. Flutter is one of the most popular SDKs we track: In the Apple AppStore it has grown steadily in usage from around 10% of all tracked free apps in 2021 to nearly 30% of all tracked free apps in 2024!\u201d\n\n\u2014 Apptopia Inc. A decade of innovation to reach the production era It\u2019s been an incredible journey, starting in 2014 (in what we now call our experimental era) as a Google experiment codenamed \u201cSky.\u201d Before Flutter, compromises were inevitable. Many developers have become skeptical that any framework can truly deliver a premium experience across multiple platforms. With the launch of Flutter 1.0 in 2018 we had a clear mission to resolve that technology dilemma: We aimed to empower developers with the ultimate app framework for crafting beautiful, high-performance user interfaces across all platforms. Also, to enable developers to reach all customers with high-quality apps on all the platforms that customers care about, but with lower cost and in less time. Our focus has remained constant through Flutter\u2019s growth era, even as we\u2019ve added support for the six major platforms across mobile, web, and desktop \u2014 and continue to push beyond, with work like Toyota\u2019s use of Flutter for infotainment systems.\n\nWe\u2019re now in the \u201cproduction era,\u201d and we\u2019re celebrating that with #FlutterInProduction! This event spotlights the achievements of developers using Flutter in real-world applications.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nBuilding in partnership with the community None of this would be possible without our amazing community of over 1,400 contributors, more than 10,000 package publishers authoring over 50,000 packages, and passionate Flutter content creators and enthusiasts. Together, we\u2019ve built a top-5 GitHub open-source project by contributions!\n\nAmazing user experiences It all starts with a focus on enabling amazing user experiences. Free from typical platform constraints, Flutter supports a broad set of design languages \u2014 support for Material Design and our Apple-inspired Cupertino widgets comes with the SDK. The ecosystem also provides a broad selection of design libraries like Windows-inspired fluent_ui , macOS-inspired macos_ui , and the Ubuntu-inspired yaru widgets.\n\nScandinavian Airlines design awards With Flutter, you have the flexibility and power to realize any design your design team envisions. This is exemplified by Scandinavian Airlines, who after creating their new mobile app with Flutter have filled their trophy case with prestigious design awards such as the Red Dot Design Award, the Webby People\u2019s Voice Award, and the iF Design Gold Award. Charlotte Svensson, EVP & CIO at SAS explains: \u201cI\u2019m extremely proud over this award, which is not just an industry award, but a global recognition. It\u2019s a testament to what we can do, when we go above-and-beyond in focusing on improving the customer experience, and when we interact and develop together with our customers. SAS has always been at the forefront of innovation in the aviation industry, and this award serves as a validation of its dedication to providing exceptional digital solutions for our customers.\u201d\n\nGreat performance & reliability Performance and reliability are crucial for a positive user experience and brand perception. Slow or crash-prone apps not only frustrate users in the short term but can also damage your brand reputation in the long run through negative reviews and word-of-mouth. Flutter has prioritized performance and reliability from the outset. By choosing the Dart programming language, we ensure fast startup times through ahead-of-time compilation to native machine code or web assembly. Dart\u2019s rich, null-safety type system helps catch errors during development, further enhancing reliability. Additionally, Flutter\u2019s custom Impeller rendering engine, designed specifically for multi-platform UI, delivers smooth animations and gives us full control over the rendering stack, top to bottom, from the UI source code to the GPU.\n\nUniversal Studios performance and reliability For example, Universal Destinations and Experiences recently reported that by adopting Flutter, they not only decreased their app size \u2014 a significant benefit for users with unreliable internet connections \u2014 but also dramatically reduced app crashes to near zero, thus lowering their total cost of ownership.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nLG Electronics performance LG Electronics has traditionally relied on web apps for their webOS-powered smart TVs due to concerns about the high development cost of traditional native apps. However, they found that web apps launch slower and consume more memory than native apps. With Flutter, LG Electronics has a solution that combines fast development speed and excellent performance. As a result, they plan to use Flutter for key applications in webOS TVs globally starting in 2025.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nFirst-class developer experience and thriving ecosystem Flutter\u2019s success is deeply rooted in its focus on developer experience. We pioneered instant developer workflows with Stateful Hot Reload, and during our growth era added Flutter DevTools to significantly accelerate diagnostics and debugging workflows. Flutter\u2019s community provides a thriving and open ecosystem of over 50,000 packages published by over 10,000 publishers, combined with robust third-party services & technologies. Also, if you want input on designing or building a new successful Flutter app, we have a large list of Flutter Consultants ready to help you.\n\nMGM and developer productivity App agency Superformula has built with Flutter since August 2020. They found that Flutter is easy to learn and well documented, enabling them to get new team members up to speed quickly and contribute effectively. Superformula also used Flutter to revitalize the digital dining experience for MGM Resorts\u2019 400+ restaurants. The new Flutter-based MGM Rewards app was rebuilt in just 4 months, cutting the total amount of code in half, and improving delivery speed by a factor of 4. One core enabler of productivity for Superformula is the ability to share code across mobile, tablet-based kiosks, and web-based tools.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nGEICO user interface elements shared across web, iOS, and Android.", "label": 0}
{"title": "Saying Goodbye to Friends", "url": "https://lifeofpablo.com/blog/saying-goodbye-to-friends", "content": "Saying Goodbye to Friends\n\nRobot saying goodbye\n\nThis post was written in English (en_US).\n\nYesterday was a bitter sweet day for me. This has been a day that has been coming. It doesn't really hit you until all their things are packed up ready to be loaded in the moving truck. I look around at my friends almost empty apartment with nothing on empty walls. The sad feeling the emotions take place. Maybe, it's the allergies. These friends are very special to me as they are one of the first friends I made in California. These friends made me feel right at home as I moved a place unfamiliar to me. It all started just by saying \"hello!\" as I was moving into the unit I was moving into. We hit it off right off the bat. We would go cruising on our boards. We did disc golfing (which I'm not good at). Go on adventures throughout California. We would go over to each other's place. They threw a welcome back party after returning from South Korea after being gone for a month. They help me experience new things that definitely got me out of my comfort zone. My friends helped me grow as a person. Most importantly, they taught me how to be more fun.\n\nI'm happy they are off to a new adventure. I want them to be happy and do what is best for them. I am grateful to have made friends with them. I know that this is goodbye for now. We'll be in touch. I'll send post cards. We'll see each other along the way. I wish you all the best. See you all soon.\n\nThank you for all the memories,\n\nPablo", "label": 1}
{"title": "Investigating the impact of HTTP3 on network latency for search", "url": "https://dropbox.tech/frontend/investigating-the-impact-of-http3-on-network-latency-for-search", "content": "Dropbox is well known for storing users\u2019 files\u2014but it\u2019s equally important we can retrieve content quickly when our users need it most. For the Retrieval Experiences team, that means building a search experience that is as fast, simple, and powerful as possible. But when we conducted a research study in July 2022, one of the most common complaints was that search was still too slow. If search was faster, these users said, they would be more likely to use Dropbox on a regular basis. At that time, we found it took ~400-450ms (p75) for the search webpage to submit a query and receive a response from the server\u2014far too slow for our users who expected quicker results. It sent us looking for ways that search latency could be improved. In our early analysis, we learned that of the time it took to fetch search query results, roughly half of that time was spent in transit to and from Dropbox servers (a.k.a. network latency) while the other half was spent on determining which search results to return (a.k.a. server latency). We decided to tackle both sides of the equation simultaneously. While some of our colleagues explored ways to reduce server latency, we investigated network latency.\n\nSearch\u2019s total latency is comprised of server time and network time\n\nNetwork latency is significantly more variable than server latency. It depends on local network conditions, the user\u2019s distance from a Dropbox datacenter, and even the time of day. During business hours, many users work at offices with strong internet connections, but at night, they are at homes with weaker internet connections. Compared to North America\u2014where the majority of Dropbox data centers are located\u2014latencies can be up to twice as high in Europe and three times as high in Asia. Considering 25% of search requests originate from Europe and 15% originate from Asia, a significant portion of Dropbox users would benefit from lower network latencies. At this point, we realized that we couldn\u2019t tackle our network latency issues alone. In collaboration with the Traffic team, we considered our options and decided to test a possible solution: HTTP3.\n\nRegional differences in network latency\n\nA hypothetical speed boost\n\nDropbox.com currently uses HTTP2, a protocol based on TCP. The latest version, HTTP3, uses UDP. This speeds up the time to establish connections and serve parallel requests by: Introducing Zero Round Trip Time (0RTT) at the beginning of connections. Compared to HTTP2, HTTP3 makes one fewer round trip because it avoids the three-way handshake mandatory for TCP-based protocols. Furthermore, with 0RTT, subsequent HTTP3 connections establish a secure connection and make the actual request in the same packet, whereas in HTTP2, these pieces of data must be sent separately.\n\nCompared to HTTP2, HTTP3 makes one fewer round trip because it avoids the three-way handshake mandatory for TCP-based protocols. Furthermore, with 0RTT, subsequent HTTP3 connections establish a secure connection and make the actual request in the same packet, whereas in HTTP2, these pieces of data must be sent separately. Eliminating head-of-line blocking. TCP is stream-oriented and thus requires packets to be processed in a strict order. If a packet in one stream is lost, packets in subsequent streams could be delayed in the client\u2019s TCP stack, even if the streams are unrelated to each other. But with UDP, if one stream is blocked, other streams can still deliver data to the application.\n\nHead-of-line blocking: In HTTP2, a blocked stream also delays subsequent streams, whereas in HTTP3, a blocked stream only affects that stream\n\nHTTP3 sounded promising. In theory, it could not only speed up search requests but also operations across all of Dropbox\u2014from file uploads to content suggestions. However, it was unclear what the real world impact would be. It was entirely possible\u2014albeit unlikely\u2014for HTTP3 to be slower than HTTP2. We needed to be sure that Dropbox would benefit from a migration to HTTP3. Rather than take an unknown leap, we decided to test HTTP3 on a portion of Dropbox traffic first.\n\nSetting up the experiment\n\nTo evaluate the performance of HTTP3 on Dropbox servers, the Traffic team created a test subdomain that served our main website with HTTP3. The test site was specifically designed so that we could safely make specific API requests over HTTP3 without negatively impacting users of the main website. As part of this test site, we built a no-op API endpoint that could successfully leverage HTTP3. Because the server doesn\u2019t perform any operations, server latency would be near zero\u2014meaning any remaining latency would be network latency. With this endpoint in place, we then devised our HTTP3 test involving a series of actions meant to simulate typical request traffic on our website\u2014including when a user performs a search. The simulation had three phases: Setup. First, we pre-warmed the cache by firing off two sequential HTTP3 requests, ignoring any timing data. This was done purely to warm up any networking caches related to the HTTP2 and HTTP3 servers equally, ensuring that subsequent HTTP2 vs. HTTP3 testing was a fair comparison. This is specifically necessary for our test because the first connection is always HTTP2; that\u2019s when the client receives information required to support HTTP3. All subsequent connections would then try to use HTTP3. Running the HTTP2 control. We then ran five parallel HTTP2 requests to the no-op API endpoint and logged the network time for each request. This simulated how users currently get data from our servers, and thus was our control. Running the HTTP3 experiment. Finally, we ran another five parallel requests to the no-op API endpoint, but this time via HTTP3. We logged the elapsed network time for each request to compare against HTTP2. The most important aspect of this test was that the requests were made in parallel. This would simulate real-world scenarios at Dropbox, where many parallel requests are fired with each interaction with Dropbox web. But more importantly, it would help us determine whether eliminating head-of-line blocking would actually speed up parallel requests; if these requests were not faster, it was unlikely HTTP3 would help us in practice. To prevent any impact to user-facing performance, we only allowed our HTTP3 tests to be conducted once per page load, and only after the user completed a search. We ran the experiment for roughly two weeks between December 2022 and January 2023. Traffic regularly exceeded 1,500 queries per second (QPS) at peak times, and we successfully collected data from a wide sample of users around the world.\n\nComparing the results\n\nOver the course of our two-week experiment, 300,000 HTTP3 requests were fired per day. For the majority of our global users, HTTP3 reduced network latencies by 5-15ms (or 5%). While this is an improvement, these wins would appear negligible to the average user. At p90, however, HTTP3 demonstrated massive improvements, with a latency reduction of 48ms (or 13%)\u2014and at p95, a reduction of 146ms (21%). This could be explained by the fact that HTTP3 is better at handling packet drops in parallel connections by eliminating head-of-line blocking; because packet drops are more likely to occur in networks with suboptimal connection quality, the benefits of HTTP3 are more visible at the higher percentiles.\n\nHTTP3 vs. HTTP2 p25 -4.23ms / -4.73% p50 -5.55ms / -4.15% p75 -13.1ms / -5.78% p90 -47.6ms / -12.5% p95 -146ms / -20.9%\n\nThe results are even more prominent when split by region at the higher percentiles. HTTP3 significantly reduced network latencies for Asia by around 77ms at p90 and by 200ms at p95. Other high-traffic regions like Europe and North and Central America experienced smaller absolute improvements, though the relative improvements are similar across the board (22% at p95).\n\nHTTP3 vs. HTTP2 North and Central America Europe Asia p25 -3.20ms / -6% -2.34ms / -2% -3.73ms / -2% p50 -4.21ms / -5% -3.84ms / -3% -5.12ms / -2% p75 -9.03ms / -8% -11.1ms / -6% -15.0ms / -4% p90 -44.9ms / -17% -47.3ms / -13% -77.3ms / -14% p95 -118ms / -22% -141ms / -21% -200ms / -22%\n\nWhat\u2019s next for HTTP3", "label": 0}
{"title": "IndieWeb Carnival November 2023 \u2013 Community and belonging", "url": "https://lifeofpablo.com/blog/indieweb-carnival-november-2023", "content": "I've been looking forward for this month's Indieweb Carnival on Community and Belonging hosted by Alex Sirac. Merci Alex pour ta pr\u00e9sentation sur la sujet de communaut\u00e9.\n\nCommunity and belonging for me means, a place I feel that I belong and a place where I can be accepted for who I am. It's been hard to integrate into some communities. Often, being the only person color in a community has been difficult at times.We enter and leave communities as we older. Some communities change for the greater good and others change not for the greater good. I belong to different communities. My background and life experiences have influenced who I've become and the things I've been interested in. I've entered new ones while I've left others. Some communities I can come back and be able to jump right in right where I left off. There are communities where it was only during a certain period of my life and might be harder to be in now. There are communities where I've had to leave because it no longer followed who I was as a person or it wasn't good for me. Some became dangerous in their vission\n\nI'd like to talk about community and belonging in these three (3) situations or places.\n\nBeing a Multilingual\n\nBeing a Student in France\n\nIndieWeb\n\nI'd like to write about these topics in this carnival but I'd like to take make a dedicated post with a slightly different approach.\n\nBeing Mexican and American\n\nGrowing up in Nebraska\n\nBeing Multilingual\n\nHere in the United States, it's not common for people to speak another language. Which is a shame. Growing up in a small town in Nebraska, it often seemed that it was few of us who would consistently speak another language. Even at a young age, I felt we had a small unintentional community. I was very fortunate to grow up bilingual - Spanish & English (in this order). Then I learned French in my teens.\n\nBeing a multilingual here in the United States has allowed me to be join new communities. With the amount of people who don't speak at least a second language on a constant basis, it almost feels exclusive to be multilingual. It allows me to have deeper connections with others. It allows me to belong in various communities. When I was living in Omaha, One example is attending French speaking events at Alliance Fran\u00e7aise and on Meet-Up. It was so amazing that I could find so many individuals who I can speak French with and share many of the world views. It was nice to meet so many francophiles and francophones who enjoyed sharing culture, language, cuisine, wine and so much more. I would become great friends with these individuals. Now that I live far away, I know if I were to visit or move back, I could rejoin this community. Going back to a place where I'm a visitor and one I am familiar with, I know I can find a place of belonging again.\n\nGrowing up in a place that lacked lingual diversity, or to simply put it - growing up in a very vanilla place, it also seemed that there was a community that didn't seem to like people speaking different languages. It seemed to alienate them. There seemed to be pressure to join the community where one needed to behave and present themselves with the majority. I know people who had my cultural background give into that community and not speak their language spoken at home. Parents gave in the idea that there kids will be more valuable in society if they don't speak another language. I felt supported and limited by these co-existing communities. I'd be lying if at one point I didn't fall in this negative community. I felt at times I needed this negative community to to belong. Learning to belong to something that conflict with my values felt like being pulled in both directions.\n\nBeing a Student in France\n\nI have very fond memories of being an exchange student in France. I studied at the Universit\u00e9 de Strasbourg at the age 22. Being a student in France was different compared the attending university in the United States. Being in University you build Not only was I apart of the exchange student community but I was also a student trying to improve my French abilities. I was the only student from the university back home who attended a program in France. I isolated myself the Americans. In a way I left the \"American\" community for many months. I saw this as a blessing to form new habits and quickly find a new community or communities. We were here to learn the French language. We all were from various parts of the world. I was hanging out with all the people and students from France, the rest of the European Union, South America, and basically anyone from around the world. Just not the Americans\n\nWe were homesick. We bonded with food. We made meals from our respective cultures. We would learn about their upbringing and how they got to France. We all bonded together because many of us had experiences with immigrant culture. Many of these students were also immigrants to France or have been in the European Union already. They were trying establish themselves and incorporate themselves in the society. Seeing them everyday reminded me of my parents telling me stories of them trying to make their way in the United States. I understand being in a new place and trying to find a sense of belonging can be daunting.\n\nYes, I could do many of these things back at my home university. There was such a deeper connection with these international students. We were living and studying in a place where being a student meant so much more. I felt I could have genuine conversations with people. I really felt like I belonged here and understand what it's like to be student outside the United States. Student community here is nothing like in the United States.\n\nIndieWeb\n\nOh the IndieWeb! I joined back in March 2023. I heard of the IndieWeb throughout the years but didn't think much of it. I'm happy that I got the interest and courage to join. This is a community that I am truly happy to have found. It has become a community I truly feel that I am apart of. The best part of it is getting to meet so many great people and interacting with them on a weekly basis. People are so willing to help each other out. I've had people reach out if there is something wrong with my website or microformats are not placed correctly. It's been pretty rad attending Homebrew Website Club. Every single one of them has been pretty amazing. I really enjoy the encouragement of taking initiative to as planning events, starting writing carnivals, etc. It's a pretty open group.\n\nThe IndieWeb has helped me grow as an individual. It has also rekindled the fire in things I didn't think I would find interest again. One example of that is blogging. It This community has helped me find a new place of belonging.\n\nI will admit since I'm still relatively new to the community, I'm still a little shy and still learning the ropes. I know that this is part of the journey.\n\nConclusion\n\nI'm happy with all these experiences.", "label": 1}
{"title": "Travel Adventures", "url": "https://shellsharks.com/blog-challenge-travel-adventures", "content": "A Blog Questions Challenge\n\nHere\u2019s another Blog Questions Challenge. This week, it\u2019s all about Travel Adventures!\n\nHere are the questions\u2026\n\nSilliest Souvenir\n\nIn 2014 me and my wife went to Costa Rica. We did a lot of travel-by-bus to various excusions and destinations across the country. During those bus rides, I (for whatever reason) witnessed an unusual amount of machete-based chopping of vegetation and general plant-related growth on the sides of the rural roads we traveled down. Now this might not be unusual for Costa Rica, but it was for me as a tourist. So naturally, I wanted to get a machete as a souvenir. At the time of purchasing it, I never considered what it would look like, trying to get a huge machete back through customs at the airport\u2026\n\nState-side, we got to security and that\u2019s when I remembered my cargo. Sure enough, going through security I was asked to come with them to a back room as they had some questions for me. I thought for sure it was because of the machete. But nope! They were interested in a different souvenir of mine. We had also purchased a \u201crain stick\u201d, which is effectively just a hollow wooden tube filled with small beads. They were really curious (and slightly concerend) about what all those little beads were inside the stick. After a brief explanation though they were satisfied and I was on my way once more. Not even a single mention of the machete. Hah!\n\nTeleport for a Day Trip\n\nThat\u2019s me in Bora Bora circa 2016. There\u2019s a very non-zero chance that I was thinking about climbing that mountain in this exact moment (in addition to just posing for a super-cool sunset pic). If I could teleport to one place for a day to do something, it wound be here, to hike Bora Bora\u2019s Mount Otemanu. Afterwards, I\u2019d just chill on the beach with a nice beverage and watch the sun set, much like I did in 2016. \ud83c\udfdd\ufe0f \ud83c\udf05\n\nWeirdest Food\n\nIn 2013, I went on my first international trip ever. So where does one go on their first trip out of the country? Africa of course! Me and my wife went to South Africa\u2014first to Cape Town and then to Kruger National Park for some safari-ing. To this day (and I\u2019ve done a fair bit of travel since), I would still say it\u2019s my favorite trip I\u2019ve ever done. One night, while dining out in the city-center of Cape Town, we ordered what I remember as some sort of \u201cexotic meat sampler\u201d. On the resulting plate, we tried a number of interesting South African-native game including Kudu, Crocodile, Springbok and if I\u2019m remembering fully, Zebra. All I really remember about that experience beyond what I tried was that Kudu was delicious, and I was not into the croc.\n\nMemorable Wrong Turn\n\nLook, my wife is the (trip) planner of the two of us, and she is amazing at it. What this means is that our trips are always fantastic, we get into a TON of fun activities, and we rarely encounter anything you might consider a \u201cwrong turn\u201d. That said, this particlar question made me think of two distinct wrong-turn-esque events\u2026\n\nOnce, while driving somewhere in Germany we witnessed this little car in front of us try to make too sharp of a turn on a highway on-ramp and completely spin out\u2014like a 360\u00b0 spin. It was wild, and quite scary to witness. A \u201cwrong turn\u201d for that individual to say the least.\n\nThe most memorable \u201cwrong turn\u201d for me however occurred when me and my wife were trying to drive back from the national park in Sintra, Portugal to where we were staying in Lisbon. We exited the park into what I imagine was the old-town part of Sintra and ended up on some very tiny streets with extremely narrow passage ways between the buildings and walls of the town. At one point, my wife had to get out of the car and help me narrowly traverse one particularly tight corridor. Our rental was a pretty small car too. Luckily, we made it out without a scratch, literally!\n\nThanks for reading!", "label": 1}
{"title": "The value of AI: How Microsoft\u2019s customers and partners are creating differentiated AI solutions to reinvent how they do business today", "url": "https://blogs.microsoft.com/blog/2025/01/28/the-value-of-ai-how-microsofts-customers-and-partners-are-creating-differentiated-ai-solutions-to-reinvent-how-they-do-business-today/", "content": "Organizational leaders in every industry around the world are evaluating ways AI can unlock opportunities, drive pragmatic innovation and yield value across their business. At Microsoft, we are dedicated to helping our customers accelerate AI Transformation by empowering human ambition with Copilots and agents, developing differentiated AI solutions and building scalable cybersecurity foundations. At Microsoft Ignite we made over 100 announcements that bring the latest innovation directly to our customers and partners, and shared how Microsoft is the only technology leader to offer three distinct AI platforms for them to build AI solutions:\n\nCopilot is your UI for AI, with Copilot Studio enabling low-code creation of agents and extensibility to your data. Azure AI Foundry is the only AI app server for building real-world, world-class, AI-native applications. Microsoft Fabric is the AI data platform that provides one common way to reason over your data \u2014no matter where it lives.\n\nAll three of these platforms are open and work synchronously to enable the development of modern AI solutions; and each is surrounded by our world-class security offerings so leaders can move their AI-first strategies forward with confidence.\n\nAs we look ahead to what we can achieve together, I remain inspired by the work we are doing today. Below are a handful of the many stories from the past quarter highlighting the differentiated AI solutions our customers and partners are driving to move business forward across industries and realize pragmatic value. Their success clearly illustrates that real results can be harnessed from AI today, and it is changing the way organizations do business.\n\nTo power its industrial IoT and AI platform, ABB Group leveraged Microsoft Azure OpenAI Service to create Genix Copilot: a generative AI-powered analytics suite aimed at solving some of the most complex industrial problems. The solution helps customers analyze key functions in their operations \u2014such as asset and process performance, energy optimization and emission monitoring \u2014 with real-time operational insights. As a result, customers are seeing up to 35% savings in operations and maintenance, and up to 20% improvement in energy and emission optimization. ABB also saw an 80% decrease in service calls with the self-service capabilities of Genix Copilot.\n\nServing government healthcare agencies across the US, Acentra Health turned to Microsoft to help introduce the latest AI capabilities that maximize talent and cut costs in a secure, HIPAA-compliant manner. Using Azure OpenAI Service, the company developed MedScribe \u2014 an AI-powered tool reducing the time specially trained nursing staff spend on appeal determination letters. This innovation saved 11,000 nursing hours and nearly $800,000, reducing time spent on each appeal determination letter by about 50%. MedScribe also significantly enhanced operational efficiency, enabling nurses to process 20 to 30 letters daily with a 99% approval rate.\n\nTo ease challenges for small farmers, Romanian agribusiness group Agricover revolutionized access to credit by developing MyAgricover. Built with help from partner Avaelgo, the scalable digital platform utilizes Microsoft Azure, Azure API Management and Microsoft Fabric to automate the loan process and enable faster approvals and disbursements. This has empowered small farmers to grow their businesses and receive faster access to financing by reducing loan approval time by 90 percent \u2014 from 10 working days to a maximum of 24 hours.\n\nBuilding on its status as a world-class airline with a strong Indian identity, Air India sought ways to enhance customer support while managing costs. By developing AI.g, one of the industry\u2019s first generative AI virtual assistants built on Azure OpenAI Service, the airline upgraded the customer experience. Today, 97% of customer queries are handled with full automation, resulting in millions of dollars of support costs saved and improved customer satisfaction \u2014 further positioning the airline for continued growth.\n\nBMW Group aimed to enhance data delivery efficiency and improve vehicle development and prototyping cycles by implementing a Mobile Data Recorder (MDR) solution with Azure App Service, Azure AI and Azure Kubernetes Service (AKS). The solution achieved 10 times more efficient data delivery, significantly improved data accessibility and elevated overall development quality. The MDR monitors and records more than 10,000 signals twice per second in every vehicle of BMW\u2019s fleet of 3,500 development cars and transmits data within seconds to a centralized cloud back end. Using Azure AI Foundry and Azure OpenAI Service, BMW Group created an MDR copilot fueled by GPT-4o. Engineers can now chat with the interface using natural language, and the MDR copilot converts the conversations into KQL queries, simplifying access to technical insights. Moving from on-premises tools to a cloud-based system with faster data management also helps engineers troubleshoot in real time. The vehicle data covered by the system has doubled, and data delivery and analysis happen 10 times faster.\n\nColes Group modernized its logistics and administrative applications using Microsoft Azure Stack HCI to scale its edge AI capabilities and improve efficiency and customer experience across its 1,800 stores. By expanding its Azure Stack HCI footprint from two stores to over 500, Coles achieved a six-fold increase in the pace of application deployment, significantly enhancing operational efficiency and enabling rapid innovation without disrupting workloads. The retailer is also using Azure Machine Learning to train and develop edge AI models, speeding up data annotation time for training models by 50%.\n\nMultinational advertising and media company Dentsu wanted to speed time to insights for its team of data scientists and media analysts to support its media planning and budget optimization. Using Microsoft Azure AI Foundry and Azure OpenAI Service, Dentsu developers built a predictive analytics copilot that uses conversational chat and draws on deep expertise in media forecasting, budgeting and optimization. This AI-driven tool has reduced time to media insights for employees and clients by 90% and cut analysis costs.\n\nTo overcome the limitations of its current systems, scale operations and automate processes across millions of workflows, Docusign created the Intelligent Agreement Management (IAM) platform on Azure. Using Azure AI, Azure Cosmos DB, Azure Logic Apps and AKS, the platform transforms agreement data into actionable insights to enhance productivity and accelerate contract review cycles. IAM also ensures better collaboration and unification across business systems to provide secure solutions tailored to diverse customer needs. For example, its customer KPC Private funds reported a 70% reduction in time and resources dedicated to agreement processes.\n\nEmirates Global Aluminium (EGA) transformed its manufacturing operations by leveraging a hybrid environment with Azure Arc, Azure Stack HCI and Azure Kubernetes Service. This digital manufacturing platform resulted in 86% cost savings for AI image and video analytics and a 13-fold improvement in AI response times. The seamless hybrid cloud architecture has enhanced EGA\u2019s operational efficiency and agility, supporting its Industry 4.0 transformation strategy.\n\nEY collaborated with Microsoft to enhance the inclusivity of AI development using Azure AI Foundry. By involving neurodivergent technologists from EY\u2019s Neuro-Diverse Centers of Excellence, they improved the accessibility and productivity of AI tools, resulting in more inclusive AI solutions, fostering innovation and ensuring that AI tools unlock the potential of all users. With an estimated 20% of the global workforce identifying as neurodivergent, inclusive AI solutions are crucial for maximizing creativity and productivity. Neurodivergent EY technologists also collaborated with Microsoft developers to make Azure AI Foundry more inclusive and help all users work productively to create innovative AI solutions.\n\nColombian household appliance manufacturer Haceb integrated AI to optimize processes, reduce costs and improve service quality. Using Microsoft Copilot Studio and Azure OpenAI Service, the company created a virtual technical support assistant, saving its 245 technicians 5 minutes per visit \u2014 a total of 5,000 minutes saved daily. This AI solution has enhanced efficiency and boosted customer satisfaction by allowing for faster issue resolution. Haceb\u2019s AI adoption has also empowered employees, boosted productivity and positioned the company as a leader in AI innovation in Colombia.\n\nTo better serve its global patients, Operation Smile \u2014 in collaboration with partner Squadra \u2014 leveraged Azure AI, Machine Learning and Microsoft Fabric to develop an AI-powered solution to predict surgical outcomes and optimize resource allocation. This innovation resulted in a 30% increase in surgical efficiency, a 90% reduction in translation errors and improved patient outcomes. Additionally, report generation is now up to 95% quicker, and repeated medical events have decreased by 15%, enabling Operation Smile to provide better care to more children worldwide.\n\nOntada \u2014 a McKesson business dedicated to oncology data and evidence, clinical education and point-of-care technologies \u2014 needed a way to generate key insights across 150 million unstructured oncology documents. Using Microsoft Azure AI and Azure OpenAI Service, Ontada developed a data platform solution called ON.Genuity to provide AI-driven insights into the patient journey, enhance patient trial matching and identify care gaps. The company also implemented large language models to target nearly 100 critical oncology data elements across 39 cancer types, enabling the company to analyze an estimated 70% of previously inaccessible data, reduce processing time by 75% and accelerate product time-to-market from months to just one week.\n\nAs the UK\u2019s largest pet care company, Pets at Home sought a way to combat fraud across its retail operations \u2014 particularly as its online business continued to grow. Working closely with its fraud team, it adopted Copilot Studio to develop an AI agent that quickly identifies suspicious transactions. The agent autonomously gathers relevant information, performs analysis and shares it with a fraud agent to enable a manual, data-intensive investigative process while ensuring a human remains in the loop. With this low-code agent extending and seamlessly integrating into existing systems, the company\u2019s fraud department can act more quickly; what used to take 20 to 30 minutes is now handled by the AI agent within seconds. The company is identifying fraud 10 times faster and is processing 20 times more cases a day. Now, the company can operate at scale with speed, efficiency and accuracy \u2014 with savings expected to be in the seven figures as it continues to build more agents.\n\nRevenue Grid, a technology company specializing in sales engagement and revenue optimization solutions, partnered with Cloud Services to modernize its data infrastructure and develop a unified data warehouse capable of handling unstructured, semi-structured and structured data. By migrating to Microsoft Fabric, Revenue Grid can now deliver data-powered revenue intelligence, driven by a unified platform, elastic scalability, enhanced analytics capabilities and streamlined operations. Revenue Grid has reduced infrastructure costs by 60% while enhancing its analytical capabilities to improve real-time data processing, empowering sales teams with accurate and diverse data.\n\nTo better manage and integrate employee data across diverse regions and systems, UST built a comprehensive Employee Data platform on Microsoft Fabric. In under a year, UST migrated 20 years of employee data with all security measures to enhance data accessibility and employee productivity. The Meta Data Driven Integration (MDDI) framework in Fabric also helped the company cut data ingestion time by 50% so employees can focus more on analysis than preparation. As a result of this implementation, the company has seen an increase in collaboration and innovation from employees, helping put its values into action.\n\nThe Microsoft Commercial Marketplace offers millions of customers worldwide a convenient place to find, try and buy software and services across 140 countries. As a Marketplace partner, WeTransact is helping independent software vendors (ISVs) list and transact their software solutions \u2014 and find opportunities for co-selling and extending their reach to enterprise customers through development of the WeTransact platform. Powered by Azure OpenAI Service, the platform is changing the way partnerships are being built by using AI pairing to facilitate a \u201cplug and play\u201d reseller network. More than 300 ISVs worldwide have joined the Microsoft Commercial Marketplace using the WeTransact platform, cutting their time to publish by 75%.\n\nThe opportunity for AI to create value is no longer an ambition for the future \u2014 it is happening now, and organizational leaders across industries are investing in AI-first strategies to change the way they do business. We believe AI should empower human achievement and enrich the lives of employees; and we are uniquely differentiated to help you accelerate your AI Transformation responsibly and securely. Choosing the right technology provider comes down to trust, and I look forward to what we will achieve together as we partner with you on your AI journey.\n\nTags: AI, Azure, Azure AI, Azure AI Foundry, Azure Arc, Azure OpenAI Service, Azure Stack HCI, Copilot, Copilot Studio, Microsoft Fabric, Microsoft Ignite 2024", "label": 0}
{"title": "OpenTelemetry Basics on Heroku Fir", "url": "https://www.heroku.com/blog/opentelemetry-basics-on-heroku-fir/", "content": "Heroku recently made the next generation platform \u2013 Fir \u2013 generally available. Fir builds on the strengths of the Cedar generation while introducing a new modern era of developer experience. Fir leverages modern cloud-native technologies to provide a seamless and performant platform.\n\nOne of the goals we set out to achieve with Fir is to modernize our platform\u2019s observability architecture. Applications being written today are becoming increasingly more distributed and complex in nature. With this increase in complexity, the need for good observability becomes critical. With solid observability practices in place, it becomes possible to gain deep insights into the internal state of these complex systems.\n\nThe Cloud Native Computing Foundation (CNCF)\u2019s second most popular project, OpenTelemetry, standardizes and simplifies the collection of observability data (logs, metrics, and traces) for distributed systems. Integrating OpenTelemetry into Fir makes it easier to monitor, troubleshoot, and improve complex applications and services. OpenTelemetry is more than just a set of tools \u2013 it is a standard you as an end-user can benefit from a growing community of vendors that support the OpenTelemetry protocol.\n\nIt is for these reasons that we have chosen to build OpenTelemetry directly into the Fir platform. In this blog post we will explain what OpenTelemetry is and how you can quickly get started using OpenTelemetry on Heroku.\n\nWhat is OpenTelemetry\n\nOpenTelemetry is an open-standard framework that provides a standardized way to collect and export telemetry data from applications. It supports three primary signals:\n\nLogs: Capture discrete events that happen over time. This signal type provides detailed context for events, aiding in debugging and auditing.\n\nMetrics: Provide quantitative measurements of system behavior captured at runtime. Metrics offer insights into system performance and resource utilization.\n\nTraces: Record the execution path of requests through a system. These help in understanding the flow of requests and diagnosing latency issues.\n\nIn addition to these three signals, two more are under development.\n\nEvents: A specific type of log, an Event is a named occurrence at an instant in time. It signals that \u201cthis thing has happened at this time\u201d. Examples of Events might include things like uncaught exceptions, network events, user login/logout, etc.\n\nProfiles: A mechanism to collect performant and consistent profiling data\n\nOpenTelemetry SDKs and Collectors\n\nThe OpenTelemetry SDK and Collector serve distinct purposes in an observability pipeline. The SDK is a library that allows developers to instrument their applications to generate telemetry like traces, metrics and logs. The collector sits downstream of the application and receives, processes and exports that telemetry data to various other backends. The collector acts as a central hub for observability data.\n\nTo recap,\n\nAn OpenTelemetry SDK:\n\nProvides language-specific implementations of the OpenTelemetry API.\n\nEmpowers the developers to instrument applications, generating telemetry data.\n\nManages the data collection and processing within the application.\n\nSends the telemetry data to a Collector or directly to an observability backend.\n\nAn OpenTelemetry Collector:\n\nIs a standalone, vendor-agnostic process.\n\nReceives telemetry data from multiple sources, including SDKs.\n\nProcesses the telemetry data through pipelines.\n\nExports processed telemetry data to observability backends like Prometheus, Jaeger and other vendors.\n\nActs as a central hub for managing telemetry pipelines.\n\nAt Heroku, our mission is to provide a platform that allows you, the developer, to focus on what matters most; building that app itself. Our platform automatically acts as the central hub for managing your telemetry pipelines.\n\nGetting started\n\nFor the purposes of this blog post we are going to use the Getting Started on Heroku Fir with Go tutorial. Zipping through most of the instructions we can bootstrap our application using only a few commands from a terminal.\n\nThe first thing we need to do is ensure that we have the latest version of the Heroku CLI installed. If you do not have the Heroku CLI installed or need to perform an update, simply follow the instructions found in the Heroku Dev Center.\n\n$ heroku version heroku/10.7.0 darwin-arm64 node-v20.19.1\n\nNow we need a Fir space, so let\u2019s create one:\n\n$ heroku spaces:create heroku-otel-demo --generation fir --team demo-team \u203a Warning: Spend Alert. Each Heroku Standard Private Space costs ~$1.39/hour (max $1000/month), pro-rated to the second. \u203a Warning: Use heroku spaces:wait to track allocation. === heroku-otel-demo ID: bdacda5f-a9b5-41a7-a613-58a546ccd645 Team: heroku-runtime-playground Region: virginia CIDR: 2600:1f18:7a42:c600::/56 Data CIDR: State: allocated Shield: off Generation: fir Created at: 2025-04-23T20:51:39Z\n\nNext we need to clone down the repository and change in our working directory:\n\n$ git clone https://github.com/heroku/go-getting-started.git Cloning into 'go-getting-started'... remote: Enumerating objects: 4352, done. remote: Counting objects: 100% (897/897), done. remote: Compressing objects: 100% (711/711), done. remote: Total 4352 (delta 470), reused 162 (delta 162), pack-reused 3455 (from 2) Receiving objects: 100% (4352/4352), 10.62 MiB | 3.26 MiB/s, done. Resolving deltas: 100% (1734/1734), done. $ cd go-getting-started/\n\nNow, we can simply create the application and push the code to Heroku:\n\n$ heroku create --space heroku-otel-demo Creating app in space heroku-otel-demo... done, \u2b22 fathomless-island-10342 http://fathomless-island-10342-6bd6dfa13d9e.aster-virginia.herokuapp.com/ | https://git.heroku.com/fathomless-island-10342.git $ git push heroku main Enumerating objects: 3679, done. Counting objects: 100% (3679/3679), done. Delta compression using up to 16 threads Compressing objects: 100% (2033/2033), done. Writing objects: 100% (3679/3679), 8.35 MiB | 448.00 KiB/s, done. Total 3679 (delta 1444), reused 3676 (delta 1444), pack-reused 0 (from 0) remote: Resolving deltas: 100% (1444/1444), done. remote: Updated 1310 paths from 49f32a9 remote: Compressing source files... done. remote: Building source: ...\n\nFinally, we can verify that the application is running using one last command:\n\n$ heroku open\n\nThis will open your default browser window. You should see something like this:\n\nGreat! We\u2019ve got a functioning application running inside a Fir Space. Our next step is to send any platform telemetry to an observability vendor. For this demo, we\u2019re going to use Grafana Cloud. Head over to grafana.com and create a Cloud Free account. Once you have signed up you will be presented with a Welcome to Grafana Cloud page.\n\nAt this point, we are going to skip the rest of the \u201cGetting started\u201d steps. The directions provided by the setup guide do not apply to how we are going to send telemetry data. For now, we can simply click \u201cSkip setup\u201d.\n\nThe easiest way to establish a Heroku Telemetry Drain to Grafana Cloud is to use a slightly different path. In a new browser tab, we will simply use the Grafana Cloud Portal. From Grafana.com click \u201cMy Account\u201d.\n\nFrom there, click the \u201cDetails\u201d button next to your Grafana Cloud stack. Mine is called herokudemo . Next click on the OpenTelemetry \u201cConfigure\u201d button.\n\nFor now, don\u2019t worry about copying any of the details to your Clipboard. Instead, scroll down to the \u201cPassword / API Token\u201d section and click on the \u201cGenerate now\u201d link. Give your token a name. Once you are done, make sure you keep a copy of the generated token for future reference. Now that we have a token, scroll down a bit more and copy the contents of the \u201cEnvironment Variables\u201d section to your clipboard.\n\nNow we can head back to our terminal window and paste environment variables. We can confirm that pasting the environment variables work by using echo quickly:\n\n$ echo $OTEL_EXPORTER_OTLP_ENDPOINT https://otlp-gateway-prod-ca-east-0.grafana.net/otlp $ echo $OTEL_EXPORTER_OTLP_HEADERS Authorization=Basic MTIzOTIwMjpnbGNfZXlKdklqb2lNVFF4TXpFMU15SXNJbTRpT2lKemRHRmpheTB4TWpNNU1qQXlMVzkwYkhBdGQzSnBkR1V0WkdWdGJ5SXNJbXNpT2lKNE5GZFZOa3hDY0RNNU16VkxOR0ptVkVjMGN6ZE9XVGNpTENKdElqcDdJbklpT2lKd2NtOWtMV05oTFdWaGMzUXRNQ0o5ZlE9PQ==\n\nNext, we will convert the headers into a json format that the Heroku CLI command expects.\n\n$ export HEROKU_OTLP_HEADERS=\"$(echo \"$OTEL_EXPORTER_OTLP_HEADERS\" | sed 's/^\\([^=]*\\)=\\(.*\\)$/{\"\\1\":\"\\2\"}/')\" $ echo $HEROKU_OTLP_HEADERS {\"Authorization\":\"Basic MTIzOTIwMjpnbGNfZXlKdklqb2lNVFF4TXpFMU15SXNJbTRpT2lKemRHRmpheTB4TWpNNU1qQXlMVzkwYkhBdGQzSnBkR1V0WkdWdGJ5SXNJbXNpT2lKNE5GZFZOa3hDY0RNNU16VkxOR0ptVkVjMGN6ZE9XVGNpTENKdElqcDdJbklpT2lKd2NtOWtMV05oTFdWaGMzUXRNQ0o5ZlE9PQ==\"}\n\nFinally, we can add the Heroku Telemetry Drain:\n\n$ heroku telemetry:add --app fathomless-island-10342 $OTEL_EXPORTER_OTLP_ENDPOINT --transport http --headers \"$HEROKU_OTLP_HEADERS\" successfully added drain https://otlp-gateway-prod-ca-east-0.grafana.net/otlp\n\nBack from the Grafana Cloud dashboard, after a few minutes you will start to see some application specific metrics flowing into Grafana Cloud.\n\nNow if you navigate back to your application in the browser (Pro Tip, use heroku open ), and hit refresh a few times you should also start to see traces and logs flowing into Grafana Cloud as well.\n\nIn Conclusion: Fir\u2019s Observability Power \u2013 And Where We Go From Here\n\nSo, as we\u2019ve shown, Heroku\u2019s Fir platform, with its built-in OpenTelemetry, streamlines the process of setting up observability for your applications. This means you can move quickly from deploying your app to gaining critical insights into its performance, as demonstrated by the walkthrough using Grafana Cloud. But what you\u2019ve seen here is just one of the many benefits of Heroku\u2019s next-generation platform. In the next part of this series, we\u2019ll dive deeper into how to effectively analyze the telemetry data you\u2019re now collecting. We\u2019ll explore techniques for querying, visualizing, and correlating traces, metrics, and logs to unlock powerful insights that will help you optimize your application\u2019s behavior and troubleshoot issues like a pro.\n\nTo get the full picture of everything the Fir platform offers, from enhanced observability to a modern developer experience, don\u2019t forget to watch the Fir launch webinar on-demand!", "label": 0}
{"title": "Journey to 1000 models: Scaling Instagram\u2019s recommendation system", "url": "https://engineering.fb.com/2025/05/21/production-engineering/journey-to-1000-models-scaling-instagrams-recommendation-system/", "content": "In this post, we explore how Instagram has successfully scaled its algorithm to include over 1000 ML models without sacrificing recommendation quality or reliability.\n\nWe delve into the intricacies of managing such a vast array of models, each with its own performance characteristics and product goals.\n\nWe share insights and lessons learned along the way\u2014from the initial realization that our infrastructure maturity was lagging behind our ambitious scaling goals, to the innovative solutions we implemented to bridge these gaps.\n\nIn the ever-evolving landscape of social media, Instagram serves as a hub for creative expression and connection, continually adapting to meet the dynamic needs of its global community. At the heart of this adaptability lies a web of machine learning (ML) models, each playing a crucial role in personalizing experiences. As Instagram\u2019s reach and influence has grown, so too has the complexity of its algorithmic infrastructure. This growth, while exciting, presents a unique set of challenges, particularly in terms of reliability and scalability.\n\nJoin us as we uncover the strategies and tools that have enabled Instagram to maintain its position at the forefront of social media innovation, ensuring a seamless and engaging experience for billions of users worldwide.\n\nAre there really that many ML models in Instagram?\n\nThough what shows up in Feed, Stories, and Reels is personally ranked, the number of ranked surfaces goes much deeper\u2014to which comments surface in Feed, which notifications are \u201cimportant,\u201d or whom you might tag in a post. These are all driven by ML recommendations.\n\nWithin a given surface, we\u2019ll have different layers of the ranking funnel: sourcing (retrieval), early-stage ranking (ESR), and late-stage ranking (LSR). We operate on fewer candidates as we progress through the funnel, as the underlying operations grow more expensive (see Figure 1 below):\n\nWithin each surface and layer, there is constant experimentation, and these permutations create a severe infrastructure challenge. We need to allow room for our ML engineers to experiment with changes such as adjusting weights for a given prediction. The net result, depicted below in Figure 2, is a large number of models serving user traffic in production:\n\nHow did we realize infra maturity wasn\u2019t going to catch up?\n\nIdentified risks\n\nWe identified several risks associated with scaling our algorithm, rooted in complaints about ML productivity and repeating patterns of issues:\n\nDiscovery: Even as a team focused on one app \u2014 Instagram \u2014 we couldn\u2019t stay on top of the growth, and product ML teams were maintaining separate sources of truth, if any, for their models in production.\n\nRelease: We didn\u2019t have a consistent way to launch new models safely, and the process was slow, impacting ML velocity and, therefore, product innovation.\n\nHealth: We lacked a consistent definition of model prediction quality, and with the diversity of surfaces and subtlety of degraded ranking, quality issues went unnoticed.\n\nSolution overview\n\nTo address these risks, we implemented several solutions:\n\nModel registry: We built a registry that serves as a ledger for production model importance and business function foremost, among other metadata. This registry serves as our foundational source of truth, upon which we can leverage automation to uplevel system-wide observability, change management, and model health.\n\nModel launch tooling: We developed a more ideal flow for launching new models that includes estimation, approval, prep, scale-up, and finalization. This process is now automated, and we\u2019ve reduced the time it takes to launch a new model from days to hours.\n\nModel stability: We defined and operationalized model stability, a pioneering metric that measures the accuracy of our model predictions. We\u2019ve leveraged model stability to produce SLOs for all models in the model registry, which enables simple understanding of the entire product surface\u2019s ML health.\n\nModel registry\n\nWhat did model investigations look like prior to the registry?\n\nBefore we created the model registry, the investigation process was a time-consuming and error-prone experience for on-call engineers and model owners. An on-call engineer had to ask multiple questions to model owners to gather information, as depicted Figure 3 below, about the context of what this model does in the stack and to clarify how important it is to the business.\n\nUnderstanding this context is extremely important to the operational response: Depending on the importance of the model and the criticality of the surface it\u2019s supporting, the response is going to differ in kind. When a model is an experiment serving a small percentage of the traffic, an appropriate response can be to end the experiment and reroute the traffic back to the main model (the baseline). But if there\u2019s a problem with the baseline model that needs to be handled with urgency, it\u2019s not possible to \u201cjust turn it off.\u201d The engineer on call has to loop in the model owner, defeating the purpose of having a dedicated on-call.\n\nTo avoid holding up an operational response on a single POC, we needed a central source of truth for model importance and business function. What if the model is not available? What if 10 of these issues happen concurrently?\n\nWith the development of the model registry, we standardized the collection of model importance and business function information, ensuring most of our operational resources were going towards the most important models.\n\nWhat problems did the model registry solve?\n\nThe model registry is a system of record built on top of Configerator, Meta\u2019s distributed configuration suite . This schematized ledger (see an example in Figure 4 and detailed further below) provides read-and-write access to operational data based on the inventory of production models. It\u2019s a flexible and extensible foundation upon which one can build automation and tools to solve problems that are specific to individual organizations within Meta that are not served by the general tooling.\n\nAs Instagram scaled its investment in AI through rapid innovation in content recommendations, the number of models and AI assets grew; as a result, it has been increasingly important \u2014 but also increasingly difficult \u2014 to maintain a minimum standard for all of our models, as we lacked an authoritative source for the business context as well as for a model\u2019s importance.\n\nIn creating the model registry, we set out to provide a structured interface for collecting business context via model types, importance via criticality, and additional metadata that would enable model understanding. Below, we\u2019ll get into the model types, criticality, and automation we\u2019ve built for this purpose.\n\nModel types\n\nAt a high level, model type describes the purpose for the ML workload where it represents a category or class of models that share a common purpose or are used in similar contexts. For example, we have \u201cig_stories_tray_mtml\u201d which is a string attached to training flows, model checkpoints, inference services, and more. Put simply, a model type identifies for the reader this model\u2019s purpose in the ranking funnel.\n\nLet\u2019s break it down:\n\n\u201cig_stories_tray_mtml\u201d \u2192 \u201cig\u201d \u201cstories\u201d \u201ctray\u201d \u201cmtml\u201d\n\n\u201c ig \u201d: This model is an \u201cig\u201d model as opposed to \u201cfb\u201d or \u201cwhatsapp\u201d.\n\n\u201c stories \u201d: This model serves IG Stories.\n\n\u201c tray \u201d: This model serves in the main IG Stories tray (as opposed to stories in some other surface).\n\n\u201cmtml\u201d: This model is a multi-task-multi-label model, commonly used in late-stage ranking.\n\nWe can then use these model type strings to tag AI assets, and since they serve as proxies for business context, we can use them also for asset management, policy enforcement, analytics, and more.\n\nThe metadata entries in the model registry are anchored on two main types that describe model instances (ModelMetadata) as well as model types (ModelTypeMetadata). These types are made up of \u201ccore\u201d attributes that are universally applicable, as well as \u201cextended\u201d attributes that allow different teams to encode their opinions about how these entries will inform operations. For example, in Instagram our extended attributes encode \u201cbaseline\u201d and \u201choldout\u201d model IDs, which are used in our ranking infrastructure to orchestrate ranking funnel execution.\n\nCriticality\n\nIn addition to defining business function, we had to establish clear guidelines for model importance. Within Meta, SEVs and services have a unified-importance tier system where the Global Service Index (GSI) records a criticality from TIER0 to TIER4 based on the maximum incident severity level the service can cause, from SEV0 as the most critical to SEV4 as simply a \u201cheads up.\u201d Since GSI criticality had social proof at the company, and infra engineers were familiar with this system, we adopted these criticalities for models and now annotate them at the model type and model level.\n\nNo longer would each team decide to raise their own model services to TIER1 for themselves, increasing the burden on all teams that support these models. Teams needed to provide an immediate response (available 24/7) on call and be able to prove that their models contributed meaningfully to critical business metrics to qualify for elevated monitoring.\n\nConfiguration structure as a foundation for automation\n\nOnce we had onboarded a critical mass of Instagram models to the model registry, we could begin to fully integrate with our monitoring and observability suite using our Meta-wide configuration solution, Configerator. With this, we could now have model performance monitoring and alerts that are fully automated and integrated with our tooling for SLIs called SLICK, dashboards that allow us to monitor models across many time series dimensions, and a suite of alerting specific to the model that is driven from the entries in the model registry.\n\nThis provided all our teams confidence that our monitoring coverage was complete and automated.\n\nLaunching\n\nWhile a point-in-time snapshot of models in production is great for static systems, Instagram\u2019s ML landscape is constantly shifting. With the rapid increase of iteration on the recommendation system driving an increased number of launches, it became clear our infrastructure support to make this happen was not adequate. Time-to-launch was a bottleneck in ML velocity, and we needed to drive it down.\n\nWhat did the process look like?\n\nConventionally, services were longstanding systems that had engineers supporting them to tune. Even when new changes would introduce new capacity regression risks, we could gate this behind change safety mechanisms.\n\nHowever, our modeling and experimentation structure was unique in that we were planning for more rapid iteration, and our options were insufficient. To safely test the extent of load a new service could support, we would clone the entire service, send shadow traffic (i.e., cloned traffic that isn\u2019t processed by our clients), and run multiple overload tests until we found a consistent peak throughput. But this wasn\u2019t a perfect science. Sometimes we didn\u2019t send enough traffic, and sometimes we\u2019d send too much, and the amount could change throughout the day due to variations in global user behavior.\n\nThis could easily take two days to get right, including actually debugging the performance itself when the results weren\u2019t expected. Once we got the result, we\u2019d then have to estimate the final cost. Below (in Figure 5) is the formula we landed on.\n\nThe actual traffic shifting portion was tedious as well. For example, when we managed to fully estimate that we needed 500 replicas to host the new service, we might not actually have 500 spares lying around to do a full replacement, so launching was a delicate process of partially sizing up by approximately 20%, sending 20% of traffic over, and then scaling down the old service by 20% to reclaim and recycle the capacity. Rinse, repeat. Inefficient!\n\nAnd by the time we got to the end of this arduous process, the ordeal still wasn\u2019t over. Each team was responsible for correctly setting up new alerts for their baseline in a timely fashion, or else their old models could and did trigger false alarms.\n\nHow does forcing virtual pools aid product growth?\n\nOne of the prerequisites for fixing competition for resources and unblocking productivity was to put up guardrails. Prior to this, it was \u201cfirst come first served,\u201d with no clear way to even \u201creserve\u201d future freed capacity. It was also hard to reason about fairness from an infra perspective: Would it make sense to give each team equal pools, or give each individual person a maximum limit?\n\nAs it turned out, not all MLEs are experimenting at the same time, due to staggered progress on their work, so individual (per-engineer) limits were not ideal. One member might be in the experimentation stage and another might be training. So our solution was to provide bandwidth to each team.\n\nOnce each team \u2014 and therefore product \u2014 had quotas distributed, their launch policy became more clear cut. Some teams established free launching as long as the team was within quota. Others required no regressions in capacity usage. But mostly this unlocked our ability to run launches in parallel, since each one required much less red tape, and prioritization was no longer done at the org level.\n\nWhat other tooling improved launching?\n\nAs mentioned earlier, preplanning with capacity estimations was critical to understanding cost and ensuring reliability. We were often asked, Why not let autoscaling take care of everything? The problem was that each service could be configured slightly differently than a previously optimized service, or some architectural change could have affected the performance of the model. We didn\u2019t have an infinite amount of supply to work with, so by the time we fully traffic-shifted everything over, we might find that we didn\u2019t have enough supply. Reverting is costly, taking hours to get through each stage.\n\nBy doing capacity estimations in advance, this also allowed us and each team to accurately evaluate metric improvement versus cost. It might be worthwhile to double our costs if something would increase time spent on the app by 1%, but likely not for a 0.05% improvement where we could better spend that capacity funding another initiative.\n\nWith partners in AI Infra, we developed two major solutions to this process: offline performance evaluation and an automated launching platform.\n\nWe simplified determining performance of a new service using recorded traffic. Pre-recorded traffic was continuously collected into a data warehouse that the benchmarker could read from, and we\u2019d spin up temporary jobs with this automation. One job would replay different levels of traffic continuously and send it to another job that was a clone of the existing experiment. By putting stoppers on desired latency and error rates, the tooling would eventually output a converged stable number that we could understand as the max load (see Figure 6).\n\nThe launch platform itself would input the numbers we captured from these tests, automatically collect demand data as defined, and run that same formula to calculate a cost. The platform would then perform the upscaling/downscaling cycle for teams as we shifted traffic.\n\nAnd finally, by leveraging the model registry, we were able to land this model change in code (see example in Figure 6), to help us better maintain and understand the 1000+ models within our fleet. Likewise, this bolstered our trust in the model registry, which was now directly tied to the model launch lifecycle.\n\nThis suite of launch automation has dramatically reduced the class of SEVs related to model launches, improved our pace of innovation from a few to more than 10 launches per week, and reduced the amount of time engineers spend conducting a launch by more than two days.\n\nModel stability\n\nAs the number of models in production increased, our organization started to feel the effects of an inconsistent measure of model health. While ranking models are run like any other distributed backend system (receive a request, produce a response), one may think a universal SLO that measures request success rate can suffice to capture holistic health. This is not the case for ranking models, as the accuracy of recommendations received carries significant importance to the end-user experience. If we consider a user who is a huge fan of golf but does not enjoy cooking content (see the \u201cavailable & irrelevant\u201d case in Figure 8 below), we see an example of this inaccuracy in practice. This is precisely what the model stability metric sought to capture.\n\nWhy is measuring ranking model reliability unique?\n\nRanking models, unlike traditional idempotent request/response backends, produce scores predicting user action given a set of candidates (PLIKE, PCOMMENT, PFOLLOW, etc.). These scores then combine and are used to determine which candidates are most relevant to an end user. It\u2019s important that these scores accurately reflect user interest, as their accuracy is directly correlated to user engagement. If we recommend irrelevant content, user engagement suffers. The model stability metric was designed to make it easy to measure this accuracy and detect inaccuracy at our scale.\n\nLet\u2019s discuss how this works.\n\nDefining model stability\n\nModels are complex, and they produce multiple output predictions. Let\u2019s take a simplified example (shown in Figure 9 below) of a multi-task-multi-label (MTML) model predicting three actions:\n\nFor us to claim this model is stable, we must also claim that each underlying prediction is stable.\n\nWhen evaluating the accuracy of a ranking model\u2019s predictions, we typically look at two metrics:\n\nModel calibration , which is based on observed real-world outcomes and answers the question, \u201cAre we over- or under-predicting user action?\u201d It is calculated as a ratio of predicted click-through-rate (CTR) and empirical CTR. A perfect predictor will have calibration centered at 1.\n\nModel normalized entropy (NE), which measures the discriminative power of a predictor, and answers the question, \u201cHow well can this predictor separate action from inaction?\u201d It is calculated as a ratio of the average log-loss per impression to what the average log-loss per impression would be if we always predicted the empirical CTR. With NE, lower values are better, and an NE of 1 is equivalent to random predictions.\n\n(For more information regarding our choice of prediction evaluation metrics, please refer to the paper, \u201cPractical Lessons from Predicting Clicks on Ads at Facebook.\u201d)\n\nA model\u2019s predictions are unstable when either calibration or NE are out of their expected healthy ranges. To determine what a healthy range is, we must look at each metric in real time, and Figure 10 below shows what these time series can look like:\n\nBy observing the trend of a healthy prediction, we can apply thresholds for our evaluation metrics. When these thresholds are breached, the underlying prediction is considered unstable.\n\nFrom here, we can define model stability as a binary indicator across a model\u2019s predictions. It is 1 if all underlying predictions are stable, and 0 if any prediction is unstable. This is an extremely powerful method of reacting to real-time prediction instability as well as a tool for understanding trends in predictive health per model or across distinct products ranking funnels.\n\nOperationalizing model stability\n\nWith a real-time view on model predictive health, we can leverage this unified definition of model stability and apply it to all of our models in production, once again leveraging the model registry as a ledger to hold this important data. In Figure 11 below, we can see the addition of model stability metric metadata after we determined the expected thresholds.\n\nGiven the large number of models in production, each producing many predictions, building a portable definition of model health applicable to all of our ranking models represented an important milestone toward upleveling Instagram\u2019s ML infrastructure maturity. This has unlocked our ability to build generic alerting to guarantee detection of our most important models becoming unstable, thereby moving us closer to mitigation when our recommendation system is at risk.\n\nSince the addition of these metrics and alerting, ML teams have discovered previously hidden issues within their models and addressed them faster than before, leading to higher-quality recommendations.\n\nKey takeaways\n\nIn our journey to scale Instagram\u2019s algorithm to manage over 1000 models, we have learned several critical lessons that have shaped our approach and infrastructure. These takeaways not only highlight the challenges we faced but also underscore the strategies that led to our success.\n\nInfra understanding is the foundation to building the right tools\n\nA unified understanding of our infrastructure footprint was essential in developing the right tools to support our scaling efforts. By identifying the gaps and potential risks in our existing systems, we were able to implement solutions such as the model registry that significantly improved our operational efficiency and reliability posture.\n\nHelping colleagues move fast means we all move faster\n\nBy addressing the model iteration bottleneck, we enabled our teams to innovate more rapidly. Our focus on creating a seamless, self-service process for model iteration empowered client teams to take ownership of their workflows. This not only accelerated their progress but also reduced the operational burden on our infrastructure team. As a result, the entire organization benefited from increased agility and productivity.\n\nReliability must consider quality\n\nEnsuring the reliability of our models required us to redefine how we measure and maintain model quality. By operationalizing model stability and establishing clear metrics for model health, we were able to proactively manage the performance of our models. This approach enables us to maintain high standards of quality across our recommendation systems, ultimately enhancing user engagement and satisfaction.\n\nOur experience in scaling Instagram\u2019s recommendation system has reinforced the importance of infrastructure understanding, collaboration, and a focus on quality. By building robust tools and processes, we have not only improved our own operations but also empowered our colleagues to drive innovation and growth across the platform.", "label": 0}
{"title": "Noisy Neighbor Detection with eBPF", "url": "https://netflixtechblog.com/noisy-neighbor-detection-with-ebpf-64b1f4b3bbdd?source=collection_home---4------16-----------------------", "content": "The sched_wakeup and sched_wakeup_new hooks are invoked when a process changes state from 'sleeping' to 'runnable.' They let us identify when a process is ready to run and is waiting for CPU time. During this event, we generate a timestamp and store it in an eBPF hash map using the process ID as the key.\n\nstruct {\n\n__uint(type, BPF_MAP_TYPE_HASH);\n\n__uint(max_entries, MAX_TASK_ENTRIES);\n\n__uint(key_size, sizeof(u32));\n\n__uint(value_size, sizeof(u64));\n\n} runq_enqueued SEC(\".maps\");\n\n\n\nSEC(\"tp_btf/sched_wakeup\")\n\nint tp_sched_wakeup(u64 *ctx)\n\n{\n\nstruct task_struct *task = (void *)ctx[0];\n\nu32 pid = task->pid;\n\nu64 ts = bpf_ktime_get_ns();\n\n\n\nbpf_map_update_elem(&runq_enqueued, &pid, &ts, BPF_NOEXIST);\n\nreturn 0;\n\n}\n\nConversely, the sched_switch hook is triggered when the CPU switches between processes. This hook provides pointers to the process currently utilizing the CPU and the process about to take over. We use the upcoming task's process ID (PID) to fetch the timestamp from the eBPF map. This timestamp represents when the process entered the queue, which we had previously stored. We then calculate the run queue latency by simply subtracting the timestamps.\n\nSEC(\"tp_btf/sched_switch\")\n\nint tp_sched_switch(u64 *ctx)\n\n{\n\nstruct task_struct *prev = (struct task_struct *)ctx[1];\n\nstruct task_struct *next = (struct task_struct *)ctx[2];\n\nu32 prev_pid = prev->pid;\n\nu32 next_pid = next->pid;\n\n\n\n// fetch timestamp of when the next task was enqueued\n\nu64 *tsp = bpf_map_lookup_elem(&runq_enqueued, &next_pid);\n\nif (tsp == NULL) {\n\nreturn 0; // missed enqueue\n\n}\n\n\n\n// calculate runq latency before deleting the stored timestamp\n\nu64 now = bpf_ktime_get_ns();\n\nu64 runq_lat = now - *tsp;\n\n\n\n// delete pid from enqueued map\n\nbpf_map_delete_elem(&runq_enqueued, &next_pid);\n\n....\n\nOne of the advantages of eBPF is its ability to provide pointers to the actual kernel data structures representing processes or threads, also known as tasks in kernel terminology. This feature enables access to a wealth of information stored about a process. We required the process's cgroup ID to associate it with a container for our specific use case. However, the cgroup information in the process struct is safeguarded by an RCU (Read Copy Update) lock.\n\nTo safely access this RCU-protected information, we can leverage kfuncs in eBPF. kfuncs are kernel functions that can be called from eBPF programs. There are kfuncs available to lock and unlock RCU read-side critical sections. These functions ensure that our eBPF program remains safe and efficient while retrieving the cgroup ID from the task struct.\n\nvoid bpf_rcu_read_lock(void) __ksym;\n\nvoid bpf_rcu_read_unlock(void) __ksym;\n\n\n\nu64 get_task_cgroup_id(struct task_struct *task)\n\n{\n\nstruct css_set *cgroups;\n\nu64 cgroup_id;\n\nbpf_rcu_read_lock();\n\ncgroups = task->cgroups;\n\ncgroup_id = cgroups->dfl_cgrp->kn->id;\n\nbpf_rcu_read_unlock();\n\nreturn cgroup_id;\n\n}\n\nOnce the data is ready, we must package it and send it to userspace. For this purpose, we chose the eBPF ring buffer. It is efficient, high-performing, and user-friendly. It can handle variable-length data records and allows data reading without necessitating extra memory copying or syscalls. However, the sheer number of data points was causing the userspace program to use too much CPU, so we implemented a rate limiter in eBPF to sample the data.\n\nstruct {\n\n__uint(type, BPF_MAP_TYPE_RINGBUF);\n\n__uint(max_entries, RINGBUF_SIZE_BYTES);\n\n} events SEC(\".maps\");\n\n\n\nstruct {\n\n__uint(type, BPF_MAP_TYPE_PERCPU_HASH);\n\n__uint(max_entries, MAX_TASK_ENTRIES);\n\n__uint(key_size, sizeof(u64));\n\n__uint(value_size, sizeof(u64));\n\n} cgroup_id_to_last_event_ts SEC(\".maps\");\n\n\n\nstruct runq_event {\n\nu64 prev_cgroup_id;\n\nu64 cgroup_id;\n\nu64 runq_lat;\n\nu64 ts;\n\n};\n\n\n\nSEC(\"tp_btf/sched_switch\")\n\nint tp_sched_switch(u64 *ctx)\n\n{\n\n// ....\n\n// The previous code\n\n// ....\n\n\n\nu64 prev_cgroup_id = get_task_cgroup_id(prev);\n\nu64 cgroup_id = get_task_cgroup_id(next);\n\n\n\n// per-cgroup-id-per-CPU rate-limiting\n\n// to balance observability with performance overhead\n\nu64 *last_ts =\n\nbpf_map_lookup_elem(&cgroup_id_to_last_event_ts, &cgroup_id);\n\nu64 last_ts_val = last_ts == NULL ? 0 : *last_ts;\n\n\n\n// check the rate limit for the cgroup_id in consideration\n\n// before doing more work\n\nif (now - last_ts_val < RATE_LIMIT_NS) {\n\n// Rate limit exceeded, drop the event\n\nreturn 0;\n\n}\n\n\n\nstruct runq_event *event;\n\nevent = bpf_ringbuf_reserve(&events, sizeof(*event), 0);\n\n\n\nif (event) {\n\nevent->prev_cgroup_id = prev_cgroup_id;\n\nevent->cgroup_id = cgroup_id;\n\nevent->runq_lat = runq_lat;\n\nevent->ts = now;\n\nbpf_ringbuf_submit(event, 0);\n\n// Update the last event timestamp for the current cgroup_id\n\nbpf_map_update_elem(&cgroup_id_to_last_event_ts, &cgroup_id,\n\n&now, BPF_ANY);\n\n\n\n}\n\n\n\nreturn 0;\n\n}\n\nOur userspace application, developed in Go, processes events from the ring buffer to emit metrics to our metrics backend, Atlas. Each event includes a run queue latency sample with a cgroup ID, which we associate with containers running on the host. We categorize it as a system service if no such association is found. When a cgroup ID is associated with a container, we emit a percentile timer Atlas metric ( runq.latency ) for that container. We also increment a counter metric ( sched.switch.out ) to monitor preemptions occurring for the container's processes. Access to the prev_cgroup_id of the preempted process allows us to tag the metric with the cause of the preemption, whether it's due to a process within the same container (or cgroup), a process in another container, or a system service.\n\nIt's important to highlight that both the runq.latency metric and the sched.switch.out metrics are needed to determine if a container is affected by noisy neighbors, which is the goal we aim to achieve \u2014 relying solely on the runq.latency metric can lead to misconceptions. For example, if a container is at or over its cgroup CPU limit, the scheduler will throttle it, resulting in an apparent spike in run queue latency due to delays in the queue. If we were only to consider this metric, we might incorrectly attribute the performance degradation to noisy neighbors when it's actually because the container is hitting its CPU quota. However, simultaneous spikes in both metrics, mainly when the cause is a different container or system process, clearly indicate a noisy neighbor issue.\n\nA Noisy Neighbor Story", "label": 0}
{"title": "On-device small language models with multimodality, RAG, and Function Calling", "url": "https://developers.googleblog.com/en/google-ai-edge-small-language-models-multimodality-rag-function-calling/", "content": "Last year Google AI Edge introduced support for on-device small language models (SLMs) with four initial models on Android, iOS, and Web. Today, we are excited to expand support to over a dozen models including the new Gemma 3 and Gemma 3n models, hosted on our new LiteRT Hugging Face community. Gemma 3n, available via Google AI Edge as an early preview, is Gemma\u2019s first multimodal on-device small language model supporting text, image, video, and audio inputs. Paired with our new Retrieval Augmented Generation (RAG) and Function Calling libraries, you have everything you need to prototype and build transformative AI features fully on the edge.\n\nSorry, your browser doesn't support playback for this video Let users control apps with on-device SLMs and our new function calling library\n\nBroader model support You can find our growing list of models to choose from in the LiteRT Hugging Face Community. Download any of these models and easily run them on-device with just a few lines of code. The models are fully optimized and converted for mobile and web. Full instructions on how to run these models can be found in our documentation and on each model card on Hugging Face. To customize any of these models, you finetune the base model and then convert and quantize the model using the appropriate AI Edge libraries. We have a Colab showing every step you need to fine-tune and then convert Gemma 3 1B. With the latest release of our quantization tools, we have new quantization schemes that allow for much higher quality int4 post training quantization. Compared to bf16, the default data type for many models, int4 quantization can reduce the size of language models by a factor of 2.5-4X while significantly decreasing latency and peak memory consumption.\n\nGemma 3 1B & Gemma 3n Earlier this year, we introduced Gemma 3 1B. At only 529MB, this model can run up to 2,585 tokens per second pre-fill on the mobile GPU, allowing it to process up to a page of content in under a second. Gemma 3 1B\u2019s small footprint allows it to support a wide range of devices and limits the size of files an end user would need to download in their application. Today, we are thrilled to add an early preview of Gemma 3n to our collection of supported models. The 2B and 4B parameter variants will both support native text, image, video, and audio inputs. The text and image modalities are available on Hugging Face with audio to follow shortly.\n\nSorry, your browser doesn't support playback for this video Gemma 3n analyzing images fully on-device", "label": 0}
{"title": "Celebrating Flutter\u2019s \u201cProduction Era\u201d", "url": "https://developers.googleblog.com/en/celebrating-flutters-production-era/", "content": "This article is cross posted on Flutter\n\nJust over six years ago, we unveiled Flutter 1.0. Today, at #FlutterInProduction, we\u2019re celebrating how far we\u2019ve come \u2014 from the immense support we\u2019ve received from thousands of contributors in the community, to the widespread adoption of Flutter as a production-grade app framework for building multi-platform app experiences. If you haven\u2019t experienced Flutter yet, we invite you to try it! As we shared today, you\u2019d be joining a big group: Flutter has over 1 million monthly active developers across the globe, and powers nearly 30% of all new iOS apps. More than 90 thousand developers actively participate in Flutter Meetups across more than sixty countries. And if you want input on designing or building a new successful Flutter app, we have a large and growing list of Flutter Consultants ready to help you. \u201cApptopia tracks millions of apps in the Apple AppStore and Google Play Store, and analyzes and detects which developer SDKs were used to create the apps. Flutter is one of the most popular SDKs we track: In the Apple AppStore it has grown steadily in usage from around 10% of all tracked free apps in 2021 to nearly 30% of all tracked free apps in 2024!\u201d\n\n\u2014 Apptopia Inc. A decade of innovation to reach the production era It\u2019s been an incredible journey, starting in 2014 (in what we now call our experimental era) as a Google experiment codenamed \u201cSky.\u201d Before Flutter, compromises were inevitable. Many developers have become skeptical that any framework can truly deliver a premium experience across multiple platforms. With the launch of Flutter 1.0 in 2018 we had a clear mission to resolve that technology dilemma: We aimed to empower developers with the ultimate app framework for crafting beautiful, high-performance user interfaces across all platforms. Also, to enable developers to reach all customers with high-quality apps on all the platforms that customers care about, but with lower cost and in less time. Our focus has remained constant through Flutter\u2019s growth era, even as we\u2019ve added support for the six major platforms across mobile, web, and desktop \u2014 and continue to push beyond, with work like Toyota\u2019s use of Flutter for infotainment systems.\n\nWe\u2019re now in the \u201cproduction era,\u201d and we\u2019re celebrating that with #FlutterInProduction! This event spotlights the achievements of developers using Flutter in real-world applications.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nBuilding in partnership with the community None of this would be possible without our amazing community of over 1,400 contributors, more than 10,000 package publishers authoring over 50,000 packages, and passionate Flutter content creators and enthusiasts. Together, we\u2019ve built a top-5 GitHub open-source project by contributions!\n\nAmazing user experiences It all starts with a focus on enabling amazing user experiences. Free from typical platform constraints, Flutter supports a broad set of design languages \u2014 support for Material Design and our Apple-inspired Cupertino widgets comes with the SDK. The ecosystem also provides a broad selection of design libraries like Windows-inspired fluent_ui , macOS-inspired macos_ui , and the Ubuntu-inspired yaru widgets.\n\nScandinavian Airlines design awards With Flutter, you have the flexibility and power to realize any design your design team envisions. This is exemplified by Scandinavian Airlines, who after creating their new mobile app with Flutter have filled their trophy case with prestigious design awards such as the Red Dot Design Award, the Webby People\u2019s Voice Award, and the iF Design Gold Award. Charlotte Svensson, EVP & CIO at SAS explains: \u201cI\u2019m extremely proud over this award, which is not just an industry award, but a global recognition. It\u2019s a testament to what we can do, when we go above-and-beyond in focusing on improving the customer experience, and when we interact and develop together with our customers. SAS has always been at the forefront of innovation in the aviation industry, and this award serves as a validation of its dedication to providing exceptional digital solutions for our customers.\u201d\n\nGreat performance & reliability Performance and reliability are crucial for a positive user experience and brand perception. Slow or crash-prone apps not only frustrate users in the short term but can also damage your brand reputation in the long run through negative reviews and word-of-mouth. Flutter has prioritized performance and reliability from the outset. By choosing the Dart programming language, we ensure fast startup times through ahead-of-time compilation to native machine code or web assembly. Dart\u2019s rich, null-safety type system helps catch errors during development, further enhancing reliability. Additionally, Flutter\u2019s custom Impeller rendering engine, designed specifically for multi-platform UI, delivers smooth animations and gives us full control over the rendering stack, top to bottom, from the UI source code to the GPU.\n\nUniversal Studios performance and reliability For example, Universal Destinations and Experiences recently reported that by adopting Flutter, they not only decreased their app size \u2014 a significant benefit for users with unreliable internet connections \u2014 but also dramatically reduced app crashes to near zero, thus lowering their total cost of ownership.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nLG Electronics performance LG Electronics has traditionally relied on web apps for their webOS-powered smart TVs due to concerns about the high development cost of traditional native apps. However, they found that web apps launch slower and consume more memory than native apps. With Flutter, LG Electronics has a solution that combines fast development speed and excellent performance. As a result, they plan to use Flutter for key applications in webOS TVs globally starting in 2025.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nFirst-class developer experience and thriving ecosystem Flutter\u2019s success is deeply rooted in its focus on developer experience. We pioneered instant developer workflows with Stateful Hot Reload, and during our growth era added Flutter DevTools to significantly accelerate diagnostics and debugging workflows. Flutter\u2019s community provides a thriving and open ecosystem of over 50,000 packages published by over 10,000 publishers, combined with robust third-party services & technologies. Also, if you want input on designing or building a new successful Flutter app, we have a large list of Flutter Consultants ready to help you.\n\nMGM and developer productivity App agency Superformula has built with Flutter since August 2020. They found that Flutter is easy to learn and well documented, enabling them to get new team members up to speed quickly and contribute effectively. Superformula also used Flutter to revitalize the digital dining experience for MGM Resorts\u2019 400+ restaurants. The new Flutter-based MGM Rewards app was rebuilt in just 4 months, cutting the total amount of code in half, and improving delivery speed by a factor of 4. One core enabler of productivity for Superformula is the ability to share code across mobile, tablet-based kiosks, and web-based tools.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nGEICO user interface elements shared across web, iOS, and Android.", "label": 0}
{"title": "Design system annotations, part 2: Advanced methods of annotating components", "url": "https://github.blog/engineering/user-experience/design-system-annotations-part-2-advanced-methods-of-annotating-components/", "content": "In part one of our design system annotation series, we discussed the ways in which accessibility can get left out of design system components from one instance to another. Our solution? Using a set of \u201cPreset annotations\u201d for each component with Primer. This allows designers to include specific pre-set details that aren\u2019t already built into the component and visually communicated in the design itself.\n\nThat being said, Preset annotations are unique to each design system \u2014 and while ours may be a helpful reference for how to build them \u2014 they\u2019re not something other organizations can utilize if you\u2019re not also using the Primer design system.\n\nLuckily, you can build your own. Here\u2019s how.\n\nHow to make Preset annotations for your design system\n\nStart by assessing components to understand which ones would need Preset annotations\u2014not all of them will. Prioritize components that would benefit most from having a Preset annotation, and build that key information into each one. Next, determine what properties should be included. Only include key information that isn\u2019t conveyed visually, isn\u2019t in the component properties, and isn\u2019t already baked into a coded component.\n\nPrioritizing components\n\nWhen a design system has 60+ components, knowing where to start can be a challenge. Which components need these annotations the most? Which ones would have the highest impact for both design teams and our users?\n\nWhen we set out to create a new set of Preset annotations based on our proof of concept, we decided to use ten Primer components that would benefit the most. To help pick them, we used an internal tool called Primer Query that tracks all component implementations across the GitHub codebase as well as any audit issues connected to them. Here is a video breakdown of how it works, if you\u2019re curious.\n\nWe then prioritized new Preset annotations based on the following criteria:\n\nComponents that align to organization priorities (i.e. high value products and/or those that receive a lot of traffic). Components that appear frequently in accessibility audit issues. Components with React implementations (as our preferred development framework). Most frequently implemented components.\n\nMapping out the properties\n\nFor each component, we cross-referenced multiple sources to figure out what component properties and attributes would need to be added in each Preset annotation. The things we were looking for may only exist in one or two of those places, and thus are less likely to be accounted for all the way through the design and development lifecycle. The sources include:\n\nComponent documentation on Primer.style\n\nDesign system docs should contain usage guidance for designers and developers, and accessibility requirements should be a part of this guidance as well. Some of the guidance and requirements get built into the component\u2019s Figma asset, while some only end up in the coded component.\n\nLook for any accessibility requirements that are not built into either Figma or code. If it\u2019s built in, putting the same info in the Preset annotation may be redundant or irrelevant.\n\nPresets can account for rare use cases While building a Preset annotation for the TextInput component, we found that implementations may use an icon alone or have a hidden input label. With GitHub\u2019s global search or filter inputs, the magnifying glass icon alone can act as the visible label, but the fields still need an accessible label for assistive technology users.\n\nCoded demos in Storybook\n\nOur component sandbox helped us see how each component is built in React or Rails, as well as what the HTML output is. We looked for any code structure or accessibility attributes that are not included in the component documentation or the Figma asset itself\u2014especially when they may vary from one implementation to another.\n\nCode attributes a designer may not see or set Storybook helped us craft our TextInput component\u2019s Preset annotation by showing some important attributes that don\u2019t get any mention elsewhere. The type attribute is to the value of text . by default. Depending on the purpose of the field, an input\u2019s type could also be search , email , number , tel , date , or time . This should be set intentionally so that users are able to use the most appropriate virtual keyboard.\n\nComponent properties in the Figma asset library\n\nLibrary assets provide a lot of flexibility through text layers, image fills, variants, and elaborate sets of component properties. We paid close attention to these options to understand what designers can and can\u2019t change. Worthwhile additions to a Preset Annotation are accessibility attributes, requirements, and usage guidance in other sources that aren\u2019t built into the Figma component.\n\nWhat\u2019s missing from the TextInput\u2019s Figma component When a TextInput is added to a design, the Figma component comes with many customizable options. There is an inputTextType property, which is about visual design and typography, not the type of form input. It\u2019s possible to set the value of the Label and input field in Figma\u2019s sidebar, but because it\u2019s hidden by default, there\u2019s no option to set the text of an error validation message. We can\u2019t assume that every design delivered in Figma will come with examples of a form showing all of its error states, so these error messages may not get the attention they require. If this message can\u2019t be built into the component as a text property, it can be added to the Preset annotation.\n\nOther potential sources\n\nExperiences from team members: The designers, developers, and accessibility specialists you work with may have insight into things that the docs and design tools may have missed. If your team and design system have been around for a while, their insights may be more valuable than those you\u2019ll find in the docs, component demos, or asset libraries. Take some time to ask which components have had challenging bugs and which get intentionally broken when implemented.\n\nThe designers, developers, and accessibility specialists you work with may have insight into things that the docs and design tools may have missed. If your team and design system have been around for a while, their insights may be more valuable than those you\u2019ll find in the docs, component demos, or asset libraries. Take some time to ask which components have had challenging bugs and which get intentionally broken when implemented. Findings from recent audits: Design system components themselves may have unresolved audit issues and remediation recommendations. If that\u2019s the case, those issues are likely present in Storybook demos and may be unaccounted for in the component documentation. Design system audit issues may have details that both help create a Preset annotation and offer insights about what should not be carried over from existing resources.\n\nPutting it all together Our new Preset annotation for the TextInput component included links to usage guidance and Storybook as well as an optional tutorial for how the component is best used in a design to avoid potential issues. There are two mandatory prompts for input type and error text, and an optional one for the occasional hidden form label.\n\nWhat we learned from creating Preset annotations\n\nPreset annotations may not be for every team or organization. However, they are especially well suited for younger design systems and those that aren\u2019t well adopted.\n\nMature design systems like Primer have frequent updates. This means that without close monitoring, the design system components themselves may fall out of sync with how a Preset annotation is built. This can end up causing confusion and rework after development starts, so it may be wise to make sure there\u2019s some capacity to maintain these annotations after they\u2019ve been created.\n\nFor newer teams at GitHub, new members of existing teams, and team members who were less familiar with the design system, the built-in guidance and links to documentation and component demos proved very useful. Those who are more experienced are also able to fine-tune the Presets and how they\u2019re used.\n\nIf you don\u2019t already have extensive experience with the design system components (or peers to help build them), it can take a lot of time to assess and map out the properties needed to build a Preset. It can also be challenging to name a component property succinctly enough that it doesn\u2019t get truncated in Figma\u2019s properties panel. If the context is not self-evident, some training or additional documentation may help.\n\nIt\u2019s not always clear that you need a Preset annotation\n\nThere may be enough overlap between the Preset annotation for a component and types of annotations that aren\u2019t specific to the design system.\n\nFor example, the GitHub Annotation Toolkit has components to annotate basic <textarea> form elements in addition to a Preset annotation for our <TextArea> Primer component:\n\nIn many instances, this flexibility may be confusing because you could use either annotation. For example, the Primer <TextArea> Preset has built-in links to specific Primer docs, and while the non-Preset version doesn\u2019t, you could always add the links manually. While there\u2019s some overlap between the two, using either one is better than none.\n\nOne way around this confusion is to add Primer-specific properties to the default set of annotations. This would allow you to do things like toggle a boolean property on a normal Button annotation and have it show links and properties specific to your design system\u2019s button component.\n\nOur Preset creation process may unlock automation\n\nThere are currently a number of existing Figma plugins that advertise the ability to scan a design file to help with annotations. That being said, the results are often mixed and contain an unmanageable amount of noise and false positives. One of the reasons these issues happen is that these public plugins are design system agnostic.\n\nCurrent automated annotation tools aren\u2019t able to understand that any design system components are being used without bespoke programming or thorough training of AI models. For plugins like this to be able to label design elements accurately, they first need to understand how to identify the components on the canvas, the variants used, and the set properties.\n\nWith that in mind, perhaps the most exciting insight is that the process of mapping out component properties for a Preset annotation\u2014the things that don\u2019t get conveyed in the visual design or in the code\u2014is also something that would need to be done in any attempt to automate more usable annotations.\n\nIn other words, if a team uses a design system and wants to automate adding annotations, the tool they use would need to understand their components. In order for it to understand their components well enough to automate accurately, these hidden component properties would need to be mapped out. The task of creating a set of Preset annotations may be a vital stepping stone to something even more streamlined.\n\nA promising new method: Figma\u2019s Code Connect\n\nWhile building our new set of Preset annotations, we experimented with other ways to enhance Primer with annotations. Though not all of those experiments worked out, one of them did: adding accessibility attributes through Code Connect.\n\nPrimer was one of the early adopters of Figma\u2019s new Code Connect feature in Dev Mode. Says Lukas Oppermann, our staff systems designer, \u201cWith Code Connect, we can actually move the design and the code a little bit further apart again. We can concentrate on creating the best UX for the designers working in Figma with design libraries and, on the code side, we can have the best developer experience.\u201d\n\nTo that end, Code Connect allows us to bypass much of our Preset annotations, as well as the downsides of some of our other experiments. It does this by adding key accessibility details directly into the code that developers can export from Figma.\n\nGitHub\u2019s Octicons are used in many of our Primer components. They are decorative by default, but they sometimes need alt text or aria-label attributes depending on how they\u2019re used. In the IconButton component, that button uses an Octicon and needs an accessible name to describe its function.\n\nWhen using a basic annotation kit, this may mean adding stamps for a Button and Decorative Image as well as a note in the margins that specifies what the aria-label should be. When using Preset annotations, there are fewer things to add to the canvas and the annotation process takes less time.\n\nWith Code Connect set up, Lukas added a hidden layer in the IconButton Figma component. It has a text property for aria-label which lets designers add the value directly from the component properties panel. No annotations needed. The hidden layer doesn\u2019t disrupt any of the visuals, and the aria-label property gets exported directly with the rest of the component\u2019s code.\n\nIt takes time to set up Code Connect with each of your design system components. Here are a few tips to help:\n\nConsistency is key. Make sure that the properties you create and how you place hidden layers is consistent across components. This helps set clear expectations so your teams can understand how these hidden layers and properties function.\n\nMake sure that the properties you create and how you place hidden layers is consistent across components. This helps set clear expectations so your teams can understand how these hidden layers and properties function. Use a branch of your design system library to experiment. Hiding attributes like aria-label is quite simple compared to other complex information that Preset annotations are capable of handling.\n\nHiding attributes like aria-label is quite simple compared to other complex information that Preset annotations are capable of handling. Use visual regression testing (VRT). Adding complexity directly to a component comes with increased risk of things breaking in the future, especially for those with many variants. Figma\u2019s merge conflict UI is helpful, but may not catch everything.\n\nAs we continue to innovate with annotations and make our components more accessible, we are aiming to release our GitHub Annotation Toolkit in the near future. Stay tuned!\n\nFurther reading\n\nAccessibility annotation kits are a great resource, provided they\u2019re used responsibly. Eric Bailey, one of the contributors to our forthcoming GitHub Annotation Toolkit, has written extensively about how annotations can highlight and amplify deeply structural issues when you\u2019re building digital products.\n\nTags:", "label": 0}
{"title": "New Google Pay features to enhance your payment flows", "url": "https://developers.googleblog.com/en/new-google-pay-features-to-enhance-your-payment-flows/", "content": "At Google I/O 2025, we unveiled updates across the Google Pay API designed to help you create smoother, safer, and more successful checkout experiences for your users. Whether you're looking to boost conversions, enable new payment scenarios, enhance security, or simplify your integration, there's something new for you. Let's dive into the key announcements developers need to know.\n\nEnhancing the checkout experience and conversion Google Pay in Android WebViews Big news! Starting with Chrome v137, users can seamlessly use Google Pay within Android WebViews, accessing an Android native experience and device tokens from their Google Wallet. Simply enable PaymentRequest in your app manifest, and tap into the opportunities of in-app browser purchases with a high-quality, secure form of payment. Take a look at the integration guide to learn more.\n\nSorry, your browser doesn't support playback for this video Figure 1: A sample checkout flow via a WebView on Android that uses Google Pay to complete the payment\n\nA more versatile API to power modern checkout flows We are introducing improvements to the Google Pay API to help you adapt to a payment ecosystem that is in continuous evolution. Here are some of our favorite updates: The Google Pay payment sheet now features richer card art and names, helping users select their preferred card faster. The payment sheet also supports dark mode for a more integrated feel within your application.\n\nFigure 2: Screenshots showcasing the Google Pay payment sheet in their dark and light versions.\n\nBuilding on last year's success, the createButton API for Web now offers more customization options (show/hide border, more button text options) to better match your UI, and continue to help boost sales by showing card details upfront. Need to show card-identifying information without using a payment button? We're introducing a new API in the coming months to enable this use case.\n\nfigure 3: An example offering Google Pay using a list selector through the Payment Metadata API\n\nWe're adding support for Merchant-Initiated Transactions (MITs) (subscriptions, auto-reloads, deferred charges) to the Google Pay Online API. This includes details in the payment sheet to inform users, device-independent tokens for payment continuity (even if users change devices), and lifecycle notifications for underlying card changes.\n\nStreamlining the developer experience We are dedicated to making the Google Pay API easier to integrate, test and maintain. Here are some updates that improve the integration experience: Testing just got easier. We have improved the test card suite, so you can now see relevant test cards (regular, tokenized, debit) for your specific PSP directly in the payments sheet when using the TEST environment. Debug your integrations faster on Android with more fine-grained build time error logs to amend your logic more easily, and detailed exceptions/error codes at runtime. Check out the troubleshooting guide if you are seeing errors in your integration.\n\n\n\nFigure 4: More detailed error messages are now surfaced via the Logcat and the debugger\n\nWe've launched new codelabs, Firebase Studio templates (one-click development environments), and a learning pathway for web developers. We are planning to add similar resources for native Android, Flutter, React JS, React Native, and Angular. Stay informed about the status of the Google Pay API with the new Google Pay API Status Dashboard. The dashboard monitors key APIs like the CreateButton, IsReadyToPay, or LoadPaymentData APIs in real-time. Check the availability of the API (99.99% uptime last year!) and get incident updates instantly.\n\nFigure 5: The Google Pay API Status Dashboard includes service uptime and health information.", "label": 0}
{"title": "Taking the plunge: The engineering journey of building a subsea cable", "url": "https://engineering.fb.com/2025/05/01/connectivity/taking-the-plunge-the-engineering-journey-of-building-a-subsea-cable/", "content": "Meta develops infrastructure all across the globe to transport information and content for the billions of people using our services around the world. At the core of this infrastructure are aggregation points \u2013 like data centers \u2013 and the digital cables that connect them. Subsea cables \u2013 the unseen digital highways of the internet \u2013 are critical for Meta to serve people wherever they are in the world. In fact, more than 95% of the world\u2019s intercontinental traffic goes through subsea cables.\n\nMeta\u2019s engineering team prioritizes both innovation and quality when designing and deploying these cables. In the latest Meta Tech Podcast, Andy Palmer-Felgate and Pascal Pecci, both subsea cable systems engineers, join Pascal Hartig on the Meta Tech podcast to discuss the latest in subsea engineering technology. This episode dives deeper into the engineering nuances of large-scale subsea cable projects like the recently announced Project Waterworth.\n\nLearn more about Meta\u2019s work on these engineering feats. Download or listen to the episode below:\n\nThe Meta Tech Podcast is a podcast, brought to you by Meta, where we highlight the work Meta\u2019s engineers are doing at every level \u2013 from low-level frameworks to end-user features.\n\nSend us feedback on Instagram, Threads, or X.\n\n\n\nAnd if you\u2019re interested in learning more about career opportunities at Meta, visit the Meta Careers page.", "label": 0}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2016-10", "content": "en\n\n\"Over the years Greek life ( or more commonly known now as Fraternity & Sorority Life), has been in the news spotlight. Unfortunately, its mostly negative news. There is truly a good side to Greek Life. I mean I was once part of the system. Then again once a Greek always Greek, right? Let me tell you my story and experiences in Greek Life.\n\nI am Pablo and I affiliate myself with the Alpha Tau Omega Fraternity (ATO or AT\u03a9.) I am proud to say that I joined the ATOs, even if it was short lived. I am sad that my fraternity is no longer on campus. Our nonexistence is not due to being under terrible spotlight like many fraternites or sororities do.\n\nDuring my active time in Greek life, I gained so much! Sure we had small numbers but I would not trade it for the world. I met friends that still to this day I talk with today! I met so many great people in the the Greek community. I am truly a social person, so I am not afraid to come meet new people.\n\nCollege is all about opening your up doors to different perspectives and finding yourself.\n\nThere you have it, a side of me that does not come up much anymore! When I do talk about it, I am always proud to look back at something I was proud to be in! I may not be active anymore, but I am proud to say that I, Pablo Morales, am Greek!\"", "label": 1}
{"title": "Midlife rage \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/01/midlife-rage/", "content": "I\u2019m turning 40 later this year, and I\u2019ve been thinking about the reality that I\u2019m approaching \u201chalfway\u201d through my statistically projected lifespan. But I\u2019m not having a crisis about my personal life choices\u2026 instead I\u2019m having a crisis of generational impotence. I\u2019m tired of waiting for my generation\u2019s turn, which I am less and less convinced will come in time to make a difference, especially since GenZ seems to be going conservative \ud83d\ude11\n\nOuter South by Conor Oberst and the Mystic Valley Band\n\nI finally got letters back from two of my reps:\n\nSenator Murray, 74yo and in office since 1993 when I was in third grade, wants me to know that she \u201cremain[s] committed to continuing to work with [her] colleagues on both sides of the aisle to find ways to make progress on immigration reform.\u201d Thank you for your complacency Senator. I don\u2019t know how immigration progress will be made with isolationist Nazis tbh \ud83e\uddd0 I don\u2019t get why Dems are taking conservative immigration stances when in my mind the problem is we are too strict on immigration, and should let in more people. There, birth rate \u201ccrisis\u201d solved too! What I want to know now is, what are the Senator\u2019s plans for retirement?\n\nRepresentative DelBene\u2019s office thinks I \u201cmay be pleased to know that [she] joined [her] Democratic colleagues in sending a letter to President Trump expressing serious concern about Mr. Abrego Garcia\u2019s case and requesting an immediate update on his wellbeing and how the administration plans to ensure his safe return to the United States.\u201d I am, in fact, not pleased. I am the opposite of pleased. Is she fucking with me? Does she not go on the Internet? In what world am I satisfied that she signed a strongly worded letter and called it good?\n\nMy resentment towards 80yo politicians who won\u2019t retire and pass their power to the next generation is reaching a rolling boil as the Democrats in charge apparently don\u2019t have an ounce of urgency left in their moldering bodies. I have to think it\u2019s intentional, especially given the way Nancy and co blocked Millennial AOC from a position of power in favor of an old dude with cancer who just died from it (\u201cGerry\u2019s a young 74, cancer notwithstanding\u201d \ud83d\ude44\ud83d\ude44\ud83d\ude44); I think Boomer politicians know how badly they\u2019ve fucked over everyone younger than them, and they will literally hold power until they die to prevent facing justice. (Yeah I\u2019m making it generational, they\u2019ve been demonizing Millennials to avoid looking in the mirror for decades.) They booted 25yo David Hogg from DNC leadership as punishment for suggesting the party should not always support incumbents, making a mockery of DEI principles by using gender equality rules to invalidate his election.\n\nA friend and I just looked it up: Nancy Pelosi was first elected in 1987. She has been in power for basically my entire life. And I am turning 40. Her legacy, in my mind? Losing women the right to bodily autonomy, for not taking Republicans seriously and doing anything about it when she could have. OK fine she did get us the ACA. Thanks lady. But it\u2019s time to go home, get some rest, and let someone else have a turn! (Sorry Bernie, I love ya man, but you too!!!)\n\nI think those in charge should have some stakes in the game. If you\u2019re already past the actuarial tables, you shouldn\u2019t get to make decisions for the rest of us who are going to live in the world our politicians make for potentially decades. (I expect the Supreme Court will be ultra conservative for the rest of my life because Dems wouldn\u2019t dare trying to expand it.) Be a mentor, yes, please! But give up the power. Normal retirement age is 67. I don\u2019t think it\u2019s ageist to suggest it might be time for someone two decades past retirement age to pass the torch.\n\nGive me a party with energy, give me politicians who fight! Oh, for an opposition party \ud83e\udd7a I don\u2019t have to agree 100% with them, if they\u2019d just have some gumption. Give me AOC energy, Shawn Fain energy, Pete Buttigieg (!) energy, Cory Booker energy, Chris Van Hollen energy, Kat Abughazaleh energy. Not whatever this pathetic excuse for \u201cleadership\u201d is. Walz had the energy and they told him to stuff it. Brilliant strategy guys.\n\nPlease, please tell me someone is primarying all my Federal reps next election, because I will vote for anyone who shows some semblance of spine \ud83d\ude21", "label": 1}
{"title": "IndieWeb Carnival December 2023 \u2014 Holiday and December traditions", "url": "https://lifeofpablo.com/blog/holiday-traditions", "content": "I really love the month of December. When the holidays approach, I get excited to see snow on the ground. There are so many celebrations and traditions that exist. For myself, I celebrate Mexican traditions and American celebrations. I live between the two cultures and grew up in the United States and have lived in Mexico. At my parents home, we celebrate as if we were in Mexico.\n\nEvery year is different. Sometimes I'm in Oaxaca during the month of December and sometimes I'm in the United States. We celebrate Mexican traditions there but more precisely, the Oaxacan traditions. I like to go see the dances and the parades. May I say it's heavenly go to the food markets and try all the delicious meals.\n\nMexican Traditions:\n\nMy family really likes cooking at home. We make many traditional dishes. We make tamales of various flavors, Mexican punch, mole, cakes, shrimp cocktails, and so much more. There's no limit. In this month, we don't normally eat out. Homemade food tastes so much better!\n\nIn the evenings, we relax and watch some holiday movies. We like to watch movies in Spanish, especially any of the classics. Every year we see Home Alone in Spanish because it's very funny. It hits differently in Spanish. Just being together is important. We can laugh as a family.\n\nWe do decorate our Christmas tree. We open the presents on the night of December 24. It's a traditon. It's always been like that.\n\nUnited States Traditions:\n\nI always make sure to meet up with my friends when I return to Nebraska. Now that we don't live near each other anymore, it's very important to be together when we are back in our hometowns. We check in with each other and see how our year went and what our goals are for the upcoming year. We build ginger houses and we have a party. What could go wrong? Depending on how the weather is, we're riding our longboards or we're going to skate park.\n\nIm always excited for a night of programming fun. We just see where the night will take us. We make it our goal to stay up until dawn. It's a very nice thing to schedule something together. I think we're going to make a video game this year. Let's see what happens...\n\nThis is my response to IndieWeb Carnival December 2023 \u2014 Holiday and December traditions Thank you for hosting Jo! Remember anyone and everyone can participate in the IndieWeb Carnival. Click here to learn more and how to participate.\n\nSyndicated on IndieNews EN", "label": 1}
{"title": "How I take my coffee", "url": "https://shellsharks.com/notes/2025/05/13/how-i-take-my-coffee", "content": "Mike Sass\n\n@shellsharks\n\nRiffing on Axxuy and Elena\u2019s posts about how they drink coffee, here\u2019s how I take my coffee\u2026 \u2615\ufe0f\n\nAs of March (2025) I\u2019ve gotten into making at-home cold-brew coffee. It\u2019s delicious! I normally take 2/3 of a pint glass with a splash of half-n-half, another splash of 2% milk, then top it off with ice (cubes). This is what I drink most of the time these days. Since I\u2019m newish to brewing my own cold brew, I\u2019m still exploring what types of beans I like most and have really been enjoying sampling different roasts and regions (speaking of, maybe I should start a sort of \u201ccoffeelog\u201d where I can do some tasting notes / reviews\u2026 \ud83e\udd14). Not sure what I like the most yet, but I do know that it\u2019s far better than the french press swill I had been making before.\n\nWhen I\u2019m out \u2018n about and ordering coffee, I typically go with an iced latte or sometimes just an iced coffee. I like getting the latte\u2019s because I can\u2019t make them at home. I never drink hot coffee. I\u2019d rather have no coffee than have it hot. I just don\u2019t enjoy hot beverages. When I do happen across a Starbucks, my go-to order is their Iced Brown Sugar Oatmilk Shaken Espresso, with just 1 pump of the syrup, otherwise it\u2019s too sweet for my liking.\n\nCheers!", "label": 1}
{"title": "The adolescents are alright. So are the common people. Let\u2019s make tech good for them.", "url": "https://tommorris.org/posts/2025/the-adolescents-are-alright-so-are-the-common-people-lets-make-tech-good-for-them/", "content": "I\u2019ve been trying and failing to write about the current vibes of technology for the last few months, with a specific focus on both the wrapping up of the Post Office Horizon Inquiry, and the threat/promise by Keir Starmer to \u201cmainline AI in the veins\u201d of the United Kingdom. But that\u2019s way too big. I want to talk about aforementioned vibes based on two recent fictional depictions that directly touch on the societal role of technology. And because we\u2019re talking about vibes, I need to give two warnings. One is that it\u2019s just my opinion, man, and it will meander around a bit.\n\nAnd the second is a spoiler warning for both Adolescence and Black Mirror Series 7 Episode 1 (\u2018Common People\u2019).\n\nTo say Adolescence has become a touch point for popular concern about technology in the UK is an understatement. Each episode is a technically flawless one shot. The mini-series starts with a dramatic early morning arrest of a thirteen year old boy for murder of a classmate. The camera then follows the plodding institutional procedure of Jamie being booked into a police custody suite, having his fingerprints and samples taken, then an interview. It should be noted that the apparent commitment to procedural accuracy gets a bit shaky when it comes to the role of the defence solicitor, but given the number of disclosure violations in the criminal justice system of England and Wales, a few more occurring in fiction is at least in keeping with the show\u2019s commitment to brutal realism.\n\nThe subsequent episodes apply the same masterful theatre-like technique to the investigation of the crime by detectives at the unpleasant secondary school where both perpetrator and victim studied, then to a pre-trial interview by a psychologist in a youth detention centre, and finally to the consequences of the conviction on Jamie\u2019s family.\n\nGiven the centrality of technology to the crime at the heart of both the drama and the societal discussion it has spawned, it is weirdly absent from the masterly manoeuvred view of the camera. It reminds me of the depiction of HIV/AIDS on screen\u2014it is always there in the sense that it shapes the lives of those it affects, but you never see it, only the wretched physical effects it has on people infected, and the emotional consequences for their partners, friends and communities. Here, the smartphone, or the gaming console or PC, is the unseen horror, a malevolent ghost that is everywhere and nowhere. The inciting incident for the murder\u2014a series of interactions on Instagram\u2014are never seen, only discussed by others. The key to interpreting them is only grasped indirectly, when the detective\u2019s son Adam takes pity and gives his father a remedial education in translating semantically-overloaded emojis from impenetrable zoomer lingo into a motive that explains murder.\n\nThe Prime Minister incorrectly referring to Adolescence in Parliament as a \u201cdocumentary\u201d is perhaps a testament to the artful camerawork, but it is practically being treated as one\u2014with efforts to add it to the National Curriculum and legislate based on the assumption that screen time is not just rotting the brains of the young, it is turning them into murderers. Something. Must. Be. Done. Ban smartphones from the schools. Oh, what, headteachers can already do that? Well, double triple quadruple ban them! And ban the kids from social media too. Maybe we could pass some kind of Online Safety Bill to\u2026 oh, wait, we already have.\n\nLet\u2019s start with some concessions. There are bad consequences of technology. Obviously. But the nuance-free debate we\u2019ve seen since seems predicated on the idea that there is this giant immutable thing called Technology, and we can\u2019t do much about it. The most we can do is to save the Jamies and Lisas from the Satanic evils of Snapchat, Instagram and TikTok until they\u2019re old enough to make their own decisions. And once we\u2019ve returned to the pre-iPhone norm, order will be restored and childhood saved.\n\nThe framing around the government\u2019s AI policy is also worth noting here. The pressing need to \u201cmainline\u201d it into our national veins is roughly \u201cthis seems like it is inevitable, we at least want it to make some money and create some jobs\u201d, combined with a little dash of \u201cwe don\u2019t know what it is, but whatever it is, we want to beat China at it\u201d. The train is rolling, we either get on board and reap some economic and geostrategic benefits, or we don\u2019t. And we\u2019d rather reap some benefits than be stuck in the past, right? Stop with all the nuance. Get on board or die. (Silicon Valley accelerationism is basically a version of this view, but with a penchant for skull measuring, and an addiction to ketamine.)\n\nBoth the post-Adolescence moralism and the boosterish \u201cmainline AI into our veins\u201d narrative seem to coexist in the political imagination of Sir Keir Starmer, and both depend on a feeling that the result is inevitable and foreordained, and all we can do is react. We either accept it, and hope we can nudge it gently towards some social benefit, or bring down the regulatory banhammer.\n\nIt\u2019s understandable why people don\u2019t feel much control over technology. It\u2019s a fact they don\u2019t have much control. That\u2019s a problem we need to fix. The solution to that is imagining an alternative where they do. Let\u2019s imagine if technology was a tool that individuals could shape in accordance with their goals for a better life.\n\nA nuanced discussion of technology that doesn\u2019t fall into inevitabilism and moral panic might help us understand technology in a way where we can respond to it in intelligent and effective ways.\n\nAs an illustration, let\u2019s talk about Roblox. I have friends with young children who spend hours and hours every day playing Roblox on an iPad. When discussing Roblox, their chief concerns are \u201care they spending too much time on this thing?\u201d and \u201care they gonna end up talking to predators?\u201d\n\nThe easy answers you\u2019ll generally hear from the media: yes and yes.\n\nThe real answers to this are: that depends on a whole load of other variables and perhaps, but that risk very hard to quantify.\n\nAre there sexual predators on Roblox? Yes. This is because Roblox is a service on the internet, and there are sexual predators on the internet, in much the same way there are sexual and other predators everywhere. Are Roblox doing enough to combat predators on their platform? Maybe. Hard to tell. We may get more information as the new regulatory regime in the Online Safety Act takes shape, along with similar regulatory regimes in EU and Australia. Or we might not. But, absolutely, be vigilant against awful people, and don\u2019t assume the company will give a damn unless it reaches a point where it affects their stock price.\n\nAre children spending too much time on Roblox? Depends on the child, what they\u2019re getting from using the service, and what the opportunity cost there is. A lot of children and young people spend a lot of time playing computer games. So do adults. I\u2019ve spent a fair amount of time playing games. Some of them were crap, some of them were amazing. Kinda like books or albums or movies or meals in restaurants. Some of the people I knew at school likely spent time hanging around rural bus stations drinking bottles of White Lightning, a substance that\u2019s only charitably referred to as cider. Others took violin lessons and went to Oxford. (You can adjudicate which ones turned out happier and better adjusted.)\n\nSome of the kids I knew at school would have had colossally high levels of what is now called \u201cscreen time\u201d. Many also had colossally high levels of bullying. If you take the view that the time wandering around Roblox or watching weird algorithmically generated YouTube kids videos is time that said kids aren\u2019t spending playing with friends in wholesome ways, or taking ballet classes or composing sonnets, then, yes, your utilitarian calculus will swing heavily against it. But then you also need to weigh up the kids who are learning to build their own websites or video games, or finding supportive communities, or reading, or playing videos games with really strong narratives. And you need to add all the stuff in the plus column against all the absolutely bloody silly things kids got up to that did not need involve screens or internet connections. \u201cScreen time\u201d is a number, not an evaluation of whether the time spent was worthwhile in the context of their life.\n\nInto those conversations with parent friends about Roblox come the concessions: yes, it might be a waste of time. Yes, there might be Bad People on there, do keep an eye out for that. But, please, keep listening, it\u2019s also way worse than that. The whole enterprise of things like Roblox are built on pester power. Roblox makes absolute bank out of users convincing their parents to buy Robux to spend on random stuff in Roblox. That\u2019s how Roblox is very, very rich.\n\nLoads of other games do likewise too. Finding the games that don\u2019t suck is hard because our rating systems are based on a perception of risk that is centred around violence, profanity and nudity/sexual content. This is understandable: the video game rating system was basically cut and paste from the rating system used for films and television. But it isn\u2019t actually helpful to think about them in the same way as you do for film and television.\n\nAn illustration of this: Elden Ring is rated 16+ by PEGI, because it contains violence and gore and some nudity, apparently. (I haven\u2019t played it. I hear it is very good.) EA Sports FC \u201925 (formerly known as FIFA) is rated 3+ because it doesn\u2019t contain any violence or gore or nudity\u2014it\u2019s a football game, after all. It has an in-game currency, which is used to purchase randomised card packs. These are ways you give the game developer money in order to have a random chance to win something of benefit to you in the game\u2014namely, players. (So I\u2019m told. I also haven\u2019t played it.) If you don\u2019t win, you can pay lots of money to keep buying more and more card packs and opening them on the off-chance of getting the prize you want. That may sound like the sort of thing gambling addicts do with a fixed odds betting terminal. But you\u2019ll be glad to know it is not within the scope of the Gambling Act 2005 because that requires it be done \u201cfor a prize\u201d. A prize is defined as \u201cmoney or money\u2019s worth\u201d. And cards that give you the ability to add football players to your team aren\u2019t that. Other jurisdictions disagree, specifically Netherlands and Belgium.\n\nSure, there have always been ways for kids to waste their pocket money, from Panini stickers to Pogs to Pokemon cards. You used to have to slog your way down to your newsagent to waste your money on Panini football stickers. Now, PEGI, the Pan European Game Information service, who rate video games in the UK and Europe have decided that a game that pummels you with both the aesthetics of, and the mechanics of, gambling in order to win the chance of signing better players for your virtual football team deserves a 3+ \u201cSuitable for all ages\u201d ranking, with a little notice that reads:\n\nThis game offers players the opportunity to purchase in-game items, in the form of an in-game currency, which can be used to purchase random card packs and other game items. Some parents or carers may want to be aware of this. Parents, carers or other responsible adults should check to see what is being offered before making any purchase on behalf of a child. It should be noted that the game can still be played without the need to purchase such items.\n\nWhat is omitted from this description is that the game will usually really suck if you don\u2019t purchase said items, so you\u2019ll be constantly pressured to do so over and over.\n\nAnother example: let\u2019s talk about Balatro. It\u2019s an indie card-based computer game, and it is incredibly addictive. It is not a gambling game but it does rely on the scoring rules of poker hands. When PEGI first rated the game last year, they gave it an 18+ rating as it contained \u201cprominent gambling imagery\u201d. This is enough to classify it as 18+. EA Sports FC, which tries to get players to play what\u2019s essentially an I Can\u2019t Believe It\u2019s Not Gambling! slot machine to build their football team, is rated 3+. Balatro was reclassified on appeal as 10+, perhaps so PEGI could still retain some shred of professional dignity. Presumably, PEGI are still committed to the idea that EA Sports FC card packs are suitable for 3-9 year olds in a way that cartoon depictions of playing cards aren\u2019t.\n\nIt has now taken me boatloads of paragraphs to explain what sucks about lots of modern video games from AAA studios in a way someone who doesn\u2019t play video games can understand, and to give you examples of how the rating system designed by the industry and approved by regulators to protect children from harm does not actually do this very well. Humans are busy people and don\u2019t want to have to think about this. By contrast, \u201ctoo much screen time bad\u201d fits in a tweet or an opinion column. Guess which one takes off more in the public consciousness. And if you\u2019re a government, banning stuff costs nothing. (Except if you do it really badly. Which we have. More on that later. Let\u2019s get back to the really important stuff, namely television.)\n\nIf the technology in Adolescence takes the form of ghoulish off-screen actions that requires a secret Gen-Z emoji-to-English decoder ring to understand, the technology in \u2018Common People\u2019, the first episode of the new series of Black Mirror is as clear as day.\n\nAmanda is a teacher, and her British partner Mike is working in manual labour in Smalltown, USA. While in front of her class, Amanda collapses into a coma and suffers brain damage. There is no hope, says the doctor, except an experimental neurotech startup called Rivermind. In walks a kind saleswoman for the company. Installing the device would be free. The affected bit of the brain would be backed up to the cloud, and restored to the McGuffin every night while you sleep. The catch? it would be $300/month to pay for the subscription service. When the alternative is being dead, $300 is a bargain! OpenAI want $200 a month for the super duper version of ChatGPT after all.\n\nThe satire goes exactly where anyone who understands the arc of consumer tech would guess from the premise. Mike takes on extra shifts to help pay for the service keeping his partner alive, and ends up hustling for money on a website that\u2019s basically Twitch for sadistic humiliation. A premium tier is rolled out, and the basic tier is rapidly enshittified (technical term, don\u2019t blame me). Contextual adverts spew from Amanda\u2019s mouth at inopportune times (without her even being aware), which leads to her being almost fired after an advert for a shady Christian counselling service is unknowingly spewed directly at a pupil needing her support. Her \u201crest mode\u201d gets further extended, and her brain starts being used for some unstated purpose to benefit the Rivermind corporation, rather like a piece of malware mining cryptocurrency for someone else\u2019s benefit, except it\u2019s living in your own brain. (That\u2019s some real body horror.)\n\nI won\u2019t spoil the ending. It\u2019s as grim as you expect from Black Mirror.\n\nYes, it\u2019s Black Mirror, it\u2019s going to be over the top. Except, of course, the satire is real. People with \u201cobsolete\u201d bionic eyes that are no longer supported by the manufacturer are now losing their sight because\u2026 well, I\u2019d love for someone to justify this, because I sure as hell can\u2019t. There was once a time when restoring someone\u2019s sight got people to start spreading rumours about you being the messiah and fulfilling the prophecies foretold in the Book of Isaiah. Well, great news, we achieved a similar feat using technology. Then we cancelled the miracle that\u2019s sitting inside the eyelids of actual human beings when it conflicted with corporate intellectual property rights, and now they will go blind again.\n\nEvery time I think about the notion of the discontinued bionic eyes losing sight, I want to punch a wall. It\u2019s just so incredibly awful and in direct opposition to any humane version of how I envision technology working in society. We have the tech. It works. \u201cAI will fix it?\u201d Fuck right off, we\u2019ve already fixed it. And yet we haven\u2019t because of this? Absolutely ghoulish.\n\nThis episode of Black Mirror is not really about tech. Unlike some vibe-coded fantasy by a founder who doesn\u2019t want to pay software developers (or indeed, large chunks of what passes for enterprise software), the tech here actually does work. The culture around it doesn\u2019t. If you wanted to imagine a world where people have to engage in public displays of self-degradation on social media in order to pay for essential healthcare costs, well, type \u201cinsulin GoFundMe\u201d into a search engine. Researchers have written papers on the phenomena.\n\nThe McGuffin in this episode doesn\u2019t need fixing. The laws do. If someone sells you what the tech industry incorrectly refers to as a \u201cproduct\u201d, then fundamentally changes the terms in this manner\u2026 that ought to be an actionable consumer rights issue. Yes, even if in teeny tiny print on page 87 of the contract it says they can do that. But rebalancing consumer rights in an age of enshittified software-as-a-service is boring, involves tradeoffs, and might cost money. Take Smartphones Off Teenagers is super easy in comparison.\n\nThe point here is technology is not a magic box brought down from on high, it is a thing we create, and we collectively can decide how to regulate and shape it. We can just accept some Adolescence-style moral panic about ghostly black rectangles, decide we have no agency to meaningfully morph it to further the goals we want. We could do something, anything, because please won\u2019t someone think of the children. Or we could engage with it in a sensible but extremely critical way. If you want to be extremely critical, you\u2019ve got to understand it first.\n\nTechnologists\u2014the people who create, design and develop the technology\u2014are incredibly critical of technology.\n\nA little sample. Blockchain nonsense: absolutely crap, pretty much just scams. AI: yeah, in small, very well-tested domains it can be useful, but there\u2019s so much hype and nonsense, most of it pushed by politicians and business executives. Data security: Jesus, we\u2019re about a decade behind where we need to be. Those printers that want a subscription fee: yeah, throw that in the garbage and buy whatever the least awful laser printer is this year and use it until it dies.\n\nI frequently find myself explaining that there are viable alternatives to the exploitative treadmill of software-as-a-service subscription-based spyware. And yes, the barely functional AI nonsense they\u2019ve crowbarred into Office, Windows, iOS or WhatsApp is not there for your benefit. It is there because there\u2019s an AI investment bubble, and investors need to be convinced that Microsoft, Apple, Meta et al. have jumped on the hype train, even if it isn\u2019t actually going anywhere worth going.\n\nLet\u2019s go back to Roblox a second. Yes, yes, child predators, pester power, wait, virtual concentration camps. Oh god, do we have to?\n\nWhat does Roblox do right? It gives kids tools to make games. Wait, they don\u2019t call them games, they call them \u201cexperiences\u201d, probably because of App Store review rules or whatever. But you can make game-like things and share them with your friends. That\u2019s really cool.\n\nWe have a creation tool here that allows children to creatively use it to express themselves. That\u2019s good at least. Ah, but, I\u2019m sorry, there\u2019s bad stuff when you look a bit harder. Not the murdering-your-classmates genre of bad, but the boring nerdy regulatory kind of bad.\n\nIf you make a game on Roblox, the nice people at the Roblox Corporation pay you money if you do well in the form of Robux, which you can use to buy other stuff on Robux. When kids pester their parents to buy them Robux, the current exchange rate is \u00a34.99 for 500 Robux. You can buy them at a discount if you buy lots of them. The official exchange rate is 1 Robux = $0.0125 (\u00a30.009572 at time of writing).\n\nIf you accumulate lots of Robux because you\u2019ve made a successful \u201cexperience\u201d (i.e. game), you can cash out through a process called Developer Exchange, or DevEx. You can only do this when you\u2019ve earned 30,000 Robux. Great, so you get $375, right? Wrong! That\u2019s the rate at which you buy Robux. When you cash them out, you only get $0.0035 per Robux. Your 30,000 Robux turns into $105. You want to ask careful critical questions about technology? \u201cHow the hell isn\u2019t this exploitation of children?\u201d is the one that immediately comes to mind, followed quickly by \u201cthis is modern day company scrip, we banned that ages ago\u201d.\n\nTo learn more about the absolutely wild world of Roblox, go watch How Roblox Is Exploiting Young Game Developers from People Make Games.\n\nEvery new thing I learn about Roblox makes me genuinely infuriated. You\u2019ve made a platform to let children build things and express their creativity and ruined it with so much garbage. The wall needs punching so bad.\n\nIf a child wants to learn how to write, they need a pen, some paper, and an imagination. The latter is free, the former cost a few pounds at most. If a child wants to learn to make computer games, they should be afforded a way to do so without entering into an exploitative relationship with some massive corporation who will pay them in scrip and keep two thirds of their earnings. Maybe we could build something that gives them the ability to make stuff and learn. And the thing they create is theirs rather than something that Roblox has a hand in.\n\nOn a broader societal level: politicians could pass legislation that is better if they thought about technology in a more nuanced way. The harshest bits of the EU\u2019s Digital Services Act only affects the twenty biggest online platforms, for instance. (Roblox isn\u2019t one of them, curiously.) This is unlike the UK\u2019s Online Safety Act which sets no compliance threshold, and so affects basically everyone. Even the most spooked parent probably doesn\u2019t stay up at night worrying about some kind of Adolescence-style scenario playing out with their little Jamie on the London Fixed Gear and Single Speed forum, a place where cycling nerds discuss, well, fixed gear and single speed bikes. Thankfully, they won\u2019t be troubled by this risk. The faff of complying with the Online Safety Act has led the site administrators to close it down. Google, Meta, Microsoft, Amazon et al. will be able to shoulder whatever regulatory burden anyone puts on them. But the legislation passed ostensibly to stick it to Big Tech has led to lots of people running little hobbyist forums having to read long and incredibly boring Ofcom documents, and take on substantial personal legal risk for their hobby websites if they get it wrong. If you do care about fixed gear bikes, I guess you could find a Facebook group to talk to others about it. That\u2019d really stick it to Zuck.\n\nWhat we lose when the entire debate becomes about the notion of \u201cscreen time\u201d and ghostly Satanic rectangles is a dream of someting better. Technology could actually be good and not something that we just hate using. It feels weird to say this because so much of technology is just the product of unpleasant, lazy, cynical people who don\u2019t give a damn. (Or maybe they do, but the incentive structures doesn\u2019t really allow them. The output looks the same.) And non-technical people not only can understand it deeply rather than in reductive tabloid soundbitey chunks and moral panics , they can help build it carefully, deliberatively and in a way that brings out more humanity.\n\nHow you achieve this is a topic I defer to far smarter people, so I can go finish the rest of the new Black Mirror which has turned out to be rather better than I expected it would.", "label": 1}
{"title": "Bringing Gemini intelligence to Google Home APIs", "url": "https://developers.googleblog.com/en/bringing-gemini-intelligence-to-google-home-apis/", "content": "The smart home is rapidly evolving into an intuitive ecosystem to make life easier, and its next era will be powered by Gemini and the Home APIs. This isn't just about connected devices; it's about creating effortless experiences. With the Home APIs, our goal has been to empower all developers to build innovative devices and experiences for the home. Now, with Gemini in the Home APIs, we're taking the next step: bringing the best of Google's AI directly to you. We're moving beyond simple device control to create an effortless smart home that truly understands, adapts, and responds to your users' needs. At Google I/O 2024, we announced the Home APIs, providing app developers with access to over 600M devices. We are excited to share that our ecosystem has grown even more to over 750M devices that developers now have access to along with Google's hubs and Matter infrastructure, and an automation engine powered by Google intelligence. We\u2019ve spent time rolling it out to a few early access partners, our Android and iOS SDKs are in public developer beta, and some developers have already leveraged the Home APIs to release new apps on Android.\n\nPartner experiences built with Home APIs Last year, we shared the innovative new ways partners like ADT, LG, and Eve built on Google Home, and now there are even more partners showcasing how Home APIs are making their customer experience even better:\n\nFirst Alert Control your smoke alarm from the First Alert app or the Google Home app and seamlessly interconnect with your existing Nest Protects.\n\nYale Yale\u2019s upcoming Matter lock, the successor to the Nest x Yale lock, takes advantage of the best-in-class lock features in the Google Home app, built using the Home APIs.\n\nCync Imagine your home automatically adjusting lighting and fan settings to ensure your pet's comfort when you're away. Cync is making this a reality.\n\niRobot Select iRobot Roomba\u00ae robots can create automations using Google Home presence sensing, so they can automatically clean your home when you leave the house, ensuring a spotless return.\n\nMotorola Moto Tag You can create custom smart home routines triggered by simple tag interactions, offering unparalleled personalization.\n\nTuya Smart Tuya Smart is enhancing seamless interoperability. Now, users can easily set up a Matter device and control devices connected to Google directly in Tuya Smart app.\n\nBringing your cameras to life with Gemini-powered Home APIs Last fall, we introduced Gemini-powered camera features in public preview in the Google Home app, allowing users to ask natural questions like, \u201cDid the kids leave their bikes in the driveway?\u201d and instantly get relevant video clips. Now we are bringing those camera experiences directly to developers too.\n\nWe're including the standard camera features you'd expect \u2013 like live streaming, event history access, two-way talk capabilities, and camera settings. But we're going further by integrating the Gemini-powered intelligence that our users love, such as AI descriptions and the ability to search camera history, making it easier to quickly identify what you are looking for in your camera history, keeping you and your family safer.\n\nMaking automation effortless with Gemini Figuring out the perfect automation to help improve your home experience and implementing it can be a daunting task many users don\u2019t want to undertake. So, we\u2019re introducing new Gemini-powered features to the Automations API designed to make creating powerful routines easier than ever: Suggested Automations: Gemini intelligently analyzes the devices in a user's home and proactively suggests potentially useful automations they might not have thought of.\n\nHelp me create: Building automations becomes as simple as a conversation. Users can tell Gemini what they want to achieve using natural language, and the automation is drafted for them.\n\nNew Automation Starters: We're adding more sophisticated triggers based on dates and weather conditions, allowing automations to respond more dynamically to the complexities of real life.\n\nThese new features will enable you to offer unprecedented Gemini-powered intelligent capabilities to your users more quickly than ever before.\n\nGemini across the Google Home surfaces The benefits don't stop within your app. When you integrate your devices using the Google Home APIs, they can participate in Gemini-powered experiences across Google's surfaces.\n\nFor instance, Google Home users, while in the Gemini app, can control and inquire about their smart home devices using natural language. We've also previewed Gemini enhancing the voice experience on smart speakers, smart displays, and Google TV, enabling more natural interaction, deeper exploration of topics, device control, and even voice-based automation creation. And we are testing a Home Summary Widget on Pixel with a select set of users, providing insights about your home without having to open an app!", "label": 0}
{"title": "TIL: Setting default browser on macOS using Nix", "url": "https://tommorris.org/posts/2024/til-setting-default-browser-on-macos-using-nix/", "content": "Let\u2019s say you\u2019ve just switched browser. If you\u2019re a normal person, the new browser will probably ask you whether you want to change your default browser. You\u2019ll click the little button and it\u2019ll happen. Or you go into System Preferences and do it.\n\nNow you can stop reading. But if you\u2019re weird and want to set it programatically on macOS, here\u2019s a hacky way to do it using Nix-Darwin.\n\nIt uses defaultbrowser, which is a teeny little CLI tool that calls the relevant Objective-C functions.\n\nIf you run defaultbrowser without an argument, it shows you what browsers you have installed. You then pass one of those browser identifier strings to it as an argument.\n\nSome options for known browsers: safari , chrome , firefox , librewolf , torbrowser .\n\nNow let\u2019s put it in a Nix-Darwin setup.\n\nlet vars = { # ... defaultbrowser = \"librewolf\"; }; in # ... your nix-darwin setup\n\ndefaultbrowser needs to be installed as a system package, so let\u2019s add it to environment.systemPackages\n\nenvironment.systemPackages = import ./modules/packages.nix pkgs ++ (if (vars ? \"defaultbrowser\" && builtins.isString vars.defaultbrowser) then [ pkgs.defaultbrowser ] else [ ]);\n\nNow we need an activation script that\u2019ll run it every time you run your Nix setup (which will be relatively often).\n\nsystem.activationScripts = (if (vars ? \"defaultbrowser\" && builtins.isString vars.defaultbrowser) then { postUserActivation.text = \"defaultbrowser ${vars.defaultbrowser}\"; } else { });\n\ndefaultbrowser is clever enough that if you tell it to set your default browser to one that already is your default browser, it\u2019ll just print a nice message to the screen like:\n\nlibrewolf is already set as the default HTTP handler\n\nI thought about adding the variable to the list of packages (or Homebrew Casks, or Mac App Store apps) that need installing, but there\u2019s a mismatch between the browser identifier string and the package name used by nixpkgs and/or Homebrew. Plus if you\u2019re using Safari, there\u2019s kind of no way to not install it. So you need to make sure you\u2019ve installed the browser, and check the browser identifier string with defaultbrowser .", "label": 1}
{"title": "Realities, perceived and presented, and their relationship to Truth \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/12/realities-perceived-and-presented-and-their-relationship-to-truth/", "content": "I\u2019ve been thinking a lot lately about how knowledge doesn\u2019t equal understanding\u2014there are things you can know to be intellectually true, but that doesn\u2019t mean your body or heart has caught up yet. \u2026 You have to prove to your nervous system, choice by choice, that this new way of thinking is actually safe\u2026 \u2014 Dipa Halder\n\n+\n\nCrises are moments of intense unpredictability. A crisis is, by definition, a rupture with the recent past, making any projection of the likely future a fool\u2019s game. Screens promise to relieve the discomfort this brings about: the unending stream, the servile virtual assistant, the fully optimized day\u2014all are designed to make things predictable, to resolve the anxiety of not knowing what could happen next. \u2014 Mandy Brown\n\n+\n\nwe long for the corporeal in a landscape governed by hypotheticals that may or may not evolve into literals. \u2014 Kyle Raymond Fitzpatrick\n\n+\n\nAmerica has no memory. Pioneering homesteaders, Gold Rushers, post-war company men with green lawns, Steve Jobs, ALL OUR GREAT MYTHOLOGIES, REALLY, NECESSITATE A BREAK FROM THE PAST. Deleting what came before you in favor of \u201cthe miraculous now\u201d\u2026 America wants to be a vacuum of time and space, a suspended hologram of perfection, unmolested by history\u2014 and unadulterated by nature. \u2014 Steven Phillips-Horst\n\n+\n\nThe complaint that real humans \u201ctalk in circles for hours\u201d and engage with \u201csubjective nuance\u201d reflects something all too real about our social skills in these times. As a society, we\u2019ve been socially deskilled in countless ways. The performance-based dynamics of social media, the biases and bigotries that divide us, and the loss of third spaces have eroded our capacity to relate to one another. \u2014 Kelly Hayes\n\n+\n\n[The phone] knows the parts of ourselves which we don\u2019t know, and cannot really understand the parts of ourselves which we are actually most interested in. \u2014 Aidan Walker\n\n+\n\nBeijing Watches Fake Sunrise On Video Screen Amid Smog Emergency (2014)\n\n+\n\n\u201cTasted a little tear gas\u2014 tasted like fascism\u201d\n\n\u201cTasted a little tear gas\u2014 tasted like fascism\u201d pic.twitter.com/o7SLl4ZWnV \u2014 Acyn (@Acyn) June 8, 2025\n\nThinking about how narrative can coopt \u2014 and eventually shape \u2014 reality in relation to the LA protests this week, and how they are being portrayed as chaos by the government and media when on-the-ground reports are that it was pretty chill till the cops showed up and started shooting people. It seems to make little difference what is true when Republicans can paint every act as a step towards the reality they desire. The troops did in fact get deployed to LA, whether or not it was legal or justified, law enforcement did in fact shoot people\u2026 that those they shot were journalists!!! and protesters practicing their First Amendment rights means nothing when MAGA can handwave them off as paid insurgents (\ud83e\udd28 the critical thinking skills have been shut down here). The occupying administration is synthesizing a false \u201ctruth\u201d from segments of reality that do not represent Truth.\n\nThe saying goes that \u201ccinema is truth 24 times per second,\u201d but the medium of film bears little relation to truth in itself, as the scene excludes whatever is cropped from the frame, the temporal context of events preceding and following, and all sensory elements besides sight and sound. Fact-checking the quote, I discovered it has a second part: \u201cand every cut is a lie.\u201d I finally read the Walter Benjamin essay \ud83e\uddbe and he writes that film inherently separates viewer from truth because of the camera intermediary: \u201cHis creation is by no means all of a piece; it is composed of many separate performances.\u201d Semblances of reality may be montaged together to create a seamless imitation of some desired reality.\n\nSee also:\n\nCreating our own unrealities\n\nCeding the work of interpretation\n\nComplementary: what is real?\n\nOn conformity and control", "label": 1}
{"title": "Java 21 Virtual Threads - Dude, Where\u2019s My Lock?", "url": "https://netflixtechblog.com/java-21-virtual-threads-dude-wheres-my-lock-3052540e231d?source=collection_home---4------19-----------------------", "content": "Java 21 Virtual Threads - Dude, Where\u2019s My Lock?\n\nGetting real with virtual threads Netflix Technology Blog 10 min read \u00b7 Jul 29, 2024 -- 35 Listen Share\n\nBy Vadim Filanovsky, Mike Huang, Danny Thomas and Martin Chalupa\n\nIntro\n\nNetflix has an extensive history of using Java as our primary programming language across our vast fleet of microservices. As we pick up newer versions of Java, our JVM Ecosystem team seeks out new language features that can improve the ergonomics and performance of our systems. In a recent article, we detailed how our workloads benefited from switching to generational ZGC as our default garbage collector when we migrated to Java 21. Virtual threads is another feature we are excited to adopt as part of this migration.\n\nFor those new to virtual threads, they are described as \u201clightweight threads that dramatically reduce the effort of writing, maintaining, and observing high-throughput concurrent applications.\u201d Their power comes from their ability to be suspended and resumed automatically via continuations when blocking operations occur, thus freeing the underlying operating system threads to be reused for other operations. Leveraging virtual threads can unlock higher performance when utilized in the appropriate context.\n\nIn this article we discuss one of the peculiar cases that we encountered along our path to deploying virtual threads on Java 21.\n\nThe problem\n\nNetflix engineers raised several independent reports of intermittent timeouts and hung instances to the Performance Engineering and JVM Ecosystem teams. Upon closer examination, we noticed a set of common traits and symptoms. In all cases, the apps affected ran on Java 21 with SpringBoot 3 and embedded Tomcat serving traffic on REST endpoints. The instances that experienced the issue simply stopped serving traffic even though the JVM on those instances remained up and running. One clear symptom characterizing the onset of this issue is a persistent increase in the number of sockets in closeWait state as illustrated by the graph below:\n\nCollected diagnostics\n\nSockets remaining in closeWait state indicate that the remote peer closed the socket, but it was never closed on the local instance, presumably because the application failed to do so. This can often indicate that the application is hanging in an abnormal state, in which case application thread dumps may reveal additional insight.\n\nIn order to troubleshoot this issue, we first leveraged our alerts system to catch an instance in this state. Since we periodically collect and persist thread dumps for all JVM workloads, we can often retroactively piece together the behavior by examining these thread dumps from an instance. However, we were surprised to find that all our thread dumps show a perfectly idle JVM with no clear activity. Reviewing recent changes revealed that these impacted services enabled virtual threads, and we knew that virtual thread call stacks do not show up in jstack -generated thread dumps. To obtain a more complete thread dump containing the state of the virtual threads, we used the \u201c jcmd Thread.dump_to_file \u201d command instead. As a last-ditch effort to introspect the state of JVM, we also collected a heap dump from the instance.\n\nAnalysis\n\nThread dumps revealed thousands of \u201cblank\u201d virtual threads:\n\n#119821 \"\" virtual\n\n\n\n#119820 \"\" virtual\n\n\n\n#119823 \"\" virtual\n\n\n\n#120847 \"\" virtual\n\n\n\n#119822 \"\" virtual\n\n...\n\nThese are the VTs (virtual threads) for which a thread object is created, but has not started running, and as such, has no stack trace. In fact, there were approximately the same number of blank VTs as the number of sockets in closeWait state. To make sense of what we were seeing, we need to first understand how VTs operate.\n\nA virtual thread is not mapped 1:1 to a dedicated OS-level thread. Rather, we can think of it as a task that is scheduled to a fork-join thread pool. When a virtual thread enters a blocking call, like waiting for a Future , it relinquishes the OS thread it occupies and simply remains in memory until it is ready to resume. In the meantime, the OS thread can be reassigned to execute other VTs in the same fork-join pool. This allows us to multiplex a lot of VTs to just a handful of underlying OS threads. In JVM terminology, the underlying OS thread is referred to as the \u201ccarrier thread\u201d to which a virtual thread can be \u201cmounted\u201d while it executes and \u201cunmounted\u201d while it waits. A great in-depth description of virtual thread is available in JEP 444.\n\nIn our environment, we utilize a blocking model for Tomcat, which in effect holds a worker thread for the lifespan of a request. By enabling virtual threads, Tomcat switches to virtual execution. Each incoming request creates a new virtual thread that is simply scheduled as a task on a Virtual Thread Executor. We can see Tomcat creates a VirtualThreadExecutor here.\n\nTying this information back to our problem, the symptoms correspond to a state when Tomcat keeps creating a new web worker VT for each incoming request, but there are no available OS threads to mount them onto.\n\nWhy is Tomcat stuck?\n\nWhat happened to our OS threads and what are they busy with? As described here, a VT will be pinned to the underlying OS thread if it performs a blocking operation while inside a synchronized block or method. This is exactly what is happening here. Here is a relevant snippet from a thread dump obtained from the stuck instance:\n\n#119515 \"\" virtual\n\njava.base/jdk.internal.misc.Unsafe.park(Native Method)\n\njava.base/java.lang.VirtualThread.parkOnCarrierThread(VirtualThread.java:661)\n\njava.base/java.lang.VirtualThread.park(VirtualThread.java:593)\n\njava.base/java.lang.System$2.parkVirtualThread(System.java:2643)\n\njava.base/jdk.internal.misc.VirtualThreads.park(VirtualThreads.java:54)\n\njava.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:219)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:754)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:990)\n\njava.base/java.util.concurrent.locks.ReentrantLock$Sync.lock(ReentrantLock.java:153)\n\njava.base/java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:322)\n\nzipkin2.reporter.internal.CountBoundedQueue.offer(CountBoundedQueue.java:54)\n\nzipkin2.reporter.internal.AsyncReporter$BoundedAsyncReporter.report(AsyncReporter.java:230)\n\nzipkin2.reporter.brave.AsyncZipkinSpanHandler.end(AsyncZipkinSpanHandler.java:214)\n\nbrave.internal.handler.NoopAwareSpanHandler$CompositeSpanHandler.end(NoopAwareSpanHandler.java:98)\n\nbrave.internal.handler.NoopAwareSpanHandler.end(NoopAwareSpanHandler.java:48)\n\nbrave.internal.recorder.PendingSpans.finish(PendingSpans.java:116)\n\nbrave.RealSpan.finish(RealSpan.java:134)\n\nbrave.RealSpan.finish(RealSpan.java:129)\n\nio.micrometer.tracing.brave.bridge.BraveSpan.end(BraveSpan.java:117)\n\nio.micrometer.tracing.annotation.AbstractMethodInvocationProcessor.after(AbstractMethodInvocationProcessor.java:67)\n\nio.micrometer.tracing.annotation.ImperativeMethodInvocationProcessor.proceedUnderSynchronousSpan(ImperativeMethodInvocationProcessor.java:98)\n\nio.micrometer.tracing.annotation.ImperativeMethodInvocationProcessor.process(ImperativeMethodInvocationProcessor.java:73)\n\nio.micrometer.tracing.annotation.SpanAspect.newSpanMethod(SpanAspect.java:59)\n\njava.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n\njava.base/java.lang.reflect.Method.invoke(Method.java:580)\n\norg.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethodWithGivenArgs(AbstractAspectJAdvice.java:637)\n\n...\n\nIn this stack trace, we enter the synchronization in brave.RealSpan.finish(RealSpan.java:134) . This virtual thread is effectively pinned \u2014 it is mounted to an actual OS thread even while it waits to acquire a reentrant lock. There are 3 VTs in this exact state and another VT identified as \u201c <redacted> @DefaultExecutor - 46542 \u201d that also follows the same code path. These 4 virtual threads are pinned while waiting to acquire a lock. Because the app is deployed on an instance with 4 vCPUs, the fork-join pool that underpins VT execution also contains 4 OS threads. Now that we have exhausted all of them, no other virtual thread can make any progress. This explains why Tomcat stopped processing the requests and why the number of sockets in closeWait state keeps climbing. Indeed, Tomcat accepts a connection on a socket, creates a request along with a virtual thread, and passes this request/thread to the executor for processing. However, the newly created VT cannot be scheduled because all of the OS threads in the fork-join pool are pinned and never released. So these newly created VTs are stuck in the queue, while still holding the socket.\n\nWho has the lock?\n\nNow that we know VTs are waiting to acquire a lock, the next question is: Who holds the lock? Answering this question is key to understanding what triggered this condition in the first place. Usually a thread dump indicates who holds the lock with either \u201c - locked <0x\u2026> (at \u2026) \u201d or \u201c Locked ownable synchronizers ,\u201d but neither of these show up in our thread dumps. As a matter of fact, no locking/parking/waiting information is included in the jcmd -generated thread dumps. This is a limitation in Java 21 and will be addressed in the future releases. Carefully combing through the thread dump reveals that there are a total of 6 threads contending for the same ReentrantLock and associated Condition . Four of these six threads are detailed in the previous section. Here is another thread:\n\n#119516 \"\" virtual\n\njava.base/java.lang.VirtualThread.park(VirtualThread.java:582)\n\njava.base/java.lang.System$2.parkVirtualThread(System.java:2643)\n\njava.base/jdk.internal.misc.VirtualThreads.park(VirtualThreads.java:54)\n\njava.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:219)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:754)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:990)\n\njava.base/java.util.concurrent.locks.ReentrantLock$Sync.lock(ReentrantLock.java:153)\n\njava.base/java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:322)\n\nzipkin2.reporter.internal.CountBoundedQueue.offer(CountBoundedQueue.java:54)\n\nzipkin2.reporter.internal.AsyncReporter$BoundedAsyncReporter.report(AsyncReporter.java:230)\n\nzipkin2.reporter.brave.AsyncZipkinSpanHandler.end(AsyncZipkinSpanHandler.java:214)\n\nbrave.internal.handler.NoopAwareSpanHandler$CompositeSpanHandler.end(NoopAwareSpanHandler.java:98)\n\nbrave.internal.handler.NoopAwareSpanHandler.end(NoopAwareSpanHandler.java:48)\n\nbrave.internal.recorder.PendingSpans.finish(PendingSpans.java:116)\n\nbrave.RealScopedSpan.finish(RealScopedSpan.java:64)\n\n...\n\nNote that while this thread seemingly goes through the same code path for finishing a span, it does not go through a synchronized block. Finally here is the 6th thread:\n\n#107 \"AsyncReporter <redacted>\"\n\njava.base/jdk.internal.misc.Unsafe.park(Native Method)\n\njava.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:221)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:754)\n\njava.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1761)\n\nzipkin2.reporter.internal.CountBoundedQueue.drainTo(CountBoundedQueue.java:81)\n\nzipkin2.reporter.internal.AsyncReporter$BoundedAsyncReporter.flush(AsyncReporter.java:241)\n\nzipkin2.reporter.internal.AsyncReporter$Flusher.run(AsyncReporter.java:352)\n\njava.base/java.lang.Thread.run(Thread.java:1583)\n\nThis is actually a normal platform thread, not a virtual thread. Paying particular attention to the line numbers in this stack trace, it is peculiar that the thread seems to be blocked within the internal acquire() method after completing the wait. In other words, this calling thread owned the lock upon entering awaitNanos() . We know the lock was explicitly acquired here. However, by the time the wait completed, it could not reacquire the lock. Summarizing our thread dump analysis:\n\nThere are 5 virtual threads and 1 regular thread waiting for the lock. Out of those 5 VTs, 4 of them are pinned to the OS threads in the fork-join pool. There\u2019s still no information on who owns the lock. As there\u2019s nothing more we can glean from the thread dump, our next logical step is to peek into the heap dump and introspect the state of the lock.\n\nInspecting the lock", "label": 0}
{"title": "Copilot for all: Introducing Microsoft 365 Copilot Chat", "url": "https://www.microsoft.com/en-us/microsoft-365/blog/2025/01/15/copilot-for-all-introducing-microsoft-365-copilot-chat/", "content": "Our ambition is to empower every employee with a Copilot and to transform every business process with agents. From Dow to Disney, companies are going big with Microsoft 365 Copilot and agents, uncovering key scenarios that can deliver real ROI. Now, organizations of all sizes are looking to scale their AI transformation and realize the enterprise-wide ROI that comes with broad adoption.\n\nToday, we\u2019re introducing Microsoft 365 Copilot Chat, a new offering that adds pay-as-you-go agents to our existing free chat experience for Microsoft 365 commercial customers.1 Copilot Chat enables your entire workforce\u2014from customer service representatives to marketing leads to frontline technicians\u2014to start using Copilot and agents today. It includes:\n\nFree, secure AI chat powered by GPT-4o.\n\npowered by GPT-4o. Agents accessible right in the chat.\n\naccessible right in the chat. IT controls, including enterprise data protection and agent management.\n\nMoving forward, every organization will have a mix of Copilot Chat and Microsoft 365 Copilot\u2014our best-in-class offering\u2014to drive AI transformation at scale. Let\u2019s walk through the new product lineup.\n\nClick to enlarge\n\nCopilot Chat: The power of chat + agents\n\nCopilot is the UI for AI, and it all starts with Copilot Chat. It\u2019s the chat experience you\u2019ll use every day\u2014powered by broad knowledge from the web, built on GPT-4o, and designed to be safe and secure for business use. It represents a foundational shift in how we work, enabling everyone to work smarter, faster, and more collaboratively.\n\nCopilot Chat includes:\n\nWeb-grounded chat with GPT-4o. You can use it to do market research, write a strategy document, or prepare for a meeting. File uploads allow you to add any document to the chat and ask Copilot to do things like summarize key points in a Word document, analyze data in an Excel spreadsheet, and suggest improvements to a PowerPoint presentation. 2 With Copilot Pages , you can collaborate on content with people and AI in real time\u2014adding content from Copilot, your files, and now from the web as well. And you can quickly create AI-generated images for campaigns, product launches, and social media posts. 3\n\nwith GPT-4o. You can use it to do market research, write a strategy document, or prepare for a meeting. allow you to add any document to the chat and ask Copilot to do things like summarize key points in a Word document, analyze data in an Excel spreadsheet, and suggest improvements to a PowerPoint presentation. With , you can collaborate on content with people and AI in real time\u2014adding content from Copilot, your files, and now from the web as well. And you can quickly create for campaigns, product launches, and social media posts. Agents . Using natural language, now anyone can easily create agents to automate repetitive tasks and business processes\u2014directly in Copilot Chat. A customer service representative can ask a customer relationship management (CRM) agent for account details before a customer meeting, while field service agents can access step-by-step instructions and real-time product knowledge stored in SharePoint. Agents are priced on a metered basis, and IT stays in control. IT admins can also build organization-wide agents and manage agent deployment, all powered by Microsoft Copilot Studio.\n\n. Using natural language, now anyone can easily create agents to automate repetitive tasks and business processes\u2014directly in Copilot Chat. A customer service representative can ask a customer relationship management (CRM) agent for account details before a customer meeting, while field service agents can access step-by-step instructions and real-time product knowledge stored in SharePoint. Agents are priced on a metered basis, and IT stays in control. IT admins can also build organization-wide agents and manage agent deployment, all powered by Microsoft Copilot Studio. Copilot Control System. Copilot Chat includes foundational capabilities of the Copilot Control System, including enterprise data protection (EDP) for data privacy and security and the ability to govern access and manage the usage and lifecycle of Copilot and agents, as well as measurement and reporting.\n\nWhile Copilot Chat is a powerful new on-ramp for everyone in your organization to build the AI habit, Microsoft 365 Copilot remains our best-in-class personal AI assistant for work. It includes everything in Copilot Chat and more. Microsoft 365 Copilot combines the power of GPT-4o grounded in your work data\u2014all your meetings, emails, chats, documents, and more; Copilot in the Microsoft 365 apps that millions rely on every day\u2014Microsoft Teams, Outlook, Word, Excel, and PowerPoint; and usage and access to agents. And we continue to rapidly add new capabilities like Copilot Actions to tackle people\u2019s biggest pain points at work. We\u2019ve empowered every IT team to lead and manage at scale with the Copilot Control System and Copilot Analytics to measure the impact and ROI of your Copilot investment.\n\nCustomers can get started with either the free or paid experience in the Microsoft 365 Copilot app, available at m365copilot.com or in the Windows, Android, or iPhone app stores.\n\nThese announcements enable every customer to accelerate their AI transformation and realize enterprise-wide ROI. Now, every employee has a Copilot and a team of agents to scale their impact.\n\nTry Copilot Chat at m365copilot.com and visit WorkLab for the latest research and insights on AI and the future of work.\n\nDownload the Microsoft 365 Copilot app Take the power of AI on the go with the Copilot app. Download today\n\nFootnotes:\n\n1 Learn more about Copilot Chat eligibility.\n\n2 File upload limits apply\n\n3 Image generation limits apply", "label": 0}
{"title": "Model Once, Represent Everywhere: UDA (Unified Data Architecture) at Netflix", "url": "https://netflixtechblog.com/uda-unified-data-architecture-6a6aee261d8d?source=collection_home---4------0-----------------------", "content": "Model Once, Represent Everywhere: UDA (Unified Data Architecture) at Netflix Netflix Technology Blog 15 min read \u00b7 Jun 12, 2025 -- 23 Listen Share\n\nBy Alex Hutter, Alexandre Bertails, Claire Wang, Haoyuan He, Kishore Banala, Peter Royal, Shervin Afshar\n\nAs Netflix\u2019s offerings grow \u2014 across films, series, games, live events, and ads \u2014 so does the complexity of the systems that support it. Core business concepts like \u2018actor\u2019 or \u2018movie\u2019 are modeled in many places: in our Enterprise GraphQL Gateway powering internal apps, in our asset management platform storing media assets, in our media computing platform that powers encoding pipelines, to name a few. Each system models these concepts differently and in isolation, with little coordination or shared understanding. While they often operate on the same concepts, these systems remain largely unaware of that fact, and of each other.\n\nAs a result, several challenges emerge:\n\nDuplicated and Inconsistent Models \u2014 Teams re-model the same business entities in different systems, leading to conflicting definitions that are hard to reconcile.\n\n\u2014 Teams re-model the same business entities in different systems, leading to conflicting definitions that are hard to reconcile. Inconsistent Terminology \u2014 Even within a single system, teams may use different terms for the same concept, or the same term for different concepts, making collaboration harder.\n\n\u2014 Even within a single system, teams may use different terms for the same concept, or the same term for different concepts, making collaboration harder. Data Quality Issues \u2014 Discrepancies and broken references are hard to detect across our many microservices. While identifiers and foreign keys exist, they are inconsistently modeled and poorly documented, requiring manual work from domain experts to find and fix any data issues.\n\n\u2014 Discrepancies and broken references are hard to detect across our many microservices. While identifiers and foreign keys exist, they are inconsistently modeled and poorly documented, requiring manual work from domain experts to find and fix any data issues. Limited Connectivity \u2014 Within systems, relationships between data are constrained by what each system supports. Across systems, they are effectively non-existent.\n\nTo address these challenges, we need new foundations that allow us to define a model once, at the conceptual level, and reuse those definitions everywhere. But it isn\u2019t enough to just document concepts; we need to connect them to real systems and data. And more than just connect, we have to project those definitions outward, generating schemas and enforcing consistency across systems. The conceptual model must become part of the control plane.\n\nThese were the core ideas that led us to build UDA.\n\nIntroducing UDA\n\nUDA (Unified Data Architecture) is the foundation for connected data in Content Engineering. It enables teams to model domains once and represent them consistently across systems \u2014 powering automation, discoverability, and semantic interoperability.\n\nUsing UDA, users and systems can:\n\nRegister and connect domain models \u2014 formal conceptualizations of federated business domains expressed as data.\n\nWhy? So everyone uses the same official definitions for business concepts, which avoids confusion and stops different teams from rebuilding similar models in conflicting ways.\n\nCatalog and map domain models to data containers, such as GraphQL type resolvers served by a Domain Graph Service, Data Mesh sources, or Iceberg tables, through their representation as a graph.\n\nWhy? To make it easy to find where the actual data for these business concepts lives (e.g., in which specific database, table, or service) and understand how it\u2019s structured there.\n\nTranspile domain models into schema definition languages like GraphQL, Avro, SQL, RDF, and Java, while preserving semantics.\n\nWhy? To automatically create consistent technical data structures (schemas) for various systems directly from the domain models, saving developers manual effort and reducing errors caused by out-of-sync definitions.\n\nMove data faithfully between data containers, such as from federated GraphQL entities to Data Mesh (a general purpose data movement and processing platform for moving data between Netflix systems at scale), Change Data Capture (CDC) sources to joinable Iceberg Data Products.\n\nWhy? To save developer time by automatically handling how data is moved and correctly transformed between different systems. This means less manual work to configure data movement, ensuring data shows up consistently and accurately wherever it\u2019s needed.\n\nDiscover and explore domain concepts via search and graph traversal.\n\nWhy? So anyone can more easily find the specific business information they\u2019re looking for, understand how different concepts and data are related, and be confident they are accessing the correct information.\n\nProgrammatically introspect the knowledge graph using Java, GraphQL, or SPARQL.\n\nWhy? So developers can build smarter applications that leverage this connected business information, automate more complex data-dependent workflows, and help uncover new insights from the relationships in the data.\n\nThis post introduces the foundations of UDA as a knowledge graph, connecting domain models to data containers through mappings, and grounded in an in-house metamodel, or model of models, called Upper. Upper defines the language for domain modeling in UDA and enables projections that automatically generate schemas and pipelines across systems.\n\nThe same domain model can be connected to semantically equivalent data containers in the UDA knowledge graph.\n\nThis post also highlights two systems that leverage UDA in production:\n\nPrimary Data Management (PDM) is our platform for managing authoritative reference data and taxonomies. PDM turns domain models into flat or hierarchical taxonomies that drive a generated UI for business users. These taxonomy models are projected into Avro and GraphQL schemas, automatically provisioning data products in the Warehouse and GraphQL APIs in the Enterprise Gateway.\n\nSphere is our self-service operational reporting tool for business users. Sphere uses UDA to catalog and relate business concepts across systems, enabling discovery through familiar terms like \u2018actor\u2019 or \u2018movie.\u2019 Once concepts are selected, Sphere walks the knowledge graph and generates SQL queries to retrieve data from the warehouse, no manual joins or technical mediation required.\n\nUDA is a Knowledge Graph\n\nUDA needs to solve the data integration problem. We needed a data catalog unified with a schema registry, but with a hard requirement for semantic integration. Connecting business concepts to schemas and data containers in a graph-like structure, grounded in strong semantic foundations, naturally led us to consider a knowledge graph approach.\n\nWe chose RDF and SHACL as the foundation for UDA\u2019s knowledge graph. But operationalizing them at enterprise scale surfaced several challenges:\n\nRDF lacked a usable information model. While RDF offers a flexible graph structure, it provides little guidance on how to organize data into named graphs, manage ontology ownership, or define governance boundaries. Standard follow-your-nose mechanisms like owl:imports apply only to ontologies and don\u2019t extend to named graphs; we needed a generalized mechanism to express and resolve dependencies between them.\n\nWhile RDF offers a flexible graph structure, it provides little guidance on how to organize data into named graphs, manage ontology ownership, or define governance boundaries. Standard follow-your-nose mechanisms like owl:imports apply only to ontologies and don\u2019t extend to named graphs; we needed a generalized mechanism to express and resolve dependencies between them. SHACL is not a modeling language for enterprise data. Designed to validate native RDF, SHACL assumes globally unique URIs and a single data graph. But enterprise data is structured around local schemas and typed keys, as in GraphQL, Avro, or SQL. SHACL could not express these patterns, making it difficult to model and validate real-world data across heterogeneous systems.\n\nDesigned to validate native RDF, SHACL assumes globally unique URIs and a single data graph. But enterprise data is structured around local schemas and typed keys, as in GraphQL, Avro, or SQL. SHACL could not express these patterns, making it difficult to model and validate real-world data across heterogeneous systems. Teams lacked shared authoring practices. Without strong guidelines, teams modeled their ontologies inconsistently breaking semantic interoperability. Even subtle differences in style, structure, or naming led to divergent interpretations and made transpilation harder to define consistently across schemas.\n\nWithout strong guidelines, teams modeled their ontologies inconsistently breaking semantic interoperability. Even subtle differences in style, structure, or naming led to divergent interpretations and made transpilation harder to define consistently across schemas. Ontology tooling lacked support for collaborative modeling. Unlike GraphQL Federation, ontology frameworks had no built-in support for modular contributions, team ownership, or safe federation. Most engineers found the tools and concepts unfamiliar, and available authoring environments lacked the structure needed for coordinated contributions.\n\nTo address these challenges, UDA adopts a named-graph-first information model. Each named graph conforms to a governing model, itself a named graph in the knowledge graph. This systematic approach ensures resolution, modularity, and enables governance across the entire graph. While a full description of UDA\u2019s information infrastructure is beyond the scope of this post, the next sections explain how UDA bootstraps the knowledge graph with its metamodel and uses it to model data container representations and mappings.\n\nUpper is Domain Modeling\n\nUpper is a language for formally describing domains \u2014 business or system \u2014 and their concepts. These concepts are organized into domain models: controlled vocabularies that define classes of keyed entities, their attributes, and their relationships to other entities, which may be keyed or nested, within the same domain or across domains. Keyed concepts within a domain model can be organized in taxonomies of types, which can be as complex as the business or the data system needs them to be. Keyed concepts can also be extended from other domain models \u2014 that is, new attributes and relationships can be contributed monotonically. Finally, Upper ships with a rich set of datatypes for attribute values, which can also be customized per domain.\n\nThe graph representation of the onepiece: domain model from our UI. Depicted here you can see how Characters are related to Devil Fruit, and that each Devil Fruit has a type.\n\nUpper domain models are data. They are expressed as conceptual RDF and organized into named graphs, making them introspectable, queryable, and versionable within the UDA knowledge graph. This graph unifies not just the domain models themselves, but also the schemas they transpile to \u2014 GraphQL, Avro, Iceberg, Java \u2014 and the mappings that connect domain concepts to concrete data containers, such as GraphQL type resolvers served by a Domain Graph Service, Data Mesh sources, or Iceberg tables, through their representations. Upper raises the level of abstraction above traditional ontology languages: it defines a strict subset of semantic technologies from the W3C tailored and generalized for domain modeling. It builds on ontology frameworks like RDFS, OWL, and SHACL so domain authors can model effectively without even needing to learn what an ontology is.\n\nUDA domain model for One Piece. Link to full definition.\n\nUpper is the metamodel for Connected Data in UDA \u2014 the model for all models. It is designed as a bootstrapping upper ontology, which means that Upper is self-referencing, because it models itself as a domain model; self-describing, because it defines the very concept of a domain model; and self-validating, because it conforms to its own model. This approach enables UDA to bootstrap its own infrastructure: Upper itself is projected into a generated Jena-based Java API and GraphQL schema used in GraphQL service federated into Netflix\u2019s Enterprise GraphQL gateway. These same generated APIs are then used by the projections and the UI. Because all domain models are conservative extensions of Upper, other system domain models \u2014 including those for GraphQL, Avro, Data Mesh, and Mappings \u2014 integrate seamlessly into the same runtime, enabling consistent data semantics and interoperability across schemas.\n\nTraversing a domain model programmatically using the Java API generated from the Upper metamodel.\n\nData Container Representations\n\nData containers are repositories of information. They contain instance data that conform to their own schema languages or type systems: federated entities from GraphQL services, Avro records from Data Mesh sources, rows from Iceberg tables, or objects from Java APIs. Each container operates within the context of a system that imposes its own structural and operational constraints.\n\nA Data Mesh source is a data container.\n\nData container representations are data. They are faithful interpretations of the members of data systems as graph data. UDA captures the definition of these systems as their own domain models, the system domains. These models encode both the information architecture of the systems and the schemas of the data containers within. They provide a blueprint for translating the systems into graph representations.", "label": 0}
{"title": "Design system annotations, part 1: How accessibility gets left out of components", "url": "https://github.blog/engineering/user-experience/design-system-annotations-part-1-how-accessibility-gets-left-out-of-components/", "content": "When it comes to design systems, every organization tends to be at a different place in their accessibility journey. Some have put a great deal of work into making their design system accessible while others have a long way to go before getting there. To help on this journey, many organizations rely on accessibility annotations to make sure there are no access barriers when a design is ready to be built.\n\nHowever, it\u2019s a common misconception (especially for organizations with mature design systems) that accessible components will result in accessible designs. While design systems are fantastic for scaling standards and consistency, they can\u2019t prevent every issue with our designs or how we build them. Access barriers can still slip through the cracks and make it into production.\n\nThis is the root of the problem our Accessibility Design team set out to solve.\n\nIn this two-part series, we\u2019ll show you exactly how accessible design system components can produce inaccessible designs. Then we\u2019ll demonstrate our solution: integrating annotations with our Primer components. This allows us to spend less time annotating, increases design system adoption, and reaches teams who may not have accessibility support. And in our next post, we\u2019ll walk you through how you can do the same for your own components.\n\nLet\u2019s dig in.\n\nWhat are annotations and their benefits?\n\nAnnotations are notes included in design projects that help make the unseen explicit by conveying design intent that isn\u2019t shown visually. They improve the usability of digital experiences by providing a holistic picture for developers of how an experience should function. Integrating annotations into our design process helps our teams work better together by closing communication gaps and preventing quality issues, accessibility audit issues, and expensive re-work.\n\nSome of the questions annotations help us answer include:\n\nHow is assistive technology meant to navigate a page from one element to another?\n\nWhat\u2019s the alternative text for informative images and buttons without labels?\n\nHow does content shift depending on viewport size, screen orientation, or zoom level?\n\nWhich virtual keyboard should be used for a form input on mobile?\n\nHow should focus be managed for complex interactions?\n\nOur answers to questions like this\u2014or the lack thereof\u2014can make or break the experience of the web for a lot of people, especially users with disabilities. Some annotation tools are built specifically to help with this by guiding designers to include key details about web standards, platform functionality, and accessibility (a11y).\n\nMost public annotation kits are well suited for teams who are creating new design system components, teams who aren\u2019t already using a design system, or teams who don\u2019t have specialized accessibility knowledge. They usually help annotate things like:\n\nControls such as buttons and links\n\nStructural elements such as headings and landmarks\n\nDecorative images and informative descriptions\n\nForms and other elements that require labels and semantic roles\n\nFocus order for assistive technology and keyboard navigation\n\nGitHub\u2019s annotation\u2019s toolkit\n\nOne of our top priorities is to meet our colleagues where they\u2019re at. We wanted all our designers to be able to use annotations out of the box because we believe they shouldn\u2019t need to be a certified accessibility specialist in order to get things built in an accessible way.\n\nTo this end, last year we began creating an internal Figma library\u2014the GitHub Annotation Toolkit (which we aim to release to the public soon). Our toolkit builds on the legacy of the former Inclusive Design team at CVS Health. Their two open source annotation kits help make documentation that\u2019s easy to create and consume, and are among the most widely used annotation libraries in the Figma Community.\n\nWhile they add clarity, annotations can also add overhead. If teams are only relying on specialists to interpret designs and technical specifications for developers, the hand-off process can take longer than it needs to. To create our annotation toolkit, we rebuilt its predecessor from the ground up to avoid that overhead, making extensive improvements and adding inline documentation to make it more intuitive and helpful for all of our designers\u2014not just accessibility specialists.\n\nDesign systems can also help reduce that overhead. When you audit your design systems for accessibility, there\u2019s less need for specialist attention on every product feature, since you\u2019re using annotations to add technical semantics and specialist knowledge into every component. This means that designers and developers only need to adhere to the usage guidelines consistently, right?\n\nThe problems with annotations and design system components\n\nUnfortunately, it\u2019s not that simple.\n\nAccessibility is not binary\n\nWhile design systems can help drive more accessible design at scale, they are constantly evolving and the work on them is never done. The accessibility of any component isn\u2019t binary. Some may have a few severe issues that create access barriers, such as being inoperable with a keyboard or missing alt text. Others may have a few trivial issues, such as generic control labels.\n\nMost of the time, it will be a misnomer to claim that your design system is \u201cfully accessible.\u201d There\u2019s always more work to do\u2014it\u2019s just a question of how much. The Web Content Accessibility Guidelines (WCAG) are a great starting point, but their \u201cSuccess Criteria\u201d isn\u2019t tailored for the unique context that is your website or product or audience.\n\nWhile the WCAG should be used as a foundation to build from, it\u2019s important to understand that it can\u2019t capture every nuance of disabled users\u2019 needs because your users\u2019 needs are not every user\u2019s needs. It would be very easy to believe that your design system is \u201cfully accessible\u201d if you never look past WCAG to talk to your users. If Primer has accessible components, it\u2019s because we feel that direct participation and input from daily assistive technology users is the most important aspect of our work. Testing plans with real users\u2014with and without disabilities\u2014is where you really find what matters most.\n\nAccessible components do not guarantee accessible designs\n\nArranging a series of accessible components on a page does not automatically create an accurate and informative heading hierarchy. There\u2019s a good chance that without additional documentation, the heading structure won\u2019t make sense visually\u2014nor as a medium for navigating with assistive technology.\n\nIt\u2019s great when accessible components are flexible and responsive, but what about when they\u2019re placed in a layout that the component guidance doesn\u2019t account for? Do they adapt to different zoom levels, viewport sizes, and screen orientations? Do they lose any functionality or context when any of those things change?\n\nComponent usage is contextual. You can add an image or icon to your design, but the design system docs can\u2019t write descriptive text for you. You can use the same image in multiple places, but the image description may need to change depending on context.\n\nSimilarly, forms built using the same input components may do different things and require different error validation messages. It\u2019s no wonder that adopting design system components doesn\u2019t get rid of all audit issues.\n\nDesign system components in Figma don\u2019t include all the details\n\nAnnotation kits don\u2019t include components for specific design systems because almost every organization is using their own. When annotation kits are adopted, teams often add ways to label their design system components.\n\nThis labeling lets developers know they can use something that\u2019s already been built, and that they don\u2019t need to build something from scratch. It also helps identify any design system components that get \u2018detached\u2019 in Figma. And it reduces the number of things that need to be annotated.\n\nLet\u2019s look at an example:\n\nIf we\u2019re using this Primer Button component from the Primer Web Figma library, there are a few important things that we won\u2019t know just by looking at the design or the component properties:\n\nFunctional differences when components are implemented. Is this a link that just looks visually like a button? If so, a developer would use the <LinkButton> React component instead of <Button> .\n\nIs this a link that just looks visually like a button? If so, a developer would use the React component instead of . Accessible labels for folks using assistive technology. The icon may need alt text. In some cases, the button text might need some visually-hidden text to differentiate it from similar buttons. How would we know what that text is? Without annotations, the Figma component doesn\u2019t have a place to display this.\n\nThe icon may need alt text. In some cases, the button text might need some visually-hidden text to differentiate it from similar buttons. How would we know what that text is? Without annotations, the Figma component doesn\u2019t have a place to display this. Whether user data is submitted. When a design doesn\u2019t include an obvious form with input fields, how do we convey that the button needs specific attributes to submit data?\n\nIt\u2019s risky to leave questions like this unanswered, hoping someone notices and guesses the correct answer.\n\nA solution that streamlines the annotation process while minimizing risk\n\nWhen creating new components, a set of detailed annotations can be a huge factor in how robust and accessible they are. Once the component is built, design teams can start to add instances of that component in their designs. When those designs are ready to be annotated, those new components shouldn\u2019t need to be annotated again. In most cases, it would be redundant and unnecessary\u2014but not in every case.\n\nThere are some important details in many Primer components that may change from one instance to another. If we use the CVS Health annotation kit out of the box, we should be able to capture those variations, but we wouldn\u2019t be able to avoid those redundant and unnecessary annotations. As we built our own annotation toolkit, we built a set of annotations for each Primer component to do both of those things at once.\n\nThis accordion component has been thoroughly annotated so that an engineer has everything they need to build it the first time. These include heading levels, semantics for <detail> and <summary> elements, landmarks, and decorative icons. All of this is built into the component so we don\u2019t need to annotate most of this when adding the accordion to our new designs.\n\nHowever, there are two important things we need to annotate, as they can change from one instance to another:\n\nThe optional title at the top. The heading level of each item within the accordion.\n\nIf we don\u2019t specify these things, we\u2019re leaving it to chance that the page\u2019s heading structure will break or that the experience will be confusing for people to understand and navigate the page. The risks may be low for a single button or basic accordion, but they grow with pattern complexity, component nesting, interaction states, duplicated instances, and so on.\n\nInstead of annotating what\u2019s already built into the component or leaving these details to chance, we can add two quick annotations. One Stamp to point to the component, and one Details annotation where we fill in some blanks to make the heading levels clear.\n\nBecause the prompts for specific component details are pre-set in the annotation, we call them Preset annotations.\n\nIntroducing our Primer A11y Preset annotations\n\nWith this proof of concept, we selected ten frequently used Primer components for the same treatment and built a new set of Preset annotations to document these easily missed accessibility details\u2014our Primer A11y Presets.\n\nThose Primer components tend to contribute to more accessibility audit issues when key details are missing on implementation. Issues for these components relate to things like lack of proper labels, error validation messages, or missing HTML or ARIA attributes.\n\nEach of our Preset annotations is linked to component docs and Storybook demos. This will hopefully help developers get straight to the technical info they need without designers having to find and add links manually. We also included guidance for how to fill out each Preset, as well as how to use the component in an accessible way. This helps designers get support inline without leaving their Figma canvas.\n\nWant to create your own? Check out Design system annotations, part 2\n\nButton components in Google\u2019s Material Design and Shopify\u2019s Polaris, IBM\u2019s Carbon, or our Primer design system are all very different from one another. Because Preset annotations are based on specific components, they only work if you\u2019re also using the design system they\u2019re made for.\n\nIn part 2 of this series, we\u2019ll walk you through how you can build your own set of Preset annotations for your design system, as well as some different ways to document important accessibility details before development starts.\n\nYou may also like:\n\nIf you\u2019re more of a visual learner, you can watch Alexis Lucio explore Preset annotations during GitHub\u2019s Dev Community Event to kick off Figma\u2019s Config 2024.\n\nTags:", "label": 0}
{"title": "One of us", "url": "https://shellsharks.com/notes/2024/05/14/one-of-us", "content": "Mike Sass\n\n@shellsharks\n\nI recently came across this post where the author laments that the IndieWeb is \u201cnot for them\u201d, simply because of their inability to implement Webmentions and thus (as they put it) the \u201cIndieWeb is a social club for developers\u201d only. (See also this one with respect to not feeling part of the \u201ccommunity\u201d.)\n\nLet me be extremely clear. You do not need Webmentions to be part of the \u201cIndieWeb\u201d. The only real \u201crequirement\u201d, if you want to think of it that way, is to just have your own site on your own domain where you put your own stuff. That\u2019s it. How you make it look, what you put on there, what fancy IndieWeb.org \u201cbuilding blocks\u201d you decide to implement, doesn\u2019t make your site any more or less \u201cIndieWeb\u201d than any other.\n\nIndieweb.org is a great resource. But I fear that for as much good as it does, it can do equal harm. On the surface, Indieweb.org has a fantastic message.\n\nWhat is the IndieWeb? The IndieWeb is a people-focused alternative to the \u201ccorporate web\u201d. It is a community of independent and personal websites connected by open standards and based on the principles of: owning your domain and using it as your primary online identity, publishing > on your own site first (optionally elsewhere), and owning your content. Your content is yours When you post something on the web, it should belong to you, not a corporation. Too many companies have gone out of business and lost all of their users\u2019 data. By joining the IndieWeb, your content stays yours and in your control. You are better connected Your articles and status messages can be distributed to any platform, not just one, allowing you to engage with everyone. Replies and likes on other services can come back to your site so they\u2019re all in one place. You are in control You can post anything you want, in any format you want, with no one monitoring you. In addition, you share simple readable links such as example.com/ideas. These links are permanent and will always work.\n\nThe message is simple. The IndieWeb is about people. It\u2019s about owning your own domain and putting your stuff there. Great! But just below the surface, as you start to click on some of the links, IndieWeb.org begins to preach complexity. IndieAuth, Webmention, Micropub, WebSub, Microsub, building blocks, microformats, backfeeds, etc\u2026 These convolutions are niche, techno-aristocratic IndieWeb fever dreams which discourage and alienate those desperate to break from corporate web-silos and start anew on a simpler, more human web. Ignore them.\n\nA note on \u201ccommunity\u201d\u2026\n\nIf you are reading this, or you have a personal site, you are already part of it. Full stop. You don\u2019t need to be shy. You have things to say and people WANT to listen, they want to read, they want to connect. You don\u2019t have to be an \u201cinfluencer\u201d or have a big following. Your writing doesn\u2019t need to be AMAZING. You don\u2019t need to write to an audience. Write for yourself. Then share it. People will be interested I promise you. The #indieweb revival is here and you are 100% invited! Tell your friends.\n\nSo, hopefully this message can make it out to those who need to hear it\u2026\n\nYou want to be part of the IndieWeb? Be one of us? Get a domain. Put your site on it. Be yourself. Because the IndieWeb is about you.", "label": 1}
{"title": "Read in 2025 \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/reading/read-in-2025/", "content": "Jump to: Books read in 2025 | Did Not Finish | Other Years\u2019 Reading | Currently reading\n\nBooks Read in 2025\n\n* indicates re-reads. \ud83d\udc4d = enjoyed a lot \ud83d\udc4d\ud83d\udc4d = loved\n\nPresented in reverse order. Links go to my review / notes.\n\nShow all Favorites \ud83d\udc4d Romance \ud83e\udd0d Everything but romance Sci-fi \ud83d\ude80 Fantasy Mystery Novella Art + Design Comics Non-fiction Cookbook LGBTQIA+ \ud83c\udf08 Self-pub Re-read Owned\n\nJump to top\n\nBooks I didn\u2019t finish reading Did Not Finish The Grimoire Grammar School Parent Teacher Association\n\nby Caitlin Rozakis\n\n(paranormal, humor)\n\nDNF 8% \u2014 I think this is going to be more focused on the parenting aspect than I\u2019m interested in, and the POV character\u2019s issues about fitting in aren\u2019t appealing\n\nby Caitlin Rozakis (paranormal, humor) DNF 8% \u2014 I think this is going to be more focused on the parenting aspect than I\u2019m interested in, and the POV character\u2019s issues about fitting in aren\u2019t appealing Reborn\n\nby Seth Haddon\n\n(romance, fantasy)\n\nDNF 8% \u2014 one character way too religious and devoted to the emperor for my taste\n\nby Seth Haddon (romance, fantasy) DNF 8% \u2014 one character way too religious and devoted to the emperor for my taste When the Tides Held the Moon\n\nby Venessa Vida Kelley\n\n(fantasy)\n\nDNF first chapter \u2014 wasn\u2019t feeling it\n\nby Venessa Vida Kelley (fantasy) DNF first chapter \u2014 wasn\u2019t feeling it Evocation\n\nby S.T. Gibson\n\n(fantasy, romance)\n\nDNF chapter two \u2014 didn\u2019t like either POV character\n\nby S.T. Gibson (fantasy, romance) DNF chapter two \u2014 didn\u2019t like either POV character How to Steal a Galaxy\n\nby Beth Revis\n\n(sci-fi, romance)\n\nthought it was standalone but it\u2019s the middle book \u2014 not sold from what I read of this one to go back and read the first\n\nby Beth Revis (sci-fi, romance) thought it was standalone but it\u2019s the middle book \u2014 not sold from what I read of this one to go back and read the first Gentle Rogue\n\nby Johanna Lindsey\n\n(romance, historical)\n\nDNF chapter 2 \u2014 we are jumping between too many POV characters\n\nby Johanna Lindsey (romance, historical) DNF chapter 2 \u2014 we are jumping between too many POV characters Rogue Enforcer\n\nby Grace Goodwin\n\n(romance, sci-fi)\n\nDNF 55%ish \u2014 hero rubbed me the wrong way, overprotective and anxious\n\nby Grace Goodwin (romance, sci-fi) DNF 55%ish \u2014 hero rubbed me the wrong way, overprotective and anxious Immediacy\n\nby Anna Kornbluh\n\n(theory, culture)\n\nhated the writing\n\nby Anna Kornbluh (theory, culture) hated the writing Never Marry a Scandalous Duke\n\nby Renee Ann Miller\n\n(romance, historical)\n\nDNF 37% \u2014 hero is about to make her move her permanent office because he\u2019s annoyed by a week of shelf installation? heroine wants to be a proper lady so she resolves to lay back and think of England? nah\n\nby Renee Ann Miller (romance, historical) DNF 37% \u2014 hero is about to make her move her permanent office because he\u2019s annoyed by a week of shelf installation? heroine wants to be a proper lady so she resolves to lay back and think of England? nah The North Wind\n\nby Alexandria Warwick\n\n(romantasy)\n\nDNF 45% the heroine is pretty loathsome and obnoxious, also the love interest Darth Vader choked her at one point\n\nby Alexandria Warwick (romantasy) DNF 45% the heroine is pretty loathsome and obnoxious, also the love interest Darth Vader choked her at one point The Fireborne Blade\n\nby Charlotte Bond\n\n(fantasy)\n\nDNF chapter 4 \u2014 didn\u2019t like the alternating book and story format\n\nby Charlotte Bond (fantasy) DNF chapter 4 \u2014 didn\u2019t like the alternating book and story format Triad\n\nby Kate Pearce\n\n(romance, sci-fi)\n\nDNF first chapter \u2014 wasn\u2019t feeling it\n\nby Kate Pearce (romance, sci-fi) DNF first chapter \u2014 wasn\u2019t feeling it Hearts of Oak\n\nby Eddie Robson\n\n(sci-fi)\n\nDNF page 10 \u2014 strange and distant\n\nby Eddie Robson (sci-fi) DNF page 10 \u2014 strange and distant Mastered by Her Mates\n\nby Grace Goodwin\n\n(romance, sci-fi)\n\nDNF 50% \u2014 didn\u2019t like one of the male leads, the heroine was annoying\n\nby Grace Goodwin (romance, sci-fi) DNF 50% \u2014 didn\u2019t like one of the male leads, the heroine was annoying Over the Line\n\nby Elise Faber\n\n(romance, contemporary)\n\nDNF 5% \u2014 heroine was crazy\n\nby Elise Faber (romance, contemporary) DNF 5% \u2014 heroine was crazy Disenchanted & Co.\n\nby Lynn Viehl\n\n(romance, steampunk)\n\nDNF 12% \u2014 first encounter with the love interest reads as assault\u2026 shame because I quite liked the main character and found the world interesting\n\nby Lynn Viehl (romance, steampunk) DNF 12% \u2014 first encounter with the love interest reads as assault\u2026 shame because I quite liked the main character and found the world interesting On Writing and Failure\n\nby Stephen Marche\n\n(non-fiction, essay)\n\nDNF page 31 \u2014 didn\u2019t like the tone or message\n\nby Stephen Marche (non-fiction, essay) DNF page 31 \u2014 didn\u2019t like the tone or message A Taste of Gold and Iron\n\nby Alexandra Rowland\n\n(romance, fantasy)\n\nDNF 6% \u2014 put it down and never went back\n\nby Alexandra Rowland (romance, fantasy) DNF 6% \u2014 put it down and never went back Mercenary Courage\n\nby Ruby Lionsdrake\n\n(romance, sci-fi)\n\nDNF 28% \u2014 wasn\u2019t feeling the conflict\n\nby Ruby Lionsdrake (romance, sci-fi) DNF 28% \u2014 wasn\u2019t feeling the conflict And Then He Kissed Me\n\nby C.M. Daniels\n\n(romance, historical)\n\nDNF 24% \u2014 slow going\n\nby C.M. Daniels (romance, historical) DNF 24% \u2014 slow going A Lady\u2019s Guide to Marvels and Misadventure\n\nby Angela Bell\n\n(romance, historical)\n\nDNF chapter 1 \u2014 heroine comes across insufferable\n\nby Angela Bell (romance, historical) DNF chapter 1 \u2014 heroine comes across insufferable Everyone in My Family Has Killed Someone\n\nby Benjamin Stevenson\n\n(mystery)\n\nDNF after chapter 1 \u2014 don\u2019t like the narrator\n\nby Benjamin Stevenson (mystery) DNF after chapter 1 \u2014 don\u2019t like the narrator Noor\n\nby Nnedi Okorafor\n\n(sci-fi)\n\nDNF after prologue \u2014 don\u2019t like the flash-forward technique\n\nby Nnedi Okorafor (sci-fi) DNF after prologue \u2014 don\u2019t like the flash-forward technique Faring Soul\n\nby Tracy Cooper-Posey\n\n(sci-fi, romance)\n\nDNF after chapter 1 \u2014 wasn\u2019t feeling it\n\nby Tracy Cooper-Posey (sci-fi, romance) DNF after chapter 1 \u2014 wasn\u2019t feeling it Strange Beasts\n\nby Susan J. Morris\n\n(mystery, steampunk)\n\nDNF after chapter 1 \u2014 heroine\u2019s motives are being concealed from the reader\n\nby Susan J. Morris (mystery, steampunk) DNF after chapter 1 \u2014 heroine\u2019s motives are being concealed from the reader A Letter to the Luminous Deep\n\nby Sylvie Cathrall\n\n(romance, fantasy)\n\nDNF third page \u2014 too much description, boring\n\nby Sylvie Cathrall (romance, fantasy) DNF third page \u2014 too much description, boring Miss Lattimore\u2019s Letter\n\nby Suzanne Allain\n\n(romance, historical)\n\nDNF 33% \u2014 just fine\n\nby Suzanne Allain (romance, historical) DNF 33% \u2014 just fine Letters to Half Moon Street\n\nby Sarah Wallace\n\n(romance, historical)\n\nDNF 20% \u2014 I think all-epistolary is holding this back, was too slow for me\n\nby Sarah Wallace (romance, historical) DNF 20% \u2014 I think all-epistolary is holding this back, was too slow for me A Vile Season\n\nby David Ferraro\n\n(fantasy, romance)\n\nDNF chapter 3 \u2014 fun but not quite feeling it at the moment\n\nby David Ferraro (fantasy, romance) DNF chapter 3 \u2014 fun but not quite feeling it at the moment Annie Bot\n\nby Sierra Greer\n\n(sci-fi, literary)\n\nDNF 14% \u2014 too much abuse for my preferences\n\nby Sierra Greer (sci-fi, literary) DNF 14% \u2014 too much abuse for my preferences Alien Desire\n\nby Melody Beckett\n\n(romance, sci-fi)\n\nDNF 50% \u2014 didn\u2019t like where the story was going\n\nby Melody Beckett (romance, sci-fi) DNF 50% \u2014 didn\u2019t like where the story was going Lady Eve\u2019s Last Con\n\nby Rebecca Fraimow\n\n(sci-fi, heist)\n\ndon\u2019t really like heist stories\n\nby Rebecca Fraimow (sci-fi, heist) don\u2019t really like heist stories Fiasco\n\nby Constance Fay\n\n(romance, sci-fi)\n\nDNF first chapter \u2014 didn\u2019t like narrator / first person / present tense\n\nby Constance Fay (romance, sci-fi) DNF first chapter \u2014 didn\u2019t like narrator / first person / present tense Eryx\n\nby Kate Stevens\n\n(romance, sci-fi)\n\nDNF 30% \u2014 this bears little resemblance to the blurb\n\nby Kate Stevens (romance, sci-fi) DNF 30% \u2014 this bears little resemblance to the blurb Comeuppance Served Cold\n\nby Marion Deeds\n\n(mystery, fantasy)\n\nDNF 14% \u2014 unbelievable and boring infodump conversations, confusing time jumps between chapters\n\nby Marion Deeds (mystery, fantasy) DNF 14% \u2014 unbelievable and boring infodump conversations, confusing time jumps between chapters The Four Dimensional Human\n\nby Laurence Scott\n\n(non-fiction, technology)\n\ntaking too long to get to the point\n\nby Laurence Scott (non-fiction, technology) taking too long to get to the point Elements of Taste\n\nby Benjamin Errett\n\n(cultural analysis)\n\nDNF after intro \u2014 idea of mapping flavors to taste profiles sounds neat but doesn\u2019t bear out\n\nby Benjamin Errett (cultural analysis) DNF after intro \u2014 idea of mapping flavors to taste profiles sounds neat but doesn\u2019t bear out Annihilation\n\nby Jeff Vandermeer\n\n(sci-fi, weird)\n\nDNF 20% \u2014 unreliable narrator, gimmick of no names, wasn\u2019t hooked by hook\n\nJump to top", "label": 1}
{"title": "Simulating a neural operating system with Gemini 2.5 Flash-Lite", "url": "https://developers.googleblog.com/en/simulating-a-neural-operating-system-with-gemini-2-5-flash-lite/", "content": "In traditional computing, user interfaces are pre-defined. Every button, menu, and window is meticulously coded by developers. But what if an interface could be generated in real time, adapting to a user's context with each interaction? We explored this question by building a research prototype (view demo app in Google AI Studio) for a generative, infinite computer experience. Our prototype simulates an operating system where each screen is generated on the fly by a large language model. It uses Gemini 2.5 Flash-Lite, a model whose low latency is critical for creating a responsive interaction that feels instantaneous. Instead of navigating a static file system, the user interacts with an environment that the model builds and rebuilds with every click. This post outlines the core technical concepts behind this prototype.\n\nSorry, your browser doesn't support playback for this video\n\nConditioning the model for on-the-fly UI generation To generate a UI on-the-fly, we need to provide the model with a clear structure and context for each request. We engineered our prompt by dividing the model's input into two parts: a \"UI constitution\" and a \"UI interaction\". The UI constitution is a system prompt that contains a fixed set of rules for UI generation. These rules define consistent elements like the OS-level styling, the home screen format, and logic for embedding elements like maps. The UI interaction is a JSON object that captures the user's most recent action, such as a mouse click on an icon. This object serves as the specific query that prompts the model to generate the next screen. For example, clicking on a \u201cSave Note\u201d icon within the Notepad app may generate an object as the following:\n\n{ // `id`: The unique ID from the button's `data-interaction-id` attribute. id: 'save_note_action', // `type`: The interaction type from `data-interaction-type`. type: 'button_press', // `value`: Because the button has a `data-value-from` attribute, the system // retrieves the content from the textarea with the ID 'notepad_main_textarea'. value: 'Meeting notes\n\n- Discuss Q3 roadmap\n\n- Finalize budget', // `elementType`: The HTML tag of the element that was clicked. elementType: 'button', // `elementText`: The visible text inside the button. elementText: 'Save Note', // `appContext`: The ID of the application the user is currently in. // This comes from the `activeApp` state in `App.tsx`. appContext: 'notepad_app' } JSON Copied", "label": 0}
{"title": "Generative AI as a magic system \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/24/generative-ai-as-a-magic-system/", "content": "We treat generative AI like magic\u2026 and magic systems have rules. When creating fantasy worlds, writers think about who can use magic, how magic is performed, what it\u2019s able to do, what its constraints are, what the source of magic is, and what it costs. I\u2019m applying a bit of reverse worldbuilding to the real world to extrapolate the rules of the AI magic system.\n\nIslands in the Sky by Death Valley Girls\n\nWho can use AI magic: magic users pay to use corporate AI magic systems. Those who are wealthy and tech savvy enough can host their own local model. Free magic use is mostly limited to corporate largesse ultimately intended to build magic dependency.\n\nHow AI magic is cast: AI spells are cast with written text input through a digital interface. Spells are refined and recast until the outcome satisfies (spells produce different results every time they are cast).\n\nWhat AI magic can do: AI spells can produce combinations of words that are interpreted as writing, code-like material that sometimes runs as code, images that resemble art, and video that resembles reality. It can create imitations of specific human creators\u2019 work, as well as individual\u2019s speech and appearance. It can also mimic human conversation for a span of time before the spell dissipates. AI magic is near instantaneous, allowing people without technical skills to produce text and graphics faster than writers and artisans.\n\nWhat AI magic cannot do: AI magic cannot produce the same outcome twice, nor act upon existing conjurations, instead casting spells anew each time. AI magic itself cannot reference sources, though may be used in tandem with other tools that enable citation (though with questionable accuracy). AI magic cannot reason or write, but its conjurations may create the illusion of intelligence through their statistical consistency with written language use.\n\nThe source of AI magic: AI magic derives from statistical analysis of human-created art, writing, speech, music, and video, classified and sorted by human laborers in low-cost geos.\n\nThe cost of AI magic: Resource costs of AI magic include power, water, and high-end chips, which themselves require specialized manufacturing and rare earth minerals.\n\nSocial costs include the reinforcement of racism and sexism, as well as mental harm to AI trainers assessing inputs to the magic system.\n\nSocietal costs include job elimination and job intensification as positions able to be reproduced in part by magic are eliminated and that magic work is shifted to the remaining workers.\n\nInformation costs include the destruction of the online publishing incentive structure / information commons, leading to more paywalled content; an increase in low-quality material, which makes finding accurate information harder; as well as the danger of political propaganda by poisoned magic systems.\n\nIndividual user costs include critical thinking skills, writing abilities, and patience for conversing with humans.\n\nFurther reading:\n\nThe new magic of AI vs. the old magic of artists by Kening Zhu\n\nSee also:\n\nGenerative AI and the Business Borg aesthetic", "label": 1}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2023-06", "content": "en\n\nI have an entire backlog of movies to watch. Yes, I apparently live under a rock. I made sure Spider-Man: Across the Spider-Verse didn't fall into this black whole.\n\nI'm not going to share any spoilers. I'll try at least. The movie was amazing. I really enjoyed every second of the movie, It was jam packed! So much action, so much emotion, and so many realizations. It's cool to connect the dots from the Marvel Cinematic Universe, whether or not it was intentional.\n\nThere were so many plot twists. So many ways things can go. You know the Spider-Verse.....? Thanks for inviting me Lisa and Luke.\n\nIt made me think about many of the decisions I made in life. It's insane how many of my memories growing up ran across. I had a few tears and emotions in the scenes between Miles and his mom. She reminded me about my mom and the close relationship I have with her. My mom is such an amazing person. I also had so many tears and emotions when I saw the interactions between Miles and his dad. My dad is also an amazing person as well. We are also close. Lot's of emotions and tears as well. It reminded me about what it was like to grow up Latino. Obviously, my experience is different but I see many shared experiences of some level.\n\nThere were many funny scenes that I was rolling over. Sorry neighbor. The funniest scenes were the ones that were subtle.\n\nI can't wait to see the next one that will come out, I will probably re watch the entire two movies again and more.\n\nI want to write more but I don't want to give spoilers.", "label": 1}
{"title": "Supercharge your notebooks: The new AI-first Google Colab is now available to everyone", "url": "https://developers.googleblog.com/en/new-ai-first-google-colab-now-available-to-everyone/", "content": "Last month at Google I/O 2025, we shared our vision for a reimagined, AI-first Colab, a true coding partner in your notebook designed to help you tackle your most challenging problems faster than ever. We\u2019ve started by rolling these features out to a small group of users, and the response has been incredible. Today, we are thrilled to make AI-first Colab available to everyone!\n\n\n\nEarly users have been embracing Colab's new agentic capabilities to accelerate their projects, learn new skills, and unlock insights from their data in ways that have delighted and inspired us.\n\n\n\nFrom early access to everyday productivity\n\nOur goal was to build an AI collaborator that understands your intentions and integrates seamlessly into your workflow. Based on user feedback, this new experience is already making a significant impact.\n\nHere are a few of the top ways people are using the new Colab AI:\n\n\n\n1: Accelerating End-to-End Machine Learning Projects\n\nUsers are leveraging Colab AI for the entire machine learning lifecycle. From taking a raw dataset and asking the agent to autonomously perform cleaning and preparation, to generating feature analysis, training models, and evaluating the results. This turns hours of work into a guided, conversational experience.\n\n\n\n2: Smarter Debugging\n\nCoding is an iterative process. Colab AI acts as a pair programmer to help you prototype ideas, generate boilerplate code, and understand new libraries. When you hit an error, the AI doesn't just help find the bug, it can suggest a fix in a clear diff view, helping you learn and keep going with your project. The result is a massive boost in productivity.\n\n\n\n3: Creating Stunning Visualizations with Zero Hassle\n\nData exploration is incomplete without visualization. Users are simply asking Colab AI to graph their data, and it generates high-quality, clearly labeled charts without the need for manual tweaking of plotting libraries.\n\n\n\nA quick look at the features powering your workflow\n\nThese use cases are powered by a suite of new, deeply integrated features:\n\nIterative Querying: A conversational experience where you can ask for code, get explanations about libraries, and intelligently fix errors.\n\nNext-Generation Data Science Agent (DSA): Trigger autonomous analytical workflows. The agent creates a plan, executes code, reasons about the results, and presents its findings, all while allowing you to provide feedback and stay in control.\n\nEffortless Code Transformation: Simply describe a change in natural language, and Colab will identify and refactor the relevant code for you.\n\n\n\nGet started with Colab AI today!\n\nWe are incredibly excited to put these powerful new capabilities into the hands of the entire Colab community. This is a major step in our journey to create a more powerful and intuitive AI-first Colab, and we\u2019re just getting started.\n\nReady to try it out? It\u2019s easy to get started:\n\n1: Open any new or existing notebook in Google Colab.\n\n2: Look for the Gemini spark icon in the bottom toolbar.", "label": 0}
{"title": "Oh that holiday spirit!!!", "url": "https://lifeofpablo.com/blog/oh-that-holiday-spirit", "content": "Oh that holiday spirit!!!\n\nThis post was written in English (en_US).\n\n\"\n\nSo who is freezing their but off? That would be me! Well we can say that winter has officially kicked in, so has the break. So it is the time to throw your shoes and load up that good ol' Netflix account. We all need to catch up on the shows that we have abandoned lately.\n\nIt has been a while since I wrote last. Let me tell you, it has been a quite the interesting semester.I stayed busy with all my activities especially with my fraternity. I'm a now an initiated member of Alpha Tau Omega. Being part of Greek life, it has opened me many doors. On my free time, I've been working out a lot lately. Since the beginning of the school year, I managed to lose over 20 pounds. What a difference it has made! I feel great. I want to thank my workout buddy for helping me out!! Props to you man for putting up with me!\n\nAll my finals went well. Let's say that I was not myself during dead week. Maybe I can make a zombie reference?? I passed all my tests and ended on a well note for the semester. There is always room for improvement.\n\nThe first day back was just filled with a lot of things to do and with a lot of surprises. Good things after another occurred.\n\nIt feels great hanging out with my family. I've missed them so much. While back in the dorm, it seemed that I really did not missed them much. Once I got home, I realized how much they were truly a missing part of me. I missed my mom's home cooked meals. What a relief that I can stay away from the cafeteria food.\n\nThen I went to my high school to see the varsity basketball game. Go Tigers!! They kicked some ass when they beat York 65-38. The game was intense. How I miss the basketball games. Once a tiger always a tiger! It was great seeing some old friends.\n\nI am glad to have made many friends this semester. I appreciate everyone of you. Let's see what is in stock for us this coming semester.\n\nHappy Holidays everyone!! Stay warm and make a snowman (if we can get enough snow.)\"", "label": 1}
{"title": "Learn to build and run AI powered apps at Firebase Demo Day \u201824", "url": "https://developers.googleblog.com/en/firebase-demo-day-24/", "content": "Welcome to Firebase Demo Day 2024\n\nWe just released 8 bite sized demo videos to showcase how Firebase helps you build and run AI-powered apps. We\u2019ll show you how to use new Firebase products and features like Firebase Genkit, Vertex AI in Firebase, Gemini in Firebase and Firebase App Hosting, to build AI features into your existing applications, monitor their performance, and create great experiences for your users.\n\nTo bring these concepts to life, we'll take you on an app dev journey through Compass, our sample travel app. We\u2019ll demonstrate how you can use Firebase to create features like personalized recommendations, smart itineraries, AI-powered chatbots, and more. Follow along as we highlight how you can leverage Firebase tools to add the same cutting-edge functionality to your own apps.\n\n\n\nWatch Firebase Demo Day 2024 from anywhere at any time, at your own pace.\n\n\n\n\n\n\n\nDemos to build AI-powered features\n\nWatch as we transform our travel app with the power of Firebase and AI. Our build demos show you how to build and deploy AI features with new Firebase products like Vertex AI, Genkit and Firebase Hosting, all while leveraging Firebase's fully managed infrastructure to get to market quickly and securely.\n\n\n\nCall Gemini from your Android app\n\nIntegrate the power of Gemini directly into your Android app using the native Vertex AI in Firebase SDK for Android to make calls to Gemini.", "label": 0}
{"title": "Heroku AI: Build and Deploy Enterprise Grade MCP Servers", "url": "https://www.heroku.com/blog/building-mcp-servers-on-heroku/", "content": "Agents hold immense power, but their true potential shines when they connect to the real world, fetching data, triggering actions, or leveraging external tools. The Model Context Protocol (MCP) offers a standardized way for AI agents to do this.\n\nMCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools. \u2013 official MCP website\n\nHeroku Managed Inference and Agents dramatically simplifies hosting these MCP servers and making them available, not only to itself, but also to external agents like Claude, Cursor, or Agentforce. These new capabilities accelerate industry standardization towards agent interoperability by reducing the infrastructure, security, and discovery challenges in building and running MCP servers. Heroku Managed Inference and Agents provides:\n\nCommunity SDK support : Build your servers using the official MCP SDK, or any other MCP SDK of your choice.\n\n: Build your servers using the official MCP SDK, or any other MCP SDK of your choice. Effortless Management : Once you have a server running, set up your Procfile and push to Heroku. The Managed Inference and Agents add-on automatically manages server registration with the MCP Toolkit.\n\n: Once you have a server running, set up your Procfile and push to Heroku. The Managed Inference and Agents add-on automatically manages server registration with the MCP Toolkit. Unified Endpoint : Managed Inference and Agents automatically has access to all registered servers. Additionally, a MCP Toolkit URL is generated, which can be used to access your servers in external clients.\n\n: Managed Inference and Agents automatically has access to all registered servers. Additionally, a MCP Toolkit URL is generated, which can be used to access your servers in external clients. Only Pay for What You Use: MCP servers managed by the MCP Toolkit are spun up when in use, and are spun down when there are no requests.\n\nThis guide walks you through setting up your own MCP server on Heroku and enabling your Agent to securely and efficiently perform real-world tasks.\n\nBefore getting started\n\nMCP Servers are just like any other software application, and therefore can be deployed to Heroku as standalone apps. So while you could build your own multi-tenant SSE server and deploy it yourself, Heroku MCP Toolkits help you do things that standalone servers cannot do.\n\nFirst and foremost, they make it seamless to integrate servers with your Heroku Managed Inference and Agents. Secondly, they allow tools to be scaled to 0 by default, and spun up only when needed \u2013 making them more cost efficient for infrequent requests. Thirdly, they provide code isolation which enables secure code execution for LLM generated code. Finally, they wrap multiple servers in a single url making it incredibly easy to connect with external clients.\n\nGetting started: Create and deploy your first MCP Server\n\nStep 1 \u2013 Build your Server Use an official MCP SDK to create an MCP Server. Note: At this stage, Heroku MCP Toolkits only support STDIO servers. We are working on streamlining platform support for SSE/http servers with authentication. MCP Servers are normal Heroku Apps built on the language of your choice. For example, if you are using node, you\u2019ll want to follow best practices and ensure your node and npm engines are set in your package.json like you would typically for a node app on Heroku. Step 2 \u2013 Add the MCP process type Define your MCP process via Procfile with a process prefix of mcp* . E.g. mcp-heroku: npm start (example) Step 3 \u2013 Deploy your server Once your app is deployed, all mcp* process types will be ready to be picked up by the Heroku Managed Inference and Agents add-on.\n\nFor more examples, take a look at the sample servers listed in our dev center documentation.\n\nCreating an MCP Toolkit\n\nAttach the Heroku Managed Inference and Agents add-on to the app that you just created. This will register any apps defined in the app to the MCP Toolkit. Each new Managed Inference and Agents add-on will correspond to a new MCP Toolkit.\n\nNavigate to Your App: Open your application\u2019s dashboard on Heroku. Go to Resources: Select the \u201cResources\u201d tab. Add Managed Inference and Agents: Search for \u201cManaged Inference and Agents\u201d in the add-ons section and add it to your app.\n\nWhat plan to select\n\nEach Managed Inference and Agents plan has a corresponding model (ex. Claude 3.5 Haiku or Stable Image Ultra). You should select the model that aligns with your needs. If your goal is to give your model access to MCP tools, then you will need to select one of the Claude chat models. If you have no need for a model, and only want to host MCP tools for external use, that can be done by selecting any plan. Inference usage is metered, so you will incur no cost if there is no usage of Heroku managed models.\n\nAs far as the MCP servers are concerned, you will pay for the dyno units consumed by the one-off dynos that are spun up. The cost of tool calls depends on the specific dyno tier selected for your app, but the default eco dynos, that is about .0008 cents/second. Each individual tool call is capped at 300 seconds.\n\nIf you decide to host your inference on Heroku, your inference model will have the following default tools free of charge. This includes tools like Code Execution and Document/Web Reader.\n\nManaging and using your MCP Toolkit\n\nThe MCP Toolkit configuration can be viewed and managed through a user-friendly tab in the Heroku Managed Inference and Agents add-on. As with all add-ons, navigate to the App Resources page, and click on the Managed Inference and Agents add-on that you provisioned. Navigate to the Tools tab. Here, you will find the following information:\n\nThe list of registered servers, and their statuses The list of tools per server, along with their request schemas\n\nThese tools are all available to your selected Managed Inference model with no extra configuration. Additionally, you will find the MCP Toolkit URL and MCP Toolkit Token on this page, which can be used for integration with external MCP Clients. The MCP Toolkit Token is masked by default for security.\n\nCaution: Your MCP Toolkit Token can be used to trigger actions in your registered MCP servers, so avoid sharing it unless necessary.\n\nFor more information, check out the dev center documentation.\n\nComing soon\n\nWe are actively working on simplifying the process of building SSE/HTTP servers with auth endpoints \u2013 both for Heroku Managed Inference and Agents, and for external MCP clients. This will make it possible for servers to access user specific resources, while adhering to the recommended security standards. Additionally, we are building an in-dashboard playground for Managed Inference and Agents so you can run quick experiments with your models and tools.\n\nWe are excited to see what you build with Heroku Managed Inference and Agents and MCP on Heroku! Attend our webinar on May 28 to see a demo and get your questions answered!", "label": 0}
{"title": "Norman Thavaud - Le comedien fran\u00e7ais", "url": "https://lifeofpablo.com/blog/norman-thavaud-le-comedien-fran%C3%A7ais", "content": "Norman Thavaud - Le comedien fran\u00e7ais\n\nThis post was written in English (en_US).\n\n\"Norman Thavaud (born 14 April 1987) is a French humourist and blogger known for his short comical YouTube videos. Each of his videos have received at least two million views, some receiving over 7 million. (Wikipedia)\n\nNorman Thavaud is a French comedian who has become an internet sensation with his video blogs. He adresses what we would say \"\"first world problems\"\" things like technology. It shows us our modern struggles in a different point of view. He really gets you laughing. Norman has made particpated in many short films and been sponsored by many companies. One in Particular the candy bar Crunch.\n\nWatching his videos has really improved my French skills buy listening and watching a true french person speak. It makes learning another language easier if you get a grip on how it is actually pronounced. Hey he has taught me a lot of slang! Which case is how many young people communicate in this day and age.\n\nI really reccommend his videos to all age groups!\n\nUne s/o a Norman pour tes cool vid\u00e9os! Chaque jour tu m\u2019empresses avec tes blagues!! Je ne peut pas attendre pour la nouvelle vid\u00e9o.\"", "label": 1}
{"title": "Back to \u201cnormal\u201d \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/15/back-to-normal/", "content": "Not to pick on this person, it\u2019s a funny protest sign \u2014 but it seems like a lot of people are thinking like we\u2019re in a \u201cone weird trick\u201d (impeachment) scenario and, once we get that done, we\u2019ll magically return to some type of \u201cnormal\u201d and we can all go home\u2026 but the concept of normality is doing some heavy lifting here.\n\nWhat \u201cnormal\u201d looks like\n\nIt\u2019s normal for minimum wage to stay the same for decades.\n\nIt\u2019s normal that people earning low wages must work multiple jobs and often still qualify for food stamps.\n\nIt\u2019s normal that billionaires exist when low-paid workers have to decide between the power bill and paying the rent.\n\nIt\u2019s normal that disabled people effectively cannot marry, since they\u2019ll often lose access to government support.\n\nIt\u2019s normal to use prisoners as slave labor (or close enough to it with hourly \u201cwages\u201d of pennies).\n\nIt\u2019s normal that immigrants from some countries must wait decades to become citizens because of arbitrary caps.\n\nIt\u2019s normal to let immigrants get paid less and treated worse in tech jobs because they\u2019re trapped by H1B visa requirements.\n\nIt\u2019s normal that *everyone* in America is to some extent trapped in their job because healthcare is ruinously expensive.\n\nIt\u2019s normal that Americans die of preventable disease for lack of money.\n\nIt\u2019s normal that women don\u2019t get to choose what medical treatment they receive if they are pregnant.\n\nIt\u2019s normal to address gun violence by blaming mental health but not increasing funding to expand access.\n\nIt\u2019s normal to \u201csolve\u201d homelessness by throwing away all of someone\u2019s worldly possessions and putting them on a one-way bus out of town.\n\nIt\u2019s normal that kids go hungry at lunch.\n\nIt\u2019s normal that Puerto Ricans and people from D.C. are denied congressional representation.\n\nIt\u2019s normal that 70- and 80-year-olds hold most of the political power in the country, and people under 40 very little.\n\nIt\u2019s normal to hand over all our personal data to megacorporations who run extensive systems of surveillance that they share with law enforcement.\n\nIt\u2019s normal that the government spies on its own citizens.\n\nIt\u2019s normal that hundreds of people die in car crashes daily.\n\nIt\u2019s normal that our spaces are built for cars, not people, and driving is often the only way to get around.\n\nIt\u2019s normal that the US government doesn\u2019t honor its treaties with Tribes.\n\nIt\u2019s normal that we supply Israel with funding while they enact genocide.\n\nIt\u2019s normal that our water rights system is so terrible, and our water use so excessive, that the Colorado River no longer reaches the Gulf of California.\n\nIt\u2019s normal that downstream communities must bear the health burdens and costs of upstream industrial polluters.\n\nThinking bigger than just getting out of this\n\n\u201cNormal\u201d fucking sucks. Outright fascism is worse, but there are plenty of things we need to change. Kat Abughazaleh, who\u2019s running for office in Illinois, says:\n\n\u201cThere\u2019s no reason every American should not be able to afford housing, groceries, health insurance, public transit (ideally), and then still have enough money to save and take your kid to the zoo or go to the movies with your friends. There\u2019s just, there\u2019s no reason \u2014 we are the wealthiest country in the world. The idea that that\u2019s unrealistic or idealistic or naive or even called childish, I think that\u2019s sucky.\u201d\n\nWe don\u2019t have to settle for returning to a normal that sucks \u2014 but protest may not be enough to force change. Our current situation has been the Republican gameplan for decades; we will not escape it by liberating ourselves from a single politician. Trump is a symptom, but the rot runs much deeper. We need to reckon emotionally with the long fights ahead; protests alone will not solve our problems. I\u2019m not saying protest isn\u2019t helpful or important \u2014 I\u2019m saying that we need to find space for both brunch and advocacy in our lives. Micah L. Sifry writes:\n\nOne of the most inspiring ways that pro-democracy movements grow their strength is by inventing and spreading ways for the silent majority to make itself both visible and influential.\n\nI\u2019ve been seeing organizers talking about the need to transform the current energy into long-term work, and not let it fizzle out. Jared Yates Sexton describes the way that authorities rein in public speech:\n\nWe are, according to those who want nothing more than to maintain control, supposed to protest enough, but not too much. Because we should rely on them. Because we need to maintain our subservient position under the party as opposed to in conversation and discourse with the party.\n\nOne strategy: focusing on local advocacy and building up\n\nMy pet theory is that most people should spend their advocacy time supporting local (and state) efforts, and donate to organizations advocating at the federal level. The collapse of local news and intense media focus on national politics encourages people to feel powerless because at the federal level most of what you can do is harass your senator. It also frames politics as something \u201cyou do\u201d once every four years for normies, every two years for the passionate, every election for the wonks and zealots. If you get people involved in local politics, I imagine that get-out-the-vote drives become a lot easier because people are already tuned in.\n\nThe local level is more influential than people realize, I suspect. In Washington State, how environmental policies often play out is that they get implemented at either King County or City of Seattle first, then several suburban cities, and then there\u2019s a model and coalition for building a state-level policy. As former city staff, I can say it\u2019s no accident it works this way \u2014 we knew our colleagues at other cities and would learn from each other, sharing lessons and draft policy language. Regional organizations support, both partnering with cities and going down to Olympia during the legislative session to advocate for (and against) bills. We know that Republicans followed a similar model in the evil direction in red states, building up a portfolio of harmful policies to bring to the federal level in the form of Project 2025\u2026 why don\u2019t we do the same for progressive policies?\n\nIf we start thinking and working now, we can set things up for the next state legislative session, then the next federal election, then the presidential election. (I\u2019m not going down rabbitholes about martial law, I don\u2019t find it helpful to indulge in that kind of despairing fatalism \u2014 I think my personal work on managing catastrophic thinking and anxiety has paid dividends here.)\n\nI haven\u2019t decided what bigger thing I want to work on yet\u2026 for now I\u2019ve just been doing ad hoc advocacy when it\u2019s pointed out by the urbanist Discord server I\u2019m in. I\u2019m pissed that Washington had a school lunch bill that failed in the legislative session, so potentially supporting another run at that\u2026 or badgering the governor about Washington\u2019s regressive taxation policy and demanding an income tax on the wealthiest Washingtonians, which he put the kibosh on this budget cycle and was why the budget was too tight to feed kids \ud83d\ude44 Come on, Bob! We have a lot of tech companies \u2014 and tech CEOs \u2014 here who could stand to pay their fair share. Or I could go harder on supporting housing; I just looked at King County\u2019s affordable housing dashboard, which identifies a need for 44,000 affordable units by 2024, and THE ENTIRE SEATTLE AREA HAD BUILT FEWER THAN FIVE THOUSAND UNITS by 2022. *screaming*\n\nOrganizing coalitions\n\nI\u2019m also spinning on the provocation of the second method of organizing described in the Win the Midwest\u2019s 10-year report (emphasis mine):\n\nThere are two approaches to base building. We can ask what we are for, and then go out and find the people who agree with us. Or we can begin by asking who we need to organize in order to build enough power to shift everything, and then go where they are: workplaces, churches, mosques, synagogues, schools, and childcare centers. Grassroots organizations that meet people where they are \u2013 physically and ideologically \u2013 and that create spaces of belonging, learning, and formation rooted in people\u2019s lives and experiences have an unlimited number of people to organize. Most people do not have clearly defined political outlooks, but they can move into public leadership roles. Organizations willing to put their base at the center of their plans can co-create a political agenda that emerges from the lived experience of their bases.\n\nI\u2019m an extremely opinionated person, and I know what outcomes I want (though I\u2019d rather listen to others on the best tactics on achieving those outcomes), but maybe I could think of this as prioritizing which outcomes to focus on based on which ones enough other people also want and will fight for. L.A. Kauffman discusses building coalitions with those we disagree with, quoting a 1981 talk / 1983 essay \u201cCoalition Politics: Turning the Century\u201d:\n\n\u201cThe reason we are stumbling,\u201d Dr. [Bernice] Reagon declared all those years ago, \u201cis that we are at the point where in order to take the next step we\u2019ve got to do it with some folk we don\u2019t care too much about. And we got to vomit over that for a little while. We must just keep going.\u201d\n\nFurther reading:\n\nWhy Do Anything? by Dj Bracken \u2014 this guy decided to start paying down student lunch debt in Utah, and eventually helped get a statewide bill passed (I was loosely involved in supporting new \u201cshare tables\u201d / fridges at our local elementary schools, AMA)\n\nFrom Aspiration to Action: Organizing Through Exhaustion, Grief, and Uncertainty by Kelly Hayes\n\nSee also:\n\nDon\u2019t let them say it\u2019s normal\n\nExtending my understanding of self-care\n\nBeing a citizen means taking ownership", "label": 1}
{"title": "How the GitHub CLI can now enable triangular workflows", "url": "https://github.blog/open-source/git/how-the-github-cli-can-now-enable-triangular-workflows/", "content": "Most developers are familiar with the standard Git workflow. You create a branch, make changes, and push those changes back to the same branch on the main repository. Git calls this a centralized workflow. It\u2019s straightforward and works well for many projects.\n\nHowever, sometimes you might want to pull changes from a different branch directly into your feature branch to help you keep your branch updated without constantly needing to merge or rebase. However, you\u2019ll still want to push local changes to your own branch. This is where triangular workflows come in.\n\nIt\u2019s possible that some of you have already used triangular workflows, even without knowing it. When you fork a repo, contribute to your fork, then open a pull request back to the original repo, you\u2019re working in a triangular workflow. While this can work seamlessly on github.com, the process hasn\u2019t always been seamless with the GitHub CLI.\n\nThe GitHub CLI team has recently made improvements (released in v2.71.2) to better support these triangular workflows, ensuring that the gh pr commands work smoothly with your Git configurations. So, whether you\u2019re working on a centralized workflow or a more complex triangular one, the GitHub CLI will be better equipped to handle your needs.\n\nIf you\u2019re already familiar with how Git handles triangular workflows, feel free to skip ahead to learn about how to use gh pr commands with triangular workflows. Otherwise, let\u2019s get into the details of how Git and the GitHub CLI have historically differed, and how four-and-a-half years after it was first requested, we have finally unlocked managing pull requests using triangular workflows in the GitHub CLI.\n\nFirst, a lesson in Git fundamentals\n\nTo provide a framework for what we set out to do, it\u2019s important to first understand some Git basics. Git, at its core, is a way to store and catalog changes on a repository and communicate those changes between copies of that repository. This workflow typically looks like the diagram below:\n\nFigure 1: A typical git branch setup\n\nThe building blocks of this diagram illustrate two important Git concepts you likely use every day, a ref and push/pull.\n\nRefs\n\nA ref is a reference to a repository and branch. It has two parts: the remote, usually a name like origin or upstream, and the branch. If the remote is the local repository, it is blank. So, in the example above, origin/branch in the purple box is a remote ref, referring to a branch named branch on the repository name origin, while branch in the green box is a local ref, referring to a branch named branch on the local machine.\n\nWhile working with GitHub, the remote ref is usually the repository you are hosting on GitHub. In the diagram above, you can consider the purple box GitHub and the green box your local machine.\n\nPushing and pulling\n\nA push and a pull refer to the same action, but from two different perspectives. Whether you are pushing or pulling is determined by whether you are sending or receiving the changes. I can push a commit to your repo, or you can pull that commit from my repo, and the references to that action would be the same.\n\nTo disambiguate this, we will refer to different refs as the headRef or baseRef, where the headRef is sending the changes (pushing them) and the baseRef is receiving the changes (pulling them).\n\nFigure 2: Disambiguating headRef and baseRef for push/pull operations\n\nWhen dealing with a branch, we\u2019ll often refer to the headRef of its pull operations as its pullRef and the baseRef of its push operations as its pushRef. That\u2019s because, in these instances, the working branch is the pull\u2019s baseRef and the push\u2019s headRef, so they\u2019re already disambiguated.\n\nThe @{push} revision syntax\n\nTurns out, Git has a handy built-in tool for referring to the pushRef for a branch: the @{push} revision syntax. You can usually determine a branch\u2019s pushRef by running the following command:\n\ngit rev-parse --abbrev-ref @{push}\n\nThis will result in a human-readable ref, like origin/branch, if one can be determined.\n\nPull Requests\n\nOn GitHub, a pull request is a proposal to integrate changes from one ref to another. In particular, they act as a simple \u201cpause\u201d before performing the actual integration operation, often called a merge, when changes are being pushed from ref to another. This pause allows for humans (code reviews) and robots (GitHub Copilot reviews and GitHub Actions workflows) to check the code before the changes are integrated. The name pull request came from this language specifically: You are requesting that a ref pulls your changes into itself.\n\nFigure 3: Demonstrating how GitHub Pull Requests correspond to pushing and pulling\n\nCommon Git workflows\n\nNow that you understand the basics, let\u2019s talk about the workflows we typically use with Git every day.\n\nA centralized workflow is how most folks interact with Git and GitHub. In this configuration, any given branch is pushing and pulling from a remote ref with the same branch name. For most of us, this type of configuration is set up by default when we clone a repo and push a branch. It is the situation shown in Figure 1.\n\nIn contrast, a triangular workflow pushes to and pulls from different refs. A common use case for this configuration is to pull directly from a remote repository\u2019s default branch into your local feature branch, eliminating the need to run commands like git rebase <default> or git merge <default> on your feature branch to ensure the branch you\u2019re working on is always up to date with the default branch. However, when pushing changes, this configuration will typically push to a remote ref with the same branch name as the feature branch.\n\nFigure 4: juxtaposing centralized workflows from triangular workflows.\n\nWe complete the triangle when considering pull requests: the headRef is the pushRef for the local ref and the baseRef is the pullRef for the local branch:\n\nFigure 5: a triangular workflow\n\nWe can go one step further and set up triangular workflows using different remotes as well. This most commonly occurs when you\u2019re developing on a fork. In this situation, you usually give the fork and source remotes different names. I\u2019ll use origin for the fork and upstream for the source, as these are common names used in these setups. This functions exactly the same as the triangular workflows above, but the remotes and branches on the pushRef and pullRef are different:\n\nFigure 6: juxtaposing triangular workflows and centralized workflows with different remotes such as with forks\n\nUsing a Git configuration file for triangular workflows\n\nThere are two primary ways that you can set up a triangular workflow using the Git configuration \u2013 typically defined in a `.git/config` or `.gitconfig` file. Before explaining these, let\u2019s take a look at what the relevant bits of a typical configuration look like in a repo\u2019s `.git/config` file for a centralized workflow:\n\n[remote \u201corigin\u201d] url = https://github.com/OWNER/REPO.git fetch = +refs/heads/*:refs/remotes/origin/* [branch \u201cdefault\u201d] remote = origin merge = refs/heads/default [branch \u201cbranch\u201d] remote = origin merge = refs/heads/branch\n\nFigure 7: A typical Git configuration setup found in .git/config\n\nThe [remote \u201corigin\u201d] part is naming the Git repository located at github.com/OWNER/REPO.git to origin, so we can reference it elsewhere by that name. We can see that reference being used in the specific [branch] configurations for both the default and branch branches in their remote keys. This key, in conjunction with the branch name, typically makes up the branch\u2019s pushRef: in this example, it is origin/branch.\n\nThe remote and merge keys are combined to make up the branch\u2019s pullRef: in this example, it is origin/branch.\n\nSetting up a triangular branch workflow\n\nThe simplest way to assemble a triangular workflow is to set the branch\u2019s merge key to a different branch name, like so:\n\n[branch \u201cbranch\u201d] remote = origin merge = refs/heads/default\n\nFigure 8: a triangular branch\u2019s Git configuration found in .git/config\n\nThis will result in the branch pullRef as origin/default, but pushRef as origin/branch, as shown in Figure 9.\n\nFigure 9: A triangular branch workflow\n\nSetting up a triangular fork workflow\n\nWorking with triangular forks requires a bit more customization than triangular branches because we are dealing with multiple remotes. Thus, our remotes in the Git config will look different than the one shown previously in Figure 7:\n\n[remote \u201cupstream\u201d] url = https://github.com/ORIGINALOWNER/REPO.git fetch = +refs/heads/*:refs/remotes/upstream/* [remote \u201corigin\u201d] url = https://github.com/FORKOWNER/REPO.git fetch = +refs/heads/*:refs/remotes/origin/*\n\nFigure 10: a Git configuration for a multi-remote Git setup found in .git/config\n\nUpstream and origin are the most common names used in this construction, so I\u2019ve used them here, but they can be named anything you want.\n\nHowever, toggling a branch\u2019s remote key between upstream and origin won\u2019t actually set up a triangular fork workflow\u2014it will just set up a centralized workflow with either of those remotes, like the centralized workflow shown in Figure 6. Luckily, there are two common Git configuration options to change this behavior.\n\nSetting a branch\u2019s pushremote\n\nA branch\u2019s configuration has a key called pushremote that does exactly what the name suggests: configures the remote that the branch will push to. A triangular fork workflow config using pushremote may look like this:\n\n[branch \u201cbranch\u201d] remote = upstream merge = refs/heads/default pushremote = origin\n\nFigure 11: a triangular fork\u2019s Git config using pushremote found in .git/config\n\nThis assembles the triangular fork repo we see in Figure 12. The pullRef is upstream/default, as determined by combining the remote and merge keys, while the pushRef is origin/branch, as determined by combining the pushremote key and the branch name.\n\nFigure 12: A triangular fork workflow\n\nSetting a repo\u2019s remote.pushDefault\n\nTo configure all branches in a repository to have the same behavior as what you\u2019re seeing in Figure 12, you can instead set the repository\u2019s pushDefault . The config for this is below:\n\n[remote] pushDefault = origin [branch \u201cbranch\u201d] remote = upstream merge = refs/heads/default\n\nFigure 13: a triangular fork\u2019s Git config using remote.pushDefault found in .git/config\n\nThis assembles the same triangular fork repo as shown in Figure 12 above, however this time the pushRef is determined by combining the remote.pushDefault key and the branch name, resulting in origin/branch.\n\nWhen using the branch\u2019s pushremote and the repo\u2019s remote.pushDefault keys together, Git will preferentially resolve the branch\u2019s configuration over the repo\u2019s, so the remote set on pushremote supersedes the remote set on remote.pushDefault .\n\nUpdating the gh pr command set to reflect Git\n\nPreviously, the gh pr command set did not resolve pushRefs and pullRefs in the same way that Git does. This was due to technical design decisions that made this change both difficult and complex. Instead of discussing that complexity\u2014a big enough topic for a whole article in itself\u2014I\u2019m going to focus here on what you can now do with the updated gh pr command set.\n\nIf you set up triangular Git workflows in the manner described above, we will automatically resolve gh pr commands in accordance with your Git configuration.\n\nTo be slightly more specific, when trying to resolve a pull request for a branch, the GitHub CLI will respect whatever @{push} resolves to first, if it resolves at all. Then it will fall back to respect a branch\u2019s pushremote, and if that isn\u2019t set, finally look for a repo\u2019s remote.pushDefault config settings.\n\nWhat this means is that the CLI is assuming your branch\u2019s pullRef is the pull request\u2019s baseRef and the branch\u2019s pushRef is the pull requests headRef. In other words, if you\u2019ve configured git pull and git push to work, then gh pr commands should just work. The diagram below, a general version of Figure 5, demonstrates this nicely:\n\nFigure 14: the triangular workflow supported by the GitHub CLI with respect to a branch\u2019s pullRef and pushRef. This is the generalized version of Figure 5\n\nConclusion\n\nWe\u2019re constantly working to improve the GitHub CLI, and we\u2019d like the behavior of the GitHub CLI to reasonably reflect the behavior of Git. This was a team effort\u2014everyone contributed to understanding, reviewing, and testing the code to enable this enhanced gh pr command set functionality.\n\nIt also couldn\u2019t have happened without the support of our contributors, so we extend our thanks to them:\n\nCLI native support for triangular workflows was 4.5 years in the making, and we\u2019re proud to have been able to provide this update for the community.\n\nThe GitHub CLI Team\n\n@andyfeller , @babakks , @bagtoad , @jtmcg , @mxie , @RyanHecht , and @williammartin\n\nTags:", "label": 0}
{"title": "Generated content is an invasive species in the online ecosystem \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2024/01/09/generated-content-is-an-invasive-species-in-the-online-ecosystem/", "content": "Invasive species disrupt ecosystems because they did not evolve in balance with the other species. Native species have adapted to fill specific niches, but the constraints they\u2019ve accepted to fit that niche in the ecosystem do not also bind invasive species. Not limited by the same factors, they reproduce faster and crowd out the native species. Time and again, we\u2019ve seen invasive species introduced to an ecosystem outcompete the more specialized native species, sometimes even driving them to extinction.\n\nLikewise, generated imagery and text is not bound by human limitations of productivity. As generated material rapaciously populates the Internet, human-created artworks will be outcompeted by generated graphics on social media platforms by virtue of volume.\n\nAnd corporations are also trying to argue that their products should not be bound by the same legalities that human artists and writers are bound by. Their products only work with copyrighted material, and that means it\u2019s only economically viable if they steal the training data. Like invasive species, they don\u2019t play by the same rules: the rest of us peons must wait 95 years to play with fucking Steamboat Willie, but they get to gobble down anything they want for free instantly and use it to (try to) drive us out of work.\n\nLet\u2019s starve out this species invasion before it collapses our information ecosystem \u270a\n\nSee also:\n\nA 21st century collage tool\n\nWe need solidarity across creative industries\n\nGenerative AI is intellectual sharecropping", "label": 1}
{"title": "Netflix\u2019s Distributed Counter Abstraction", "url": "https://netflixtechblog.com/netflixs-distributed-counter-abstraction-8d0c45eb66b2?source=collection_home---4------11-----------------------", "content": "Netflix\u2019s Distributed Counter Abstraction Netflix Technology Blog 19 min read \u00b7 Nov 12, 2024 -- 22 Listen Share\n\nBy: Rajiv Shringi, Oleksii Tkachuk, Kartik Sathyanarayanan\n\nIntroduction\n\nIn our previous blog post, we introduced Netflix\u2019s TimeSeries Abstraction, a distributed service designed to store and query large volumes of temporal event data with low millisecond latencies. Today, we\u2019re excited to present the Distributed Counter Abstraction. This counting service, built on top of the TimeSeries Abstraction, enables distributed counting at scale while maintaining similar low latency performance. As with all our abstractions, we use our Data Gateway Control Plane to shard, configure, and deploy this service globally.\n\nDistributed counting is a challenging problem in computer science. In this blog post, we\u2019ll explore the diverse counting requirements at Netflix, the challenges of achieving accurate counts in near real-time, and the rationale behind our chosen approach, including the necessary trade-offs.\n\nNote: When it comes to distributed counters, terms such as \u2018accurate\u2019 or \u2018precise\u2019 should be taken with a grain of salt. In this context, they refer to a count very close to accurate, presented with minimal delays.\n\nUse Cases and Requirements\n\nAt Netflix, our counting use cases include tracking millions of user interactions, monitoring how often specific features or experiences are shown to users, and counting multiple facets of data during A/B test experiments, among others.\n\nAt Netflix, these use cases can be classified into two broad categories:\n\nBest-Effort: For this category, the count doesn\u2019t have to be very accurate or durable. However, this category requires near-immediate access to the current count at low latencies, all while keeping infrastructure costs to a minimum. Eventually Consistent: This category needs accurate and durable counts, and is willing to tolerate a slight delay in accuracy and a slightly higher infrastructure cost as a trade-off.\n\nBoth categories share common requirements, such as high throughput and high availability. The table below provides a detailed overview of the diverse requirements across these two categories.\n\nDistributed Counter Abstraction\n\nTo meet the outlined requirements, the Counter Abstraction was designed to be highly configurable. It allows users to choose between different counting modes, such as Best-Effort or Eventually Consistent, while considering the documented trade-offs of each option. After selecting a mode, users can interact with APIs without needing to worry about the underlying storage mechanisms and counting methods.\n\nLet\u2019s take a closer look at the structure and functionality of the API.\n\nAPI\n\nCounters are organized into separate namespaces that users set up for each of their specific use cases. Each namespace can be configured with different parameters, such as Type of Counter, Time-To-Live (TTL), and Counter Cardinality, using the service\u2019s Control Plane.\n\nThe Counter Abstraction API resembles Java\u2019s AtomicInteger interface:\n\nAddCount/AddAndGetCount: Adjusts the count for the specified counter by the given delta value within a dataset. The delta value can be positive or negative. The AddAndGetCount counterpart also returns the count after performing the add operation.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"counter_name\": \"counter123\",\n\n\"delta\": 2,\n\n\"idempotency_token\": {\n\n\"token\": \"some_event_id\",\n\n\"generation_time\": \"2024-10-05T14:48:00Z\"\n\n}\n\n}\n\nThe idempotency token can be used for counter types that support them. Clients can use this token to safely retry or hedge their requests. Failures in a distributed system are a given, and having the ability to safely retry requests enhances the reliability of the service.\n\nGetCount: Retrieves the count value of the specified counter within a dataset.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"counter_name\": \"counter123\"\n\n}\n\nClearCount: Effectively resets the count to 0 for the specified counter within a dataset.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"counter_name\": \"counter456\",\n\n\"idempotency_token\": {...}\n\n}\n\nNow, let\u2019s look at the different types of counters supported within the Abstraction.\n\nTypes of Counters\n\nThe service primarily supports two types of counters: Best-Effort and Eventually Consistent, along with a third experimental type: Accurate. In the following sections, we\u2019ll describe the different approaches for these types of counters and the trade-offs associated with each.\n\nBest Effort Regional Counter\n\nThis type of counter is powered by EVCache, Netflix\u2019s distributed caching solution built on the widely popular Memcached. It is suitable for use cases like A/B experiments, where many concurrent experiments are run for relatively short durations and an approximate count is sufficient. Setting aside the complexities of provisioning, resource allocation, and control plane management, the core of this solution is remarkably straightforward:\n\n// counter cache key\n\ncounterCacheKey = <namespace>:<counter_name>\n\n\n\n// add operation\n\nreturn delta > 0\n\n? cache.incr(counterCacheKey, delta, TTL)\n\n: cache.decr(counterCacheKey, Math.abs(delta), TTL);\n\n\n\n// get operation\n\ncache.get(counterCacheKey);\n\n\n\n// clear counts from all replicas\n\ncache.delete(counterCacheKey, ReplicaPolicy.ALL);\n\nEVCache delivers extremely high throughput at low millisecond latency or better within a single region, enabling a multi-tenant setup within a shared cluster, saving infrastructure costs. However, there are some trade-offs: it lacks cross-region replication for the increment operation and does not provide consistency guarantees, which may be necessary for an accurate count. Additionally, idempotency is not natively supported, making it unsafe to retry or hedge requests.\n\nEdit: A note on probabilistic data structures:\n\nProbabilistic data structures like HyperLogLog (HLL) can be useful for tracking an approximate number of distinct elements, like distinct views or visits to a website, but are not ideally suited for implementing distinct increments and decrements for a given key. Count-Min Sketch (CMS) is an alternative that can be used to adjust the values of keys by a given amount. Data stores like Redis support both HLL and CMS. However, we chose not to pursue this direction for several reasons:\n\nWe chose to build on top of data stores that we already operate at scale.\n\nProbabilistic data structures do not natively support several of our requirements, such as resetting the count for a given key or having TTLs for counts. Additional data structures, including more sketches, would be needed to support these requirements.\n\nOn the other hand, the EVCache solution is quite simple, requiring minimal lines of code and using natively supported elements. However, it comes at the trade-off of using a small amount of memory per counter key.\n\nEventually Consistent Global Counter\n\nWhile some users may accept the limitations of a Best-Effort counter, others opt for precise counts, durability and global availability. In the following sections, we\u2019ll explore various strategies for achieving durable and accurate counts. Our objective is to highlight the challenges inherent in global distributed counting and explain the reasoning behind our chosen approach.\n\nApproach 1: Storing a Single Row per Counter\n\nLet\u2019s start simple by using a single row per counter key within a table in a globally replicated datastore.\n\nLet\u2019s examine some of the drawbacks of this approach:\n\nLack of Idempotency : There is no idempotency key baked into the storage data-model preventing users from safely retrying requests. Implementing idempotency would likely require using an external system for such keys, which can further degrade performance or cause race conditions.\n\n: There is no idempotency key baked into the storage data-model preventing users from safely retrying requests. Implementing idempotency would likely require using an external system for such keys, which can further degrade performance or cause race conditions. Heavy Contention: To update counts reliably, every writer must perform a Compare-And-Swap operation for a given counter using locks or transactions. Depending on the throughput and concurrency of operations, this can lead to significant contention, heavily impacting performance.\n\nSecondary Keys: One way to reduce contention in this approach would be to use a secondary key, such as a bucket_id, which allows for distributing writes by splitting a given counter into buckets, while enabling reads to aggregate across buckets. The challenge lies in determining the appropriate number of buckets. A static number may still lead to contention with hot keys, while dynamically assigning the number of buckets per counter across millions of counters presents a more complex problem.\n\nLet\u2019s see if we can iterate on our solution to overcome these drawbacks.\n\nApproach 2: Per Instance Aggregation\n\nTo address issues of hot keys and contention from writing to the same row in real-time, we could implement a strategy where each instance aggregates the counts in memory and then flushes them to disk at regular intervals. Introducing sufficient jitter to the flush process can further reduce contention.\n\nHowever, this solution presents a new set of issues:\n\nVulnerability to Data Loss : The solution is vulnerable to data loss for all in-memory data during instance failures, restarts, or deployments.\n\n: The solution is vulnerable to data loss for all in-memory data during instance failures, restarts, or deployments. Inability to Reliably Reset Counts : Due to counting requests being distributed across multiple machines, it is challenging to establish consensus on the exact point in time when a counter reset occurred.\n\n: Due to counting requests being distributed across multiple machines, it is challenging to establish consensus on the exact point in time when a counter reset occurred. Lack of Idempotency: Similar to the previous approach, this method does not natively guarantee idempotency. One way to achieve idempotency is by consistently routing the same set of counters to the same instance. However, this approach may introduce additional complexities, such as leader election, and potential challenges with availability and latency in the write path.\n\nThat said, this approach may still be suitable in scenarios where these trade-offs are acceptable. However, let\u2019s see if we can address some of these issues with a different event-based approach.\n\nApproach 3: Using Durable Queues\n\nIn this approach, we log counter events into a durable queuing system like Apache Kafka to prevent any potential data loss. By creating multiple topic partitions and hashing the counter key to a specific partition, we ensure that the same set of counters are processed by the same set of consumers. This setup simplifies facilitating idempotency checks and resetting counts. Furthermore, by leveraging additional stream processing frameworks such as Kafka Streams or Apache Flink, we can implement windowed aggregations.\n\nHowever, this approach comes with some challenges:\n\nPotential Delays : Having the same consumer process all the counts from a given partition can lead to backups and delays, resulting in stale counts.\n\n: Having the same consumer process all the counts from a given partition can lead to backups and delays, resulting in stale counts. Rebalancing Partitions: This approach requires auto-scaling and rebalancing of topic partitions as the cardinality of counters and throughput increases.\n\nFurthermore, all approaches that pre-aggregate counts make it challenging to support two of our requirements for accurate counters:\n\nAuditing of Counts : Auditing involves extracting data to an offline system for analysis to ensure that increments were applied correctly to reach the final value. This process can also be used to track the provenance of increments. However, auditing becomes infeasible when counts are aggregated without storing the individual increments.\n\n: Auditing involves extracting data to an offline system for analysis to ensure that increments were applied correctly to reach the final value. This process can also be used to track the provenance of increments. However, auditing becomes infeasible when counts are aggregated without storing the individual increments. Potential Recounting: Similar to auditing, if adjustments to increments are necessary and recounting of events within a time window is required, pre-aggregating counts makes this infeasible.\n\nBarring those few requirements, this approach can still be effective if we determine the right way to scale our queue partitions and consumers while maintaining idempotency. However, let\u2019s explore how we can adjust this approach to meet the auditing and recounting requirements.\n\nApproach 4: Event Log of Individual Increments\n\nIn this approach, we log each individual counter increment along with its event_time and event_id. The event_id can include the source information of where the increment originated. The combination of event_time and event_id can also serve as the idempotency key for the write.\n\nHowever, in its simplest form, this approach has several drawbacks:\n\nRead Latency : Each read request requires scanning all increments for a given counter potentially degrading performance.\n\n: Each read request requires scanning all increments for a given counter potentially degrading performance. Duplicate Work : Multiple threads might duplicate the effort of aggregating the same set of counters during read operations, leading to wasted effort and subpar resource utilization.\n\n: Multiple threads might duplicate the effort of aggregating the same set of counters during read operations, leading to wasted effort and subpar resource utilization. Wide Partitions : If using a datastore like Apache Cassandra, storing many increments for the same counter could lead to a wide partition, affecting read performance.\n\n: If using a datastore like Apache Cassandra, storing many increments for the same counter could lead to a wide partition, affecting read performance. Large Data Footprint: Storing each increment individually could also result in a substantial data footprint over time. Without an efficient data retention strategy, this approach may struggle to scale effectively.\n\nThe combined impact of these issues can lead to increased infrastructure costs that may be difficult to justify. However, adopting an event-driven approach seems to be a significant step forward in addressing some of the challenges we\u2019ve encountered and meeting our requirements.\n\nHow can we improve this solution further?\n\nNetflix\u2019s Approach\n\nWe use a combination of the previous approaches, where we log each counting activity as an event, and continuously aggregate these events in the background using queues and a sliding time window. Additionally, we employ a bucketing strategy to prevent wide partitions. In the following sections, we\u2019ll explore how this approach addresses the previously mentioned drawbacks and meets all our requirements.\n\nNote: From here on, we will use the words \u201crollup\u201d and \u201caggregate\u201d interchangeably. They essentially mean the same thing, i.e., collecting individual counter increments/decrements and arriving at the final value.\n\nTimeSeries Event Store:\n\nWe chose the TimeSeries Data Abstraction as our event store, where counter mutations are ingested as event records. Some of the benefits of storing events in TimeSeries include:\n\nHigh-Performance: The TimeSeries abstraction already addresses many of our requirements, including high availability and throughput, reliable and fast performance, and more.\n\nReducing Code Complexity: We reduce a lot of code complexity in Counter Abstraction by delegating a major portion of the functionality to an existing service.\n\nTimeSeries Abstraction uses Cassandra as the underlying event store, but it can be configured to work with any persistent store. Here is what it looks like:\n\nHandling Wide Partitions: The time_bucket and event_bucket columns play a crucial role in breaking up a wide partition, preventing high-throughput counter events from overwhelming a given partition. For more information regarding this, refer to our previous blog.\n\nNo Over-Counting: The event_time, event_id and event_item_key columns form the idempotency key for the events for a given counter, enabling clients to retry safely without the risk of over-counting.\n\nEvent Ordering: TimeSeries orders all events in descending order of time allowing us to leverage this property for events like count resets.\n\nEvent Retention: The TimeSeries Abstraction includes retention policies to ensure that events are not stored indefinitely, saving disk space and reducing infrastructure costs. Once events have been aggregated and moved to a more cost-effective store for audits, there\u2019s no need to retain them in the primary storage.\n\nNow, let\u2019s see how these events are aggregated for a given counter.\n\nAggregating Count Events:\n\nAs mentioned earlier, collecting all individual increments for every read request would be cost-prohibitive in terms of read performance. Therefore, a background aggregation process is necessary to continually converge counts and ensure optimal read performance.\n\nBut how can we safely aggregate count events amidst ongoing write operations?\n\nThis is where the concept of Eventually Consistent counts becomes crucial. By intentionally lagging behind the current time by a safe margin, we ensure that aggregation always occurs within an immutable window.\n\nLets see what that looks like:\n\nLet\u2019s break this down:\n\nlastRollupTs : This represents the most recent time when the counter value was last aggregated. For a counter being operated for the first time, this timestamp defaults to a reasonable time in the past.\n\n: This represents the most recent time when the counter value was last aggregated. For a counter being operated for the first time, this timestamp defaults to a reasonable time in the past. Immutable Window and Lag: Aggregation can only occur safely within an immutable window that is no longer receiving counter events. The \u201cacceptLimit\u201d parameter of the TimeSeries Abstraction plays a crucial role here, as it rejects incoming events with timestamps beyond this limit. During aggregations, this window is pushed slightly further back to account for clock skews.\n\nThis does mean that the counter value will lag behind its most recent update by some margin (typically in the order of seconds). This approach does leave the door open for missed events due to cross-region replication issues. See \u201cFuture Work\u201d section at the end.\n\nAggregation Process: The rollup process aggregates all events in the aggregation window since the last rollup to arrive at the new value.\n\nRollup Store:\n\nWe save the results of this aggregation in a persistent store. The next aggregation will simply continue from this checkpoint.\n\nWe create one such Rollup table per dataset and use Cassandra as our persistent store. However, as you will soon see in the Control Plane section, the Counter service can be configured to work with any persistent store.\n\nLastWriteTs: Every time a given counter receives a write, we also log a last-write-timestamp as a columnar update in this table. This is done using Cassandra\u2019s USING TIMESTAMP feature to predictably apply the Last-Write-Win (LWW) semantics. This timestamp is the same as the event_time for the event. In the subsequent sections, we\u2019ll see how this timestamp is used to keep some counters in active rollup circulation until they have caught up to their latest value.\n\nRollup Cache\n\nTo optimize read performance, these values are cached in EVCache for each counter. We combine the lastRollupCount and lastRollupTs into a single cached value per counter to prevent potential mismatches between the count and its corresponding checkpoint timestamp.\n\nBut, how do we know which counters to trigger rollups for? Let\u2019s explore our Write and Read path to understand this better.\n\nAdd/Clear Count:\n\nAn add or clear count request writes durably to the TimeSeries Abstraction and updates the last-write-timestamp in the Rollup store. If the durability acknowledgement fails, clients can retry their requests with the same idempotency token without the risk of overcounting. Upon durability, we send a fire-and-forget request to trigger the rollup for the request counter.\n\nGetCount:\n\nWe return the last rolled-up count as a quick point-read operation, accepting the trade-off of potentially delivering a slightly stale count. We also trigger a rollup during the read operation to advance the last-rollup-timestamp, enhancing the performance of subsequent aggregations. This process also self-remediates a stale count if any previous rollups had failed.\n\nWith this approach, the counts continually converge to their latest value. Now, let\u2019s see how we scale this approach to millions of counters and thousands of concurrent operations using our Rollup Pipeline.\n\nRollup Pipeline:\n\nEach Counter-Rollup server operates a rollup pipeline to efficiently aggregate counts across millions of counters. This is where most of the complexity in Counter Abstraction comes in. In the following sections, we will share key details on how efficient aggregations are achieved.\n\nLight-Weight Roll-Up Event: As seen in our Write and Read paths above, every operation on a counter sends a light-weight event to the Rollup server:\n\nrollupEvent: {\n\n\"namespace\": \"my_dataset\",\n\n\"counter\": \"counter123\"\n\n}\n\nNote that this event does not include the increment. This is only an indication to the Rollup server that this counter has been accessed and now needs to be aggregated. Knowing exactly which specific counters need to be aggregated prevents scanning the entire event dataset for the purpose of aggregations.\n\nIn-Memory Rollup Queues: A given Rollup server instance runs a set of in-memory queues to receive rollup events and parallelize aggregations. In the first version of this service, we settled on using in-memory queues to reduce provisioning complexity, save on infrastructure costs, and make rebalancing the number of queues fairly straightforward. However, this comes with the trade-off of potentially missing rollup events in case of an instance crash. For more details, see the \u201cStale Counts\u201d section in \u201cFuture Work.\u201d\n\nMinimize Duplicate Effort: We use a fast non-cryptographic hash like XXHash to ensure that the same set of counters end up on the same queue. Further, we try to minimize the amount of duplicate aggregation work by having a separate rollup stack that chooses to run fewer beefier instances.\n\nAvailability and Race Conditions: Having a single Rollup server instance can minimize duplicate aggregation work but may create availability challenges for triggering rollups. If we choose to horizontally scale the Rollup servers, we allow threads to overwrite rollup values while avoiding any form of distributed locking mechanisms to maintain high availability and performance. This approach remains safe because aggregation occurs within an immutable window. Although the concept of now() may differ between threads, causing rollup values to sometimes fluctuate, the counts will eventually converge to an accurate value within each immutable aggregation window.\n\nRebalancing Queues: If we need to scale the number of queues, a simple Control Plane configuration update followed by a re-deploy is enough to rebalance the number of queues.\n\n\"eventual_counter_config\": {\n\n\"queue_config\": {\n\n\"num_queues\" : 8, // change to 16 and re-deploy\n\n...\n\nHandling Deployments: During deployments, these queues shut down gracefully, draining all existing events first, while the new Rollup server instance starts up with potentially new queue configurations. There may be a brief period when both the old and new Rollup servers are active, but as mentioned before, this race condition is managed since aggregations occur within immutable windows.\n\nMinimize Rollup Effort: Receiving multiple events for the same counter doesn\u2019t mean rolling it up multiple times. We drain these rollup events into a Set, ensuring a given counter is rolled up only once during a rollup window.\n\nEfficient Aggregation: Each rollup consumer processes a batch of counters simultaneously. Within each batch, it queries the underlying TimeSeries abstraction in parallel to aggregate events within specified time boundaries. The TimeSeries abstraction optimizes these range scans to achieve low millisecond latencies.\n\nDynamic Batching: The Rollup server dynamically adjusts the number of time partitions that need to be scanned based on cardinality of counters in order to prevent overwhelming the underlying store with many parallel read requests.\n\nAdaptive Back-Pressure: Each consumer waits for one batch to complete before issuing the rollups for the next batch. It adjusts the wait time between batches based on the performance of the previous batch. This approach provides back-pressure during rollups to prevent overwhelming the underlying TimeSeries store.\n\nHandling Convergence:\n\nIn order to prevent low-cardinality counters from lagging behind too much and subsequently scanning too many time partitions, they are kept in constant rollup circulation. For high-cardinality counters, continuously circulating them would consume excessive memory in our Rollup queues. This is where the last-write-timestamp mentioned previously plays a crucial role. The Rollup server inspects this timestamp to determine if a given counter needs to be re-queued, ensuring that we continue aggregating until it has fully caught up with the writes.\n\nNow, let\u2019s see how we leverage this counter type to provide an up-to-date current count in near-realtime.\n\nExperimental: Accurate Global Counter\n\nWe are experimenting with a slightly modified version of the Eventually Consistent counter. Again, take the term \u2018Accurate\u2019 with a grain of salt. The key difference between this type of counter and its counterpart is that the delta, representing the counts since the last-rolled-up timestamp, is computed in real-time.\n\nAnd then, currentAccurateCount = lastRollupCount + delta\n\nAggregating this delta in real-time can impact the performance of this operation, depending on the number of events and partitions that need to be scanned to retrieve this delta. The same principle of rolling up in batches applies here to prevent scanning too many partitions in parallel. Conversely, if the counters in this dataset are accessed frequently, the time gap for the delta remains narrow, making this approach of fetching current counts quite effective.\n\nNow, let\u2019s see how all this complexity is managed by having a unified Control Plane configuration.\n\nControl Plane\n\nThe Data Gateway Platform Control Plane manages control settings for all abstractions and namespaces, including the Counter Abstraction. Below, is an example of a control plane configuration for a namespace that supports eventually consistent counters with low cardinality:\n\n\"persistence_configuration\": [\n\n{\n\n\"id\": \"CACHE\", // Counter cache config\n\n\"scope\": \"dal=counter\",\n\n\"physical_storage\": {\n\n\"type\": \"EVCACHE\", // type of cache storage\n\n\"cluster\": \"evcache_dgw_counter_tier1\" // Shared EVCache cluster\n\n}\n\n},\n\n{\n\n\"id\": \"COUNTER_ROLLUP\",\n\n\"scope\": \"dal=counter\", // Counter abstraction config\n\n\"physical_storage\": {\n\n\"type\": \"CASSANDRA\", // type of Rollup store\n\n\"cluster\": \"cass_dgw_counter_uc1\", // physical cluster name\n\n\"dataset\": \"my_dataset_1\" // namespace/dataset\n\n},\n\n\"counter_cardinality\": \"LOW\", // supported counter cardinality\n\n\"config\": {\n\n\"counter_type\": \"EVENTUAL\", // Type of counter\n\n\"eventual_counter_config\": { // eventual counter type\n\n\"internal_config\": {\n\n\"queue_config\": { // adjust w.r.t cardinality\n\n\"num_queues\" : 8, // Rollup queues per instance\n\n\"coalesce_ms\": 10000, // coalesce duration for rollups\n\n\"capacity_bytes\": 16777216 // allocated memory per queue\n\n},\n\n\"rollup_batch_count\": 32 // parallelization factor\n\n}\n\n}\n\n}\n\n},\n\n{\n\n\"id\": \"EVENT_STORAGE\",\n\n\"scope\": \"dal=ts\", // TimeSeries Event store\n\n\"physical_storage\": {\n\n\"type\": \"CASSANDRA\", // persistent store type\n\n\"cluster\": \"cass_dgw_counter_uc1\", // physical cluster name\n\n\"dataset\": \"my_dataset_1\", // keyspace name\n\n},\n\n\"config\": {\n\n\"time_partition\": { // time-partitioning for events\n\n\"buckets_per_id\": 4, // event buckets within\n\n\"seconds_per_bucket\": \"600\", // smaller width for LOW card\n\n\"seconds_per_slice\": \"86400\", // width of a time slice table\n\n},\n\n\"accept_limit\": \"5s\", // boundary for immutability\n\n},\n\n\"lifecycleConfigs\": {\n\n\"lifecycleConfig\": [\n\n{\n\n\"type\": \"retention\", // Event retention\n\n\"config\": {\n\n\"close_after\": \"518400s\",\n\n\"delete_after\": \"604800s\" // 7 day count event retention\n\n}\n\n}\n\n]\n\n}\n\n}\n\n]\n\nUsing such a control plane configuration, we compose multiple abstraction layers using containers deployed on the same host, with each container fetching configuration specific to its scope.\n\nProvisioning\n\nAs with the TimeSeries abstraction, our automation uses a bunch of user inputs regarding their workload and cardinalities to arrive at the right set of infrastructure and related control plane configuration. You can learn more about this process in a talk given by one of our stunning colleagues, Joey Lynch : How Netflix optimally provisions infrastructure in the cloud.\n\nPerformance\n\nAt the time of writing this blog, this service was processing close to 75K count requests/second globally across the different API endpoints and datasets:\n\nwhile providing single-digit millisecond latencies for all its endpoints:\n\nFuture Work\n\nWhile our system is robust, we still have work to do in making it more reliable and enhancing its features. Some of that work includes:\n\nRegional Rollups: Cross-region replication issues can result in missed events from other regions. An alternate strategy involves establishing a rollup table for each region, and then tallying them in a global rollup table. A key challenge in this design would be effectively communicating the clearing of the counter across regions.\n\nCross-region replication issues can result in missed events from other regions. An alternate strategy involves establishing a rollup table for each region, and then tallying them in a global rollup table. A key challenge in this design would be effectively communicating the clearing of the counter across regions. Error Detection and Stale Counts: Excessively stale counts can occur if rollup events are lost or if a rollup fails and isn\u2019t retried. This isn\u2019t an issue for frequently accessed counters, as they remain in rollup circulation. This issue is more pronounced for counters that aren\u2019t accessed frequently. Typically, the initial read for such a counter will trigger a rollup, self-remediating the issue. However, for use cases that cannot accept potentially stale initial reads, we plan to implement improved error detection, rollup handoffs, and durable queues for resilient retries.\n\nConclusion\n\nDistributed counting remains a challenging problem in computer science. In this blog, we explored multiple approaches to implement and deploy a Counting service at scale. While there may be other methods for distributed counting, our goal has been to deliver blazing fast performance at low infrastructure costs while maintaining high availability and providing idempotency guarantees. Along the way, we make various trade-offs to meet the diverse counting requirements at Netflix. We hope you found this blog post insightful.\n\nStay tuned for Part 3 of Composite Abstractions at Netflix, where we\u2019ll introduce our Graph Abstraction, a new service being built on top of the Key-Value Abstraction and the TimeSeries Abstraction to handle high-throughput, low-latency graphs.\n\nAcknowledgments\n\nSpecial thanks to our stunning colleagues who contributed to the Counter Abstraction\u2019s success: Joey Lynch, Vinay Chella, Kaidan Fullerton, Tom DeVoe, Mengqing Wang, Varun Khaitan", "label": 0}
{"title": "learning \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/tag/learning/", "content": "Liked Rob Sheffield on the Joys of the CD, Music\u2019s Least-Glamorous Format by Rob Sheffield ( Rolling Stone ) Compact discs never had the romance of vinyl or the convenience of MP3s. But they\u2019re still the ideal format for getting lost inside your music collection.\n\nI still have a CD player in my car so I hung onto my favorite 20-30 albums, and I somewhat regret getting rid of my collection. It wasn\u2019t ever as big as my parent\u2019s, but I had ~50-70 jazz CDs (donated to the local HS music department so hopefully someone\u2019s still using them) plus probably a hundred albums. Some indie shit I probably couldn\u2019t replace if I wanted (I like to think I\u2019ve become slightly more thoughtful about what I give away in my wiser 30s \ud83d\ude02).\n\nBut to be fair, my listening has shifted a lot since college, so the music I listen to most I don\u2019t have on CD, and I got rid of a bunch of albums that I had kinda outgrown, so maybe it\u2019s not a bad thing. I\u2019ve only bought one or two CDs a year for the past decade, indie bands I wanted to give some extra support (and listen in the car).\n\nThere\u2019s something about having a tangible object that makes it easier to flip through your collection and pull out things you haven\u2019t listened to in a while. Growing up I was obsessed with learning to recognize every song that came on, so I was constantly comparing against the back of the CD. I liked looking through the liner art, and had a great visual memory for what the cover of every album was. Now it\u2019s hard for me to remember what artist performed what song \u2013 I think that physical object of the jewel case was an anchor point for my memory. I also listen to playlists primarily these days, and know only a single song (or handful) by any given artist.\n\n(Related? Structures of Thought)\n\nI think there\u2019s a place for both CDs and playlists in a musical library \u2013 I\u2019ve benefitted from both styles of listening. But I do miss my five disc changer from my youth\u2026 hooked up to massive speakers nearly 3\u2032 tall in my living room so they could punch some damn volume \ud83d\ude02 (No idea if they were any good or not \ud83e\udd37\u200d\u2640\ufe0f)\n\nI also think it\u2019s worth remembering we got rid of our CDs for a reason\u2026 they do take up a lot of space and jewel cases are shit\u2026 but now we\u2019ve spent time without them we can recognize what we\u2019ve lost along with them.\n\nI keep thinking about looking up a used CD player, maybe I\u2019ll actually get around to it once day \ud83e\udd37\u200d\u2640\ufe0f\ud83d\ude02 I\u2019m curious how much I would listen to CDs if we upgraded the music system in our car and Bluetooth wasn\u2019t an enormous pain \u2013 would I revert to mostly Spotify playlists on my phone? \ud83e\udd14", "label": 1}
{"title": "Exploring the Magic Mirror: an interactive experience powered by the Gemini models", "url": "https://developers.googleblog.com/en/magic-mirror-interactive-experience-powered-by-gemini-models/", "content": "Imagine gazing into a mirror and seeing not just your reflection, but a gateway to information, creativity, and a touch of enchantment. This is precisely what the Gemini backed Magic Mirror project brings to life. Moving beyond a simple display, this project showcases the incredible interactive capabilities of the Gemini API and JavaScript GenAI SDK, transforming a familiar object into a new chat interface.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nThis project creates its interactive experience using several features of the Gemini API:\n\n1: Fluid, Real-Time Conversations with the Live API The foundation of the magic mirror's interactivity is the Live API. This allows for continuous, real-time voice interactions. You speak, and the mirror doesn't just listen for a single command, it engages in a flowing conversation by processing your speech as you talk, allowing for a more natural back-and-forth dialogue in either text or audio. On top of this, the Live API is able to understand when you\u2019re speaking during playback and interpret that interruption to pivot the narrative and conversation based on your inputs, allowing for dynamic audible conversations alongside text.\n\nLink to Youtube Video (visible only when JS is disabled)\n\n2: The enchanted storyteller On top of being able to have a conversation through the Live API, the magic mirror can also be customized to weave tales, all thanks to the Gemini model's advanced generation capabilities by providing specific system instructions and updating speech configurations during initialization to include different dialects or accents, voices, and a variety of other attributes.\n\nLink to Youtube Video (visible only when JS is disabled)\n\n3: Instant information: grounding with Google Search While conversations and stories are great, sometimes you want to be able to know about the world around you as it\u2019s happening. This magic mirror project leverages the model\u2019s ability to integrate with Grounding with Google Search, providing grounded, up-to-date information.\n\nLink to Youtube Video (visible only when JS is disabled)\n\n4: Visual alchemy: image generation on command Using Function Calling with the Gemini API, the magic mirror is able to generate visuals based on your descriptions, adding depth to stories and deepening the experience of interacting with the Gemini model. The Gemini model determines that your request requires image generation and calls a predefined function based on stated characteristics, passing along the detailed prompt it derives from your spoken words.\n\nLink to Youtube Video (visible only when JS is disabled)", "label": 0}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2014-06", "content": "en\n\nSo last week was the end of my speech class. The first few days sucked because I had surgery on my nose. Everything went well. That's a different story thought :)\n\nThis was the first time I ever bought a college book. That sad part was that we barely used it in the three weeks of the speech class. What a waste of money! Anyhow, it was a fun class but weird at the same time. Weird reasosn to be undisclosed.... I got to be around people I knew. This gave me reassurance, well whatever I got out of it. I hated doing speeches. I'm actually okay at them but give me panic attacks (often you can't tell.) In order to survive one had to\n\nFake it 'til you make it!\n\nThe class got more comfortable as time went on. Taking this speech class has taught me a lot about myself. I found some weaknesses that I have improved on.\n\nBeing more confident\n\nMore ability to speak to large audience.\n\nLess worrying of what others things\n\netc.\n\nTip of the day. Take a college speech class in the summer. Not only is it shorter but also you have less people to present towards than a whole lecture hall full of students during the school year. It is less stressful.\n\nTo end...", "label": 1}
{"title": "How we improved availability through iterative simplification", "url": "https://github.blog/engineering/engineering-principles/how-we-improved-availability-through-iterative-simplification/", "content": "Solving and staying ahead of problems when scaling up a system of GitHub\u2019s size is a delicate process. The stack is complex, and even small changes can have a big ripple effect. Here\u2019s a look at some of the tools in GitHub\u2019s toolbox, and how we\u2019ve used them to solve problems. We\u2019ll also share some of our wins and lessons we learned along the way.\n\nThere are several tools that we use to keep pace with our growing system. While we can\u2019t list them all, here are some that have been instrumental for our growth.\n\nAs we serve requests, there is a constant stream of related numbers that we care about. For example, we might want to know how often events are happening or how traffic levels compare to expected use. We can record metrics for each event in Datadog to see patterns over time and break them down across different dimensions, identifying areas that need focus.\n\nEvents also contain context that can help identify details for issues we\u2019re troubleshooting. We send all this context to Splunk for further analysis.\n\nMuch of our application data is stored in MySQL, and query performance can degrade over time due to factors like database size and query frequency. We have written custom monitors that detect and report slow and timed-out queries for further investigation and remediation.\n\nWhen we introduce changes, we often need to know how those changes affect performance. We use Scientist to test proposed changes. With this tool, we measure and report results before making the changes permanent.\n\nWhen we\u2019re ready to release a change, we roll it out incrementally to ensure it works as expected for all use cases. We also need to be able to roll back in the event of unexpected behavior. We use Flipper to limit the rollout to early access users, then to an increasing percentage of users as we build the confidence.\n\nAchieving faster database queries\n\nWe recently observed a SQL query causing a high number of timeouts. Our investigation in Splunk tracked it down to GitHub\u2019s Command Palette feature, which was loading a list of repositories. The code to generate that list looked something like this:\n\norg_repo_ids = Repository.where(owner: org).pluck(:id) suggested_repo_ids = Contribution.where(user: viewer, repository_id: org_repo_ids).pluck(:repository_id)\n\nIf an org has many active repositories, the second line could generate a SQL query with a large IN (...) clause with an increased risk of timing out. While we\u2019d seen this type of problem before, there was something unique about this particular use case. We might be able to improve performance by querying the user first since a given user contributes to a relatively small number of repositories.\n\ncontributor_repo_ids = Contribution.where(user: viewer).pluck(:repository_id) suggested_repo_ids = Repository.where(owner: org, id: contributor_repo_ids)\n\nWe created a Scientist experiment with a new candidate code block to evaluate performance. The Datadog dashboard for the experiment confirmed two things: the candidate code block returned the same results and improved performance by 80-90%.\n\nWe also did a deeper dive into the queries this feature was generating and found a couple of possible additional improvements.\n\nThe first involved eliminating a SQL query and sorting results in the application rather than asking the SQL server to sort. We followed the same process with a new experiment and found that the candidate code block performed 40-80% worse than the control. We removed the candidate code block and ended the experiment.\n\nThe second was a query filtering results based on the viewer\u2019s level of access and did so by iterating through the list of results. The access check we needed can be batched. So, we started another experiment to do the filtering with a single batched query and confirmed that the candidate code block improved performance by another 20-80%.\n\nWhile we were wrapping up these experiments, we checked for similar patterns in related code and found a similar filter we could batch. We confirmed a 30-40% performance improvement with a final experiment, and left the feature in a better place that made our developers, database administrators, and users happier.\n\nRemoving unused code\n\nWhile our tooling does surface problem areas to focus on, it\u2019s preferable to get ahead of performance issues and fix problematic areas before they cause a degraded experience. We recently analyzed the busiest request endpoints for one of our teams and found room to improve one of them before it escalated to an urgent problem.\n\nData for each request to the GitHub Rails application is logged in Splunk and tagged with the associated controller and action. We started by querying Splunk for the top 10 controller/action pairs in the endpoints owned by the team. We used that list to create a Datadog dashboard with a set of graphs for each controller/action that showed the total request volume, average and P99 request latency, and max request latency. We found that the busiest endpoint on the dashboard was an action responsible for a simple redirect, and that performance regularly degraded to the timeout threshold.\n\nWe needed to know what was slowing these requests down, so we dug into Datadog\u2019s APM feature to show requests for the problematic controller/endpoint. We sorted those requests by elapsed request time to see the slowest requests first. We identified a pattern where slow requests spent a long time performing an access check that wasn\u2019t required to send the redirect response.\n\nMost requests to the GitHub Rails application generate HTML responses where we need to be careful to ensure that all data in the response is accessible to the viewer. We\u2019re able to simplify the code involved by using shared Rails controller filters to verify that the viewer is allowed to see the resources they\u2019re requesting that run before the server renders a response. These checks aren\u2019t required for the redirect, so we wanted to confirm we could serve those requests using a different set of filters and that this approach would improve performance.\n\nSince Rails controller filters are configured when the application boots rather than when each request is processed, we weren\u2019t able to use a Scientist experiment to test a candidate code block. However, filters can be configured to run conditionally, which enabled us to use a Flipper feature flag to change behavior. We identified the set of filters that weren\u2019t required for the redirect, and configured the controller to skip those filters when the feature flag was enabled. The feature flag controls let us ramp up this behavior while monitoring both performance and request status via Datadog and keeping watch for unexpected problems via Splunk.\n\nAfter confirming that performance improved for P75/P99 request latency\u2014and more importantly, reduced max latency to be more consistent and much less likely to time out\u2014we graduated the feature and generalized the behavior so other similar controllers can use it.\n\nWhat did we learn?\n\nThere are several lessons we learned throughout this process. Here are some of the main points we keep in mind.\n\nThe investment in observability is totally worth it! We identified and solved problems quickly because of the metric and log information we track.\n\nEven when you\u2019re troubleshooting a problem that\u2019s been traditionally difficult to solve, the use case may be subtly different in a way that presents a new solution.\n\nWhen you\u2019re working on a fix, look around at adjacent code. There may be related issues you can tackle while you\u2019re there.\n\nPerformance problems are a moving target. Keeping an eye open for the next one helps you fix it when it\u2019s gotten slow rather than when it starts causing timeouts and breaking things.\n\nMake small changes in ways that you can control with a gradual rollout and measure results.", "label": 0}
{"title": "Survived the first week of College", "url": "https://lifeofpablo.com/blog/survived-the-first-week-of-college", "content": "Survived the first week of College\n\nThis post was written in English (en_US).\n\n\"Hey guys! Pablo here!\n\n[caption id=\"\"attachment_311\"\" align=\"\"alignnone\"\" width=\"\"240\"\"] Work out #selfie[/caption]\n\nWondering if I was still alive or not? Well here I am. I made it through the first week of college. Since the minute I stepped foot outside my car I knew I was going to enjoy it. Most of it, minus the move in day process.\n\nI got to see my good friend Lucas, who just came back from Brazil to come study here at UNK. Man I missed this guy. He is my roommate that I will be staying with all year long. I just hope we don't kill each by then. So far we are getting along great.\n\n.\n\nLater that morning I got to meet my suite mates: Thomas and Riley. These two are from around North Platte, NE. Who knew we would all be alike? We are all like brothers from another mother. They've made the week great.I wish I would have met them earlier!\n\n[caption id=\"\"attachment_307\"\" align=\"\"alignnone\"\" width=\"\"300\"\"] Pabs and Tom (right)[/caption]\n\nSince we are now bros, we have done many fun activities. Activities like working out or even just walking around (towards the cafeteria of course :) )\n\nThe first weekend of college was great. There were a lot of fun activities. There was a lot of free food and free stuff!! I got a free stylus for my touch screen computer. That was the highlight of my free stuff shopping spree. I really wanted the free iPad. Oh Well.\n\nWho doesn't like free stuff?? I know I do!\n\nI have made many friends so far. I am glad to see many of my friends that I went to school or that I knew especially from the World Leaders Camp.\n\nPeople on my floor are amazing. I've met people from Italy to Venezuela. I enjoy being on such a diverse campus where people immerse you in their culture. This will really want me to study abroad.\n\nThere have been some funny moments on my floor. Like this one kid who danced to anaconda\n\nIt has been such a great week here at UNK. I felt welcome and lucky to be at such a great school. I hope you all enjoyed this post. I hope to hear from many of you guys.\n\nGO LOPERS!!!\n\nHere are some more pictures.!\n\nMAN I <3 COLLEGE!!\n\n\"", "label": 1}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2016-04", "content": "en\n\nIn the United States we are exposed to many different cultures. Examples of different cultures are the hispanic culture, italian, french, etc. Especially here at UNK we are more exposed than ever to different cultures.\n\nToday I want to talk about how people should be open minded and talk about how one should expose themselves too a different culture. Many of us who are small town people never really grew up with people of a different race or background. There is no harm in that. Once we all came to college we saw how diverse our world really is.. For me I've grown up in two cultures parallel to each other. I grew up in a very hispanic family while being born in to the American culture that I have known since the day I was born. So I could say that I am used to different cultures.\n\nThrough out our lives, we find people who are different from us in various ways. Different can mean being from another country. Being from a different country you have different aspects of the world. Being different is good. Those who are totally different from you are International Students.\n\nI say one should befriend an international student. Help them out. It would mean the world to them. You thought you were scared when you went off to college, they are probably three times as scared since they are in a country that they have never been in before. They will be greatful when you do help them. I know if you ever need a favor from them, they will gladly go up and beyond to help you suceed if you need help from them. It does not hurt to hangout with them. They will learn so much just being around you. You will as well. It is a win-win situation. Take them out to lunch sometime so you get to know them. They will gladly prepare a meal for you sometime. You may not think about it now but they could be of future help when needing to get a job, if they are already in the workforce.\n\nI love to learn about the other cultures. It fascinates me how different and similar cultures are. Me being the avid FOOD LOVER <3 I love to \"\"test\"\" new food for my belly to enjoy. I did not realize that I loved Japanese foodso much. I think that best part other than the insides is the seaweed to make some of them sushi entrees. NO one has anything to lose by opening themselves up to something different. You know what they say\n\nYOU NEVER KNOW UNTIL YOU TRY!\n\nI say go for it!\n\nWhile you can enjoy a culture festival. Here at UNK we have many of these. The most recent one I went to was the Japanese Food Festival. I learned so much about the Japanese culture. I got to see what their traditions truly embrace than just rely on stereotypes. I have made many Japanese friends as well. Being around theses students, you really get a one to one learning opportunity. It really teaches others not to be afraid and to always remember where one came from originally.\n\nThe food was great! Job well done to those in charge of the event. It was a one to remember. Who know it would of been SO much fun to make Japanese food. I want more! Someone hook me up!!\n\nMaybe I'll see you guys at the International Food Festival this Sunday!\n\nCheck out my gallery!!\n\n\"", "label": 1}
{"title": "Read in 2025 \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/reading/read-in-2025/", "content": "Jump to: Books read in 2025 | Did Not Finish | Other Years\u2019 Reading | Currently reading\n\nBooks Read in 2025\n\n* indicates re-reads. \ud83d\udc4d = enjoyed a lot \ud83d\udc4d\ud83d\udc4d = loved\n\nPresented in reverse order. Links go to my review / notes.\n\nShow all Favorites \ud83d\udc4d Romance \ud83e\udd0d Everything but romance Sci-fi \ud83d\ude80 Fantasy Mystery Novella Art + Design Comics Non-fiction Cookbook LGBTQIA+ \ud83c\udf08 Self-pub Re-read Owned\n\nJump to top\n\nBooks I didn\u2019t finish reading Did Not Finish The Grimoire Grammar School Parent Teacher Association\n\nby Caitlin Rozakis\n\n(paranormal, humor)\n\nDNF 8% \u2014 I think this is going to be more focused on the parenting aspect than I\u2019m interested in, and the POV character\u2019s issues about fitting in aren\u2019t appealing\n\nby Caitlin Rozakis (paranormal, humor) DNF 8% \u2014 I think this is going to be more focused on the parenting aspect than I\u2019m interested in, and the POV character\u2019s issues about fitting in aren\u2019t appealing Reborn\n\nby Seth Haddon\n\n(romance, fantasy)\n\nDNF 8% \u2014 one character way too religious and devoted to the emperor for my taste\n\nby Seth Haddon (romance, fantasy) DNF 8% \u2014 one character way too religious and devoted to the emperor for my taste When the Tides Held the Moon\n\nby Venessa Vida Kelley\n\n(fantasy)\n\nDNF first chapter \u2014 wasn\u2019t feeling it\n\nby Venessa Vida Kelley (fantasy) DNF first chapter \u2014 wasn\u2019t feeling it Evocation\n\nby S.T. Gibson\n\n(fantasy, romance)\n\nDNF chapter two \u2014 didn\u2019t like either POV character\n\nby S.T. Gibson (fantasy, romance) DNF chapter two \u2014 didn\u2019t like either POV character How to Steal a Galaxy\n\nby Beth Revis\n\n(sci-fi, romance)\n\nthought it was standalone but it\u2019s the middle book \u2014 not sold from what I read of this one to go back and read the first\n\nby Beth Revis (sci-fi, romance) thought it was standalone but it\u2019s the middle book \u2014 not sold from what I read of this one to go back and read the first Gentle Rogue\n\nby Johanna Lindsey\n\n(romance, historical)\n\nDNF chapter 2 \u2014 we are jumping between too many POV characters\n\nby Johanna Lindsey (romance, historical) DNF chapter 2 \u2014 we are jumping between too many POV characters Rogue Enforcer\n\nby Grace Goodwin\n\n(romance, sci-fi)\n\nDNF 55%ish \u2014 hero rubbed me the wrong way, overprotective and anxious\n\nby Grace Goodwin (romance, sci-fi) DNF 55%ish \u2014 hero rubbed me the wrong way, overprotective and anxious Immediacy\n\nby Anna Kornbluh\n\n(theory, culture)\n\nhated the writing\n\nby Anna Kornbluh (theory, culture) hated the writing Never Marry a Scandalous Duke\n\nby Renee Ann Miller\n\n(romance, historical)\n\nDNF 37% \u2014 hero is about to make her move her permanent office because he\u2019s annoyed by a week of shelf installation? heroine wants to be a proper lady so she resolves to lay back and think of England? nah\n\nby Renee Ann Miller (romance, historical) DNF 37% \u2014 hero is about to make her move her permanent office because he\u2019s annoyed by a week of shelf installation? heroine wants to be a proper lady so she resolves to lay back and think of England? nah The North Wind\n\nby Alexandria Warwick\n\n(romantasy)\n\nDNF 45% the heroine is pretty loathsome and obnoxious, also the love interest Darth Vader choked her at one point\n\nby Alexandria Warwick (romantasy) DNF 45% the heroine is pretty loathsome and obnoxious, also the love interest Darth Vader choked her at one point The Fireborne Blade\n\nby Charlotte Bond\n\n(fantasy)\n\nDNF chapter 4 \u2014 didn\u2019t like the alternating book and story format\n\nby Charlotte Bond (fantasy) DNF chapter 4 \u2014 didn\u2019t like the alternating book and story format Triad\n\nby Kate Pearce\n\n(romance, sci-fi)\n\nDNF first chapter \u2014 wasn\u2019t feeling it\n\nby Kate Pearce (romance, sci-fi) DNF first chapter \u2014 wasn\u2019t feeling it Hearts of Oak\n\nby Eddie Robson\n\n(sci-fi)\n\nDNF page 10 \u2014 strange and distant\n\nby Eddie Robson (sci-fi) DNF page 10 \u2014 strange and distant Mastered by Her Mates\n\nby Grace Goodwin\n\n(romance, sci-fi)\n\nDNF 50% \u2014 didn\u2019t like one of the male leads, the heroine was annoying\n\nby Grace Goodwin (romance, sci-fi) DNF 50% \u2014 didn\u2019t like one of the male leads, the heroine was annoying Over the Line\n\nby Elise Faber\n\n(romance, contemporary)\n\nDNF 5% \u2014 heroine was crazy\n\nby Elise Faber (romance, contemporary) DNF 5% \u2014 heroine was crazy Disenchanted & Co.\n\nby Lynn Viehl\n\n(romance, steampunk)\n\nDNF 12% \u2014 first encounter with the love interest reads as assault\u2026 shame because I quite liked the main character and found the world interesting\n\nby Lynn Viehl (romance, steampunk) DNF 12% \u2014 first encounter with the love interest reads as assault\u2026 shame because I quite liked the main character and found the world interesting On Writing and Failure\n\nby Stephen Marche\n\n(non-fiction, essay)\n\nDNF page 31 \u2014 didn\u2019t like the tone or message\n\nby Stephen Marche (non-fiction, essay) DNF page 31 \u2014 didn\u2019t like the tone or message A Taste of Gold and Iron\n\nby Alexandra Rowland\n\n(romance, fantasy)\n\nDNF 6% \u2014 put it down and never went back\n\nby Alexandra Rowland (romance, fantasy) DNF 6% \u2014 put it down and never went back Mercenary Courage\n\nby Ruby Lionsdrake\n\n(romance, sci-fi)\n\nDNF 28% \u2014 wasn\u2019t feeling the conflict\n\nby Ruby Lionsdrake (romance, sci-fi) DNF 28% \u2014 wasn\u2019t feeling the conflict And Then He Kissed Me\n\nby C.M. Daniels\n\n(romance, historical)\n\nDNF 24% \u2014 slow going\n\nby C.M. Daniels (romance, historical) DNF 24% \u2014 slow going A Lady\u2019s Guide to Marvels and Misadventure\n\nby Angela Bell\n\n(romance, historical)\n\nDNF chapter 1 \u2014 heroine comes across insufferable\n\nby Angela Bell (romance, historical) DNF chapter 1 \u2014 heroine comes across insufferable Everyone in My Family Has Killed Someone\n\nby Benjamin Stevenson\n\n(mystery)\n\nDNF after chapter 1 \u2014 don\u2019t like the narrator\n\nby Benjamin Stevenson (mystery) DNF after chapter 1 \u2014 don\u2019t like the narrator Noor\n\nby Nnedi Okorafor\n\n(sci-fi)\n\nDNF after prologue \u2014 don\u2019t like the flash-forward technique\n\nby Nnedi Okorafor (sci-fi) DNF after prologue \u2014 don\u2019t like the flash-forward technique Faring Soul\n\nby Tracy Cooper-Posey\n\n(sci-fi, romance)\n\nDNF after chapter 1 \u2014 wasn\u2019t feeling it\n\nby Tracy Cooper-Posey (sci-fi, romance) DNF after chapter 1 \u2014 wasn\u2019t feeling it Strange Beasts\n\nby Susan J. Morris\n\n(mystery, steampunk)\n\nDNF after chapter 1 \u2014 heroine\u2019s motives are being concealed from the reader\n\nby Susan J. Morris (mystery, steampunk) DNF after chapter 1 \u2014 heroine\u2019s motives are being concealed from the reader A Letter to the Luminous Deep\n\nby Sylvie Cathrall\n\n(romance, fantasy)\n\nDNF third page \u2014 too much description, boring\n\nby Sylvie Cathrall (romance, fantasy) DNF third page \u2014 too much description, boring Miss Lattimore\u2019s Letter\n\nby Suzanne Allain\n\n(romance, historical)\n\nDNF 33% \u2014 just fine\n\nby Suzanne Allain (romance, historical) DNF 33% \u2014 just fine Letters to Half Moon Street\n\nby Sarah Wallace\n\n(romance, historical)\n\nDNF 20% \u2014 I think all-epistolary is holding this back, was too slow for me\n\nby Sarah Wallace (romance, historical) DNF 20% \u2014 I think all-epistolary is holding this back, was too slow for me A Vile Season\n\nby David Ferraro\n\n(fantasy, romance)\n\nDNF chapter 3 \u2014 fun but not quite feeling it at the moment\n\nby David Ferraro (fantasy, romance) DNF chapter 3 \u2014 fun but not quite feeling it at the moment Annie Bot\n\nby Sierra Greer\n\n(sci-fi, literary)\n\nDNF 14% \u2014 too much abuse for my preferences\n\nby Sierra Greer (sci-fi, literary) DNF 14% \u2014 too much abuse for my preferences Alien Desire\n\nby Melody Beckett\n\n(romance, sci-fi)\n\nDNF 50% \u2014 didn\u2019t like where the story was going\n\nby Melody Beckett (romance, sci-fi) DNF 50% \u2014 didn\u2019t like where the story was going Lady Eve\u2019s Last Con\n\nby Rebecca Fraimow\n\n(sci-fi, heist)\n\ndon\u2019t really like heist stories\n\nby Rebecca Fraimow (sci-fi, heist) don\u2019t really like heist stories Fiasco\n\nby Constance Fay\n\n(romance, sci-fi)\n\nDNF first chapter \u2014 didn\u2019t like narrator / first person / present tense\n\nby Constance Fay (romance, sci-fi) DNF first chapter \u2014 didn\u2019t like narrator / first person / present tense Eryx\n\nby Kate Stevens\n\n(romance, sci-fi)\n\nDNF 30% \u2014 this bears little resemblance to the blurb\n\nby Kate Stevens (romance, sci-fi) DNF 30% \u2014 this bears little resemblance to the blurb Comeuppance Served Cold\n\nby Marion Deeds\n\n(mystery, fantasy)\n\nDNF 14% \u2014 unbelievable and boring infodump conversations, confusing time jumps between chapters\n\nby Marion Deeds (mystery, fantasy) DNF 14% \u2014 unbelievable and boring infodump conversations, confusing time jumps between chapters The Four Dimensional Human\n\nby Laurence Scott\n\n(non-fiction, technology)\n\ntaking too long to get to the point\n\nby Laurence Scott (non-fiction, technology) taking too long to get to the point Elements of Taste\n\nby Benjamin Errett\n\n(cultural analysis)\n\nDNF after intro \u2014 idea of mapping flavors to taste profiles sounds neat but doesn\u2019t bear out\n\nby Benjamin Errett (cultural analysis) DNF after intro \u2014 idea of mapping flavors to taste profiles sounds neat but doesn\u2019t bear out Annihilation\n\nby Jeff Vandermeer\n\n(sci-fi, weird)\n\nDNF 20% \u2014 unreliable narrator, gimmick of no names, wasn\u2019t hooked by hook\n\nJump to top", "label": 1}
{"title": "Weeknotes: June 7-13, 2025 \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/13/weeknotes-june-7-13-2025/", "content": "Highlight of the week: went on a neighborhood garden tour \ud83c\udf3c had such a great time! \ud83d\ude0a\n\nLooking forward to: hoping to meet some neighbors at a community potluck \ud83e\udd1e all the neighbors we knew moved away\n\nStuff I did:\n\n6.75 hours consulting \u2014 sent in a first draft deliverable for the new project\n\n3.25 hours writing\n\nupdated my retirement portfolio tracker spreadsheet and set up a Roth IRA for my husband \u2014 what\u2019s one more column?! \ud83e\udee0\n\nwrote in to the Planning Commission in support of upzoning some large sites in my city \u2014 the overall plans moved forward \ud83d\udc4f but apparently financing big apartment buildings is iffy rn so they allowed townhouses in hopes the project will redevelop sooner\u2026 sounds like the right call sadly\n\nspent an hour submitting complaints on a community feedback map about intersections and other conflict zones between cars and people walking and biking\u2026 feel like I already did this for the Active Transportation Plan but I\u2019m not missing a single opportunity to point out safety problems\n\nswitched the heat pump from heating mode to cooling mode and brought the thermostat upstairs\u2026 except then it got cold again! Junuary strikes again\n\ngot sick of long hair and lopped off maybe 8\u2033 \u2014 I went shorter than last time and used a different technique of splitting my hair in three parts \u2014 I need some haircutting shears because my sewing scissors aren\u2019t cutting it, literally \u2702\ufe0f\n\nbaked sourdough banana bread muffins \ud83d\udc4d\u2013 made 12 muffins + a ramekin\n\nhusband needed to pick some international colleagues up at the airport and ferry them around when I wanted to do the garden tour, so we rented a car for the day \u2014 a little annoying but better than owning a second car for these rare occasions\n\nwent out to dinner with husband\u2019s colleagues, only a minor fiasco \ud83d\ude02\ud83e\udd37\u200d\u2640\ufe0f\n\none virtual appointment\n\none walk with my friend \u2014 too hot on the other day we had planned\n\nDinners:\n\nleftover Indian food\n\n\u201c7\u201d layer dip with chips and pineapple\n\nleftover egg salad + muffin\n\ncorn quesadillas with avocado and sour cream + prickly pear lemonade (with this prickly pear syrup)\n\nThai \u2014 radh nah with tofu\n\nsaucy chickpeas on ciabatta \u2014 made a double batch, took 45 minutes with prep, not bad\n\nfrozen waffles with strawberries, blueberries, and syrup + frozen veggie sausages + air fryer potatoes (I get this kind called Bintje from the local market and I swear it is the Platonic ideal of potato flavor)\n\nReading:\n\nRough reading week, wasn\u2019t feeling a lot of what I tried.\n\nRead The Work of Art in the Age of Mechanical Reproduction by Walter Benjamin and Roommate by Sarina Bowen\n\nStarted reading We Will Not Cancel Us by Adrienne Maree Brown and Simulacra and Simulation by Jean Baudrillard\n\nDNF\u2019d Immediacy by Anna Kornbluh, Rogue Enforcer by Grace Goodwin, Gentle Rogue by Johanna Lindsey, How to Steal a Galaxy by Beth Revis, Evocation by S.T. Gibson, and When the Tides Held the Moon by Venessa Vida Kelley\n\nPre-ordered Ew, It\u2019s Beautiful by Joshua Barkman and Blank Space by W. David Marx\n\nOrdered a used copy of The Unreal America: Architecture and Illusion by Ada Louise Huxtable and For Scale print issue 2\n\nWords I looked up / concepts I learned:\n\nergodic theory\n\neffulge (what an ugly word for a pretty concept)\n\ndiscretization\n\ncelerity\n\naffright (see also: word I will never use)\n\nredounds (this one is actually a kinda cool word for the archaic meaning, has a good sound to it; do not like it for the meaning \u201caccrue\u201d which is a much better known word)\n\nstopples\n\nalterity (probably a useful word but has an air of haughtiness?)\n\nphenotypic plasticity (via this video on locusts and grasshoppers)\n\nelite panic (via)\n\nPretty stuff I saw:\n\nNew music I listened to:\n\nNature notes:", "label": 1}
{"title": "Recommending for Long-Term Member Satisfaction at Netflix", "url": "https://netflixtechblog.com/recommending-for-long-term-member-satisfaction-at-netflix-ac15cada49ef?source=collection_home---4------17-----------------------", "content": "Recommending for Long-Term Member Satisfaction at Netflix Netflix Technology Blog 8 min read \u00b7 Aug 29, 2024 -- 9 Listen Share\n\nBy Jiangwei Pan, Gary Tang, Henry Wang, and Justin Basilico\n\nIntroduction\n\nOur mission at Netflix is to entertain the world. Our personalization algorithms play a crucial role in delivering on this mission for all members by recommending the right shows, movies, and games at the right time. This goal extends beyond immediate engagement; we aim to create an experience that brings lasting enjoyment to our members. Traditional recommender systems often optimize for short-term metrics like clicks or engagement, which may not fully capture long-term satisfaction. We strive to recommend content that not only engages members in the moment but also enhances their long-term satisfaction, which increases the value they get from Netflix, and thus they\u2019ll be more likely to continue to be a member.\n\nRecommendations as Contextual Bandit\n\nOne simple way we can view recommendations is as a contextual bandit problem. When a member visits, that becomes a context for our system and it selects an action of what recommendations to show, and then the member provides various types of feedback. These feedback signals can be immediate (skips, plays, thumbs up/down, or adding items to their playlist) or delayed (completing a show or renewing their subscription). We can define reward functions to reflect the quality of the recommendations from these feedback signals and then train a contextual bandit policy on historical data to maximize the expected reward.\n\nImproving Recommendations: Models and Objectives\n\nThere are many ways that a recommendation model can be improved. They may come from more informative input features, more data, different architectures, more parameters, and so forth. In this post, we focus on a less-discussed aspect about improving the recommender objective by defining a reward function that tries to better reflect long-term member satisfaction.\n\nRetention as Reward?\n\nMember retention might seem like an obvious reward for optimizing long-term satisfaction because members should stay if they\u2019re satisfied, however it has several drawbacks:\n\nNoisy : Retention can be influenced by numerous external factors, such as seasonal trends, marketing campaigns, or personal circumstances unrelated to the service.\n\n: Retention can be influenced by numerous external factors, such as seasonal trends, marketing campaigns, or personal circumstances unrelated to the service. Low Sensitivity : Retention is only sensitive for members on the verge of canceling their subscription, not capturing the full spectrum of member satisfaction.\n\n: Retention is only sensitive for members on the verge of canceling their subscription, not capturing the full spectrum of member satisfaction. Hard to Attribute : Members might cancel only after a series of bad recommendations.\n\n: Members might cancel only after a series of bad recommendations. Slow to Measure: We only get one signal per account per month.\n\nDue to these challenges, optimizing for retention alone is impractical.\n\nProxy Rewards\n\nInstead, we can train our bandit policy to optimize a proxy reward function that is highly aligned with long-term member satisfaction while being sensitive to individual recommendations. The proxy reward r(user, item) is a function of user interaction with the recommended item. For example, if we recommend \u201cOne Piece\u201d and a member plays then subsequently completes and gives it a thumbs-up, a simple proxy reward might be defined as r(user, item) = f(play, complete, thumb).\n\nClick-through rate (CTR)\n\nClick-through rate (CTR), or in our case play-through rate, can be viewed as a simple proxy reward where r(user, item) = 1 if the user clicks a recommendation and 0 otherwise. CTR is a common feedback signal that generally reflects user preference expectations. It is a simple yet strong baseline for many recommendation applications. In some cases, such as ads personalization where the click is the target action, CTR may even be a reasonable reward for production models. However, in most cases, over-optimizing CTR can lead to promoting clickbaity items, which may harm long-term satisfaction.\n\nBeyond CTR\n\nTo align the proxy reward function more closely with long-term satisfaction, we need to look beyond simple interactions, consider all types of user actions, and understand their true implications on user satisfaction.\n\nWe give a few examples in the Netflix context:\n\nFast season completion \u2705: Completing a season of a recommended TV show in one day is a strong sign of enjoyment and long-term satisfaction.\n\n\u2705: Completing a season of a recommended TV show in one day is a strong sign of enjoyment and long-term satisfaction. Thumbs-down after completion \u274c: Completing a TV show in several weeks followed by a thumbs-down indicates low satisfaction despite significant time spent.\n\n\u274c: Completing a TV show in several weeks followed by a thumbs-down indicates low satisfaction despite significant time spent. Playing a movie for just 10 minutes \u2753: In this case, the user\u2019s satisfaction is ambiguous. The brief engagement might indicate that the user decided to abandon the movie, or it could simply mean the user was interrupted and plans to finish the movie later, perhaps the next day.\n\n\u2753: In this case, the user\u2019s satisfaction is ambiguous. The brief engagement might indicate that the user decided to abandon the movie, or it could simply mean the user was interrupted and plans to finish the movie later, perhaps the next day. Discovering new genres \u2705 \u2705: Watching more Korean or game shows after \u201cSquid Game\u201d suggests the user is discovering something new. This discovery was likely even more valuable since it led to a variety of engagements in a new area for a member.\n\nReward Engineering\n\nReward engineering is the iterative process of refining the proxy reward function to align with long-term member satisfaction. It is similar to feature engineering, except that it can be derived from data that isn\u2019t available at serving time. Reward engineering involves four stages: hypothesis formation, defining a new proxy reward, training a new bandit policy, and A/B testing. Below is a simple example.\n\nChallenge: Delayed Feedback\n\nUser feedback used in the proxy reward function is often delayed or missing. For example, a member may decide to play a recommended show for just a few minutes on the first day and take several weeks to fully complete the show. This completion feedback is therefore delayed. Additionally, some user feedback may never occur; while we may wish otherwise, not all members provide a thumbs-up or thumbs-down after completing a show, leaving us uncertain about their level of enjoyment.\n\nWe could try and wait to give a longer window to observe feedback, but how long should we wait for delayed feedback before computing the proxy rewards? If we wait too long (e.g., weeks), we miss the opportunity to update the bandit policy with the latest data. In a highly dynamic environment like Netflix, a stale bandit policy can degrade the user experience and be particularly bad at recommending newer items.\n\nSolution: predict missing feedback\n\nWe aim to update the bandit policy shortly after making a recommendation while also defining the proxy reward function based on all user feedback, including delayed feedback. Since delayed feedback has not been observed at the time of policy training, we can predict it. This prediction occurs for each training example with delayed feedback, using already observed feedback and other relevant information up to the training time as input features. Thus, the prediction also gets better as time progresses.\n\nThe proxy reward is then calculated for each training example using both observed and predicted feedback. These training examples are used to update the bandit policy.\n\nBut aren\u2019t we still only relying on observed feedback in the proxy reward function? Yes, because delayed feedback is predicted based on observed feedback. However, it is simpler to reason about rewards using all feedback directly. For instance, the delayed thumbs-up prediction model may be a complex neural network that takes into account all observed feedback (e.g., short-term play patterns). It\u2019s more straightforward to define the proxy reward as a simple function of the thumbs-up feedback rather than a complex function of short-term interaction patterns. It can also be used to adjust for potential biases in how feedback is provided.\n\nThe reward engineering diagram is updated with an optional delayed feedback prediction step.\n\nTwo types of ML models\n\nIt\u2019s worth noting that this approach employs two types of ML models:\n\nDelayed Feedback Prediction Models : These models predict p(final feedback | observed feedbacks). The predictions are used to define and compute proxy rewards for bandit policy training examples. As a result, these models are used offline during the bandit policy training.\n\n: These models predict p(final feedback | observed feedbacks). The predictions are used to define and compute proxy rewards for bandit policy training examples. As a result, these models are used offline during the bandit policy training. Bandit Policy Models: These models are used in the bandit policy \u03c0(item | user; r) to generate recommendations online and in real-time.\n\nChallenge: Online-Offline Metric Disparity\n\nImproved input features or neural network architectures often lead to better offline model metrics (e.g., AUC for classification models). However, when these improved models are subjected to A/B testing, we often observe flat or even negative online metrics, which can quantify long-term member satisfaction.\n\nThis online-offline metric disparity usually occurs when the proxy reward used in the recommendation policy is not fully aligned with long-term member satisfaction. In such cases, a model may achieve higher proxy rewards (offline metrics) but result in worse long-term member satisfaction (online metrics).\n\nNevertheless, the model improvement is genuine. One approach to resolve this is to further refine the proxy reward definition to align better with the improved model. When this tuning results in positive online metrics, the model improvement can be effectively productized. See [1] for more discussions on this challenge.\n\nSummary and Open Questions\n\nIn this post, we provided an overview of our reward engineering efforts to align Netflix recommendations with long-term member satisfaction. While retention remains our north star, it is not easy to optimize directly. Therefore, our efforts focus on defining a proxy reward that is aligned with long-term satisfaction and sensitive to individual recommendations. Finally, we discussed the unique challenge of delayed user feedback at Netflix and proposed an approach that has proven effective for us. Refer to [2] for an earlier overview of the reward innovation efforts at Netflix.\n\nAs we continue to improve our recommendations, several open questions remain:\n\nCan we learn a good proxy reward function automatically by correlating behavior with retention?\n\nHow long should we wait for delayed feedback before using its predicted value in policy training?\n\nHow can we leverage Reinforcement Learning to further align the policy with long-term satisfaction?\n\nReferences\n\n[1] Deep learning for recommender systems: A Netflix case study. AI Magazine 2021. Harald Steck, Linas Baltrunas, Ehtsham Elahi, Dawen Liang, Yves Raimond, Justin Basilico.\n\n[2] Reward innovation for long-term member satisfaction. RecSys 2023. Gary Tang, Jiangwei Pan, Henry Wang, Justin Basilico.", "label": 0}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2017", "content": "en\n\nBeing away from home these last few months have been great. I enjoy the freedoms, living independently and eating good food whenever I want. Life has treated me good here. Until more recently, a certain stage of being away finally starts. Lately, homesickness has really started to kick in. I\u2019ve really started to miss my friends, family and things I really took for granted back home. It really is bad when I am craving my mom\u2019s homemade food. (Mexican Food.) Midterms and all the studying is really killing my vibe at this point of the semester. My program is not your typical study abroad program that you go away for 3 months (not to mention that it doesn\u2019t even apply to your major) on a semi vacation. I actually have to try real hard as my french studies here in Strasbourg France will help me graduate next year. Enough bashing.\n\nWith all this, one just feels that they are slowly losing control. Sure I am enjoying myself out here in France but still there is this emptiness of something that I have been away from for far too long. Hopefully, I hope some sort of enlightenment will come bring me back to life. Then It hit me, I would see my friends in just a few weeks. I honestly couldn\u2019t believe in such short time I would be right in front of them.\n\nSo I would be meeting up my good friend Sammy and soon-to-be friend Mitch. They have never been to Europe before, so I figured why not show them around Paris, a city that I know decently and use my French to get to Point A to Point B and of course with the help of good ol\u2019 Google Maps. Paris- Weekend 1\n\nThese guys had a hell of a time getting to Paris during the 24 hours before arrival. I\u2019ll let them explain that one.\n\nSo I picked the guys up from Charles De Gaulle Aiport. I still feel bad on making you guys wait for me. :/ I ran up to my dude Sammy and gave him a bro hug! It has been way to long since I have seen that dude! I also got to meet Mitch and got to know him more through out the weekend. Cool Dude! So I showed them around Paris. To be honest, Paris is beautiful with history written all around. As with any major city, Paris is no exception, it is dirty and smelly. I really hope that the guys did not have too high of hopes for the \u201cCity of Love.\u201d Paris is overrated in my opinion but why not take advantage of leaving Strasbourg for a few days.\n\nSo we went to various monuments such as The Louvre, Arc of Triumph, Versailles and so much more. Many of the places were breath taking. The Eiffel Tour is always a good place to scope out as well as walking along the Seine River. Getting around Paris is no piece of cake but we managed. Never have I ever gone up so many round staircases. Climbing the Cathedral of Notre Dame was such a struggle. Whenever you are in Europe, walking is really the way to get around.\n\nNight life in Paris is pretty popping! We went to a few clubs in the Grand Boulevard District! This district did not disappoint. I wish we could have checked out a few other ones. One of my favorite places was the Irish Pub. Music, dancing, girls and drinks. Man was that a fun night!\n\nWe ate some good food throughout Paris. The guys had their first legit French Croissants and baguette sandwich. It\u2019s totally acceptable to have a sandwich on you at all times. I also had them try one of my favorites, a kebab sandwich. Man are they delicious! Sammy had actual ramen noodles for the first time ever! Overall we pretty much stuffed our faces all weekend.\n\nBruxelles- Weekend 2\n\nAh Bruxelles!! The place known for its deliciouos fries and waffles. You have no idea how much I stuffed my face this weekend. Just writing this post makes my mouth water. Was eating all this food this worth it? You bet it was!! When I arrived in Bruxelles, it was gloomy and misting. Not the way I wanted to start the weekend. The weather stayed the same all weekend. That did not stop the fun.\n\nSpending the weekend in Brussels was totally worth the waffles! The sugar definitely made me bounce off the walls. #belgium #worldtravel #unkstudyabroad A post shared by Pablo Morales (@pablo.morales1) on Mar 19, 2017 at 9:55am PDT\n\nWe checked out many popular attractions throughout the city. There was so much we saw, I started to lose my mind a bit. One of the coolest places I saw was a \u201cWorld Shop.\u201d They had souvenirs and artsy things from all around the world. I was so intrigued with the place that I ended up buying a few things from there.\n\nThe other purpose of coming to Bruxelles was to meet up with our friend Kellie and Sammy\u2019s brother, Louis. It was good to see some other friends from back home. I missed those two! Kellie is just a keeper, to keep a long story short. These two are actually on Spring Break right now. I am so jealous of you guys spending your holiday in Malta! I hope you enjoy every second of it!\n\nBruxelles treated everyone well!\n\nYour browser does not support the video tag.\n\nThese last few weekends have been pretty awesome! It was good to see a few friends from back home. I couldn\u2019t of asked for a better time. Traveling becomes more exciting when you are hanging out with people you know and love. I can\u2019t wait for the next adventure with them! So, is it sprangggg breakkk yet? I wish!", "label": 1}
{"title": "Breaking down CPU speed: How utilization impacts performance", "url": "https://github.blog/engineering/architecture-optimization/breaking-down-cpu-speed-how-utilization-impacts-performance/", "content": "Introduction \u26f5\n\nThe GitHub Performance Engineering team regularly conducts experiments to observe how our systems perform under varying load conditions. A consistent pattern in these experiments is the significant impact of CPU utilization on system performance. We\u2019ve observed that as CPU utilization rises, it can lead to increased latency, which provides an opportunity to optimize system efficiency. Addressing this challenge allows us to maintain performance levels while reducing the need for additional machines, ultimately preventing inefficiencies.\n\nAlthough we recognized the correlation between higher CPU utilization and increased latency, we saw an opportunity to explore the specific thresholds and impacts at various stages in greater detail. With a diverse set of instance types powered by different CPU families, we focused on understanding the unique performance characteristics of each CPU model. This deeper insight empowered us to make smarter, data-driven decisions, enabling us to provision our infrastructure with greater efficiency and confidence.\n\nWith these goals in mind, we embarked on a new journey of exploration and experimentation to uncover these insights.\n\nExperiment setup \ud83e\uddf0\n\nCollecting accurate data for this type of experiment was no easy feat. We needed to gather data from workloads that were as close to our production as possible, while also capturing how the system behaves under different phases of load. Since CPU usage patterns vary across workloads, we focused primarily on our flagship workloads. However, increasing the load could introduce small performance discrepancies, so our goal was to minimize disruption for our users.\n\nFortunately, a year ago, the Performance Engineering team developed an environment designed to meet these requirements, codenamed Large Unicorn Collider (LUC). This environment operates within a small portion of our Kubernetes clusters, mirroring the same architecture and configuration as our flagship workloads. It also has the flexibility to be hosted on dedicated machines, preventing interference from or with other workloads. Typically, the LUC environment remains idle, but when needed, we can direct a small, adjustable amount of traffic towards it. Activating or deactivating this traffic takes only seconds, allowing us to react quickly if performance concerns arise.\n\nTo accurately assess the impact of CPU utilization, we first established a baseline by sending moderate production traffic to a LUC Kubernetes pod hosted on one of its dedicated machines. This provided us with a benchmark for comparison. Importantly, the number of requests handled by the LUC pods remained constant throughout the experiment, ensuring consistent CPU load over time.\n\nOnce the baseline was set, we gradually increased CPU utilization using a tool called \u201cstress,\u201d which artificially occupies a specified number of CPU cores by running random processing tasks. Each instance type has a different number of CPU cores, so we adjusted the steps accordingly. However, the common factor across all instances was the total CPU utilization.\n\nNote: It\u2019s important to recognize that this is not a direct 1:1 comparison to the load generated by actual production workloads. The stress tool continuously runs mathematical operations, while our production workloads involve I/O operations and interrupts, which place different demands on system resources. Nevertheless, this approach still offers valuable insights into how our CPUs perform under load.\n\nWith the environment set up and our plan in place, we proceeded to collect as much data as possible to analyze the impact.\n\nResults \ud83d\udcc3\n\nWith our experiment setup finalized, let\u2019s examine the data we gathered. As previously mentioned, we repeated the process across different instance types. Each instance type showed unique behavior and varying thresholds where performance started to decline.\n\nAs anticipated, CPU time increased for all instance types as CPU utilization rose. The graph below illustrates the CPU time per request as CPU utilization increases.\n\nCPU time per request vs CPU utilization\n\nThe latency differences between instance types are expected due to the variations in CPU models. Focusing on the percentage increase in latency may provide more meaningful insights.\n\nLatency percentage increase vs CPU utilization\n\nIn both graphs, one line stands out by deviating more than the others. We\u2019ll examine this case in detail shortly.\n\nTurbo Boost effect\n\nAn interesting observation is how CPU frequency changes as utilization increases, which can be attributed to Intel\u2019s Turbo Boost Technology. Since all the instances we used are equipped with Intel CPUs, the impact of Turbo Boost is noticeable across all of them. In the graph below, you can see how the CPU frequency decreases as the CPU utilization increases. The red arrows are showing the CPU utilization level.\n\nCPU Cores Frequency\n\nWhen CPU utilization remains at lower levels (around 30% or below), we benefit from increased core frequencies, leading to faster CPU times and, consequently, lower overall latency. However, as the demand for more CPU cores rises and utilization increases, we are likely to reach the CPU\u2019s thermal and power limits, causing frequencies to decrease. In essence, lower CPU utilization results in better performance, while higher utilization leads to a decline in performance. For instance, a workload running on a specific node with approximately 30% CPU utilization will report faster response times compared to the same workload on the same VM when CPU utilization exceeds 50%.\n\nVariations in CPU frequency are not the only factors influencing performance changes. All our nodes have Hyper-Threading enabled, an Intel technology that allows a single physical CPU core to operate as two virtual cores. Although there is only one physical core, the Linux kernel recognizes it as two virtual CPU cores. The kernel attempts to distribute the CPU load across these cores, aiming to keep only one hardware thread (virtual core) busy per physical core. This approach is effective until we reach a certain level of CPU utilization. Beyond this threshold, we cannot fully utilize both virtual CPU cores, resulting in reduced performance compared to normal operation.\n\nFinding the \u201cGolden Ratio\u201d of CPU utilization\n\nUnderutilized nodes lead to wasted resources, power, and space in our data centers, while nodes that are excessively utilized also create inefficiencies. As noted, higher CPU utilization results in decreased performance, which can give a misleading impression that additional resources are necessary, resulting in a cycle of over-provisioning. This issue is particularly pronounced with blocking workloads that do not follow an asynchronous model. As CPU performance deteriorates, each process can manage fewer tasks per second, making existing capacity inadequate. To achieve the optimal balance\u2014the \u201cGolden Ratio\u201d of CPU utilization\u2014we must identify a threshold where CPU utilization is sufficiently high to ensure efficiency without significantly impairing performance. Striving to keep our nodes near this threshold will enable us to utilize our current hardware more effectively alongside our existing software.\n\nSince we already have experimental data demonstrating how CPU time increases with rising utilization, we can develop a mathematical model to identify this threshold. First, we need to determine what percentage of CPU time degradation is acceptable for our specific use case. This may depend on user expectations or performance Service Level Agreements (SLAs). Once we establish this threshold, it will help us select a level of CPU utilization that remains within acceptable limits.\n\nWe can plot the CPU utilization vs. CPU time (latency) and find the point where:\n\nCPU utilization is high enough to avoid resource underutilization.\n\nCPU time degradation does not exceed your acceptable limit.\n\nA specific example derived from the data above can be illustrated in the following graph.\n\nPercentage Increase in P50 Latency vs CPU Utilization\n\nIn this example, we aim to achieve less than 40% CPU time degradation, which would correspond to a CPU utilization of 61% on the specific instance.\n\nOutlier case\n\nAs previously mentioned, there was a specific instance that displayed some outlying data points. Our experiment confirmed an already recognized issue where certain instances were not achieving their advertised maximum Turbo Boost CPU frequency. Instead, we observed steady CPU frequencies that fell below the maximum advertised value under low CPU utilization. In the example below, you can see an instance from a CPU family that advertises Turbo Boost frequencies above 3 GHz, but it is only reporting a maximum CPU frequency of 2.8 GHz.\n\nCPU cores frequency\n\nThis issue turned out to be caused by a disabled CPU C-state, which prevented the CPU cores from halting even when they were not in use. As a result, these cores were perceived as \u201cbusy\u201d by the turbo driver, limiting our ability to take advantage of Turbo Boost benefits with higher CPU frequencies. By enabling the C-state and allowing for optimization and power reduction during idle mode, we observed the expected Turbo Boost behavior. This change had an immediate impact on the CPU time spent by our test workloads. The images below illustrate the prompt changes in CPU frequencies and latency reported following the C-state adjustment.\n\nCPU cores frequency\n\nP50 CPU time on a request\n\nUpon re-evaluating the percentage change in CPU time, we now observe similar behavior across all instances.\n\nPercentage Increase in P50 Latency vs CPU Utilization\n\nAs we anticipated many of these insights, our objective was to validate our theories using data from our complex system. While we confirmed that performance lowers as CPU utilization increases across different CPU families, by identifying optimal CPU utilization thresholds, we can achieve a better balance between performance and efficiency, ensuring that our infrastructure remains both cost-effective and high performing. Going forward, these insights will inform us of our resource provisioning strategies and help us maximize the effectiveness of our hardware investments.\n\nThank you for sticking with us until the end!! A special shout-out to @adrmike , @schlubbi , @terrorobe , the @github/compute-platform and finally the @github/performance-engineering team for their invaluable assistance throughout these experiments, data analysis, and for reviewing the content for accuracy and consistency. \u2764\ufe0f\n\nTags:", "label": 0}
{"title": "29", "url": "https://lifeofpablo.com/blog/29", "content": "29\n\nPablo eating Gunther's Ice Cream\n\nThis post was written in English (en_US).\n\n\n\nPablo eating Gunther's Ice Cream in Sacramento\n\nToday is the 6th of May, and I turn 29 years old! Woo hoo! I've lived on this earth for 29 years. Time flies and I honestly didn't think I would have made it this far but here I am! That being said, I have officially entered my last year of my 20s. It\u2019s a tad bitter sweet. but the positive overtakes the negative by a long shot. I reflect the growth I\u2019ve had when I entered my 20s.\n\n28 was a fun year of adventures and of learning. It was very fulfilling. I'm smiling as I think of the things I did this year. I would say 28 was a level up in all aspects. I felt more accomplished and I had an increased sense of self-direction.\n\nTraveled with friends\n\nTook myself on Artist Dates\n\nHelped develop a game with Friends\n\nGetting Google Certified in a few areas\n\nVoted in my first California Primary\n\nSang in a few choir concerts\n\n29 will be a great transition year into the next decade of my life. I\u2019m still discovering who I am as an individual. I now know more about myself than ever. I feel a bit wiser. This has been a year of a lot of reflection.\n\nThere is so much to learn about this world and where I stand in it. I continue to learn from people around me and the things unknown to me. What does it mean to be a Mexican-American at 29 years of age?\n\nA lot will happen this year. Many events this year will have an effect on me directly and indirectly. I am excited and terrified for what's to come. I will keep my head up and fight for the greater good. I've learned not to be anxious of things I can't control yet because they haven't happened yet. Being more in the present is key to the well being of myself.\n\nI've made many mistakes in years past. that I don't want to replicate. There are things I am not proud of but I've come to terms of not living in the past. I don't allow myself to beat myself up as hard as I used to. There's no need spend energy on things that don't matter anymore.\n\nJust learning to love myself this past year and I am continuing to work on myself. I'm in a better place now than when I had just turned 28. My mental health, my physical health, and emotional wellness are more in tune. Now that I've set the path, it's only up for these. I'm more balanced now.\n\nOn a positive note, I'm really excited to start school again. I really value learning and I have a desire to have a master's degree. Later this summer I'm also getting a promotion at work.\n\nSome items I wish to accomplish (no particular order) at 29\n\nLearn Chinese\n\nI WILL join cross fit or attend exercise classes.\n\nI would like to learn to dance.\n\nGet the startup I'm working on off the ground.\n\nWork on personal and work projects that have been on the back burner. I hope to reach these goals.\n\nJoin a LGBT sports league.\n\nGet my financials in order. I should set up a will.\n\nGo on a solo adventure far away from home.\n\nGo on more hikes!\n\nLooking Forward\n\nThere are so many positive things happening in my life. It's only up from here! I'm proud of who I'm becoming Here's to my 29th birthday and what's to come in the next 365 days around the sun!", "label": 1}
{"title": "Design system annotations, part 2: Advanced methods of annotating components", "url": "https://github.blog/engineering/user-experience/design-system-annotations-part-2-advanced-methods-of-annotating-components/", "content": "In part one of our design system annotation series, we discussed the ways in which accessibility can get left out of design system components from one instance to another. Our solution? Using a set of \u201cPreset annotations\u201d for each component with Primer. This allows designers to include specific pre-set details that aren\u2019t already built into the component and visually communicated in the design itself.\n\nThat being said, Preset annotations are unique to each design system \u2014 and while ours may be a helpful reference for how to build them \u2014 they\u2019re not something other organizations can utilize if you\u2019re not also using the Primer design system.\n\nLuckily, you can build your own. Here\u2019s how.\n\nHow to make Preset annotations for your design system\n\nStart by assessing components to understand which ones would need Preset annotations\u2014not all of them will. Prioritize components that would benefit most from having a Preset annotation, and build that key information into each one. Next, determine what properties should be included. Only include key information that isn\u2019t conveyed visually, isn\u2019t in the component properties, and isn\u2019t already baked into a coded component.\n\nPrioritizing components\n\nWhen a design system has 60+ components, knowing where to start can be a challenge. Which components need these annotations the most? Which ones would have the highest impact for both design teams and our users?\n\nWhen we set out to create a new set of Preset annotations based on our proof of concept, we decided to use ten Primer components that would benefit the most. To help pick them, we used an internal tool called Primer Query that tracks all component implementations across the GitHub codebase as well as any audit issues connected to them. Here is a video breakdown of how it works, if you\u2019re curious.\n\nWe then prioritized new Preset annotations based on the following criteria:\n\nComponents that align to organization priorities (i.e. high value products and/or those that receive a lot of traffic). Components that appear frequently in accessibility audit issues. Components with React implementations (as our preferred development framework). Most frequently implemented components.\n\nMapping out the properties\n\nFor each component, we cross-referenced multiple sources to figure out what component properties and attributes would need to be added in each Preset annotation. The things we were looking for may only exist in one or two of those places, and thus are less likely to be accounted for all the way through the design and development lifecycle. The sources include:\n\nComponent documentation on Primer.style\n\nDesign system docs should contain usage guidance for designers and developers, and accessibility requirements should be a part of this guidance as well. Some of the guidance and requirements get built into the component\u2019s Figma asset, while some only end up in the coded component.\n\nLook for any accessibility requirements that are not built into either Figma or code. If it\u2019s built in, putting the same info in the Preset annotation may be redundant or irrelevant.\n\nPresets can account for rare use cases While building a Preset annotation for the TextInput component, we found that implementations may use an icon alone or have a hidden input label. With GitHub\u2019s global search or filter inputs, the magnifying glass icon alone can act as the visible label, but the fields still need an accessible label for assistive technology users.\n\nCoded demos in Storybook\n\nOur component sandbox helped us see how each component is built in React or Rails, as well as what the HTML output is. We looked for any code structure or accessibility attributes that are not included in the component documentation or the Figma asset itself\u2014especially when they may vary from one implementation to another.\n\nCode attributes a designer may not see or set Storybook helped us craft our TextInput component\u2019s Preset annotation by showing some important attributes that don\u2019t get any mention elsewhere. The type attribute is to the value of text . by default. Depending on the purpose of the field, an input\u2019s type could also be search , email , number , tel , date , or time . This should be set intentionally so that users are able to use the most appropriate virtual keyboard.\n\nComponent properties in the Figma asset library\n\nLibrary assets provide a lot of flexibility through text layers, image fills, variants, and elaborate sets of component properties. We paid close attention to these options to understand what designers can and can\u2019t change. Worthwhile additions to a Preset Annotation are accessibility attributes, requirements, and usage guidance in other sources that aren\u2019t built into the Figma component.\n\nWhat\u2019s missing from the TextInput\u2019s Figma component When a TextInput is added to a design, the Figma component comes with many customizable options. There is an inputTextType property, which is about visual design and typography, not the type of form input. It\u2019s possible to set the value of the Label and input field in Figma\u2019s sidebar, but because it\u2019s hidden by default, there\u2019s no option to set the text of an error validation message. We can\u2019t assume that every design delivered in Figma will come with examples of a form showing all of its error states, so these error messages may not get the attention they require. If this message can\u2019t be built into the component as a text property, it can be added to the Preset annotation.\n\nOther potential sources\n\nExperiences from team members: The designers, developers, and accessibility specialists you work with may have insight into things that the docs and design tools may have missed. If your team and design system have been around for a while, their insights may be more valuable than those you\u2019ll find in the docs, component demos, or asset libraries. Take some time to ask which components have had challenging bugs and which get intentionally broken when implemented.\n\nThe designers, developers, and accessibility specialists you work with may have insight into things that the docs and design tools may have missed. If your team and design system have been around for a while, their insights may be more valuable than those you\u2019ll find in the docs, component demos, or asset libraries. Take some time to ask which components have had challenging bugs and which get intentionally broken when implemented. Findings from recent audits: Design system components themselves may have unresolved audit issues and remediation recommendations. If that\u2019s the case, those issues are likely present in Storybook demos and may be unaccounted for in the component documentation. Design system audit issues may have details that both help create a Preset annotation and offer insights about what should not be carried over from existing resources.\n\nPutting it all together Our new Preset annotation for the TextInput component included links to usage guidance and Storybook as well as an optional tutorial for how the component is best used in a design to avoid potential issues. There are two mandatory prompts for input type and error text, and an optional one for the occasional hidden form label.\n\nWhat we learned from creating Preset annotations\n\nPreset annotations may not be for every team or organization. However, they are especially well suited for younger design systems and those that aren\u2019t well adopted.\n\nMature design systems like Primer have frequent updates. This means that without close monitoring, the design system components themselves may fall out of sync with how a Preset annotation is built. This can end up causing confusion and rework after development starts, so it may be wise to make sure there\u2019s some capacity to maintain these annotations after they\u2019ve been created.\n\nFor newer teams at GitHub, new members of existing teams, and team members who were less familiar with the design system, the built-in guidance and links to documentation and component demos proved very useful. Those who are more experienced are also able to fine-tune the Presets and how they\u2019re used.\n\nIf you don\u2019t already have extensive experience with the design system components (or peers to help build them), it can take a lot of time to assess and map out the properties needed to build a Preset. It can also be challenging to name a component property succinctly enough that it doesn\u2019t get truncated in Figma\u2019s properties panel. If the context is not self-evident, some training or additional documentation may help.\n\nIt\u2019s not always clear that you need a Preset annotation\n\nThere may be enough overlap between the Preset annotation for a component and types of annotations that aren\u2019t specific to the design system.\n\nFor example, the GitHub Annotation Toolkit has components to annotate basic <textarea> form elements in addition to a Preset annotation for our <TextArea> Primer component:\n\nIn many instances, this flexibility may be confusing because you could use either annotation. For example, the Primer <TextArea> Preset has built-in links to specific Primer docs, and while the non-Preset version doesn\u2019t, you could always add the links manually. While there\u2019s some overlap between the two, using either one is better than none.\n\nOne way around this confusion is to add Primer-specific properties to the default set of annotations. This would allow you to do things like toggle a boolean property on a normal Button annotation and have it show links and properties specific to your design system\u2019s button component.\n\nOur Preset creation process may unlock automation\n\nThere are currently a number of existing Figma plugins that advertise the ability to scan a design file to help with annotations. That being said, the results are often mixed and contain an unmanageable amount of noise and false positives. One of the reasons these issues happen is that these public plugins are design system agnostic.\n\nCurrent automated annotation tools aren\u2019t able to understand that any design system components are being used without bespoke programming or thorough training of AI models. For plugins like this to be able to label design elements accurately, they first need to understand how to identify the components on the canvas, the variants used, and the set properties.\n\nWith that in mind, perhaps the most exciting insight is that the process of mapping out component properties for a Preset annotation\u2014the things that don\u2019t get conveyed in the visual design or in the code\u2014is also something that would need to be done in any attempt to automate more usable annotations.\n\nIn other words, if a team uses a design system and wants to automate adding annotations, the tool they use would need to understand their components. In order for it to understand their components well enough to automate accurately, these hidden component properties would need to be mapped out. The task of creating a set of Preset annotations may be a vital stepping stone to something even more streamlined.\n\nA promising new method: Figma\u2019s Code Connect\n\nWhile building our new set of Preset annotations, we experimented with other ways to enhance Primer with annotations. Though not all of those experiments worked out, one of them did: adding accessibility attributes through Code Connect.\n\nPrimer was one of the early adopters of Figma\u2019s new Code Connect feature in Dev Mode. Says Lukas Oppermann, our staff systems designer, \u201cWith Code Connect, we can actually move the design and the code a little bit further apart again. We can concentrate on creating the best UX for the designers working in Figma with design libraries and, on the code side, we can have the best developer experience.\u201d\n\nTo that end, Code Connect allows us to bypass much of our Preset annotations, as well as the downsides of some of our other experiments. It does this by adding key accessibility details directly into the code that developers can export from Figma.\n\nGitHub\u2019s Octicons are used in many of our Primer components. They are decorative by default, but they sometimes need alt text or aria-label attributes depending on how they\u2019re used. In the IconButton component, that button uses an Octicon and needs an accessible name to describe its function.\n\nWhen using a basic annotation kit, this may mean adding stamps for a Button and Decorative Image as well as a note in the margins that specifies what the aria-label should be. When using Preset annotations, there are fewer things to add to the canvas and the annotation process takes less time.\n\nWith Code Connect set up, Lukas added a hidden layer in the IconButton Figma component. It has a text property for aria-label which lets designers add the value directly from the component properties panel. No annotations needed. The hidden layer doesn\u2019t disrupt any of the visuals, and the aria-label property gets exported directly with the rest of the component\u2019s code.\n\nIt takes time to set up Code Connect with each of your design system components. Here are a few tips to help:\n\nConsistency is key. Make sure that the properties you create and how you place hidden layers is consistent across components. This helps set clear expectations so your teams can understand how these hidden layers and properties function.\n\nMake sure that the properties you create and how you place hidden layers is consistent across components. This helps set clear expectations so your teams can understand how these hidden layers and properties function. Use a branch of your design system library to experiment. Hiding attributes like aria-label is quite simple compared to other complex information that Preset annotations are capable of handling.\n\nHiding attributes like aria-label is quite simple compared to other complex information that Preset annotations are capable of handling. Use visual regression testing (VRT). Adding complexity directly to a component comes with increased risk of things breaking in the future, especially for those with many variants. Figma\u2019s merge conflict UI is helpful, but may not catch everything.\n\nAs we continue to innovate with annotations and make our components more accessible, we are aiming to release our GitHub Annotation Toolkit in the near future. Stay tuned!\n\nFurther reading\n\nAccessibility annotation kits are a great resource, provided they\u2019re used responsibly. Eric Bailey, one of the contributors to our forthcoming GitHub Annotation Toolkit, has written extensively about how annotations can highlight and amplify deeply structural issues when you\u2019re building digital products.\n\nTags:", "label": 0}
{"title": "Juanita Garden Tour \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/07/juanita-garden-tour/", "content": "I had a blast visiting five gardens around the Juanita neighborhood \ud83d\ude03 There were 12 gardens on the tour but I think you\u2019d be hard-pressed to see them all! I started at 11 and was wiped by 2:30 so I didn\u2019t push myself to squeeze in one more.\n\nEnjoyed talking with several of the homeowners too \u2014 one of them has written a book on mushrooming! He had a massive table spread with morels drying in the sun.\n\n\u201cChampagne Creek\u201d\n\n\u201cThis private garden has expanded and grown into a utopia of rare plants, exotic fowl, native habitat and so much more.\u201d \u201cWhimsical eclecticism is Kathy\u2019s guiding principle.\u201d\n\n\u201cBloom\u201d\n\n\u201cThis 1/2-acre garden is an artistic attempt to paint with flowers and plants: full of color, texture and whimsy.\u201d\n\n\u201cThe Half Acre Wood\u201d\n\n\u201cEvergreen Hollow\u201d\n\n\u201cNestled in the lush climate of the PNW, Evergreen Hollow is a harmonious fusion of Japanese serenity, whimsical fairy charm, and intricate bonsai artistry.\u201d\n\n\u201cDahlia House Gardens\u201d\n\n\u201cOur gardens, at over an acre, are a controlled chaos of thousands of perennials, fruit trees, berry bushes and veggie gardens. We have collected seeds and plants from Tibet, Europe and elsewhere and enjoy a very English style profusion with lawn and intertwined stairs and hidden trails.\u201d (This garden was very much my vibe.)\n\nTakeaways\n\nIt\u2019s lovely to see stunning gardens like these, but also having paid for landscaping, I have to distinguish between what someone could accomplish with many tens of thousands of dollars of hardscaping and what could be adaptable to my yard. Some of the gardens leaned on sweat equity and cleverness, which is what my budget calls for \ud83d\ude09 Boulders are $$$ as much as I love how they look \ud83e\udd7a (Our landscape plan originally called for a fountain but the landscaper wasn\u2019t willing to wait for electricity to be trenched, since it would need an inspection, even though they saw the plans when we got the quote \ud83d\ude44 so, we have a bird bath)\n\nI think some cheapish landscaping cobblestones, metal arches, and more yard art could go a fair ways to adding finish to my garden. I also suspect another couple Japanese maples could pull things together. More paths / focal points in the backyard would be beneficial visually \u2014 we didn\u2019t bother because the freeway is so loud we just wanted it to look pretty out the window \u2014 but I\u2019m finding that there is a lack of places for the eye to land when it\u2019s all green all the time. Without a lawn, and with the path so close to the house you can\u2019t see it from inside, we have few \u201cedges\u201d.", "label": 1}
{"title": "Google Cloud announces general availability of APIM Operator for Apigee", "url": "https://developers.googleblog.com/en/google-cloud-announces-apim-operator-for-apigee-general-availability/", "content": "We're excited to announce the general availability of the Apigee APIM Operator, a new feature that brings lightweight API Management and API Gateway capabilities to your GKE environment. This release marks a significant step in our strategy to enable Apigee for API management on any gateway, anywhere.\n\n\n\nWhat does this mean for you?\n\nDeveloper-Native Tooling: For the many cloud-native businesses using CNCF-standardized tooling, you can now configure API management using Kubernetes-like YAML, eliminating the need to switch between different tools.\n\nReduced Friction: By supporting APIM with Kubernetes and CNCF toolchains, we're reducing the conceptual and operational friction for service developers and platform administrators.\n\nPolicy Management: Admins can create APIM template rules with RBAC, allowing different groups to use different sets of policies based on their needs. Users and admins can also add various Apigee policies to APIM templates, achieving a comparable level of functionality to Apigee Hybrid.\n\n\n\nKey Features and Capabilities\n\nThe GA release enables customers to configure a GKE cluster and GKE Gateway to use an Apigee Hybrid instance for API management through a traffic extension (ext-proc callout). It also provides API lifecycle management using YAML-based policies operated through the Kubernetes/CNCF toolchain and offers factory-built-in starter defaults for Day-Zero with workload tailoring.\n\n\n\nAddressing Customer Needs\n\nThis feature addresses the growing need for developer-friendly tooling that simplifies API management. Apigee, with its perceived complexity and the requirement to switch from kubectl to other tooling, was seen as less agile. The APIM Operator is our response to this feedback, providing a more streamlined and efficient way to manage APIs.\n\n\n\nLooking Ahead\n\nBuilding on the strong foundation of this GA release, we are exploring potential future enhancements such as gRPC and GraphQL support to accommodate a wider range of API types. We are also evaluating ways to address current limitations concerning the number of Gateway resources and policy attachments, and will update the community as new features and support become available.\n\nWe believe that the APIM Operator will significantly improve the developer experience and streamline API management for our customers. We are excited to see the innovative ways you will use this feature to build and deploy your applications.", "label": 0}
{"title": "Lies, damn lies, and business cases for AI hype", "url": "https://tommorris.org/posts/2024/lies-damn-lies-and-business-cases-for-ai-hype/", "content": "This week, to great fanfare, a report on how AI could \u201ctransform the state\u201d has been published by the Tony Blair Institute of Global Change.\n\nThe hype around this report has been increased somewhat by the recent British general election, and whether the views are likely to get a better reception with Keir Starmer in Number 10 instead of Rishi Sunak.\n\nI\u2019ll leave that kind of palace intrigue to others, and instead have a look at the report. It\u2019s also available as a PDF, but the PDF version lacks some very exciting AI generated graphics, apparently.\n\nThe Executive Summary gives the pitch for why so-called AI is important.\n\nThe latest iterations of artificial-intelligence systems \u2013 generative AI such as large language models (LLMs) \u2013 are matching humans for quality and beating them on speed and cost. Knowledge workers using the GPT-4 model from OpenAI completed 12 per cent more tasks, 25 per cent quicker, with a bigger boost in productivity for less-skilled workers. Businesses using AI tools are 40 per cent more efficient and have 70 per cent higher customer and employee satisfaction than businesses that do not.\n\nOoh, statistics! Facts! Studies! The \u201c25% quicker\u201d number is from the Noy and Zhang study. Contrary to the excited MIT press release which claims it is \u201copen access\u201d, it isn\u2019t. If you want to read it, you may have to rely on the working paper which has known mistakes.\n\nThere\u2019s some interesting statistical issues with this paper pointed out in this Twitter thread, but I\u2019m broadly of the view that it\u2019s probably about right. If you look at the Supplementary Materials and Methods document that goes with the trial, you can see the sort of writing tasks that the study participants were given\u2026\n\nmanagers and HR professionals were told that their employer had built a VR/metaverse-style \u201cvirtual office space\u201d where employees working-from-home could hang out, but they weren\u2019t using it because they quite like the solitude of not talking to weird cartoon avatars, and don\u2019t want to wear annoying VR helmets. They were tasked with writing a company-wide email of around 400 words pleading with the employees to try to use the horrible metaverse nonsense more.\n\nmanagers were told to write an email explaining how the company was shifting towards a flatter organisational structure\u2026 but without any concrete details as to what that would be\n\ndata analysts were asked to write \u201ccode notebooks\u201d setting out the steps they would take in analysing data for a bank to reduce customer churn, and for push notification marketing for an ecommerce service. A code notebook, it should be noted, is not actually something like a Jupyter notebook, but something akin to a diary explaining the steps the analyst would take, what tools they might use (Excel vs. Tableau, Python vs. R), and a summary of the kind of analysis they might look at: clustering, pivot tables, segmenting, A/B testing etc.\n\nmarketers were asked to write a press releases for self-driving e-bikes and for augmented-reality glasses\n\nfor consultants, they had to read passages from a number of reports and write a short benefits and risks summary for a business\n\nfor grant writers, they had to write cover letters for a grant application\n\nYou\u2019ll note that these tasks are ones anyone could tell you that large language models are quite good at, because there is no link between the task and actual reality. In addition, I\u2019m a little dubious about how much effort most people are going to put into an online study when compared to the amount of effort they are likely to put into their actual job, where the consequences of performing badly include loss of income, social embarassment, and lack of professional advancement\u2013all of which are rather more significant than missing out on a couple of extra dollars in one\u2019s beer money pot.\n\nAnyway, let\u2019s move onto study number two. What did the Tony Blair Institute report say?\n\nBusinesses using AI tools are 40 per cent more efficient and have 70 per cent higher customer and employee satisfaction than businesses that do not.\n\nIt links to a blog post from Google Cloud, announcing a Harvard Business Review study sponsored by Google Cloud. That\u2019d be the Google Cloud who attributed 28% of their revenue growth in Q1 2024 to AI. Totally unbiased research, in other words.\n\nIf you don\u2019t want to give Google Cloud your personal details to read this report, here\u2019s the link. Burner emails are useful.\n\nSo what\u2019s the methodology used by the report? They did a survey of business executives, and compared the results of that survey with results from a set of executives defined as \u201cleaders\u201d. (What makes them leaders? Your guess is as good as mine.)\n\nThey asked them whether their companies had been doing data analytics AND/OR \u201cAI/ML\u201d, then asked them whether over the last year, their organization had improved in a number of categories, including the introduction of new products and services, operational efficiency, customer satisfaction, revenue/growth, customer loyalty/retention, profitability, market share, employee satisfaction, and the predictability of IT costs.\n\nThey also asked them whether they\u2019d increased the usage of data analytics and/or AI/ML over the two years preceding the survey, and whether the use of those technologies had increased in importance over the same two years. It\u2019s worth noting here that on these questions, AI/ML is bolted on almost as an afterthought. They get one question, the rest are on cloud services, data storage, analytics, use of APIs, open source etc.\n\nThe results? The business leaders in the specially selected category of super-duper leaders say they are doing more data analytics and AI than the normies back in economy class. They think it\u2019s important for their business. And more of them say their businesses are doing significantly or slightly better on all those various metrics than they were previously.\n\nWhat does it not say? Well, that the interest or investment in data analytics and/or AI/ML technologies caused any improvement in those business metrics. Not that it could say that, because there is no verification of the results. Have the companies actually increased on those metrics? I mean, if they\u2019re publicly traded, you can probably check on revenue, growth and profit, and you could use some independent survey data on customer satisfaction, and maybe you might be able to get some numbers from Glassdoor or whatever on employee retention which might stand in as a proxy for employee satisfaction.\n\nThe extremely scant methodology section also notes that 23% of respondents are in the technology sector (followed by 11% in financial services, 10% in healthcare, 9% in government/non-profits, 9% in manufacturing, and then an unreported long tail).\n\nPeople in the technology sector think investing in technology is important to the success of their business? I\u2019m shocked, your honour, I genuinely had no idea.\n\n\u201cIt\u2019s a paid-for report put out by a corporation with a target audience of business executives, not a Cochrane Review. Of course it\u2019s bullshit, who cares?\u201c, you might ask.\n\nWell, it\u2019s somewhat important that the promises of brand new magic computers are considered with a little more skepticism than management consultant nonsense. The executive summary goes on to argue that \u201c[a]dopting AI in the public sector is a question of strategic prioritisation that supersedes everything else\u201d, and AI could bring public sector productivity improvements worth \u00a340 billion a year. By golly, that\u2019s over two Brexit\u2019s worth of benefits!\n\nHow would we achieve this goal?\n\nInteroperable data, for one. The government needs to \u201csecure upfront funding to rapidly link data across government that will make the implementation of AI at scale possible, maintaining privacy and anonymity\u201d. Nobody\u2019s tried that before, unless you ignore data.gov.uk, and the massive push across government to get more data, to use data science techniques etc.\n\nAlso on the agenda: buying a boatload of GPUs for the government\u2019s data centers. This seems to rather put the cart before the horse. Google and Meta and OpenAI kind of need them because they\u2019re training lots of machine learning models, but is the British government going to suddenly need to? And if they need to, why can\u2019t they just make a sensible procurement decision between training their models on AWS/GCP/Azure/whatever and buying their own hardware at the time when that becomes a live issue?\n\nThe Civil Service could apparently be rejuvenated through hiring a bunch of AI experts, including a \u201cgraduate-entry route for AI experts through the Fast Stream\u201d. The report envisions civil servants being \u201cguided and supported in their day-to-day tasks by a Multidisciplinary AI Support Team (or MAST) platform\u201d.\n\nThe benefits this would bring to citizens of the United Kingdom would apparently be immense. For instance, it could speed up responding to Freedom of Information Act requests.\n\nCurrently, responding to FOI requests requires a significant investment of time from officials to find and format information as well as make decisions about what can and cannot be shared, often inconsistently. Rather than deal with individual queries on an ad hoc basis, MAST allows departments to use open-data platforms for FOI requests, using the same mechanisms as in the previous examples.\n\nRight, so I send in an FOI request. It magically attempts to do a fancy JOIN command across a bunch of CSVs that may or may not be up-to-date and sends it back to me. If the data is already published, I can do that myself already. But if the data isn\u2019t published, instead of the government department providing the data I asked for, I\u2019ll get Clippy either making up data I didn\u2019t ask for, or denying it on the basis of a clearly inappliable FOI exemption. All the joys of WhatsApp customer service chatbots but as applied to government transparency.\n\nIt also imagines that \u201cthe MAST platform\u201d can help with public procurement. Once an area of interest only for the wonkiest of policy wonks, the last few years of controversy around VIP lanes for COVID PPE has certainly made it interesting again.\n\nWith AI analysis of large data sets on economic activity and past contracts, departments can reach out directly to organisations that meet different thresholds for risk, the vendor\u2019s financial health and track record\u2026 Vendors, in turn, streamline the process of putting together a bid with AI- generated responses and receive an immediate assessment of their fit prior to its submission, demystifying the procurement process for SMEs.\n\nI have some questions. Quite boring ones, I am sure, but I fear they might be of some importance.\n\nImagine you run a small or medium sized company participating in the procurement process, and the government encourages you to use their magic AI \u201chelp vendors fill in forms\u201d system. It screws up and makes a material misstatement to the government. The government relies on that statement, but you can\u2019t deliver and so you breach the contract. Who will be liable? The vendor who trusted the crappy computer system the government told them to use, or the government for nudging them into using the crappy system.\n\nThe \u201cimmediate assessment\u201d the system gives\u2014is that something a vendor can rely on? What if it the system incorrectly tells a vendor that their bid is unlikely to succeed and they give up when they would otherwise have had a very good shot at getting the contract? A bold new frontier in the loss of a chance doctrine awaits!\n\nNext: Regulation 18 of the Public Contracts Regulations 2015 states that \u201ccontracting authorities\u201d (read, the government or public sector body) \u201cshall treat economic operators\u201d (suppliers) \u201cequally and without discrimination and shall act in a transparent and proportionate manner\u201d.\n\nIs the magic AI procurement bot going to handle that? If and when it screws up, is the central government body who administers it going to cover the cost of the consequences, or will it come out of the departmental budget? Will the company who supplies the magic fix-everything technology take any responsibility? Or will we just say \u201cit\u2019s Agile, you\u2019re holding it wrong\u201d and move on.\n\n\u201cWhat if it goes wrong? How long until it goes to court?\u201d are totally valid questions to ask, especially given the considerable sums that\u2019s just been handed out willy-nilly to party donors, chums and spivs one met down the pub to provide unusable PPE.\n\nAn important question that has to be considered in all attempts to use machine learning (and \u201cAI\u201d, for whatever that vague term is worth) in government is how it fits with the rule of law-type obligations that public bodies have to make decisions that are fair, unbiased, explainable, and compatible with human rights. Keep a beady eye out for the judge over your shoulder. How exactly one makes technology that sits well with these obligations is a matter on which a former Prime Minister could potentially impart some insight, and on which this report is remarkably quiet. The only real attempt to do so is framed around privacy. Privacy is important, but it\u2019s only one of a number of policy concerns that really need a decent answer.\n\nAs with bold blockchain pronouncements and other tech hype, every experiment or prototype gets magically transformed by an army of consultants from \u201cwe\u2019re kinda looking at it a bit\u201d to \u201cwe\u2019re trying it\u201d to \u201cwe\u2019re using it\u201d, and then on to \u201cit definitely works\u201d and \u201cit\u2019s the greatest damn thing since the invention of the wheel\u201d. One of the consequences of TED-style dilletantism and \u201cnaive wonderment\u201d\u2014of which there is a lot in the political and business leadership class of this country\u2014is the perception that real problems can only be solved with the new and sexy and exciting.\n\nMeanwhile, the practicalities remain in short supply. Where are all those AI experts lurking in every Whitehall department to build citizen-facing chatbots and data platforms going to come from? Tech hiring is hard even when the UK university sector wasn\u2019t facing a funding crisis that borders on existential. You can have an incredibly clever AI triage system in A&E, but if you don\u2019t have the doctors, nurses and hospital beds to actually send them to, you\u2019re spending a lot of money reshuffling the order of queues where hundreds of thousands of people are now waiting over 12 hours, rather than reducing the queue by actually treating them.\n\nBefore building the next magical mystery machine that will totally fix everything, ask yourself \u201cyeah, but will it actually do that though?\u201d If the case for it consists of crappy paid-for online surveys, prototypes/experiments that have been puffed up into dead-cert successes, and either silence or hand-waving on how one resolves the actual difficult practical problems, tread with considerable caution. And be aware that actual technologists are a hell of a lot more cautious in their claims and promises than the business and political leaders they work for.", "label": 0}
{"title": "Meta\u2019s Full-stack HHVM optimizations for GenAI", "url": "https://engineering.fb.com/2025/05/20/web/metas-full-stack-hhvm-optimizations-for-genai/", "content": "As Meta has launched new, innovative products leveraging generative AI (GenAI), we need to make sure the underlying infrastructure components evolve along with it. Applying infrastructure knowledge and optimizations have allowed us to adapt to changing product requirements, delivering a better product along the way. Ultimately, our infrastructure systems need to balance our need to ship high-quality experiences with a need to run systems sustainability.\n\nSplitting GenAI inference traffic out into a dedicated WWW tenant, which allows specialized runtime and warm-up configuration, has enabled us to meet both of those goals while delivering a 30% improvement in latency.\n\nWho we are\n\nAs the Web Foundation team, we operate Meta\u2019s monolithic web tier, running Hack. The team is composed of cross-functional engineers who make sure the infrastructure behind the web tier is healthy and well designed. We jump into incident response, work on some of the most complex areas of the infrastructure, and help build whatever we need to keep the site happily up and running.\n\nTo accomplish this, we have established a series of best practices on being a \u201cgood citizen\u201d of the shared tier. We need to ensure that all requests comply with these guidelines to prevent issues from spilling over and affecting other teams\u2019 products. One core rule is the request runtime\u2014limiting a request to 30 seconds of execution. This is a consequence of the HHVM (HipHop Virtual Machine) runtime\u2014each request has a corresponding worker thread, of which there is a finite number. To ensure there are always threads available to serve incoming requests, we need to balance the resources available on each host with its expected throughput. If requests are taking too long, there will be fewer available threads to process new requests, leading to user-visible unavailability.\n\nThe changing landscape\n\nClassically, webservers at Meta are optimized for serving front-end requests\u2014rendering webpages and serving GraphQL queries. These requests\u2019 latency is typically measured in hundreds of milliseconds to seconds (substantially below the 30-second limit), which enables hosts to process approximately 500 queries per second.\n\nAdditionally, a web server will spend about two-thirds of its time doing input/output (I/O), and the remaining third doing CPU work. This fact has influenced the design of the Hack language, which supports asyncio, a type of cooperative multi-tasking, and all the core libraries support these primitives to increase performance and decrease the amount of time the CPU is sitting idle, waiting for I/O.\n\nGenAI products, especially LLMs, have a different set of requirements. These are driven by the core inference flow: The model responds with a stream of tokens that can take seconds or minutes to complete. A user may see this as a chatbot \u201ctyping\u201d a response. This isn\u2019t an effect to make our products seem friendlier; it\u2019s the speed at which our models think! After a user submits a query to the model, we need to start streaming these responses back to the user as fast as possible. On top of that, the total latency of the request is now substantially longer (measured in seconds). These properties have two effects on the infrastructure\u2014minimal overhead on the critical path before calling the LLM, and a long duration for the rest of the request, most of which is spent waiting on I/O. (See Figures 1 and 2 below).\n\nA series of optimizations\n\nThis shift in requirements allowed Web Foundation to reexamine the rules of running the monolithic web tier. We then launched a dedicated web tenant (a standalone deployment of WWW) that allowed custom configuration, which we could better tune to the needs of the workload.\n\nRequest timeout\n\nFirst, running on an isolated web tier allowed us to increase the runtime limit for GenAI requests. This is a straightforward change, but it allowed us to isolate the longer-running traffic to avoid adverse impacts on the rest of the production tier. This way, we can avoid requests timing out if inference takes longer than 30 seconds.\n\nThread-pool sizing\n\nRunning requests for longer means there is reduced availability of worker threads (which, remember, map 1:1 with processed requests). Since webservers have a finite amount of memory, we can divide the total memory available by the per-request memory limit to get a peak number of active requests; this in turn tells us how many requests we can execute simultaneously. We ended up running with approximately 1000 threads on GenAI hosts, as compared to a couple of hundred on normal webservers.\n\nJIT cache and \u201cjumpstart\u201d\n\nHHVM is a just-in-time (JIT) interpreted language, which means the first time a given function executes, the machine needs to compile it to lower-level machine code for execution. Additionally, a technique called Jump-Start allows a webserver to seed its JIT cache with outputs from a previously warmed server. By allowing GenAI hosts to use Jump-Start profiles from the main web tier, we are able to greatly speed up execution, even if the code overlap is not identical.\n\nRequest warm-up\n\nHHVM also supports the execution of dummy requests at server startup, which we can execute, and then we can discard the results. The intent here is to warm non-code caches within the webserver. Configuration values and service discovery info are normally fetched inline the first time they are needed and then cached within the webserver. By fetching and caching this information in warm-up requests, we prevent our users from observing the latency of these initial fetches.\n\nShadow traffic\n\nFinally, Meta heavily uses real-time configuration to control feature rollouts, which means that jumpstart profiles consumed at startup time might not cover all future code paths the server will execute. To maintain coverage in the steady state, we also added request shadowing, so we can ensure that gating changes are still covered in the JIT cache.", "label": 0}
{"title": "Introducing Azure AI Foundry Labs: A hub for the latest AI research and experiments at Microsoft", "url": "https://azure.microsoft.com/en-us/blog/introducing-azure-ai-foundry-labs-a-hub-for-the-latest-ai-research-and-experiments-at-microsoft/", "content": "We\u2019re thrilled to announce the launch of Azure AI Foundry Labs, a hub for developers, startups, and enterprises to explore groundbreaking innovations from research at Microsoft.\n\nToday we\u2019re launching Azure AI Foundry Labs, a hub for developers, startups, and enterprises to explore groundbreaking innovations from research at Microsoft. Foundry Labs unites cutting-edge research with real-world applications, to enable developers and creators across industries to discover new possibilities, solve complex problems, and share insights to shape the future of AI.\n\nMicrosoft\u2019s newest AI breakthrough\u2014Muse, a first-of-its-kind World and Human Action Model (WHAM), available today in Azure AI Foundry\u2014is the latest example of bringing cutting-edge research innovation to our AI platform for customers to use.\n\nWith Azure AI Foundry Labs, we\u2019re excited to unveil new assets for our latest research-driven projects that empower developers to explore, engage, and experiment. Projects across models and agentic frameworks include:\n\nAurora: A large-scale atmospheric model providing high-resolution weather forecasts and air pollution predictions, outperforming traditional tools.\n\nExACT: An open-source project enabling agents to learn from past interactions and improve search efficiency dynamically.\n\nMagentic-One: A multi-agent system solving complex problems by orchestrating multiple agents, built on the AutoGen framework.\n\nMatterSim: A deep learning model for atomistic simulations, predicting material properties with high precision.\n\nOmniParser v2: A vision-based module converting UI screenshots into structured elements, enhancing agents\u2019 action generation.\n\nTamGen: A generative AI model for drug design, using a GPT-like chemical language model for target-aware molecule generation and refinement.\n\nThen versus now\n\nIn the early days of global positioning systems (GPS) technology, it took roughly a decade for GPS to make its way from specialized, military-grade instruments into everyday consumer use. What started as a niche innovation in the 1970\u2019s didn\u2019t become truly mainstream until the late 1990\u2019s and early 2000\u2019s, when GPS receivers became standard features in cars, cell phones, and handheld devices. Ten years might sound like a reasonable adoption curve\u2014until you look at how quickly innovations are moving in AI today.\n\nIn recent years, the pace of AI advancement has accelerated dramatically. We\u2019ve witnessed a shift from unveiling a new model every 4\u20136 months to releasing breakthroughs every 4\u20136 days. The amount of compute used for training AI models has grown 10 times every 12 months, turbocharging both research and commercialization. And time-to-product from foundational research to full-scale product deployment has gone from years to months.\n\nAt this velocity, ideas and prototypes need to be iterated upon, validated, and deployed faster than ever before. This rapid evolution demands new thinking in how we bridge research and application.\n\nAccelerating research to impact\n\nAzure AI Foundry Labs highlights the long-term collaboration between research and engineering teams at Microsoft and provides a single access point for developers and the broader AI community to experiment with new models, explore the latest frameworks, and be at the forefront of innovation. Developers can create prototypes using experimental research in Azure AI Foundry Labs, collaborate with researchers and engineering teams by sharing feedback, and help speed up the time to market for some of the most promising technologies.\n\nThe next chapter\n\nThe gap between breakthrough and impact has never been smaller. What once took years now takes weeks, and what was once confined to research labs now runs on devices in our pockets. Azure AI Foundry Labs exists to collapse this gap even further\u2014to ensure that every breakthrough in AI research finds its way to the developers, creators, and innovators who can transform it into real-world impact.\n\nAzure AI Foundry Labs Bridging research and application. Discover more\n\nThis isn\u2019t just about sharing research\u2014it\u2019s about accelerating the cycle of innovation itself. Whether you\u2019re a developer, researcher, startup founder, or enterprise builder, Azure AI Foundry Labs gives you direct access to the bleeding edge of AI advancement. The tools and models available today are just the beginning.\n\nVisit Azure AI Foundry Labs to start building the future.", "label": 0}
{"title": "Neat Websites \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/blogroll/neat-websites/", "content": "Jump to: Neat projects | Webcomics | Plants & nature | Food | Collections | Math & science | Places | Tools | Seattle\n\n\ud83c\udd95 added April 2025\n\nMore collections: blogroll | interesting people | cool artists | graphic design resources | indie shops | wishlist | big questions\n\nNeat projects\n\nIt\u2019s Post Day! (Sarah Avenir) \u2014 email art project\n\nHow Not to Make a Book (Robin Rendle) \u2014 documenting the process of creating a book about typography\n\nWerner\u2019s Nomenclature of Colors (Nicholas Rougeux) \u2014 A recreation of the original 1821 color guidebook with new cross references, photographic examples, and posters\n\n\n\nScreens, research and hypertext \u2014 hypertext book about hypertext \u2014 love that meta\n\nbrr.fyi \u2014 blog by an anonymous IT worker who overwintered in Antarctica\n\nWA 100 Peaks \u2014 photographer and climber Scott Kranz climbed 100 peaks in Washington\n\nJohannes Klingebiel\u2019s digital garden \u2014 nice design\n\nEmmanuel Quartey\u2019s \u201cquestions\u201d \u2014 I like the framing and organization of information\n\nThe Shape of Music Albums \u2014 visualization of the characteristics Spotify assigns to tracks, created by Greg Wolanski\n\nTinnitus Tracker \u2014 concerts attended by Rob Weychert\n\nAtlas of Intangibles \u2014 cool interactive visualization of markers of distinctiveness and wear in London locations\n\n\ud83c\udd95 Atlas of Surveillance \u2014 police tech by state in the US (an EFF project)\n\nAdvocacy\n\n\ud83c\udd95 Regulations.gov \u2014 did you know there was one website where you could leave comments on like every U.S. rule change? I did not!\n\n\ud83c\udd95 Choose Democracy\n\n\ud83c\udd95 Beautiful Trouble Toolbox \u2014 \u201can interconnected web of ideas and creative best practices that puts the power in your hands\u201d\n\nSpecific Suggestions \u2014 \u201cThe most potent tools for fighting injustice are the ones already in your hands.\u201d\n\nWebcomics\n\nFalse Knees (Joshua Barkman) \u2013 comic strips with goofy birds\n\nwebcomic name (Alex Norris) \u2013 \u201coh no\u201d\n\nPoorly Drawn Lines (Reza Farazmand) \u2013 comic strips with returning animal characters\n\nCat and Girl (Dorothy) \u2013 social commentary from Cat and Girl\n\nThe Creator\u2019s Guide to Comics* Devices (Reimena Yee) \u2014 illustrated guide to tools that comic artists can use in storytelling\n\nInfo\n\nPlants and nature\n\n\u2013> check out my garden section\n\nSmall Seasons \u2014 the year divided into two-week segments named for the natural phenomena that tend to happen then (in Japan)\n\nNative Plants PNW \u2014 comprehensive listings of northwest native plants\n\nPacific Northwest Wildflowers \u2014 photographic database of wildflowers filterable by color and useful for identification\n\nThe Natural Navigator (Tristan Gooley) \u2013 interpreting nature sign\n\nCotswald Diary (Chris) \u2013 a look into the ongoing work of natural restoration projects\n\nNew Hampshire Garden Solutions \u2013 pretty plants and insects\n\noakland garden club (Alexis Madrigal)* \u2013 plants and art\n\nClamsplaining (Dan Killam) \u2013 clam science\n\nNatural World Facts\u2019 Deep Sea Hub (Leo Richards) \u2014 YouTube channel \u2014 mesmeric deep ocean videos\n\nWildhope.tv \u2014 YouTube channel \u2014 documentaries of conservation projects around the world\n\nBumble Bee Watch \u2014 report sightings of native bees\n\nPangea Seed \u2014 funding marine conservation through art\n\nFood\n\n\u2013> check out recipes I like and saved recipes to try\n\nBudget Bytes Vegetarian Recipes \u2014 cheap recipes, usually easy\n\nSmitten Kitchen (Deb Perelmen) \u2014 Deb has an inviting writing style \u2013 consistently good source of baking recipes\n\nStill Tasty \u2014 database of how long food lasts and storage instructions for a wide variety of foods\n\nThe Good Enough Weekly (Devin K. Pope)* \u2013 climate and food\n\nEat This Newsletter (Jeremy Cherfas)* \u2013 food\n\nTasting History (Max Miller) \u2014 YouTube channel \u2014 he cooks a historic recipe from basically any time period and talks about its context while it\u2019s cooking\n\nKenji\u2019s Cooking Show (J. Kenji L\u00f3pez-Alt) \u2014 YouTube channel \u2014 down to earth cooking advice from a science minded chef\n\nBlack Farmers Index \u2014 directory of Black farmers by region of the USA\n\nCollections\n\nPubMed Central \u2014 a free government database of medical journal papers, many of which include free full-text access because the researchers received grant funding \ud83d\ude4c\n\nFederal Open Science Repository of Canada \u2014 \u201cfederally authored scientific articles and publications from participating science-based departments and agencies\u201d \u2014 climate & environment portal\n\nSprout Distro \u2014 free printable zines\n\n\ud83c\udd95 An Incomplete SFF Criticism and Studies Reading List (Molly Templeton)\n\nPlaces\n\n\u2013> check out my road trip page\n\nAtlas Obscura \u2014 a searchable map and repository of cool destinations around the world \u2014 I always check this when I\u2019m planning a trip\n\nClose.city \u2014 map with overlays for walking / biking / transit times to major destinations \u2014 looks similar to WalkScore but without a score\n\n\ud83c\udd95 Lushootseed Place Names \u2014 Google Map of western Washington\n\nMath and Science\n\nStand-up Maths (Matt Parker) \u2014 YouTube channel \u2014 goofy math questions explored with demos and field trips\n\nVeritasium (Derek Muller) \u2014 YouTube channel \u2013longer explainer videos on science and engineering topics, often with cool models\n\nPractical Engineering (Grady Hillhouse) \u2014 YouTube channel \u2014 simple explanations of engineering practices with model demonstrations and case studies\n\n\ud83c\udd95 Defense Against Dishonest Charts (Nathan Yao) \u2014 visual breakdown of the elements of charts and what to look for in evaluating a chart for accuracy\n\nDiff Text \u2013 compare two text blocks\n\nLoot Lasso Portfolio Rebalancing Calculator \u2013 saw recommended on Reddit, haven\u2019t used \ud83d\ude09\n\nsymbol.wtf \u2013 quickly copy and paste symbols\n\n60 Seconds of Advice on Surviving a Nuclear Blast \u2013 also see NUKEMAP by Alex Wellerstein\n\n\ud83c\udd95 PairDrop \u2013 pair devices to share files instead of emailing\n\n\ud83c\udd95 cFIREsim \u2014 calculator for financial planning\n\n\ud83c\udd95 Kinopio \u2014 \u201cthinking canvas for new ideas and hard problems\u201d\n\n\ud83c\udd95 Scribe.rip \u2014 front end for Medium articles\n\nSeattle area\n\nWashington Smoke Information \u2013 invaluable during smoke season\n\nThe Urbanist \u2013 Seattle area urbanist news\n\nLushootseed (Tulalip Tribe) \u2013 phrases and pronunciation of Lushootseed words \u201cthe language of Puget Sound\u201d\n\n\ud83c\udd95 \u201cThe Voices of Lushootseed\u201d online Lushootseed lessons *with audio recordings*\n\nWashington Trails Association \u2013 an amazing repository of trip notes with current conditions from hikers all across Washington", "label": 1}
{"title": "Denver Trip", "url": "https://lifeofpablo.com/blog/denver-trip", "content": "Denver Trip\n\nThis post was written in English (en_US).\n\n\"\n\nLet the road trip begin A photo posted by Pablo Morales (@pmorales18) on Jan 5, 2016 at 9:33am PST\n\n\"\"Oh its hard to live in the city\"\" - Albert Hammond Jr.\n\nIts been an interesting start to the new year.\n\nThis week I am on a trip to Denver with my three best friends Sammy and Sam. These are some of the coolest dudes you guys will ever meet. It has been an interesting trip so far.\n\nWe left Nebraska on in the morning on Tuesday. When we got there we got some grub at a great place. Then we hit up downtown. We did a bit of shopping and just walked around. It was fun just being in that urban enviorment. All the buildings are beautiful. Downtown Denver has really cleaned up their act. I like seeing the street performers (the few in the winter).\n\nThen we hit up Redrocks Amphitheater and Park where we did some climbing up the stairs and enjoyed the beautiful sights and sceneries of the area. It was so relaxing. Any part of outside Denver is nice! I like nature. Twas fun. I do not think that I could live in the city\n\nWe had dinner at Five Guys, the burgers were so delicious. Where has it been my whole life. There were many places where we hit up for yummy food!\n\nThe nightlife in Denver is pretty swell!! The city surprises me so much!!!\n\nFound some good books at the Thrift Shop!! Who knew thrifts shops were so badass.\n\nWe went everywhere in Denver. The malls here are amazing and huge. So many sights we wanted to see but time was the enemy. The next trip will be even better.\n\nTime to hit the road! This was a trip that I really enjoyed especially with your buddies. Now it is time to go home and get back to reality. School starts soon and I got to get back to the whole routine. See you soon Nebraska!!\n\nFOLLOW ME ON INSTAGRAM\n\nTWITTER\"", "label": 1}
{"title": "Why I Attend Weekly IndieWeb Meetings", "url": "https://lifeofpablo.com/blog/why-i-attend-weekly-indieweb-meetings", "content": "Why I Attend Weekly IndieWeb Meetings\n\nThis post was written in English (en_US).\n\nA screenshot of Homebrew Website Club featuring David Shanske, Angelo Gladding, Tracy Durnell, Kevin Yank, and me!\n\nMany dread attending video conferences due to their job or don't like it for some reason. I don't mind it for many reasons. Especially after the pandemic, the methods I use to interact with people have drastically changed.\n\nAttending online meetings is fun for me especially if I enjoy attending meetings on topics I enjoy! Attending anything IndieWeb-related via Homebrew Website Club. Topics discussed are the IndieWeb technologies, technical solutions, receiving advice on building something, and more. Of course, the meetings are not always technical. One of my favorite things we do in Homebrew Website Club is showing off the changes we've done on our personal websites! I love it when someone shares that new feature on their site or that blog post that someone finally finished! It makes me happy that people are proud of their work!\n\nWe also share those articles we like or share that cool website with that cool font or the picture someone took today. The meetings are for everyone to share a topic that is interesting to them. The conversation possibilities are endless. I've received great advice from fellow IndieWeb members on how to do XYZ. I also love picking the brains of others on XYZ especially when it comes to my personal website. I am very grateful for this. Some days I don't participate as often but I'm always attentive to what I can learn from others.\n\nI always look forward to events beyond Homebrew Website Club meetings! These are something to look forward to! These events usually have a specific goal or topic in mind. For example, I love attending writing meet-ups for dedicated writing time. It helps provide a dedicated time to write a blog post that's been marinating in my drafts for a while. Maybe you get the inspiration to write something completely new. I also enjoy the Build a Website in an Hour event to create a new page on your personal website or launch a completely new website. The event ideas are infinite! It'd be nice to see more variations on events. My goal is to host an event soon! Stay tuned!\n\nEveryone and anyone is welcome to attend these online events. You don't need to be part of the IndieWeb. If you're curious about joining one of the events, just come hang out with us! You don't have to have a personal website or be tech-savy. We'd be happy to answer your questions! Attending these meetings is a great way to connect with people located on the other side of the world. Maybe you'll meet someone who lives in the same city as you?\n\nThis blog post has been posted on IndieNews", "label": 1}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2024-01", "content": "en\n\nI've lived in California for over 3 years (who's counting? Oh wait, me!) I've got to the point I know where most of the popular places to visit in San Francisco, Sacramento or places in between. I would consider myself feeling somewhat knowledgeable of the area. My common expression here is, \"I'm new here!\" I live in Sacramento but I go to San Francisco all the time that I know the city pretty well enough to wonder off to different places. I like to wonder around even if my feet start hurting so I simply push through.\n\nI'm showing my colleagues' brother, Rami since he's visiting from far away. I feel comfortable walking around in San Francisco. I'm very fortunate to have friends who live in the city who I've learned so much from. I feel that I can be a good tour guide. We're both dudes who are young and like to have fun. That makes it easy to keep our minds open and try new experiences. I love doing all the touristy things again especially when friends or family come to visit. I get to help people experience the city the way I did or at least provide that feeling of novelty.", "label": 1}
{"title": "Weeknotes: June 7-13, 2025 \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/13/weeknotes-june-7-13-2025/", "content": "Highlight of the week: went on a neighborhood garden tour \ud83c\udf3c had such a great time! \ud83d\ude0a\n\nLooking forward to: hoping to meet some neighbors at a community potluck \ud83e\udd1e all the neighbors we knew moved away\n\nStuff I did:\n\n6.75 hours consulting \u2014 sent in a first draft deliverable for the new project\n\n3.25 hours writing\n\nupdated my retirement portfolio tracker spreadsheet and set up a Roth IRA for my husband \u2014 what\u2019s one more column?! \ud83e\udee0\n\nwrote in to the Planning Commission in support of upzoning some large sites in my city \u2014 the overall plans moved forward \ud83d\udc4f but apparently financing big apartment buildings is iffy rn so they allowed townhouses in hopes the project will redevelop sooner\u2026 sounds like the right call sadly\n\nspent an hour submitting complaints on a community feedback map about intersections and other conflict zones between cars and people walking and biking\u2026 feel like I already did this for the Active Transportation Plan but I\u2019m not missing a single opportunity to point out safety problems\n\nswitched the heat pump from heating mode to cooling mode and brought the thermostat upstairs\u2026 except then it got cold again! Junuary strikes again\n\ngot sick of long hair and lopped off maybe 8\u2033 \u2014 I went shorter than last time and used a different technique of splitting my hair in three parts \u2014 I need some haircutting shears because my sewing scissors aren\u2019t cutting it, literally \u2702\ufe0f\n\nbaked sourdough banana bread muffins \ud83d\udc4d\u2013 made 12 muffins + a ramekin\n\nhusband needed to pick some international colleagues up at the airport and ferry them around when I wanted to do the garden tour, so we rented a car for the day \u2014 a little annoying but better than owning a second car for these rare occasions\n\nwent out to dinner with husband\u2019s colleagues, only a minor fiasco \ud83d\ude02\ud83e\udd37\u200d\u2640\ufe0f\n\none virtual appointment\n\none walk with my friend \u2014 too hot on the other day we had planned\n\nDinners:\n\nleftover Indian food\n\n\u201c7\u201d layer dip with chips and pineapple\n\nleftover egg salad + muffin\n\ncorn quesadillas with avocado and sour cream + prickly pear lemonade (with this prickly pear syrup)\n\nThai \u2014 radh nah with tofu\n\nsaucy chickpeas on ciabatta \u2014 made a double batch, took 45 minutes with prep, not bad\n\nfrozen waffles with strawberries, blueberries, and syrup + frozen veggie sausages + air fryer potatoes (I get this kind called Bintje from the local market and I swear it is the Platonic ideal of potato flavor)\n\nReading:\n\nRough reading week, wasn\u2019t feeling a lot of what I tried.\n\nRead The Work of Art in the Age of Mechanical Reproduction by Walter Benjamin and Roommate by Sarina Bowen\n\nStarted reading We Will Not Cancel Us by Adrienne Maree Brown and Simulacra and Simulation by Jean Baudrillard\n\nDNF\u2019d Immediacy by Anna Kornbluh, Rogue Enforcer by Grace Goodwin, Gentle Rogue by Johanna Lindsey, How to Steal a Galaxy by Beth Revis, Evocation by S.T. Gibson, and When the Tides Held the Moon by Venessa Vida Kelley\n\nPre-ordered Ew, It\u2019s Beautiful by Joshua Barkman and Blank Space by W. David Marx\n\nOrdered a used copy of The Unreal America: Architecture and Illusion by Ada Louise Huxtable and For Scale print issue 2\n\nWords I looked up / concepts I learned:\n\nergodic theory\n\neffulge (what an ugly word for a pretty concept)\n\ndiscretization\n\ncelerity\n\naffright (see also: word I will never use)\n\nredounds (this one is actually a kinda cool word for the archaic meaning, has a good sound to it; do not like it for the meaning \u201caccrue\u201d which is a much better known word)\n\nstopples\n\nalterity (probably a useful word but has an air of haughtiness?)\n\nphenotypic plasticity (via this video on locusts and grasshoppers)\n\nelite panic (via)\n\nPretty stuff I saw:\n\nNew music I listened to:\n\nNature notes:", "label": 1}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2023-05", "content": "en\n\nEver since I was young, I was the guy who always had a smile on his face. I mean like 90% of the time. Any where I went, any where I was spotted, any encounter. I, Pablo Morales was the smiling kid.\n\nFa\u00e7ade\n\nSmiling would help me put on a fa\u00e7ade or a filter to mask the many emotions I feel at any given moment.\n\nAbroad\n\nWhen I visit Mexico, I've learned not to smile as much. I just experience life differently. I try not to stand out. I am always happy to see the people I love.\n\nWhen I was studying in Europe, I truly learned not to smile. Many Europeans don't smile when they are out and about with their day. During studying abroad, I made it a goal to not stand out as an \"American\", especially during the political turmoil happening in the United States.\n\nEven when I reverted back to my \"typical smiling,\" I never truly reverted back.\n\nIt's okay not to smile\n\nBy not smiling, I am learning to express myself more. I can be in a great mood but not smile? Yes, I can! By smiling all the time, it taught me I wasn't true to myself. I wasn't being fair to myself. I was defrauding myself. How can I be a real person if I can't express myself?\n\nAm I happy?\n\nYou're asking yourself this question? Is Pablo Morales, actually happy with himself? Yes, I am happy.\n\nHas it been hard to love myself? Yes. Overall, happiness does not fall into one size fits all scenario.\n\nThere is a side of me that I am afraid to explore. I have a battle ahead. I am still looking for this battle with a smile or frown.", "label": 1}
{"title": "Using Signal to communicate securely \u2022 Cory Dransfeldt", "url": "https://www.coryd.dev/posts/2025/using-signal-to-communicate-securely", "content": "Now more than ever it's important to keep your communications with those you know secure and private. Signal is the best available option for doing so. It is secure, private and run by a non-profit organization that makes it freely available.\n\nDownload Signal - Private Messenger from your mobile device's app store.\n\nApp Store\n\nGoogle Play\n\nRegister and provide your phone number if required.\n\nDo not set a profile picture. Use your first name or a pseudonym.\n\nAdjust your settings:\n\nChats: disable Generate Link Previews. Disable Share Contacts with iOS or Android.\n\ndisable Generate Link Previews. Disable Share Contacts with iOS or Android. Notifications: under Notification Content, select No Name or Content.\n\nunder Notification Content, select No Name or Content. Privacy: under Phone Number set both options to nobody. Enable Hide Screen in App Switcher. Enable Screen Lock, select 5 minutes for Screen Lock Timeout.\n\nTo share your Signal contact information, provide your username or tap on the QR code at the top of your settings (by your profile information) and send the QR code or link over SMS. Then communicate using Signal.\n\nQuestions? Email me or schedule a call with me\n\nAdditional resources\n\nIf you have an iPhone, use Apple's Translate app. When prompted to provide recordings to improve the app, select no.\n\nAvoid using any of Meta's services to share sensitive information.\n\nI would strongly recommend not using Gmail or other popular email providers. The following table contains a list of reputable alternatives.", "label": 0}
{"title": "ESL Teacher Interview", "url": "https://lifeofpablo.com/blog/esl-teacher-interview", "content": "ESL Teacher Interview\n\nThis post was written in English (en_US).\n\n\"\n\nI am currently taking an ESL course and we had the opportunity to interview an ESL teacher. I interviewed Denise Teetor who is the ESL Teacher at Hastings Senior High School. She told me about her experiences getting into this subject field and how not always was she the ESL Teacher. I learned so much from this opportunity. This interview has me really excited to be a teacher. All the information here will hopefully help those people who want to become teachers. Getting the insights from another teacher lets you have a feeling of what you are getting yourself into. I am not trying to scare anyone, I'm just showing it how it is. Enjoy the interview! I will have the audio podcast available soon! Click the picture below to download a copy of the document to your device.\n\n\"\n\nI am currently taking an ESL course and we had the opportunity to interview an ESL teacher. I interviewed Denise Teetor who is the ESL Teacher at Hastings Senior High School. She told me about her experiences getting into this subject field and how not always was she the ESL Teacher. I learned so much from this opportunity. This interview has me really excited to be a teacher. All the information here will hopefully help those people who want to become teachers. Getting the insights from another teacher lets you have a feeling of what its like to be in the classroom. Enjoy the interview! I will have the audio podcast available soon! Click here to download a copy of the document to your device.\n\nDenise Teetor: ESL at Hastings Senior High School\n\nInterviewed by\n\nPablo MORALES GARCIA\n\nMrs. Denise Teetor is an English as a Second Language Teacher at Hastings Senior High School in Hastings Nebraska. She has been teaching at the high school for more than 30 years. She has quite a different perspective of the world since she has traveled outside of the United States plenty of times. She has seen so many changes in ESL over the years. She has quite the story to tell in which she becomes the ESL teacher she is today.\n\nMrs. Teetor was not an ESL teacher from the beginning. She use to teach physical education and coach for a long time. She decided to pursue a different area of teaching and left behind being an PE teacher and decided to teach English in Japan. She wanted to do something different with her time during the summers instead of wasting it. She saw an advertisement to teach in Japan for six weeks and thought \u201cthat sounds pretty cool\u201d, and that\u2019s how she got involved with Japan. She really enjoyed it and had so much fun with it.\n\nAfter one summer, she decided to pursue her ESL endorsements. She was thinking ahead that she could teach ESL at the high school level at some point down the road. One summer as she was getting ready to go back to Japan, she knew that the ELL teacher at Hastings High would be leaving that summer, even though she had not filed her resignation letter. Mrs. Teetor took some steps before the other teacher quit. She let the school know that she was interested in the position. The teacher did end up leaving and she received a phone call asking if she was interested in the ELL position and she said \u201cYes\u201d. This is how she moved from physical education to ELL. Japan really peaked her interest working with foreign kids.\n\nShe is happy with how much ESL/ELL has evolved over the years, especially since she saw there was not a lot of curriculum that existed. The number of students has changed over the years. The program development has come a long way. There was curriculum that really existed. They were fortunate when they were able to find books related to ELL/ESL. Now there are whole curriculums that one can use. That has been an advancement of ELL programs. It is like all other curriculums in schools such as science where you have books, notebooks and a foundation to use. You start at the foundational levels and move through the different stages of it. It has been one of the biggest tools available so teachers don\u2019t have to go out and make their own material.\n\nShe describes the program at Hastings Senior High School. She gets all different levels of ESL students in each class period. She explains that in an ideal world, students are grouped by their level such as all level 1 students are together, level 2 are together, and so forth. Unfortunately, she doesn\u2019t have that luxury at the high school. She works with each level of students individually so each group of students can be differentiated as needed. She splits the groups up between her and Mrs. Brenneman, the para educator, to help each leveled group. The program at HHS is not a program that is like those implemented in other schools. The program isn\u2019t a dual language or other type. They just work based on the situation they have or a melting pot as she states. She is very proud of the graduation rate that has increased. If the kids start with the program, they tend to finish with the program unless they move away. She is proud that the ELL students have been able to take all the required classes or core classes needed to graduate. Even though they might not have a high level of English, \u201cthey do pretty well\u201d. Even though teachers provide differentiation, they learn all the same material as regular students. The students are able to get into the classroom right away. This is good for them since they can get \u201creal world\u201d experience and immerse themselves in the culture and listen to how students talk. No one ever follows grammar rules when the speak.\n\nMrs. Teetor mentions that students have to take a test to see if they need ESL services based on responses on intake forms when transferring into the district. She uses the newly implemented ELPA to screen prospective and current students who might or are using ESL services to measure their level of English. Newly arriving students fill out a questionnaire that askes the language they spoke first, what language is spoken primarily at home, and what language they want to be communicated with. If any of these are not English, they need to be tested. Even though a student knows English, they still have to take the test if their language spoken at home is not English. She uses a computer screener test. Anyone that is four (4) or above, they do not need to be in the program but anything below that, students should be enrolled in ESL/ELL. Things are done differently at the elementary level compared to the high school level.\n\nMrs. Teetor works closely with other teachers. She also likes to know what the teachers are teaching so she can meet the needs of students such as preparing a presentation or report. With American History, the students have no background especially with the foreign students. American history pertains to the United States. Math is math, science is science, these things are the same all the way across cultures. She enjoys how things are taught at HHS. Students are in ELL for at least 2 class periods. She tells us about an example student who doesn\u2019t speak a lot of but is very bright. Looking at her transcripts, she is a very well-rounded person. She has taken courses in psychology to advanced math. She was top of her class before moving. This student is \u201cbrilliant\u201d, according to Mrs. Teetor. She sees how this student can feel out of place due to the language barrier. She sees another barrier with math leaning towards story-problems. It intimidates students since it involves reading instead of involving direct math problems. What is truly moving is that she truly advocates for students to get them in the regular classroom where they will have to think, perform in the real thing. She knows it will be hard but it would be a \u201cdisservice\u201d if she does not.\n\nShe hasn\u2019t faced any teachers who have rejected or negated students. This at least hasn\u2019t occurred during her time her but she says that \u201cit might have happened before I started.\u201d She believes this hasn\u2019t happened because of her strong personality and she knows the teachers will be supportive at Hastings Senior High School. She hopes that the next person who replaces her will continue on standing up for the students. She also knows which teachers will be best for her students and she know the teachers who will not be a good fit. She moves students as necessary to put them in the right classrooms. She would rather do something else with the student if the teacher is not a good fit for the students. For a new teacher, this is something that he or she will not know and this will take time to figure out. She is hopeful the other teacher will be the best advocate for the ESL students.\n\nShe has great advice for new ESL teachers especially for the teacher replacing her. The best advice would be just to have fun with these students. They do need to learn but also everything is not so rigid that they can\u2019t enjoy. Simply cramming information gets them discouraged. It has been interested and fun for them to know. Everyone has some kid within each other. We need to promote this with the kids. She gets a lot out of the kids when they have fun. If they are sitting bored, they are not going anywhere. This is her overall advice.\n\nThank you, Mrs. Teetor for allowing me to interview you. It was such a great insight of ESL and very helpful for me for my future career as an ESL Teacher. I wish you the best in luck with retirement and hope to hear about your adventures in Japan! Cheers!\n\nA podcast of this will be available on my SoundCloud page soon!", "label": 1}
{"title": "Announcing the Agent2Agent Protocol (A2A)", "url": "https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/", "content": "A new era of Agent Interoperability\n\n\n\nAI agents offer a unique opportunity to help people be more productive by autonomously handling many daily recurring or complex tasks. Today, enterprises are increasingly building and deploying autonomous agents to help scale, automate and enhance processes throughout the workplace\u2013from ordering new laptops, to aiding customer service representatives, to assisting in supply chain planning.\n\nTo maximize the benefits from agentic AI, it is critical for these agents to be able to collaborate in a dynamic, multi-agent ecosystem across siloed data systems and applications. Enabling agents to interoperate with each other, even if they were built by different vendors or in a different framework, will increase autonomy and multiply productivity gains, while lowering long-term costs.\n\n\n\nToday, we\u2019re launching a new, open protocol called Agent2Agent (A2A), with support and contributions from more than 50 technology partners like Atlassian, Box, Cohere, Intuit, Langchain, MongoDB, PayPal, Salesforce, SAP, ServiceNow, UKG and Workday; and leading service providers including Accenture, BCG, Capgemini, Cognizant, Deloitte, HCLTech, Infosys, KPMG, McKinsey, PwC, TCS, and Wipro. The A2A protocol will allow AI agents to communicate with each other, securely exchange information, and coordinate actions on top of various enterprise platforms or applications. We believe the A2A framework will add significant value for customers, whose AI agents will now be able to work across their entire enterprise application estates.\n\nThis collaborative effort signifies a shared vision of a future when AI agents, regardless of their underlying technologies, can seamlessly collaborate to automate complex enterprise workflows and drive unprecedented levels of efficiency and innovation.\n\nA2A is an open protocol that complements Anthropic's Model Context Protocol (MCP), which provides helpful tools and context to agents. Drawing on Google's internal expertise in scaling agentic systems, we designed the A2A protocol to address the challenges we identified in deploying large-scale, multi-agent systems for our customers. A2A empowers developers to build agents capable of connecting with any other agent built using the protocol and offers users the flexibility to combine agents from various providers. Critically, businesses benefit from a standardized method for managing their agents across diverse platforms and cloud environments. We believe this universal interoperability is essential for fully realizing the potential of collaborative AI agents.", "label": 0}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2020-04", "content": "en\n\nToday, 30 years ago on April 8th, 1990 Ryan White passed away. Today, he would have been 48 years old. I have always been an advocate for people with HIV/AIDS. I learned about Ryan when I was in high school. Ryan White contracted HIV through contaminated blood transfusions for hemophilia. He was the poster child during a time that many people didn't know what HIV/AIDS was as a disease. He spoke out against all adversaries. Ryan was forced out of school many times. He did \"The Fight to Go to School.\" So much ignorance occurred such as thinking it could be passed through casual contact. Ryan became a national spokesman. Many famous people such as Michael Jackson, befriended Ryan in a time that nobody wanted to be near him. Having people of high status led the way in showing that this disease is not transmitted just by shaking someone's hand, giving somebody a kiss or giving somebody a kiss. Ryan didn't stay quiet about his disease, he helped inform all of us that this disease can affect anyone, not just certain groups of people. This disease doesn't discriminate your gender, race, sexual orientation, etc. We all need to bond together to find a cure and stop the hatred. The Ryan White Act was created in 1990 to help people who have been diagnosed with this disease. Today the battle continues against HIV. We must remember those who fought to help educate us. RIP Ryan White", "label": 1}
{"title": "What I bought at Trader Joe's", "url": "https://lifeofpablo.com/blog/what-i-bought-at-trader-joe-s", "content": "What I bought at Trader Joe's\n\nTrader Joe's Extra Virgin Olive Oil being poured in a pan.\n\nThis post was written in English (en_US).\n\nI recently went to Trader Joe's on my day off today. I noticed prices over the last year have been going up everywhere and Trader Joe's is no exception. I've been working on only buying the things I only need or things to supplement the items I have at home. For example, I bought spinach dip to help me finish the chips I have at home and I bought baby carrots (Cut & Peeled Carrots) to help finish the spinach dip. Inflation is coming on all of us hard. I almost gasped out loud in the store when I saw the price of a litter bottle of the Extra Virgin Olive Oil. I remember it being $8.99 now it's $10.99. It's just the little things that add up. I have to be extra mindful of the inventory of my kitchen and buy only the things I need and think of items that be versatile in use.\n\nOne of my goals this year is to simply not be as wasteful. Adulting is a learning process.\n\nHere is what I bought on this trip.", "label": 1}
{"title": "Building a better smart home", "url": "https://developers.googleblog.com/en/building-a-better-smart-home-expanding-access-for-developers-and-users/", "content": "Expanding access for app and device developers\n\nThe smart home industry is constantly evolving, and we're committed to staying at the forefront of innovation. At I/O 2024 we announced how we reimagined Google Home as a platform for all developers. Since then we have reached major milestones and our first partners have begun launching apps built on our platform.\n\nOur goal has remained the same - to make it easier for developers to create amazing experiences for users, and for those users to enjoy seamless connectivity and interoperability across all their devices. To that end, we are making a series of investments designed to enable all developers to build for the home.\n\n\n\nEmpowering developers with Home APIs\n\nWe believe that open platforms foster greater innovation, and it's clear that developers do too. We\u2019ve had nearly 2,000 developers sign up to learn more about the Home APIs since I/O 2024 and that's why we're thrilled to announce the public developer beta launch of Home APIs - today for Android, and in the coming months for iOS. These APIs provide developers with the tools they need to build richer and more integrated smart home experiences.\n\nThis isn't just theoretical, our partners are already making headway. Early access partners Eve, Nanoleaf, LG, ADT and Tuya Smart have launched new apps and features built with the Google Home Platform, with even more partners like Cync, GE Appliances, Yale, and Aqara releasing in the coming months. At CES 2025, our partners are showcasing device control and automation experiences in their apps built using these APIs. Hisense and Aqara will demo how the Home APIs have helped them to create and surface automations for different areas of their users\u2019 lives. SDMC will demonstrate how the Device & Structure API enables their users to control devices connected to Google Home directly in their apps.\n\n\n\nInvesting in connectivity and interoperability\n\nDelivering a truly smart home requires seamless connectivity made effortless via our platform. Our investments in Matter are a foundational layer to the smart home experience, and we continue to increase our investments in this area across multiple fronts:\n\nExpanding Matter support: A hub for Google Home is critical to unlocking Matter's fast, secure and reliable experience - it enables remote access and fully local control of Matter devices. We've significantly expanded the reach of hubs for Google Home by integrating the Google Home runtime into over 40 million devices, including Google Nest devices, Chromecasts, Google TV devices on Android 14 and eligible LG TVs. This means more users can enjoy the benefits of Matter connectivity, with less work. You can learn more details on our expanded Matter support here.\n\nIncreasing our investment to improve Matter quality: We firmly believe in Matter's potential to unify the smart home, and we are committed to its growth. That's why we, along with Apple and Samsung, are going beyond our existing commitments to Matter to further accelerate the improvement of quality for Matter. At Google, we're investing in Matter's growth in a number of new ways, including increasing development resources to enhance certification automation, interoperability scripting, and SDK bug fixes & maintenance.\n\nStreamlining certification with Connectivity Standards Alliance: In an effort to reduce time and costs for developers looking to certify software and products across multiple ecosystems, Google is excited to join Apple and Samsung in accepting Connectivity Standards Alliance Interop Lab test results. With this, you can now get certified with Works With Google Home for Matter devices without an additional certification process with Google.\n\nMaking Thread more universal: One key customer challenge with Matter today is that Thread devices require a Thread Border Router (TBR) in the home for control. To address this challenge at the ecosystem level and expand TBR availability, we\u2019ve partnered with MediaTek on the new Trinity chip (MT7903) that includes Wi-Fi, Bluetooth LE and Thread on a single system-on-a-chip (SoC). This makes it easier and more affordable for device OEMs to build Thread into all their new products.\n\n\n\nThe Future of the smart home\n\nWe're incredibly excited about the future of the smart home, and we believe these investments will pave the way for a new era of innovation and interoperability. By empowering developers and fostering a robust ecosystem, we're making the dream of a truly connected and intuitive smart home a reality.\n\nTo make sure you are staying up to date with the latest news, announcements, and resources from Google Home, be sure to subscribe to the Google Home Developer Newsletter.", "label": 0}
{"title": "Introducing Netflix\u2019s TimeSeries Data Abstraction Layer", "url": "https://netflixtechblog.com/introducing-netflix-timeseries-data-abstraction-layer-31552f6326f8?source=collection_home---4------13-----------------------", "content": "Introducing Netflix\u2019s TimeSeries Data Abstraction Layer Netflix Technology Blog 18 min read \u00b7 Oct 8, 2024 -- 16 Listen Share\n\nBy Rajiv Shringi, Vinay Chella, Kaidan Fullerton, Oleksii Tkachuk, Joey Lynch\n\nIntroduction\n\nAs Netflix continues to expand and diversify into various sectors like Video on Demand and Gaming, the ability to ingest and store vast amounts of temporal data \u2014 often reaching petabytes \u2014 with millisecond access latency has become increasingly vital. In previous blog posts, we introduced the Key-Value Data Abstraction Layer and the Data Gateway Platform, both of which are integral to Netflix\u2019s data architecture. The Key-Value Abstraction offers a flexible, scalable solution for storing and accessing structured key-value data, while the Data Gateway Platform provides essential infrastructure for protecting, configuring, and deploying the data tier.\n\nBuilding on these foundational abstractions, we developed the TimeSeries Abstraction \u2014 a versatile and scalable solution designed to efficiently store and query large volumes of temporal event data with low millisecond latencies, all in a cost-effective manner across various use cases.\n\nIn this post, we will delve into the architecture, design principles, and real-world applications of the TimeSeries Abstraction, demonstrating how it enhances our platform\u2019s ability to manage temporal data at scale.\n\nNote: Contrary to what the name may suggest, this system is not built as a general-purpose time series database. We do not use it for metrics, histograms, timers, or any such near-real time analytics use case. Those use cases are well served by the Netflix Atlas telemetry system. Instead, we focus on addressing the challenge of storing and accessing extremely high-throughput, immutable temporal event data in a low-latency and cost-efficient manner.\n\nChallenges\n\nAt Netflix, temporal data is continuously generated and utilized, whether from user interactions like video-play events, asset impressions, or complex micro-service network activities. Effectively managing this data at scale to extract valuable insights is crucial for ensuring optimal user experiences and system reliability.\n\nHowever, storing and querying such data presents a unique set of challenges:\n\nHigh Throughput : Managing up to 10 million writes per second while maintaining high availability.\n\n: Managing up to 10 million writes per second while maintaining high availability. Efficient Querying in Large Datasets : Storing petabytes of data while ensuring primary key reads return results within low double-digit milliseconds, and supporting searches and aggregations across multiple secondary attributes.\n\n: Storing petabytes of data while ensuring primary key reads return results within low double-digit milliseconds, and supporting searches and aggregations across multiple secondary attributes. Global Reads and Writes : Facilitating read and write operations from anywhere in the world with adjustable consistency models.\n\n: Facilitating read and write operations from anywhere in the world with adjustable consistency models. Tunable Configuration : Offering the ability to partition datasets in either a single-tenant or multi-tenant datastore, with options to adjust various dataset aspects such as retention and consistency.\n\n: Offering the ability to partition datasets in either a single-tenant or multi-tenant datastore, with options to adjust various dataset aspects such as retention and consistency. Handling Bursty Traffic : Managing significant traffic spikes during high-demand events, such as new content launches or regional failovers.\n\n: Managing significant traffic spikes during high-demand events, such as new content launches or regional failovers. Cost Efficiency: Reducing the cost per byte and per operation to optimize long-term retention while minimizing infrastructure expenses, which can amount to millions of dollars for Netflix.\n\nTimeSeries Abstraction\n\nThe TimeSeries Abstraction was developed to meet these requirements, built around the following core design principles:\n\nPartitioned Data : Data is partitioned using a unique temporal partitioning strategy combined with an event bucketing approach to efficiently manage bursty workloads and streamline queries.\n\n: Data is partitioned using a unique temporal partitioning strategy combined with an event bucketing approach to efficiently manage bursty workloads and streamline queries. Flexible Storage : The service is designed to integrate with various storage backends, including Apache Cassandra and Elasticsearch, allowing Netflix to customize storage solutions based on specific use case requirements.\n\n: The service is designed to integrate with various storage backends, including Apache Cassandra and Elasticsearch, allowing Netflix to customize storage solutions based on specific use case requirements. Configurability : TimeSeries offers a range of tunable options for each dataset, providing the flexibility needed to accommodate a wide array of use cases.\n\n: TimeSeries offers a range of tunable options for each dataset, providing the flexibility needed to accommodate a wide array of use cases. Scalability : The architecture supports both horizontal and vertical scaling, enabling the system to handle increasing throughput and data volumes as Netflix expands its user base and services.\n\n: The architecture supports both horizontal and vertical scaling, enabling the system to handle increasing throughput and data volumes as Netflix expands its user base and services. Sharded Infrastructure: Leveraging the Data Gateway Platform, we can deploy single-tenant and/or multi-tenant infrastructure with the necessary access and traffic isolation.\n\nLet\u2019s dive into the various aspects of this abstraction.\n\nData Model\n\nWe follow a unique event data model that encapsulates all the data we want to capture for events, while allowing us to query them efficiently.\n\nLet\u2019s start with the smallest unit of data in the abstraction and work our way up.\n\nEvent Item : An event item is a key-value pair that users use to store data for a given event. For example: {\u201cdevice_type\u201d: \u201cios\u201d}.\n\n: An event item is a key-value pair that users use to store data for a given event. For example: {\u201cdevice_type\u201d: \u201cios\u201d}. Event : An event is a structured collection of one or more such event items. An event occurs at a specific point in time and is identified by a client-generated timestamp and an event identifier (such as a UUID). This combination of event_time and event_id also forms part of the unique idempotency key for the event, enabling users to safely retry requests.\n\n: An event is a structured collection of one or more such event items. An event occurs at a specific point in time and is identified by a client-generated timestamp and an event identifier (such as a UUID). This combination of and also forms part of the unique idempotency key for the event, enabling users to safely retry requests. Time Series ID : A time_series_id is a collection of one or more such events over the dataset\u2019s retention period. For instance, a device_id would store all events occurring for a given device over the retention period. All events are immutable, and the TimeSeries service only ever appends events to a given time series ID.\n\n: A is a collection of one or more such events over the dataset\u2019s retention period. For instance, a would store all events occurring for a given device over the retention period. All events are immutable, and the TimeSeries service only ever appends events to a given time series ID. Namespace: A namespace is a collection of time series IDs and event data, representing the complete TimeSeries dataset. Users can create one or more namespaces for each of their use cases. The abstraction applies various tunable options at the namespace level, which we will discuss further when we explore the service\u2019s control plane.\n\nAPI\n\nThe abstraction provides the following APIs to interact with the event data.\n\nWriteEventRecordsSync: This endpoint writes a batch of events and sends back a durability acknowledgement to the client. This is used in cases where users require a guarantee of durability.\n\nWriteEventRecords: This is the fire-and-forget version of the above endpoint. It enqueues a batch of events without the durability acknowledgement. This is used in cases like logging or tracing, where users care more about throughput and can tolerate a small amount of data loss.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"events\": [\n\n{\n\n\"timeSeriesId\": \"profile100\",\n\n\"eventTime\": \"2024-10-03T21:24:23.988Z\",\n\n\"eventId\": \"550e8400-e29b-41d4-a716-446655440000\",\n\n\"eventItems\": [\n\n{\n\n\"eventItemKey\": \"deviceType\",\n\n\"eventItemValue\": \"aW9z\"\n\n},\n\n{\n\n\"eventItemKey\": \"deviceMetadata\",\n\n\"eventItemValue\": \"c29tZSBtZXRhZGF0YQ==\"\n\n}\n\n]\n\n},\n\n{\n\n\"timeSeriesId\": \"profile100\",\n\n\"eventTime\": \"2024-10-03T21:23:30.000Z\",\n\n\"eventId\": \"123e4567-e89b-12d3-a456-426614174000\",\n\n\"eventItems\": [\n\n{\n\n\"eventItemKey\": \"deviceType\",\n\n\"eventItemValue\": \"YW5kcm9pZA==\"\n\n}\n\n]\n\n}\n\n]\n\n}\n\nReadEventRecords: Given a combination of a namespace, a timeSeriesId, a timeInterval, and optional eventFilters, this endpoint returns all the matching events, sorted descending by event_time, with low millisecond latency.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"timeSeriesId\": \"profile100\",\n\n\"timeInterval\": {\n\n\"start\": \"2024-10-02T21:00:00.000Z\",\n\n\"end\": \"2024-10-03T21:00:00.000Z\"\n\n},\n\n\"eventFilters\": [\n\n{\n\n\"matchEventItemKey\": \"deviceType\",\n\n\"matchEventItemValue\": \"aW9z\"\n\n}\n\n],\n\n\"pageSize\": 100,\n\n\"totalRecordLimit\": 1000\n\n}\n\nSearchEventRecords: Given a search criteria and a time interval, this endpoint returns all the matching events. These use cases are fine with eventually consistent reads.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"timeInterval\": {\n\n\"start\": \"2024-10-02T21:00:00.000Z\",\n\n\"end\": \"2024-10-03T21:00:00.000Z\"\n\n},\n\n\"searchQuery\": {\n\n\"booleanQuery\": {\n\n\"searchQuery\": [\n\n{\n\n\"equals\": {\n\n\"eventItemKey\": \"deviceType\",\n\n\"eventItemValue\": \"aW9z\"\n\n}\n\n},\n\n{\n\n\"range\": {\n\n\"eventItemKey\": \"deviceRegistrationTimestamp\",\n\n\"lowerBound\": {\n\n\"eventItemValue\": \"MjAyNC0xMC0wMlQwMDowMDowMC4wMDBa\",\n\n\"inclusive\": true\n\n},\n\n\"upperBound\": {\n\n\"eventItemValue\": \"MjAyNC0xMC0wM1QwMDowMDowMC4wMDBa\"\n\n}\n\n}\n\n}\n\n],\n\n\"operator\": \"AND\"\n\n}\n\n},\n\n\"pageSize\": 100,\n\n\"totalRecordLimit\": 1000\n\n}\n\nAggregateEventRecords: Given a search criteria and an aggregation mode (e.g. DistinctAggregation) , this endpoint performs the given aggregation within a given time interval. Similar to the Search endpoint, users can tolerate eventual consistency and a potentially higher latency (in seconds).\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"timeInterval\": {\n\n\"start\": \"2024-10-02T21:00:00.000Z\",\n\n\"end\": \"2024-10-03T21:00:00.000Z\"\n\n},\n\n\"searchQuery\": {...some search criteria...},\n\n\"aggregationQuery\": {\n\n\"distinct\": {\n\n\"eventItemKey\": \"deviceType\",\n\n\"pageSize\": 100\n\n}\n\n}\n\n}\n\nIn the subsequent sections, we will talk about how we interact with this data at the storage layer.\n\nStorage Layer\n\nThe storage layer for TimeSeries comprises a primary data store and an optional index data store. The primary data store ensures data durability during writes and is used for primary read operations, while the index data store is utilized for search and aggregate operations. At Netflix, Apache Cassandra is the preferred choice for storing durable data in high-throughput scenarios, while Elasticsearch is the preferred data store for indexing. However, similar to our approach with the API, the storage layer is not tightly coupled to these specific data stores. Instead, we define storage API contracts that must be fulfilled, allowing us the flexibility to replace the underlying data stores as needed.\n\nPrimary Datastore\n\nIn this section, we will talk about how we leverage Apache Cassandra for TimeSeries use cases.\n\nPartitioning Scheme\n\nAt Netflix\u2019s scale, the continuous influx of event data can quickly overwhelm traditional databases. Temporal partitioning addresses this challenge by dividing the data into manageable chunks based on time intervals, such as hourly, daily, or monthly windows. This approach enables efficient querying of specific time ranges without the need to scan the entire dataset. It also allows Netflix to archive, compress, or delete older data efficiently, optimizing both storage and query performance. Additionally, this partitioning mitigates the performance issues typically associated with wide partitions in Cassandra. By employing this strategy, we can operate at much higher disk utilization, as it reduces the need to reserve large amounts of disk space for compactions, thereby saving costs.\n\nHere is what it looks like :\n\nTime Slice: A time slice is the unit of data retention and maps directly to a Cassandra table. We create multiple such time slices, each covering a specific interval of time. An event lands in one of these slices based on the event_time. These slices are joined with no time gaps in between, with operations being start-inclusive and end-exclusive, ensuring that all data lands in one of the slices. By utilizing these time slices, we can efficiently implement retention by dropping entire tables, which reduces storage space and saves on costs.\n\nWhy not use row-based Time-To-Live (TTL)?\n\nUsing TTL on individual events would generate a significant number of tombstones in Cassandra, degrading performance, especially during range scans. By employing discrete time slices and dropping them, we avoid the tombstone issue entirely. The tradeoff is that data may be retained slightly longer than necessary, as an entire table\u2019s time range must fall outside the retention window before it can be dropped. Additionally, TTLs are difficult to adjust later, whereas TimeSeries can extend the dataset retention instantly with a single control plane operation.\n\nTime Buckets: Within a time slice, data is further partitioned into time buckets. This facilitates effective range scans by allowing us to target specific time buckets for a given query range. The tradeoff is that if a user wants to read the entire range of data over a large time period, we must scan many partitions. We mitigate potential latency by scanning these partitions in parallel and aggregating the data at the end. In most cases, the advantage of targeting smaller data subsets outweighs the read amplification from these scatter-gather operations. Typically, users read a smaller subset of data rather than the entire retention range.\n\nEvent Buckets: To manage extremely high-throughput write operations, which may result in a burst of writes for a given time series within a short period, we further divide the time bucket into event buckets. This prevents overloading the same partition for a given time range and also reduces partition sizes further, albeit with a slight increase in read amplification.\n\nNote: With Cassandra 4.x onwards, we notice a substantial improvement in the performance of scanning a range of data in a wide partition. See Future Enhancements at the end to see the Dynamic Event bucketing work that aims to take advantage of this.\n\nStorage Tables\n\nWe use two kinds of tables\n\nData tables : These are the time slices that store the actual event data.\n\n: These are the time slices that store the actual event data. Metadata table: This table stores information about how each time slice is configured per namespace.\n\nData tables\n\nThe partition key enables splitting events for a time_series_id over a range of time_bucket(s) and event_bucket(s), thus mitigating hot partitions, while the clustering key allows us to keep data sorted on disk in the order we almost always want to read it. The value_metadata column stores metadata for the event_item_value such as compression.\n\nWriting to the data table:\n\nUser writes will land in a given time slice, time bucket, and event bucket as a factor of the event_time attached to the event. This factor is dictated by the control plane configuration of a given namespace.\n\nFor example:\n\nDuring this process, the writer makes decisions on how to handle the data before writing, such as whether to compress it. The value_metadata column records any such post-processing actions, ensuring that the reader can accurately interpret the data.\n\nReading from the data table:\n\nThe below illustration depicts at a high-level on how we scatter-gather the reads from multiple partitions and join the result set at the end to return the final result.\n\nMetadata table\n\nThis table stores the configuration data about the time slices for a given namespace.\n\nNote the following:\n\nNo Time Gaps : The end_time of a given time slice overlaps with the start_time of the next time slice, ensuring all events find a home.\n\n: The end_time of a given time slice overlaps with the start_time of the next time slice, ensuring all events find a home. Retention : The status indicates which tables fall inside and outside of the retention window.\n\n: The status indicates which tables fall inside and outside of the retention window. Flexible: This metadata can be adjusted per time slice, allowing us to tune the partition settings of future time slices based on observed data patterns in the current time slice.\n\nThere is a lot more information that can be stored into the metadata column (e.g., compaction settings for the table), but we only show the partition settings here for brevity.\n\nIndex Datastore\n\nTo support secondary access patterns via non-primary key attributes, we index data into Elasticsearch. Users can configure a list of attributes per namespace that they wish to search and/or aggregate data on. The service extracts these fields from events as they stream in, indexing the resultant documents into Elasticsearch. Depending on the throughput, we may use Elasticsearch as a reverse index, retrieving the full data from Cassandra, or we may store the entire source data directly in Elasticsearch.\n\nNote: Again, users are never directly exposed to Elasticsearch, just like they are not directly exposed to Cassandra. Instead, they interact with the Search and Aggregate API endpoints that translate a given query to that needed for the underlying datastore.\n\nIn the next section, we will talk about how we configure these data stores for different datasets.\n\nControl Plane\n\nThe data plane is responsible for executing the read and write operations, while the control plane configures every aspect of a namespace\u2019s behavior. The data plane communicates with the TimeSeries control stack, which manages this configuration information. In turn, the TimeSeries control stack interacts with a sharded Data Gateway Platform Control Plane that oversees control configurations for all abstractions and namespaces.\n\nSeparating the responsibilities of the data plane and control plane helps maintain the high availability of our data plane, as the control plane takes on tasks that may require some form of schema consensus from the underlying data stores.\n\nNamespace Configuration\n\nThe below configuration snippet demonstrates the immense flexibility of the service and how we can tune several things per namespace using our control plane.\n\n\"persistence_configuration\": [\n\n{\n\n\"id\": \"PRIMARY_STORAGE\",\n\n\"physical_storage\": {\n\n\"type\": \"CASSANDRA\", // type of primary storage\n\n\"cluster\": \"cass_dgw_ts_tracing\", // physical cluster name\n\n\"dataset\": \"tracing_default\" // maps to the keyspace\n\n},\n\n\"config\": {\n\n\"timePartition\": {\n\n\"secondsPerTimeSlice\": \"129600\", // width of a time slice\n\n\"secondPerTimeBucket\": \"3600\", // width of a time bucket\n\n\"eventBuckets\": 4 // how many event buckets within\n\n},\n\n\"queueBuffering\": {\n\n\"coalesce\": \"1s\", // how long to coalesce writes\n\n\"bufferCapacity\": 4194304 // queue capacity in bytes\n\n},\n\n\"consistencyScope\": \"LOCAL\", // single-region/multi-region\n\n\"consistencyTarget\": \"EVENTUAL\", // read/write consistency\n\n\"acceptLimit\": \"129600s\" // how far back writes are allowed\n\n},\n\n\"lifecycleConfigs\": {\n\n\"lifecycleConfig\": [ // Primary store data retention\n\n{\n\n\"type\": \"retention\",\n\n\"config\": {\n\n\"close_after\": \"1296000s\", // close for reads/writes\n\n\"delete_after\": \"1382400s\" // drop time slice\n\n}\n\n}\n\n]\n\n}\n\n},\n\n{\n\n\"id\": \"INDEX_STORAGE\",\n\n\"physicalStorage\": {\n\n\"type\": \"ELASTICSEARCH\", // type of index storage\n\n\"cluster\": \"es_dgw_ts_tracing\", // ES cluster name\n\n\"dataset\": \"tracing_default_useast1\" // base index name\n\n},\n\n\"config\": {\n\n\"timePartition\": {\n\n\"secondsPerSlice\": \"129600\" // width of the index slice\n\n},\n\n\"consistencyScope\": \"LOCAL\",\n\n\"consistencyTarget\": \"EVENTUAL\", // how should we read/write data\n\n\"acceptLimit\": \"129600s\", // how far back writes are allowed\n\n\"indexConfig\": {\n\n\"fieldMapping\": { // fields to extract to index\n\n\"tags.nf.app\": \"KEYWORD\",\n\n\"tags.duration\": \"INTEGER\",\n\n\"tags.enabled\": \"BOOLEAN\"\n\n},\n\n\"refreshInterval\": \"60s\" // Index related settings\n\n}\n\n},\n\n\"lifecycleConfigs\": {\n\n\"lifecycleConfig\": [\n\n{\n\n\"type\": \"retention\", // Index retention settings\n\n\"config\": {\n\n\"close_after\": \"1296000s\",\n\n\"delete_after\": \"1382400s\"\n\n}\n\n}\n\n]\n\n}\n\n}\n\n]\n\nProvisioning Infrastructure\n\nWith so many different parameters, we need automated provisioning workflows to deduce the best settings for a given workload. When users want to create their namespaces, they specify a list of workload desires, which the automation translates into concrete infrastructure and related control plane configuration. We highly encourage you to watch this ApacheCon talk, by one of our stunning colleagues Joey Lynch, on how we achieve this. We may go into detail on this subject in one of our future blog posts.\n\nOnce the system provisions the initial infrastructure, it then scales in response to the user workload. The next section describes how this is achieved.\n\nScalability\n\nOur users may operate with limited information at the time of provisioning their namespaces, resulting in best-effort provisioning estimates. Further, evolving use-cases may introduce new throughput requirements over time. Here\u2019s how we manage this:\n\nHorizontal scaling : TimeSeries server instances can auto-scale up and down as per attached scaling policies to meet the traffic demand. The storage server capacity can be recomputed to accommodate changing requirements using our capacity planner.\n\n: TimeSeries server instances can auto-scale up and down as per attached scaling policies to meet the traffic demand. The storage server capacity can be recomputed to accommodate changing requirements using our capacity planner. Vertical scaling : We may also choose to vertically scale our TimeSeries server instances or our storage instances to get greater CPU, RAM and/or attached storage capacity.\n\n: We may also choose to vertically scale our TimeSeries server instances or our storage instances to get greater CPU, RAM and/or attached storage capacity. Scaling disk : We may attach EBS to store data if the capacity planner prefers infrastructure that offers larger storage at a lower cost rather than SSDs optimized for latency. In such cases, we deploy jobs to scale the EBS volume when the disk storage reaches a certain percentage threshold.\n\n: We may attach EBS to store data if the capacity planner prefers infrastructure that offers larger storage at a lower cost rather than SSDs optimized for latency. In such cases, we deploy jobs to scale the EBS volume when the disk storage reaches a certain percentage threshold. Re-partitioning data: Inaccurate workload estimates can lead to over or under-partitioning of our datasets. TimeSeries control-plane can adjust the partitioning configuration for upcoming time slices, once we realize the nature of data in the wild (via partition histograms). In the future we plan to support re-partitioning of older data and dynamic partitioning of current data.\n\nDesign Principles\n\nSo far, we have seen how TimeSeries stores, configures and interacts with event datasets. Let\u2019s see how we apply different techniques to improve the performance of our operations and provide better guarantees.\n\nEvent Idempotency\n\nWe prefer to bake in idempotency in all mutation endpoints, so that users can retry or hedge their requests safely. Hedging is when the client sends an identical competing request to the server, if the original request does not come back with a response in an expected amount of time. The client then responds with whichever request completes first. This is done to keep the tail latencies for an application relatively low. This can only be done safely if the mutations are idempotent. For TimeSeries, the combination of event_time, event_id and event_item_key form the idempotency key for a given time_series_id event.\n\nSLO-based Hedging\n\nWe assign Service Level Objectives (SLO) targets for different endpoints within TimeSeries, as an indication of what we think the performance of those endpoints should be for a given namespace. We can then hedge a request if the response does not come back in that configured amount of time.\n\n\"slos\": {\n\n\"read\": { // SLOs per endpoint\n\n\"latency\": {\n\n\"target\": \"0.5s\", // hedge around this number\n\n\"max\": \"1s\" // time-out around this number\n\n}\n\n},\n\n\"write\": {\n\n\"latency\": {\n\n\"target\": \"0.01s\",\n\n\"max\": \"0.05s\"\n\n}\n\n}\n\n}\n\nPartial Return\n\nSometimes, a client may be sensitive to latency and willing to accept a partial result set. A real-world example of this is real-time frequency capping. Precision is not critical in this case, but if the response is delayed, it becomes practically useless to the upstream client. Therefore, the client prefers to work with whatever data has been collected so far rather than timing out while waiting for all the data. The TimeSeries client supports partial returns around SLOs for this purpose. Importantly, we still maintain the latest order of events in this partial fetch.\n\nAdaptive Pagination\n\nAll reads start with a default fanout factor, scanning 8 partition buckets in parallel. However, if the service layer determines that the time_series dataset is dense \u2014 i.e., most reads are satisfied by reading the first few partition buckets \u2014 then it dynamically adjusts the fanout factor of future reads in order to reduce the read amplification on the underlying datastore. Conversely, if the dataset is sparse, we may want to increase this limit with a reasonable upper bound.\n\nLimited Write Window\n\nIn most cases, the active range for writing data is smaller than the range for reading data \u2014 i.e., we want a range of time to become immutable as soon as possible so that we can apply optimizations on top of it. We control this by having a configurable \u201cacceptLimit\u201d parameter that prevents users from writing events older than this time limit. For example, an accept limit of 4 hours means that users cannot write events older than now() \u2014 4 hours. We sometimes raise this limit for backfilling historical data, but it is tuned back down for regular write operations. Once a range of data becomes immutable, we can safely do things like caching, compressing, and compacting it for reads.\n\nBuffering Writes\n\nWe frequently leverage this service for handling bursty workloads. Rather than overwhelming the underlying datastore with this load all at once, we aim to distribute it more evenly by allowing events to coalesce over short durations (typically seconds). These events accumulate in in-memory queues running on each instance. Dedicated consumers then steadily drain these queues, grouping the events by their partition key, and batching the writes to the underlying datastore.\n\nThe queues are tailored to each datastore since their operational characteristics depend on the specific datastore being written to. For instance, the batch size for writing to Cassandra is significantly smaller than that for indexing into Elasticsearch, leading to different drain rates and batch sizes for the associated consumers.\n\nWhile using in-memory queues does increase JVM garbage collection, we have experienced substantial improvements by transitioning to JDK 21 with ZGC. To illustrate the impact, ZGC has reduced our tail latencies by an impressive 86%:\n\nBecause we use in-memory queues, we are prone to losing events in case of an instance crash. As such, these queues are only used for use cases that can tolerate some amount of data loss .e.g. tracing/logging. For use cases that need guaranteed durability and/or read-after-write consistency, these queues are effectively disabled and writes are flushed to the data store almost immediately.\n\nDynamic Compaction\n\nOnce a time slice exits the active write window, we can leverage the immutability of the data to optimize it for read performance. This process may involve re-compacting immutable data using optimal compaction strategies, dynamically shrinking and/or splitting shards to optimize system resources, and other similar techniques to ensure fast and reliable performance.\n\nThe following section provides a glimpse into the real-world performance of some of our TimeSeries datasets.\n\nReal-world Performance\n\nThe service can write data in the order of low single digit milliseconds\n\nwhile consistently maintaining stable point-read latencies:\n\nAt the time of writing this blog, the service was processing close to 15 million events/second across all the different datasets at peak globally.\n\nTime Series Usage @ Netflix\n\nThe TimeSeries Abstraction plays a vital role across key services at Netflix. Here are some impactful use cases:\n\nTracing and Insights: Logs traces across all apps and micro-services within Netflix, to understand service-to-service communication, aid in debugging of issues, and answer support requests.\n\nLogs traces across all apps and micro-services within Netflix, to understand service-to-service communication, aid in debugging of issues, and answer support requests. User Interaction Tracking : Tracks millions of user interactions \u2014 such as video playbacks, searches, and content engagement \u2014 providing insights that enhance Netflix\u2019s recommendation algorithms in real-time and improve the overall user experience.\n\n: Tracks millions of user interactions \u2014 such as video playbacks, searches, and content engagement \u2014 providing insights that enhance Netflix\u2019s recommendation algorithms in real-time and improve the overall user experience. Feature Rollout and Performance Analysis : Tracks the rollout and performance of new product features, enabling Netflix engineers to measure how users engage with features, which powers data-driven decisions about future improvements.\n\n: Tracks the rollout and performance of new product features, enabling Netflix engineers to measure how users engage with features, which powers data-driven decisions about future improvements. Asset Impression Tracking and Optimization : Tracks asset impressions ensuring content and assets are delivered efficiently while providing real-time feedback for optimizations.\n\n: Tracks asset impressions ensuring content and assets are delivered efficiently while providing real-time feedback for optimizations. Billing and Subscription Management: Stores historical data related to billing and subscription management, ensuring accuracy in transaction records and supporting customer service inquiries.\n\nand more\u2026\n\nFuture Enhancements\n\nAs the use cases evolve, and the need to make the abstraction even more cost effective grows, we aim to make many improvements to the service in the upcoming months. Some of them are:\n\nTiered Storage for Cost Efficiency: Support moving older, lesser-accessed data into cheaper object storage that has higher time to first byte, potentially saving Netflix millions of dollars.\n\nSupport moving older, lesser-accessed data into cheaper object storage that has higher time to first byte, potentially saving Netflix millions of dollars. Dynamic Event Bucketing: Support real-time partitioning of keys into optimally-sized partitions as events stream in, rather than having a somewhat static configuration at the time of provisioning a namespace. This strategy has a huge advantage of not partitioning time_series_ids that don\u2019t need it, thus saving the overall cost of read amplification. Also, with Cassandra 4.x, we have noted major improvements in reading a subset of data in a wide partition that could lead us to be less aggressive with partitioning the entire dataset ahead of time.\n\nSupport real-time partitioning of keys into optimally-sized partitions as events stream in, rather than having a somewhat static configuration at the time of provisioning a namespace. This strategy has a huge advantage of not partitioning time_series_ids that don\u2019t need it, thus saving the overall cost of read amplification. Also, with Cassandra 4.x, we have noted major improvements in reading a subset of data in a wide partition that could lead us to be less aggressive with partitioning the entire dataset ahead of time. Caching: Take advantage of immutability of data and cache it intelligently for discrete time ranges.\n\nTake advantage of immutability of data and cache it intelligently for discrete time ranges. Count and other Aggregations: Some users are only interested in counting events in a given time interval rather than fetching all the event data for it.\n\nConclusion\n\nThe TimeSeries Abstraction is a vital component of Netflix\u2019s online data infrastructure, playing a crucial role in supporting both real-time and long-term decision-making. Whether it\u2019s monitoring system performance during high-traffic events or optimizing user engagement through behavior analytics, TimeSeries Abstraction ensures that Netflix operates seamlessly and efficiently on a global scale.\n\nAs Netflix continues to innovate and expand into new verticals, the TimeSeries Abstraction will remain a cornerstone of our platform, helping us push the boundaries of what\u2019s possible in streaming and beyond.\n\nStay tuned for Part 2, where we\u2019ll introduce our Distributed Counter Abstraction, a key element of Netflix\u2019s Composite Abstractions, built on top of the TimeSeries Abstraction.\n\nAcknowledgments\n\nSpecial thanks to our stunning colleagues who contributed to TimeSeries Abstraction\u2019s success: Tom DeVoe Mengqing Wang, Kartik Sathyanarayanan, Jordan West, Matt Lehman, Cheng Wang, Chris Lohfink .", "label": 0}
{"title": "GitHub Issues search now supports nested queries and boolean operators: Here\u2019s how we (re)built it", "url": "https://github.blog/developer-skills/application-development/github-issues-search-now-supports-nested-queries-and-boolean-operators-heres-how-we-rebuilt-it/", "content": "Originally, Issues search was limited by a simple, flat structure of queries. But with advanced search syntax, you can now construct searches using logical AND/OR operators and nested parentheses, pinpointing the exact set of issues you care about.\n\nBuilding this feature presented significant challenges: ensuring backward compatibility with existing searches, maintaining performance under high query volume, and crafting a user-friendly experience for nested searches. We\u2019re excited to take you behind the scenes to share how we took this long-requested feature from idea to production.\n\nHere\u2019s what you can do with the new syntax and how it works behind the scenes\n\nIssues search now supports building queries with logical AND/OR operators across all fields, with the ability to nest query terms. For example is:issue state:open author:rileybroughten (type:Bug OR type:Epic) finds all issues that are open AND were authored by rileybroughten AND are either of type bug or epic.\n\nHow did we get here?\n\nPreviously, as mentioned, Issues search only supported a flat list of query fields and terms, which were implicitly joined by a logical AND. For example, the query assignee:@me label:support new-project translated to \u201cgive me all issues that are assigned to me AND have the label support AND contain the text new-project.\u201d\n\nBut the developer community has been asking for more flexibility in issue search, repeatedly, for nearly a decade now. They wanted to be able to find all issues that had either the label support or the label question , using the query label:support OR label:question . So, we shipped an enhancement towards this request in 2021, when we enabled an OR style search using a comma-separated list of values.\n\nHowever, they still wanted the flexibility to search this way across all issue fields, and not just the labels field. So we got to work.\n\nTechnical architecture and implementation\n\nFrom an architectural perspective, we swapped out the existing search module for Issues (IssuesQuery), with a new search module (ConditionalIssuesQuery), that was capable of handling nested queries while continuing to support existing query formats.\n\nThis involved rewriting IssueQuery, the search module that parsed query strings and mapped them into Elasticsearch queries.\n\nTo build a new search module, we first needed to understand the existing search module, and how a single search query flowed through the system. At a high level, when a user performs a search, there are three stages in its execution:\n\nParse: Breaking the user input string into a structure that is easier to process (like a list or a tree) Query: Transforming the parsed structure into an Elasticsearch query document, and making a query against Elasticsearch. Normalize: Mapping the results obtained from Elasticsearch (JSON) into Ruby objects for easy access and pruning the results to remove records that had since been removed from the database.\n\nEach stage presented its own challenges, which we\u2019ll explore in more detail below. The Normalize step remained unchanged during the re-write, so we won\u2019t dive into that one.\n\nParse stage\n\nThe user input string (the search phrase) is first parsed into an intermediate structure. The search phrase could include:\n\nQuery terms: The relevant words the user is trying to find more information about (ex: \u201cmodels\u201d)\n\nThe relevant words the user is trying to find more information about (ex: \u201cmodels\u201d) Search filters: These restrict the set of returned search documents based on some criteria (ex: \u201cassignee:Deborah-Digges\u201d)\n\nExample search phrase:\n\nFind all issues assigned to me that contain the word \u201ccodespaces\u201d: is:issue assignee:@me codespaces\n\nFind all issues with the label documentation that are assigned to me: assignee:@me label:documentation\n\n\n\nThe old parsing method: flat list\n\nWhen only flat, simple queries were supported, it was sufficient to parse the user\u2019s search string into a list of search terms and filters, which would then be passed along to the next stage of the search process.\n\nThe new parsing method: abstract syntax tree\n\nAs nested queries may be recursive, parsing the search string into a list was no longer sufficient. We changed this component to parse the user\u2019s search string into an Abstract Syntax Tree (AST) using the parsing library parslet.\n\nWe defined a grammar (a PEG or Parsing Expression Grammar) to represent the structure of a search string. The grammar supports both the existing query syntax and the new nested query syntax, to allow for backward compatibility.\n\nA simplified grammar for a boolean expression described by a PEG grammar for the parslet parser is shown below:\n\nclass Parser < Parslet::Parser rule(:space) { match[\" \"].repeat(1) } rule(:space?) { space.maybe } rule(:lparen) { str(\"(\") >> space? } rule(:rparen) { str(\")\") >> space? } rule(:and_operator) { str(\"and\") >> space? } rule(:or_operator) { str(\"or\") >> space? } rule(:var) { str(\"var\") >> match[\"0-9\"].repeat(1).as(:var) >> space? } # The primary rule deals with parentheses. rule(:primary) { lparen >> or_operation >> rparen | var } # Note that following rules are both right-recursive. rule(:and_operation) { (primary.as(:left) >> and_operator >> and_operation.as(:right)).as(:and) | primary } rule(:or_operation) { (and_operation.as(:left) >> or_operator >> or_operation.as(:right)).as(:or) | and_operation } # We start at the lowest precedence rule. root(:or_operation) end\n\nFor example, this user search string:\n\nis:issue AND (author:deborah-digges OR author:monalisa )\n\nwould be parsed into the following AST:\n\n{ \"root\": { \"and\": { \"left\": { \"filter_term\": { \"attribute\": \"is\", \"value\": [ { \"filter_value\": \"issue\" } ] } }, \"right\": { \"or\": { \"left\": { \"filter_term\": { \"attribute\": \"author\", \"value\": [ { \"filter_value\": \"deborah-digges\" } ] } }, \"right\": { \"filter_term\": { \"attribute\": \"author\", \"value\": [ { \"filter_value\": \"monalisa\" } ] } } } } } } }\n\nQuery\n\nOnce the query is parsed into an intermediate structure, the next steps are to:\n\nTransform this intermediate structure into a query document that Elasticsearch understands Execute the query against Elasticsearch to obtain results\n\nExecuting the query in step 2 remained the same between the old and new systems, so let\u2019s only go over the differences in building the query document below.\n\nThe old query generation: linear mapping of filter terms using filter classes\n\nEach filter term (Ex: label:documentation ) has a class that knows how to convert it into a snippet of an Elasticsearch query document. During query document generation, the correct class for each filter term is invoked to construct the overall query document.\n\nThe new query generation: recursive AST traversal to generate Elasticsearch bool query\n\nWe recursively traversed the AST generated during parsing to build an equivalent Elasticsearch query document. The nested structure and boolean operators map nicely to Elasticsearch\u2019s boolean query with the AND, OR, and NOT operators mapping to the must, should, and should_not clauses.\n\nWe re-used the building blocks for the smaller pieces of query generation to recursively construct a nested query document during the tree traversal.\n\nContinuing from the example in the parsing stage, the AST would be transformed into a query document that looked like this:\n\n{ \"query\": { \"bool\": { \"must\": [ { \"bool\": { \"must\": [ { \"bool\": { \"must\": { \"prefix\": { \"_index\": \"issues\" } } } }, { \"bool\": { \"should\": { \"terms\": { \"author_id\": [ \"<DEBORAH_DIGGES_AUTHOR_ID>\", \"<MONALISA_AUTHOR_ID>\" ] } } } } ] } } ] } // SOME TERMS OMITTED FOR BREVITY } }\n\nWith this new query document, we execute a search against Elasticsearch. This search now supports logical AND/OR operators and parentheses to search for issues in a more fine-grained manner.\n\nConsiderations\n\nIssues is one of the oldest and most heavily -used features on GitHub. Changing core functionality like Issues search, a feature with an average of nearly 2000 queries per second (QPS)\u2014that\u2019s almost 160M queries a day!\u2014presented a number of challenges to overcome.\n\nEnsuring backward compatibility\n\nIssue searches are often bookmarked, shared among users, and linked in documents, making them important artifacts for developers and teams. Therefore, we wanted to introduce this new capability for nested search queries without breaking existing queries for users.\n\nWe validated the new search system before it even reached users by:\n\nTesting extensively : We ran our new search module against all unit and integration tests for the existing search module. To ensure that the GraphQL and REST API contracts remained unchanged, we ran the tests for the search endpoint both with the feature flag for the new search system enabled and disabled.\n\n: We ran our new search module against all unit and integration tests for the existing search module. To ensure that the GraphQL and REST API contracts remained unchanged, we ran the tests for the search endpoint both with the feature flag for the new search system enabled and disabled. Validating correctness in production with dark-shipping: For 1% of issue searches, we ran the user\u2019s search against both the existing and new search systems in a background job, and logged differences in responses. By analyzing these differences we were able to fix bugs and missed edge cases before they reached our users. We weren\u2019t sure at the outset how to define \u201cdifferences,\u201d but we settled on \u201cnumber of results\u201d for the first iteration. In general, it seemed that we could determine whether a user would be surprised by the results of their search against the new search capability if a search returned a different number of results when they were run within a second or less of each other.\n\nFor 1% of issue searches, we ran the user\u2019s search against both the existing and new search systems in a background job, and logged differences in responses. By analyzing these differences we were able to fix bugs and missed edge cases before they reached our users.\n\nPreventing performance degradation\n\nWe expected more complex nested queries to use more resources on the backend than simpler queries, so we needed to establish a realistic baseline for nested queries, while ensuring no regression in the performance of existing, simpler ones.\n\nFor 1% of Issue searches, we ran equivalent queries against both the existing and the new search systems. We used scientist, GitHub\u2019s open source Ruby library, for carefully refactoring critical paths, to compare the performance of equivalent queries to ensure that there was no regression.\n\nPreserving user experience\n\nWe didn\u2019t want users to have a worse experience than before just because more complex searches were possible.\n\nWe collaborated closely with product and design teams to ensure usability didn\u2019t decrease as we added this feature by:\n\nLimiting the number of nested levels in a query to five. From customer interviews, we found this to be a sweet spot for both utility and usability.\n\nin a query to five. From customer interviews, we found this to be a sweet spot for both utility and usability. Providing helpful UI/UX cues: We highlight the AND/OR keywords in search queries, and provide users with the same auto-complete feature for filter terms in the UI that they were accustomed to for simple flat queries.\n\nMinimizing risk to existing users\n\nFor a feature that is used by millions of users a day, we needed to be intentional about rolling it out in a way that minimized risk to users.\n\nWe built confidence in our system by:\n\nLimiting blast radius : To gradually build confidence, we only integrated the new system in the GraphQL API and the Issues tab for a repository in the UI to start. This gave us time to collect, respond to, and incorporate feedback without risking a degraded experience for all consumers. Once we were happy with its performance, we rolled it out to the Issues dashboard and the REST API.\n\n: To gradually build confidence, we only integrated the new system in the GraphQL API and the Issues tab for a repository in the UI to start. This gave us time to collect, respond to, and incorporate feedback without risking a degraded experience for all consumers. Once we were happy with its performance, we rolled it out to the Issues dashboard and the REST API. Testing internally and with trusted partners: As with every feature we build at GitHub, we tested this feature internally for the entire period of its development by shipping it to our own team during the early days, and then gradually rolling it out to all GitHub employees. We then shipped it to trusted partners to gather initial user feedback.\n\nAnd there you have it, that\u2019s how we built, validated, and shipped the new and improved Issues search!\n\nFeedback\n\nWant to try out this exciting new functionality? Head to our docs to learn about how to use boolean operators and parentheses to search for the issues you care about!\n\nIf you have any feedback for this feature, please drop us a note on our community discussions.\n\nAcknowledgements\n\nSpecial thanks to AJ Schuster, Riley Broughten, Stephanie Goldstein, Eric Jorgensen Mike Melanson and Laura Lindeman for the feedback on several iterations of this blog post!\n\nTags:", "label": 0}
{"title": "From idea to app: Introducing Stitch, a new way to design UIs", "url": "https://developers.googleblog.com/en/stitch-a-new-way-to-design-uis/", "content": "That's precisely the problem Stitch aims to solve \u2013 Stitch is a new experiment from Google Labs that allows you to turn simple prompt and image inputs into complex UI designs and frontend code in minutes.\n\nBuilding great applications always comes down to a powerful partnership between design and development. Designers envision the user experience, crafting intuitive and engaging interfaces. Developers then bring those designs to life with functional code. Traditionally, connecting design ideas to working code took a lot of manual effort and back-and-forth.\n\nStitch was born of an idea between a designer and an engineer, both looking to build a product that optimized their respective workflows. It leverages the multimodal capabilities of Gemini 2.5 Pro to create a more fluid and integrated workflow between design and development. And, with an option to refine your design with image inputs, an interactive chat, theme selectors, and a paste to Figma function, Stitch lets you truly hone in on your creative designs and development needs.\n\n\n\nHere\u2019s what Stitch offers today to enhance your design and development process:\n\n\n\nGenerate UI from natural language\n\nDescribe the application you want to build in plain English, including details like color palettes or desired user experience. Stitch can generate a visual interface tailored to your description.\n\n\n\nGenerate UI from images or wireframes\n\nHave a design sketch on a whiteboard, a screenshot of a compelling UI, or a rough wireframe? Upload it to Stitch. Stitch processes the image to produce a corresponding digital UI, bridging your initial visual ideas to a functional design.\n\n\n\nRapid iteration and design exploration\n\nDesign is an iterative process, and Stitch facilitates this by allowing you to generate multiple variants of your interface. Experiment with different layouts, components, and styles to achieve the desired look and feel.\n\n\n\nSeamless transition to development\n\nOnce you're satisfied with your design, Stitch provides crucial bridges to the development workflow:\n\nPaste to Figma: Your generated design can be seamlessly pasted to Figma for easy further refinement, collaboration with design teams, and integration into existing design systems.\n\nExport front-end code: Stitch generates clean, functional front-end code based on your design, so you have a fully functional UI ready to go.\n\n\n\nStitch is about unlocking the magic of app creation for everyone. We're thrilled to bring this experiment to you and can't wait to see what you'll build with it.\n\nTry out Stitch at stitch.withgoogle.com and let us know what you think!", "label": 0}
{"title": "My Future Endeavors", "url": "https://lifeofpablo.com/blog/my-future-endeavors", "content": "My Future Endeavors\n\nThis post was written in English (en_US).\n\nCareer\n\nI am so excited to become a teacher down the road and spread my global awareness to my future students. Teachers are so needed to help build the next generation of students who will lead the way to a better future. Living in the world that we live in today, it is needed to learn another language, to love cultures and to love those people who are different from us. All this has me so stoked!\n\nPondering\n\nLately, I've been thinking about my future. I've been asking myself, \"\"what can I do to become more than just an ordinary teacher?\"\" I definitely want to be more well-rounded as a person. I'm not the type of person who is satisfied settling for one thing. I've been becoming the person who really wants to try new things and hope to see where such road takes me? Even though I should have minored in it at least, I still feel like I can be successful learning new skills. I want to start a new journey this summer in learning new skills. I want to leave my comfort zone and just do me? So what exactly am I doing?\n\n3 things\n\nI decided that I want to learn and continue learning 3 things: learn the skills to graphic design, continue HTML/CSS & other programming languages and learn to make a simple app. This is something I've always wanted to do for a while but was scared to start. I have received inspiration from people around me. This is gives me hope that even though I am only a beginner. Mastering all of these skills will take more than a summer. I have accepted this.\n\nI also plan on being able to continuously read a book at all times. Knowledge will help me sooth the savage beast from driving me crazy. I want to be able to grow the mind and soul in ways I haven't felt in a longtime.\n\nPersonal Growth\n\nOverall, I want this to be my summer to help me grow, to enlighten me and overall, to help me become a better person. I won't just limit myself to the items mentioned above. I'm going to try all sorts of things. How I go about this will be all up to me. My attitude toward things will decide how bad or great my outcome will be. I know that things will be positive. I will face obstacles but I will overcome them. What matters is that I tried something new this summer.\n\nI want to thank you for all of your support. I hope to be able to share my adventure as I go. Even though I am hoping to grow this summer, I want to let you know that you will also grow this summer. You and I will find the right path for a bright future.\"", "label": 1}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2023-10", "content": "Background Information\n\nA few weeks ago I attended Climate Week NYC in New York City. It was such a great time listening to all the great talks and participating in different activities such as driving a Lucid electric car with my friends. It got to meet such amazing people (and networked) who are in the climate change space. It made me step back an realize how much time we have lost not fighting the environment and how we only have a limited time to speed up on our actions.\n\nIt was nice to be in a place full of public transportation and get my steps in. I was also wired on Matcha Lattes to keep up with the revolving door of events occuring all at once.\n\nGetting Around to Building the Website\n\nI've been meaning to build a website for a quick summary of my adventures in the city. I finally found the perfect day to do it. I finally was able attended a Build a Website in an Hour event. Thank you James G for hosting this event!\n\nJust as the name of the event states, I built (most) of the website in an hour. It was fun and challenging because of the following:\n\nI didn't have a plan on how this site was going to look It was a challenge to get the site made in an hour. Talk about pressure I got to hangout with people in the IndieWeb. Show off the adventures I went on.\n\nWhere is this site located?\n\nThis website is located at https://climateweeknyc.lifeofpablo.com .\n\nTechnologies Used\n\nThis website was made using vanilla HTML & CSS. I used Tachyions CSS Framework for the CSS. I use it on my website, primarily on my blog, to change the appearance of my posts. I like this CSS framework because is it doesn't interfere with existing CSS frameworks or existing CSS stylesheets. The classes are easy to learn\n\nA Breakdown of the Site\n\nHero Banner Description: A green hero banner with a navigation. It says \"NYC Climate Week 2023\" and \"A Wakeup Call About Climate Change\"\n\nOther Sections Include:\n\nIntroduction\n\nAttendees\n\nEvents Attended\n\nPhotos\n\nWebmentions\n\nThis site supports webmentions!\n\nClosing\n\nI hope you enjoy the website! Let me know if you have any comments, questions or concerns. Don't forget to say, \"hello\"!", "label": 1}
{"title": "Creating rituals: remembering Coach Taylor \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2024/08/16/creating-rituals-remembering-coach-taylor/", "content": "My high school track and cross country coach Mr. Taylor was a master at creating rituals and culture. A former seminary student who was obsessed with ancient Greece, he started each practice with a philosophy discussion. He wove in, too, tales of past runners and triumphs \u2014 he\u2019d been coach for nearly thirty years already \u2014 creating our own pantheon of heroes, the team\u2019s own mythos. Who of us would one day join its ranks?\n\nMr. Taylor recognized how much the mind and body are connected. He treated our labor seriously, and expected us to do the same. Excellence \u2014 ar\u00eate \u2014 was our goal. I\u2019ll admit that plenty of times I tuned out the talk, but there was something about sitting down as a team to transition from school to sport, using that time to mentally prepare for the physical work.\n\nOne of my favorite rituals was to honor outgoing seniors on the cross-country team. At the final home race of the season, each senior released one of his homing pigeons. (I was annoyed because I had to share a pigeon \ud83d\ude02) As far as I can remember, there was no special meaning to this, it just was a thing that he made happen each year. A created ceremony we nonetheless looked forward to; simply having some marker, however arbitrary, gave the season an emotional culmination beyond the final race, recognition of our contributions to the team regardless of how we placed.\n\nHe built the team\u2019s culture through these shared rites and rituals, building a sense of continuity with past runners both at our school and at the origins of track in Ancient Greece.\n\nI can\u2019t talk about ritual and Mr. Taylor without also talking about Ar\u00eate West. Starting in 1979, every other year he took a group of about eight students from the track or cross country team to Greece. At first, they went to compete, but over the years it shifted into more of a philosophical journey and extended into greater Europe. Over the course of twenty trips, the experience developed its own rituals. (I went on trip thirteen.)\n\nThe first: secret invitation. My best friend hounded me, but I wouldn\u2019t confess that I\u2019d received The Postcard telling me when and where our first meeting would be. (We met every other week throughout the school year to study and prepare for the trip.) We all showed up that first night unsure who the seven other voyagers would be. In my year, the eighth person no-showed \u2014 we never found out who else had been invited and declined to join.\n\nThe trip itself teemed with rituals. My least favorite involved running through the hot, dusty Greek countryside to an ancient track where we were expected to take off our shoes and run a lap in bare feet, like the runners of old. On Ithaca, we ran, separately, to \u201cTelemachos\u2019 beach\u201d. In Delphi, we were meant to sleep in \u201cthe meadow\u201d where many past Arete trips had slept (but he couldn\u2019t find it in the twilight, so we slept on a random dirt road \ud83d\ude12 Apparently on my sister\u2019s trip there was a dead dog in the meadow and they said no fucking way).\n\nMr. Taylor believed strongly in the power of place and designed the experience to provoke encounters with meaning. (Some more successful than others\u2026) I lugged my clarinet across Europe so I could play Mozart\u2019s clarinet concerto outside the building where he died (?). Everywhere we went, one or another of us had been assigned topics to present on or locations to give a reading. (I botched my reading from The Agony and The Ecstasy at the Vatican and read a passage about the wrong Michaelangelo Mother and Child sculpture \ud83e\udd23) Overlooking the Acropolis, one of the boys had to read us Pericles\u2019s funeral speech, as each trip had done before us. In Sils Maria, we walked in Nietzsche\u2019s footsteps and discussed quotes at various waypoints along the trail. Our trip was our own, yet inextricably linked with previous trips through shared touchstones and repeated experiences: rituals meant to connect us to the past, both modern and ancient.\n\nThis is my contribution to the August IndieWeb Blog Carnival, hosted by Steve Ledlow on the theme of ritual.", "label": 1}
{"title": "Building Dash: How RAG and AI agents help us meet the needs of businesses", "url": "https://dropbox.tech/machine-learning/building-dash-rag-multi-step-ai-agents-business-users", "content": "Knowledge workers today face myriad challenges in managing their digital workflows. Information is often scattered across multiple applications and formats, and finding the right document, message, or piece of information can be both tedious and time-consuming. This fragmentation creates two major problems for businesses: it hinders collaboration and productivity, and it can lead to costly security issues. To address these challenges, we launched Dropbox Dash, a universal search and knowledge management product that combines AI-powered features with in-depth content access control. Designed to help knowledge workers organize their digital lives, Dash allows users to find, organize, share, and secure content across their apps so they can focus on the work that matters most. At its core, Dash is a universal search product powered by many machine learning technologies and supercharged by generative AI. It offers a powerful AI-driven search experience with advanced filtering capabilities that allow users to quickly locate the information they need, regardless of where it\u2019s stored. With granular access controls, Dash also makes sure employees and external partners see only the right content so that sensitive company information isn\u2019t surfaced unintentionally. And with advanced AI features, Dash can summarize, answer questions, surface insights, and generate drafts. Throughout our development process, we experimented extensively and explored numerous solutions to build an AI product for businesses. In order to meet the challenges of modern work in data-intensive environments, we ultimately turned to retrieval-augmented generation (RAG) and AI agents. Additionally, we engineered a minimal Python interpreter focused exclusively on essential features required by our AI agents and supported by extensive testing and security reviews to ensure safe code execution. In the following sections, we\u2019ll dive into the specific challenges we faced while building Dash, the innovative solutions we developed to address them, and important lessons that\u2019ll inform our work moving forward.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images\u2014plus generative AI capabilities across even more connected apps. See what's new \u2192\n\nChallenges in making an AI product that\u2019s ready for businesses Building an AI product like Dash presents a unique set of challenges that differ from those that developers typically encounter with consumer-facing applications. These challenges stem from the inherent complexities of business data environments, which are characterized by diversity, fragmentation, and multiple data modalities. Understanding and addressing these challenges is crucial for delivering effective AI solutions that meet the sophisticated needs of business users. Before we dive into how we solved \u200cthese challenges, let\u2019s first take a look at what each of these data environments entails. Data diversity Data diversity refers to the wide range of data types that a business handles, including emails, documents, meeting notes, task management data, and more. Each type of data has its own structure and context, and that can complicate AI processing.\n\nExample of identifying the right data source, which requires domain knowledge and contextual information\n\nEffectively managing diverse data types is critical because each type of data has its own structure and context. For Dash to perform well in a business setting, it must seamlessly process and understand all these different data types. Data fragmentation Data fragmentation occurs when an organization\u2019s data is spread across multiple applications. This means that relevant information isn\u2019t stored in a single location, but is instead scattered across different tools and services. Data fragmentation complicates the process of retrieving and synthesizing information. For users, this means context switching between multiple apps to manually search for the information they need, which is time-consuming, tedious, and inefficient. An AI system that can aggregate and make sense of fragmented data would greatly enhance the user experience by providing a unified and accessible information repository.\n\nExample of information spread across multiple apps, which requires combining fragmented information to construct a complete answer\n\nData modalities Data modalities refer to the different forms or modes in which data exists. Common modalities include text, images, audio, and video. Handling multiple data modalities is essential for providing a comprehensive AI solution. Business users often deal with a mix of text documents, images, presentations, and videos, among other formats. An AI system that can process and integrate all these modalities can provide a more complete and accurate response to user queries.\n\nExample of information spread across multiple modalities\n\nIn summary, these challenges\u2014data diversity, data fragmentation, and data modalities\u2014present unique complexities when building these types of AI products. Addressing these challenges is essential to creating a robust and effective AI solution that meets the various needs of knowledge workers in dynamic and data-intensive environments. To pull this off, we implemented and experimented with multiple solutions. That\u2019s where retrieval-augmented generation (RAG) and AI agents come in.\n\nLeveraging retrieval-augmented generation When building Dash, we knew that delivering accurate and relevant responses to user queries was paramount. That\u2019s why we turned to RAG, an industry-standard approach for tasks like query responses and summarization. RAG's ability to combine external information retrieval with state-of-the-art generative models makes it the perfect fit for our product, especially in the complex landscape of enterprise data. RAG works by first retrieving the most relevant pieces of content from a dataset or knowledge base and then using a large language model (LLM) to generate a response based on that content. This approach ensures that the answers that the AI system provides aren\u2019t only contextually relevant but also up to date, which is crucial in business environments where data is constantly evolving.\n\nRetrieval-augmented generation (RAG)\n\nChoosing the right retrieval system The retrieval system is the backbone of any RAG pipeline. It determines not just speed but also whether the LLM has the right context to generate meaningful answers\u2014and a misstep here can compromise the entire user experience. Put another way, the retrieval system sets the bounds of what your LLM can \u201cknow\u201d at inference time. It also greatly affects latency, which in turn impacts user satisfaction. And, it shapes the perceived quality of the final answer, since retrieval coverage can make or break correctness and completeness. There are many options when it comes to designing a retrieval system. For question-answering systems, the most common one is to have a vector index where chunked data is indexed on their embeddings. Embeddings are simply a low-dimensional semantic representation of the chunk of data. To retrieve from such an index, semantic search is often used. Another choice is to go with a more traditional approach to a search index, where documents are indexed by their lexical features (e.g., words that appear in the title or body). This approach, however, adds extra latency due to the need for on-the-fly chunking and re-ranking of the chunks during serving time. There are many other options that prioritize data freshness, such as by directly interacting with the API for the platform where your data is stored, for example. But there are a few trade-offs to consider for each of these approaches: Latency vs. quality: There\u2019s a pervasive assumption that you can\u2019t have both extremely low latency and high-quality results. Why? Because advanced or larger embedding-based semantic searches may take longer due to their complexity\u2014whether that\u2019s a heavier model or additional reranking steps. For example, if you want more than 95% of your requests to reliably complete in under 1\u20132 seconds, you might have to use smaller embedding models, which can reduce retrieval accuracy.\n\nThere\u2019s a pervasive assumption that you can\u2019t have both extremely low latency and high-quality results. Why? Because advanced or larger embedding-based semantic searches may take longer due to their complexity\u2014whether that\u2019s a heavier model or additional reranking steps. For example, if you want more than 95% of your requests to reliably complete in under 1\u20132 seconds, you might have to use smaller embedding models, which can reduce retrieval accuracy. Data freshness vs. scalability: Many projects need to keep their data fresh\u2014for instance, re-indexing a news site every few minutes. Frequent re-indexing processes can hinder system throughput or spike latency when they\u2019re underway. Alternatively, on-the-fly API calls to third-party data can push latency well above a few seconds. If near-real-time information is crucial (e.g., updating stock quotes), your system might spend more resources on frequent indexing or caching, throttling your ability to scale.\n\nMany projects need to keep their data fresh\u2014for instance, re-indexing a news site every few minutes. Frequent re-indexing processes can hinder system throughput or spike latency when they\u2019re underway. Alternatively, on-the-fly API calls to third-party data can push latency well above a few seconds. If near-real-time information is crucial (e.g., updating stock quotes), your system might spend more resources on frequent indexing or caching, throttling your ability to scale. Budget vs. user experience: High-quality solutions\u2014advanced embeddings, re-ranking steps, and large-chunked indexes\u2014often require additional compute, and more compute means more cost. If the user experience demands near-instant results with best-in-class recall, the resource burn can be significant. And if budgets are constrained, you might be forced to choose a simpler retrieval pipeline that could degrade the overall quality. For Dash use cases, we prioritized reasonable latency but also high-quality and reasonable data freshness, with both periodic data syncs and the implementation of webhooks whenever appropriate. Specifically, we stayed under 1\u20132 seconds for over 95% of our queries, which allows us some latency budget for the rest of the pipeline so that our users don\u2019t click away because the response time is too long. Ultimately, we landed on a traditional information retrieval (IR) approach combined with on-the-fly chunking and reranking: Traditional IR: We use a lexical-based system, along with smarter rerankers that use embedding features.\n\nWe use a lexical-based system, along with smarter rerankers that use embedding features. On-the-fly chunking: Documents are chunked at query time to ensure we\u2019re pulling only the relevant sections.\n\nDocuments are chunked at query time to ensure we\u2019re pulling only the relevant sections. Reranking: A larger, but still efficient, embedding model then re-sorts those results to place the most relevant chunks at the top. In practice, this yields high-quality results in under 2 seconds for over 95% of our queries, balancing speed and relevance. The combination allows us to keep costs in check while avoiding the pitfalls of purely semantic or purely lexical retrieval. Quality is best when measured end-to-end because all the parts of the RAG system need to work together effectively. Once we chose the retrieval system that best fit our needs, it was time to pick the best LLM for the job.\n\nChoosing the right model To ensure this approach met our requirements, we conducted a rigorous evaluation. We tested multiple retrieval methods and model variations on several public datasets, including Google\u2019s Natural Questions (featuring real user queries with large documents); MuSiQue (with multi-hop questions requiring information linking across different passages); and Microsoft\u2019s Machine Reading Comprehension (containing often short passages and multi-document queries from Bing logs). We also designed hand-tuned metrics to help evaluate the quality of generated answers. These included an LLM judge for answer correctness (passing retrieved evidence through an LLM to score final answer accuracy), an LLM judge for completeness (measuring the extent to which all relevant question aspects are addressed), as well as source precision, recall, and F1 metrics to evaluate how accurately we retrieved key passages needed for correct answers. By cross-referencing these metrics, we could directly compare multiple open-source and closed-source LLMs in a consistent environment. This led us to narrow down a few model families that best suited Dash\u2019s use cases. Our RAG system remains model agnostic: We want to provide the flexibility of choosing the models and providers our customers are most comfortable with. Being model agnostic also allows us to be prepared to adapt to rapid developments in the field of LLMs. Although RAG provides a solution for the most common types of questions\u2014kinds that require fetching information from one or more documents\u2014it\u2019s incapable of performing complex, multi-step tasks. This is where AI agents come in.\n\nThe role of AI agents Imagine you\u2019ve asked a colleague to help you with a complex task, such as, \u201cWhat\u2019s the progress on projects in my team\u2019s Q1 OKRs?\u201d This person would likely find the answer to this question by first breaking it down into individual steps before tackling those steps one at a time. To handle the business challenges outlined above, we need an AI system that can approach complex tasks like humans do. These tasks may require domain knowledge, contextual information, and planning and executing multiple steps\u2014and AI agents are exceptional at doing just that. The term \"AI agent\" is often used loosely across the tech industry, and with various interpretations. However, there\u2019s a common theme among all of them: an AI agent is a system that can autonomously perform tasks with very little to no human interaction. At Dropbox, our interpretation of AI agents is more specific and aligned with the needs of business applications. We view AI agents as multi-step orchestration systems that can dynamically break down user queries into individual steps, execute those steps using available resources and information from the current user, and generate a final response\u2014all while requiring minimal human oversight.\n\nAgents as multi-step orchestration\n\nThe multi-step orchestration in our AI agents includes two stages: planning and execution. Stage 1: Planning The planning stage involves breaking down a user's query into a sequence of high-level steps. This is done by an LLM, which interprets the query and generates simple code statements to express the logic of responding to the user\u2019s query. The LLM-generated code is written in our domain-specific language (DSL), which is similar to the Python programming language. The initial plan of responding to the user\u2019s query is restricted to high-level or simple code statements, which ensures clarity and precision in defining each step. For example, let\u2019s explore the request, \"Show me the notes for tomorrow\u2019s all-hands meeting.\" These steps contain the logic necessary to respond to the query: Resolve concrete dates and times for the phrase \u201ctomorrow.\u201d There needs to be an established time window to identify what \u201ctomorrow\u201d is referring to. This must be done relative to the current date and time. Identify the meeting. There needs to be a search conducted for a meeting with a title matching \"all-hands\" (and within the determined time window). Retrieve notes. Documents attached to or linked from the identified meeting must be fetched. The AI agent, however, expresses this logic as statements of code in our Python-like DSL. Below is a simplified version of what that\u2019d look like:\n\nCopy time_window: TimeWindow = time_helper.get_time_window_for_tomorrow() meeting: Meeting = meetings_helper.find_meeting(title=\"all-hands\", time_window=time_window) notes: list[Document] = meetings_helper.get_attached_documents(meeting=meeting)\n\nEach XXXX_helper object in the generated code contains functionality that acts as a building block for the LLM to use when expressing the logic of responding to the user\u2019s query.\n\nStage 2: Execution The next step is to validate and execute the logic that was expressed as code. The code is validated through static analysis to ensure correctness, safety, and to detect missing functionality. We intentionally allow the LLM to assume that missing functionality exists. If missing functionality is identified, we use the LLM a second time to implement the missing code. This two-stage approach to generating code allows the agents to be clear and focused with an overall plan, while also being adaptable to new types and variations of user queries. Below is a simplified version of what the result of each of the steps might look like: 1. Time window retrieval: Resolve the relative phrase \u201ctomorrow\u201d to concrete values.\n\nCopy time_window: TimeWindow = time_helper.get_time_window_for_tomorrow() # TimeWindow(start=\"2025-03-19\", end=\"2025-03-20\")\n\n2. Meeting identification: Search for the \"all-hands\" meeting within the resolved time window.\n\nCopy meeting: Meeting = meetings_helper.find_meeting(title=\"all-hands\", time_window=time_window) # Meeting(title=\"Company All-Hands\", start_time=..., attendees=...)\n\n3. Document retrieval: Finally, fetch the notes attached to the identified meeting.\n\nCopy notes: list[Document] = meetings_helper.get_attached_documents(meeting=meeting) # [Document(title=\"All-Hands-Notes\", content=\"...\")]\n\nThe final response to the user\u2019s query is the result of the last step. In this example, the list of documents will be returned to the user. Validation and testing The interpreter we use to execute the LLM-generated code was developed from scratch here at Dropbox. This gave us full control over everything inside the interpreter, including integrating static analysis passes and \u201cdry runs,\u201d in addition to having run-time type enforcement. Static analysis allows our interpreter to examine the code without executing it, helping us automatically identify potential security risks, missing functionality, or code correctness errors. Having run-time type enforcement helps ensure that the data and objects being operated on are the types of values that we expect. In our example, the list of documents returned to the user will always be a list of documents. Normally, testing LLM integrations can be an ever-moving target. As new model versions are released, slight changes in how things are phrased or reacted to can be expected. Knowing exactly why a test failed or why the final response differed from expectations is often challenging. However, as a result of the LLM using code to express its logic in responding to the user, we\u2019re able to make the LLM \u201cshow the work.\u201d This helps with understanding at which step the logic failed, having more deterministic testing, and evaluating the response to a query. For example: Logic failure: \u201cCan\u2019t answer this question\u201d vs. \u201cError on step 3 when fetching attached documents to meeting\u2026\u201d\n\n\u201cCan\u2019t answer this question\u201d vs. \u201cError on step 3 when fetching attached documents to meeting\u2026\u201d More deterministic testing : Does resolving \u201ctomorrow\u201d always return the correct time window?\n\n: Does resolving \u201ctomorrow\u201d always return the correct time window? Evaluating responses: \u201cDoes the response text have the approximate same meaning as what we expected?\u201d vs. \u201cDoes the response value match the expected type list[Document] ?\u201d\n\nSecurity and efficiency To address security concerns, we implemented security controls in our interpreter and its development process. Only the minimal required functionality is implemented in its runtime\u2014feature parity with CPython isn\u2019t the goal. This turns major security risks that exist in other full-featured interpreters into non-issues. As we\u2019ve explored, AI agents play a pivotal role in addressing the complexities of business tasks through their ability to plan and execute multi-step workflows autonomously. By leveraging LLMs and DSLs, these agents break down intricate queries into actionable steps, ensuring precision and efficiency. The structured approach, combined with strong typing and built-in security controls, enhances reliability and mitigates security risks. The future of AI agents in business environments is promising. And as we continue to refine and expand their capabilities, they\u2019ll become indispensable in streamlining operations, enhancing productivity, and driving innovation.", "label": 0}
{"title": "How We Generated Millions of Content Annotations", "url": "https://engineering.atspotify.com/2024/10/how-we-generated-millions-of-content-annotations", "content": "With the fields of machine learning (ML) and generative AI (GenAI) continuing to rapidly evolve and expand, it has become increasingly important for innovators in this field to anchor their model development on high-quality data.\n\nAs one of the foundational teams at Spotify focused on understanding and enriching the core content in our catalogs, we leverage ML in many of our products. For example, we use ML to detect content relations so a new track or album will be automatically placed on the right Artist Page. We also use it to analyze podcast audio, video, and metadata to identify platform policy violations. To power such experiences, we need to build several ML models that cover entire content catalogs \u2014 hundreds of millions of tracks and podcast episodes. To implement ML at this scale, we needed a strategy to collect high-quality annotations to train and evaluate our models. We wanted to improve the data collection process to be more efficient and connected and to include the right context for engineers and domain experts to operate more effectively.\n\nTo address this, we had to evaluate the end-to-end workflow. We took a straightforward ML classification project, identified the manual steps to generate annotations, and aimed to automate them. We developed scripts to sample predictions, served data for operator review, and integrated the results with model training and evaluation workflows. We increased the corpus of annotations by 10 times and did so with three times the improvement in annotator productivity.\n\nTaking that as a promising sign, we further experimented with this workflow for other ML tasks. Once we confirmed the benefits of our approach, we decided to invest in this solution in earnest. Our next objective was to define the strategy to build a platform that would scale to millions of annotations.\n\nBuilding and scaling our annotation platform\n\nWe centered our strategy around three main pillars:\n\nScaling human expertise. Implementing annotation tooling capabilities. Establishing foundational infrastructure and integration.\n\n1. Scaling human expertise.\n\nIn order to scale operations, it was imperative that we defined processes to centralize and organize our annotation resources.\n\nWe established large-scale expert human workforces in several domains to address our growing use cases, with multiple levels of experts, including the following:\n\nCore annotator workforces: These workforces are domain experts, who provide first-pass review of all annotation cases.\n\nQuality analysts: Quality analysts are top-level domain experts, who act as the escalation point for all ambiguous or complex cases identified by the core annotator workforce.\n\nProject managers: This includes individuals who connect engineering and product teams to the workforce, establish and maintain training materials, and organize feedback on data collection strategies.\n\nBeyond human expertise, we also built a configurable, LLM-based system that runs in parallel to the human experts. It has allowed us to significantly grow our corpus of high-quality annotation data with low effort and cost.\n\n2. Implementing annotation tooling capabilities.\n\nAlthough we started with a simple classification annotation project (the annotation task being answering a question), we soon realized that we had more complex use cases \u2014 such as annotating audio/video segments, natural language processing, etc. \u2014 which led to the development of custom interfaces, so we could easily spin up new projects.\n\nIn addition, we invested in tools to manage backend work, such as project management, access control, and distribution of annotations across multiple experts. This enabled us to deploy and run dozens of annotation projects in parallel, all while ensuring that experts remained productive across multiple projects.\n\nAnother focus area was project metrics \u2014 such as project completion rate, data volumes, annotations per annotator, etc. These metrics helped project managers and ML teams track their projects. We also examined the annotation data itself. For some of our use cases, there were nuances in the annotation task \u2014 for example, detecting music that was overlaid in a podcast episode audio snippet. In these cases, different experts may have different answers and opinions, so we started to compute an overall \u201cagreement\u201d metric. Any data points without a clear resolution were automatically escalated to our quality analysts. This ensures that our models receive the highest confidence annotation for training and evaluation.\n\n3. Establishing foundational infrastructure and integration.\n\nAt Spotify\u2019s scale, no one tool or application will satisfy all our needs \u2014 optionality is key. When we designed integrations with annotation tools, we were intentional about building the right abstractions. They have to be flexible and adaptable to different tools so we can leverage the right tool for the right use case. Our data models, APIs, and interfaces are generic and can be used with multiple types of annotation tooling.\n\nWe built bindings for direct integration with ML workflows at various stages from inception to production. For early/new ML development, we built CLIs and UIs for ad hoc projects. For production workflows, we built integrations with internal batch orchestration and workflow infrastructure.\n\nConclusion\n\nThe annotation platform now allows for flexibility, agility, and speed within our annotation spaces. By democratizing high-quality annotations, we\u2019ve been able to significantly reduce the time it takes to develop new ML models and iterate on existing systems.\n\nPutting an emphasis from the onset on both scaling our human domain expertise and machine capabilities was key. Scaling humans without scaling technical capabilities to support them would have presented various challenges, and only focusing on scaling technically would have resulted in lost opportunities.\n\nIt was a major investment to move from ad hoc projects to a full-scale platform solution to support ML and GenAI use cases. We continue to iterate on and improve the platform offering, incorporating the latest advancements in the industry.\n\nAcknowledgments\n\nA special thanks to Linden Vongsathorn and Marqia Williams for their support in launching this initiative and to the many people at Spotify today who continue to contribute to this important mission.", "label": 0}
{"title": "Maestro: Data/ML Workflow Orchestrator at Netflix", "url": "https://netflixtechblog.com/maestro-netflixs-workflow-orchestrator-ee13a06f9c78?source=collection_home---4------20-----------------------", "content": "Maestro: Data/ML Workflow Orchestrator at Netflix Netflix Technology Blog 18 min read \u00b7 Jul 22, 2024 -- 12 Listen Share\n\nBy Jun He, Natallia Dzenisenka, Praneeth Yenugutala, Yingyi Zhang, and Anjali Norwood\n\nTL;DR\n\nWe are thrilled to announce that the Maestro source code is now open to the public! Please visit the Maestro GitHub repository to get started. If you find it useful, please give us a star.\n\nWhat is Maestro\n\nMaestro is a horizontally scalable workflow orchestrator designed to manage large-scale Data/ML workflows such as data pipelines and machine learning model training pipelines. It oversees the entire lifecycle of a workflow, from start to finish, including retries, queuing, task distribution to compute engines, etc.. Users can package their business logic in various formats such as Docker images, notebooks, bash script, SQL, Python, and more. Unlike traditional workflow orchestrators that only support Directed Acyclic Graphs (DAGs), Maestro supports both acyclic and cyclic workflows and also includes multiple reusable patterns, including foreach loops, subworkflow, and conditional branch, etc.\n\nOur Journey with Maestro\n\nSince we first introduced Maestro in this blog post, we have successfully migrated hundreds of thousands of workflows to it on behalf of users with minimal interruption. The transition was seamless, and Maestro has met our design goals by handling our ever-growing workloads. Over the past year, we\u2019ve seen a remarkable 87.5% increase in executed jobs. Maestro now launches thousands of workflow instances and runs half a million jobs daily on average, and has completed around 2 million jobs on particularly busy days.\n\nScalability and Versatility\n\nMaestro is a fully managed workflow orchestrator that provides Workflow-as-a-Service to thousands of end users, applications, and services at Netflix. It supports a wide range of workflow use cases, including ETL pipelines, ML workflows, AB test pipelines, pipelines to move data between different storages, etc. Maestro\u2019s horizontal scalability ensures it can manage both a large number of workflows and a large number of jobs within a single workflow.\n\nAt Netflix, workflows are intricately connected. Splitting them into smaller groups and managing them across different clusters adds unnecessary complexity and degrades the user experience. This approach also requires additional mechanisms to coordinate these fragmented workflows. Since Netflix\u2019s data tables are housed in a single data warehouse, we believe a single orchestrator should handle all workflows accessing it.\n\nJoin us on this exciting journey by exploring the Maestro GitHub repository and contributing to its ongoing development. Your support and feedback are invaluable as we continue to improve the Maestro project.\n\nIntroducing Maestro\n\nNetflix Maestro offers a comprehensive set of features designed to meet the diverse needs of both engineers and non-engineers. It includes the common functions and reusable patterns applicable to various use cases in a loosely coupled way.\n\nA workflow definition is defined in a JSON format. Maestro combines user-supplied fields with those managed by Maestro to form a flexible and powerful orchestration definition. An example can be found in the Maestro repository wiki.\n\nA Maestro workflow definition comprises two main sections: properties and versioned workflow including its metadata. Properties include author and owner information, and execution settings. Maestro preserves key properties across workflow versions, such as author and owner information, run strategy, and concurrency settings. This consistency simplifies management and aids in trouble-shootings. If the ownership of the current workflow changes, the new owner can claim the ownership of the workflows without creating a new workflow version. Users can also enable the triggering or alerting features for a given workflow over the properties.\n\nVersioned workflow includes attributes like a unique identifier, name, description, tags, timeout settings, and criticality levels (low, medium, high) for prioritization. Each workflow change creates a new version, enabling tracking and easy reversion, with the active or the latest version used by default. A workflow consists of steps, which are the nodes in the workflow graph defined by users. Steps can represent jobs, another workflow using subworkflow step, or a loop using foreach step. Steps consist of unique identifiers, step types, tags, input and output step parameters, step dependencies, retry policies, and failure mode, step outputs, etc. Maestro supports configurable retry policies based on error types to enhance step resilience.\n\nThis high-level overview of Netflix Maestro\u2019s workflow definition and properties highlights its flexibility to define complex workflows. Next, we dive into some of the useful features in the following sections.\n\nWorkflow Run Strategy\n\nUsers want to automate data pipelines while retaining control over the execution order. This is crucial when workflows cannot run in parallel or must halt current executions when new ones occur. Maestro uses predefined run strategies to decide whether a workflow instance should run or not. Here is the list of predefined run strategies Maestro offers.\n\nSequential Run Strategy\n\nThis is the default strategy used by maestro, which runs workflows one at a time based on a First-In-First-Out (FIFO) order. With this run strategy, Maestro runs workflows in the order they are triggered. Note that an execution does not depend on the previous states. Once a workflow instance reaches one of the terminal states, whether succeeded or not, Maestro will start the next one in the queue.\n\nStrict Sequential Run Strategy\n\nWith this run strategy, Maestro will run workflows in the order they are triggered but block execution if there\u2019s a blocking error in the workflow instance history. Newly triggered workflow instances are queued until the error is resolved by manually restarting the failed instances or marking the failed ones unblocked.\n\nIn the above example, run5 fails at 5AM, then later runs are queued but do not run. When someone manually marks run5 unblocked or restarts it, then the workflow execution will resume. This run strategy is useful for time insensitive but business critical workflows. This gives the workflow owners the option to review the failures at a later time and unblock the executions after verifying the correctness.\n\nFirst-only Run Strategy\n\nWith this run strategy, Maestro ensures that the running workflow is complete before queueing a new workflow instance. If a new workflow instance is queued while the current one is still running, Maestro will remove the queued instance. Maestro will execute a new workflow instance only if there is no workflow instance currently running, effectively turning off queuing with this run strategy. This approach helps to avoid idempotency issues by not queuing new workflow instances.\n\nLast-only Run Strategy\n\nWith this run strategy, Maestro ensures the running workflow is the latest triggered one and keeps only the last instance. If a new workflow instance is queued while there is an existing workflow instance already running, Maestro will stop the running instance and execute the newly triggered one. This is useful if a workflow is designed to always process the latest data, such as processing the latest snapshot of an entire table each time.\n\nParallel with Concurrency Limit Run Strategy\n\nWith this run strategy, Maestro will run multiple triggered workflow instances in parallel, constrained by a predefined concurrency limit. This helps to fan out and distribute the execution, enabling the processing of large amounts of data within the time limit. A common use case for this strategy is for backfilling the old data.\n\nParameters and Expression Language Support\n\nIn Maestro, parameters play an important role. Maestro supports dynamic parameters with code injection, which is super useful and powerful. This feature significantly enhances the flexibility and dynamism of workflows, allowing using parameters to control execution logic and enable state sharing between workflows and their steps, as well as between upstream and downstream steps. Together with other Maestro features, it makes the defining of workflows dynamic and enables users to define parameterized workflows for complex use cases.\n\nHowever, code injection introduces significant security and safety concerns. For example, users might unintentionally write an infinite loop that creates an array and appends items to it, eventually crashing the server with out-of-memory (OOM) issues. While one approach could be to ask users to embed the injected code within their business logic instead of the workflow definition, this would impose additional work on users and tightly couple their business logic with the workflow. In certain cases, this approach blocks users to design some complex parameterized workflows.\n\nTo mitigate these risks and assist users to build parameterized workflows, we developed our own customized expression language parser, a simple, secure, and safe expression language (SEL). SEL supports code injection while incorporating validations during syntax tree parsing to protect the system. It leverages the Java Security Manager to restrict access, ensuring a secure and controlled environment for code execution.\n\nSimple, Secure, and Safe Expression Language (SEL)\n\nSEL is a homemade simple, secure, and safe expression language (SEL) to address the risks associated with code injection within Maestro parameterized workflows. It is a simple expression language and the grammar and syntax follow JLS (Java Language Specifications). SEL supports a subset of JLS, focusing on Maestro use cases. For example, it supports data types for all Maestro parameter types, raising errors, datetime handling, and many predefined utility methods. SEL also includes additional runtime checks, such as loop iteration limits, array size checks, object memory size limits and so on, to enhance security and reliability. For more details about SEL, please refer to the Maestro GitHub documentation.\n\nOutput Parameters\n\nTo further enhance parameter support, Maestro allows for callable step execution, which returns output parameters from user execution back to the system. The output data is transmitted to Maestro via its REST API, ensuring that the step runtime does not have direct access to the Maestro database. This approach significantly reduces security concerns.\n\nParameterized Workflows\n\nThanks to the powerful parameter support, users can easily create parameterized workflows in addition to static ones. Users enjoy defining parameterized workflows because they are easy to manage and troubleshoot while being powerful enough to solve complex use cases.\n\nStatic workflows are simple and easy to use but come with limitations. Often, users have to duplicate the same workflow multiple times to accommodate minor changes. Additionally, workflow and jobs cannot share the states without using parameters.\n\nOn the other hand, completely dynamic workflows can be challenging to manage and support. They are difficult to debug or troubleshoot and hard to be reused by others.\n\nParameterized workflows strike a balance by being initialized step by step at runtime based on user defined parameters. This approach provides great flexibility for users to control the execution at runtime while remaining easy to manage and understand.\n\nAs we described in the previous Maestro blog post, parameter support enables the creation of complex parameterized workflows, such as backfill data pipelines.\n\nWorkflow Execution Patterns\n\nMaestro provides multiple useful building blocks that allow users to easily define dataflow patterns or other workflow patterns. It provides support for common patterns directly within the Maestro engine. Direct engine support not only enables us to optimize these patterns but also ensures a consistent approach to implementing them. Next, we will talk about the three major building blocks that Maestro provides.\n\nForeach Support\n\nIn Maestro, the foreach pattern is modeled as a dedicated step within the original workflow definition. Each iteration of the foreach loop is internally treated as a separate workflow instance, which scales similarly as any other Maestro workflow based on the step executions (i.e. a sub-graph) defined within the foreach definition block. The execution of sub-graph within a foreach step is delegated to a separate workflow instance. Foreach step then monitors and collects the status of these foreach workflow instances, each managing the execution of a single iteration. For more details, please refer to our previous Maestro blog post.\n\nThe foreach pattern is frequently used to repeatedly run the same jobs with different parameters, such as data backfilling or machine learning model tuning. It would be tedious and time consuming to request users to explicitly define each iteration in the workflow definition (potentially hundreds of thousands of iterations). Additionally, users would need to create new workflows if the foreach range changes, further complicating the process.\n\nConditional Branch Support\n\nThe conditional branch feature allows subsequent steps to run only if specific conditions in the upstream step are met. These conditions are defined using the SEL expression language, which is evaluated at runtime. Combined with other building blocks, users can build powerful workflows, e.g. doing some remediation if the audit check step fails and then run the job again.\n\nSubworkflow Support\n\nThe subworkflow feature allows a workflow step to run another workflow, enabling the sharing of common functions across multiple workflows. This effectively enables \u201cworkflow as a function\u201d and allows users to build a graph of workflows. For example, we have observed complex workflows consisting of hundreds of subworkflows to process data across hundreds tables, where subworkflows are provided by multiple teams.\n\nThese patterns can be combined together to build composite patterns for complex workflow use cases. For instance, we can loop over a set of subworkflows or run nested foreach loops. One example that Maestro users developed is an auto-recovery workflow that utilizes both conditional branch and subworkflow features to handle errors and retry jobs automatically.\n\nIn this example, subworkflow `job1` runs another workflow consisting of extract-transform-load (ETL) and audit jobs. Next, a status check job leverages the Maestro parameter and SEL support to retrieve the status of the previous job. Based on this status, it can decide whether to complete the workflow or to run a recovery job to address any data issues. After resolving the issue, it then executes subworkflow `job2`, which runs the same workflow as subworkflow `job1`.\n\nStep Runtime and Step Parameter\n\nStep Runtime Interface\n\nIn Maestro, we use step runtime to describe a job at execution time. The step runtime interface defines two pieces of information:\n\nA set of basic APIs to control the behavior of a step instance at execution runtime. Some simple data structures to track step runtime state and execution result.\n\nMaestro offers a few step runtime implementations such as foreach step runtime, subworkflow step runtime (mentioned in previous section). Each implementation defines its own logic for start, execute and terminate operations. At runtime, these operations control the way to initialize a step instance, perform the business logic and terminate the execution under certain conditions (i.e. manual intervention by users).\n\nAlso, Maestro step runtime internally keeps track of runtime state as well as the execution result of the step. The runtime state is used to determine the next state transition of the step and tell if it has failed or terminated. The execution result hosts both step artifacts and the timeline of step execution history, which are accessible by subsequent steps.\n\nStep Parameter Merging\n\nTo control step behavior in a dynamic way, Maestro supports both runtime parameters and tags injection in step runtime. This makes a Maestro step more flexible to absorb runtime changes (i.e. overridden parameters) before actually being started. Maestro internally maintains a step parameter map that is initially empty and is updated by merging step parameters in the order below:\n\nDefault General Parameters : Parameters merging starts from default parameters that in general every step should have. For example, workflow_instance_id, step_instance_uuid, step_attempt_id and step_id are required parameters for each maestro step. They are internally reserved by maestro and cannot be passed by users.\n\n: Parameters merging starts from default parameters that in general every step should have. For example, workflow_instance_id, step_instance_uuid, step_attempt_id and step_id are required parameters for each maestro step. They are internally reserved by maestro and cannot be passed by users. Injected Parameters : Maestro then merges injected parameters (if present) into the parameter map. The injected parameters come from step runtime, which are dynamically generated based on step schema. Each type of step can have its own schema with specific parameters associated with this step. The step schema can evolve independently with no need to update Maestro code.\n\n: Maestro then merges injected parameters (if present) into the parameter map. The injected parameters come from step runtime, which are dynamically generated based on step schema. Each type of step can have its own schema with specific parameters associated with this step. The step schema can evolve independently with no need to update Maestro code. Default Typed Parameters : After injecting runtime parameters, Maestro tries to merge default parameters that are related to a specific type of step. For example, foreach step has loop_params and loop_index default parameters which are internally set by maestro and used for foreach step only.\n\n: After injecting runtime parameters, Maestro tries to merge default parameters that are related to a specific type of step. For example, foreach step has loop_params and loop_index default parameters which are internally set by maestro and used for foreach step only. Workflow and Step Info Parameters : These parameters contain information about step and the workflow it belongs to. This can be identity information, i.e. workflow_id and will be merged to step parameter map if present.\n\n: These parameters contain information about step and the workflow it belongs to. This can be identity information, i.e. workflow_id and will be merged to step parameter map if present. Undefined New Parameters : When starting or restarting a maestro workflow instance, users can specify new step parameters that are not present in initial step definition. ParamsManager merges these parameters to ensure they are available at execution time.\n\n: When starting or restarting a maestro workflow instance, users can specify new step parameters that are not present in initial step definition. ParamsManager merges these parameters to ensure they are available at execution time. Step Definition Parameters : These step parameters are defined by users at definition time and get merged if they are not empty.\n\n: These step parameters are defined by users at definition time and get merged if they are not empty. Run and Restart Parameters: When starting or restarting a maestro workflow instance, users can override defined parameters by providing run or restart parameters. These two types of parameters are merged at the end so that step runtime can see the most recent and accurate parameter space.\n\nThe parameters merging logic can be visualized in the diagram below.\n\nStep Dependencies and Signals\n\nSteps in the Maestro execution workflow graph can express execution dependencies using step dependencies. A step dependency specifies the data-related conditions required by a step to start execution. These conditions are usually defined based on signals, which are pieces of messages carrying information such as parameter values and can be published through step outputs or external systems like SNS or Kafka messages.\n\nSignals in Maestro serve both signal trigger pattern and signal dependencies (a publisher-subscriber) pattern. One step can publish an output signal (a sample example) that can unblock the execution of multiple other steps that depend on it. A signal definition includes a list of mapped parameters, allowing Maestro to perform \u201csignal matching\u201d on a subset of fields. Additionally, Maestro supports signal operators like <, >, etc., on signal parameter values.\n\nNetflix has built various abstractions on top of the concept of signals. For instance, a ETL workflow can update a table with data and send signals that unblock steps in downstream workflows dependent on that data. Maestro supports \u201csignal lineage,\u201d which allows users to navigate all historical instances of signals and the workflow steps that match (i.e. publishing or consuming) those signals. Signal triggering guarantees exactly-once execution for the workflow subscribing a signal or a set of joined signals. This approach is efficient, as it conserves resources by only executing the workflow or step when the specified conditions in the signals are met. A signal service is implemented for those advanced abstractions. Please refer to the Maestro blog for further details on it.\n\nBreakpoint\n\nMaestro allows users to set breakpoints on workflow steps, functioning similarly to code-level breakpoints in an IDE. When a workflow instance executes and reaches a step with a breakpoint, that step enters a \u201cpaused\u201d state. This halts the workflow graph\u2019s progression until a user manually resumes from the breakpoint. If multiple instances of a workflow step are paused at a breakpoint, resuming one instance will only affect that specific instance, leaving the others in a paused state. Deleting the breakpoint will cause all paused step instances to resume.\n\nThis feature is particularly useful during the initial development of a workflow, allowing users to inspect step executions and output data. It is also beneficial when running a step multiple times in a \u201cforeach\u201d pattern with various input parameters. Setting a single breakpoint on a step will cause all iterations of the foreach loop to pause at that step for debugging purposes. Additionally, the breakpoint feature allows human intervention during the workflow execution and can also be used for other purposes, e.g. supporting mutating step states while the workflow is running.\n\nTimeline\n\nMaestro includes a step execution timeline, capturing all significant events such as execution state machine changes and the reasoning behind them. This feature is useful for debugging, providing insights into the status of a step. For example, it logs transitions such as \u201cCreated\u201d and \u201cEvaluating params\u201d, etc. An example of a timeline is included here for reference. The implemented step runtimes can add the timeline events into the timeline to surface the execution information to the end users.\n\nRetry Policies\n\nMaestro supports retry policies for steps that reach a terminal state due to failure. Users can specify the number of retries and configure retry policies, including delays between retries and exponential backoff strategies, in addition to fixed interval retries. Maestro distinguishes between two types of retries: \u201cplatform\u201d and \u201cuser.\u201d Platform retries address platform-level errors unrelated to user logic, while user retries are for user-defined conditions. Each type can have its own set of retry policies.\n\nAutomatic retries are beneficial for handling transient errors that can be resolved without user intervention. Maestro provides the flexibility to set retries to zero for non-idempotent steps to avoid retry. This feature ensures that users have control over how retries are managed based on their specific requirements.\n\nAggregated View\n\nBecause a workflow instance can have multiple runs, it is important for users to see an aggregated state of all steps in the workflow instance. Aggregated view is computed by merging base aggregated view with current runs instance step statuses. For example, as you can see on the figure below simulating a simple case, there is a first run, where step1 and step2 succeeded, step3 failed, and step4 and step5 have not started. When the user restarts the run, the run starts from step3 in run 2 with step1 and step2 skipped which succeeded in the previous run. After all steps succeed, the aggregated view shows the run states for all steps.\n\nRollup\n\nRollup provides a high-level summary of a workflow instance, detailing the status of each step and the count of steps in each status. It flattens steps across the current instance and any nested non-inline workflows like subworkflows or foreach steps. For instance, if a successful workflow has three steps, one of which is a subworkflow corresponding to a five-step workflow, the rollup will indicate that seven steps succeeded. Only leaf steps are counted in the rollup, as other steps serve merely as pointers to concrete workflows.\n\nRollup also retains references to any non-successful steps, offering a clear overview of step statuses and facilitating easy navigation to problematic steps, even within nested workflows. The aggregated rollup for a workflow instance is calculated by combining the current run\u2019s runtime data with a base rollup. The current state is derived from the statuses of active steps, including aggregated rollups for foreach and subworkflow steps. The base rollup is established when the workflow instance begins and includes statuses of inline steps (excluding foreach and subworkflows) from the previous run that are not part of the current run.\n\nFor subworkflow steps, the rollup simply reflects the rollup of the subworkflow instance. For foreach steps, the rollup combines the base rollup of the foreach step with the current state rollup. The base is derived from the previous run\u2019s aggregated rollup, excluding the iterations to be restarted in the new run. The current state is periodically updated by aggregating rollups of running iterations until all iterations reach a terminal state.\n\nDue to these processes, the rollup model is eventually consistent. While the figure below illustrates a straightforward example of rollup, the calculations can become complex and recursive, especially with multiple levels of nested foreaches and subworkflows.\n\nMaestro Event Publishing\n\nWhen workflow definition, workflow instance or step instance is changed, Maestro generates an event, processes it internally and publishes the processed event to external system(s). Maestro has both internal and external events. The internal event tracks changes within the life cycle of workflow, workflow instance or step instance. It is published to an internal queue and processed within Maestro. After internal events are processed, some of them will be transformed into external event and sent out to the external queue (i.e. SNS, Kafka). The external event carries maestro status change information for downstream services. The event publishing flow is illustrated in the diagram below:\n\nAs shown in the diagram, the Maestro event processor bridges the two aforementioned Maestro events. It listens on the internal queue to get the published internal events. Within the processor, the internal job event is processed based on its type and gets converted to an external event if needed. The notification publisher at the end emits the external event so that downstream services can consume.\n\nThe downstream services are mostly event-driven. The Maestro event carries the most useful message for downstream services to capture different changes in Maestro. In general, these changes can be classified into two categories: workflow change and instance status change. The workflow change event is associated with actions at workflow level, i.e definition or properties of a workflow has changed. Meanwhile, instance status change tracks status transition on workflow instance or step instance.\n\nGet Started with Maestro\n\nMaestro has been extensively used within Netflix, and today, we are excited to make the Maestro source code publicly available. We hope that the scalability and usability that Maestro offers can expedite workflow development outside Netflix. We invite you to try Maestro, use it within your organization, and contribute to its development.\n\nYou can find the Maestro code repository at github.com/Netflix/maestro. If you have any questions, thoughts, or comments about Maestro, please feel free to create a GitHub issue in the Maestro repository. We are eager to hear from you.\n\nWe are taking workflow orchestration to the next level and constantly solving new problems and challenges, please stay tuned for updates. If you are passionate about solving large scale orchestration problems, please join us.\n\nAcknowledgements\n\nThanks to other Maestro team members, Binbing Hou, Zhuoran Dong, Brittany Truong, Deepak Ramalingam, Moctar Ba, for their contributions to the Maestro project. Thanks to our Product Manager Ashim Pokharel for driving the strategy and requirements. We\u2019d also like to thank Andrew Seier, Romain Cledat, Olek Gorajek, and other stunning colleagues at Netflix for their contributions to the Maestro project. We also thank Prashanth Ramdas, Eva Tse, David Noor, Charles Smith and other leaders of Netflix engineering organizations for their constructive feedback and suggestions on the Maestro project.", "label": 0}
{"title": "tech industry \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/tag/tech-industry/", "content": "This feels like a sister piece to Ed Zitron\u2019s essay Era of the Business Idiots and Mandy Brown\u2019s essay Toolmen. Fair warning, this is a 5000 word post; I\u2019ve been working on this for weeks, pulling together what I\u2019ve learned about generative AI and culture over the past two years, so I hope it is worth your time \ud83d\ude04 Bonus: it doubles as a playlist \ud83c\udfb6\n\n\u201c\u2018Real power\u2019 is achieved when a technology \u2018[leaves] mythology and [enters] banality,'\u201d Marion Fourcade and Kieran Healy quote Vincent Mosco in The Ordinal Society. We\u2019ve had the mythology stage \u2014 the world tour with grandiose prophecies of imminent AGI \u2014 but now the race to normalize generative AI* is on: tech corporations are attempting to inure people to generative AI, an expression of the Business Borg aesthetic that currently carries a negative stigma outside of tech.\n\n*(My rule of thumb: if something is described as AI, it\u2019s probably predatory and/or bullshit; if it\u2019s described as machine learning, it probably does something useful. Not always true but a helpful predictor.)\n\nIn general, people like what we recognize better than what we don\u2019t \u2014 we prefer cultural works we can categorize to the unfamiliar and undefinable \u2014 and we are facing an inescapable shock-and-awe barrage of genAI graphics across the web to inundate our synapses with uncanny synthetic renderings.\n\nCurrently, generative AI is shunned by many artists and writers, the traditional arbiters of good taste and culture, because it has been developed through the theft of their labor. But tech CEOs stand to make (even bigger) fortunes if they can convince people that genAI doesn\u2019t signify bad taste, or make it seem like an irrevocable fact of life, like spam emails and text scammers. It\u2019s being deployed upon us with the same lockstep corporate solidarity that forced us to pay fees for checked luggage on flights (younger folks, before 2008 your bag used to be included with your ticket! Stowing your carry-on wasn\u2019t a competitive sport back in the day.).", "label": 1}
{"title": "Round-Up of October 2023 IndieWeb Carnival", "url": "https://lifeofpablo.com/blog/indieweb-carnival-selfcare-round-up", "content": "October is a wrap! \ud83c\udf83 I hosted October's IndieWeb Carnival on self-care. Thank you to everyone who participated in this month's IndieWeb Carnival topic. I appreciate all the responses I recieved. I recieved so many perspectives on the topic. I chose this theme as this is something I am working on improving for myself.\n\nCarnival Stats Responses 10 Happiness \u221e\n\nResponses\n\nSara Jak\u0161a - Using 4 E of Cognition to Conceptualise Self-Care\n\nI really enjoyed the way way you structured your reponse using the 4 E of cognition: embodied, embedded, extended, and enactivistic. I really appreciate the approaches using your educational background to explain. It has really helped me see things in a different perspective. I agree that self-care is \"a process to adapt my life\". One must adapt. You're right, I can't ignore self-care because whether or not I felt I was ignoring it. I was doing it whether I admit it or not. I now see it as having a low battery. It had some juice. I realize now I was also scared of myself, nervous of outcomes, and more. I realized many things. I ignored my ehealth. I really enjoy how you ended the response, \"do the self-care for the reason of your better wellbeing.\" I am doing this for myself.Thank you for being so straight foward and not beating around the bush.\n\nGrigor Malo - Should we care about self-care?\n\nI really enjoyed \"[your] answer to the prompt\" and the approach. I would agree with you some self-care/help guides do take the wrong approach. It often seems like how to appear to be fine on the outside but not actually on the inside. It's been hard in the pastto find self-care/help material related to my cultural background or for individuals who live in a multiculural life. There is no one size fits all. Hyper-individualism or individualism has been a conflicting thing for me. Why do we need to isolate ourself when things can also be experienced with other people as well. I also agree that action at the end of the day starts with the individual. \"Caring about your self is a fundamentally selfish endeavour? Not really, or at least not if practiced properly.\" That is a beautifully written phrase. I share many of the same thoughts on wellness industry and the corporate wellness. You really put in the persepective how much we buy into capitalism to feel better about ourselves.\n\nFrank Meeuwsen - Global PSC Awareness Day and selfcare\n\nFirst, I like the multilingual approach created for this post. Thank you for sharing about Primary Sclerosing Cholangitis and your experience living with this disease. I really enjoy the approach you take. With self-care you learn to see things in a positive light. I agree with \"to rest more and consciously slow down.\" This has been something I've been learning as well. It became unhealth and bad for mental health. Strecthing yourself thin is not feasible anymore. I appreciate you aspect on learning to take control of a situation. \"I know I can affect it. I know now that I can change it.\" This resonates with me so much!\n\nJeremy Cherfasi - Self-care Success\n\nI really your approach of enjoying the little and big things in life. I can agree that traveling has always been a way of self-care. A way to disconnect physically from the place one is based out of to a different environment. Self-care to you seems that you truly enjoy life and you genuinely enjoy the things you do such as biking, baking, etc. You wrote having \"a sense of purpose and of mastery\" is something I resonate with. I'm still trying to find my sense of purpose. I'm learning not to stretch myself thin an focus on the things I love. I love who you wish the drivers to take care of themselves\n\nTracy Durnell - Extending my understanding of self-care\n\nRight off the bat, your approach on the \"bigger whole\" is a like a piece of the puzzle or a train network. If any of it is missing, it simply doesn't work well. Community is so important. Unforntunately we live in a very individualistic society and we need community more than ever. \"So much of self-care is treating the symptoms rather than the source, because the source is systemic.\" I didn't think about it this way. I recently started going to therapy and I thought the goal was to get to the get to the root of the problem while hitting the symptoms like a ping pong. Thank you for sharing the books you mentioned in your response. There are so many self-care books it's hard to find where to start. I'm trying to get to the right path. Oh man, this is hits hard: \"Showing up for our friends better, sometimes that means showing up for ourselves, and sometimes it\u2019s shared.\" This makes me reflect, when I wasn't for there for myself, I wasn't there for my community in this case, my friends. If I wasn't Self-care shows that you can care for yourself and for others. This has been a year of improvement but so much I still have to reach. Thank you for sharing this perspective on community and individuals. This opened my eyes in so many ways.\n\nAnthony Ciccarello - Creating space to feel my emotions\n\nI can relate so much with Anthony's response. Right away, the second sentence, \"My natural inclination is to bury my own emotions to protect my relationships\" is very similar to what I've felt over the years. All at my expense. Bottling it all up just wears a peson down. I've been very hesitant about sharing that I've been seeing a therapist and how I've started to open up a lot more to people on what's happening in life. I really appreciate your emphasis and approach on the word, space. A space can have so many purposes. I've been learning how to use spaces inside and outside as more than being inside and outside. The space one uses can truly affect the outcomes of self-care. Spaces to think, spaces, to move, spaces to unwind, and a space to grow demonstrate healthy, safe, self-reflection and so much more.\n\nJo - Things to do offline\n\nJo does a great job at \"kill[ing] two birds with one stone\" by addressing the self-care prompt and a post I wrote on books. The response is amazing. I really like perspective on reading books. I don't discourage reading not matter the format. We live in a world where we have so many different mediums of reading. Reading away from anything that has a microchip of any kind is for me a form of reset. Just like you mentioned, sometimes the only way to get access to specific content is via digital means. Reading physical really helps reground and reconnect with the digital world. Having solo time and hanging out with people is great but a balance is necessary. You defintely don't want to go to far on the solo end or not having solo time and spending too much time with others. It's a preference in many aspects. I enjoyed your self-care activity list! I get lost with looking at vinyls at the record shop.\n\nCJ - Self-Care and Routine\n\nCJ has some great points on find things at are less \"productive-feeling.\" For the longest time, doing things like watching Netflix or go to the movies, I felt that I was wasting time and I pushed these activities away. It's been hard just to disconnect from my obligations and I've come to the realizations that I need to sit my ass down and just relax. It's not necessary to be always engaged in things that are \"productive.\" It's productive to let your mind engage in other things!\n\nAlex Sirac - Improving our relationship to news\n\nAlex has a written a great post on self-care by focusing on news consumption. Right away you wrote, \"Not only do I spend too much time on the Internet \u2013 I spend way too much time consuming content and news.\" You took the words right out of my mouth. News isn't limited to listening to reading it online, or watching it on the television (or historically radio). Now the news in the 21st comes in so many different mediums such as podcasts. I've seen an increase of this negative news. It discourages me to use any form of social media, even decentralized social media. I want to ignore but it seems hard not to even if you don't click links. \"Curating a me-friendly news experience\" by \"find[ing] the things that bring you joy and give your brain a break from the hate.\" It's important to step away from even if it's for a little while. We can't let it consume us to the core. It will make use dark and bitter. I catch myself becoming angry. You make a great point on finding the right sources and filtering things out by using an RSS reader. Even I as an avid New York Times and leftist news person, have to turn off my computer and throw my phone across the room to reattach myself to the world around me. It's got to the point where I have to go into my terminal and turn off the Docker container and turn of my RSS reader. There has to be a better way for everything. Support local journalism because we need to know what's happening in our hometowns and the places we live. We need to now what's happening in our communities to make a difference and become aware what's around us.\n\nJames G - Self Care\n\nJames wrote beautiful response regarding self-care. It is complicated! This has been something I've had to learn the hardway, \"I know, deep down, that feeling is unsustainable: one can't be creative every day.\" We want to create more every damn day. It's not always feasible. I agree with the following, \"I don't need to create new things all the time.\" The way I see it, is work on the things you are working on and give it your full attention in a reasonable manner. It's great to find comfort in things we are familiar with. It allows us to be vulnerable.\n\nLate or Missing Reponses\n\nIf you still would like to write a response for October, I'd be happy to recieve more. If you sent me a response and I missed it, please email me directly at pablo@lifeofpablo.com. I'll update this post.\n\nNovember IndieWeb Carnival\n\nNow, I will pass torche to Alex Sirac. Alex is writing on community and belonging for the month of November. C'est parti !\n\nInterested in participating and/or hosting an IndieWeb Carnival, please click here! Remember, anyone and everyone can participate.\n\nThis post has been syndicated to IndieWeb News.", "label": 1}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2017-03", "content": "en\n\nBeing away from home these last few months have been great. I enjoy the freedoms, living independently and eating good food whenever I want. Life has treated me good here. Until more recently, a certain stage of being away finally starts. Lately, homesickness has really started to kick in. I\u2019ve really started to miss my friends, family and things I really took for granted back home. It really is bad when I am craving my mom\u2019s homemade food. (Mexican Food.) Midterms and all the studying is really killing my vibe at this point of the semester. My program is not your typical study abroad program that you go away for 3 months (not to mention that it doesn\u2019t even apply to your major) on a semi vacation. I actually have to try real hard as my french studies here in Strasbourg France will help me graduate next year. Enough bashing.\n\nWith all this, one just feels that they are slowly losing control. Sure I am enjoying myself out here in France but still there is this emptiness of something that I have been away from for far too long. Hopefully, I hope some sort of enlightenment will come bring me back to life. Then It hit me, I would see my friends in just a few weeks. I honestly couldn\u2019t believe in such short time I would be right in front of them.\n\nSo I would be meeting up my good friend Sammy and soon-to-be friend Mitch. They have never been to Europe before, so I figured why not show them around Paris, a city that I know decently and use my French to get to Point A to Point B and of course with the help of good ol\u2019 Google Maps. Paris- Weekend 1\n\nThese guys had a hell of a time getting to Paris during the 24 hours before arrival. I\u2019ll let them explain that one.\n\nSo I picked the guys up from Charles De Gaulle Aiport. I still feel bad on making you guys wait for me. :/ I ran up to my dude Sammy and gave him a bro hug! It has been way to long since I have seen that dude! I also got to meet Mitch and got to know him more through out the weekend. Cool Dude! So I showed them around Paris. To be honest, Paris is beautiful with history written all around. As with any major city, Paris is no exception, it is dirty and smelly. I really hope that the guys did not have too high of hopes for the \u201cCity of Love.\u201d Paris is overrated in my opinion but why not take advantage of leaving Strasbourg for a few days.\n\nSo we went to various monuments such as The Louvre, Arc of Triumph, Versailles and so much more. Many of the places were breath taking. The Eiffel Tour is always a good place to scope out as well as walking along the Seine River. Getting around Paris is no piece of cake but we managed. Never have I ever gone up so many round staircases. Climbing the Cathedral of Notre Dame was such a struggle. Whenever you are in Europe, walking is really the way to get around.\n\nNight life in Paris is pretty popping! We went to a few clubs in the Grand Boulevard District! This district did not disappoint. I wish we could have checked out a few other ones. One of my favorite places was the Irish Pub. Music, dancing, girls and drinks. Man was that a fun night!\n\nWe ate some good food throughout Paris. The guys had their first legit French Croissants and baguette sandwich. It\u2019s totally acceptable to have a sandwich on you at all times. I also had them try one of my favorites, a kebab sandwich. Man are they delicious! Sammy had actual ramen noodles for the first time ever! Overall we pretty much stuffed our faces all weekend.\n\nBruxelles- Weekend 2\n\nAh Bruxelles!! The place known for its deliciouos fries and waffles. You have no idea how much I stuffed my face this weekend. Just writing this post makes my mouth water. Was eating all this food this worth it? You bet it was!! When I arrived in Bruxelles, it was gloomy and misting. Not the way I wanted to start the weekend. The weather stayed the same all weekend. That did not stop the fun.\n\nSpending the weekend in Brussels was totally worth the waffles! The sugar definitely made me bounce off the walls. #belgium #worldtravel #unkstudyabroad A post shared by Pablo Morales (@pablo.morales1) on Mar 19, 2017 at 9:55am PDT\n\nWe checked out many popular attractions throughout the city. There was so much we saw, I started to lose my mind a bit. One of the coolest places I saw was a \u201cWorld Shop.\u201d They had souvenirs and artsy things from all around the world. I was so intrigued with the place that I ended up buying a few things from there.\n\nThe other purpose of coming to Bruxelles was to meet up with our friend Kellie and Sammy\u2019s brother, Louis. It was good to see some other friends from back home. I missed those two! Kellie is just a keeper, to keep a long story short. These two are actually on Spring Break right now. I am so jealous of you guys spending your holiday in Malta! I hope you enjoy every second of it!\n\nBruxelles treated everyone well!\n\nYour browser does not support the video tag.\n\nThese last few weekends have been pretty awesome! It was good to see a few friends from back home. I couldn\u2019t of asked for a better time. Traveling becomes more exciting when you are hanging out with people you know and love. I can\u2019t wait for the next adventure with them! So, is it sprangggg breakkk yet? I wish!", "label": 1}
{"title": "Investigation of a Workbench UI Latency Issue", "url": "https://netflixtechblog.com/investigation-of-a-workbench-ui-latency-issue-faa017b4653d?source=collection_home---4------12-----------------------", "content": "Investigation of a Workbench UI Latency Issue Netflix Technology Blog 12 min read \u00b7 Oct 14, 2024 -- 4 Listen Share\n\nBy: Hechao Li and Marcelo Mayworm\n\nWith special thanks to our stunning colleagues Amer Ather, Itay Dafna, Luca Pozzi, Matheus Le\u00e3o, and Ye Ji.\n\nOverview\n\nAt Netflix, the Analytics and Developer Experience organization, part of the Data Platform, offers a product called Workbench. Workbench is a remote development workspace based on Titus that allows data practitioners to work with big data and machine learning use cases at scale. A common use case for Workbench is running JupyterLab Notebooks.\n\nRecently, several users reported that their JupyterLab UI becomes slow and unresponsive when running certain notebooks. This document details the intriguing process of debugging this issue, all the way from the UI down to the Linux kernel.\n\nSymptom\n\nMachine Learning engineer Luca Pozzi reported to our Data Platform team that their JupyterLab UI on their workbench becomes slow and unresponsive when running some of their Notebooks. Restarting the ipykernel process, which runs the Notebook, might temporarily alleviate the problem, but the frustration persists as more notebooks are run.\n\nQuantify the Slowness\n\nWhile we observed the issue firsthand, the term \u201cUI being slow\u201d is subjective and difficult to measure. To investigate this issue, we needed a quantitative analysis of the slowness.\n\nItay Dafna devised an effective and simple method to quantify the UI slowness. Specifically, we opened a terminal via JupyterLab and held down a key (e.g., \u201cj\u201d) for 15 seconds while running the user\u2019s notebook. The input to stdin is sent to the backend (i.e., JupyterLab) via a WebSocket, and the output to stdout is sent back from the backend and displayed on the UI. We then exported the .har file recording all communications from the browser and loaded it into a Notebook for analysis.\n\nUsing this approach, we observed latencies ranging from 1 to 10 seconds, averaging 7.4 seconds.\n\nBlame The Notebook\n\nNow that we have an objective metric for the slowness, let\u2019s officially start our investigation. If you have read the symptom carefully, you must have noticed that the slowness only occurs when the user runs certain notebooks but not others.\n\nTherefore, the first step is scrutinizing the specific Notebook experiencing the issue. Why does the UI always slow down after running this particular Notebook? Naturally, you would think that there must be something wrong with the code running in it.\n\nUpon closely examining the user\u2019s Notebook, we noticed a library called pystan , which provides Python bindings to a native C++ library called stan, looked suspicious. Specifically, pystan uses asyncio. However, because there is already an existing asyncio event loop running in the Notebook process and asyncio cannot be nested by design, in order for pystan to work, the authors of pystan recommend injecting pystan into the existing event loop by using a package called nest_asyncio, a library that became unmaintained because the author unfortunately passed away.\n\nGiven this seemingly hacky usage, we naturally suspected that the events injected by pystan into the event loop were blocking the handling of the WebSocket messages used to communicate with the JupyterLab UI. This reasoning sounds very plausible. However, the user claimed that there were cases when a Notebook not using pystan runs, the UI also became slow.\n\nMoreover, after several rounds of discussion with ChatGPT, we learned more about the architecture and realized that, in theory, the usage of pystan and nest_asyncio should not cause the slowness in handling the UI WebSocket for the following reasons:\n\nEven though pystan uses nest_asyncio to inject itself into the main event loop, the Notebook runs on a child process (i.e., the ipykernel process) of the jupyter-lab server process, which means the main event loop being injected by pystan is that of the ipykernel process, not the jupyter-server process. Therefore, even if pystan blocks the event loop, it shouldn\u2019t impact the jupyter-lab main event loop that is used for UI websocket communication. See the diagram below:\n\nIn other words, pystan events are injected to the event loop B in this diagram instead of event loop A. So, it shouldn\u2019t block the UI WebSocket events.\n\nYou might also think that because event loop A handles both the WebSocket events from the UI and the ZeroMQ socket events from the ipykernel process, a high volume of ZeroMQ events generated by the notebook could block the WebSocket. However, when we captured packets on the ZeroMQ socket while reproducing the issue, we didn\u2019t observe heavy traffic on this socket that could cause such blocking.\n\nA stronger piece of evidence to rule out pystan was that we were ultimately able to reproduce the issue even without it, which I\u2019ll dive into later.\n\nBlame Noisy Neighbors\n\nThe Workbench instance runs as a Titus container. To efficiently utilize our compute resources, Titus employs a CPU oversubscription feature, meaning the combined virtual CPUs allocated to containers exceed the number of available physical CPUs on a Titus agent. If a container is unfortunate enough to be scheduled alongside other \u201cnoisy\u201d containers \u2014 those that consume a lot of CPU resources \u2014 it could suffer from CPU deficiency.\n\nHowever, after examining the CPU utilization of neighboring containers on the same Titus agent as the Workbench instance, as well as the overall CPU utilization of the Titus agent, we quickly ruled out this hypothesis. Using the top command on the Workbench, we observed that when running the Notebook, the Workbench instance uses only 4 out of the 64 CPUs allocated to it. Simply put, this workload is not CPU-bound.\n\nBlame The Network\n\nThe next theory was that the network between the web browser UI (on the laptop) and the JupyterLab server was slow. To investigate, we captured all the packets between the laptop and the server while running the Notebook and continuously pressing \u2018j\u2019 in the terminal.\n\nWhen the UI experienced delays, we observed a 5-second pause in packet transmission from server port 8888 to the laptop. Meanwhile, traffic from other ports, such as port 22 for SSH, remained unaffected. This led us to conclude that the pause was caused by the application running on port 8888 (i.e., the JupyterLab process) rather than the network.\n\nThe Minimal Reproduction\n\nAs previously mentioned, another strong piece of evidence proving the innocence of pystan was that we could reproduce the issue without it. By gradually stripping down the \u201cbad\u201d Notebook, we eventually arrived at a minimal snippet of code that reproduces the issue without any third-party dependencies or complex logic:\n\nimport time\n\nimport os\n\nfrom multiprocessing import Process\n\n\n\nN = os.cpu_count()\n\n\n\ndef launch_worker(worker_id):\n\ntime.sleep(60)\n\n\n\nif __name__ == '__main__':\n\nwith open('/root/2GB_file', 'r') as file:\n\ndata = file.read()\n\nprocesses = []\n\nfor i in range(N):\n\np = Process(target=launch_worker, args=(i,))\n\nprocesses.append(p)\n\np.start()\n\n\n\nfor p in processes:\n\np.join()\n\nThe code does only two things:\n\nRead a 2GB file into memory (the Workbench instance has 480G memory in total so this memory usage is almost negligible). Start N processes where N is the number of CPUs. The N processes do nothing but sleep.\n\nThere is no doubt that this is the most silly piece of code I\u2019ve ever written. It is neither CPU bound nor memory bound. Yet it can cause the JupyterLab UI to stall for as many as 10 seconds!\n\nQuestions\n\nThere are a couple of interesting observations that raise several questions:\n\nWe noticed that both steps are required in order to reproduce the issue . If you don\u2019t read the 2GB file (that is not even used!), the issue is not reproducible. Why using 2GB out of 480GB memory could impact the performance?\n\n. If you don\u2019t read the 2GB file (that is not even used!), the issue is not reproducible. When the UI delay occurs, the jupyter-lab process CPU utilization spikes to 100% , hinting at contention on the single-threaded event loop in this process (event loop A in the diagram before). What does the jupyter-lab process need the CPU for, given that it is not the process that runs the Notebook?\n\n, hinting at contention on the single-threaded event loop in this process (event loop A in the diagram before). The code runs in a Notebook, which means it runs in the ipykernel process, that is a child process of the jupyter-lab process. How can anything that happens in a child process cause the parent process to have CPU contention?\n\nThe workbench has 64CPUs. But when we printed os.cpu_count(), the output was 96. That means the code starts more processes than the number of CPUs. Why is that?\n\nLet\u2019s answer the last question first. In fact, if you run lscpu and nproc commands inside a Titus container, you will also see different results \u2014 the former gives you 96, which is the number of physical CPUs on the Titus agent, whereas the latter gives you 64, which is the number of virtual CPUs allocated to the container. This discrepancy is due to the lack of a \u201cCPU namespace\u201d in the Linux kernel, causing the number of physical CPUs to be leaked to the container when calling certain functions to get the CPU count. The assumption here is that Python os.cpu_count() uses the same function as the lscpu command, causing it to get the CPU count of the host instead of the container. Python 3.13 has a new call that can be used to get the accurate CPU count, but it\u2019s not GA\u2019ed yet.\n\nIt will be proven later that this inaccurate number of CPUs can be a contributing factor to the slowness.\n\nMore Clues\n\nNext, we used py-spy to do a profiling of the jupyter-lab process. Note that we profiled the parent jupyter-lab process, not the ipykernel child process that runs the reproduction code. The profiling result is as follows:\n\nAs one can see, a lot of CPU time (89%!!) is spent on a function called __parse_smaps_rollup. In comparison, the terminal handler used only 0.47% CPU time. From the stack trace, we see that this function is inside the event loop A, so it can definitely cause the UI WebSocket events to be delayed.\n\nThe stack trace also shows that this function is ultimately called by a function used by a Jupyter lab extension called jupyter_resource_usage. We then disabled this extension and restarted the jupyter-lab process. As you may have guessed, we could no longer reproduce the slowness!\n\nBut our puzzle is not solved yet. Why does this extension cause the UI to slow down? Let\u2019s keep digging.\n\nRoot Cause Analysis\n\nFrom the name of the extension and the names of the other functions it calls, we can infer that this extension is used to get resources such as CPU and memory usage information. Examining the code, we see that this function call stack is triggered when an API endpoint /metrics/v1 is called from the UI. The UI apparently calls this function periodically, according to the network traffic tab in Chrome\u2019s Developer Tools.\n\nNow let\u2019s look at the implementation starting from the call get(jupter_resource_usage/api.py:42) . The full code is here and the key lines are shown below:\n\ncur_process = psutil.Process()\n\nall_processes = [cur_process] + cur_process.children(recursive=True)\n\n\n\nfor p in all_processes:\n\ninfo = p.memory_full_info()\n\nBasically, it gets all children processes of the jupyter-lab process recursively, including both the ipykernel Notebook process and all processes created by the Notebook. Obviously, the cost of this function is linear to the number of all children processes. In the reproduction code, we create 96 processes. So here we will have at least 96 (sleep processes) + 1 (ipykernel process) + 1 (jupyter-lab process) = 98 processes when it should actually be 64 (allocated CPUs) + 1 (ipykernel process) + 1 (jupyter-lab process) = 66 processes, because the number of CPUs allocated to the container is, in fact, 64.\n\nThis is truly ironic. The more CPUs we have, the slower we are!\n\nAt this point, we have answered one question: Why does starting many grandchildren processes in the child process cause the parent process to be slow? Because the parent process runs a function that\u2019s linear to the number all children process recursively.\n\nHowever, this solves only half of the puzzle. If you remember the previous analysis, starting many child processes ALONE doesn\u2019t reproduce the issue. If we don\u2019t read the 2GB file, even if we create 2x more processes, we can\u2019t reproduce the slowness.\n\nSo now we must answer the next question: Why does reading a 2GB file in the child process affect the parent process performance, especially when the workbench has as much as 480GB memory in total?\n\nTo answer this question, let\u2019s look closely at the function __parse_smaps_rollup. As the name implies, this function parses the file /proc/<pid>/smaps_rollup.\n\ndef _parse_smaps_rollup(self):\n\nuss = pss = swap = 0\n\nwith open_binary(\"{}/{}/smaps_rollup\".format(self._procfs_path, self.pid)) as f:\n\nfor line in f:\n\nif line.startswith(b\u201dPrivate_\u201d):\n\n# Private_Clean, Private_Dirty, Private_Hugetlb\n\ns uss += int(line.split()[1]) * 1024\n\nelif line.startswith(b\u201dPss:\u201d):\n\npss = int(line.split()[1]) * 1024\n\nelif line.startswith(b\u201dSwap:\u201d):\n\nswap = int(line.split()[1]) * 1024\n\nreturn (uss, pss, swap)\n\nNaturally, you might think that when memory usage increases, this file becomes larger in size, causing the function to take longer to parse. Unfortunately, this is not the answer because:\n\nFirst, the number of lines in this file is constant for all processes .\n\n. Second, this is a special file in the /proc filesystem, which should be seen as a kernel interface instead of a regular file on disk. In other words, I/O operations of this file are handled by the kernel rather than disk.\n\nThis file was introduced in this commit in 2017, with the purpose of improving the performance of user programs that determine aggregate memory statistics. Let\u2019s first focus on the handler of open syscall on this /proc/<pid>/smaps_rollup.\n\nFollowing through the single_open function, we will find that it uses the function show_smaps_rollup for the show operation, which can translate to the read system call on the file. Next, we look at the show_smaps_rollup implementation. You will notice a do-while loop that is linear to the virtual memory area.\n\nstatic int show_smaps_rollup(struct seq_file *m, void *v) {\n\n\u2026\n\nvma_start = vma->vm_start;\n\ndo {\n\nsmap_gather_stats(vma, &mss, 0);\n\nlast_vma_end = vma->vm_end;\n\n\u2026\n\n} for_each_vma(vmi, vma);\n\n\u2026\n\n}\n\nThis perfectly explains why the function gets slower when a 2GB file is read into memory. Because the handler of reading the smaps_rollup file now takes longer to run the while loop. Basically, even though smaps_rollup already improved the performance of getting memory information compared to the old method of parsing the /proc/<pid>/smaps file, it is still linear to the virtual memory used.\n\nMore Quantitative Analysis\n\nEven though at this point the puzzle is solved, let\u2019s conduct a more quantitative analysis. How much is the time difference when reading the smaps_rollup file with small versus large virtual memory utilization? Let\u2019s write some simple benchmark code like below:\n\nimport os\n\n\n\ndef read_smaps_rollup(pid):\n\nwith open(\"/proc/{}/smaps_rollup\".format(pid), \"rb\") as f:\n\nfor line in f:\n\npass\n\n\n\nif __name__ == \u201c__main__\u201d:\n\npid = os.getpid()\n\n\n\nread_smaps_rollup(pid)\n\n\n\nwith open(\u201c/root/2G_file\u201d, \u201crb\u201d) as f:\n\ndata = f.read()\n\n\n\nread_smaps_rollup(pid)\n\nThis program performs the following steps:\n\nReads the smaps_rollup file of the current process. Reads a 2GB file into memory. Repeats step 1.\n\nWe then use strace to find the accurate time of reading the smaps_rollup file.\n\n$ sudo strace -T -e trace=openat,read python3 benchmark.py 2>&1 | grep \u201csmaps_rollup\u201d -A 1\n\n\n\nopenat(AT_FDCWD, \u201c/proc/3107492/smaps_rollup\u201d, O_RDONLY|O_CLOEXEC) = 3 <0.000023>\n\nread(3, \u201c560b42ed4000\u20137ffdadcef000 \u2014 -p 0\u201d\u2026, 1024) = 670 <0.000259>\n\n...\n\nopenat(AT_FDCWD, \u201c/proc/3107492/smaps_rollup\u201d, O_RDONLY|O_CLOEXEC) = 3 <0.000029>\n\nread(3, \u201c560b42ed4000\u20137ffdadcef000 \u2014 -p 0\u201d\u2026, 1024) = 670 <0.027698>\n\nAs you can see, both times, the read syscall returned 670, meaning the file size remained the same at 670 bytes. However, the time it took the second time (i.e., 0.027698 seconds) is 100x the time it took the first time (i.e., 0.000259 seconds)! This means that if there are 98 processes, the time spent on reading this file alone will be 98 * 0.027698 = 2.7 seconds! Such a delay can significantly affect the UI experience.\n\nSolution\n\nThis extension is used to display the CPU and memory usage of the notebook process on the bar at the bottom of the Notebook:\n\nWe confirmed with the user that disabling the jupyter-resource-usage extension meets their requirements for UI responsiveness, and that this extension is not critical to their use case. Therefore, we provided a way for them to disable the extension.\n\nSummary\n\nThis was such a challenging issue that required debugging from the UI all the way down to the Linux kernel. It is fascinating that the problem is linear to both the number of CPUs and the virtual memory size \u2014 two dimensions that are generally viewed separately.\n\nOverall, we hope you enjoyed the irony of:\n\nThe extension used to monitor CPU usage causing CPU contention. An interesting case where the more CPUs you have, the slower you get!\n\nIf you\u2019re excited by tackling such technical challenges and have the opportunity to solve complex technical challenges and drive innovation, consider joining our Data Platform teams. Be part of shaping the future of Data Security and Infrastructure, Data Developer Experience, Analytics Infrastructure and Enablement, and more. Explore the impact you can make with us!", "label": 0}
{"title": "How enterprise engineering teams can successfully adopt AI", "url": "https://github.com/resources/whitepapers/how-enterprise-engineering-teams-can-successfully-adopt-ai", "content": "Afghanistan ( +93 ) \u00c5land ( +358 ) Albania ( +355 ) Algeria ( +213 ) American Samoa ( +1 ) Andorra ( +376 ) Angola ( +244 ) Anguilla ( +1 ) Antigua and Barbuda ( +1 ) Argentina ( +54 ) Armenia ( +374 ) Aruba ( +297 ) Australia ( +61 ) Austria ( +43 ) Azerbaijan ( +994 ) Bahamas ( +1 ) Bahrain ( +973 ) Bangladesh ( +880 ) Barbados ( +1 ) Belarus ( +375 ) Belgium ( +32 ) Belize ( +501 ) Benin ( +229 ) Bermuda ( +1 ) Bhutan ( +975 ) Bolivia ( +591 ) Bonaire, Sint Eustatius and Saba ( +599 ) Bosnia and Herzegovina ( +387 ) Botswana ( +267 ) Brazil ( +55 ) British Indian Ocean Territory ( +246 ) Brunei Darussalam ( +673 ) Bulgaria ( +359 ) Burkina Faso ( +226 ) Burundi ( +257 ) Cambodia ( +855 ) Cameroon ( +237 ) Canada ( +1 ) Cape Verde ( +238 ) Cayman Islands ( +1 ) Central African Republic ( +236 ) Chad ( +235 ) Chile ( +56 ) China ( +86 ) Christmas Island ( +61 ) Cocos (Keeling) Islands ( +61 ) Colombia ( +57 ) Comoros ( +269 ) Congo (Brazzaville) ( +242 ) Congo (Kinshasa) ( +243 ) Cook Islands ( +682 ) Costa Rica ( +506 ) C\u00f4te d'Ivoire ( +225 ) Croatia ( +385 ) Cura\u00e7ao ( +599 ) Cyprus ( +357 ) Czech Republic ( +420 ) Denmark ( +45 ) Djibouti ( +253 ) Dominica ( +1 ) Dominican Republic ( +1 ) Ecuador ( +593 ) Egypt ( +20 ) El Salvador ( +503 ) Equatorial Guinea ( +240 ) Eritrea ( +291 ) Estonia ( +372 ) Ethiopia ( +251 ) Falkland Islands ( +500 ) Faroe Islands ( +298 ) Fiji ( +679 ) Finland ( +358 ) France ( +33 ) French Guiana ( +594 ) French Polynesia ( +689 ) Gabon ( +241 ) Gambia ( +220 ) Georgia ( +995 ) Germany ( +49 ) Ghana ( +233 ) Gibraltar ( +350 ) Greece ( +30 ) Greenland ( +299 ) Grenada ( +1 ) Guadeloupe ( +590 ) Guam ( +1 ) Guatemala ( +502 ) Guernsey ( +44 ) Guinea ( +224 ) Guinea-Bissau ( +245 ) Guyana ( +592 ) Haiti ( +509 ) Honduras ( +504 ) Hong Kong ( +852 ) Hungary ( +36 ) Iceland ( +354 ) India ( +91 ) Indonesia ( +62 ) Iran ( +98 ) Iraq ( +964 ) Ireland ( +353 ) Isle of Man ( +44 ) Israel ( +972 ) Italy ( +39 ) Jamaica ( +1 ) Japan ( +81 ) Jersey ( +44 ) Jordan ( +962 ) Kazakhstan ( +7 ) Kenya ( +254 ) Kiribati ( +686 ) Korea, South ( +82 ) Kuwait ( +965 ) Kyrgyzstan ( +996 ) Laos ( +856 ) Latvia ( +371 ) Lebanon ( +961 ) Lesotho ( +266 ) Liberia ( +231 ) Libya ( +218 ) Liechtenstein ( +423 ) Lithuania ( +370 ) Luxembourg ( +352 ) Macau ( +853 ) Macedonia ( +389 ) Madagascar ( +261 ) Malawi ( +265 ) Malaysia ( +60 ) Maldives ( +960 ) Mali ( +223 ) Malta ( +356 ) Marshall Islands ( +692 ) Martinique ( +596 ) Mauritania ( +222 ) Mauritius ( +230 ) Mayotte ( +262 ) Mexico ( +52 ) Micronesia ( +691 ) Moldova ( +373 ) Monaco ( +377 ) Mongolia ( +976 ) Montenegro ( +382 ) Montserrat ( +1 ) Morocco ( +212 ) Mozambique ( +258 ) Myanmar ( +95 ) Namibia ( +264 ) Nauru ( +674 ) Nepal ( +977 ) Netherlands ( +31 ) New Caledonia ( +687 ) New Zealand ( +64 ) Nicaragua ( +505 ) Niger ( +227 ) Nigeria ( +234 ) Niue ( +683 ) Norfolk Island ( +672 ) Northern Mariana Islands ( +1 ) Norway ( +47 ) Oman ( +968 ) Pakistan ( +92 ) Palau ( +680 ) Palestine ( +970 ) Panama ( +507 ) Papua New Guinea ( +675 ) Paraguay ( +595 ) Peru ( +51 ) Philippines ( +63 ) Poland ( +48 ) Portugal ( +351 ) Puerto Rico ( +1 ) Qatar ( +974 ) Reunion ( +262 ) Romania ( +40 ) Rwanda ( +250 ) Saint Barth\u00e9lemy ( +590 ) Saint Helena ( +290 ) Saint Kitts and Nevis ( +1 ) Saint Lucia ( +1 ) Saint Martin (French part) ( +590 ) Saint Pierre and Miquelon ( +508 ) Saint Vincent and the Grenadines ( +1 ) Samoa ( +685 ) San Marino ( +378 ) Sao Tome and Principe ( +239 ) Saudi Arabia ( +966 ) Senegal ( +221 ) Serbia ( +381 ) Seychelles ( +248 ) Sierra Leone ( +232 ) Singapore ( +65 ) Sint Maarten (Dutch part) ( +1 ) Slovakia ( +421 ) Slovenia ( +386 ) Solomon Islands ( +677 ) Somalia ( +252 ) South Africa ( +27 ) South Sudan ( +211 ) Spain ( +34 ) Sri Lanka ( +94 ) Sudan ( +249 ) Suriname ( +597 ) Svalbard and Jan Mayen Islands ( +47 ) Swaziland ( +268 ) Sweden ( +46 ) Switzerland ( +41 ) Taiwan ( +886 ) Tajikistan ( +992 ) Tanzania ( +255 ) Thailand ( +66 ) Timor-Leste ( +670 ) Togo ( +228 ) Tokelau ( +690 ) Tonga ( +676 ) Trinidad and Tobago ( +1 ) Tunisia ( +216 ) T\u00fcrkiye ( +90 ) Turkmenistan ( +993 ) Turks and Caicos Islands ( +1 ) Tuvalu ( +688 ) Uganda ( +256 ) Ukraine ( +380 ) United Arab Emirates ( +971 ) United Kingdom ( +44 ) United States of America ( +1 ) Uruguay ( +598 ) Uzbekistan ( +998 ) Vanuatu ( +678 ) Vatican City ( +39 ) Venezuela ( +58 ) Vietnam ( +84 ) Virgin Islands, British ( +1 ) Virgin Islands, U.S. ( +1 ) Wallis and Futuna Islands ( +681 ) Yemen ( +967 ) Zambia ( +260 ) Zimbabwe ( +263 )\n\n+1", "label": 0}
{"title": "Title Launch Observability at Netflix Scale", "url": "https://netflixtechblog.com/title-launch-observability-at-netflix-scale-c88c586629eb?source=collection_home---4------10-----------------------", "content": "Title Launch Observability at Netflix Scale\n\nPart 1: Understanding The Challenges Netflix Technology Blog 5 min read \u00b7 Dec 17, 2024 -- 7 Listen Share\n\nBy: Varun Khaitan\n\nWith special thanks to my stunning colleagues: Mallika Rao, Esmir Mesic, Hugo Marques\n\nIntroduction\n\nAt Netflix, we manage over a thousand global content launches each month, backed by billions of dollars in annual investment. Ensuring the success and discoverability of each title across our platform is a top priority, as we aim to connect every story with the right audience to delight our members. To achieve this, we are committed to building robust systems that deliver comprehensive observability, enabling us to take full accountability for every title on our service.\n\nThe Challenge of Title Launch Observability\n\nAs engineers, we\u2019re wired to track system metrics like error rates, latencies, and CPU utilization \u2014 but what about metrics that matter to a title\u2019s success?\n\nConsider the following example of two different Netflix Homepages:\n\nSample Homepage A\n\nSample Homepage B\n\nTo a basic recommendation system, the two sample pages might appear equivalent as long as the viewer watches the top title. Yet, these pages couldn\u2019t be more different. Each title represents countless hours of effort and creativity, and our systems need to honor that uniqueness.\n\nHow do we bridge this gap? How can we design systems that recognize these nuances and empower every title to shine and bring joy to our members?\n\nThe Operational Needs of a Personalization System\n\nIn the early days of Netflix Originals, our launch team would huddle together at midnight, manually verifying that titles appeared in all the right places. While this hands-on approach worked for a handful of titles, it quickly became clear that it couldn\u2019t scale. As Netflix expanded globally and the volume of title launches skyrocketed, the operational challenges of maintaining this manual process became undeniable.\n\nOperating a personalization system for a global streaming service involves addressing numerous inquiries about why certain titles appear or fail to appear at specific times and places.\n\nSome examples:\n\nWhy is title X not showing on the Coming Soon row for a particular member?\n\nWhy is title Y missing from the search page in Brazil?\n\nIs title Z being displayed correctly in all product experiences as intended?\n\nAs Netflix scaled, we faced the mounting challenge of providing accurate, timely answers to increasingly complex queries about title performance and discoverability. This led to a suite of fragmented scripts, runbooks, and ad hoc solutions scattered across teams \u2014 an approach that was neither sustainable nor efficient.\n\nThe stakes are even higher when ensuring every title launches flawlessly. Metadata and assets must be correctly configured, data must flow seamlessly, microservices must process titles without error, and algorithms must function as intended. The complexity of these operational demands underscored the urgent need for a scalable solution.\n\nAutomating the Operations\n\nIt becomes evident over time that we need to automate our operations to scale with the business. As we thought more about this problem and possible solutions, two clear options emerged.\n\nOption 1: Log Processing\n\nLog processing offers a straightforward solution for monitoring and analyzing title launches. By logging all titles as they are displayed, we can process these logs to identify anomalies and gain insights into system performance. This approach provides a few advantages:\n\nLow burden on existing systems: Log processing imposes minimal changes to existing infrastructure. By leveraging logs, which are already generated during regular operations, we can scale observability without significant system modifications. This allows us to focus on data analysis and problem-solving rather than managing complex system changes. Using the source of truth: Logs serve as a reliable \u201csource of truth\u201d by providing a comprehensive record of system events. They allow us to verify whether titles are presented as intended and investigate any discrepancies. This capability is crucial for ensuring our recommendation systems and user interfaces function correctly, supporting successful title launches.\n\nHowever, taking this approach also presents several challenges:\n\nCatching Issues Ahead of Time: Logging primarily addresses post-launch scenarios, as logs are generated only after titles are shown to members. To detect issues proactively, we need to simulate traffic and predict system behavior in advance. Once artificial traffic is generated, discarding the response object and relying solely on logs becomes inefficient. Appropriate Accuracy: Comprehensive logging requires services to log both included and excluded titles, along with reasons for exclusion. This could lead to an exponential increase in logged data. Utilizing probabilistic logging methods could compromise accuracy, making it difficult to ascertain whether a title\u2019s absence in logs is due to exclusion or random chance. SLA and Cost Considerations: Our existing online logging systems do not natively support logging at the title granularity level. While reengineering these systems to accommodate this additional axis is possible, it would entail increased costs. Additionally, the time-sensitive nature of these investigations precludes the use of cold storage, which cannot meet the stringent SLAs required.\n\nOption 2: Observability Endpoints in Our Personalization Systems\n\nTo prioritize title launch observability, we could adopt a centralized approach. By introducing observability endpoints across all systems, we can enable real-time data flow into a dedicated microservice for title launch observability. This approach embeds observability directly into the very fabric of services managing title launches and personalization, ensuring seamless monitoring and insights. Key benefits and strategies include:\n\nReal-Time Monitoring: Observability endpoints enable real-time monitoring of system performance and title placements, allowing us to detect and address issues as they arise. Proactive Issue Detection: By simulating future traffic(an aspect we call \u201ctime travel\u201d) and capturing system responses ahead of time, we can preemptively identify potential issues before they impact our members or the business. Enhanced Accuracy: Observability endpoints provide precise data on title inclusions and exclusions, allowing us to make accurate assertions about system behavior and title visibility. It also provides us with advanced debugability information needed to fix identified issues. Scalability and Cost Efficiency: While initial implementation required some investment, this approach ultimately offers a scalable and cost-effective solution to managing title launches at Netflix scale.\n\nChoosing this option also comes with some tradeoffs:\n\nSignificant Initial Investment: Several systems would need to create new endpoints and refactor their codebases to adopt this new method of prioritizing launches. Synchronization Risk: There would be a potential risk that these new endpoints may not accurately represent production behavior, thus necessitating conscious efforts to ensure all endpoints remain synchronized.\n\nUp Next\n\nBy adopting a comprehensive observability strategy that includes real-time monitoring, proactive issue detection, and source of truth reconciliation, we\u2019ve significantly enhanced our ability to ensure the successful launch and discovery of titles across Netflix, enriching the global viewing experience for our members. In the next part of this series, we\u2019ll dive into how we achieved this, sharing key technical insights and details.\n\nStay tuned for a closer look at the innovation behind the scenes in Part 2!", "label": 0}
{"title": "Considerations for making a tree view component accessible", "url": "https://github.blog/engineering/user-experience/considerations-for-making-a-tree-view-component-accessible/", "content": "Tree views are a core part of the GitHub experience. You\u2019ve encountered one if you\u2019ve ever navigated through a repository\u2019s file structure or reviewed a pull request.\n\nOn GitHub, a tree view is the list of folders and the files they contain. It is analogous to the directory structure your operating system uses as a way of organizing things.\n\nTree views are notoriously difficult to implement in an accessible way. This post is a deep dive into some of the major considerations that went into how we made GitHub\u2019s tree view component accessible. We hope that it can be used as a reference and help others.\n\nStart with Windows\n\nIt\u2019s important to have components with complex interaction requirements map to something people are already familiar with using. This allows for responsiveness to the keypresses they will try to navigate and take action on our tree view instances.\n\nWe elected to adopt Windows File Explorer\u2019s tree view implementation, given the prominence of Windows\u2019 usage for desktop screen reader users.\n\nNavigating and taking actions on items in Windows\u2019 tree view using NVDA and JAWS helped us get a better understanding of how things worked, including factors such as focus management, keyboard shortcuts, and expected assistive technology announcements.\n\nThen maybe reference the APG\n\nThe ARIA Authoring Practices Guide (APG) is a bit of an odd artifact. It looks official but is no longer recognized by the W3C as a formal document.\n\nThis is to say that the APG can serve as a helpful high-level resource for things to consider for your overall approach, but its suggestions for code necessitate deeper scrutiny.\n\nBuild from a solid, semantic foundation\n\nAt its core, a tree view is a list of lists. Because of this, we used ul and li elements for parent and child nodes:\n\n<ul> <li> <ul> <li>.github/</li> <li>source/</li> <li>test/</li> </ul> </li> <li>.gitignore</li> <li>README.md</li> </ul>\n\nThere are a few reasons for doing this, but the main considerations are:\n\nBetter assurance that a meaningful accessibility tree is generated,\n\nLessening the work we need for future maintenance, and consequential re-verification that our updates continue to work properly, and\n\nBetter guaranteed interoperability between different browsers, apps, and other technologies.\n\nNOTE: GitHub currently does not virtualize its file trees. We would need to revisit this architectural decision if this ever changes.\n\nBetter broad assistive technology support\n\nThe more complicated an interactive pattern is, the greater the risk that there are bugs or gaps with assistive technology support.\n\nGiven the size of the audience GitHub serves, it\u2019s important that we consider more than just majority share assistive technology considerations.\n\nWe found that utilizing semantic HTML elements also performed better for some less-common assistive technologies. This was especially relevant with some lower-power devices, like an entry-level Android smartphone from 2021.\n\nBetter Forced Color Mode support\n\nSemantic HTML elements also map to native operating system UI patterns, meaning that Forced Color Mode\u2019s heuristics will recognize them without any additional effort. This is helpful for people who rely on the mode to see screen content.\n\nThe heuristic mapping behavior does not occur if we used semantically neutral div or span elements, and would have to be manually recreated and maintained.\n\nUse a composite widget\n\nA composite widget allows a component that contains multiple interactive elements to only require one tab stop unless someone chooses to interact with it further.\n\nConsider a file tree for a repository that contains 500+ files in 20+ directories. Without a composite widget treatment, someone may have to press Tab far too many times to bypass the file tree component and get what they need.\n\nThink about wrapping it in a landmark\n\nLike using a composite widget, landmark regions help some people quickly and efficiently navigate through larger overall sections of the page. Because of this, we wrapped the entire file tree in a nav landmark element.\n\nThis does not mean every tree view component should be a landmark, however! We made this decision for the file tree because it is frequently interacted with as a way to navigate through a repository\u2019s content.\n\nGo with a roving tabindex approach\n\nA roving tabindex is a technique that uses tabindex=\"-1\" applied to each element in a series, and then updates the tabindex value to use 0 instead in response to user keyboard input. This allows someone to traverse the series of elements, as focus \u201croves\u201d to follow their keypresses.\n\n<li tabindex=\"-1\">File 1</li> <li tabindex=\"-1\">File 2</li> <li tabindex=\"0\">File 3</li> <li tabindex=\"-1\">File 4</li>\n\nThe roving tabindex approach performed better than utilizing aria-activedescendant , which had issues with VoiceOver on macOS and iOS.\n\nEnhance with ARIA\n\nWe use a considered set of ARIA declarations to build off our semantic foundation.\n\nNote that while we intentionally started with semantic HTML, there are certain ARIA declarations that are needed. The use of ARIA here is necessary and intentional, as it expands the capabilities of HTML to describe something that HTML alone cannot describe\u2014a tree view construct.\n\nOur overall approach follows what the APG suggests, in that we use the following:\n\nrole=\"tree\" is placed on the parent ul element, to communicate that it is a tree view construct.\n\nis placed on the parent element, to communicate that it is a tree view construct. role=\"treeitem\" is placed on the child li elements, to communicate that they are tree view nodes.\n\nis placed on the child elements, to communicate that they are tree view nodes. role=\"group\" is declared on child ul elements, to communicate that they contain branch and leaf nodes.\n\nis declared on child elements, to communicate that they contain branch and leaf nodes. aria-expanded is declared on directories, with a value of true to communicate that the branch node is in an opened state and a value of false to communicate that it is in a collapsed state instead.\n\nis declared on directories, with a value of to communicate that the branch node is in an opened state and a value of to communicate that it is in a collapsed state instead. aria-selected is used to indicate if branch or leaf nodes have been chosen by user navigation, and can therefore have user actions applied to them.\n\nWe also made the following additions:\n\naria-hidden=\"true\" is applied to SVG icons (folders, files, etc.) to ensure its content is not announced.\n\nis applied to SVG icons (folders, files, etc.) to ensure its content is not announced. aria-current=\"true\" is placed on the selected node to better support when a node is deep linked to via URL.\n\nNOTE: We use \u201cbranch node\u201d and \u201cleaf node\u201d as broad terms that can apply to all tree view components we use on GitHub. For the file tree, branch nodes would correspond to directories and subdirectories, and leaf nodes would correspond to files.\n\nSupport expected navigation techniques\n\nThe following behaviors are what people will try when operating a tree view construct, so we support them:\n\nKeyboard keypresses\n\nTab : Places focus on the entire tree view component, then moves focus to the next focusable item on the view.\n\n: Places focus on the entire tree view component, then moves focus to the next focusable item on the view. Enter : If a branch node is selected: Displays the directory\u2019s contents. If a leaf node is selected: Displays the leaf node\u2019s contents.\n\n: Down : Moves selection to the next node that can be selected without opening or closing a node.\n\n: Moves selection to the next node that can be selected without opening or closing a node. Up : Moves selection to the previous node that can be selected without opening or closing a node.\n\n: Moves selection to the previous node that can be selected without opening or closing a node. Right : If a branch node is selected and in a collapsed state: Expands the selected collapsed branch node and does not move selection. If a branch node is selected and in an expanded state: Moves selection to the directory\u2019s first child node.\n\n: Left : If a branch node is selected and in an expanded state: Collapses the selected collapsed directory node and does not move selection. If a branch node is selected and in a collapsed state: Moves selection to the node\u2019s parent directory. If a leaf node is selected: Moves selection to the leaf node\u2019s parent directory.\n\n: End : Moves selection to the last node that can be selected.\n\n: Moves selection to the last node that can be selected. Home : Moves selection to the first node that can be selected.\n\nWe also support typeahead selection, as we are modeling Windows File Explorer\u2019s tree view behaviors. Here, we move selection to the node closest to the currently selected node whose name matches what the user types.\n\nMiddle clicking\n\nNodes on tree view constructs are tree items, not links. Because of this, tree view nodes do not support the behaviors you get with using an anchor element, such as opening its URL in a new tab or window.\n\nWe use JavaScript to listen for middle clicks and Control + Enter keypresses to replicate this behavior.\n\nConsider states\n\nLoading\n\nTree views on GitHub can take time to retrieve their content, and we may not always know how much content a branch node contains.\n\nLive region announcements are tricky to get right, but integral to creating an equivalent experience. We use the following announcements:\n\nIf there is a known amount of nodes that load, we enumerate the incoming content with an announcement that reads, \u201cLoading {x} items.\u201d\n\nIf there is an unknown number of nodes that load, we instead use a more generic announcement of, \u201cLoading\u2026\u201d\n\nIf there are no nodes that load we use an announcement message that reads, \u201c{branch node name} is empty.\u201d\n\nAdditionally, we manage focus for loading content:\n\nIf focus is placed on a placeholder loading node when the content loads in: Move focus from the placeholder node to the first child node in the branch node.\n\nIf focus is on a placeholder loading node but the branch node does not contain content: Move focus back to the branch node. Additionally, we remove the branch node\u2019s aria-expanded declaration.\n\nErrors\n\nCircumstances can conspire to interfere with a tree view component\u2019s intended behavior. Examples of this could be a branch node failing to retrieve content or a partial system outage.\n\nIn these scenarios, the tree view component will use a straightforward dialog component to communicate the error.\n\nFix interoperability issues\n\nAs previously touched on, complicated interaction patterns run the risk of compatibility issues. Because of this, it\u2019s essential to test your efforts with actual assistive technology to ensure it actually works.\n\nWe made the following adjustments to provide better assistive technology support:\n\nUse aria-level\n\nScreen readers can report on the depth of a nested list item. For example, a li element placed inside of a ul element nested three levels deep can announce itself as such.\n\nWe found that we needed to explicitly declare the level on each li element to recreate this behavior for a tree view. For our example, we\u2019d also need to set aria-level=\"3\" on the li element.\n\nThis fix addressed multiple forms of assistive technology we tested with.\n\nExplicitly set the node\u2019s accessible name on the li element\n\nA node\u2019s accessible name is typically set by the text string placed inside the li element:\n\n<li>README.md</li>\n\nHowever, we found that VoiceOver on macOS and iOS did not support this. This may be because of the relative complexity of each node\u2019s inner DOM structure.\n\nWe used aria-labelledby to get around this problem, with a value that pointed to the id set on the text portion of each node:\n\n<li aria-labelledby=\"readme-md\"> <div> <!-- Icon --> </div> <div id=\"readme-md\"> README.md </div> </li>\n\nThis guarantees that:\n\nthe node\u2019s accessible name is announced when focus is placed on the li element,\n\nelement, and that the announcement matches what is shown visually.\n\nWhere we\u2019d like to go from here\n\nThere\u2019s a couple areas we\u2019re prototyping and iterating on to better serve our users:\n\nBrowsers apply a lot of behaviors to anchor elements, such as the ability to copy the URL.\n\nWe\u2019d like to replace the JavaScript that listens for middle clicks with a more robust native solution, only without sacrificing interoperability and assistive technology support.\n\nSupporting multiple actions per node\n\nTree views constructs were designed assuming a user will only ever navigate to a node and activate it.\n\nGitHub has use cases that require actions other than activating the node, and we\u2019re exploring how to accomplish that. This is exciting, as it represents an opportunity to evolve the tree view construct on the web.\n\nAlways learning\n\nAn accessible tree view is a complicated component to make, and it requires a lot of effort and testing to get right. However, this work helps to ensure that everyone can use a core part of GitHub, regardless of device, circumstance, or ability.\n\nWe hope that highlighting the considerations that went into our work can help you on your accessibility journey.\n\n\n\nShare your experience: We\u2019d love to hear from you if you\u2019ve run into issues using our tree view component with assistive technology. This feedback is invaluable to helping us continue to make GitHub more accessible.", "label": 0}
{"title": "Behind the Scenes: Building a Robust Ads Event Processing Pipeline", "url": "https://netflixtechblog.com/behind-the-scenes-building-a-robust-ads-event-processing-pipeline-e4e86caf9249?source=collection_home---4------1-----------------------", "content": "Behind the Scenes: Building a Robust Ads Event Processing Pipeline Netflix Technology Blog 8 min read \u00b7 May 9, 2025 -- 12 Listen Share\n\nKinesh Satiya\n\nIntroduction\n\nIn a digital advertising platform, a robust feedback system is essential for the lifecycle and success of an ad campaign. This system comprises of diverse sub-systems designed to monitor, measure, and optimize ad campaigns. At Netflix, we embarked on a journey to build a robust event processing platform that not only meets the current demands but also scales for future needs. This blog post delves into the architectural evolution and technical decisions that underpin our Ads event processing pipeline.\n\nAd serving acts like the \u201cbrain\u201d \u2014 making decisions, optimizing delivery and ensuring right Ad is shown to the right member at the right time. Meanwhile, ad events, after an Ad is rendered, function like \u201cheartbeats\u201d, continuously providing real-time feedback (oxygen/nutrients) that fuels better decision-making, optimizations, reporting, measurement, and billing. Expanding on this analogy:\n\nJust as the brain relies on continuous blood flow, ad serving depends on a steady stream of ad events to adjust next ad serving decision, frequency capping, pacing, and personalization.\n\nIf the nervous system stops sending signals (ad events stop flowing), the brain (ad serving) lacks critical insights and starts making poor decisions or even fails.\n\nThe healthier and more accurate the event stream (just like strong heart function), the better the ad serving system can adapt, optimize, and drive business outcomes.\n\nLet\u2019s dive into the journey of building this pipeline.\n\nThe Pilot\n\nIn November 2022, we launched a brand new basic ads plan, in partnership with Microsoft. The software systems extended the existing Netflix playback systems to play ads. Initially, the system was designed to be simple, secure, and efficient, with an underlying ethos of device-originated and server-proxied operations. The system consisted of three main components: the Microsoft Ad Server, Netflix Ads Manager, and Ad Event Handler. Each ad served required tracking to ensure the feedback loop functioned effectively, providing the external ad server with insights on impressions, frequency capping (advertiser policy that limits the number of times a user sees a specific ad), and monetization processes.\n\nKey features of this system include:", "label": 0}
{"title": "Google Cloud donates A2A to Linux Foundation", "url": "https://developers.googleblog.com/en/google-cloud-donates-a2a-to-linux-foundation/", "content": "Today at Open Source Summit North America, the Linux Foundation announced the formation of the Agent2Agent project with Amazon Web Services, Cisco, Google, Microsoft, Salesforce, SAP, and ServiceNow. With the formation of this new, independent entity, the companies will collaborate closely on fostering an open and interoperable ecosystem for AI agents with the Agent2Agent (A2A) protocol and other interoperability technology. The project will be hosted by the Linux Foundation and will be seeded with Google\u2019s transfer of the groundbreaking Agent2Agent (A2A) protocol specification, accompanying SDKs, and developer tooling.\n\nThe A2A protocol, an open standard for communication and collaboration between distinct AI agents, aims to break down the silos that currently limit the potential of artificial intelligence. More than 100 companies now support the protocol, with AWS and Cisco as its newest validators. By providing a common language for AI agents to discover each other\u2019s capabilities, securely exchange information, and coordinate complex tasks, the A2A protocol is paving the way for a new era of more powerful, collaborative, and innovative AI applications.\n\nThe formation of the Agent2Agent project under the neutral governance of the Linux Foundation will ensure that this critical component remains vendor-agnostic and community-driven. This move is designed to accelerate the adoption and development of the A2A protocol by providing a robust framework for open collaboration, intellectual property management, and long-term stewardship.", "label": 0}
{"title": "Similarity clustering to catch fraud rings", "url": "https://stripe.com/blog/similarity-clustering", "content": "Stripe enables businesses in many countries worldwide to onboard easily so they can accept payments as quickly as possible. Stripe\u2019s scale makes our platform a common target for payments fraud and cybercrime, so we\u2019ve built a deep understanding of the patterns bad actors use. We take these threats seriously because they harm both our users and our ecosystem; every fraudulent transaction we circumvent keeps anyone impacted from having a bad day.\n\nWe provide our risk analysts with automated tools to make informed decisions while sifting legitimate users from potentially fraudulent accounts. One of the most useful tools we\u2019ve developed uses machine learning to identify similar clusters of accounts created by fraudsters trying to scale their operations. Many of these attempts are easy to detect and we can reverse engineer the fingerprints they leave behind to shut them down in real-time. In turn, this allows our analysts to spend more time on sophisticated cases that have the potential to do more harm to our users.\n\nFraud in the payments ecosystem\n\nFraud can generally be separated into two large categories: transaction fraud and merchant fraud. Transaction fraud applies to individual charges (such as those protected by Radar), where a fraudster may purchase items with a stolen credit card to resell later.\n\nMerchant fraud occurs when someone signs up for a Stripe account to later defraud cardholders. For example, a fraudster may attempt to use stolen card numbers through their account, so they\u2019ll try to provide a valid website, account activity, and charge activity to appear legitimate. The fraudster hopes to be paid out to their bank account before Stripe finds out. Eventually, the actual cardholders will request a chargeback from their bank for the unauthorized transaction. Stripe will reimburse chargebacks to issuing banks (and by proxy, the cardholder) and attempt to debit the fraudster\u2019s account. However, if they have already been paid out then it may be too late to recover those funds and Stripe ultimately covers those costs as fraud losses.\n\nFraudsters also may attempt to defraud Stripe at a larger scale by setting up a predatory or scam business. For example, the fraudster will create a Stripe account, claiming to sell expensive apparel or electronics for low prices. Unsuspecting customers think they are getting a great deal, but they never receive the product they ordered. Once again, the fraudster hopes to be paid out before they are shut down or overwhelmed with chargebacks.\n\nUsing similarity information to reduce fraud\n\nFraudsters tend to create Stripe accounts with reused information and attributes. Typically, low-effort fraudsters will not try to hide links to previous accounts, and this activity can be detected immediately at signup. More sophisticated fraudsters will put more work into hiding their tracks in order to prevent any association with prior fraud attempts. Some attributes like name or date of birth are trivial to fabricate, whereas others are more difficult\u2014for example, it requires significant effort to obtain a new bank account.\n\nLinking accounts together via shared attributes is reasonably effective at catching obvious fraud attempts, but we wanted to move from a system based on heuristics to one powered by machine learning models. While heuristics may be effective in certain cases, machine learning models are significantly more effective at learning predictive rules.\n\nSuppose a pair of accounts are assigned a similarity score based on the number of attributes they share. This similarity score could then help predict future behavior: if an account looks similar to a known fraudulent account, there\u2019s a significant likelihood they are more likely to also be fraudulent. The challenge here is to accurately quantify similarity. For example, two accounts who share dates of birth should have a lower similarity score than two accounts who share a bank account.\n\nBy training a machine learning model, we remove the need for guesswork and hand-constructed heuristics. Now, we can automatically retrain the model over time as we obtain more data. Automatic retraining enables our models to continually improve in accuracy, adapt to new fraud trends, and learn the signatures of particular adversarial groups.\n\nChoosing a clustering approach\n\nMachine learning tasks are generally classified as either supervised or unsupervised. The goal of supervised learning is to make predictions given an existing dataset of labeled examples (for example, a label that indicates whether an account is fraudulent), whereas in unsupervised learning the usual goal is learn a generative model for the raw data (in other words, to understand the underlying structure of the data). Traditionally, clustering tasks fall into the class of unsupervised learning: unlabeled data needs to be grouped into clusters that capture some understanding of similarity or likeness.\n\nFortunately, we\u2019re able to use supervised models, which are generally easier to train and may be more accurate. We already have a large body of data demonstrating whether a given account has been created by a fraudster based on the downstream impact (e.g. we observe a significant number of chargebacks and fraud losses). This allows us to confidently label millions of legitimate and illegitimate businesses from our dataset.\n\nIn particular, our approach is an example of similarity learning where the objective is to learn a symmetric function based on training data. Over the years, our risk underwriting teams have manually compiled many examples of existing clusters of fraudulent accounts through our investigations of fraud rings, and we can use these reference clusters as training data to learn our similarity function. By sampling edges from these groups, we obtain a dataset consisting of pairs of accounts along with a label for each pair indicating whether or not the two accounts belong to the same cluster. We use intra-cluster edges as positive training examples and inter-cluster edges as negative training examples, where an edge denotes a pair of accounts.\n\nWe use known clusters of accounts to train our predictive models.\n\nNow that we have the labels specified, we must decide what features to use for our model. We want to convert pairs of Stripe accounts into useful model inputs that have predictive power. The feature generation process takes two Stripe accounts and produces a number of features that are defined on the pair. Due to the rich nature of Stripe accounts and their associated data, we can construct an extensive set of features for any given pair. Some examples of the features we\u2019d include are categorical features that store the values of common attributes such as the account\u2019s email domain, any overlap in card numbers used on both accounts, and measures of text similarity.\n\nUsing gradient-boosted decision trees\n\nBecause of the wide variety of features we can construct from given pairs of accounts, we decided to use gradient-boosted decision trees (GBDTs) to represent our similarity model. In practice, we\u2019ve found GBDTs strike the right balance between being easy to train, having strong predictive power and being robust despite variations in the data. When we started this project we wanted to get something out the door quickly that was effective, had well-understood properties, and was straightforward to fine-tune. The variant that we use, XGBoost, is one of the best performing off-the-shelf models for cases with structured (also known as tabular) data, and we have well-developed infrastructure to train and serve them. You can read more about the infrastructure we use to train machine learning models at Stripe in a previous post.\n\nNow that we have a trained model, we can use it to predict fraudulent activity. Since this model operates on pairs of Stripe accounts, it\u2019s not feasible to feed it all possible pairs of accounts and compute scores across all pairs. Instead, we first generate a candidate set of edges to be scored. We do this by taking recently created Stripe accounts and creating edges between accounts that share certain attributes. Although this isn\u2019t an exhaustive approach, this heuristic works well in practice to prune the set of candidate edges to a reasonable number.\n\nOnce the candidate edges are scored, we then filter edges by selecting those with a similarity score above some threshold. We then compute the connected components on the resulting graph. The final output is a set of high-fidelity account clusters which we can analyze, process, or manually inspect together as a unit. In particular, a fraud analyst may want to examine clusters which contain known fraudulent accounts and investigate the remaining accounts in that cluster.\n\nThis is an iterative process; as each individual cluster grows, we can quickly identify increasing similarity as fake accounts in a fraudster\u2019s operation are created. And the more fraud rings we detect and shutdown at Stripe, the more accurate our clustering model becomes at identifying new clusters in the future.\n\nEach edge is weighted by a similarity score; we identify clusters by finding connected components in the resulting graph.\n\nBenefits of the clustering system\n\nSo far, we\u2019ve discussed the overall structure of the account clustering system. Although we have other models and systems in place to catch fraudulent accounts, using clustering information has the following advantages:\n\nWe\u2019re even better at catching obvious fraud. It\u2019s difficult for fraudsters to completely separate new accounts from previous accounts they\u2019ve created in the past, or from accounts created by other fraudsters. Whether this is due to reusing basic attribute data or more complex measures of similarity, the account clustering system catches and blocks hundreds of fraudulent accounts weekly with very few false positives.\n\nIt\u2019s difficult for fraudsters to completely separate new accounts from previous accounts they\u2019ve created in the past, or from accounts created by other fraudsters. Whether this is due to reusing basic attribute data or more complex measures of similarity, the account clustering system catches and blocks hundreds of fraudulent accounts weekly with very few false positives. Fraudsters can only use their resources once. Whenever someone decides to defraud Stripe, they need to invest in resources such as stolen IDs and bank accounts, each of which incur monetary cost or inconvenience. In effect, by requiring fraudsters to use a new set of resources every time they create a Stripe account, we slow them down and increase the cost of defrauding Stripe. Clustering is a key tool since it invalidates resources such as bank accounts that have been previously used on fraudulent accounts.\n\nWhenever someone decides to defraud Stripe, they need to invest in resources such as stolen IDs and bank accounts, each of which incur monetary cost or inconvenience. In effect, by requiring fraudsters to use a new set of resources every time they create a Stripe account, we slow them down and increase the cost of defrauding Stripe. Clustering is a key tool since it invalidates resources such as bank accounts that have been previously used on fraudulent accounts. Our risk analysts conduct more efficient reviews. When accounts require manual inspection by an analyst, they spend time trying to understand the intentions and motivations of the person behind the account. Analysts focus on the details of the business to sift legitimate users from a set of identified potentially fraudulent accounts. With the help of our clustering technique, analysts can easily identify common patterns and outliers and apply the same judgments to multiple accounts at once with a smaller likelihood of error.\n\nWhen accounts require manual inspection by an analyst, they spend time trying to understand the intentions and motivations of the person behind the account. Analysts focus on the details of the business to sift legitimate users from a set of identified potentially fraudulent accounts. With the help of our clustering technique, analysts can easily identify common patterns and outliers and apply the same judgments to multiple accounts at once with a smaller likelihood of error. Account clusters are a building block for other systems. Understanding whether two accounts are duplicates or measuring their degree of similarity is a useful primitive that extends beyond the use cases described here. For example, we use the similarity model to expand our training sets for models which have sparse training data.\n\nCatching fraud in action\n\nStripe faces a multitude of threats from fraudsters who attempt to steal money in creative and complex ways. Identifying similarities between accounts and clustering them together enhances our effectiveness and improves our ability to block fraudulent accounts. Clustering accounts together and identifying duplicate attempts to create fraudulent accounts makes life more difficult for fraudsters. One goal of our models is to change the economic model of fraud by raising the required cost for unused bank accounts, IP addresses, devices and other tools they use. This leads to a negative expected value for fraudsters, weakens the underlying supply chain for stolen credentials and user data, and disincentivizes committing fraud at scale.\n\nWe often think about fraud as an adversarial game; uncovering fraudulent clusters allows us to tip the game in our favor. Using common tools like XGBoost enabled us to quickly deploy a solution that naturally fit into our machine learning platform and allows us to easily adapt our approach over time. We\u2019re continuing to explore new techniques to catch fraud to ensure Stripe can reliably operate a low-friction global payment network for millions of businesses.", "label": 0}
{"title": "IndieWeb Carnival September 2023: My Kind of Weather", "url": "https://lifeofpablo.com/blog/indieweb-carnival-september-2023-my-kind-of-weather", "content": "Growing up in the Midwestern region of the United States (or commonly known as the Midwest) I was so used to irritable weather. What I mean by irritable weather was facing, \"four seasons in one week.\" You're probably thinking the following: \"How is this possible?\" It simply does! If you know, you know. Here are a few examples (No particular order)\n\nOne day it can be a nice spring day with all the flowers blooming being happy as can be. The following day you could wake up to find out that there is a blizzard in effect. A few days, it could be scorching hot and wondering how we got here? The last few days of the week, it could be a beautiful, soothing fall day and you forget about the weather tragedies of earlier of the week.\n\nAs chaotic as it sounds, I do miss it. I do miss sometimes not knowing if I will melt one day or be trapped in my home for many days at a time. My favorite is knowing if my home would fly away in a tornado. As silly and boring the topic of weather is, it is also a bonding activity with people who live in this region. It's almost a pastime knowing we could just die at any minute.\n\nIf I were to pick a favorite season, it would be winter. Are you in shock? Even I don't believe it. I do miss dressing up all nice for the winter. Who doesn't like wearing cute winter outfits. If were going to suffer in the cold we might as well look our best!\n\nI moved to California to live in a different place and subconsciously to escape the cold. In Northern California it is generally just hot here all the time. I do miss having season changes. Geography and location tends to influence the type of weather or climate one lives in.\n\nYou can take the person out of the midwest but you can't take the Midwestern out of the person.\n\nAlso posted on IndieNews", "label": 1}
{"title": "New Student Enrollment Leader (NSE) at UNK", "url": "https://lifeofpablo.com/blog/new-student-enrollment-leader-nse-at-unk", "content": "New Student Enrollment Leader (NSE) at UNK\n\nThis post was written in English (en_US).\n\nPablo Morales-Garcia\n\nNew Student Enrollment Leader\n\nPhone: (308) 865-8526 | Email: BeALoper@unk.edu\n\nFavorite Place On Campus\n\nI really love the campus in its entirety from the art to the history of building and monuments. One of my favorite features on campus are the trails and sidewalks that connect you to all of campus but also to all of the bike trails in the city of Kearney. It is so easy to get to Cottonmill or Yanney Park on the nice trails. Around campus, I really enjoying riding around on my longboard! I can use my longboard to get point A to point B in a snap! You'll thank me later when you're running behind for class! I really enjoy the social aspect by meeting fellow longboarders.\n\n\n\nFavorite UNK Tradition\n\nMy favorite UNK Tradition would be Big Blue and Gold Week. It is such a good way to get started on the new school year and it is such an opportunity to get to know people by just having fun. One of my favorite events during BBGW is going to destination downtown and watch the food eating competition. One of my fondest memories is being able to ride on a float during and just having a blast! Not every day do you have the chance to ride on a float during a parade, cool, eh?\n\n\n\nWhat are you passionate about?\n\nI have always been passionate about travel and languages since a young age. My views of the world have expanded so much ever since I traveled when I was a kid and even more when I studied abroad in France. Traveling gives a person experiences that you cannot get in your backyard or in your daily routine!\n\nI want to spread the importance of learning more than one language. I want to share my passion of being open minded about the world and embrace languages. Just by learning another language opens the door for you in many ways you could never ever imagine!\n\n\n\nI Chose UNK Because...\n\nWhen I came for a tour of UNK, I instantly became in love with the campus. Just being here I didn't feel overwhelmed being on the campus. Everyone I ran across was so nice! I felt such a personal connection especially when I met with the chair, advisor, and a professor of my program. Of all the school visits I had gone to the past, not one had offered me to meet with my potential advisor to do this. At that moment I realized that \"This is the school I want to attend!\" It all came down to the little things that would make such a difference in picking a school! After that....the rest is history! 2018 Staff\n\nJennifer Garcia\n\nMary Dworak\n\nNoah Journey\n\nOdwuar Quinonez\n\nPablo Morales-Garcia\n\nShelby Hoffmann\n\nValeria Lozano\n\nTaylor Janicek\n\nUniversity of Nebraska at Kearney\n\n\"", "label": 1}
{"title": "Summer College Class", "url": "https://lifeofpablo.com/blog/summer-college-class", "content": "Summer College Class\n\nThis post was written in English (en_US).\n\nSo last week was the end of my speech class. The first few days sucked because I had surgery on my nose. Everything went well. That's a different story thought :)\n\nThis was the first time I ever bought a college book. That sad part was that we barely used it in the three weeks of the speech class. What a waste of money! Anyhow, it was a fun class but weird at the same time. Weird reasosn to be undisclosed.... I got to be around people I knew. This gave me reassurance, well whatever I got out of it. I hated doing speeches. I'm actually okay at them but give me panic attacks (often you can't tell.) In order to survive one had to\n\nFake it 'til you make it!\n\nThe class got more comfortable as time went on. Taking this speech class has taught me a lot about myself. I found some weaknesses that I have improved on.\n\nBeing more confident\n\nMore ability to speak to large audience.\n\nLess worrying of what others things\n\netc.\n\nTip of the day. Take a college speech class in the summer. Not only is it shorter but also you have less people to present towards than a whole lecture hall full of students during the school year. It is less stressful.\n\nTo end...", "label": 1}
{"title": "Send a Friend a Webmention Day", "url": "https://lifeofpablo.com/blog/send-a-friend-a-webmention-day", "content": "It's been a great year so far! Lot's of good things have happened. I've overcame a few things. I started my blog again. The little things add up. One of the major events was becoming part of the IndieWeb Community. It's been a great community that is welcoming. I have learned so much from the people in the community and at Homebrew Website Club. Every single one of them has been pretty amazing.\n\nSince it's Send a Friend a Webmention Day, I want to send a webmention to a few people.\n\nAngelo Gladding\n\nYou were the first person I interacted with at Homebrew Website Club. He gave me the run down and help me connect the pieces based on the information I knew already. Thanks for being so rad! Your bot trained with your voice is pretty tight alongside your mediasoup-based setup.\n\nTracy Durnell I really enjoy your style of writing! I have been looking for better ways to express myself through writing and to find my style of blogging, it's inspiring and I enjoy your content. I've been wanting to make the Apple crumb pie. I'm excited to try it soon! If you need some more information on Oaxacan cuisine, I'm your guy!\n\nJames G Everything you do is super cool especially the programming language you created! You'll have to try some Coffee (and hot chocolate) from Oaxaca. I really enjoyed this month's IndieWeb Carnival topic.\n\nBenji I love what you are doing with your site. I love the minimalist approach. The Sparkles is so rad! It works beautifully with one of my sites.\n\ngRegor Love I hope you picked the blue shirt! I still haven't seen the movie? Yay or nay? Thank you for pointing me in the right directions when it comes to marking up content in a different language. More of content in different languages coming soon.\n\nAlex Sirac Ton site, R\u00e9ussir Mes \u00c9tudes est super cool et informatif! J'aime le blog de ton site web principal. You've inspired me to write in French again!\n\nJo dead.garden Since I love languages, I started looking into toki pona. Thank you so much for sharing this. Hopefully we can speak soon.\n\nAnthony Ciccarello Thank you for helping me out microformats and \"likes\" and getting that squared away. Every time I see you post about your puppy I immediately want to pet but we haven't broken the virtual-physical barrier yet to pet dogs yet. Any ideas?\n\nAlso as a Midwesterner, I saw you have a recipe for Puppy Chow. I will probably be making a batch once I get home.\n\nColin Walker I'm really digging your site. I also stumbled on your Music and I'm really digging it! I'd love to talk about your music since I am teaching a sound engineering class. I've been making progress on the e-book as well.", "label": 1}
{"title": "Visiting the BuzzFeed Office", "url": "https://lifeofpablo.com/blog/visiting-the-buzzfeed-office", "content": "One of my wishes came true! I've been a huge fan of BuzzFeed for many years for as long as I can remember. I remember stumbling on BuzzFeed's home page in the late 2000s. I've taken many of their quizzes posted on the site. There are other things on the site beyond the quizzes. I was very fortunate this week to visit BuzzFeed/Complex's office this week. I was very fortunate to get a tour and check things out! Going there was a whole experience. I got to share this experience with people I know an love. You know that feeling you get when you're a little kid at the grocery store eagerly waiting to see if my parents would buy me any candy. That's what I was feeling. I'm pretty sure I cried a few tears of joy. The office had such a cool color scheme and it went well with everything around it. It seemed very organized. I got to see how the office works and what the type of mood exists. The space was a welcoming right when you walk into. I would say it is a pretty laid back office.It was everything I hoped and dreamed for. I was in heaven. I can't say too much more other than, one more thing checked off my, \"places I wan't to see some day.\"", "label": 1}
{"title": "Leveraging BigQuery JSON for Optimized MongoDB Dataflow Pipelines", "url": "https://developers.googleblog.com/en/leveraging-bigquery-json-for-optimized-mongodb-dataflow-pipelines/", "content": "This streamlined approach saves time and resources, empowering users to unlock the full potential of their data through advanced data analytics and machine learning.\n\nWe're delighted to introduce a major enhancement to our Google Cloud Dataflow templates for MongoDB Atlas. By enabling direct support for JSON data types, users can now seamlessly integrate their MongoDB Atlas data into BigQuery, eliminating the need for complex data transformations.\n\nLimitations without JSON support\n\nTraditionally, Dataflow pipelines designed to handle MongoDB Atlas data often necessitate the transformation of data into JSON strings or flattening complex structures to a single level of nesting before loading into BigQuery. Although this approach is viable, it can result in several drawbacks:\n\nIncreased latency: The multiple data conversions required can lead to increased latency and can significantly slow down the overall pipeline execution time.\n\nHigher operational costs: The extra data transformations and storage requirements associated with this approach can lead to increased operational costs.\n\nReduced query performance: Flattening complex document structures in JSON String format can impact query performance and make it difficult to analyze nested data.\n\n\n\nSo, what\u2019s new?\n\nBigQuery's Native JSON format addresses these challenges by enabling users to directly load nested JSON data from MongoDB Atlas into BigQuery without any intermediate conversions.\n\nThis approach offers numerous benefits:\n\nReduced operating costs: By eliminating the need for additional data transformations, users can significantly reduce operational expenses, including those associated with infrastructure, storage, and compute resources.\n\nEnhanced query performance: BigQuery's optimized storage and query engine is designed to efficiently process data in Native JSON format, resulting in significantly faster query execution times and improved overall query performance.\n\nImproved data flexibility: users can easily query and analyze complex data structures, including nested and hierarchical data, without the need for time-consuming and error-prone flattening or normalization processes.\n\nA significant advantage of this pipeline lies in its ability to directly leverage BigQuery's powerful JSON functions on the MongoDB data loaded into BigQuery. This eliminates the need for a complex and time-consuming data transformation process. The JSON data within BigQuery can be queried and analyzed using standard BQML queries.\n\nWhether you prefer a streamlined cloud-based approach or a hands-on, customizable solution, the Dataflow pipeline can be deployed either through the Google Cloud console or by running the code from github repository.\n\n\n\nEnabling data-driven decision-making\n\nTo summarize, Google\u2019s Dataflow template provides a flexible solution for transferring data from MongoDB to BigQuery. It can process entire collections or capture incremental changes using MongoDB's Change Stream functionality. The pipeline's output format can be customized to suit your specific needs. Whether you prefer a raw JSON representation or a flattened schema with individual fields, you can easily configure it through the userOption parameter. Additionally, data transformation can be performed during template execution using User-Defined Functions (UDFs).\n\nBy adopting BigQuery Native JSON format in your Dataflow pipelines, you can significantly enhance the efficiency, performance, and cost-effectiveness of your data processing workflows. This powerful combination empowers you to extract valuable insights from your data and make data-driven decisions.\n\nFollow the Google Documentation to learn how to set up the Dataflow templates for MongoDB Atlas and BigQuery.", "label": 0}
{"title": "Microsoft Build 2025: The age of AI agents and building the open agentic web", "url": "https://blogs.microsoft.com/blog/2025/05/19/microsoft-build-2025-the-age-of-ai-agents-and-building-the-open-agentic-web/", "content": "TL;DR? Hear the news as an AI-generated audio overview made using Microsoft 365 Copilot. You can read the transcript here.\n\nWe\u2019ve entered the era of AI agents. Thanks to groundbreaking advancements in reasoning and memory, AI models are now more capable and efficient, and we\u2019re seeing how AI systems can help us all solve problems in new ways.\n\nFor example, 15 million developers are already using GitHub Copilot, and features like agent mode and code review are streamlining the way they code, check, deploy and troubleshoot.\n\nHundreds of thousands of customers are using Microsoft 365 Copilot to help research, brainstorm and develop solutions, and more than 230,000 organizations \u2014 including 90% of the Fortune 500 \u2014 have already used Copilot Studio to build AI agents and automations.\n\nCompanies like Fujitsu and NTT DATA are using Azure AI Foundry to build and manage AI apps and agents that help prioritize sales leads, speed proposal creation and surface client insights. Stanford Health Care is using Microsoft\u2019s healthcare agent orchestrator to build and test AI agents that can help alleviate the administrative burden and speed up the workflow for tumor board preparation.\n\nDevelopers are at the center of it all. For 50 years Microsoft has been empowering developers with tools and platforms to turn their ideas into reality, accelerating innovation at every stage. From AI-driven automation to seamless cloud integration and more, it\u2019s exciting to see how developers are fueling the next generation of digital transformation.\n\nSo, what\u2019s next?\n\nWe envision a world in which agents operate across individual, organizational, team and end-to-end business contexts. This emerging vision of the internet is an open agentic web, where AI agents make decisions and perform tasks on behalf of users or organizations.\n\nAt Microsoft Build we\u2019re showing the steps we\u2019re taking to make this vision a reality through our platforms, products and infrastructure. We\u2019re putting new models and coding agents in the hands of developers, introducing enterprise-grade agents, making our platforms like Azure AI Foundry, GitHub and Windows the best places to build, embracing open protocols and accelerating scientific discovery with AI, all so that developers and organizations can go invent the next big thing.\n\nHere\u2019s a glimpse at just a few of the announcements today:\n\nReimagining the software development lifecycle with AI\n\nAI is fundamentally shifting how code is written, deployed and maintained. Developers are using AI to stay in the flow of their environment longer and to shift their focus to more strategic tasks. And as the software development lifecycle is being transformed, we\u2019re providing new features across platforms including GitHub, Azure AI Foundry and Windows that enable developers to work faster, think bigger and build at scale.\n\nGitHub Copilot coding agent and new updates to GitHub Models: GitHub Copilot is evolving from an in-editor assistant to an agentic AI partner with a first-of-its-kind asynchronous coding agent integrated into the GitHub platform. We\u2019re adding prompt management, lightweight evaluations and enterprise controls to GitHub Models so teams can experiment with best-in-class models, without leaving GitHub. Microsoft is also open-sourcing GitHub Copilot Chat in VS Code. The AI-powered capabilities from GitHub Copilot extensions will now be part of the same open-source repository that drives the world\u2019s most popular development tool. As the home of over 150 million developers, this reinforces our commitment to open, collaborative, AI-powered software development. Learn more about GitHub Copilot updates.\n\nIntroducing Windows AI Foundry: For developers, Windows remains one of the most open and widely used platforms available, with scale, flexibility and growing opportunity. Windows AI Foundry offers a unified and reliable platform supporting the AI developer lifecycle across training and inference. With simple model APIs for vision and language tasks, developers can manage and run open source LLMs via Foundry Local or bring a proprietary model to convert, fine-tune and deploy across client and cloud. Windows AI Foundry is available to get started today. To learn more visit our Windows Developer Blog.\n\nAzure AI Foundry Models and new tools for model evaluation: Azure AI Foundry is a unified platform for developers to design, customize and manage AI applications and agents. With Azure AI Foundry Models, we\u2019re bringing Grok 3 and Grok 3 mini models from xAI to our ecosystem, hosted and billed directly by Microsoft. Developers can now choose from more than 1,900 partner-hosted and Microsoft-hosted AI models, while managing secure data integration, model customization and enterprise-grade governance. We\u2019re also introducing new tools like the Model Leaderboard, which ranks the top-performing AI models across different categories and tasks, and the Model Router, designed to select an optimal model for a specific query or task in real-time. Read more about Azure AI Foundry Models.\n\nMaking AI agents more capable and secure\n\nAI agents are not only changing how developers build, but how individuals, teams and companies get work done. At Build, we\u2019re unveiling new pre-built agents, custom agent building blocks, multi-agent capabilities and new models to help developers and organizations build and deploy agents securely to help increase productivity in meaningful ways.\n\nWith the general availability of Azure AI Foundry Agent Service, Microsoft is bringing new capabilities to empower professional developers to orchestrate multiple specialized agents to handle complex tasks, including bringing Semantic Kernel and AutoGen into a single, developer-focused SDK and Agent-to-Agent (A2A) and Model Context Protocol (MCP) support. To help developers build trust and confidence in their AI agents, we\u2019re announcing new features in Azure AI Foundry Observability for built-in observability into metrics for performance, quality, cost and safety, all incorporated alongside detailed tracing in a streamlined dashboard. Learn more about how to deploy enterprise-grade AI agents in Azure AI Foundry Service.\n\nDiscover, protect and govern in Azure AI Foundry: With Microsoft Entra Agent ID, now in preview, agents that developers create in Microsoft Copilot Studio or Azure AI Foundry are automatically assigned unique identities in an Entra directory, helping enterprises securely manage agents right from the start and avoid \u201cagent sprawl\u201d that could lead to blind spots. Apps and agents built with Foundry further benefit from Purview data security and compliance controls. Foundry also offers enhanced governance tools to set risk parameters, run automated evaluations and receive detailed reports. Learn more about Microsoft Entra Agent ID and Azure AI Foundry integrations with Microsoft Purview Compliance Manager.\n\nIntroducing Microsoft 365 Copilot Tuning and multi-agent orchestration: With Copilot Tuning, customers can use their own company data, workflows and processes to train models and create agents in a simple, low-code way. These agents perform highly accurate, domain-specific tasks securely from within the Microsoft 365 service boundary. For example, a law firm can create an agent that generates documents aligned with its organization\u2019s expertise and style. Additionally, new multi-agent orchestration in Copilot Studio connects multiple agents, allowing them to combine skills and tackle broader, more complex tasks. Check out the Microsoft 365 blog to learn how to access these new tools as well as the Microsoft 365 Copilot Wave 2 spring release, which has moved to general availability and begins rolling out today.\n\nSupporting the open agentic web\n\nTo realize the future of AI agents, we\u2019re advancing open standards and shared infrastructure to provide unique capabilities for customers.\n\nSupporting Model Context Protocol (MCP): Microsoft is delivering broad first-party support for Model Context Protocol (MCP) across its agent platform and frameworks, spanning GitHub, Copilot Studio, Dynamics 365, Azure AI Foundry, Semantic Kernel and Windows 11. In addition, Microsoft and GitHub have joined the MCP Steering Committee to help advance secure, at-scale adoption of the open protocol and announced two new contributions to the MCP ecosystem, an updated authorization specification, which enables people to use their existing trusted sign-in methods to give agents and LLM-powered apps access to data and services such as personal storage drives or subscription services, and the design of an MCP server registry service, which allows anyone to implement public or private, up-to-date, centralized repositories for MCP server entries. Check out the GitHub repository. As we expand our MCP capabilities, our top priority is to ensure we\u2019re building upon a secure foundation. To learn more about this approach see: Securing the Model Context Protocol: Building a Safe Agentic Future on Windows.\n\nA new open project called NLWeb: Microsoft is introducing NLWeb, which we believe can play a similar role to HTML for the agentic web. NLWeb makes it easy for websites to provide a conversational interface for their users with the model of their choice and their own data, allowing users to interact directly with web content in a rich, semantic manner. Every NLWeb endpoint is also an MCP server, so websites can make their content easily discoverable and accessible to AI agents if they choose. Learn more here.\n\nAccelerating scientific discovery with AI\n\nScience may be one of the most important applications of AI, helping to tackle humanity\u2019s most pressing challenges, from drug discovery to sustainability. At Build we\u2019re introducing Microsoft Discovery, an extensible platform built to empower researchers to transform the entire discovery process with agentic AI, helping research and development departments across various industries accelerate the time to market for new products and accelerate and expand the end-to-end discovery process for all scientists. Learn more here.\n\nThis is only a small selection of the many exciting features and updates we will be announcing at Build. We\u2019re looking forward to connecting with those who have registered to join us virtually and in-person, for keynote sessions, live code deep dives, hack sessions and more \u2014 much of which will be available on demand.\n\nPlus, you can get more on all these announcements by exploring the Book of News, the official compendium of all today\u2019s news.\n\nTags: AI, Azure AI, Azure AI Foundry, Book of News, GitHub, GitHub Copilot, Microsoft 365 Copilot, Microsoft Copilot, Microsoft Purview", "label": 0}
{"title": "Enhancing Netflix Reliability with Service-Level Prioritized Load Shedding", "url": "https://netflixtechblog.com/enhancing-netflix-reliability-with-service-level-prioritized-load-shedding-e735e6ce8f7d?source=collection_home---4------21-----------------------", "content": "Without prioritized load-shedding, both user-initiated and prefetch availability drop when latency is injected. However, after adding prioritized load-shedding, user-initiated requests maintain a 100% availability and only prefetch requests are throttled.\n\nWe were ready to roll this out to production and see how it performed in the wild!\n\nReal-World Application and Results\n\nNetflix engineers work hard to keep our systems available, and it was a while before we had a production incident that tested the efficacy of our solution. A few months after deploying prioritized load shedding, we had an infrastructure outage at Netflix that impacted streaming for many of our users. Once the outage was fixed, we got a 12x spike in pre-fetch requests per second from Android devices, presumably because there was a backlog of queued requests built up.\n\nSpike in Android pre-fetch RPS\n\nThis could have resulted in a second outage as our systems weren\u2019t scaled to handle this traffic spike. Did prioritized load-shedding in PlayAPI help us here?\n\nYes! While the availability for prefetch requests dropped as low as 20%, the availability for user-initiated requests was > 99.4% due to prioritized load-shedding.\n\nAvailability of pre-fetch and user-initiated requests\n\nAt one point we were throttling more than 50% of all requests but the availability of user-initiated requests continued to be > 99.4%.\n\nGeneric service work prioritization\n\nBased on the success of this approach, we have created an internal library to enable services to perform prioritized load shedding based on pluggable utilization measures, with multiple priority levels.\n\nUnlike API gateway, which needs to handle a large volume of requests with varying priorities, most microservices typically receive requests with only a few distinct priorities. To maintain consistency across different services, we have introduced four predefined priority buckets inspired by the Linux tc-prio levels:\n\nCRITICAL : Affect core functionality \u2014 These will never be shed if we are not in complete failure.\n\n: Affect core functionality \u2014 These will never be shed if we are not in complete failure. DEGRADED : Affect user experience \u2014 These will be progressively shed as the load increases.\n\n: Affect user experience \u2014 These will be progressively shed as the load increases. BEST_EFFORT : Do not affect the user \u2014 These will be responded to in a best effort fashion and may be shed progressively in normal operation.\n\n: Do not affect the user \u2014 These will be responded to in a best effort fashion and may be shed progressively in normal operation. BULK: Background work, expect these to be routinely shed.\n\nServices can either choose the upstream client\u2019s priority or map incoming requests to one of these priority buckets by examining various request attributes, such as HTTP headers or the request body, for more precise control. Here is an example of how services can map requests to priority buckets:\n\nResourceLimiterRequestPriorityProvider requestPriorityProvider() {\n\nreturn contextProvider -> {\n\nif (contextProvider.getRequest().isCritical()) {\n\nreturn PriorityBucket.CRITICAL;\n\n} else if (contextProvider.getRequest().isHighPriority()) {\n\nreturn PriorityBucket.DEGRADED;\n\n} else if (contextProvider.getRequest().isMediumPriority()) {\n\nreturn PriorityBucket.BEST_EFFORT;\n\n} else {\n\nreturn PriorityBucket.BULK;\n\n}\n\n};\n\n}\n\nGeneric CPU based load-shedding\n\nMost services at Netflix autoscale on CPU utilization, so it is a natural measure of system load to tie into the prioritized load shedding framework. Once a request is mapped to a priority bucket, services can determine when to shed traffic from a particular bucket based on CPU utilization. In order to maintain the signal to autoscaling that scaling is needed, prioritized shedding only starts shedding load after hitting the target CPU utilization, and as system load increases, more critical traffic is progressively shed in an attempt to maintain user experience.\n\nFor example, if a cluster targets a 60% CPU utilization for auto-scaling, it can be configured to start shedding requests when the CPU utilization exceeds this threshold. When a traffic spike causes the cluster\u2019s CPU utilization to significantly surpass this threshold, it will gradually shed low-priority traffic to conserve resources for high-priority traffic. This approach also allows more time for auto-scaling to add additional instances to the cluster. Once more instances are added, CPU utilization will decrease, and low-priority traffic will resume being served normally.\n\nPercentage of requests (Y-axis) being load-shed based on CPU utilization (X-axis) for different priority buckets\n\nExperiments with CPU based load-shedding\n\nWe ran a series of experiments sending a large request volume at a service which normally targets 45% CPU for auto scaling but which was prevented from scaling up for the purpose of monitoring CPU load shedding under extreme load conditions. The instances were configured to shed noncritical traffic after 60% CPU and critical traffic after 80%.\n\nAs RPS was dialed up past 6x the autoscale volume, the service was able to shed first noncritical and then critical requests. Latency remained within reasonable limits throughout, and successful RPS throughput remained stable.\n\nExperimental behavior of CPU based load-shedding using synthetic traffic.\n\nP99 latency stayed within a reasonable range throughout the experiment, even as RPS surpassed 6x the autoscale target.\n\nAnti-patterns with load-shedding\n\nAnti-pattern 1 \u2014 No shedding\n\nIn the above graphs, the limiter does a good job keeping latency low for the successful requests. If there was no shedding here, we\u2019d see latency increase for all requests, instead of a fast failure in some requests that can be retried. Further, this can result in a death spiral where one instance becomes unhealthy, resulting in more load on other instances, resulting in all instances becoming unhealthy before auto-scaling can kick in.\n\nNo load-shedding: In the absence of load-shedding, increased latency can degrade all requests instead of rejecting some requests (that can be retried), and can make instances unhealthy\n\nAnti-pattern 2 \u2014 Congestive failure\n\nAnother anti-pattern to watch out for is congestive failure or shedding too aggressively. If the load-shedding is due to an increase in traffic, the successful RPS should not drop after load-shedding. Here is an example of what congestive failure looks like:\n\nCongestive failure: After 16:57, the service starts rejecting most requests and is not able to sustain a successful 240 RPS that it was before load-shedding kicked in. This can be seen in fixed concurrency limiters or when load-shedding consumes too much CPU preventing any other work from being done\n\nWe can see in the Experiments with CPU based load-shedding section above that our load-shedding implementation avoids both these anti-patterns by keeping latency low and sustaining as much successful RPS during load-shedding as before.\n\nGeneric IO based load-shedding\n\nSome services are not CPU-bound but instead are IO-bound by backing services or datastores that can apply back pressure via increased latency when they are overloaded either in compute or in storage capacity. For these services we re-use the prioritized load shedding techniques, but we introduce new utilization measures to feed into the shedding logic. Our initial implementation supports two forms of latency based shedding in addition to standard adaptive concurrency limiters (themselves a measure of average latency):\n\nThe service can specify per-endpoint target and maximum latencies, which allow the service to shed when the service is abnormally slow regardless of backend. The Netflix storage services running on the Data Gateway return observed storage target and max latency SLO utilization, allowing services to shed when they overload their allocated storage capacity.\n\nThese utilization measures provide early warning signs that a service is generating too much load to a backend, and allow it to shed low priority work before it overwhelms that backend. The main advantage of these techniques over concurrency limits alone is they require less tuning as our services already must maintain tight latency service-level-objectives (SLOs), for example a p50 < 10ms and p100 < 500ms. So, rephrasing these existing SLOs as utilizations allows us to shed low priority work early to prevent further latency impact to high priority work. At the same time, the system will accept as much work as it can while maintaining SLO\u2019s.\n\nTo create these utilization measures, we count how many requests are processed slower than our target and maximum latency objectives, and emit the percentage of requests failing to meet those latency goals. For example, our KeyValue storage service offers a 10ms target with 500ms max latency for each namespace, and all clients receive utilization measures per data namespace to feed into their prioritized load shedding. These measures look like:\n\nutilization(namespace) = {\n\noverall = 12\n\nlatency = {\n\nslo_target = 12,\n\nslo_max = 0\n\n}\n\nsystem = {\n\nstorage = 17,\n\ncompute = 10,\n\n}\n\n}\n\nIn this case, 12% of requests are slower than the 10ms target, 0% are slower than the 500ms max latency (timeout), and 17% of allocated storage is utilized. Different use cases consult different utilizations in their prioritized shedding, for example batches that write data daily may get shed when system storage utilization is approaching capacity as writing more data would create further instability.\n\nAn example where the latency utilization is useful is for one of our critical file origin services which accepts writes of new files in the AWS cloud and acts as an origin (serves reads) for those files to our Open Connect CDN infrastructure. Writes are the most critical and should never be shed by the service, but when the backing datastore is getting overloaded, it is reasonable to progressively shed reads to files which are less critical to the CDN as it can retry those reads and they do not affect the product experience.\n\nTo achieve this goal, the origin service configured a KeyValue latency based limiter that starts shedding reads to files which are less critical to the CDN when the datastore reports a target latency utilization exceeding 40%. We then stress tested the system by generating over 50Gbps of read traffic, some of it to high priority files and some of it to low priority files:", "label": 0}
{"title": "November 2023", "url": "https://lifeofpablo.com/blog/published:2023-11", "content": "I've been looking forward for this month's Indieweb Carnival on Community and Belonging hosted by Alex Sirac. Merci Alex pour ta pr\u00e9sentation sur la sujet de communaut\u00e9.\n\nCommunity and belonging for me means, a place I feel that I belong and a place where I can be accepted for who I am. It's been hard to integrate into some communities. Often, being the only person color in a community has been difficult at times.We enter and leave communities as we older. Some communities change for the greater good and others change not for the greater good. I belong to different communities. My background and life experiences have influenced who I've become and the things I've been interested in. I've entered new ones while I've left others. Some communities I can come back and be able to jump right in right where I left off. There are communities where it was only during a certain period of my life and might be harder to be in now. There are communities where I've had to leave because it no longer followed who I was as a person or it wasn't good for me. Some became dangerous in their vission\n\nI'd like to talk about community and belonging in these three (3) situations or places.\n\nBeing a Multilingual\n\nBeing a Student in France\n\nIndieWeb\n\nI'd like to write about these topics in this carnival but I'd like to take make a dedicated post with a slightly different approach.\n\nBeing Mexican and American\n\nGrowing up in Nebraska\n\nBeing Multilingual\n\nHere in the United States, it's not common for people to speak another language. Which is a shame. Growing up in a small town in Nebraska, it often seemed that it was few of us who would consistently speak another language. Even at a young age, I felt we had a small unintentional community. I was very fortunate to grow up bilingual - Spanish & English (in this order). Then I learned French in my teens.\n\nBeing a multilingual here in the United States has allowed me to be join new communities. With the amount of people who don't speak at least a second language on a constant basis, it almost feels exclusive to be multilingual. It allows me to have deeper connections with others. It allows me to belong in various communities. When I was living in Omaha, One example is attending French speaking events at Alliance Fran\u00e7aise and on Meet-Up. It was so amazing that I could find so many individuals who I can speak French with and share many of the world views. It was nice to meet so many francophiles and francophones who enjoyed sharing culture, language, cuisine, wine and so much more. I would become great friends with these individuals. Now that I live far away, I know if I were to visit or move back, I could rejoin this community. Going back to a place where I'm a visitor and one I am familiar with, I know I can find a place of belonging again.\n\nGrowing up in a place that lacked lingual diversity, or to simply put it - growing up in a very vanilla place, it also seemed that there was a community that didn't seem to like people speaking different languages. It seemed to alienate them. There seemed to be pressure to join the community where one needed to behave and present themselves with the majority. I know people who had my cultural background give into that community and not speak their language spoken at home. Parents gave in the idea that there kids will be more valuable in society if they don't speak another language. I felt supported and limited by these co-existing communities. I'd be lying if at one point I didn't fall in this negative community. I felt at times I needed this negative community to to belong. Learning to belong to something that conflict with my values felt like being pulled in both directions.\n\nBeing a Student in France\n\nI have very fond memories of being an exchange student in France. I studied at the Universit\u00e9 de Strasbourg at the age 22. Being a student in France was different compared the attending university in the United States. Being in University you build Not only was I apart of the exchange student community but I was also a student trying to improve my French abilities. I was the only student from the university back home who attended a program in France. I isolated myself the Americans. In a way I left the \"American\" community for many months. I saw this as a blessing to form new habits and quickly find a new community or communities. We were here to learn the French language. We all were from various parts of the world. I was hanging out with all the people and students from France, the rest of the European Union, South America, and basically anyone from around the world. Just not the Americans\n\nWe were homesick. We bonded with food. We made meals from our respective cultures. We would learn about their upbringing and how they got to France. We all bonded together because many of us had experiences with immigrant culture. Many of these students were also immigrants to France or have been in the European Union already. They were trying establish themselves and incorporate themselves in the society. Seeing them everyday reminded me of my parents telling me stories of them trying to make their way in the United States. I understand being in a new place and trying to find a sense of belonging can be daunting.\n\nYes, I could do many of these things back at my home university. There was such a deeper connection with these international students. We were living and studying in a place where being a student meant so much more. I felt I could have genuine conversations with people. I really felt like I belonged here and understand what it's like to be student outside the United States. Student community here is nothing like in the United States.\n\nIndieWeb\n\nOh the IndieWeb! I joined back in March 2023. I heard of the IndieWeb throughout the years but didn't think much of it. I'm happy that I got the interest and courage to join. This is a community that I am truly happy to have found. It has become a community I truly feel that I am apart of. The best part of it is getting to meet so many great people and interacting with them on a weekly basis. People are so willing to help each other out. I've had people reach out if there is something wrong with my website or microformats are not placed correctly. It's been pretty rad attending Homebrew Website Club. Every single one of them has been pretty amazing. I really enjoy the encouragement of taking initiative to as planning events, starting writing carnivals, etc. It's a pretty open group.\n\nThe IndieWeb has helped me grow as an individual. It has also rekindled the fire in things I didn't think I would find interest again. One example of that is blogging. It This community has helped me find a new place of belonging.\n\nI will admit since I'm still relatively new to the community, I'm still a little shy and still learning the ropes. I know that this is part of the journey.\n\nConclusion\n\nI'm happy with all these experiences.", "label": 1}
{"title": "Improve Your Next Experiment by Learning Better Proxy Metrics From Past Experiments", "url": "https://netflixtechblog.com/improve-your-next-experiment-by-learning-better-proxy-metrics-from-past-experiments-64c786c2a3ac?source=collection_home---4------18-----------------------", "content": "We are excited to share our work on how to learn good proxy metrics from historical experiments at KDD 2024. This work addresses a fundamental question for technology companies and academic researchers alike: how do we establish that a treatment that improves short-term (statistically sensitive) outcomes also improves long-term (statistically insensitive) outcomes? Or, faced with multiple short-term outcomes, how do we optimally trade them off for long-term benefit?\n\nFor example, in an A/B test, you may observe that a product change improves the click-through rate. However, the test does not provide enough signal to measure a change in long-term retention, leaving you in the dark as to whether this treatment makes users more satisfied with your service. The click-through rate is a proxy metric (S, for surrogate, in our paper) while retention is a downstream business outcome or north star metric (Y). We may even have several proxy metrics, such as other types of clicks or the length of engagement after click. Taken together, these form a vector of proxy metrics.\n\nThe goal of our work is to understand the true relationship between the proxy metric(s) and the north star metric \u2014 so that we can assess a proxy\u2019s ability to stand in for the north star metric, learn how to combine multiple metrics into a single best one, and better explore and compare different proxies.\n\nSeveral intuitive approaches to understanding this relationship have surprising pitfalls:\n\nLooking only at user-level correlations between the proxy S and north star Y. Continuing the example from above, you may find that users with a higher click-through rate also tend to have a higher retention. But this does not mean that a product change that improves the click-through rate will also improve retention (in fact, promoting clickbait may have the opposite effect). This is because, as any introductory causal inference class will tell you, there are many confounders between S and Y \u2014 many of which you can never reliably observe and control for.\n\nContinuing the example from above, you may find that users with a higher click-through rate also tend to have a higher retention. But this does not mean that a product change that improves the click-through rate will also improve retention (in fact, promoting clickbait may have the opposite effect). This is because, as any introductory causal inference class will tell you, there are many confounders between S and Y \u2014 many of which you can never reliably observe and control for. Looking naively at treatment effect correlations between S and Y. Suppose you are lucky enough to have many historical A/B tests. Further imagine the ordinary least squares (OLS) regression line through a scatter plot of Y on S in which each point represents the (S,Y)-treatment effect from a previous test. Even if you find that this line has a positive slope, you unfortunately cannot conclude that product changes that improve S will also improve Y. The reason for this is correlated measurement error \u2014 if S and Y are positively correlated in the population, then treatment arms that happen to have more users with high S will also have more users with high Y.\n\nBetween these naive approaches, we find that the second one is the easier trap to fall into. This is because the dangers of the first approach are well-known, whereas covariances between estimated treatment effects can appear misleadingly causal. In reality, these covariances can be severely biased compared to what we actually care about: covariances between true treatment effects. In the extreme \u2014 such as when the negative effects of clickbait are substantial but clickiness and retention are highly correlated at the user level \u2014 the true relationship between S and Y can be negative even if the OLS slope is positive. Only more data per experiment could diminish this bias \u2014 using more experiments as data points will only yield more precise estimates of the badly biased slope. At first glance, this would appear to imperil any hope of using existing experiments to detect the relationship.\n\nThis figure shows a hypothetical treatment effect covariance matrix between S and Y (white line; negative correlation), a unit-level sampling covariance matrix creating correlated measurement errors between these metrics (black line; positive correlation), and the covariance matrix of estimated treatment effects which is a weighted combination of the first two (orange line; no correlation).\n\nTo overcome this bias, we propose better ways to leverage historical experiments, inspired by techniques from the literature on weak instrumental variables. More specifically, we show that three estimators are consistent for the true proxy/north-star relationship under different constraints (the paper provides more details and should be helpful for practitioners interested in choosing the best estimator for their setting):\n\nA Total Covariance (TC) estimator allows us to estimate the OLS slope from a scatter plot of true treatment effects by subtracting the scaled measurement error covariance from the covariance of estimated treatment effects. Under the assumption that the correlated measurement error is the same across experiments (homogeneous covariances), the bias of this estimator is inversely proportional to the total number of units across all experiments, as opposed to the number of members per experiment.\n\nestimator allows us to estimate the OLS slope from a scatter plot of true treatment effects by subtracting the scaled measurement error covariance from the covariance of estimated treatment effects. Under the assumption that the correlated measurement error is the same across experiments (homogeneous covariances), the bias of this estimator is inversely proportional to the total number of units across all experiments, as opposed to the number of members per experiment. Jackknife Instrumental Variables Estimation (JIVE) converges to the same OLS slope as the TC estimator but does not require the assumption of homogeneous covariances. JIVE eliminates correlated measurement error by removing each observation\u2019s data from the computation of its instrumented surrogate values.\n\nconverges to the same OLS slope as the TC estimator but does not require the assumption of homogeneous covariances. JIVE eliminates correlated measurement error by removing each observation\u2019s data from the computation of its instrumented surrogate values. A Limited Information Maximum Likelihood (LIML) estimator is statistically efficient as long as there are no direct effects between the treatment and Y (that is, S fully mediates all treatment effects on Y). We find that LIML is highly sensitive to this assumption and recommend TC or JIVE for most applications.\n\nOur methods yield linear structural models of treatment effects that are easy to interpret. As such, they are well-suited to the decentralized and rapidly-evolving practice of experimentation at Netflix, which runs thousands of experiments per year on many diverse parts of the business. Each area of experimentation is staffed by independent Data Science and Engineering teams. While every team ultimately cares about the same north star metrics (e.g., long-term revenue), it is highly impractical for most teams to measure these in short-term A/B tests. Therefore, each has also developed proxies that are more sensitive and directly relevant to their work (e.g., user engagement or latency). To complicate matters more, teams are constantly innovating on these secondary metrics to find the right balance of sensitivity and long-term impact.\n\nIn this decentralized environment, linear models of treatment effects are a highly useful tool for coordinating efforts around proxy metrics and aligning them towards the north star:\n\nManaging metric tradeoffs. Because experiments in one area can affect metrics in another area, there is a need to measure all secondary metrics in all tests, but also to understand the relative impact of these metrics on the north star. This is so we can inform decision-making when one metric trades off against another metric. Informing metrics innovation. To minimize wasted effort on metric development, it is also important to understand how metrics correlate with the north star \u201cnet of\u201d existing metrics. Enabling teams to work independently. Lastly, teams need simple tools in order to iterate on their own metrics. Teams may come up with dozens of variations of secondary metrics, and slow, complicated tools for evaluating these variations are unlikely to be adopted. Conversely, our models are easy and fast to fit, and are actively used to develop proxy metrics at Netflix.\n\nWe are thrilled about the research and implementation of these methods at Netflix \u2014 while also continuing to strive for great and always better, per our culture. For example, we still have some way to go to develop a more flexible data architecture to streamline the application of these methods within Netflix. Interested in helping us? See our open job postings!\n\nFor feedback on this blog post and for supporting and making this work better, we thank Apoorva Lal, Martin Tingley, Patric Glynn, Richard McDowell, Travis Brooks, and Ayal Chen-Zion.", "label": 0}
{"title": "How Stripe builds interactive docs with Markdoc", "url": "https://stripe.com/blog/markdoc", "content": "At Stripe, our product docs are designed to feel like an application rather than a traditional user manual. For example, we incorporate a user's own API test key into code samples, making it possible to copy and paste code that seamlessly works with the user's own account. We have client-side interactivity, like checklists and collapsible sections. We tailor the content to the individual user, conditionally displaying content based on their location or the Stripe features they use. These features result in a high-quality user experience that reduces friction and contributes to the success of developers.\n\nFor these capabilities to have the desired impact we have to make it easy for writers to use them in their content. Delivering a good user experience without compromising the authoring experience required us to develop an authoring format that enables writers to express interactivity and simple page logic without mixing code and content.\n\nOver several years, we learned how to balance interactivity, customization, and authoring productivity while undertaking a major overhaul of our documentation platform.\n\nPast is prologue\n\nTo understand how we got here it's important to understand where we started. The legacy documentation platform that we replaced was a monolithic Ruby application built with ERB templates and Sinatra routing. The content freely mixed HTML, Markdown, Ruby, and ERB helper functions.\n\nMixing code and content provided a natural way to programmatically tailor the docs to the developer, but it posed serious challenges to quality and maintainability when the body of content grew to hundreds of pages. Alongside the technical burden of maintaining code within the content, the behavior of the code can make the content itself harder to understand and manipulate safely, particularly when used by many different teams with different objectives, timetables, and areas of expertise. Content authoring effectively became software development, and with that became subject to the same technical complexity and overhead.\n\nWe wanted to introduce new content formats with significantly more interactivity and more sophisticated frontend logic, but we knew that the limitations of a code-first approach would prevent us from using these features widely. For example, our integration builder format, which was originally created as a React application with content authored in JSON, became much easier for technical writers to reproduce and maintain when it was migrated to use Markdoc for the authoring experience.\n\nDesigning Markdoc\n\nWhen we began building our current documentation platform, we wanted to simplify our authoring experience by adopting an intuitive format like Markdown. Although Markdown is significantly easier to read and reason about than ERB templates, its simplicity also imposes limitations that make it challenging to use for rich content like our product docs.\n\nMarkdown is a relatively flat format that isn't designed to express complex structure or hierarchy. It offers a small number of formatting features and provides limited control over presentation. It does not have exotic templating features like support for custom page logic, variables, conditionals, or content reuse. Markdown's enduring success and relative ubiquity are largely due to its intentionally narrow scope and the restraint exercised in its design. It is easy and enjoyable to use because it prioritizes readability and leans heavily on intuitive plain-text authoring conventions.\n\nOur custom authoring format, called Markdoc, was designed to decouple code and content while enforcing proper discipline at the boundaries. Instead of allowing each page to be treated like an open-ended application, it imposes constraints on styling and programming, providing prescriptive rails for content extensibility. It extends Markdown with custom syntax that meets the needs of our documentation platform without sacrificing Markdown's simplicity, familiarity, or ease of use for writing prose. Following the ethos and design sensibility of Markdown, Markdoc keeps the overall surface area of new features small by adding a few highly-composable primitives that can be used together to express all the functionality we need.\n\nMarkdoc provides an extensible system for defining custom tags that can be used seamlessly in Markdown content. Using the custom tag syntax, we're able to support features like conditional content, content inclusion, and variable interpolation.\n\n# This is a heading {% #section %} {% callout type=\"info\" %} This is a paragraph with *formatted* text inside the callout {% /callout %} {% if $condition %} This content shows if `$condition` is true. {% /if %} ~\n\nThe features we decided to leave out of Markdoc in order to protect content maintainability are a critical aspect of its design. For example, when deciding what built-in flow control to include in Markdoc, we deliberately chose not to include looping. We wanted to discourage writers from performing procedural content generation from inside of a document, forcing them to move it outside of the system for better encapsulation. We also decided to leave out variable assignment in order to ensure that the content is fully stateless, thus eliminating an entire class of potential bugs.\n\nI like Markdoc because it lets us still do anything we want with code in the docs without bogging down the content authoring experience. If we need some new component, designers and engineers can whip that up. So as a writer, I can work in the docs content and stay focused. Lucie Lozinski, Technical Writer\n\nReact integration\n\nMarkdoc has a modular rendering system that supports multiple output formats. Using Markdoc\u2019s React renderer, a Markdoc document can be rendered to a React virtual DOM. Custom Markdoc tags can be configured to output React components, passing through tag attributes as React props. Markdoc also supports assigning custom React components to standard Markdown document nodes such as headings and paragraphs.\n\nDefining custom Markdoc tags that output React components makes it possible to include interactive features, like tab switchers and collapsible content sections, inside of documents. Using custom tags to express these features helps create a writer-friendly interface for the functionality.\n\nThe React ecosystem also has a wealth of useful and interesting libraries that we can incorporate into our documentation to enrich presentation. For example, we're using the React Flow library to create interactive diagrams in our documentation. We defined a set of Markdoc tags for expressing the contents of a diagram, making it easy for writers to build beautiful and consistent visual representations of APIs and technical concepts from a set of composable elements.\n\n{% diagram type=\"sequence\" description=\"Usage-based billing\" %} {% node #customer icon=\"customer\" %} Customer {% /node %} {% node #typographic icon=\"platform\" %} Typographic {% /node %} {% node #stripe icon=\"stripe\" %} Stripe {% /node %} {% edge from=\"customer\" to=\"typographic\" %} Select plan {% /edge %} {% edge from=\"typographic\" to=\"typographic\" %} Create [usage records](/docs/api/usage_records) {% /edge %} ~\n\nUnlike static images, the diagrams that are built with React Flow can easily incorporate interactivity and clickable links. They are also easier to localize and can be restyled universally.\n\nMoving our documentation frontend to React was an important goal of our platform overhaul. Stripe already used React across many parts of the user experience, including our API reference docs and user dashboard. Enabling integration and cross-pollination between those surfaces and the product docs opens up a lot of exciting opportunities for future innovation, like showing API reference overlays when the reader hovers their cursor over a function or parameter in a code example. Sharing a common set of components from Stripe's internal design pattern library helps improve cohesion.\n\nReact also offers some compelling technical advantages. The implementation of interactive frontend components in our legacy stack was split between markup ERB templates and logic written in JavaScript which made it difficult to properly encapsulate, extend, and reuse functionality\u2014a set of problems that modern component-based frontend frameworks address in a more satisfying way.\n\nMarkdoc comes with two distinct React renderers: a renderer that dynamically builds the React virtual DOM tree on the client side, and a static renderer that transpiles the document to JavaScript code. We use the dynamic renderer in our documentation platform at Stripe, but the static renderer is useful in cases where you want to treat a piece of Markdoc content as though it is a React component or JavaScript module. For example, the static renderer makes it possible to implement a Markdoc loader for Webpack in only five lines of code.\n\nconst Markdoc = require('@markdoc/markdoc'); module.exports = function (source) { const {schema} = this.getOptions() || {}; const ast = Markdoc.parse(source); const transformed = Markdoc.transform(ast, schema); const output = Markdoc.renderers.reactStatic(transformed); return `import React from 'react'; export default ${output}`; }; ~\n\nModular rendering\n\nAlongside the React renderers, Markdoc also includes a string-based HTML renderer that can be used for conventional server-side rendering or integration with standards-based Web Components. Markdoc's modular rendering architecture makes it possible for third parties to build custom renderers for additional frameworks and systems.\n\nMarkdoc content is entirely agonistic with respect to the technology used to present the rendered document. Fully decoupling rendering from the document format gives us the flexibility to present the same content in radically different ways in the future\u2014like incorporating it into a native mobile application or generating a print-ready output format such as a PDF. I even used Markdoc to make the slide deck for my presentation at the Write the Docs conference back in 2020. The Markdoc community has already started bringing support to other frameworks, including Vue and Svelte.\n\nEnsuring that rendering implementation details don't bleed into the content also helps to improve the authoring experience, avoiding complexity and simplifying maintainability.\n\nDocumentation as data\n\nMarkdoc's fully declarative syntax parses to an Abstract Syntax Tree (AST), a data structure that represents the content of the document. We can take advantage of the AST to perform advanced static analysis and programmatically manipulate our content.\n\nMarkdoc lets us treat our documentation like data, writing simple scripts to programmatically inspect the content. If we want to perform tasks like identifying all of the fenced code blocks that contain a specific string or all of the places where we have a heading nested inside of a callout, we can do that robustly with the AST instead of relying on text scraping and regular expressions.\n\nWe are building automated refactoring tools that use the AST, making it possible to perform complex edits across our entire body of content with a higher degree of robustness than old-fashioned find-and-replace.\n\nOne of the most important ways that we use the AST today is for validation. For every Markdoc tag and document node type, there's a schema definition specifying the names and types of the attributes it accepts, what kind of document nodes can be nested inside of it as children, and other relevant metadata. The Markdoc validator uses this information to verify the correctness of a given Markdoc document.\n\nSchemas can also include arbitrary logic that analyzes the document nodes and returns custom errors. We use this to support features like link validation, checking to make sure that every link between pages within our documentation points to a valid route. It can also be useful for enforcing certain style guidelines that relate to the document structure, like preventing authors from using the wrong heading levels in certain places.\n\nWe run the Markdoc validator in our continuous integration system to ensure correctness at build time, but we also have an internal Visual Studio Code extension that exposes validation errors in real time while the user is typing.\n\nMarkdoc makes it easy for me to build rich, interactive experiences around documentation, then surface that capability to other authors through a simple declarative interface. John O'Sullivan, Software Engineer\n\nUnder the hood\n\nMarkdoc's parser is written in JavaScript and built on top of a popular open-source Markdown library called markdown-it. Markdoc is relatively lightweight\u2014the markdown-it library is its only direct dependency. It is intended to run in Node.js and similar server-side JavaScript environments, but it can also be bundled for use in the browser.\n\nMarkdoc uses markdown-it as a tokenizer, building its AST from the array of tokens emitted by markdown-it. Parsing logic for Markdoc's custom tag syntax is generated from a peg.js grammar and integrates with markdown-it via a plugin.\n\nMarkdoc has its own dedicated rendering architecture rather than relying on markdown-it to generate its output. Developing an independent rendering system was necessary in order to handle Markdoc's custom tags and support multiple output formats.\n\nMarkdoc rendering is performed in several phases. First, the variable resolution step converts all of the variables in the document into their corresponding values. Next, the transformation step recursively walks through the document nodes in the AST and uses the node and tag schema definitions to generate a tree of renderable document nodes\u2014a data structure that corresponds with the shape of the rendered output. Finally, the tree of renderable document nodes is passed into the desired renderer, which emits the actual rendered output.\n\nMarkdoc document's AST can be serialized to JSON and cached for later use, improving performance by obviating the need to parse the document every time it is rendered. Our product documentation platform at Stripe maintains an in-memory cache of the AST at runtime, but we are considering moving to an architecture where we serialize the AST at build time in order to eliminate runtime Markdoc parsing entirely.\n\nWhen I first started using Markdoc at Stripe, I was delighted by how easy it was to structure docs exactly as I envisioned them. With other authoring tools, useful visual elements like collapsible sections, asides, tabs, tables, multiple-language code samples, and many more often required heavy customization or new development. With Markdoc, I have a full palette ready to use. After using Markdoc, it's hard to imagine going back to another authoring tool. Ryan Young, Technical Writer\n\nMay the source be with you\n\nOur team at Stripe spends a lot of time thinking about the authoring experience and how to get it right. In many ways, Markdoc is the embodiment of our obsession with building a better authoring experience. It's our way of bottling up everything we have learned about this topic and sharing it in a reproducible way.\n\nAfter migrating all of our content to Markdoc and seeing the advantages fully realized in production, we set out to make Markdoc available under an MIT license so that others could benefit from our efforts.\n\nWe released Markdoc to the public in May, publishing a package on npm. We also published a draft specification that formally describes the Markdoc tag syntax, with the aim of making it easier for developers to incorporate support for Markdoc tags into other Markdoc parsing libraries.\n\nMarkdoc is hardly the final word on content authoring, but we hope that our contribution to the dialog will inspire others and help elevate discussion about the importance of the authoring experience in documentation.\n\nInterested in using Markdoc at work? Let us know how we can help.", "label": 0}
{"title": "Introducing Netflix\u2019s TimeSeries Data Abstraction Layer", "url": "https://netflixtechblog.com/introducing-netflix-timeseries-data-abstraction-layer-31552f6326f8?source=collection_home---4------13-----------------------", "content": "Introducing Netflix\u2019s TimeSeries Data Abstraction Layer Netflix Technology Blog 18 min read \u00b7 Oct 8, 2024 -- 16 Listen Share\n\nBy Rajiv Shringi, Vinay Chella, Kaidan Fullerton, Oleksii Tkachuk, Joey Lynch\n\nIntroduction\n\nAs Netflix continues to expand and diversify into various sectors like Video on Demand and Gaming, the ability to ingest and store vast amounts of temporal data \u2014 often reaching petabytes \u2014 with millisecond access latency has become increasingly vital. In previous blog posts, we introduced the Key-Value Data Abstraction Layer and the Data Gateway Platform, both of which are integral to Netflix\u2019s data architecture. The Key-Value Abstraction offers a flexible, scalable solution for storing and accessing structured key-value data, while the Data Gateway Platform provides essential infrastructure for protecting, configuring, and deploying the data tier.\n\nBuilding on these foundational abstractions, we developed the TimeSeries Abstraction \u2014 a versatile and scalable solution designed to efficiently store and query large volumes of temporal event data with low millisecond latencies, all in a cost-effective manner across various use cases.\n\nIn this post, we will delve into the architecture, design principles, and real-world applications of the TimeSeries Abstraction, demonstrating how it enhances our platform\u2019s ability to manage temporal data at scale.\n\nNote: Contrary to what the name may suggest, this system is not built as a general-purpose time series database. We do not use it for metrics, histograms, timers, or any such near-real time analytics use case. Those use cases are well served by the Netflix Atlas telemetry system. Instead, we focus on addressing the challenge of storing and accessing extremely high-throughput, immutable temporal event data in a low-latency and cost-efficient manner.\n\nChallenges\n\nAt Netflix, temporal data is continuously generated and utilized, whether from user interactions like video-play events, asset impressions, or complex micro-service network activities. Effectively managing this data at scale to extract valuable insights is crucial for ensuring optimal user experiences and system reliability.\n\nHowever, storing and querying such data presents a unique set of challenges:\n\nHigh Throughput : Managing up to 10 million writes per second while maintaining high availability.\n\n: Managing up to 10 million writes per second while maintaining high availability. Efficient Querying in Large Datasets : Storing petabytes of data while ensuring primary key reads return results within low double-digit milliseconds, and supporting searches and aggregations across multiple secondary attributes.\n\n: Storing petabytes of data while ensuring primary key reads return results within low double-digit milliseconds, and supporting searches and aggregations across multiple secondary attributes. Global Reads and Writes : Facilitating read and write operations from anywhere in the world with adjustable consistency models.\n\n: Facilitating read and write operations from anywhere in the world with adjustable consistency models. Tunable Configuration : Offering the ability to partition datasets in either a single-tenant or multi-tenant datastore, with options to adjust various dataset aspects such as retention and consistency.\n\n: Offering the ability to partition datasets in either a single-tenant or multi-tenant datastore, with options to adjust various dataset aspects such as retention and consistency. Handling Bursty Traffic : Managing significant traffic spikes during high-demand events, such as new content launches or regional failovers.\n\n: Managing significant traffic spikes during high-demand events, such as new content launches or regional failovers. Cost Efficiency: Reducing the cost per byte and per operation to optimize long-term retention while minimizing infrastructure expenses, which can amount to millions of dollars for Netflix.\n\nTimeSeries Abstraction\n\nThe TimeSeries Abstraction was developed to meet these requirements, built around the following core design principles:\n\nPartitioned Data : Data is partitioned using a unique temporal partitioning strategy combined with an event bucketing approach to efficiently manage bursty workloads and streamline queries.\n\n: Data is partitioned using a unique temporal partitioning strategy combined with an event bucketing approach to efficiently manage bursty workloads and streamline queries. Flexible Storage : The service is designed to integrate with various storage backends, including Apache Cassandra and Elasticsearch, allowing Netflix to customize storage solutions based on specific use case requirements.\n\n: The service is designed to integrate with various storage backends, including Apache Cassandra and Elasticsearch, allowing Netflix to customize storage solutions based on specific use case requirements. Configurability : TimeSeries offers a range of tunable options for each dataset, providing the flexibility needed to accommodate a wide array of use cases.\n\n: TimeSeries offers a range of tunable options for each dataset, providing the flexibility needed to accommodate a wide array of use cases. Scalability : The architecture supports both horizontal and vertical scaling, enabling the system to handle increasing throughput and data volumes as Netflix expands its user base and services.\n\n: The architecture supports both horizontal and vertical scaling, enabling the system to handle increasing throughput and data volumes as Netflix expands its user base and services. Sharded Infrastructure: Leveraging the Data Gateway Platform, we can deploy single-tenant and/or multi-tenant infrastructure with the necessary access and traffic isolation.\n\nLet\u2019s dive into the various aspects of this abstraction.\n\nData Model\n\nWe follow a unique event data model that encapsulates all the data we want to capture for events, while allowing us to query them efficiently.\n\nLet\u2019s start with the smallest unit of data in the abstraction and work our way up.\n\nEvent Item : An event item is a key-value pair that users use to store data for a given event. For example: {\u201cdevice_type\u201d: \u201cios\u201d}.\n\n: An event item is a key-value pair that users use to store data for a given event. For example: {\u201cdevice_type\u201d: \u201cios\u201d}. Event : An event is a structured collection of one or more such event items. An event occurs at a specific point in time and is identified by a client-generated timestamp and an event identifier (such as a UUID). This combination of event_time and event_id also forms part of the unique idempotency key for the event, enabling users to safely retry requests.\n\n: An event is a structured collection of one or more such event items. An event occurs at a specific point in time and is identified by a client-generated timestamp and an event identifier (such as a UUID). This combination of and also forms part of the unique idempotency key for the event, enabling users to safely retry requests. Time Series ID : A time_series_id is a collection of one or more such events over the dataset\u2019s retention period. For instance, a device_id would store all events occurring for a given device over the retention period. All events are immutable, and the TimeSeries service only ever appends events to a given time series ID.\n\n: A is a collection of one or more such events over the dataset\u2019s retention period. For instance, a would store all events occurring for a given device over the retention period. All events are immutable, and the TimeSeries service only ever appends events to a given time series ID. Namespace: A namespace is a collection of time series IDs and event data, representing the complete TimeSeries dataset. Users can create one or more namespaces for each of their use cases. The abstraction applies various tunable options at the namespace level, which we will discuss further when we explore the service\u2019s control plane.\n\nAPI\n\nThe abstraction provides the following APIs to interact with the event data.\n\nWriteEventRecordsSync: This endpoint writes a batch of events and sends back a durability acknowledgement to the client. This is used in cases where users require a guarantee of durability.\n\nWriteEventRecords: This is the fire-and-forget version of the above endpoint. It enqueues a batch of events without the durability acknowledgement. This is used in cases like logging or tracing, where users care more about throughput and can tolerate a small amount of data loss.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"events\": [\n\n{\n\n\"timeSeriesId\": \"profile100\",\n\n\"eventTime\": \"2024-10-03T21:24:23.988Z\",\n\n\"eventId\": \"550e8400-e29b-41d4-a716-446655440000\",\n\n\"eventItems\": [\n\n{\n\n\"eventItemKey\": \"deviceType\",\n\n\"eventItemValue\": \"aW9z\"\n\n},\n\n{\n\n\"eventItemKey\": \"deviceMetadata\",\n\n\"eventItemValue\": \"c29tZSBtZXRhZGF0YQ==\"\n\n}\n\n]\n\n},\n\n{\n\n\"timeSeriesId\": \"profile100\",\n\n\"eventTime\": \"2024-10-03T21:23:30.000Z\",\n\n\"eventId\": \"123e4567-e89b-12d3-a456-426614174000\",\n\n\"eventItems\": [\n\n{\n\n\"eventItemKey\": \"deviceType\",\n\n\"eventItemValue\": \"YW5kcm9pZA==\"\n\n}\n\n]\n\n}\n\n]\n\n}\n\nReadEventRecords: Given a combination of a namespace, a timeSeriesId, a timeInterval, and optional eventFilters, this endpoint returns all the matching events, sorted descending by event_time, with low millisecond latency.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"timeSeriesId\": \"profile100\",\n\n\"timeInterval\": {\n\n\"start\": \"2024-10-02T21:00:00.000Z\",\n\n\"end\": \"2024-10-03T21:00:00.000Z\"\n\n},\n\n\"eventFilters\": [\n\n{\n\n\"matchEventItemKey\": \"deviceType\",\n\n\"matchEventItemValue\": \"aW9z\"\n\n}\n\n],\n\n\"pageSize\": 100,\n\n\"totalRecordLimit\": 1000\n\n}\n\nSearchEventRecords: Given a search criteria and a time interval, this endpoint returns all the matching events. These use cases are fine with eventually consistent reads.\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"timeInterval\": {\n\n\"start\": \"2024-10-02T21:00:00.000Z\",\n\n\"end\": \"2024-10-03T21:00:00.000Z\"\n\n},\n\n\"searchQuery\": {\n\n\"booleanQuery\": {\n\n\"searchQuery\": [\n\n{\n\n\"equals\": {\n\n\"eventItemKey\": \"deviceType\",\n\n\"eventItemValue\": \"aW9z\"\n\n}\n\n},\n\n{\n\n\"range\": {\n\n\"eventItemKey\": \"deviceRegistrationTimestamp\",\n\n\"lowerBound\": {\n\n\"eventItemValue\": \"MjAyNC0xMC0wMlQwMDowMDowMC4wMDBa\",\n\n\"inclusive\": true\n\n},\n\n\"upperBound\": {\n\n\"eventItemValue\": \"MjAyNC0xMC0wM1QwMDowMDowMC4wMDBa\"\n\n}\n\n}\n\n}\n\n],\n\n\"operator\": \"AND\"\n\n}\n\n},\n\n\"pageSize\": 100,\n\n\"totalRecordLimit\": 1000\n\n}\n\nAggregateEventRecords: Given a search criteria and an aggregation mode (e.g. DistinctAggregation) , this endpoint performs the given aggregation within a given time interval. Similar to the Search endpoint, users can tolerate eventual consistency and a potentially higher latency (in seconds).\n\n{\n\n\"namespace\": \"my_dataset\",\n\n\"timeInterval\": {\n\n\"start\": \"2024-10-02T21:00:00.000Z\",\n\n\"end\": \"2024-10-03T21:00:00.000Z\"\n\n},\n\n\"searchQuery\": {...some search criteria...},\n\n\"aggregationQuery\": {\n\n\"distinct\": {\n\n\"eventItemKey\": \"deviceType\",\n\n\"pageSize\": 100\n\n}\n\n}\n\n}\n\nIn the subsequent sections, we will talk about how we interact with this data at the storage layer.\n\nStorage Layer\n\nThe storage layer for TimeSeries comprises a primary data store and an optional index data store. The primary data store ensures data durability during writes and is used for primary read operations, while the index data store is utilized for search and aggregate operations. At Netflix, Apache Cassandra is the preferred choice for storing durable data in high-throughput scenarios, while Elasticsearch is the preferred data store for indexing. However, similar to our approach with the API, the storage layer is not tightly coupled to these specific data stores. Instead, we define storage API contracts that must be fulfilled, allowing us the flexibility to replace the underlying data stores as needed.\n\nPrimary Datastore\n\nIn this section, we will talk about how we leverage Apache Cassandra for TimeSeries use cases.\n\nPartitioning Scheme\n\nAt Netflix\u2019s scale, the continuous influx of event data can quickly overwhelm traditional databases. Temporal partitioning addresses this challenge by dividing the data into manageable chunks based on time intervals, such as hourly, daily, or monthly windows. This approach enables efficient querying of specific time ranges without the need to scan the entire dataset. It also allows Netflix to archive, compress, or delete older data efficiently, optimizing both storage and query performance. Additionally, this partitioning mitigates the performance issues typically associated with wide partitions in Cassandra. By employing this strategy, we can operate at much higher disk utilization, as it reduces the need to reserve large amounts of disk space for compactions, thereby saving costs.\n\nHere is what it looks like :\n\nTime Slice: A time slice is the unit of data retention and maps directly to a Cassandra table. We create multiple such time slices, each covering a specific interval of time. An event lands in one of these slices based on the event_time. These slices are joined with no time gaps in between, with operations being start-inclusive and end-exclusive, ensuring that all data lands in one of the slices. By utilizing these time slices, we can efficiently implement retention by dropping entire tables, which reduces storage space and saves on costs.\n\nWhy not use row-based Time-To-Live (TTL)?\n\nUsing TTL on individual events would generate a significant number of tombstones in Cassandra, degrading performance, especially during range scans. By employing discrete time slices and dropping them, we avoid the tombstone issue entirely. The tradeoff is that data may be retained slightly longer than necessary, as an entire table\u2019s time range must fall outside the retention window before it can be dropped. Additionally, TTLs are difficult to adjust later, whereas TimeSeries can extend the dataset retention instantly with a single control plane operation.\n\nTime Buckets: Within a time slice, data is further partitioned into time buckets. This facilitates effective range scans by allowing us to target specific time buckets for a given query range. The tradeoff is that if a user wants to read the entire range of data over a large time period, we must scan many partitions. We mitigate potential latency by scanning these partitions in parallel and aggregating the data at the end. In most cases, the advantage of targeting smaller data subsets outweighs the read amplification from these scatter-gather operations. Typically, users read a smaller subset of data rather than the entire retention range.\n\nEvent Buckets: To manage extremely high-throughput write operations, which may result in a burst of writes for a given time series within a short period, we further divide the time bucket into event buckets. This prevents overloading the same partition for a given time range and also reduces partition sizes further, albeit with a slight increase in read amplification.\n\nNote: With Cassandra 4.x onwards, we notice a substantial improvement in the performance of scanning a range of data in a wide partition. See Future Enhancements at the end to see the Dynamic Event bucketing work that aims to take advantage of this.\n\nStorage Tables\n\nWe use two kinds of tables\n\nData tables : These are the time slices that store the actual event data.\n\n: These are the time slices that store the actual event data. Metadata table: This table stores information about how each time slice is configured per namespace.\n\nData tables\n\nThe partition key enables splitting events for a time_series_id over a range of time_bucket(s) and event_bucket(s), thus mitigating hot partitions, while the clustering key allows us to keep data sorted on disk in the order we almost always want to read it. The value_metadata column stores metadata for the event_item_value such as compression.\n\nWriting to the data table:\n\nUser writes will land in a given time slice, time bucket, and event bucket as a factor of the event_time attached to the event. This factor is dictated by the control plane configuration of a given namespace.\n\nFor example:\n\nDuring this process, the writer makes decisions on how to handle the data before writing, such as whether to compress it. The value_metadata column records any such post-processing actions, ensuring that the reader can accurately interpret the data.\n\nReading from the data table:\n\nThe below illustration depicts at a high-level on how we scatter-gather the reads from multiple partitions and join the result set at the end to return the final result.\n\nMetadata table\n\nThis table stores the configuration data about the time slices for a given namespace.\n\nNote the following:\n\nNo Time Gaps : The end_time of a given time slice overlaps with the start_time of the next time slice, ensuring all events find a home.\n\n: The end_time of a given time slice overlaps with the start_time of the next time slice, ensuring all events find a home. Retention : The status indicates which tables fall inside and outside of the retention window.\n\n: The status indicates which tables fall inside and outside of the retention window. Flexible: This metadata can be adjusted per time slice, allowing us to tune the partition settings of future time slices based on observed data patterns in the current time slice.\n\nThere is a lot more information that can be stored into the metadata column (e.g., compaction settings for the table), but we only show the partition settings here for brevity.\n\nIndex Datastore\n\nTo support secondary access patterns via non-primary key attributes, we index data into Elasticsearch. Users can configure a list of attributes per namespace that they wish to search and/or aggregate data on. The service extracts these fields from events as they stream in, indexing the resultant documents into Elasticsearch. Depending on the throughput, we may use Elasticsearch as a reverse index, retrieving the full data from Cassandra, or we may store the entire source data directly in Elasticsearch.\n\nNote: Again, users are never directly exposed to Elasticsearch, just like they are not directly exposed to Cassandra. Instead, they interact with the Search and Aggregate API endpoints that translate a given query to that needed for the underlying datastore.\n\nIn the next section, we will talk about how we configure these data stores for different datasets.\n\nControl Plane\n\nThe data plane is responsible for executing the read and write operations, while the control plane configures every aspect of a namespace\u2019s behavior. The data plane communicates with the TimeSeries control stack, which manages this configuration information. In turn, the TimeSeries control stack interacts with a sharded Data Gateway Platform Control Plane that oversees control configurations for all abstractions and namespaces.\n\nSeparating the responsibilities of the data plane and control plane helps maintain the high availability of our data plane, as the control plane takes on tasks that may require some form of schema consensus from the underlying data stores.\n\nNamespace Configuration\n\nThe below configuration snippet demonstrates the immense flexibility of the service and how we can tune several things per namespace using our control plane.\n\n\"persistence_configuration\": [\n\n{\n\n\"id\": \"PRIMARY_STORAGE\",\n\n\"physical_storage\": {\n\n\"type\": \"CASSANDRA\", // type of primary storage\n\n\"cluster\": \"cass_dgw_ts_tracing\", // physical cluster name\n\n\"dataset\": \"tracing_default\" // maps to the keyspace\n\n},\n\n\"config\": {\n\n\"timePartition\": {\n\n\"secondsPerTimeSlice\": \"129600\", // width of a time slice\n\n\"secondPerTimeBucket\": \"3600\", // width of a time bucket\n\n\"eventBuckets\": 4 // how many event buckets within\n\n},\n\n\"queueBuffering\": {\n\n\"coalesce\": \"1s\", // how long to coalesce writes\n\n\"bufferCapacity\": 4194304 // queue capacity in bytes\n\n},\n\n\"consistencyScope\": \"LOCAL\", // single-region/multi-region\n\n\"consistencyTarget\": \"EVENTUAL\", // read/write consistency\n\n\"acceptLimit\": \"129600s\" // how far back writes are allowed\n\n},\n\n\"lifecycleConfigs\": {\n\n\"lifecycleConfig\": [ // Primary store data retention\n\n{\n\n\"type\": \"retention\",\n\n\"config\": {\n\n\"close_after\": \"1296000s\", // close for reads/writes\n\n\"delete_after\": \"1382400s\" // drop time slice\n\n}\n\n}\n\n]\n\n}\n\n},\n\n{\n\n\"id\": \"INDEX_STORAGE\",\n\n\"physicalStorage\": {\n\n\"type\": \"ELASTICSEARCH\", // type of index storage\n\n\"cluster\": \"es_dgw_ts_tracing\", // ES cluster name\n\n\"dataset\": \"tracing_default_useast1\" // base index name\n\n},\n\n\"config\": {\n\n\"timePartition\": {\n\n\"secondsPerSlice\": \"129600\" // width of the index slice\n\n},\n\n\"consistencyScope\": \"LOCAL\",\n\n\"consistencyTarget\": \"EVENTUAL\", // how should we read/write data\n\n\"acceptLimit\": \"129600s\", // how far back writes are allowed\n\n\"indexConfig\": {\n\n\"fieldMapping\": { // fields to extract to index\n\n\"tags.nf.app\": \"KEYWORD\",\n\n\"tags.duration\": \"INTEGER\",\n\n\"tags.enabled\": \"BOOLEAN\"\n\n},\n\n\"refreshInterval\": \"60s\" // Index related settings\n\n}\n\n},\n\n\"lifecycleConfigs\": {\n\n\"lifecycleConfig\": [\n\n{\n\n\"type\": \"retention\", // Index retention settings\n\n\"config\": {\n\n\"close_after\": \"1296000s\",\n\n\"delete_after\": \"1382400s\"\n\n}\n\n}\n\n]\n\n}\n\n}\n\n]\n\nProvisioning Infrastructure\n\nWith so many different parameters, we need automated provisioning workflows to deduce the best settings for a given workload. When users want to create their namespaces, they specify a list of workload desires, which the automation translates into concrete infrastructure and related control plane configuration. We highly encourage you to watch this ApacheCon talk, by one of our stunning colleagues Joey Lynch, on how we achieve this. We may go into detail on this subject in one of our future blog posts.\n\nOnce the system provisions the initial infrastructure, it then scales in response to the user workload. The next section describes how this is achieved.\n\nScalability\n\nOur users may operate with limited information at the time of provisioning their namespaces, resulting in best-effort provisioning estimates. Further, evolving use-cases may introduce new throughput requirements over time. Here\u2019s how we manage this:\n\nHorizontal scaling : TimeSeries server instances can auto-scale up and down as per attached scaling policies to meet the traffic demand. The storage server capacity can be recomputed to accommodate changing requirements using our capacity planner.\n\n: TimeSeries server instances can auto-scale up and down as per attached scaling policies to meet the traffic demand. The storage server capacity can be recomputed to accommodate changing requirements using our capacity planner. Vertical scaling : We may also choose to vertically scale our TimeSeries server instances or our storage instances to get greater CPU, RAM and/or attached storage capacity.\n\n: We may also choose to vertically scale our TimeSeries server instances or our storage instances to get greater CPU, RAM and/or attached storage capacity. Scaling disk : We may attach EBS to store data if the capacity planner prefers infrastructure that offers larger storage at a lower cost rather than SSDs optimized for latency. In such cases, we deploy jobs to scale the EBS volume when the disk storage reaches a certain percentage threshold.\n\n: We may attach EBS to store data if the capacity planner prefers infrastructure that offers larger storage at a lower cost rather than SSDs optimized for latency. In such cases, we deploy jobs to scale the EBS volume when the disk storage reaches a certain percentage threshold. Re-partitioning data: Inaccurate workload estimates can lead to over or under-partitioning of our datasets. TimeSeries control-plane can adjust the partitioning configuration for upcoming time slices, once we realize the nature of data in the wild (via partition histograms). In the future we plan to support re-partitioning of older data and dynamic partitioning of current data.\n\nDesign Principles\n\nSo far, we have seen how TimeSeries stores, configures and interacts with event datasets. Let\u2019s see how we apply different techniques to improve the performance of our operations and provide better guarantees.\n\nEvent Idempotency\n\nWe prefer to bake in idempotency in all mutation endpoints, so that users can retry or hedge their requests safely. Hedging is when the client sends an identical competing request to the server, if the original request does not come back with a response in an expected amount of time. The client then responds with whichever request completes first. This is done to keep the tail latencies for an application relatively low. This can only be done safely if the mutations are idempotent. For TimeSeries, the combination of event_time, event_id and event_item_key form the idempotency key for a given time_series_id event.\n\nSLO-based Hedging\n\nWe assign Service Level Objectives (SLO) targets for different endpoints within TimeSeries, as an indication of what we think the performance of those endpoints should be for a given namespace. We can then hedge a request if the response does not come back in that configured amount of time.\n\n\"slos\": {\n\n\"read\": { // SLOs per endpoint\n\n\"latency\": {\n\n\"target\": \"0.5s\", // hedge around this number\n\n\"max\": \"1s\" // time-out around this number\n\n}\n\n},\n\n\"write\": {\n\n\"latency\": {\n\n\"target\": \"0.01s\",\n\n\"max\": \"0.05s\"\n\n}\n\n}\n\n}\n\nPartial Return\n\nSometimes, a client may be sensitive to latency and willing to accept a partial result set. A real-world example of this is real-time frequency capping. Precision is not critical in this case, but if the response is delayed, it becomes practically useless to the upstream client. Therefore, the client prefers to work with whatever data has been collected so far rather than timing out while waiting for all the data. The TimeSeries client supports partial returns around SLOs for this purpose. Importantly, we still maintain the latest order of events in this partial fetch.\n\nAdaptive Pagination\n\nAll reads start with a default fanout factor, scanning 8 partition buckets in parallel. However, if the service layer determines that the time_series dataset is dense \u2014 i.e., most reads are satisfied by reading the first few partition buckets \u2014 then it dynamically adjusts the fanout factor of future reads in order to reduce the read amplification on the underlying datastore. Conversely, if the dataset is sparse, we may want to increase this limit with a reasonable upper bound.\n\nLimited Write Window\n\nIn most cases, the active range for writing data is smaller than the range for reading data \u2014 i.e., we want a range of time to become immutable as soon as possible so that we can apply optimizations on top of it. We control this by having a configurable \u201cacceptLimit\u201d parameter that prevents users from writing events older than this time limit. For example, an accept limit of 4 hours means that users cannot write events older than now() \u2014 4 hours. We sometimes raise this limit for backfilling historical data, but it is tuned back down for regular write operations. Once a range of data becomes immutable, we can safely do things like caching, compressing, and compacting it for reads.\n\nBuffering Writes\n\nWe frequently leverage this service for handling bursty workloads. Rather than overwhelming the underlying datastore with this load all at once, we aim to distribute it more evenly by allowing events to coalesce over short durations (typically seconds). These events accumulate in in-memory queues running on each instance. Dedicated consumers then steadily drain these queues, grouping the events by their partition key, and batching the writes to the underlying datastore.\n\nThe queues are tailored to each datastore since their operational characteristics depend on the specific datastore being written to. For instance, the batch size for writing to Cassandra is significantly smaller than that for indexing into Elasticsearch, leading to different drain rates and batch sizes for the associated consumers.\n\nWhile using in-memory queues does increase JVM garbage collection, we have experienced substantial improvements by transitioning to JDK 21 with ZGC. To illustrate the impact, ZGC has reduced our tail latencies by an impressive 86%:\n\nBecause we use in-memory queues, we are prone to losing events in case of an instance crash. As such, these queues are only used for use cases that can tolerate some amount of data loss .e.g. tracing/logging. For use cases that need guaranteed durability and/or read-after-write consistency, these queues are effectively disabled and writes are flushed to the data store almost immediately.\n\nDynamic Compaction\n\nOnce a time slice exits the active write window, we can leverage the immutability of the data to optimize it for read performance. This process may involve re-compacting immutable data using optimal compaction strategies, dynamically shrinking and/or splitting shards to optimize system resources, and other similar techniques to ensure fast and reliable performance.\n\nThe following section provides a glimpse into the real-world performance of some of our TimeSeries datasets.\n\nReal-world Performance\n\nThe service can write data in the order of low single digit milliseconds\n\nwhile consistently maintaining stable point-read latencies:\n\nAt the time of writing this blog, the service was processing close to 15 million events/second across all the different datasets at peak globally.\n\nTime Series Usage @ Netflix\n\nThe TimeSeries Abstraction plays a vital role across key services at Netflix. Here are some impactful use cases:\n\nTracing and Insights: Logs traces across all apps and micro-services within Netflix, to understand service-to-service communication, aid in debugging of issues, and answer support requests.\n\nLogs traces across all apps and micro-services within Netflix, to understand service-to-service communication, aid in debugging of issues, and answer support requests. User Interaction Tracking : Tracks millions of user interactions \u2014 such as video playbacks, searches, and content engagement \u2014 providing insights that enhance Netflix\u2019s recommendation algorithms in real-time and improve the overall user experience.\n\n: Tracks millions of user interactions \u2014 such as video playbacks, searches, and content engagement \u2014 providing insights that enhance Netflix\u2019s recommendation algorithms in real-time and improve the overall user experience. Feature Rollout and Performance Analysis : Tracks the rollout and performance of new product features, enabling Netflix engineers to measure how users engage with features, which powers data-driven decisions about future improvements.\n\n: Tracks the rollout and performance of new product features, enabling Netflix engineers to measure how users engage with features, which powers data-driven decisions about future improvements. Asset Impression Tracking and Optimization : Tracks asset impressions ensuring content and assets are delivered efficiently while providing real-time feedback for optimizations.\n\n: Tracks asset impressions ensuring content and assets are delivered efficiently while providing real-time feedback for optimizations. Billing and Subscription Management: Stores historical data related to billing and subscription management, ensuring accuracy in transaction records and supporting customer service inquiries.\n\nand more\u2026\n\nFuture Enhancements\n\nAs the use cases evolve, and the need to make the abstraction even more cost effective grows, we aim to make many improvements to the service in the upcoming months. Some of them are:\n\nTiered Storage for Cost Efficiency: Support moving older, lesser-accessed data into cheaper object storage that has higher time to first byte, potentially saving Netflix millions of dollars.\n\nSupport moving older, lesser-accessed data into cheaper object storage that has higher time to first byte, potentially saving Netflix millions of dollars. Dynamic Event Bucketing: Support real-time partitioning of keys into optimally-sized partitions as events stream in, rather than having a somewhat static configuration at the time of provisioning a namespace. This strategy has a huge advantage of not partitioning time_series_ids that don\u2019t need it, thus saving the overall cost of read amplification. Also, with Cassandra 4.x, we have noted major improvements in reading a subset of data in a wide partition that could lead us to be less aggressive with partitioning the entire dataset ahead of time.\n\nSupport real-time partitioning of keys into optimally-sized partitions as events stream in, rather than having a somewhat static configuration at the time of provisioning a namespace. This strategy has a huge advantage of not partitioning time_series_ids that don\u2019t need it, thus saving the overall cost of read amplification. Also, with Cassandra 4.x, we have noted major improvements in reading a subset of data in a wide partition that could lead us to be less aggressive with partitioning the entire dataset ahead of time. Caching: Take advantage of immutability of data and cache it intelligently for discrete time ranges.\n\nTake advantage of immutability of data and cache it intelligently for discrete time ranges. Count and other Aggregations: Some users are only interested in counting events in a given time interval rather than fetching all the event data for it.\n\nConclusion\n\nThe TimeSeries Abstraction is a vital component of Netflix\u2019s online data infrastructure, playing a crucial role in supporting both real-time and long-term decision-making. Whether it\u2019s monitoring system performance during high-traffic events or optimizing user engagement through behavior analytics, TimeSeries Abstraction ensures that Netflix operates seamlessly and efficiently on a global scale.\n\nAs Netflix continues to innovate and expand into new verticals, the TimeSeries Abstraction will remain a cornerstone of our platform, helping us push the boundaries of what\u2019s possible in streaming and beyond.\n\nStay tuned for Part 2, where we\u2019ll introduce our Distributed Counter Abstraction, a key element of Netflix\u2019s Composite Abstractions, built on top of the TimeSeries Abstraction.\n\nAcknowledgments\n\nSpecial thanks to our stunning colleagues who contributed to TimeSeries Abstraction\u2019s success: Tom DeVoe Mengqing Wang, Kartik Sathyanarayanan, Jordan West, Matt Lehman, Cheng Wang, Chris Lohfink .", "label": 0}
{"title": "November 2024", "url": "https://lifeofpablo.com/blog/published:2024-11", "content": "en\n\nI'm lying on my couch in my San Francisco apartment trying to recover from the sickness I acquired. The last few days have been rough! Being sick is something I don't like to experience but here we are.\n\nSince I couldn't return to Nebraska this year for the holiday, I had plans to do a Friendsgiving.\n\nEveryone is enjoying themselves and having a grand old time.\n\nIt would be selfish of me to show up sick, and miserable, and risk getting people sick. I know people understand, but I don't like to disappoint others.\n\nI am grateful for a few things:\n\nMy Parents\n\nEven though I couldn't return to Nebraska, I am very grateful for them. They have been very supportive in various aspects of life. I know they miss me and they wish I could move back. They love how I've branched off to live in a different place. I miss them a lot but I will see them soon!\n\nMy Friend Evan\n\nMy friend Evan was so kind enough for me to bring me crackers as I was struggling yesterday. He's a keeper! Those crackers brought me back to life. I'm so appreciative to have Evan in my life.\n\nEvan even walked me to McDonald's this morning making sure I got a light meal. McDonald's was closed. :( I just wanted two burritos.\n\nMy Friends Sarah and Mike\n\nMy friend Sarah has been amazing over the years especially when I moved to Sacramento. She has helped me so much in getting used to California living and of course, having to put up with my shenanigans.\n\nMike is a great friend as well. He helped me guide my career to where it is now! He is a great person to have an amazing conversation with. I love talking about ethics with him and anything related to tech.\n\nI miss them both!\n\nAlone Time\n\nBeing sick allowed me to do nothing, which is not bad. I ordered some soup in hopes of making me feel better sooner. I probably should watch a movie instead of doom-scrolling. I deserve to enjoy myself a little. Maybe I will code and work on my website and working on an API.\n\nHomebrew Website Club\n\nI am grateful I hung out remotely at Homebrew Website Club yesterday! I didn't participate as much as I wanted but I still made the effort to attend. There was some great conversation happening! Shoutout to Tracy and Angelo for hanging out and tucking me in virtually when I finally crashed.\n\nHappy Holidays.", "label": 1}
{"title": "How AI code generation works", "url": "https://github.blog/ai-and-ml/generative-ai/how-ai-code-generation-works/", "content": "Generative AI coding tools are changing software production for enterprises. Not just for their code generation abilities\u2014from vulnerability detection and facilitating comprehension of unfamiliar codebases, to streamlining documentation and pull request descriptions, they\u2019re fundamentally reshaping how developers approach application infrastructure, deployment, and their own work experience.\n\nWe\u2019re now witnessing a significant turning point. As AI models get better, refusing adoption would be like \u201casking an office worker to use a typewriter instead of a computer,\u201d says Albert Ziegler, principal researcher and member of the GitHub Next research and development team.\n\nIn this post, we\u2019ll dive into the inner workings of AI code generation, exploring how it functions, its capabilities and benefits, and how developers can use it to enhance their development experience while propelling your enterprise forward in today\u2019s competitive landscape.\n\nHow to use AI to generate code\n\nAI code generation refers to full or partial lines of code that are generated by machines instead of human developers. This emerging technology leverages advanced machine learning models, particularly large language models (LLMs), to understand and replicate the syntax, patterns, and paradigms found in human-generated code.\n\nThe AI models powering these tools, like ChatGPT and GitHub Copilot, are trained on natural language text and source code from publicly available sources that include a diverse range of code examples. This training enables them to understand the nuances of various programming languages, coding styles, and common practices. As a result, the AI can generate code suggestions that are syntactically correct and contextually relevant based on input from developers.\n\nFavored by 55% of developers, our AI-powered pair programmer, GitHub Copilot, provides contextualized coding assistance based on your organization\u2019s codebase across dozens of programming languages, and targets developers of all experience levels. With GitHub Copilot, developers can use AI to generate code in three ways:\n\n1. Type code and AI can autocomplete the code\n\nAutocompletions are the earliest version of AI code generation. John Berryman, a senior researcher of ML on the GitHub Copilot team, explains the user experience: \u201cI\u2019ll be writing code and taking a pause to think. While I\u2019m doing that, the agent itself is also thinking, looking at surrounding code and content in neighboring tabs. Then it pops up on the screen as gray \u2018ghost text\u2019 that I can reject, partially accept, or fully accept and then, if necessary, modify.\u201d\n\nWhile every developer can reap the benefits of using AI coding tools, experienced programmers can often feel these gains even more so. \u201cIn many cases, especially for experienced programmers in a familiar environment, this suggestion speeds us up. I would have written the same thing. It\u2019s just faster to hit \u2018tab\u2019 (thus accepting the suggestion) than it is to write out those 20 characters by myself,\u201d says Johan Rosenkilde, principal researcher for GitHub Next.\n\nWhether developers are new or highly skilled, they\u2019ll often have to work in less familiar languages, and code completion suggestions using GitHub Copilot can lend a helping hand. \u201cUsing GitHub Copilot for code completion has really helped speed up my learning experience,\u201d says Berryman. \u201cI will often accept the suggestion because it\u2019s something I wouldn\u2019t have written on my own since I don\u2019t know the syntax.\u201d\n\nUsing an AI coding tool has become an invaluable skill in itself. Why? Because the more developers practice coding with these tools, the faster they\u2019ll get at using them.\n\nFor experienced developers in unfamiliar environments, tools like GitHub Copilot can even help jog their memories.\n\nLet\u2019s say a developer imports a new type of library they haven\u2019t used before, or that they don\u2019t remember. Maybe they\u2019re looking to figure out the standard library function or the order of the argument. In these cases, it can be helpful to make GitHub Copilot more explicitly aware of where the developer wants to go by writing a comment.\n\n\u201cIt\u2019s quite likely that the developer might not remember the formula, but they can recognize the formula, and GitHub Copilot can remember it by being prompted,\u201d says Rosenkilde. This is where natural language commentary comes into play: it can be a shortcut for explaining intent when the developer is struggling with the first few characters of code that they need.\n\nIf developers give specific names to their functions and variables, and write documentation, they can get better suggestions, too. That\u2019s because GitHub Copilot can read the variable names and use them as an indicator for what that function should do.\n\nSuddenly that changes how developers write code for the better, because code with good variable and function names are more maintainable. And oftentimes the main job of a programmer is to maintain code, not write it from scratch.\n\n\u201cWhen you push that code, someone is going to review it, and they will likely have a better time reviewing that code if it\u2019s well named, if there\u2019s even a hint of documentation in it, and so on,\u201d says Rosenkilde. In this sense, the symbiotic relationship between the developer and the AI coding tool is not just beneficial for the developer, but for the entire team.\n\n3. Chat directly with AI\n\nWith AI chatbots, code generation can be more interactive. GitHub Copilot Chat, for example, allows developers to interact with code by asking it to explain code, improve syntax, provide ideas, generate tests, and modify existing code\u2014making it a versatile ally in managing coding tasks.\n\nRosenkilde uses the different functionalities of GitHub Copilot:\n\n\u201cWhen I want to do something and I can\u2019t remember how to do it, I type the first few letters of it, and then I wait to see if Copilot can guess what I\u2019m doing,\u201d he says. \u201cIf that doesn\u2019t work, maybe I delete those characters and I write a one liner in commentary and see whether Copilot can guess the next line. If that doesn\u2019t work, then I go to Copilot Chat and explain in more detail what I want done.\u201d\n\nTypically, Copilot Chat returns with something much more verbose and complete than what you get from GitHub Copilot code completion. \u201cNamely, it describes back to you what it is you want done and how it can be accomplished. It gives you code examples, and you can respond and say, oh, I see where you\u2019re going. But actually I meant it like this instead,\u201d says Rosenkilde.\n\nBut using AI chatbots doesn\u2019t mean developers should be hands off. Mistakes in reasoning could lead the AI down a path of further mistakes if left unchecked. Berryman recommends that users should interact with the chat assistant in much the same way that you would when pair programming with a human. \u201cGo back and forth with it. Tell the assistant about the task you are working on, ask it for ideas, have it help you write code, and critique and redirect the assistant\u2019s work in order to keep it on the right track.\u201d\n\nThe importance of code reviews\n\nGitHub Copilot is designed to empower developers to execute their ideas. As long as there is some context for it to draw on, it will likely generate the type of code the developer wants. But this doesn\u2019t replace code reviews between developers.\n\nCode reviews play an important role in maintaining code quality and reliability in software projects, regardless of whether AI coding tools are involved. In fact, the earlier developers can spot bugs in the code development process, the cheaper it is by orders of magnitude.\n\nOrdinary verification would be: does the code parse? Do the tests work? With AI code generation, Ziegler explains that developers should, \u201cScrutinize it in enough detail so that you can be sure the generated code is correct and bug-free. Because if you use tools like that in the wrong way and just accept everything, then the bugs that you introduce are going to cost you more time than you save.\u201d\n\nRosenkilde adds, \u201cA review with another human being is not the same as that, right? It\u2019s a conversation between two developers about whether this change fits into the kind of software they\u2019re building in this organization. GitHub Copilot doesn\u2019t replace that.\u201d\n\nThe advantages of using AI to generate code\n\nWhen developer teams use AI coding tools across the software development cycle, they experience a host of benefits, including:\n\nFaster development, more productivity\n\nAI code generation can significantly speed up the development process by automating repetitive and time-consuming tasks. This means that developers can focus on high-level architecture and problem-solving. In fact, 88% of developers reported feeling more productive when using GitHub Copilot.\n\nRosenkilde reflects on his own experience with GitHub\u2019s AI pair programmer: \u201c95% of the time, Copilot brings me joy and makes my day a little bit easier. And this doesn\u2019t change the code I would have written. It doesn\u2019t change the way I would have written it. It doesn\u2019t change the design of my code. All it does is it makes me faster at writing that same code.\u201d And Rosenkilde isn\u2019t alone: 60% of developers feel more fulfilled with their jobs when using GitHub Copilot.\n\nMental load alleviated\n\nThe benefits of faster development aren\u2019t just about speed: they\u2019re also about alleviating the mental effort that comes with completing tedious tasks. For example, when it comes to debugging, developers have to reverse engineer what went wrong. Detecting a bug can involve digging through an endless list of potential hiding places where it might be lurking, making it repetitive and tedious work.\n\nRosenkilde explains, \u201cSometimes when you\u2019re debugging, you just have to resort to creating print statements that you can\u2019t get around. Thankfully, Copilot is brilliant at print statements.\u201d\n\nA whopping 87% of developers reported spending less mental effort on repetitive tasks with the help of GitHub Copilot.\n\nLess context switching\n\nIn software development, context switching is when developers move between different tasks, projects, or environments, which can disrupt their workflow and decrease productivity. They also often deal with the stress of juggling multiple tasks, remembering syntax details, and managing complex code structures.\n\nWith GitHub Copilot developers can bypass several levels of context switching, staying in their IDE instead of searching on Google or jumping into external documentation.\n\n\u201cWhen I\u2019m writing natural language commentary,\u201d says Rosenkilde, \u201cGitHub Copilot code completion can help me. Or if I use Copilot Chat, it\u2019s a conversation in the context that I\u2019m in, and I don\u2019t have to explain quite as much.\u201d\n\nGenerating code with AI helps developers offload the responsibility of recalling every detail, allowing them to focus on higher-level thinking, problem-solving, and strategic planning.\n\nBerryman adds, \u201cWith GitHub Copilot Chat, I don\u2019t have to restate the problem because the code never leaves my trusted environment. And I get an answer immediately. If there is a misunderstanding or follow-up questions, they are easy to communicate with.\u201d\n\nBefore you implement any AI into your workflow, you should always review and test tools thoroughly to make sure they\u2019re a good fit for your organization. Here are a few considerations to keep in mind.\n\nCompliance\n\nRegulatory compliance . Does the tool comply with relevant regulations in your industry?\n\n. Does the tool comply with relevant regulations in your industry? Compliance certifications. Are there attestations that demonstrate the tool\u2019s compliance with regulations?\n\nSecurity\n\nEncryption . Is the data transmission and storage encrypted to protect sensitive information?\n\n. Is the data transmission and storage encrypted to protect sensitive information? Access controls . Are you able to implement strong authentication measures and access controls to prevent unauthorized access?\n\n. Are you able to implement strong authentication measures and access controls to prevent unauthorized access? Compliance with security standards . Is the tool compliant with industry standards?\n\n. Is the tool compliant with industry standards? Security audits. Does the tool undergo regular security audits and updates to address vulnerabilities?\n\nPrivacy\n\nData handling . Are there clear policies for handling user data and does it adhere to privacy regulations like GDPR, CCPA, etc.?\n\n. Are there clear policies for handling user data and does it adhere to privacy regulations like GDPR, CCPA, etc.? Data anonymization. Does the tool support anonymization techniques to protect user privacy?\n\nPermissioning\n\nRole-based access control . Are you able to manage permissions based on user roles and responsibilities?\n\n. Are you able to manage permissions based on user roles and responsibilities? Granular permissions . Can you control access to different features and functionalities within the tool?\n\n. Can you control access to different features and functionalities within the tool? Opt-in/Opt-out mechanisms. Can users control the use of their data and opt out if needed?\n\nPricing\n\nUnderstand the pricing model . is it based on usage, number of users, features, or other metrics?\n\n. is it based on usage, number of users, features, or other metrics? Look for transparency . Is the pricing structure clear with no hidden costs?\n\n. Is the pricing structure clear with no hidden costs? Scalability. Does the pricing scale with your usage and business growth?\n\nAdditionally, consider factors such as customer support, ease of integration with existing systems, performance, and user experience when evaluating AI coding tools. Lastly, it\u2019s important to thoroughly assess how well the tool aligns with your organization\u2019s specific requirements and priorities in each of these areas.\n\nVisit the GitHub Copilot Trust Center to learn more around security, privacy, and other topics.\n\nCan AI code generation be detected?\n\nThe short answer here is: maybe.\n\nLet\u2019s first give some context to the question. It\u2019s never really the case that a whole code base is generated with AI, because large chunks of AI-generated code are very likely to be wrong. The standard code review process is a good way to avoid this, since large swaths of completely auto-generated code would stand out to a human developer as simply not working.\n\nFor smaller amounts of AI-generated code, there is no way at the moment to detect traces of AI in code with true confidence. There are offerings that purport to classify whether content has AI-generated text, but there are limited equivalents for code, since you\u2019d need a dedicated model to do it. Ziegler explains, \u201cComputer generated code is good enough that it doesn\u2019t leave any particular traces and normally has no clear tells.\u201d\n\nAt GitHub, the Copilot team makes use of a duplicate detection filter that detects exact duplicates in code. So, if you\u2019re writing code and it\u2019s an exact copy of something that exists elsewhere, then it\u2019ll flag it.\n\nIs AI code generation secure?\n\nAI code generation is not any more insecure than human generated code. A combination of testing, manual code reviews, scanning, monitoring, and feedback loops can produce the same quality of code as your human-generated code.\n\nWhen it comes to code generated by GitHub Copilot, developers can use tools like code scanning, which actively reviews your code for potential security issues in real-time and seamlessly integrates the findings into the developer workflow.\n\nUltimately, AI code generation will have vulnerabilities\u2014but so does code written by human developers. As Ziegler explains, \u201cIt\u2019s unclear whether computer generated code does particularly worse. So, the answer is not if you have GitHub Copilot, use a vulnerability checker. The answer is always use a vulnerability checker.\u201d\n\nWatch this video for more tips and words of advice around secure coding best practices with AI.\n\nEmpower your enterprise with AI code generation\n\nWhile the benefits to using AI code generation tools can be significant, it\u2019s important to note that human oversight remains crucial to ensure that the generated code aligns with project goals, coding standards, and business needs.\n\nTech leaders should embrace the use of AI code generation\u2014not only to streamline development, but also to empower developer teams to collaborate, drive meaningful business outcomes, and deliver exceptional value to customers.\n\nLearn more or get started with the world\u2019s most widely adopted AI developer tool.\n\nWant to learn how GitHub can help your organization do more with AI? At GitHub Galaxy 2024, we\u2019ll explore cutting-edge research and best practices in the rapidly evolving world of AI\u2014empowering your business to maximize productivity and innovate at scale. Register now >", "label": 0}
{"title": "Showing Friends Around Where I Live", "url": "https://lifeofpablo.com/blog/showing-friends-around-where-i-live", "content": "Showing Friends Around Where I Live\n\nshowing places to go\n\nThis post was written in English (en_US).\n\nI've lived in California for over 3 years (who's counting? Oh wait, me!) I've got to the point I know where most of the popular places to visit in San Francisco, Sacramento or places in between. I would consider myself feeling somewhat knowledgeable of the area. My common expression here is, \"I'm new here!\" I live in Sacramento but I go to San Francisco all the time that I know the city pretty well enough to wonder off to different places. I like to wonder around even if my feet start hurting so I simply push through.\n\nI'm showing my colleagues' brother, Rami since he's visiting from far away. I feel comfortable walking around in San Francisco. I'm very fortunate to have friends who live in the city who I've learned so much from. I feel that I can be a good tour guide. We're both dudes who are young and like to have fun. That makes it easy to keep our minds open and try new experiences. I love doing all the touristy things again especially when friends or family come to visit. I get to help people experience the city the way I did or at least provide that feeling of novelty.", "label": 1}
{"title": "Deeper-Level Questions to Learn About People", "url": "https://lifeofpablo.com/blog/deeper-level-questions-to-learn-about-people", "content": "Deeper-Level Questions to Learn About People\n\nWhat can you do today that you couldn't do a year ago?\n\nThis post was written in English (en_US).\n\nIt's currently 1 am in Nebraska. I got bored, so I asked ChatGPT the following question.\n\nHello! Can you provide me 100 questions to learn more about people? Deeper level questions.\n\nI've been thinking about asking people different questions. These can be random people, family members, friends, colleagues, etc. I'm more of a listener.\n\nI also thought why not ask myself (or others) these questions and write about it. So instead of waiting for the New Year to start a new habit. Why not start now? I haven't blogged as much lately so this will incentivize me!\n\nI will find a way to answer these questions. I will do my best to do it in order but I'm okay with jumping around!\n\n100 Questions in 100 Days! Let's begin!\n\nWould you like to help me answer some of these questions? Please email me at pablo [at] lifeofpablo.com or send me a webmention! Make sure to tell me which question you'd like to discuss!\n\nKey:\n\nUnanswered \ud83d\udd34: The question has not been addressed yet.\n\n\ud83d\udd34: The question has not been addressed yet. Answered \u2705: The question has been answered.\n\n\u2705: The question has been answered. Link to Response \ud83d\udd17: A link to the detailed response is available. (Hover or click the link for details.)\n\nPhilosophy & Mindset\n\nWhat is your definition of success? \ud83d\udd34\u2705 \ud83d\udd17 What motivates you to work hard? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s the biggest risk you\u2019ve ever taken? \ud83d\udd34\u2705 \ud83d\udd17 What would you do if you knew you could not fail? \ud83d\udd34\u2705 \ud83d\udd17 How do you define happiness? \ud83d\udd34\u2705 \ud83d\udd17 What are your top values? \ud83d\udd34\u2705 \ud83d\udd17 How do you handle failure? \ud83d\udd34\u2705 \ud83d\udd17 How do you stay focused on your goals? \ud83d\udd34\u2705 \ud83d\udd17 What is the best decision you\u2019ve ever made? \ud83d\udd34\u2705 \ud83d\udd17 What does a perfect day look like for you? \ud83d\udd34\u2705 \ud83d\udd17\n\nPurpose & Motivation\n\nWhat drives you to succeed? \ud83d\udd34\u2705 \ud83d\udd17 What is your biggest passion in life? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s the most important thing in life? \ud83d\udd34\u2705 \ud83d\udd17 What would you do with unlimited time and resources? \ud83d\udd34\u2705 \ud83d\udd17 How do you stay motivated during difficult times? \ud83d\udd34\u2705 \ud83d\udd17 What are the key factors that contribute to your success? \ud83d\udd34\u2705 \ud83d\udd17 What legacy do you want to leave behind? \ud83d\udd34\u2705 \ud83d\udd17 How do you find meaning in life? \ud83d\udd34\u2705 \ud83d\udd17 What makes you feel fulfilled? \ud83d\udd34\u2705 \ud83d\udd17 What does success mean to you? \ud83d\udd34\u2705 \ud83d\udd17\n\nHobbies & Interests\n\nWhat are your favorite hobbies? \ud83d\udd34\u2705 \ud83d\udd17 What activity do you do to relax? \ud83d\udd34\u2705 \ud83d\udd17 What is something you\u2019ve always wanted to try but never have? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s the best hobby you\u2019ve picked up during quarantine? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s your favorite way to spend a Saturday? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s one hobby you\u2019ve always been curious about? \ud83d\udd34\u2705 \ud83d\udd17 What kind of books do you like to read? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s your favorite type of music? \ud83d\udd34\u2705 \ud83d\udd17 What do you enjoy doing when you need to clear your mind? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s a new hobby you\u2019ve picked up recently? \ud83d\udd34\u2705 \ud83d\udd17\n\nWell-being & Health\n\nWhat\u2019s your favorite way to stay active? \ud83d\udd34\u2705 \ud83d\udd17 How do you take care of your mental health? \ud83d\udd34\u2705 \ud83d\udd17 How do you practice self-care? \ud83d\udd34\u2705 \ud83d\udd17 How do you manage stress? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s your morning routine like? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s your go-to comfort food? \ud83d\udd34\u2705 \ud83d\udd17 What do you do to maintain a healthy lifestyle? \ud83d\udd34\u2705 \ud83d\udd17 What do you do when you\u2019re feeling overwhelmed? \ud83d\udd34\u2705 \ud83d\udd17 How do you stay energized throughout the day? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s the best piece of advice you\u2019ve received about health? \ud83d\udd34\u2705 \ud83d\udd17\n\nMiscellaneous & Fun\n\nWhat\u2019s the most memorable vacation you\u2019ve been on? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s your favorite season of the year? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s one place you\u2019d love to visit? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s your favorite meal of the day: breakfast, lunch, or dinner? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s the most adventurous thing you\u2019ve ever done? \ud83d\udd34\u2705 \ud83d\udd17 If you could switch lives with anyone for a day, who would it be? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s your idea of a perfect day? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s the best movie you\u2019ve seen recently? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s something you\u2019ve learned that you wish you knew earlier? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s your favorite way to unwind after a long week? \ud83d\udd34\u2705 \ud83d\udd17\n\nDreams & Ambitions\n\nWhat\u2019s your biggest dream in life? \ud83d\udd34\u2705 \ud83d\udd17 Where do you see yourself in 10 years? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s the most important thing you hope to achieve in the next year? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s the best way to set and achieve goals? \ud83d\udd34\u2705 \ud83d\udd17 If you could change one thing about your life, what would it be? \ud83d\udd34\u2705 \ud83d\udd17 What would you do if you knew you had no time limit to achieve your dreams? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s one goal you\u2019ve set that you\u2019ve accomplished? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s one dream you hope to fulfill in the next five years? \ud83d\udd34\u2705 \ud83d\udd17 What inspires you to keep going, even when things get tough? \ud83d\udd34\u2705 \ud83d\udd17 How do you stay motivated when faced with obstacles? \ud83d\udd34\u2705 \ud83d\udd17\n\nDreams & Spirituality\n\nWhat do you believe happens after we die? \ud83d\udd34\u2705 \ud83d\udd17 What is your biggest spiritual belief? \ud83d\udd34\u2705 \ud83d\udd17 How has your spiritual journey shaped your life? \ud83d\udd34\u2705 \ud83d\udd17 Do you believe in life after death? \ud83d\udd34\u2705 \ud83d\udd17 What role does faith play in your life? \ud83d\udd34\u2705 \ud83d\udd17 Do you believe that everything happens for a reason? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s your view on karma? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s the most profound spiritual experience you\u2019ve had? \ud83d\udd34\u2705 \ud83d\udd17 How do you define enlightenment or spiritual awakening? \ud83d\udd34\u2705 \ud83d\udd17 How do you reconcile science and spirituality? \ud83d\udd34\u2705 \ud83d\udd17\n\nGoals & Success\n\nWhat\u2019s your definition of success? \ud83d\udd34\u2705 \ud83d\udd17 How do you measure your success? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s the most important goal you\u2019re currently working on? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s the best piece of advice you\u2019ve received about setting goals? \ud83d\udd34\u2705 \ud83d\udd17 How do you overcome setbacks when pursuing your goals? \ud83d\udd34\u2705 \ud83d\udd17 What does it take to be successful in life? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s the most important trait you think is necessary to succeed? \ud83d\udd34\u2705 \ud83d\udd17 How do you keep yourself motivated when things aren\u2019t going as planned? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s a personal goal you\u2019ve set for the next year? \ud83d\udd34\u2705 \ud83d\udd17 How do you stay focused on your long-term goals? \ud83d\udd34\u2705 \ud83d\udd17\n\nMotivation & Strategy\n\nWhat motivates you to take action towards your dreams? \ud83d\udd34\u2705 \ud83d\udd17 How do you maintain consistency when working towards your goals? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s your strategy for overcoming procrastination? \ud83d\udd34\u2705 \ud83d\udd17 How do you prioritize tasks when you have a lot to do? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s your approach to time management? \ud83d\udd34\u2705 \ud83d\udd17 How do you stay disciplined when working towards your goals? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s the most effective habit you\u2019ve developed to stay motivated? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s one thing you do every day that contributes to your success? \ud83d\udd34\u2705 \ud83d\udd17 How do you stay positive and motivated when things aren\u2019t going your way? \ud83d\udd34\u2705 \ud83d\udd17 What\u2019s your number one piece of advice for staying motivated? \ud83d\udd34\u2705 \ud83d\udd17\n\nFriendship & Support", "label": 1}
{"title": "Here's the latest version of our Engineering Career Framework", "url": "https://dropbox.tech/culture/our-updated-engineering-career-framework", "content": "Two years ago, we shared the Engineering Career Framework we use at Dropbox. The framework is intended to help our engineers have greater impact in their roles and on their teams. For each role, we outline Core Responsibilities, or CRs\u2014key behaviors that define what impactful work looks like and how to deliver it. We hoped that by providing consistent expectations for each level, we could better help Dropboxers grow in their engineering careers. The framework is now widely used as a reference during interviewing and hiring, performance reviews and calibrations, and the rating and promotion process. We recently refreshed this framework to address some specific needs and reduce ambiguity. Along with sharing the updates, we wanted to go into a little more detail about how we manage this framework and what the update process is like. We used these updates for the 2022 review and calibration cycle, and feedback indicates that the changes did improve the clarity and accuracy of the framework descriptions\u2014though some issues remain. Our work here is not finished. We have updated the originally published framework in place, but to summarize: We added both technical craft expectations for engineering managers, and business acumen expectations for managers and many higher-level roles.\n\nWe clarified and expanded on expectations for decision-making, collaboration, and contributions to organizational health.\n\nWe strengthened and clarified the expectations for ownership over code, processes, and operational systems. We hope that sharing some of our thought process here may prove useful to others undertaking a similar project. We\u2019ll talk about why we decided to make the update and what changes we focused on for this round of changes. Then we\u2019ll explain how we deployed the framework operationally, and our new technical approach for making updates, now and in the future. Finally, we\u2019ll look in more detail at the feedback we received and the path ahead.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images\u2014plus generative AI capabilities across even more connected apps. See what's new \u2192\n\nIn late 2021, Dropbox staff engineers gathered for our annual summit where we assessed the state of engineering at Dropbox. Our goal was to identify concrete things we could improve. Two relevant initiatives came out of that effort: Update the CRs to promote engineering efficiency\n\nProvide clearer paths and CRs to help senior engineers grow These initiatives were based on the perception that reviews and promotions were too heavily biased towards big, complex, or high-profile achievements. Taking ownership, making tough decisions, or doing so-called keeping-the-lights-on (KTLO) work is no less important, but was often deemed less impactful. We heard this resulted in a tendency to build the wrong things or build them in the wrong way. We also received repeated feedback that the CRs were not as helpful for senior engineers looking to understand their performance relative to expectations. This was especially the case for more experienced engineers looking for paths to promotion. Several high-level engineers and engineering managers (EMs) formed a working group to update our framework accordingly. This was done in coordination with a broader Dropbox-wide initiative to refresh our CRs to take into account Virtual First, as well as the critical role that managers play in building and sustaining successful cultures. Because the CRs explicitly state the expectations of engineers at all levels, they can be a powerful lever for affecting cultural change. We had the explicit goal of better rewarding the kinds of engineering behaviors that we wanted to see, and more clearly defining the different roles or archetypes more senior engineers can function in and how those map to the CRs. Specifically, we wanted promotion evaluations to favor a variety of possible contributions, rather than over-indexing on the building of large, complex things.\n\nWhat changes did we make?\n\nWe started with a survey of engineers on how well the CRs mapped to the work they were doing and how they were evaluated. Of 106 respondents, more than a quarter reported they didn\u2019t feel the CRs reflected the work they were doing on a daily basis. More than a fifth reported that they didn\u2019t clearly understand what was expected to get to the next level. While the career framework explicitly states that it is not intended to be a checklist for promotion, the fact remains that because it is the canonical source of level expectations, some people will use it in this way. Much of the feedback we received centered on how the framework didn\u2019t effectively capture how different CRs were weighted at review time, and how recognition for chores like on-call toil, KLTO, glue work, and documentation were under-valued. We also heard a desire for increased clarity around how more senior engineers (IC4+) could fulfill various roles or archetypes. For instance, how might a specific IC4 who is functioning more as a tech lead be evaluated fairly in comparison with an IC4 who is functioning more as an architect? To address this feedback, we made numerous minor updates to the CRs and descriptions, particularly at higher levels, in order to encourage the behaviors we want engineers to exemplify. For example: Before, IC3 software engineers didn\u2019t have clear expectations of what ownership looked like at that level, so we added the following craft CR: \u201cI look for ways to reduce future toil and tech debt for existing components my team owns.\u201d\n\nTo ensure security-by-design, we often ask IC4 security engineers to collaborate with other teams during the early stages of an effort\u2014but this wasn\u2019t expressed in the CRs and wasn\u2019t being assessed consistently in calibrations. We added the following culture CR: \u201cI am effective at working with cross-functional stakeholders to identify technical blindspots and clarify ambiguity in their ideas.\u201d\n\nIC3+ software engineers didn\u2019t have guidance about expectations for their role within the larger organization beyond their team, so we added a business acumen CR, tailored per level. This is an example of what it looks like for an IC4: \u201cI have a working knowledge of Dropbox\u2019s org/team structure and how teams work together across Dropbox, and am able to help my team effectively collaborate effectively with other teams across our org.\u201d\n\nA sample of some of the updates to the IC4 Software Engineer CRs that shows the level of thought and work that goes into modifying each line\n\nWe also added two special sections to the appendix of the framework. One explains and highlights different Engineering Archetype behaviors\u2014heavily derived from Will Larson\u2019s definitions\u2014and how they map to levels and CRs. The other provides context and clarification around the CRs more generally and dispels myths about how they are to be used. The goal of these sections is to add archetypes and related behaviors to the language of growth and evaluation here at Dropbox, and to be a single source for greater clarity on how evaluations actually work.\n\nOperational approach\n\nWe wanted to deploy the updated framework quickly so that we could see the benefit from the changes. However, we saw early on that suddenly changing the evaluation criteria for anyone would feel like moving the goalposts mid-cycle, and therefore not align with one of our company values, Make Work Human. So our working group coordinated with the People team and established a plan for previewing changes early, before the mid-year lightweight review cycle, in order to get feedback and encourage engineering ICs and EMs to start aligning with the new descriptions. After the mid-year cycle, we incorporated the feedback, then heavily promoted and circulated the updates\u2014via Slack announcements, emails, engineering all-hands meetings, etc. This was to ensure everyone was aware of the changes and had plenty of lead time to incorporate them into their models of how they and others would be evaluated. We wanted to make sure that nobody felt like we were moving the goalposts on them, even while we worked out the kinks and clarified things in response to feedback. Here\u2019s our plan, right out of our working group coordination doc: Establish working group and work streams\n\nCollect feedback via survey\n\nUpdate and iterate on CRs\n\nShare with VPs and HR to get early feedback and update\n\nCollect feedback from engineering leadership team and update\n\nShare internally with all of engineering\n\nIncorporate any feedback and publish final updates\n\nPublish externally\n\nTechnical approach\n\nWith these changes in mind\u2014and looking ahead to future iterations of this framework\u2014we also went looking for a better way to manage updates to the underlying documents. We wanted a system that would, among other things, let us: facilitate the review and approval process preserve a history of changes over time and allow comparison between versions and related roles easily (re)generate content based on controlled source files have multiple, simultaneous, independent updates exist in various stages of the process Naturally, if you put a bunch of engineers and EMs in a room and give them requirements like this, their solutions will inevitably involve bespoke programming and source control\u2014and we are no exception. Early proposals got fairly sophisticated; at one point, we considered modeling each role as a class hierarchy and then generating comparisons programmatically. But in the end, we took our own advice\u2014impactful work doesn\u2019t always have to be big or complex\u2014and settled for something simple. We set up an internal git repository to contain the text content of the framework, and a bare-bones Django site for simple internal hosting. The underlying documents are stored in a folder hierarchy of markdown files, organized by role. Each file is a complete, self-contained description of the CRs for a single role and level (for example, \u201cIC5 Staff Security Engineer\u201d).\n\nUsing a comparison tool to assess changes to the framework in the process of writing this article\n\nStoring the CRs as markdown files in a git repository comes with some big advantages. They are easily rendered, but the source is also human readable and diff-able. No external database, CRM, or other system is necessary to maintain the contents outside of the source tree; updates can be controlled according to normal source code review processes. And when someone decides we need to change how we present the framework, the markdown files are easily portable to any number of static site generation systems. Here\u2019s our guidance for any Dropboxer who wants to update the framework, directly from the README.md file: The Engineering Career Framework is a living document that will evolve over time, and anyone within engineering at Dropbox should feel empowered to suggest changes. For lightweight changes such as syntax or formatting changes, a simple diff should be reviewable by the members of the Engineering Career Framework working group and result in changes being visible by the next deployment [of our the internal tool]. Larger-scale changes such as significant rewording, adding/removing copy, or level/role changes will involve multiple levels of approvals. While these cannot be merged through a single diff approval, the diffs themselves can be used as the first step in surfacing where the engineering community at large feels there is a gap between the CRs and their day-to-day responsibilities. With this system in place, each individual update to the CRs can be made with the following process: Create a diff with the desired changes and circulate to stakeholders. Review the changes with stakeholders and respond to feedback. Update the changelog with a description of the changes, then merge the diff. Deploy the updated source to the internal mini-site.\n\nExample of a diff correcting a typographical error\n\nWe implemented the changes described above as we worked the kinks out of the process. We found that minor, non-material changes (like correcting typographical errors, removing duplication, etc.) could easily be applied with a relatively light review, while more substantial updates could be reviewed with increased rigor, as appropriate. When it came time to publish, we were able to export a static version of the framework and update our externally-facing repo and site. The externally published version is for transparency and the benefit of the community at large. Naturally, we don\u2019t take pull requests on it.\n\nResults and path forward\n\nAfter launching this framework internally and using it for an entire full-year review cycle, our working group again ran a survey asking engineers who used the framework about the changes. We didn\u2019t have as many respondents as the first survey, but nine out of ten agreed that the updated framework better reflects the work they are doing at their level, which is confirmation that this round of updates is a step in the right direction. We also received a lot of valuable comments. Some suggested the new archetypes are valuable, but could use more clarity around how they tend to be expressed in lower levels. Others felt the CRs are still too wordy and vague (most of this update cycle was spent adding clarification, not trimming and streamlining). And engineers still wanted a clearer explanation of the differences between levels (while the files are structured for easy comparison using a comparison tool, it would be better to make this a first-class capability of the presentation system). After going through this whole process, we found some downsides to using markdown, too. The markdown table syntax, while easily human-readable, is not at all conducive to tracking changes across large tables full of text\u2014as we discovered the hard way. Reformatting the pages to avoid the use of tables mitigated this, but we had plenty of feedback that removing the tables made the text harder to ingest and compare. We also discovered the joys of maintaining large blocks of text in source control, and trying to decide whether to keep things in single lines, or apply line breaks. Late in the game we discovered SemBr, an approach for breaking up text with line breaks on semantic boundaries for easier comparison. We are looking to adopt this moving forward as the framework continues to evolve. Finally, we found that because so many of the CRs across roles have similar language, updating wording can be cumbersome, both for implementation and review. Storing each individual role as separate text allows for easy tailoring, but makes maintaining consistency more costly. We\u2019re still exploring ways to do both, but we haven\u2019t landed on a solution for this yet.\n\nConclusion", "label": 0}
{"title": "Enhancing Netflix Reliability with Service-Level Prioritized Load Shedding", "url": "https://netflixtechblog.com/enhancing-netflix-reliability-with-service-level-prioritized-load-shedding-e735e6ce8f7d?source=collection_home---4------21-----------------------", "content": "Without prioritized load-shedding, both user-initiated and prefetch availability drop when latency is injected. However, after adding prioritized load-shedding, user-initiated requests maintain a 100% availability and only prefetch requests are throttled.\n\nWe were ready to roll this out to production and see how it performed in the wild!\n\nReal-World Application and Results\n\nNetflix engineers work hard to keep our systems available, and it was a while before we had a production incident that tested the efficacy of our solution. A few months after deploying prioritized load shedding, we had an infrastructure outage at Netflix that impacted streaming for many of our users. Once the outage was fixed, we got a 12x spike in pre-fetch requests per second from Android devices, presumably because there was a backlog of queued requests built up.\n\nSpike in Android pre-fetch RPS\n\nThis could have resulted in a second outage as our systems weren\u2019t scaled to handle this traffic spike. Did prioritized load-shedding in PlayAPI help us here?\n\nYes! While the availability for prefetch requests dropped as low as 20%, the availability for user-initiated requests was > 99.4% due to prioritized load-shedding.\n\nAvailability of pre-fetch and user-initiated requests\n\nAt one point we were throttling more than 50% of all requests but the availability of user-initiated requests continued to be > 99.4%.\n\nGeneric service work prioritization\n\nBased on the success of this approach, we have created an internal library to enable services to perform prioritized load shedding based on pluggable utilization measures, with multiple priority levels.\n\nUnlike API gateway, which needs to handle a large volume of requests with varying priorities, most microservices typically receive requests with only a few distinct priorities. To maintain consistency across different services, we have introduced four predefined priority buckets inspired by the Linux tc-prio levels:\n\nCRITICAL : Affect core functionality \u2014 These will never be shed if we are not in complete failure.\n\n: Affect core functionality \u2014 These will never be shed if we are not in complete failure. DEGRADED : Affect user experience \u2014 These will be progressively shed as the load increases.\n\n: Affect user experience \u2014 These will be progressively shed as the load increases. BEST_EFFORT : Do not affect the user \u2014 These will be responded to in a best effort fashion and may be shed progressively in normal operation.\n\n: Do not affect the user \u2014 These will be responded to in a best effort fashion and may be shed progressively in normal operation. BULK: Background work, expect these to be routinely shed.\n\nServices can either choose the upstream client\u2019s priority or map incoming requests to one of these priority buckets by examining various request attributes, such as HTTP headers or the request body, for more precise control. Here is an example of how services can map requests to priority buckets:\n\nResourceLimiterRequestPriorityProvider requestPriorityProvider() {\n\nreturn contextProvider -> {\n\nif (contextProvider.getRequest().isCritical()) {\n\nreturn PriorityBucket.CRITICAL;\n\n} else if (contextProvider.getRequest().isHighPriority()) {\n\nreturn PriorityBucket.DEGRADED;\n\n} else if (contextProvider.getRequest().isMediumPriority()) {\n\nreturn PriorityBucket.BEST_EFFORT;\n\n} else {\n\nreturn PriorityBucket.BULK;\n\n}\n\n};\n\n}\n\nGeneric CPU based load-shedding\n\nMost services at Netflix autoscale on CPU utilization, so it is a natural measure of system load to tie into the prioritized load shedding framework. Once a request is mapped to a priority bucket, services can determine when to shed traffic from a particular bucket based on CPU utilization. In order to maintain the signal to autoscaling that scaling is needed, prioritized shedding only starts shedding load after hitting the target CPU utilization, and as system load increases, more critical traffic is progressively shed in an attempt to maintain user experience.\n\nFor example, if a cluster targets a 60% CPU utilization for auto-scaling, it can be configured to start shedding requests when the CPU utilization exceeds this threshold. When a traffic spike causes the cluster\u2019s CPU utilization to significantly surpass this threshold, it will gradually shed low-priority traffic to conserve resources for high-priority traffic. This approach also allows more time for auto-scaling to add additional instances to the cluster. Once more instances are added, CPU utilization will decrease, and low-priority traffic will resume being served normally.\n\nPercentage of requests (Y-axis) being load-shed based on CPU utilization (X-axis) for different priority buckets\n\nExperiments with CPU based load-shedding\n\nWe ran a series of experiments sending a large request volume at a service which normally targets 45% CPU for auto scaling but which was prevented from scaling up for the purpose of monitoring CPU load shedding under extreme load conditions. The instances were configured to shed noncritical traffic after 60% CPU and critical traffic after 80%.\n\nAs RPS was dialed up past 6x the autoscale volume, the service was able to shed first noncritical and then critical requests. Latency remained within reasonable limits throughout, and successful RPS throughput remained stable.\n\nExperimental behavior of CPU based load-shedding using synthetic traffic.\n\nP99 latency stayed within a reasonable range throughout the experiment, even as RPS surpassed 6x the autoscale target.\n\nAnti-patterns with load-shedding\n\nAnti-pattern 1 \u2014 No shedding\n\nIn the above graphs, the limiter does a good job keeping latency low for the successful requests. If there was no shedding here, we\u2019d see latency increase for all requests, instead of a fast failure in some requests that can be retried. Further, this can result in a death spiral where one instance becomes unhealthy, resulting in more load on other instances, resulting in all instances becoming unhealthy before auto-scaling can kick in.\n\nNo load-shedding: In the absence of load-shedding, increased latency can degrade all requests instead of rejecting some requests (that can be retried), and can make instances unhealthy\n\nAnti-pattern 2 \u2014 Congestive failure\n\nAnother anti-pattern to watch out for is congestive failure or shedding too aggressively. If the load-shedding is due to an increase in traffic, the successful RPS should not drop after load-shedding. Here is an example of what congestive failure looks like:\n\nCongestive failure: After 16:57, the service starts rejecting most requests and is not able to sustain a successful 240 RPS that it was before load-shedding kicked in. This can be seen in fixed concurrency limiters or when load-shedding consumes too much CPU preventing any other work from being done\n\nWe can see in the Experiments with CPU based load-shedding section above that our load-shedding implementation avoids both these anti-patterns by keeping latency low and sustaining as much successful RPS during load-shedding as before.\n\nGeneric IO based load-shedding\n\nSome services are not CPU-bound but instead are IO-bound by backing services or datastores that can apply back pressure via increased latency when they are overloaded either in compute or in storage capacity. For these services we re-use the prioritized load shedding techniques, but we introduce new utilization measures to feed into the shedding logic. Our initial implementation supports two forms of latency based shedding in addition to standard adaptive concurrency limiters (themselves a measure of average latency):\n\nThe service can specify per-endpoint target and maximum latencies, which allow the service to shed when the service is abnormally slow regardless of backend. The Netflix storage services running on the Data Gateway return observed storage target and max latency SLO utilization, allowing services to shed when they overload their allocated storage capacity.\n\nThese utilization measures provide early warning signs that a service is generating too much load to a backend, and allow it to shed low priority work before it overwhelms that backend. The main advantage of these techniques over concurrency limits alone is they require less tuning as our services already must maintain tight latency service-level-objectives (SLOs), for example a p50 < 10ms and p100 < 500ms. So, rephrasing these existing SLOs as utilizations allows us to shed low priority work early to prevent further latency impact to high priority work. At the same time, the system will accept as much work as it can while maintaining SLO\u2019s.\n\nTo create these utilization measures, we count how many requests are processed slower than our target and maximum latency objectives, and emit the percentage of requests failing to meet those latency goals. For example, our KeyValue storage service offers a 10ms target with 500ms max latency for each namespace, and all clients receive utilization measures per data namespace to feed into their prioritized load shedding. These measures look like:\n\nutilization(namespace) = {\n\noverall = 12\n\nlatency = {\n\nslo_target = 12,\n\nslo_max = 0\n\n}\n\nsystem = {\n\nstorage = 17,\n\ncompute = 10,\n\n}\n\n}\n\nIn this case, 12% of requests are slower than the 10ms target, 0% are slower than the 500ms max latency (timeout), and 17% of allocated storage is utilized. Different use cases consult different utilizations in their prioritized shedding, for example batches that write data daily may get shed when system storage utilization is approaching capacity as writing more data would create further instability.\n\nAn example where the latency utilization is useful is for one of our critical file origin services which accepts writes of new files in the AWS cloud and acts as an origin (serves reads) for those files to our Open Connect CDN infrastructure. Writes are the most critical and should never be shed by the service, but when the backing datastore is getting overloaded, it is reasonable to progressively shed reads to files which are less critical to the CDN as it can retry those reads and they do not affect the product experience.\n\nTo achieve this goal, the origin service configured a KeyValue latency based limiter that starts shedding reads to files which are less critical to the CDN when the datastore reports a target latency utilization exceeding 40%. We then stress tested the system by generating over 50Gbps of read traffic, some of it to high priority files and some of it to low priority files:", "label": 0}
{"title": "Picotron", "url": "https://lifeofpablo.com/blog/picotron", "content": "My friends and I started building a game using PICO-8 to build a game. One of my students was telling me about another project by Lexaloffle. It is called, Picotron. I started messing with the preview version available on their site. It's been fun poking at it and seeing what it can do. I would say it's pretty neat and I've been having a blast.\n\nPicotron is a Fantasy Workstation for creating various things, not just games. On the surface reminds me of the Mac OS 8.1 emulator or the macintosh.js that runs as an electron app (javascript) on the surface. Picotron is web based compared to PICO-8 you have to install on your system.\n\nThere is a plenty of information provided for picotron for anyone to learn more as well as a roadmap for the different features to be available over time. I'm really excited for certain features.\n\nIt looks like it will cost $20 or so to have.\n\nHere are some of the demos on Picotron:\n\nBells Your browser does not support the video tag. Chonky Your browser does not support the video tag. Highway Your browser does not support the video tag. Dots Your browser does not support the video tag.\n\nThis was my first expirment for an interactive page.", "label": 1}
{"title": "TIL: track changes in Emacs with highlight-changes-mode", "url": "https://tommorris.org/posts/2025/til-track-changes-in-emacs-with-highlight-changes-mode/", "content": "I find myself having to review and edit a lot of plain text. You\u2019d think a \u201cbicycle for the mind\u201d could make this better.\n\nNon-programmers have Word\u2019s Tracked Changes. Or the Google Docs equivalent. The point of this is if Person A wrote a document, and sent it to Person B for review, Person B could make a bunch of changes, then send it back to Person A.\n\nProgrammers, meanwhile, have tools that are\u2026 more sophisticated? Like diff for one. Both Vim and Emacs have much more sophisticated versions of undo/redo, and tooling like undo-fu because that complexity can be a lot. Plus we have version control systems like Git (or even fancier stuff like Jujutsu which I can\u2019t get on with). If you have the self-discipline and/or tooling to commit lots of little \u201cwip: refactored\u201d changes, you can then use rebase -i , cherry-pick and friends to stitch all those little scraps into a beautiful series of atomic commits that tell a lovely story for both your future self and others. Or git commit -am \"did stuff\" and embrace the chaos demon. But version control is perhaps over-the-top for drafting emails, blog posts and the like.\n\n(If you\u2019re vibe coding, feel free to disregard. Version control is completely optional, and it\u2019s way funnier not to use it.)\n\nIf you write lots of prose, you (or a hallucinating chatbot acting on your behalf) frequently cranks out a crappy first version, or a rough outline, and then you want to go through and polish it meticulously until it doesn\u2019t suck. When doing this, you want to make sure you\u2019ve properly edited everything. If you\u2019ve been noodling around in the intro, then get distracted and pootle around elsewhere, you might entirely skip doing anything about entire paragraphs. In the early days of word procesing, one often printed out the first draft, grabbed a red pen to review it methodically, then returned to the computer to make the edits. While doing this, you could obviously tick off each paragraph after you\u2019ve reviewed it. As previously noted, I see great merit in taking notes by hand, and so, yes, great if you can. But that means you have to use printers which are cursed. And it is a lot of faff, so you often can\u2019t.\n\nThe print-it-out-then-redline-it method is what Tracked Changes reimplements electronically, but Tracked Changes seems to have a baked-in assumption that the person doing the reviewing is separate from the person who wrote it. Which is often the case in a corporate environment\u2026 but often not the case. Also, I don\u2019t wanna use Word or Word clones because plain text is good.\n\nWhat I want then is a way to visually see \u201cwhat have I changed?\u201d as I\u2019m editing a document.\n\nEnter Emacs highlight-changes-mode .\n\nOpen a buffer in Emacs. Type M-x highlight-changes-mode . Start editing. The text you change will be highlighted. The text you don\u2019t change won\u2019t be. If you delete some text, the closest character will be marked with a delete symbol. By default, the changed text is coloured red, and the delete symbol is an underscore.\n\nIf you M-x highlight-changes-mode again, the change markers go away.\n\nI found the default red to be a bit over-the-top, and would rather have the change applied to the background rather than the text itself (also, avoids affecting syntax highlighting). I picked a colour that better matched my preferences and theme.\n\n(custom-set-faces! '(highlight-changes :foreground nil :background \"#004272\") '(highlight-changes-delete :underline t :foreground nil :background \"#004272\"))\n\nHighlight Changes mode does some other stuff too\u2014you can have it cycle through multiple colours, and distinguish between saved and unsaved changes. You can bounce around between bits of the text that you\u2019ve changed and bits you haven\u2019t. I\u2019m not sure I need that\u2026 yet. The EmacsWiki has some other fun stuff about how people have used highlight changes, along with some grumpiness about the choice of colours.", "label": 1}
{"title": "Exploring the Magic Mirror: an interactive experience powered by the Gemini models", "url": "https://developers.googleblog.com/en/magic-mirror-interactive-experience-powered-by-gemini-models/", "content": "Imagine gazing into a mirror and seeing not just your reflection, but a gateway to information, creativity, and a touch of enchantment. This is precisely what the Gemini backed Magic Mirror project brings to life. Moving beyond a simple display, this project showcases the incredible interactive capabilities of the Gemini API and JavaScript GenAI SDK, transforming a familiar object into a new chat interface.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nThis project creates its interactive experience using several features of the Gemini API:\n\n1: Fluid, Real-Time Conversations with the Live API The foundation of the magic mirror's interactivity is the Live API. This allows for continuous, real-time voice interactions. You speak, and the mirror doesn't just listen for a single command, it engages in a flowing conversation by processing your speech as you talk, allowing for a more natural back-and-forth dialogue in either text or audio. On top of this, the Live API is able to understand when you\u2019re speaking during playback and interpret that interruption to pivot the narrative and conversation based on your inputs, allowing for dynamic audible conversations alongside text.\n\nLink to Youtube Video (visible only when JS is disabled)\n\n2: The enchanted storyteller On top of being able to have a conversation through the Live API, the magic mirror can also be customized to weave tales, all thanks to the Gemini model's advanced generation capabilities by providing specific system instructions and updating speech configurations during initialization to include different dialects or accents, voices, and a variety of other attributes.\n\nLink to Youtube Video (visible only when JS is disabled)\n\n3: Instant information: grounding with Google Search While conversations and stories are great, sometimes you want to be able to know about the world around you as it\u2019s happening. This magic mirror project leverages the model\u2019s ability to integrate with Grounding with Google Search, providing grounded, up-to-date information.\n\nLink to Youtube Video (visible only when JS is disabled)\n\n4: Visual alchemy: image generation on command Using Function Calling with the Gemini API, the magic mirror is able to generate visuals based on your descriptions, adding depth to stories and deepening the experience of interacting with the Gemini model. The Gemini model determines that your request requires image generation and calls a predefined function based on stated characteristics, passing along the detailed prompt it derives from your spoken words.\n\nLink to Youtube Video (visible only when JS is disabled)", "label": 0}
{"title": "Detecting memory leaks in Android applications", "url": "https://dropbox.tech/mobile/detecting-memory-leaks-in-android-applications", "content": "Memory leaks occur when an application allocates memory for an object, but then fails to release the memory when the object is no longer being used. Over time, leaked memory accumulates and results in poor app performance and even crashes. Leaks can happen in any program and on any platform, but they\u2019re especially prevalent in Android apps due to complications with activity lifecycles. Recent Android patterns such as ViewModel and LifecycleObserver can help avoid memory leaks, but if you\u2019re following older patterns or don\u2019t know what to look out for, it\u2019s easy to let mistakes slip through.\n\nCommon examples\n\nReference to a long-lived service\n\nA fragment references an activity which references a long-lived service.\n\nIn this case, we have a standard setup with an activity that holds a reference to some long-living service, then a fragment and its view that hold references to the activity. For example, say that the activity somehow creates a reference to its child fragment. Then, for as long as the activity sticks around, the fragment will continue living too. This causes a leak for the duration between the fragment\u2019s onDestroy and the activity\u2019s onDestroy.\n\nThe fragment will never be used again, yet it persists in memory.\n\nLong-lived service which references a fragment\u2019s view What if, in the other direction, the service obtained a reference to the fragment\u2019s view? First, the view would now stay alive for the entire duration of the service. Furthermore, because the view holds a reference to its parent activity, the activity now leaks as well.\n\nAs long as the Service lives, the FragmentView and Activity will squander memory.\n\nDetecting memory leaks\n\nNow that we know how memory leaks happen, let\u2019s discuss what we can do to detect them. An obvious first step is to check if your app ever crashes due to OutOfMemoryError. Unless there\u2019s a single screen that eats more memory than your phone has available, you have a memory leak somewhere.\n\nThis approach only tells you the existence of the problem\u2014not the root cause. The memory leak could have happened anywhere, and the crash that\u2019s logged doesn\u2019t point to the leak, only to the screen that finally tipped memory usage over the limit. You could inspect all the breadcrumbs to see if there\u2019s some similarity, but chances are the culprit won\u2019t be easy to discern. Let\u2019s explore other options. LeakCanary One of the best tools out there is LeakCanary, a memory leak detection library for Android. We simply add a dependency on our build.gradle file. The next time we install and run our app, LeakCanary will be running alongside it. As we navigate through our app, LeakCanary will pause occasionally to dump the memory and provide leak traces of detected leaks. This one step is vastly better than what we had before. But the process is still manual, and each developer will only have a local copy of the memory leaks they\u2019ve personally encountered. We can do better! LeakCanary and Bugsnag LeakCanary provides a very handy code recipe for uploading found leaks to Bugsnag. We\u2019re then able to track memory leaks just as we do any other warning or crash in the app. We can even take this one step further and use Bugsnag\u2019s integrations to hook it up to project management software such as Jira for even more visibility and accountability.\n\nBugsnag connected to Jira\n\nLeakCanary and integration tests Another way to improve automation is to hook up LeakCanary to CI tests. Again, we are given a code recipe to start with. From the official documentation: LeakCanary provides an artifact dedicated to detecting leaks in UI tests which provides a run listener that waits for the end of a test, and if the test succeeds then it looks for retained objects, trigger a heap dump if needed and perform an analysis. Be aware that LeakCanary will slow down testing, as it dumps the heap after each test to which it listens. In our case, because of our selective testing and sharding set up, the extra time added is negligible. Our end result is that memory leaks are surfaced just as any other build or test failure on CI, with the leak trace at the time of the leak recorded. Running LeakCanary on CI has helped us learn better coding patterns, especially when it comes to new libraries, before any code hits production. For example, it caught this leak when we were working with MvRx mocks:\n\nCopy <failure>Test failed because application memory leaks were detected: ==================================== HEAP ANALYSIS RESULT ==================================== 4 APPLICATION LEAKS References underlined with \"~~~\" are likely causes. Learn more at https://squ.re/leaks. 198449 bytes retained by leaking objects Signature: 6bf2ba80511dcb6ab9697257143e3071fca4 \u252c\u2500\u2500\u2500 \u2502 GC Root: System class \u2502 \u251c\u2500 com.airbnb.mvrx.mocking.MockableMavericks class \u2502 Leaking: NO (a class is never leaking) \u2502 \u2193 static MockableMavericks.mockStateHolder \u2502 ~~~~~~~~~~~~~~~ \u251c\u2500 com.airbnb.mvrx.mocking.MockStateHolder instance \u2502 Leaking: UNKNOWN \u2502 \u2193 MockStateHolder.delegateInfoMap \u2502 ~~~~~~~~~~~~~~~ \u251c\u2500 java.util.LinkedHashMap instance \u2502 Leaking: UNKNOWN \u2502 \u2193 LinkedHashMap.header \u2502 ~~~~~~ \u251c\u2500 java.util.LinkedHashMap$LinkedEntry instance \u2502 Leaking: UNKNOWN \u2502 \u2193 LinkedHashMap$LinkedEntry.prv \u2502 ~~~ \u251c\u2500 java.util.LinkedHashMap$LinkedEntry instance \u2502 Leaking: UNKNOWN \u2502 \u2193 LinkedHashMap$LinkedEntry.key \u2502 ~~~ \u2570\u2192 com.dropbox.product.android.dbapp.photos.ui.view.PhotosFragment instance Leaking: YES (ObjectWatcher was watching this because com.dropbox.product.android.dbapp.photos.ui.view.PhotosFragment received Fragment#onDestroy() callback and Fragment#mFragmentManager is null) key = 391c9051-ad2c-4282-9279-d7df13d205c3 watchDurationMillis = 7304 retainedDurationMillis = 2304 198427 bytes retained by leaking objects Signature: d1c9f9707034dd15604d8f2e63ff3bf3ecb61f8\n\nIt turned out that we hadn\u2019t properly cleaned up the mocks when writing the test. Adding a few lines of code avoids the leak:\n\nCopy @After fun teardown() { scenario.close() val holder = MockableMavericks.mockStateHolder holder.clearAllMocks() }\n\nYou may be wondering: Since this memory leak only happens in tests, is it really that important to fix? Well, that\u2019s up to you! Like linters, leak detection can tell you when there\u2019s code smell or bad coding patterns. It can help teach engineers to write more robust code\u2014in this case, we learned about the existence of clearAllMocks(). The severity of a leak and whether or not it\u2019s imperative to fix are decisions an engineer can make. For tests on which we don\u2019t want to run leak detection, we wrote a simple annotation:\n\nCopy @Retention(RetentionPolicy.RUNTIME) @Target({ElementType.METHOD, ElementType.TYPE}) public @interface SkipLeakDetection { /** * The reason why the test should skip leak detection. */ String value(); }\n\nand in our class which overrides LeakCanary\u2019s FailOnLeakRunListener():\n\nCopy override fun skipLeakDetectionReason(description: Description): String? { return when { description.getAnnotation(SkipLeakDetection::class.java) != null -> \"is annotated with @SkipLeakDetection\" description.testClass.isAnnotationPresent(SkipLeakDetection::class.java) -> \"class is annotated with @SkipLeakDetection\" else -> null } }\n\nIndividual tests or entire test classes can use this annotation to skip leak detection.\n\nFixing memory leaks\n\nNow that we\u2019ve gone over various ways to find and surface memory leaks, let\u2019s talk about how to actually understand and fix them. The leak trace provided by LeakCanary will be the single most useful tool for diagnosing a leak. Essentially, the leak trace prints out a chain of references associated with the leaked object, and provides an explanation of why it\u2019s considered a leak. LeakCanary already has great documentation on how to read and use its leak trace, so there\u2019s no need to repeat it here. Instead, let\u2019s go over two categories of memory leaks that I mostly found myself dealing with. Views It\u2019s common to see views declared as class level variables in fragments: private TextView myTextView; or, now that more Android code is being written in Kotlin: private lateinit var myTextView: TextView\u2014common enough for us not to realize that these can all cause memory leaks. Unless these fields are nulled out in the fragment\u2019s onDestroyView, (which you can\u2019t do for a lateinit variable), the references to the views now live for the duration of the fragment\u2019s lifecycle, and not the fragment\u2019s view lifecycle as they should. The simplest scenario of how this causes a leak: We are on FragmentA. We navigate to FragmentB, and now FragmentA is on the back stack. FragmentA is not destroyed, but FragmentA\u2019s view is destroyed. Any views that are tied to FragmentA\u2019s lifecycle are now held in memory when they don\u2019t need to be. For the most part, these leaks are small enough to not cause any performance issues or crashes. But for views that hold objects and data, images, view/data binding and the like, we are more likely to run into trouble. So when possible, avoid storing views in class-level variables, or be sure to clean them up properly in onDestroyView. Speaking of view/data binding, Android\u2019s view binding documentation tells us exactly that: the field must be cleared to prevent leaks. Their code snippet recommends we do the following:\n\nCopy private var _binding: ResultProfileBinding? = null // This property is only valid between onCreateView and // onDestroyView. private val binding get() = _binding!! override fun onCreateView(inflater: LayoutInflater, container: ViewGroup?, savedInstanceState: Bundle? ): View? { _binding = ResultProfileBinding.inflate(inflater, container, false) val view = binding.root return view } override fun onDestroyView() { super.onDestroyView() _binding = null }\n\nThis is lot of boilerplate to put in every fragment (also, avoid using !! which will throw a KotlinNullPointerException if the variable is null. Use explicit null handling instead.) We addressed this issue is by creating a ViewBindingHolder (and DataBindingHolder) that fragments can then implement:\n\nCopy interface ViewBindingHolder<B : ViewBinding> { var binding: B? // Only valid between onCreateView and onDestroyView. fun requireBinding() = checkNotNull(binding) fun requireBinding(lambda: (B) -> Unit) { binding?.let { lambda(it) }} /** * Make sure to use this with Fragment.viewLifecycleOwner */ fun registerBinding(binding: B, lifecycleOwner: LifecycleOwner) { this.binding = binding lifecycleOwner.lifecycle.addObserver(object : DefaultLifecycleObserver { override fun onDestroy(owner: LifecycleOwner) { owner.lifecycle.removeObserver(this) this@ViewBindingHolder.binding = null } }) } } interface DataBindingHolder<B : ViewDataBinding> : ViewBindingHolder<B>\n\nThis provides an easy and clean way for fragments to: Ensure binding is present when it\u2019s required\n\nOnly execute certain code if the binding is available\n\nClean up binding on onDestroyView automatically Temporal leaks These are leaks that only stick around for a short duration of time. In particular, one that we ran into was caused by an EditTextView's async task. The async task lasted just longer than LeakCanary\u2019s default wait time, so a leak was reported even though the memory was cleaned up properly soon afterward. If you suspect you are running into a temporal leak, a good way to check is to use Android Studio\u2019s memory profiler. Once you start a session within the profiler, take the steps to reproduce the leak, but wait for a longer period of time before dumping the heap and inspecting. The leak may be gone after the extra time.\n\nAndroid Studio\u2019s memory profiler shows the effect of temporal leaks that get cleaned up.\n\nTest often, fix early", "label": 0}
{"title": "How Dropbox Replay keeps everyone in sync", "url": "https://dropbox.tech/application/how-dropbox-replay-keeps-everyone-in-sync", "content": "How do you recreate the experience of an in-person screening room with a remote, distributed team? This is one of the reasons we built Dropbox Replay, our new video collaboration tool. Dropbox customers told us the shift to virtual work had turned their once-straightforward review sessions into lengthy, inefficient video calls. It was clear their previous in-person workflows hadn\u2019t made an elegant transition online.\n\nWhen we looked at the market for virtual screening tools, we mostly found expensive Hollywood-scale solutions or clunky DIY experiences run over video conferencing tools. Nothing quite offered the kind of collaborative, live, and synchronous playback experience we envisioned\u2014something that would approximate the feeling of being together in a screening room, with shared feedback, cursors, and playback controls. We knew there was an opportunity for an accessible yet high-quality online screening experience where people could collaborate in realtime as they would in person, but virtually\u2014and where everyone could see the same thing and seamlessly annotate what they were seeing in real time. We created Dropbox Replay\u2019s Live Review feature in response.\n\n\n\nBut keeping a virtual screening room with multiple collaborators in sync is a harder problem to solve than you might think. Anyone in a Live Review session can pause, adjust the playback speed, or scrub to a different frame at anytime. This is great for keeping screenings open and collaborative, but it also means that everyone might be sending conflicting commands at once. What if two people try to change the position of the video at the same time?\n\n\n\nFor Live Review to work, we need to make sure that everyone converges on the same playback state in a timely manner. When a person is ready to discuss a frame, they must be able to trust that everyone is seeing the same frame too.\n\nChanging states\n\nIn a Live Review session, we care about two types of state\u2014single client state and shared client state. The position of someone\u2019s mouse cursor or the drawings they make on a video frame are examples of single client state. These can\u2019t conflict with other clients, so they\u2019re relatively easy to handle: changes in single client state are sent to the server, which echoes them to the other clients, and that\u2019s that.\n\nThe shared client state, on the other hand, is what keeps local playback synchronized between all of the clients in a Live Review session. When someone joins a Live Review session, their client opens a WebSocket connection with a Dropbox Replay server (we use an open source Go library called Gorilla, which is already used elsewhere within Dropbox). Anytime someone presses play, pause, or changes the position of the video, the client sends a message conveying this change to the server, which updates the shared client state and then sends it to everyone else in the session.\n\nThe playback state is encoded using Protocol Buffers, which offer an extensible, schematized, and space-efficient way to package the data. We like the combination of Protocol Buffers and WebSockets; Protocol Buffers don\u2019t concern themselves with message framing and leave the format on the wire up to the developer. WebSockets, meanwhile, are largely content-agnostic and offer text or binary messages that are framed, delivered reliably and in order, and even take care of ping/pong heartbeats, which can keep sessions alive through proxies or firewalls that might otherwise terminate an idle connection.\n\nIn a perfect world, only one person would interact with the video at a time. But that\u2019s not how most screenings work. Everyone has something to say, or something they want the group to see\u2014often at the same time! It\u2019s precisely because anyone can influence playback at any time that our protocol must be resilient enough to handle multiple concurrent interactions and still produce a consistent outcome for all participants.\n\nEstablishing an order of events\n\nOne approach might be to simply send all state changes\u2014\u201cvideo paused at frame\u201d or \u201ccursor is now at position\u201d\u2014to all other clients. However, when the state is shared between clients you can quickly see how this approach will lead to inconsistent states if more than one client changes the state at the same time\u2014for example, if Patty makes a change, sends it to Steven, but Steven makes and sends a change before Patty\u2019s change has arrived.", "label": 0}
{"title": "Extending the Malbec subsea cable to Southern Brazil", "url": "https://engineering.fb.com/2025/05/22/connectivity/extending-malbec-subsea-cable-southern-brazil/", "content": "Meta is partnering with V.tal to extend the Malbec subsea cable to Porto Alegre, Brazil by 2027.\n\nWith this new extension, Malbec will become the first subsea cable to land in the state of Rio Grande do Sul, bringing more connectivity to millions of people in Southern Brazil and neighboring countries.\n\nMalbec will improve the scale and reliability of digital infrastructure in Porto Alegre, establishing it as a digital hub and improving online experiences across Southern Brazil, Argentina, Chile, Paraguay, and Uruguay.\n\nToday, we\u2019re announcing the extension of the Malbec subsea cable to the city of Porto Alegre, Brazil. Developed by Meta, in partnership with V.tal, Malbec is a 2,500 km cable that entered service in 2021 to provide connectivity between the Southern Cone of South America and Brazil. The new extension will be operational in 2027 and will link Porto Alegre to the cities of Rio de Janeiro and S\u00e3o Paulo, Brazil and Buenos Aires, Argentina.\n\n\u201cThe expansion of Malbec to Porto Alegre is a milestone for connectivity in South America, benefiting millions of people in Brazil and positioning the capital of Rio Grande do Sul as the first major international digital hub in the south of the country,\u201d explained Ana Luiza Valadares, Meta\u2019s Public Policy Director, Connectivity & Infra, LatAm. \u201cIt will contribute to attracting digital infrastructure companies, lowering costs for companies and improving consumer services.\u201d\n\nFelipe Campos, CEO of V.tal, added, \u201cThe impact of this project will be significant for the local digital economy, positioning Porto Alegre as a new connectivity hub. It will be a unique infrastructure that will attract the interest of operators and internet providers, as well as other submarine cable companies.\n\nIn addition, all the Southern Cone countries will benefit from this new ecosystem, not to mention the end users and companies who will have a better experience when using the internet and digital applications.\u201d\n\nThis extension is one of the latest in Meta\u2019s digital infrastructure investments to support growing demand for digital capacity, resilience, and global reach. Earlier this year, Meta also activated a Point of Presence (PoP) in Porto Alegre. PoPs facilitate the efficient delivery of content locally, which reduces the network management costs for internet service providers while improving the quality of experience for their customers. With the advent of AI and increasing demand for online services, digital infrastructure deployments play an important role in ensuring that the benefits of AI and other emerging technologies are available to everyone, regardless of where they live or work.\n\n\u201cThis investment in submarine connectivity, fully aligned with our Economic, Inclusive and Sustainable Development Plan, represents a strategic milestone for the state\u2019s future,\u201d said Rio Grand do Sul Governor, Eduardo Leite. \u201cFurthermore, it fosters artificial intelligence projects, technologies that are already transforming the present and will define the future of innovation, a sector in which Rio Grande do Sul is a leader in Brazil, according to the ranking of state competitiveness.\u201d\n\nMalbec will be the first international subsea cable to land in Rio Grande do Sul, bringing with it over 84 terabits of international capacity and direct connectivity to northern Brazil and Argentina. Like most subsea cables, local service providers will be able to acquire capacity on Malbec to serve additional bandwidth to millions of people in Brazil\u2019s southern states. The providers will also extend Malbec\u2019s capacity by connecting with providers in the neighboring countries of Argentina, Chile, Paraguay, and Uruguay, further positioning Brazil as a South American connectivity hub.", "label": 0}
{"title": "Learning Rust and what's to come...", "url": "https://lifeofpablo.com/blog/learning-rust-and-what-s-to-come", "content": "Learning Rust and what's to come...\n\nThis post was written in English (en_US).\n\nWe're 9 days into the New Year of 2022 2023.\n\nThese last few years, I haven't been in the right mindset. I've been in a dark place for many, many years. This dark place has hindered me in so many aspects of my life. I don't know who I am anymore. It's been hard to love myself for as long as I remember.\n\nThe keyword: Avoiding.\n\nI hope my friends and family understand. I know I have hurt you in some form. I am really sorry.\n\nThe past can't be changed. All I can hope is to learn from it and not be stuck in it.\n\nThis year I am going to push through to improve myself. The things I want to do are not a deterrent to the problems at hand. I would like them to help guide me into finding clarity. To help point me in the direction of the light. I am finally going to dedicate myself to formally learn a few programming languages and actually understand that missing disconnect/mental block.\n\nI'm going to start with Rust. A language I've been very interested in for many years. A language I've pick up for a while and would just stop out of no where. I hope to finally get some certifications in Google, Amazon Web Services, Artificial intelligence, front-end/back-end development. On top of all this, I'd like to start my Master's in Information Technology in Education.\n\nI know I am capable of great things. I just hope I can acknowledge this.\n\nI promised to build something. It'll happen.\n\nThe bigger issues need to be solved through other channels. I also acknowledge this.\n\nI guess I should try to work on my body too.\n\nThat is all I got.\n\n-Pablo", "label": 1}
{"title": "The making of an igloo!", "url": "https://lifeofpablo.com/blog/the-making-of-an-igloo", "content": "The making of an igloo!\n\nThis post was written in English (en_US).\n\n\"Yesterday was quite the cold adventure! It involved snow but I mean A LOT of it! I love winter. Having snow days in colllege is the best thing that could happen to sleep-deprived students. I managed to get some extra shut eye. Seriously though, these snowdays make me feel like a little kid again. You do not know the excitement when I found out UNK canceled clasess to have not one but two snowdays in a row? Man I LOVE COLLEGE! Its music to my ears.\n\nInstead of being a lazy bum and staying in bed all day, I decided to spend my day outside in the good ol' Nebraska cold winter. My friends and I spend the day outside in the beautiful Nebraska weather, winter. The goal for the day was to make an igloo. We started right after lunch. My best friend, Sammy and I spent the day carving out a snow pile to make an igloo. Of course we could not go without throwing the occasional snowball at each other and others that walked by! We bad! ?We used our inner architecture and construction skills as well as our inner child to good use to make the perfect one! It was fun to work on it together!\n\nHere is the end result with me in it.\n\nQuestion of the day!??\n\nDid you spend your day outside? Shoot me a commnet or an email of your pictures or links at\n\nI decided to make an igloo today with my number 1 @officialsamharp . The outdoors are better! #friendshipgoals #outdoors #nebraska #lopers ?credz @audrey_irene_ A photo posted by Pablo Morales (@pmorales18) on Feb 2, 2016 at 4:59pm PST\n\n\"", "label": 1}
{"title": "Bringing Gemini intelligence to Google Home APIs", "url": "https://developers.googleblog.com/en/bringing-gemini-intelligence-to-google-home-apis/", "content": "The smart home is rapidly evolving into an intuitive ecosystem to make life easier, and its next era will be powered by Gemini and the Home APIs. This isn't just about connected devices; it's about creating effortless experiences. With the Home APIs, our goal has been to empower all developers to build innovative devices and experiences for the home. Now, with Gemini in the Home APIs, we're taking the next step: bringing the best of Google's AI directly to you. We're moving beyond simple device control to create an effortless smart home that truly understands, adapts, and responds to your users' needs. At Google I/O 2024, we announced the Home APIs, providing app developers with access to over 600M devices. We are excited to share that our ecosystem has grown even more to over 750M devices that developers now have access to along with Google's hubs and Matter infrastructure, and an automation engine powered by Google intelligence. We\u2019ve spent time rolling it out to a few early access partners, our Android and iOS SDKs are in public developer beta, and some developers have already leveraged the Home APIs to release new apps on Android.\n\nPartner experiences built with Home APIs Last year, we shared the innovative new ways partners like ADT, LG, and Eve built on Google Home, and now there are even more partners showcasing how Home APIs are making their customer experience even better:\n\nFirst Alert Control your smoke alarm from the First Alert app or the Google Home app and seamlessly interconnect with your existing Nest Protects.\n\nYale Yale\u2019s upcoming Matter lock, the successor to the Nest x Yale lock, takes advantage of the best-in-class lock features in the Google Home app, built using the Home APIs.\n\nCync Imagine your home automatically adjusting lighting and fan settings to ensure your pet's comfort when you're away. Cync is making this a reality.\n\niRobot Select iRobot Roomba\u00ae robots can create automations using Google Home presence sensing, so they can automatically clean your home when you leave the house, ensuring a spotless return.\n\nMotorola Moto Tag You can create custom smart home routines triggered by simple tag interactions, offering unparalleled personalization.\n\nTuya Smart Tuya Smart is enhancing seamless interoperability. Now, users can easily set up a Matter device and control devices connected to Google directly in Tuya Smart app.\n\nBringing your cameras to life with Gemini-powered Home APIs Last fall, we introduced Gemini-powered camera features in public preview in the Google Home app, allowing users to ask natural questions like, \u201cDid the kids leave their bikes in the driveway?\u201d and instantly get relevant video clips. Now we are bringing those camera experiences directly to developers too.\n\nWe're including the standard camera features you'd expect \u2013 like live streaming, event history access, two-way talk capabilities, and camera settings. But we're going further by integrating the Gemini-powered intelligence that our users love, such as AI descriptions and the ability to search camera history, making it easier to quickly identify what you are looking for in your camera history, keeping you and your family safer.\n\nMaking automation effortless with Gemini Figuring out the perfect automation to help improve your home experience and implementing it can be a daunting task many users don\u2019t want to undertake. So, we\u2019re introducing new Gemini-powered features to the Automations API designed to make creating powerful routines easier than ever: Suggested Automations: Gemini intelligently analyzes the devices in a user's home and proactively suggests potentially useful automations they might not have thought of.\n\nHelp me create: Building automations becomes as simple as a conversation. Users can tell Gemini what they want to achieve using natural language, and the automation is drafted for them.\n\nNew Automation Starters: We're adding more sophisticated triggers based on dates and weather conditions, allowing automations to respond more dynamically to the complexities of real life.\n\nThese new features will enable you to offer unprecedented Gemini-powered intelligent capabilities to your users more quickly than ever before.\n\nGemini across the Google Home surfaces The benefits don't stop within your app. When you integrate your devices using the Google Home APIs, they can participate in Gemini-powered experiences across Google's surfaces.\n\nFor instance, Google Home users, while in the Gemini app, can control and inquire about their smart home devices using natural language. We've also previewed Gemini enhancing the voice experience on smart speakers, smart displays, and Google TV, enabling more natural interaction, deeper exploration of topics, device control, and even voice-based automation creation. And we are testing a Home Summary Widget on Pixel with a select set of users, providing insights about your home without having to open an app!", "label": 0}
{"title": "Extending the Malbec subsea cable to Southern Brazil", "url": "https://engineering.fb.com/2025/05/22/connectivity/extending-malbec-subsea-cable-southern-brazil/", "content": "Meta is partnering with V.tal to extend the Malbec subsea cable to Porto Alegre, Brazil by 2027.\n\nWith this new extension, Malbec will become the first subsea cable to land in the state of Rio Grande do Sul, bringing more connectivity to millions of people in Southern Brazil and neighboring countries.\n\nMalbec will improve the scale and reliability of digital infrastructure in Porto Alegre, establishing it as a digital hub and improving online experiences across Southern Brazil, Argentina, Chile, Paraguay, and Uruguay.\n\nToday, we\u2019re announcing the extension of the Malbec subsea cable to the city of Porto Alegre, Brazil. Developed by Meta, in partnership with V.tal, Malbec is a 2,500 km cable that entered service in 2021 to provide connectivity between the Southern Cone of South America and Brazil. The new extension will be operational in 2027 and will link Porto Alegre to the cities of Rio de Janeiro and S\u00e3o Paulo, Brazil and Buenos Aires, Argentina.\n\n\u201cThe expansion of Malbec to Porto Alegre is a milestone for connectivity in South America, benefiting millions of people in Brazil and positioning the capital of Rio Grande do Sul as the first major international digital hub in the south of the country,\u201d explained Ana Luiza Valadares, Meta\u2019s Public Policy Director, Connectivity & Infra, LatAm. \u201cIt will contribute to attracting digital infrastructure companies, lowering costs for companies and improving consumer services.\u201d\n\nFelipe Campos, CEO of V.tal, added, \u201cThe impact of this project will be significant for the local digital economy, positioning Porto Alegre as a new connectivity hub. It will be a unique infrastructure that will attract the interest of operators and internet providers, as well as other submarine cable companies.\n\nIn addition, all the Southern Cone countries will benefit from this new ecosystem, not to mention the end users and companies who will have a better experience when using the internet and digital applications.\u201d\n\nThis extension is one of the latest in Meta\u2019s digital infrastructure investments to support growing demand for digital capacity, resilience, and global reach. Earlier this year, Meta also activated a Point of Presence (PoP) in Porto Alegre. PoPs facilitate the efficient delivery of content locally, which reduces the network management costs for internet service providers while improving the quality of experience for their customers. With the advent of AI and increasing demand for online services, digital infrastructure deployments play an important role in ensuring that the benefits of AI and other emerging technologies are available to everyone, regardless of where they live or work.\n\n\u201cThis investment in submarine connectivity, fully aligned with our Economic, Inclusive and Sustainable Development Plan, represents a strategic milestone for the state\u2019s future,\u201d said Rio Grand do Sul Governor, Eduardo Leite. \u201cFurthermore, it fosters artificial intelligence projects, technologies that are already transforming the present and will define the future of innovation, a sector in which Rio Grande do Sul is a leader in Brazil, according to the ranking of state competitiveness.\u201d\n\nMalbec will be the first international subsea cable to land in Rio Grande do Sul, bringing with it over 84 terabits of international capacity and direct connectivity to northern Brazil and Argentina. Like most subsea cables, local service providers will be able to acquire capacity on Malbec to serve additional bandwidth to millions of people in Brazil\u2019s southern states. The providers will also extend Malbec\u2019s capacity by connecting with providers in the neighboring countries of Argentina, Chile, Paraguay, and Uruguay, further positioning Brazil as a South American connectivity hub.", "label": 0}
{"title": "Hosting IndieWebCamp Sacramento and Calls for Co-Hosts", "url": "https://lifeofpablo.com/blog/hosting-indiewebcamp-sacramento-and-calls-for-co-hosts", "content": "Hosting IndieWebCamp Sacramento and Calls for Co-Hosts\n\nIndieWeb Logo\n\nThis post was written in English (en_US).\n\nHello, I'm Pablo Morales from Sacramento, California! I'm a participant in the IndieWeb.\n\nWhat is the IndieWeb?\n\nThe IndieWeb is a people-focused alternative to the \u201ccorporate web\u201d. It is a community of independent and personal websites connected by open standards and based on the principles of: owning your domain and using it as your primary online identity, publishing on your own site first (optionally elsewhere), and owning your content.\n\nI am volunteering to host and organize the first IndieWebCamp Sacramento (IWC). It is a great way to bring together your local IndieWeb community and people from the greater IndieWeb community. As the pandemic changed how we interact, online meet-ups have kept the IndieWeb going. It would be nice to meet many of the familiar faces we've seen through the last few years in person!\n\nOrganizing and planning an IndieWebCamp in person involves various parts such as finding co-organizers, planning where to have the event, and the many aspects of event planning.\n\nI'm looking for a co-host or co-host. A co-host would be someone who could assist me in planning for a successful event. It can be someone local near Sacramento or someone who can assist remotely. Having a co-host would help in making the event come to fruition and have the event be successful.\n\nIWC Dates:\n\nThe new plan is to host it during the fall or winter when the weather starts cooling off!\n\nLate September\n\nOctober\n\nNovember\n\nDecember\n\nIf you or someone else is interested in helping plan IndieWebCamp Sacramento, I'd love to have you! Send me an email by clicking here.\n\nLet's have this last half of the year be full of fun events and one of those can be in Sacramento! If you are interested in attending, more information will be available soon!\n\nAnyone and everyone is welcome to participate in the IndieWeb.\n\nThis blog post has been posted on IndieNews", "label": 1}
{"title": "How we reduced the size of our JavaScript bundles by 33%", "url": "https://dropbox.tech/frontend/how-we-reduced-the-size-of-our-javascript-bundles-by-33-percent", "content": "When was the last time you were about to click a button on a website, only to have the page shift\u2014causing you to click the wrong button instead? Or the last time you rage-quit a page that took too long to load? These problems are only amplified in applications as rich and interactive as ours. The more front-end code is written to support more complex features, the more bytes are sent to the browser to be parsed and executed, and the worse performance can get. At Dropbox, we understand how incredibly annoying such experiences can be. Over the past year, our web performance engineering team narrowed some of our performance problems down to an oft-overlooked culprit: the module bundler. Miller\u2019s Law states that the human brain can only hold so much information at any given time\u2014which is partially why most modern codebases (including ours) are broken up into smaller modules. A module bundler takes the various components of an application\u2014such as JavaScript and CSS\u2014and amalgamates them into bundles, which are then downloaded by the browser when a page is loaded. Most commonly, this takes the form of a minified JavaScript file that contains most of the logic for a web app. The first iteration of our module bundler was conceived way back in 2014\u2014around the time that performance-first approaches to module bundling were becoming more popular (most notably by Webpack and Rollup in 2012 and 2015, respectively). For this reason, it was quite barebones relative to more modern options; our module bundler didn\u2019t incorporate many performance optimizations and was onerous to work with, hampering our user experience and slowing down development velocity. As it became clear our existing bundler was showing its age, we decided the best way to optimize performance going forward would be to replace it. That was also the perfect time to do so since we were in the middle of migrating our pages to Edison\u2014our new web serving stack\u2014which presented an opportunity to piggyback on an existing migration plan and also provided an architecture that made it simpler to integrate a modern bundler into our static asset pipeline.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images\u2014plus generative AI capabilities across even more connected apps. See what's new \u2192\n\nExisting architecture\n\nWhile our existing bundler was relatively build-time efficient, it resulted in massive bundle sizes and proved to be a burden for engineers to maintain. We relied on engineers to manually define which scripts to bundle with a package, and we simply shipped all packages involved in rendering a page with few optimizations. Over time, the problems with this approach became clear: Problem #1: Multiple versions of bundled code\n\nUntil recently we used a custom web architecture called Dropbox Web Server (DWS). In short, each page consisted of multiple pagelets (i.e. subsections of pages), resulting in multiple JS entry points per page, with each servlet being served by its own controller on the backend. While this sped-up deployment in cases where a page was being worked on by multiple teams, it sometimes resulted in pagelets being on different backend code versions. This required DWS to support delivering separate versions of packaged code on the same page, which could potentially result in consistency issues (e.g. multiple instances of a singleton being loaded on the same page). Our migration to Edison would eliminate this pagelet architecture, giving us the flexibility to adopt a more industry-standard bundling scheme. Problem #2: Manual code-splitting\n\nCode splitting is the process of splitting a JavaScript bundle into smaller chunks, so that the browser only loads the parts of the codebase that are necessary for the current page. For example, assume a user visits dropbox.com/home, then dropbox.com/recents. Without code-splitting, the entire bundle.js is downloaded, which can significantly slow down the initial navigation to a page.\n\nAll code for all pages is served via a single file\n\nAfter code-splitting, however, only the chunks needed by the page are downloaded. This speeds up the initial navigation to dropbox.com/home, since less code is downloaded by the browser\u2014and has several additional benefits too. Critical scripts are loaded first, after which non-critical scripts are loaded, parsed, and executed asynchronously. Shared pieces of code are also cached by the browser, further reducing the amount of JavaScript downloaded when moving between pages. All of the above can greatly reduce the load time of web apps.\n\nOnly the new chunks that are needed for the page are downloaded\n\nSince our existing bundler didn\u2019t have any built-in code-splitting, engineers had to manually define packages. More specifically, our packaging map was a massive 6,000+ line dictionary that specified which modules were included in which package. As you can imagine, this became incredibly complex to maintain over time. To avoid sub-optimal packaging, we enforced a rigorous set of tests\u2014the packager tests\u2014which became dreaded by engineers since they would often require a manual reshuffling of modules with each change. This also resulted in a lot more code than what was needed by certain pages. For instance, assume we have the following package map:\n\nCopy { \"pkg-a\": [\"a\", \"b\"], \"pkg-c\": [\"c\", \"d\"], }\n\nIf a page depends on modules a, b, and c, the browser would only need to make two HTTP calls (i.e. to fetch pkg-a and pkg-b) instead of three separate calls, once per module. While this would reduce the HTTP call overhead, it would often result in having to load unnecessary modules\u2014in this case, module d. Not only were we loading unnecessary code due to a lack of tree shaking, but we were also loading entire modules that weren\u2019t necessary for a page, resulting in an overall slower user experience. Problem #3: No tree shaking\n\nTree shaking is a bundle-optimization technique to reduce bundle sizes by eliminating unused code. Let\u2019s assume your app imports a third-party library that contains several modules. Without tree shaking, much of the bundled code is unused.\n\nAll code is bundled, regardless of whether or not it\u2019s used\n\nWith tree shaking, the static structure of the code is analyzed and any code that is not directly referenced by other code is removed. This results in a final bundle that is much leaner.\n\nOnly used code is bundled\n\nSince our existing bundler was barebones, there wasn\u2019t any tree shaking functionality either. The resulting packages would often contain large swaths of unused code, especially from third-party libraries, which translated to unnecessarily longer wait times for page loads. Also, since we used Protobuf definitions for efficient data transfer from the front-end to the back-end, instrumenting certain observability metrics would often end up introducing several additional megabytes of unused code!\n\nWhy Rollup\n\nAlthough we considered many solutions over the years, we realized that our primary requirement was having certain features like automatic code-splitting, tree shaking, and, optionally, some plugins for further optimizing the bundling pipeline. Rollup was the most mature at the time and most flexible to incorporate into our existing build pipeline, which is mainly why we settled on it. Another reason: less engineering overhead. Since we were already using Rollup for bundling our NPM modules (albeit without many of its useful features), expanding our adoption of Rollup would require less engineering overhead than integrating an entirely foreign tool in our build process. Additionally, this meant that we had more engineering expertise with Rollup\u2019s quirks in our codebase versus that of other bundlers, reducing the the likelihood of so-called unknown unknowns. Also, replicating Rollup\u2019s features within our existing module bundler would require significantly more engineering time than if we just integrated Rollup more deeply in our build process.\n\nRollup rollout\n\nWe knew that rolling out a module bundler safely and gradually would be no easy feat, especially since we\u2019d need to reliably support two module bundlers (and consequently, two different sets of generated bundles) at the same time. Our primary concerns included ensuring stable and bug-free bundled code, the increased load on our build systems and CI, and how we would incentivize teams to opt-in to using Rollup bundles for the pages they owned. With reliability and scalability in mind, we divided the rollout process to four stages: The developer preview stage allowed engineers to opt-in to Rollup bundles in their dev environment. This allowed us to effectively crowdsource QA testing by having developers surface any unexpected application behavior introduced by Rollup bundles early on, giving us plenty of time to address bugs and scope changes.\n\nstage allowed engineers to opt-in to Rollup bundles in their dev environment. This allowed us to effectively crowdsource QA testing by having developers surface any unexpected application behavior introduced by Rollup bundles early on, giving us plenty of time to address bugs and scope changes. The Dropboxer preview stage involved serving Rollup bundles to all internal Dropbox employees, which allowed us to gather early performance data and further gather feedback on any application behavioral changes.\n\nstage involved serving Rollup bundles to all internal Dropbox employees, which allowed us to gather early performance data and further gather feedback on any application behavioral changes. The general availability stage involved gradually rolling out to all Dropbox users, both internal and external. This only happened once our Rollup packaging was thoroughly tested and deemed stable enough for users.\n\nstage involved gradually rolling out to all Dropbox users, both internal and external. This only happened once our Rollup packaging was thoroughly tested and deemed stable enough for users. The maintenance stage involved addressing any tech debt left over in the project and iterating on our use of Rollup to further optimize performance and the developer experience. We realized that projects of such a massive scale will inevitably end up accumulating some tech debt, and we should proactively plan to address it at some stage instead of sweeping it under the rug. To support each of these stages, we used a mix of cookie-based gating and our in-house feature-gating system. Historically, most rollouts at Dropbox are exclusively done using our in-house feature gating system; however, we decided to allow cookie-based gating to quickly toggle between Rollup and legacy packages, which sped up debugging. Nested within each of these rollout stages were gradual rollouts, which involved ramping up from 1%, 10%, 25%, 50%, to 100%. This gave us the flexibility to collect early performance and stability results\u2014and to seamlessly roll-back any breaking changes if they occurred\u2014while minimizing impact to both internal and external users. Because of the large number of pages we had to migrate, we not only needed a strategy to switch pages over to Rollup safely, but also to incentivize page owners to switch in the first place. Since our web stack was about to undergo a major renovation with Edison, we realized that piggybacking on Edison\u2019s rollout could solve both our problems. If Rollup was an Edison-only feature, developer teams would have greater incentive to migrate to both Rollup and Edison, and we could tightly couple our migration strategy with Edison\u2019s too. Edison was also expected to have its own performance and development velocity improvements. We figured that coupling Edison and Rollup together would have a transformational synergy strongly felt throughout the company.\n\nChallenges and roadblocks\n\nWhile we did expect to run into some unexpected challenges, we realized that daisy-chaining one build system (Rollup) with another (our existing Bazel-based infrastructure) proved to be more challenging than anticipated. Firstly, running two different module bundlers at the same time proved to be more resource-intensive than we estimated. Rollup\u2019s tree-shaking algorithm, while quite mature, still had to load all modules into memory and generate the abstract syntax trees needed to analyze relationships and shake code out. Also, our integration of Rollup into Bazel limited us in being able to cache intermediary build results, requiring our CI to rebuild and re-minify all Rollup chunks on each build. This caused our CI builds to time-out due to memory exhaustion, and delayed the rollout significantly. We also found several bugs with Rollup\u2019s tree-shaking algorithm which resulted in overly aggressive tree shaking. Thankfully, this only resulted in minor bugs that were caught and fixed during the developer preview phase without ever impacting our users. Additionally, we found that our legacy bundler was serving some code from third-party libraries that was incompatible with JavaScript\u2019s strict mode. Serving this same code via the new bundler with strict mode enabled resulted in fail-hard runtime errors in the browser. This required us to conduct a one-time audit of our entire codebase and patch code that was incompatible with strict mode. Finally, during the Dropboxer preview phase, we found that our A/B telemetry metrics between Rollup and the legacy bundler weren\u2019t showing as much of a TTVC improvement as we expected. We eventually narrowed this down to Rollup producing a lot more chunks than what our legacy packager produced. Although we initially hypothesized that HTTP2\u2019s multiplexing would negate any performance degradations from a greater number of chunks, we found that too many chunks would result in the browser spending significantly more time in discovering all the modules needed for the page. Increasing the number of chunks also resulted in lower compression efficiency, since compression algorithms such as Zlib use a sliding-window approach to compression, which results in greater compression efficiency for one large file vs. many smaller files.\n\n\n\nResults", "label": 0}
{"title": "Explore the latest updates on Google Wallet", "url": "https://developers.googleblog.com/en/explore-the-latest-updates-google-wallet-io-25/", "content": "Last year, we were thrilled to expand access to Google Wallet for users in more than 90 countries and territories. Recently we expanded Google Wallet adding +50 more countries, allowing users to view and use digital passes in the app and on the web. And we've worked hard to make Google Wallet even more robust, flexible, and ultimately, more feature-rich for you, our developer community.\n\nLet\u2019s dive into the exciting new capabilities we\u2019ve announced at our I/O session this year.\n\n\n\nDigital IDs: A foundation of trust, ease, and interoperability\n\nDigital IDs in Google Wallet are live today, built with trust, ease of use, and interoperability as our top priorities. We're committed to bringing more digital IDs to Google Wallet to support our users across the globe. Now, it\u2019s easier to prove age and identity with Google Wallet.\n\nExpanding Availability: Residents in Arkansas, Montana, Puerto Rico and West Virginia will soon be able to save their government-issued digital IDs to Google Wallet. And in Arizona, Georgia, Maryland and New Mexico, users will also be able to use their mobile IDs at the DMV for improved and streamlined customer experiences.\n\nU.K. Passport Support: U.K. passport holders will soon be able to create digital ID passes with their U.K. passports and securely and conveniently store them in Google Wallet. At launch, we\u2019re partnering with Rail Delivery Group, which will offer train travellers the opportunity to use their digital ID to verify that they meet the eligibility criteria for select Railcards on its Railcard retailing platform railcard.co.uk.\n\nNew use cases: New use cases are on the way in collaboration with our strong partner ecosystem. Soon, you'll be able to use your digital ID to recover Amazon accounts, access online health services with CVS Health and MyChart by Epic, verify profiles on platforms like Uber and more.\n\nIntroducing the Digital Credentials API: To empower you to leverage these digital IDs, we collaborated with ecosystem partners in the W3C to develop our Digital Credentials API. This unified and secure framework allows apps and websites to request verifiable proof of age or identity directly from any digital wallet on a user's device.\n\n\n\nConnecting with families: Google Wallet enabled for kids\n\nWe're very excited to provide parents and guardians a way to allow their children to access Google Wallet with appropriate supervision.\n\nParents and guardians in select countries can now allow their kids to tap and pay in stores, plus keep supported passes like event tickets, library cards, and gift cards, all in one place, on their Android devices. Safety is key, so parents have controls with Family Link. They will get notified by email about every transaction, and can easily track recent purchases, remove payment cards, and turn off passes access in Family Link.\n\nBest of all, there are no changes in the Google Wallet API to support this enhancement.\n\n\n\nElevating user engagement: more granular notifications\n\nLast year, we expanded mobile push notifications for the Google Wallet API, empowering you to deliver timely updates to your users. This year, we're taking it a step further with the introduction of field update notifications.\n\nImagine being able to trigger a push notification not just on a general update, but specifically when a particular field within a pass changes. For example, when a user's points balance crosses a threshold, or when their tier status is upgraded, a push notification can be triggered which will notify them that the data on their pass has changed. This granularity allows for a much more engaging and user-centric experience, driving higher engagement and utility on the pass.\n\n\n\nProximity power: introducing Nearby Passes Notifications\n\nSpeaking of notifications, we\u2019re really excited to announce that we will support Nearby Passes notifications for the Google Wallet API. If enabled by the user, this feature is designed to provide timely and relevant information by alerting users about pertinent passes, such as loyalty cards, offers, boarding passes, or event tickets, when they approach a designated point of interest.\n\nThe Google Wallet app sends a contextual notification when the user is near a specific location. This notification serves as a direct gateway, allowing users to seamlessly access the associated pass with a single tap. This direct access promotes a more fluid and intuitive interaction with the stored passes, encouraging users to leverage the wallet's capabilities more effectively.\n\nTo ensure user control and flexibility, we\u2019ve introduced two new toggles to help users control their notification experience. The first is on the pass details screen, that allows users to turn on or off notifications from that specific pass. This applies to all notifications related to your pass, including field updates, and nearby passes notifications. The second is through the Nearby Passes notifications channel that allows users to control whether they receive nearby passes notifications. This empowers users to tailor their notification settings based on their specific needs and preferences.\n\n\n\nBeyond transactions: unlocking Value Added Opportunities\n\nEngaging users goes beyond just notifications. To address this, we\u2019re introducing Value Added Opportunities which empowers you to integrate personalized modules directly into your passes, showcasing relevant deals, promotions, and additional services.\n\nThis is a significant step towards transforming passes into a more dynamic engagement surface for you and your users. By highlighting these value-added benefits, such as exclusive offers or upgrade options, you can guide users back to your app or website, creating a dynamic gateway for ongoing user interaction.\n\n\n\nBridging the gap: introducing the Pass Upgrade experience\n\nUsers have saved millions of passes by manually adding their loyalty cards to Google Wallet. These passes are unlinked from their merchant accounts, and, as a developer, you can\u2019t update, or engage users on these passes. We\u2019ve now introduced a Pass Upgrade experience that prompts users to sign into their merchant account and save a linked version of the pass. All you need to do is integrate with the Wallet API\u2019s User Loyalty Program Enrollment feature, and ensure that users can save the linked pass once they successfully sign in. We\u2019ll be expanding on this feature in the future to enable users who have added a pass using our \u201ceverything else\u201d feature to link to the merchant account as well.\n\n\n\nSeamless journeys: enhancements for travel\n\nAt last year's I/O, we announced Auto Linked Passes, allowing you to add an additional related pass automatically to your users\u2019 Google Wallet provided they already have an existing pass issued by you. Now, we're happy to announce an expansion of this feature for airlines. Developers from airlines that integrate loyalty cards for their frequent flyer programs with the Google Wallet API can automatically push boarding passes to their users\u2019 wallets once they check in for a flight.\n\nGoogle Wallet users already benefit from streamlined travel on open-loop EMV transit systems, using tokenized payment methods for seamless fare transactions. Our solution further enhances this experience by providing riders with detailed journey and fare construction details. The tokenized open loop payment card acts as a bridge between user payments and transit systems, facilitating this communication.\n\nBuilding on this foundation, we're excited to announce the upcoming expansion of Google Transit Insights to support specific pass types sales, such as season passes, directly linked to the users' tokenized open loop payment card. With this new capability, developers will soon be able to leverage the Google Wallet APIs to implement seamless pass purchasing and management, eliminating the need for separate transit cards.\n\nUsers of Google Wallet will now see real-time transit pass updates, such as on-time or delayed train status, directly on their passes. This enhanced experience is powered by the Google Wallet API's new live status support and a seamless integration with Google Maps. Train operators can enable this feature by ensuring their tickets contain the required fields and providing a real-time GTFS feed, making it accessible to major operators worldwide.\n\n\n\nPersonalization and security: Secure Private Images\n\nFor some use cases, it\u2019s important for you to create a pass that includes a profile photo of the user you\u2019re distributing the pass to. To achieve this, we\u2019re enabling Secure Private Images on passes.\n\nWith this feature, you\u2019re able to define passes that include images that are only accessible to the holder of the pass. For example, this allows you to create digital business cards, membership passes, or event tickets with the user profile image on it.\n\nPlease note that this feature can\u2019t be used for official identity verification.\n\n\n\nLearn more and build with Google Wallet\n\nGoogle Wallet continues to evolve, offering developers powerful new capabilities to create richer, more engaging user experiences. From expanding digital identity features like the Digital Credentials API and secure private images, to enhancing user engagement with granular notifications and Value Added Opportunities, and streamlining travel with expanded Auto Linked Passes and real-time transit updates, these features are designed to help you build innovative solutions that connect more deeply with your users and unlock new value within the Google Wallet platform.\n\nPlease take a look at the following resources to learn more:\n\nLearn everything about Google Wallet in the developer documentation website.\n\nStay tuned with upcoming and past events at our events page.\n\nTry one of our codelabs to have a hands-on code experience.\n\nUse one of our client libraries for your favorite language/platform.", "label": 0}
{"title": "Technical Decision-Making in a Fragmented Space: Spotify In-Car Case Study", "url": "https://engineering.atspotify.com/2024/7/technical-decision-making-in-a-fragmented-space-spotify-in-car-case-study", "content": "Car rides have become connected and interactive these days with drivers jamming to music or catching up on podcasts or audiobooks while they\u2019re on the road. A big portion of Spotify listening happens in the car \u2014 a key reason why it\u2019s important for Spotify to ensure users have a smooth experience in the car, striking the perfect balance between safety, simplicity, and control. Achieving such balance is not easy due to various challenges: to maintain interoperability among different Spotify experiences, provide seamless usage on all kinds of screens, and manage drivers\u2019 limited focus.\n\nInteroperability among different experiences\n\nThere are three ways to play Spotify in your car:\n\nUsing a Bluetooth connection: This method utilizes the car\u2019s speakers as audio outputs. Users can execute basic playback operations such as play/pause, skip forward, and skip backward, using controls located on the car\u2019s media head unit and steering wheel. Via a projected experience: Examples of this include Apple CarPlay and Android Auto, which project phone applications onto the car\u2019s media unit. Through the projected Spotify app, users gain access to more-advanced functionalities such as browsing playlists, conducting searches, and utilizing a specialized Now Playing view, in addition to the basic playback controls mentioned above. With an embedded app: An increasing number of car manufacturers are integrating specialized operating systems into their vehicles\u2019 hardware. These systems provide access to the car\u2019s hardware and offer a dedicated media unit to the driver, featuring standalone media and navigation applications. Tesla and Android Automotive cars are prime examples of this integration.\n\nIt\u2019s worth noting that most cars come equipped with Bluetooth capabilities, and on top of that, many modern vehicles offer projected and/or embedded experiences. This means that Spotify can provide users with multiple experiences within a single vehicle. Moreover, considering other surfaces where the app is utilized simultaneously (smart speakers, TVs, smartwatches, etc.), it is imperative that Spotify operates seamlessly across all platforms.\n\nDissimilar screen dimensions\n\nCars are equipped with screens of various sizes and dimensions, ranging from compact displays that barely show the track name to larger ones akin to a tablet. Adding to the complexity, there are many ways users interact with these screens. They might use physical buttons, rotary knobs, touch screens, buttons on the steering wheel, and even voice to manage music playback and adjust volume. On top of this, the car manufacturers also strive for fragmentation to better position their unique offerings, which makes tackling the automotive domain even harder for developers. Naturally, Spotify is expected to work seamlessly across all these different modalities for input.\n\nLimited focus from user\n\nWhen it comes to selecting what to play in the car, it\u2019s a bit different compared to other situations, where the user can devote their full attention. Safety and navigation take top priority for drivers, which adds a layer of complexity to the experience. The principles are as follows:\n\nMinimizing cognitive load: Since selecting and playing music isn\u2019t typically the first thing on a driver\u2019s mind, Spotify needs to step in to make it as easy as possible. The goal is to reduce the mental effort required, ensuring that choosing music is simple and doesn\u2019t distract from the primary task of driving. Reducing distractions: Our goal is to keep drivers focused on the road at all times. That means minimizing anything that could pull their attention away, like unexpected interruptions in audio playback or confusion about what\u2019s playing. These distractions can cause drivers to glance away from the road, which must be avoided.\n\nBy addressing these challenges and ensuring that Spotify integrates seamlessly into the driving experience \u2014 without compromising safety \u2014 we can create a safer and more enjoyable audio experience for drivers.\n\nOur approach for technical decision-making\n\nGiven the assortment of challenges, the automotive domain is often seen as fragmented. Bringing together teams from different parts of the company is crucial to address these complexities, and effective modes of technical decision-making are paramount.\n\nBecause of our distributed workforce, Slack serves as our primary tool for communication. However, it\u2019s not always the platform most conducive to in-depth technical discussions, as conversations tend to get lost in various threads, and the level of detail may not suit all stakeholders.\n\nFor technical decision-making, we prefer Request for Comments (RFC) documents. RFCs focus on a single problem, allow stakeholders to discuss that problem asynchronously, and refine the solution. You can find various templates for writing an RFC online, but the gist of it is that you start by describing a problem statement, give a little context around it, then move on to share your proposed solution for the problem along with some alternate solutions. When you\u2019re ready, you can share the RFC document with stakeholders to get feedback on your suggested approach.\n\nMaking the most out of RFCs\n\nBreak down the problem (if applicable).\n\nTechnical problems come in all shapes and sizes. Some are small enough to conveniently fit one RFC, and others are hefty \u2013 the ones that cannot be squeezed into just one RFC. Addressing such issues with a single RFC can result in a long and complicated document, which is challenging to understand and can overwhelm the reader.\n\nWhen we encountered such large issues, we came up with a plan \u2014 to break down the problem into smaller, bite-size pieces. Therefore, we first presented a big-picture RFC. This gave all stakeholders a bird\u2019s-eye view of the problem and framed potential high-level solutions, just to get the ball rolling. Once we had a decision on that, we started producing smaller, focused RFCs, each one diving deep into a specific issue, and shared these with the relevant teams.\n\nFor example: consider automatic playback when a car is connected. We expect that when a car is connected, Spotify should start playing music based on the user\u2019s recent listening history. The bird\u2019s-eye RFC on the topic contained high-level details like architectural design: Should this be a completely new system, or can we add this feature to an existing system? How would the API look? What content should be played? Etc. Once the architecture had been finalized, we moved on to more detailed RFCs covering topics such as the following:\n\nDetecting a car connection signal\n\nDetermining the circumstances when automatic playback should not be triggered\n\nExperimenting with different types of content to provide the best user experience\n\nFiltering content considering that the user is in the car\n\nThe audience for these RFCs varied based on the topic. For instance, content-related RFCs were shared only with content teams, while car-signal-related RFCs were shared with teams responsible for the connected accessories space.\n\nThis approach makes problem-solving way less daunting, keeps tasks manageable, and prevents information overload, ensuring that each stakeholder team can focus on the aspects most relevant to them.\n\nInclude diagrams and code snippets where feasible.\n\nWe often face the challenge of explaining significant changes in our systems, and clarity is crucial in these situations. So, what do we do? We rely on visuals \u2014 diagrams, code snippets, and even draft pull requests when necessary.\n\nDiagrams can easily show how things are connected, how components depend on one another, and how work flows, making complex ideas easier to understand. Apart from using architectural diagrams in our RFCs, we also use diagrams to show user journeys, with the aim of highlighting shortcomings in our current systems.\n\nAs for code, it is our trusty go-to for showing standardized examples for implementing specific protocols or functionalities. It works best when suggesting changes to other teams\u2019 systems. By keeping things consistent across different setups or platforms, we make sure everyone is on the same page.\n\nRevisiting the example of automatic playback in the car, we created an RFC to highlight various situations where automatic playback could fail and proposed actions to prevent such failures. This RFC was intended for external teams that owned the systems we depended on. To clearly illustrate the problem areas, we used various user journey diagrams, providing a visual overview at a glance. Additionally, we included code examples to demonstrate the suggested improvements, making the technical solutions more concrete and actionable.\n\nUsing diagrams and code snippets throughout our RFCs provides readers with more context and details without unnecessary repetition, like giving readers a road map and a manual, all in one, making everything well-organized and easy to follow.\n\nUtilize RFCs for problem-solving and encouraging collaboration.\n\nSometimes we encounter issues that leave us even more puzzled. One such instance involved an issue with playback in the car that had been initially flagged as a bug. Turns out, it was a tricky one to reproduce in our usual testing setup, but as we dug deeper, it seemed like it could be part of a larger issue spanning multiple teams, with a more significant impact than we had initially anticipated.\n\nTo understand and map out the unknowns with the expertise of other related teams, we compiled everything that we knew into an RFC. We explained our part of the system and left sections for other affected or participating teams to fill out. This approach allowed us to present the problem comprehensively and provided a platform for all stakeholders to verify our findings, while also facilitating easier buy-in on proposed solutions.\n\nInstead of patching the issue locally, the collaborative process helped us to consider all the perspectives and assess the potential impact before sorting through possible fixes and prioritizing what to tackle first.\n\nUse RFCs to align between different functions.\n\nOne of the remarkable aspects of RFCs is that because of the way they are normally structured, they can bring people from different functions of a company together to tackle tough problems. It starts with defining the problem, diving into why the problem needs to be fixed, and finally, brainstorming ideas for how to solve it.\n\nThis method worked like a charm for us when we were deciding on new logging for a feature. We used RFCs to align between Product, Engineering, and Product Insights teams. Our product managers laid out what we needed and why, while the engineers and data scientists brainstormed ideas for how to make it happen.\n\nFor another example: implementing the support for features like AI DJ and Jam in cars. When implementing support for any external feature in the car, there are some key questions to ask, such as:\n\nDoes the current architecture support this new feature or does it need any enhancements?\n\nDoes the car integration need more information than currently exposed by core API?\n\nHow will the logging work to determine that the listening is happening in a car? And so on.\n\nRFCs always provide a good way for the engineers, product managers, and data scientists from different verticals of the company to come together and find optimal solutions to such issues.\n\nConclusion\n\nNavigating the integration of Spotify into the fragmented automotive ecosystem has posed its challenges, requiring intricate solutions. Throughout this journey, RFCs have proven to be a valuable tool for facilitating collaboration, iteration, and informed decision-making in the development and implementation of our systems.\n\nWhile we\u2019ve fine-tuned RFCs to align with our organizational needs over the years, it\u2019s essential to recognize that every team is unique. What proved effective for us may not necessarily yield the same results for others. Therefore, we encourage teams to experiment and explore the full potential of RFCs within their own contexts.\n\nBy embracing flexibility and innovation, teams can discover new avenues for leveraging RFCs, ultimately enhancing their collaboration and decision-making processes.\n\nSpecial thanks to Marko Tiidla and Robert Abramczyk for all the guidance and support during writing of this blog post.", "label": 0}
{"title": "LiteRT: Maximum performance, simplified", "url": "https://developers.googleblog.com/en/litert-maximum-performance-simplified/", "content": "Over the past decade, mobile phones have incorporated increasingly powerful purpose-specific accelerators including GPUs and recently, more powerful NPUs (Neural Processing Units). By accelerating your AI models on mobile GPUs and NPUs, you can speed up your models by up to 25x compared to CPU while also reducing power consumption by up to 5x. However, unlocking these outstanding performance benefits has proven challenging for most developers, as it requires wrangling HW-specific APIs in case of GPU inference or wrangling vendor-specific SDKs, formats, and runtimes for NPU inference. Listening to your feedback, the Google AI Edge team is excited to announce multiple improvements to LiteRT solving the challenges above, and accelerating AI on mobile more easily with increased performance. Our new release includes a new LiteRT API making on-device ML inference easier than ever, our latest cutting-edge GPU acceleration, new NPU support co-developed with MediaTek and Qualcomm (open for early access), and advanced inference features to maximize performance for on-device applications. Let\u2019s dive in!\n\nMLDrift: Best GPU Acceleration Yet GPUs have always been at the heart of LiteRT\u2019s acceleration story, providing the broadest support and most consistent performance improvement. MLDrift, our latest version of GPU acceleration, pushes the bar even further with faster performance and improvements to support models of a significantly larger size through: Smarter Data Organization: MLDrift arranges data in a more efficient way by using optimized tensor layouts and storage types specifically tailored for how GPUs process data, reducing memory access time and speeding up AI calculations. Workgroup Optimization: Smart computation based on context (stage) and resource constraints Improved Data Handling: Streamlining the way the accelerator receives and sends out tensor data to reduce overhead in data transfer and conversion optimizing for data locality.\n\nThis results in significantly faster performance than CPUs, than previous versions of our TFLite GPU delegate, and even other GPU enabled frameworks particularly for CNN and Transformer models.\n\nFigure: Inference latency per model of LiteRT GPU compared to TFLite GPU, measured on Samsung 24.\n\nFind examples in our documentation and give GPU acceleration a try today.\n\nMediaTek and Qualcomm NPU Support NPUs, AI specific accelerators, are becoming increasingly common in flagship phones. They allow you to run AI models much more efficiently, and in many cases much faster. In our internal testing compared to CPUs this acceleration can be up to 25x faster, and 5x more power efficient. (May 2025, based on internal testing) Typically, each vendor provides their own SDKs, including compilers, runtime, and other dependencies, to compile and execute models on their SoCs. The SDK must precisely match the specific SoC version and requires proper download and installation. LiteRT now provides a uniform way to develop and deploy models on NPUs, abstracting away all these complexities. Vendor compiler distribution: When installing the LiteRT PyPI package, we will automatically download the vendor SDKs for compiling models. Model and vendor runtime distribution: The compiled model and SoC runtime will need to be distributed with the app. As a developer you can handle this distribution yourself, or you can have Google Play distribute them for you. In our example code you can see how to use AI Packs and Feature Delivery to deliver the right model and runtime to the right device.\n\nWe\u2019re excited to partner with MediaTek and Qualcomm to allow developers to accelerate a wide variety of classic ML models, such as vision, audio, and NLP models, on MediaTek and Qualcomm NPUs. Increased model and domain support will continue over the coming year. This feature is available in private preview. For early access apply here.\n\nSimplified GPU and NPU Hardware Acceleration We\u2019ve made GPUs and NPUs easier than ever to use by simplifying the process in the latest version of the LiteRT APIs. With the latest changes, we have simplified the setup significantly with the ability to specify the target backend as an option. As an example, this is how a developer would specify GPU acceleration:\n\n// 1. Load model. auto model = *Model::Load(\"mymodel.tflite\"); // 2. Create a compiled model targeting GPU. auto compiled_model = *CompiledModel::Create(model, kLiteRtHwAcceleratorGpu); C++ Copied\n\nAs you can see, the new CompiledModel API greatly simplifies how to specify the model and target backend(s) for acceleration.\n\nAdvanced Inference for Performance Optimization While using high performance backends is helpful, optimal performance of your application can be hindered by memory, or processor bottlenecks. With the new LiteRT APIs, you can address these challenges by leveraging built-in buffer interoperability to eliminate costly memory copy operations, and asynchronous execution to utilize idle processors in parallel.\n\nSeamless Buffer Interoperability The new TensorBuffer API provides an efficient way to handle input/output data with LiteRT. It allows you to directly use data residing in hardware memory, such as OpenGL Buffers, as inputs or outputs for your CompiledModel, completely eliminating the need for costly CPU copies.\n\nauto tensor_buffer = *litert::TensorBuffer::CreateFromGlBuffer(tensor_type, opengl_buffer); C++ Copied\n\nThis significantly reduces unnecessary CPU overhead and boosts performance.\n\n\n\nAdditionally, the TensorBuffer API enables seamless copy-free conversions between different hardware memory types when supported by the system. Imagine effortlessly transforming data from an OpenGL Buffer to an OpenCL Buffer or even to an Android HardwareBuffer without any intermediate CPU transfers. This technique is key to handling the increasing data volumes and demanding performance required by increasingly complex AI models. You can find examples in our documentation on how to use TensorBuffer.\n\nAsynchronous Execution Asynchronous execution allows different parts of the AI model or independent tasks to run concurrently across CPU, GPU, and NPUs allowing you to opportunistically leverage available compute cycles from different processors to improve efficiency and responsiveness. For instance: the CPU might handle data preprocessing the GPU could accelerate matrix multiplications in a neural network layer, and the NPU might efficiently manage specific inference tasks \u2013 all happening in parallel.\n\nIn applications which require real-time AI interactions, a task can be initiated on one processor and continue with other operations on another. Parallel processing minimizes latency and provides a smoother, more interactive user experience. By effectively managing and overlapping computations across multiple processors, asynchronous execution maximizes system throughput and ensures that the AI application remains fluid and reactive, even under heavy computational loads. Async execution is implemented by using OS-level mechanisms (e.g., sync fences on Android/Linux) allowing one HW accelerator to trigger upon the completion of another HW accelerator directly without involving the CPU. This reduces latency (up to 2x in our GPU async demo) and power consumption while making the pipeline more deterministic. Here is the code snippet showing async inference with OpenGL buffer input:\n\n// Create an input TensorBuffer based on tensor_type that wraps the given OpenGL // Buffer. env is an LiteRT environment to use existing EGL display and context. auto tensor_buffer_from_opengl = *litert::TensorBuffer::CreateFromGlBuffer(env, tensor_type, opengl_buffer); // Create an input event and attach it to the input buffer. Internally, it // creates and inserts a fence sync object into the current EGL command queue. auto input_event = *Event::CreateManaged(env, LiteRtEventTypeEglSyncFence); tensor_buffer_from_opengl.SetEvent(std::move(input_event)); // Create the input and output TensorBuffers\u2026 // Run async inference compiled_model1.RunAsync(input_buffers, output_buffers); C++ Copied", "label": 0}
{"title": "Building Confidence: A Case Study in How to Create Confidence Scores for GenAI Applications", "url": "https://engineering.atspotify.com/2024/12/building-confidence-a-case-study-in-how-to-create-confidence-scores-for-genai-applications", "content": "TL;DR Getting a response from GenAI is quick and straightforward. But what about the confidence level for that response? In certain applications, especially in the financial domain, confidence scores are required. In a document-parsing task related to financial automation, we tested three approaches to address confidence level: calibrator models, logarithmic probabilities (logprobs), and majority voting. Majority voting turned out to be the most performant technique. Although the idea is relatively simple, the devil is in the details.\n\nIntroduction\n\nThe introduction of GenAI technology has proven to be revolutionary in the current business landscape, including its improvements to internal business efficiency. Repetitive tasks that were once burdensome to humans can be automated by GenAI. Compared to traditional deterministic methods or machine learning (ML) models, using GenAI has the following benefits:\n\nFast development: Rapid training, testing, and implementation reduce development time.\n\nScalability and flexibility: GenAI can easily scale to incorporate new cases with simple prompt adjustments.\n\nMaintainability: In many cases, GenAI-powered solutions are easier to maintain than traditional ML models, making them more cost-effective over time.\n\nDespite its advantages, GenAI faces significant challenges in accuracy and reliability, with its lack of confidence scores and its hallucinations.\n\nConfidence levels are crucial in building trust and informing decisions, but they are not integral to AI technologies. In this post, we will explore how we created confidence scores in GenAI applications for a financial automation use case, detailing the approaches, important implementation details, and challenges/limits.\n\nAssessing confidence in a financial automation application\n\nOur team sits in the Financial Engineering org at Spotify and aims to enhance efficiency in financial processes by automation. Recently we worked on automating invoice parsing, which was part of a larger initiative to streamline invoice processing. The invoices, from a large distribution of vendors globally, come in various languages, formats, and structures. This complexity makes deterministic models inadequate, as they struggle with high numbers of edge cases and with ambiguous and incomplete data.\n\nGenAI, on the other hand, has proven to be more versatile and adaptable. It can also make adequate inferences for complex tasks such as invoice parsing. However, a significant challenge arises while employing GenAI models in this task: confidence scores must be produced to support human-in-the-loop decisions and to meet regulatory requirements like IT general controls (ITGC) and the Sarbanes\u2013Oxley Act (SOX). Specific thresholds need to be crossed for the models\u2019 outputs to be trusted. To address the need, we researched the following approaches and implemented a reliable way to generate confidence scores for GenAI model outputs.\n\nApproaches evaluated\n\nCalibrator\n\n\u201cCalibrator\u201d refers to using a separate single GenAI model to evaluate the outputs generated by other models and assign confidence scores. This approach offers an independent and unbiased evaluation of the outputs from other models. The calibrator model is also trainable by learning from feedback and can potentially improve over time.\n\nHowever, confidence scores generated by calibrators are difficult to interpret and sometimes counterintuitive. Besides, the scores are not consistent across multiple runs. In financial applications where confidence in technology is crucial, unclear scoring and inconsistencies are not acceptable.\n\nLogprobs\n\nLogprobs, or logarithmic probabilities, are the log-transformed probabilities for each token (a unit of text, such as a word or part of a word) in a generated output. These probabilities indicate how confident the model is in choosing a token. A higher (less negative) logprob means the model is more confident in the choice. However, the methodology is not always transparent \u2013 it varies by provider, and for many models it is unknown how they were calculated.\n\nThe confidence of an entire response can be deduced by combining the logprobs of its tokens. We averaged the logprobs instead of summing to normalize for output length. The confidence score was then calculated by exponentiating the average logprobs of the tokens.\n\nWe tested the approach for different extraction fields in our application. We plotted the results for one numeric field (invoice total) and one text field (invoice number) against the accuracy in the figure below. No clear correlation was found between lobprob-based confidence score and accuracy, and the conclusion holds for other text and numeric fields as well. The lack of correlation suggests that averaging logprobs is not a reliable measure of overall confidence.\n\nMajority voting\n\nMajority voting is an ensemble method that selects the final output by choosing the most common response from multiple prompts or among multiple GenAI models. For example, if five models are asked to classify an image and four suggest \u201ccat\u201d while one suggests \u201cdog,\u201d the final output would be \u201ccat.\u201d The confidence score can be calculated based on the proportion of agreeing models, in this case 80% (four out of five models).\n\nWe tested the same application using five GenAI models and majority voting and observed a strong positive correlation between confidence score and accuracy. The same two fields were plotted below, and the conclusion holds for other fields as well.\n\nThe careful art of majority voting\n\nWith the evaluation, we concluded that only majority voting was a suitable choice for our application, as it\u2019s the only method that showed strong positive correlation with accuracy and is relatively consistent and interpretable. While its concept is straightforward, in practice we realized that the implementation requires careful consideration of many factors.\n\nDeciding the number of models\n\nThe optimal number of models depends on factors such as task complexity, model diversity, available resources, and specific project goals and constraints. Larger ensembles typically offer greater stability and accuracy by reducing individual model errors. However, they increase computational complexity and may yield diminishing returns if the models are too similar. Smaller ensembles are more efficient but less stable.\n\nLiterature often suggests using four to seven models in an optimal ensemble. In our use cases, we leveraged five or six different LLMs. We found that this number provided sufficient diversity and struck a good balance between speed and diversity.\n\nAssigning weights to voting\n\nTo calculate the final score, a weighted majority voting approach was implemented. Weight for each model was based on model accuracy and then normalized to sum to one. This method minimizes the chances of a tie in the voting process, as opposed to an unweighted vote. It also improves the accuracy by giving greater influence to better-performing models.\n\nWe evaluated both linear weights (where weights are linearly correlated with model accuracy) and exponential weights (where top-performing models get weights close to or exceeding half). Both methods showed similar performance. We opted to use linear weights for our application, as it created more balanced and consistent outcomes and was easier to explain.\n\nCalibrating confidence score\n\nWhile there is a positive correlation between confidence and accuracy, the relationship is not one-to-one and can vary across different fields.To correct over- or under-confidence in various fields, we applied Platt scaling, a technique commonly used to calibrate probabilistic outputs. This calibration process adjusts the raw confidence scores to better align with the accuracy, as shown in the two examples in the chart below:\n\nMajority voting \u2014 limitations, challenges, and explorations\n\nLong text fields\n\nMajority voting works effectively for numeric and short text responses but faces challenges with long text fields, such as addresses or item descriptions. Due to variations in phrasing, it is less likely to get model agreement using direct string matching. As an attempt to address this issue, we explored two methods to get a majority response in these scenarios:\n\nEmbedding similarity, which groups similar text responses into clusters based on cosine similarity of their embeddings. The largest cluster is the majority vote. This approach requires careful tuning of distance thresholds. GenAI selection, where a separate GenAI model selects the majority vote from multiple responses. This approach requires extensive prompt engineering to ensure the selector prioritizes factual accuracy and consistency.\n\nDuring our testing, both approaches were able to identify similar outputs for long text fields. However, they failed to catch some spelling differences \u2014 for example, not distinguishing between \u201c0\u201d and \u201cO,\u201d which can lead to significant errors in a financial application. Therefore, we decided not to implement them for production.\n\nAs a workaround, we broke down long text fields into smaller, more manageable components (e.g., splitting addresses into street, city, state, and zip code). It improved the likelihood of model agreement during majority voting. However, developing a more robust solution for the long text field remains a key area for further research.\n\nThe granularity problem\n\nGiven the limited number of models in an ensemble, majority voting brings a granularity problem. With seven models (the upper end of the normal range), each additional vote results in a significant ~14% step change in confidence level. This isn\u2019t granular enough for applications requiring fine-grained confidence figures (e.g., needing 95% confidence to pass a check).\n\nTo improve granularity, we experimented with the permutation approach: by using multiple prompts per model, the total responses increased from X to X * Y (where X is the number of models and Y is the number of prompts per model). We tested with seven GenAI models and five different prompts, and 35 responses were generated. The outcome is summarized in the table below.\n\nApproach # of GenAI Models # of Prompts Total Responses # of Agreed Responses Output Confidence Original 7 1 7 6 Majority voting output is correct. 86% (6/7) Permutation 7 5 35 33 Majority voting output is correct. 94% (33/35)\n\nThe permutation method could potentially increase the pass rate for our system. If the confidence threshold was 90%, the original approach would result in a false rejection, while the permutation approach would successfully return the correct output.\n\nHowever, it significantly increases the number of model runs, where cost increases linearly and the time to finish now depends on the slowest model out of the 35, and a potential failure is more likely . Although the experiments provided valuable insights and guided further discussions with the business regarding thresholds and their implications, future research is still warranted for a more cost-effective long-term solution.\n\nConclusion\n\nIn this post, we examined a unique challenge that arises when using GenAI in financial applications \u2014 determining the confidence score. We discussed the various approaches we explored, culminating in a successfully implemented technique. However, as always, the devil is in the details, and many aspects of implementation require careful consideration. Finally, there are still challenges and limitations in our approach. We explored a few solutions, but some problems remain the topic of future research. We are optimistic that new developments will continue to evolve this groundbreaking technology, and we hope our explorations inspire not only our teams at Spotify but also practitioners across the industry.", "label": 0}
{"title": "Accelerating GPU indexes in Faiss with NVIDIA cuVS", "url": "https://engineering.fb.com/2025/05/08/data-infrastructure/accelerating-gpu-indexes-in-faiss-with-nvidia-cuvs/", "content": "Meta and NVIDIA collaborated to accelerate vector search on GPUs by integrating NVIDIA cuVS into Faiss v1.10 , Meta\u2019s open source library for similarity search.\n\nThis new implementation of cuVS will be more performant than classic GPU-accelerated search in some areas.\n\nFor inverted file (IVF) indexing, NVIDIA cuVS outperforms classical GPU-accelerated IVF build times by up to 4.7x; and search latency is reduced by as much as 8.1x.\n\nFor graph indexing, CUDA ANN Graph (CAGRA) outperforms CPU Hierarchical Navigable Small World graphs (HNSW) build times by up to 12.3x; and search latency is reduced by as much as 4.7x.\n\nThe Faiss library\n\nThe Faiss library is an open source library, developed by Meta FAIR, for efficient vector search and clustering of dense vectors. Faiss pioneered vector search on GPUs, as well as the ability to seamlessly switch between GPUs and CPUs. It has made a lasting impact in both research and industry, being used as an integrated library in several databases (e.g., Milvus and OpenSearch), machine learning libraries, data processing libraries, and AI workflows. Faiss is also used heavily by researchers and data scientists as a standalone library, often paired with PyTorch.\n\nCollaboration with NVIDIA\n\nThree years ago, Meta and NVIDIA worked together to enhance the capabilities of vector search technology and to accelerate vector search on GPUs. Previously, in 2016, Meta had incorporated high performing vector search algorithms made for NVIDIA GPUs: GpuIndexFlat; GpuIndexIVFFlat; GpuIndexIVFPQ. After the partnership, NVIDIA rapidly contributed GpuIndexCagra, a state-of-the art graph-based index designed specifically for GPUs. In its latest release, Faiss 1.10.0 officially includes these algorithms from the NVIDIA cuVS library.\n\nFaiss 1.10.0 also includes a new conda package that unlocks the ability to choose between the classic Faiss GPU implementations and the newer NVIDIA cuVS algorithms, making it easy for users to switch between GPU and CPU.\n\nBenchmarking\n\nThe following benchmarks were conducted using the cuVS-bench tool.\n\nWe measured:\n\nA tall, slender image dataset: A subset of 100 million vectors from the Deep1B dataset by 96 dimensions.\n\nA short, wide dataset of text embeddings: 5 million vector embeddings, curated using the OpenAI text-embedding-ada-002 model .\n\nTests for index build times and search latency were conducted on an NVIDIA H100 GPU and compared to an Intel Xeon Platinum 8480CL system. Results are reported in the tables below at 95% recall along the pareto frontiers for k=10 nearest neighbors.\n\nBuild time (95% recall@10)\n\nIndex Embeddings\n\n100M x 96\n\n(seconds) Embeddings\n\n5M x 1536\n\n(seconds) Faiss Classic Faiss cuVS Faiss Classic Faiss cuVS Faiss Classic Faiss cuVS IVF Flat IVF Flat 101.4 37.9 (2.7x) 24.4 15.2 (1.6x) IVF PQ IVF PQ 168.2 72.7 (2.3x) 42.0 9.0 (4.7x) HNSW (CPU) CAGRA 3322.1 518.5 (6.4x) 1106.1 89.7 (12.3x)\n\nTable 1: Index build times for Faiss-classic and Faiss-cuVS in seconds (with NVIDIA cuVS speedups in parentheses).\n\nSearch latency (95% recall@10)\n\nIndex Embeddings\n\n100M x 96\n\n(milliseconds) Embeddings\n\n5M x 1536\n\n(milliseconds) Faiss Classic Faiss cuVS Faiss Classic Faiss cuVS Faiss Classic Faiss cuVS IVF Flat IVF Flat 0.75 0.39 (1.9x) 1.98 1.14 (1.7x) IVF PQ IVF PQ 0.49 0.17 (2.9x) 1.78 0.22 (8.1x) HNSW (CPU) CAGRA 0.56 0.23 (2.4x) 0.71 0.15 (4.7x)\n\nTable 2: Online (i.e., one at a time) search query latency for Faiss-classic and Faiss-cuVS in milliseconds (with NVIDIA cuVS speedups in parentheses).\n\nLooking forward\n\nThe emergence of state-of-the-art NVIDIA GPUs has revolutionized the field of vector search, enabling high recall and lightning-fast search speeds. The integration of Faiss and cuVS will continue to incorporate state-of-the-art algorithms, and we look forward to unlocking new innovations in this partnership between Meta and NVIDIA.\n\nRead here for more details about NVIDIA cuVS.", "label": 0}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2025", "content": "en\n\nToday, I turn 30 years old. Woohoo! I guess I\u2019m no longer a 20-something-year-old. These last 10 years have flown by quickly but man, have they been memorable. It\u2019s a tad bitter sweet. The positive overtakes the negative by a long shot. I can\u2019t stop the aging process. If only it were that easy!\n\nHonestly, at this very moment, I don\u2019t feel any different. I still feel energetic and craving adventure. I think these song lyrics from T-Swizzle describe how I feel.\n\n(Hey!) I don't know about you But I'm feeling twenty-two\n\nIn hindsight, looking at myself in the mirror more recently, I notice my facial hair is starting to turn white. Nothing like a fresh shave that still makes me look young and boyish. I must being doing something right with my skin routine (and maybe good genes from my parents)!\n\nReflections\n\nI reflect on the growth I\u2019ve had since I entered my 20s. My 20s were about\n\nLearning\n\nFalling and rising\n\nGoing on adventures.\n\nLoved ones\n\nI learned some great lessons along the way. Some lessons were not so easy to swallow. Man, did I learn. There are so many things I know now that I wish a 20-year-old me knew. One of the things I wish he knew was, \u201cLove yourself more!\u201d I do love myself more than ever. I\u2019m happier with who I\u2019ve become. I\u2019m not done learning who I become. I don\u2019t think that ever stops. There were many moments in life when I hit my absolute lowest. Those are moments I\u2019m not proud of. While I did manage to get myself out of them, I did come back stronger. Another lesson here - ask for help and use the resources available to you. You\u2019re not alone, I promise.\n\nTake risks! Take more! I'm thrilled to have moved to California. I still can\u2019t believe that this guy from Nebraska moved to San Francisco. I was grateful to have gone on so many adventures. I learned so much about myself getting away. Some of my best learning moments have been from strangers and experiences with myself in another country. Who knew you could learn something so new about yourself at 3 am while eating a d\u00f5ner kebab on the streets of Berlin?\n\nWhen I was in my early 20s, I said something along the lines of, \u201cI get to be away from my parents! Freedom!\u201d When I was in college, I didn\u2019t want to come home as often, or if I went home to visit, I would not stay for too long. Now more than ever, I miss my parents. Now, I live far away from them. I only get to have home-cooked meals a few times a year. It honestly makes me sad watching them get older. I still need my parents. I do my best to call every day. My relationship with them has been stronger than ever. I am thankful for all the things they\u2019ve done for me and my siblings\n\nWho\u2019s cutting the internet onions on here?\n\nTravel\n\nI\u2019m very fortunate that I traveled a lot in my 20s. I want to thank my parents for taking us to Mexico as a young kid.\n\nI\u2019ve traveled to Canada, all over Europe, South Korea, Mexico, and various parts of the United States.\n\nTravel Buddies\n\nI got to travel with my favorite travel buddies, Sammy and Manny! We've have gone on some wild adventures. I\u2019m very happy we stayed friends. Best travel buddies ever!\n\nBest Birthday Ever! - Czech Republic\n\nOne of my favorite birthdays was when I was studying abroad in Europe. I visited my friends in the Czech Republic.\n\nPablo\u2019s 22nd birthday in 2017 in Czech Republic]\n\nFriends\n\nI\u2019ve made an amazing community of friends. People, I consider my family away from home. Learning from people whom I loved, making friends wherever I go! I\u2019ve made so many friends.\n\nWhat goals do I have in my 30s?\n\nThere are a lot of things I want to accomplish during my 30s. I have a vision for the next few years! I\u2019m hopeful I can find the right path to reach my goals\n\nEducation\n\nI truly value education. It got me where I am today! I was very fortunate to receive a great public school education. Others also need to have access to a great public school education. We must fight against the privatization of the education system. I want to continue to fight for public education. Public education provided me with so many opportunities.\n\nI am currently doing my master\u2019s degree at the University of Nebraska Kearney. My goal one day is to teach a class or two at the university level. It would be awesome to guide future teachers and help them be confident with technology use and go beyond 21st-century learning. I do miss being in the classroom.\n\nI would like to receive a PhD.\n\nI also want to improve my programming skills.\n\nHealth\n\nI\u2019m working towards getting in better shape and thinking of long-term health. As I age, I know I won\u2019t bounce back from injury as quickly. I know I must do everything in my power to take control of my health. Preventative measures are what will keep the hospital away. Mental health is just as important.\n\nAdventures\n\nI want to continue traveling. I want to visit more parts of the world. Ever since I last renewed my passport, the visa pages have been blank. The passport I had in my early twenties was pretty full. It\u2019s so satisfying to look at the stamps and stickers on each page. I want to take my parents on a fun adventure as well\n\nStartup\n\nI\u2019ve been helping a friend with a startup and launching an idea that we have. I am hopeful this year will be great!\n\nFinancial Independence\n\nI want to make smarter choices with money. I\u2019ve been reading up on financial literature to become better with money. Times are changing, and I must keep up with inflation, changing markets, AI, etc.\n\nLooking Forward\n\nOverall, things are finally clicking. I\u2019m excited for what\u2019s to come and all the things to learn. I will reevaluate and reprioritize many things in life.\n\nWhat have you learned in your 30s?\n\nHow am I celebrating my birthday?\n\nLuckily, I live in San Francisco, so there\u2019s a lot to do! I will be celebrating the entire weekend with my friends in San Francisco! So stoked to watch Drag Disco with Trixie Mattel! Let\u2019s put on some pink clothes and dance into my 30s!", "label": 1}
{"title": "Train Rides & Trader Joe's", "url": "https://lifeofpablo.com/blog/train-rides-and-trader-joes", "content": "Train Rides & Trader Joe's\n\nA view of the train from the inside\n\nThis post was written in English (en_US).\n\nEver since I was a kid I was always facinated with public transporation. I remember watching television and videos on the internet of people getting on public transportation. I always wondered why it isn't common to have it everywhere. I've lived in various parts around the world where it is simply normal to have various methods of public transportation. Having lived in Mexico for short periods of times, I did ride buses to get to the city because it was the most economical way. One slight issue living in the United States, I didn't grow up in an area where public transportion was none existent. I grew up in Nebraska. You needed a car to get around. It was also weird to be walking around. I found that idea to be weird.\n\nThe world has been surpassing the United States for decades. It's getting more obvious every day.\n\nSince I moved the California, I've taken the Amtrak Train thirty-five (35) times since 2021. It's an easy trip to do between Sacramento and San Francisco or even to other cities in California.\n\nTimes I've Taken the Train Year Progress 2021 - 7 Times 2022 - 9 Times 2023 - 17 Times\n\nThe last time I did even remotely close to this was when I was living in France. I still have all the tickets to show.\n\nReasons I Take The Train:\n\nEnvironment - A lot more people can move around all at one time and it elimates so many cars off the roads. It helps reduce emissions.\n\nConvenience - There is enough trains throught out the day, that you can just book your ticket minutes before it heads out.\n\nAvoiding traffic - Driving isn't fun when you're surrounded by cars waiting for the line to move. It's nice not having to wait around in traffic when the train is simply going on its way on the the tracks.\n\nNot paying for parking - Parking in cities such as San Francisco is so expensive or you have to move your car for city services. It seems so much of a hastle to be worrying about your car in a place where you can walk.\n\nNot worrying about my car in a big city. - With so many car break-ins happening throughout cities in the United States, I can breath a sigh of relief knowing I don't have to worry about my car left alone or pondering if I did my best to get everything out of sight.\n\nI can get work done on the train. - I'm always working on my computer and I can get so much done weather it's for work or for personal endeavors.\n\nI also enjoy getting groceries at Trader Joe's before I get on the train especially if I'm going to get back to Sacramento late. It's nice to be able to get groceries for the week and take them with me on the train. It's a small little habit or tradition. I also save money by buying snacks ahead of time and not having to spend extra on snacks on the train.\n\nI hope my friend Sammy can come out to California for a fun train ride from Sacramento to San Francisco. I know he'd love it. Heck even one of these days, I'd love to catch the train that runs (Amtrak California Zephyr) through our hometowns in Nebraska and ride up to Sacramento. Hopefully soon!", "label": 1}
{"title": "Google Pay inside sandboxed iframe for PCI DSS v4 compliance", "url": "https://developers.googleblog.com/en/google-pay-inside-sandboxed-iframe-for-pci-dss-v4-compliance/", "content": "Using a sandboxed iframe satisfies any concerns with compliance since scripts within the iFrame will not have access to the parent DOM. See the following illustration for an example:\n\nOne way to comply with this requirement is to use a technique like Subresource Integrity (SRI) . However, the Google Pay JavaScript (pay.js) build and release process does not allow for a long-lived, stable hash required by techniques like SRI.\n\nIf you are developing or maintaining a checkout page you might come across PCI DSS v4 which includes the following requirement under 6.4.3:\n\nIn this case the domain \u201ccdn.somewhereelse.com\u201d would load Google Pay\u2019s pay.js JavaScript file. After a successful transaction, the inner iframe can communicate with the parent page through mechanisms like window.postMessage() if needed.\n\nIn order for Google Pay to work in all browsers we need the following 4 sandbox attribute values in addition to allow=\u201dpayment\u201d :\n\nallow-scripts\n\nTo allow the iframe to execute scripts (pay.js as an example)\n\nallow-popups\n\nAllows the embedded page to create 'child browsing contexts'. In practice, this flag enables the embedded iframe to open new tabs and windows when the user clicks a link.\n\nallow-same-origin\n\nIf not set, fails on various occasions for browsers. If set, the iframe has access to the parents storage and cookies.\n\nallow-forms\n\nAllows forms such as the Google Pay login to submit the data.\n\nSee this test page to see the various iframe sandbox values in action.\n\n\n\nShopify successfully certified for PCI DSS v4\n\nGoogle Pay partnered with Shopify to implement the above solution. Shopify was able to successfully pass the PCI DSS v4 audit by using a sandboxed iframe to display the Google Pay button. Here is what Shopify has to say:\n\nWe\u2019ve built Shopify Checkout in such a way that Google Pay code executes in a secure sandboxed environment, allowing us to maintain the integrity of our checkout and comply with PCI DSS V4 requirements.\n\n\n\n\u2013 Ilya Grigorik, Distinguished Engineer at Shopify\n\nFor more information on how Shopify built their checkout solution using sandboxed iframes, their \u201cPowering Shopify\u2019s High-Performance, PCI DSS v4 Compliant Checkout with Sandboxing\u201d blog post has the insights.\n\n\n\nConclusion\n\nWrapping your Google Pay integration in a sandboxed iframe can help you to comply with PCI DSS v4 requirements. For more assistance with your implementation, sign in to the Google Pay & Wallet Console to create a support ticket. In addition, you can join the developer community in the #payments channel on Discord.\n\nFollow @GooglePayDevs on X for future updates. If you have questions, tag @GooglePayDevs and include #AskGooglePayDevs in your tweets.", "label": 0}
{"title": "A Moment of Joy: Pride", "url": "https://lifeofpablo.com/blog/a-moment-of-joy-pride", "content": "The month of pride is officially over. Actually, it was about six days ago at the time of this writing. We celebrate pride and the LGBT+ community everyday! It's a party everyday.\n\nBelieve it or not, I attended my first Pride event. Growing in Nebraska, attending pride was not possible due to events not existing. The state of Nebraska is conservative. Events such as pride were simply not in my face. Luckily, I educated myself. Often times, I would travel outside of the country so it wasn't possible for me to attend. I'll be honest, I didn't even know pride was a thing until a few years ago. Another struggle of mine is not knowing what is happening in Sacramento. Hey, better late than never. Right????\n\nI finally got to attend.\n\nI attended to San Francisco Pride as my first pride festival. I was very excited to be there. Being there was everything I hoped for and so much more. I loved the energy of everything happening around me. People were dancing, singing, cheering, throwing things. People were expressing themselves at 1000%. I believe everyone should do that everyday!\n\nI got so many hugs. These hugs were empowering.\n\nI encountered so many moments of joy. I told my friends I loved them. I told strangers, I loved them. I told myself, I loved myself. Every interaction was a moment of joy. I was smiling ear to ear. I was radiating. I hope to feel this way everyday.\n\nI can't wait to attend another Pride event. I want to go to New York City Pride and see what it has to offer. Each event is different.", "label": 1}
{"title": "Introducing Gemma 3n: The developer guide", "url": "https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/", "content": "The first Gemma model launched early last year and has since grown into a thriving Gemmaverse of over 160 million collective downloads. This ecosystem includes our family of over a dozen specialized models for everything from safeguarding to medical applications and, most inspiringly, the countless innovations from the community. From innovators like Roboflow building enterprise computer vision to the Institute of Science Tokyo creating highly-capable Japanese Gemma variants, your work has shown us the path forward. Building on this incredible momentum, we're excited to announce the full release of Gemma 3n. While last month's preview offered a glimpse, today unlocks the full power of this mobile-first architecture. Gemma 3n is designed for the developer community that helped shape Gemma. It\u2019s supported by your favorite tools including Hugging Face Transformers, llama.cpp, Google AI Edge, Ollama, MLX, and many others, enabling you to fine-tune and deploy for your specific on-device applications with ease. This post is the developer deep dive: we'll explore some of the innovations behind Gemma 3n, share new benchmark results, and show you how to start building today.\n\nWhat\u2019s new in Gemma 3n? Gemma 3n represents a major advancement for on-device AI, bringing powerful multimodal capabilities to edge devices with performance previously only seen in last year's cloud-based frontier models.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nMultimodal by design: Gemma 3n natively supports image, audio, video, and text inputs and text outputs. Optimized for on-device: Engineered with a focus on efficiency, Gemma 3n models are available in two sizes based on effective parameters: E2B and E4B. While their raw parameter count is 5B and 8B respectively, architectural innovations allow them to run with a memory footprint comparable to traditional 2B and 4B models, operating with as little as 2GB (E2B) and 3GB (E4B) of memory. Groundbreaking architecture: At its core, Gemma 3n features novel components like the MatFormer architecture for compute flexibility, Per Layer Embeddings (PLE) for memory efficiency, LAuReL and AltUp for architectural efficiency, and new audio and MobileNet-v5 based vision encoders optimized for on-device use cases. Enhanced quality: Gemma 3n delivers quality improvements across multilinguality (supporting 140 languages for text and multimodal understanding of 35 languages), math, coding, and reasoning. The E4B version achieves an LMArena score over 1300, making it the first model under 10 billion parameters to reach this benchmark.\n\nAchieving this leap in on-device performance required rethinking the model from the ground up. The foundation is Gemma 3n\u2019s unique mobile-first architecture, and it all starts with MatFormer.\n\nMatFormer: One model, many sizes At the core of Gemma 3n is the MatFormer (\ud83e\ude86Matryoshka Transformer) architecture, a novel nested transformer built for elastic inference. Think of it like Matryoshka dolls: a larger model contains smaller, fully functional versions of itself. This approach extends the concept of Matryoshka Representation Learning from just embeddings to all transformer components.\n\nDuring the MatFormer training of the 4B effective parameter (E4B) model, a 2B effective parameter (E2B) sub-model is simultaneously optimized within it, as shown in the figure above. This provides developers two powerful capabilities and use cases today: 1: Pre-extracted models: You can directly download and use either the main E4B model for the highest capabilities, or the standalone E2B sub-model which we have already extracted for you, offering up to 2x faster inference. 2: Custom sizes with Mix-n-Match: For more granular control tailored to specific hardware constraints, you can create a spectrum of custom-sized models between E2B and E4B using a method we call Mix-n-Match. This technique allows you to precisely slice the E4B model's parameters, primarily by adjusting the feed forward network hidden dimension per layer (from 8192 to 16384) and selectively skipping some layers. We are releasing the MatFormer Lab, a tool that shows how to retrieve these optimal models, which were identified by evaluating various settings on benchmarks like MMLU.\n\nMMLU scores for the pre-trained Gemma 3n checkpoints at different model sizes (using Mix-n-Match)\n\nLooking ahead, the MatFormer architecture also paves the way for elastic execution. While not part of today\u2019s launched implementations, this capability allows a single deployed E4B model to dynamically switch between E4B and E2B inference paths on the fly, enabling real-time optimization of performance and memory usage based on the current task and device load.\n\nPer-Layer Embeddings (PLE): Unlocking more memory efficiency Gemma 3n models incorporate Per-Layer Embeddings (PLE). This innovation is tailored for on-device deployment as it dramatically improves model quality without increasing the high-speed memory footprint required on your device's accelerator (GPU/TPU). While the Gemma 3n E2B and E4B models have a total parameter count of 5B and 8B respectively, PLE allows a significant portion of these parameters (the embeddings associated with each layer) to be loaded and computed efficiently on the CPU. This means only the core transformer weights (approximately 2B for E2B and 4B for E4B) need to sit in the typically more constrained accelerator memory (VRAM).\n\nWith Per-Layer Embeddings, you can use Gemma 3n E2B while only having ~2B parameters loaded in your accelerator.\n\nKV Cache sharing: Faster long-context processing Processing long inputs, such as the sequences derived from audio and video streams, is essential for many advanced on-device multimodal applications. Gemma 3n introduces KV Cache Sharing, a feature designed to significantly accelerate time-to-first-token for streaming response applications. KV Cache Sharing optimizes how the model handles the initial input processing stage (often called the \"prefill\" phase). The keys and values of the middle layer from local and global attention are directly shared with all the top layers, delivering a notable 2x improvement on prefill performance compared to Gemma 3 4B. This means the model can ingest and understand lengthy prompt sequences much faster than before.\n\nAudio understanding: Introducing speech to text and translation Gemma 3n uses an advanced audio encoder based on the Universal Speech Model (USM). The encoder generates a token for every 160ms of audio (about 6 tokens per second), which are then integrated as input to the language model, providing a granular representation of the sound context. This integrated audio capability unlocks key features for on-device development, including: Automatic Speech Recognition (ASR): Enable high-quality speech-to-text transcription directly on the device. Automatic Speech Translation (AST): Translate spoken language into text in another language. We've observed particularly strong AST results for translation between English and Spanish, French, Italian, and Portuguese, offering great potential for developers targeting applications in these languages. For tasks like speech translation, leveraging Chain-of-Thought prompting can significantly enhance results. Here\u2019s an example:", "label": 0}
{"title": "Design system annotations, part 1: How accessibility gets left out of components", "url": "https://github.blog/engineering/user-experience/design-system-annotations-part-1-how-accessibility-gets-left-out-of-components/", "content": "When it comes to design systems, every organization tends to be at a different place in their accessibility journey. Some have put a great deal of work into making their design system accessible while others have a long way to go before getting there. To help on this journey, many organizations rely on accessibility annotations to make sure there are no access barriers when a design is ready to be built.\n\nHowever, it\u2019s a common misconception (especially for organizations with mature design systems) that accessible components will result in accessible designs. While design systems are fantastic for scaling standards and consistency, they can\u2019t prevent every issue with our designs or how we build them. Access barriers can still slip through the cracks and make it into production.\n\nThis is the root of the problem our Accessibility Design team set out to solve.\n\nIn this two-part series, we\u2019ll show you exactly how accessible design system components can produce inaccessible designs. Then we\u2019ll demonstrate our solution: integrating annotations with our Primer components. This allows us to spend less time annotating, increases design system adoption, and reaches teams who may not have accessibility support. And in our next post, we\u2019ll walk you through how you can do the same for your own components.\n\nLet\u2019s dig in.\n\nWhat are annotations and their benefits?\n\nAnnotations are notes included in design projects that help make the unseen explicit by conveying design intent that isn\u2019t shown visually. They improve the usability of digital experiences by providing a holistic picture for developers of how an experience should function. Integrating annotations into our design process helps our teams work better together by closing communication gaps and preventing quality issues, accessibility audit issues, and expensive re-work.\n\nSome of the questions annotations help us answer include:\n\nHow is assistive technology meant to navigate a page from one element to another?\n\nWhat\u2019s the alternative text for informative images and buttons without labels?\n\nHow does content shift depending on viewport size, screen orientation, or zoom level?\n\nWhich virtual keyboard should be used for a form input on mobile?\n\nHow should focus be managed for complex interactions?\n\nOur answers to questions like this\u2014or the lack thereof\u2014can make or break the experience of the web for a lot of people, especially users with disabilities. Some annotation tools are built specifically to help with this by guiding designers to include key details about web standards, platform functionality, and accessibility (a11y).\n\nMost public annotation kits are well suited for teams who are creating new design system components, teams who aren\u2019t already using a design system, or teams who don\u2019t have specialized accessibility knowledge. They usually help annotate things like:\n\nControls such as buttons and links\n\nStructural elements such as headings and landmarks\n\nDecorative images and informative descriptions\n\nForms and other elements that require labels and semantic roles\n\nFocus order for assistive technology and keyboard navigation\n\nGitHub\u2019s annotation\u2019s toolkit\n\nOne of our top priorities is to meet our colleagues where they\u2019re at. We wanted all our designers to be able to use annotations out of the box because we believe they shouldn\u2019t need to be a certified accessibility specialist in order to get things built in an accessible way.\n\nTo this end, last year we began creating an internal Figma library\u2014the GitHub Annotation Toolkit (which we aim to release to the public soon). Our toolkit builds on the legacy of the former Inclusive Design team at CVS Health. Their two open source annotation kits help make documentation that\u2019s easy to create and consume, and are among the most widely used annotation libraries in the Figma Community.\n\nWhile they add clarity, annotations can also add overhead. If teams are only relying on specialists to interpret designs and technical specifications for developers, the hand-off process can take longer than it needs to. To create our annotation toolkit, we rebuilt its predecessor from the ground up to avoid that overhead, making extensive improvements and adding inline documentation to make it more intuitive and helpful for all of our designers\u2014not just accessibility specialists.\n\nDesign systems can also help reduce that overhead. When you audit your design systems for accessibility, there\u2019s less need for specialist attention on every product feature, since you\u2019re using annotations to add technical semantics and specialist knowledge into every component. This means that designers and developers only need to adhere to the usage guidelines consistently, right?\n\nThe problems with annotations and design system components\n\nUnfortunately, it\u2019s not that simple.\n\nAccessibility is not binary\n\nWhile design systems can help drive more accessible design at scale, they are constantly evolving and the work on them is never done. The accessibility of any component isn\u2019t binary. Some may have a few severe issues that create access barriers, such as being inoperable with a keyboard or missing alt text. Others may have a few trivial issues, such as generic control labels.\n\nMost of the time, it will be a misnomer to claim that your design system is \u201cfully accessible.\u201d There\u2019s always more work to do\u2014it\u2019s just a question of how much. The Web Content Accessibility Guidelines (WCAG) are a great starting point, but their \u201cSuccess Criteria\u201d isn\u2019t tailored for the unique context that is your website or product or audience.\n\nWhile the WCAG should be used as a foundation to build from, it\u2019s important to understand that it can\u2019t capture every nuance of disabled users\u2019 needs because your users\u2019 needs are not every user\u2019s needs. It would be very easy to believe that your design system is \u201cfully accessible\u201d if you never look past WCAG to talk to your users. If Primer has accessible components, it\u2019s because we feel that direct participation and input from daily assistive technology users is the most important aspect of our work. Testing plans with real users\u2014with and without disabilities\u2014is where you really find what matters most.\n\nAccessible components do not guarantee accessible designs\n\nArranging a series of accessible components on a page does not automatically create an accurate and informative heading hierarchy. There\u2019s a good chance that without additional documentation, the heading structure won\u2019t make sense visually\u2014nor as a medium for navigating with assistive technology.\n\nIt\u2019s great when accessible components are flexible and responsive, but what about when they\u2019re placed in a layout that the component guidance doesn\u2019t account for? Do they adapt to different zoom levels, viewport sizes, and screen orientations? Do they lose any functionality or context when any of those things change?\n\nComponent usage is contextual. You can add an image or icon to your design, but the design system docs can\u2019t write descriptive text for you. You can use the same image in multiple places, but the image description may need to change depending on context.\n\nSimilarly, forms built using the same input components may do different things and require different error validation messages. It\u2019s no wonder that adopting design system components doesn\u2019t get rid of all audit issues.\n\nDesign system components in Figma don\u2019t include all the details\n\nAnnotation kits don\u2019t include components for specific design systems because almost every organization is using their own. When annotation kits are adopted, teams often add ways to label their design system components.\n\nThis labeling lets developers know they can use something that\u2019s already been built, and that they don\u2019t need to build something from scratch. It also helps identify any design system components that get \u2018detached\u2019 in Figma. And it reduces the number of things that need to be annotated.\n\nLet\u2019s look at an example:\n\nIf we\u2019re using this Primer Button component from the Primer Web Figma library, there are a few important things that we won\u2019t know just by looking at the design or the component properties:\n\nFunctional differences when components are implemented. Is this a link that just looks visually like a button? If so, a developer would use the <LinkButton> React component instead of <Button> .\n\nIs this a link that just looks visually like a button? If so, a developer would use the React component instead of . Accessible labels for folks using assistive technology. The icon may need alt text. In some cases, the button text might need some visually-hidden text to differentiate it from similar buttons. How would we know what that text is? Without annotations, the Figma component doesn\u2019t have a place to display this.\n\nThe icon may need alt text. In some cases, the button text might need some visually-hidden text to differentiate it from similar buttons. How would we know what that text is? Without annotations, the Figma component doesn\u2019t have a place to display this. Whether user data is submitted. When a design doesn\u2019t include an obvious form with input fields, how do we convey that the button needs specific attributes to submit data?\n\nIt\u2019s risky to leave questions like this unanswered, hoping someone notices and guesses the correct answer.\n\nA solution that streamlines the annotation process while minimizing risk\n\nWhen creating new components, a set of detailed annotations can be a huge factor in how robust and accessible they are. Once the component is built, design teams can start to add instances of that component in their designs. When those designs are ready to be annotated, those new components shouldn\u2019t need to be annotated again. In most cases, it would be redundant and unnecessary\u2014but not in every case.\n\nThere are some important details in many Primer components that may change from one instance to another. If we use the CVS Health annotation kit out of the box, we should be able to capture those variations, but we wouldn\u2019t be able to avoid those redundant and unnecessary annotations. As we built our own annotation toolkit, we built a set of annotations for each Primer component to do both of those things at once.\n\nThis accordion component has been thoroughly annotated so that an engineer has everything they need to build it the first time. These include heading levels, semantics for <detail> and <summary> elements, landmarks, and decorative icons. All of this is built into the component so we don\u2019t need to annotate most of this when adding the accordion to our new designs.\n\nHowever, there are two important things we need to annotate, as they can change from one instance to another:\n\nThe optional title at the top. The heading level of each item within the accordion.\n\nIf we don\u2019t specify these things, we\u2019re leaving it to chance that the page\u2019s heading structure will break or that the experience will be confusing for people to understand and navigate the page. The risks may be low for a single button or basic accordion, but they grow with pattern complexity, component nesting, interaction states, duplicated instances, and so on.\n\nInstead of annotating what\u2019s already built into the component or leaving these details to chance, we can add two quick annotations. One Stamp to point to the component, and one Details annotation where we fill in some blanks to make the heading levels clear.\n\nBecause the prompts for specific component details are pre-set in the annotation, we call them Preset annotations.\n\nIntroducing our Primer A11y Preset annotations\n\nWith this proof of concept, we selected ten frequently used Primer components for the same treatment and built a new set of Preset annotations to document these easily missed accessibility details\u2014our Primer A11y Presets.\n\nThose Primer components tend to contribute to more accessibility audit issues when key details are missing on implementation. Issues for these components relate to things like lack of proper labels, error validation messages, or missing HTML or ARIA attributes.\n\nEach of our Preset annotations is linked to component docs and Storybook demos. This will hopefully help developers get straight to the technical info they need without designers having to find and add links manually. We also included guidance for how to fill out each Preset, as well as how to use the component in an accessible way. This helps designers get support inline without leaving their Figma canvas.\n\nWant to create your own? Check out Design system annotations, part 2\n\nButton components in Google\u2019s Material Design and Shopify\u2019s Polaris, IBM\u2019s Carbon, or our Primer design system are all very different from one another. Because Preset annotations are based on specific components, they only work if you\u2019re also using the design system they\u2019re made for.\n\nIn part 2 of this series, we\u2019ll walk you through how you can build your own set of Preset annotations for your design system, as well as some different ways to document important accessibility details before development starts.\n\nYou may also like:\n\nIf you\u2019re more of a visual learner, you can watch Alexis Lucio explore Preset annotations during GitHub\u2019s Dev Community Event to kick off Figma\u2019s Config 2024.\n\nTags:", "label": 0}
{"title": "How the GitHub CLI can now enable triangular workflows", "url": "https://github.blog/open-source/git/how-the-github-cli-can-now-enable-triangular-workflows/", "content": "Most developers are familiar with the standard Git workflow. You create a branch, make changes, and push those changes back to the same branch on the main repository. Git calls this a centralized workflow. It\u2019s straightforward and works well for many projects.\n\nHowever, sometimes you might want to pull changes from a different branch directly into your feature branch to help you keep your branch updated without constantly needing to merge or rebase. However, you\u2019ll still want to push local changes to your own branch. This is where triangular workflows come in.\n\nIt\u2019s possible that some of you have already used triangular workflows, even without knowing it. When you fork a repo, contribute to your fork, then open a pull request back to the original repo, you\u2019re working in a triangular workflow. While this can work seamlessly on github.com, the process hasn\u2019t always been seamless with the GitHub CLI.\n\nThe GitHub CLI team has recently made improvements (released in v2.71.2) to better support these triangular workflows, ensuring that the gh pr commands work smoothly with your Git configurations. So, whether you\u2019re working on a centralized workflow or a more complex triangular one, the GitHub CLI will be better equipped to handle your needs.\n\nIf you\u2019re already familiar with how Git handles triangular workflows, feel free to skip ahead to learn about how to use gh pr commands with triangular workflows. Otherwise, let\u2019s get into the details of how Git and the GitHub CLI have historically differed, and how four-and-a-half years after it was first requested, we have finally unlocked managing pull requests using triangular workflows in the GitHub CLI.\n\nFirst, a lesson in Git fundamentals\n\nTo provide a framework for what we set out to do, it\u2019s important to first understand some Git basics. Git, at its core, is a way to store and catalog changes on a repository and communicate those changes between copies of that repository. This workflow typically looks like the diagram below:\n\nFigure 1: A typical git branch setup\n\nThe building blocks of this diagram illustrate two important Git concepts you likely use every day, a ref and push/pull.\n\nRefs\n\nA ref is a reference to a repository and branch. It has two parts: the remote, usually a name like origin or upstream, and the branch. If the remote is the local repository, it is blank. So, in the example above, origin/branch in the purple box is a remote ref, referring to a branch named branch on the repository name origin, while branch in the green box is a local ref, referring to a branch named branch on the local machine.\n\nWhile working with GitHub, the remote ref is usually the repository you are hosting on GitHub. In the diagram above, you can consider the purple box GitHub and the green box your local machine.\n\nPushing and pulling\n\nA push and a pull refer to the same action, but from two different perspectives. Whether you are pushing or pulling is determined by whether you are sending or receiving the changes. I can push a commit to your repo, or you can pull that commit from my repo, and the references to that action would be the same.\n\nTo disambiguate this, we will refer to different refs as the headRef or baseRef, where the headRef is sending the changes (pushing them) and the baseRef is receiving the changes (pulling them).\n\nFigure 2: Disambiguating headRef and baseRef for push/pull operations\n\nWhen dealing with a branch, we\u2019ll often refer to the headRef of its pull operations as its pullRef and the baseRef of its push operations as its pushRef. That\u2019s because, in these instances, the working branch is the pull\u2019s baseRef and the push\u2019s headRef, so they\u2019re already disambiguated.\n\nThe @{push} revision syntax\n\nTurns out, Git has a handy built-in tool for referring to the pushRef for a branch: the @{push} revision syntax. You can usually determine a branch\u2019s pushRef by running the following command:\n\ngit rev-parse --abbrev-ref @{push}\n\nThis will result in a human-readable ref, like origin/branch, if one can be determined.\n\nPull Requests\n\nOn GitHub, a pull request is a proposal to integrate changes from one ref to another. In particular, they act as a simple \u201cpause\u201d before performing the actual integration operation, often called a merge, when changes are being pushed from ref to another. This pause allows for humans (code reviews) and robots (GitHub Copilot reviews and GitHub Actions workflows) to check the code before the changes are integrated. The name pull request came from this language specifically: You are requesting that a ref pulls your changes into itself.\n\nFigure 3: Demonstrating how GitHub Pull Requests correspond to pushing and pulling\n\nCommon Git workflows\n\nNow that you understand the basics, let\u2019s talk about the workflows we typically use with Git every day.\n\nA centralized workflow is how most folks interact with Git and GitHub. In this configuration, any given branch is pushing and pulling from a remote ref with the same branch name. For most of us, this type of configuration is set up by default when we clone a repo and push a branch. It is the situation shown in Figure 1.\n\nIn contrast, a triangular workflow pushes to and pulls from different refs. A common use case for this configuration is to pull directly from a remote repository\u2019s default branch into your local feature branch, eliminating the need to run commands like git rebase <default> or git merge <default> on your feature branch to ensure the branch you\u2019re working on is always up to date with the default branch. However, when pushing changes, this configuration will typically push to a remote ref with the same branch name as the feature branch.\n\nFigure 4: juxtaposing centralized workflows from triangular workflows.\n\nWe complete the triangle when considering pull requests: the headRef is the pushRef for the local ref and the baseRef is the pullRef for the local branch:\n\nFigure 5: a triangular workflow\n\nWe can go one step further and set up triangular workflows using different remotes as well. This most commonly occurs when you\u2019re developing on a fork. In this situation, you usually give the fork and source remotes different names. I\u2019ll use origin for the fork and upstream for the source, as these are common names used in these setups. This functions exactly the same as the triangular workflows above, but the remotes and branches on the pushRef and pullRef are different:\n\nFigure 6: juxtaposing triangular workflows and centralized workflows with different remotes such as with forks\n\nUsing a Git configuration file for triangular workflows\n\nThere are two primary ways that you can set up a triangular workflow using the Git configuration \u2013 typically defined in a `.git/config` or `.gitconfig` file. Before explaining these, let\u2019s take a look at what the relevant bits of a typical configuration look like in a repo\u2019s `.git/config` file for a centralized workflow:\n\n[remote \u201corigin\u201d] url = https://github.com/OWNER/REPO.git fetch = +refs/heads/*:refs/remotes/origin/* [branch \u201cdefault\u201d] remote = origin merge = refs/heads/default [branch \u201cbranch\u201d] remote = origin merge = refs/heads/branch\n\nFigure 7: A typical Git configuration setup found in .git/config\n\nThe [remote \u201corigin\u201d] part is naming the Git repository located at github.com/OWNER/REPO.git to origin, so we can reference it elsewhere by that name. We can see that reference being used in the specific [branch] configurations for both the default and branch branches in their remote keys. This key, in conjunction with the branch name, typically makes up the branch\u2019s pushRef: in this example, it is origin/branch.\n\nThe remote and merge keys are combined to make up the branch\u2019s pullRef: in this example, it is origin/branch.\n\nSetting up a triangular branch workflow\n\nThe simplest way to assemble a triangular workflow is to set the branch\u2019s merge key to a different branch name, like so:\n\n[branch \u201cbranch\u201d] remote = origin merge = refs/heads/default\n\nFigure 8: a triangular branch\u2019s Git configuration found in .git/config\n\nThis will result in the branch pullRef as origin/default, but pushRef as origin/branch, as shown in Figure 9.\n\nFigure 9: A triangular branch workflow\n\nSetting up a triangular fork workflow\n\nWorking with triangular forks requires a bit more customization than triangular branches because we are dealing with multiple remotes. Thus, our remotes in the Git config will look different than the one shown previously in Figure 7:\n\n[remote \u201cupstream\u201d] url = https://github.com/ORIGINALOWNER/REPO.git fetch = +refs/heads/*:refs/remotes/upstream/* [remote \u201corigin\u201d] url = https://github.com/FORKOWNER/REPO.git fetch = +refs/heads/*:refs/remotes/origin/*\n\nFigure 10: a Git configuration for a multi-remote Git setup found in .git/config\n\nUpstream and origin are the most common names used in this construction, so I\u2019ve used them here, but they can be named anything you want.\n\nHowever, toggling a branch\u2019s remote key between upstream and origin won\u2019t actually set up a triangular fork workflow\u2014it will just set up a centralized workflow with either of those remotes, like the centralized workflow shown in Figure 6. Luckily, there are two common Git configuration options to change this behavior.\n\nSetting a branch\u2019s pushremote\n\nA branch\u2019s configuration has a key called pushremote that does exactly what the name suggests: configures the remote that the branch will push to. A triangular fork workflow config using pushremote may look like this:\n\n[branch \u201cbranch\u201d] remote = upstream merge = refs/heads/default pushremote = origin\n\nFigure 11: a triangular fork\u2019s Git config using pushremote found in .git/config\n\nThis assembles the triangular fork repo we see in Figure 12. The pullRef is upstream/default, as determined by combining the remote and merge keys, while the pushRef is origin/branch, as determined by combining the pushremote key and the branch name.\n\nFigure 12: A triangular fork workflow\n\nSetting a repo\u2019s remote.pushDefault\n\nTo configure all branches in a repository to have the same behavior as what you\u2019re seeing in Figure 12, you can instead set the repository\u2019s pushDefault . The config for this is below:\n\n[remote] pushDefault = origin [branch \u201cbranch\u201d] remote = upstream merge = refs/heads/default\n\nFigure 13: a triangular fork\u2019s Git config using remote.pushDefault found in .git/config\n\nThis assembles the same triangular fork repo as shown in Figure 12 above, however this time the pushRef is determined by combining the remote.pushDefault key and the branch name, resulting in origin/branch.\n\nWhen using the branch\u2019s pushremote and the repo\u2019s remote.pushDefault keys together, Git will preferentially resolve the branch\u2019s configuration over the repo\u2019s, so the remote set on pushremote supersedes the remote set on remote.pushDefault .\n\nUpdating the gh pr command set to reflect Git\n\nPreviously, the gh pr command set did not resolve pushRefs and pullRefs in the same way that Git does. This was due to technical design decisions that made this change both difficult and complex. Instead of discussing that complexity\u2014a big enough topic for a whole article in itself\u2014I\u2019m going to focus here on what you can now do with the updated gh pr command set.\n\nIf you set up triangular Git workflows in the manner described above, we will automatically resolve gh pr commands in accordance with your Git configuration.\n\nTo be slightly more specific, when trying to resolve a pull request for a branch, the GitHub CLI will respect whatever @{push} resolves to first, if it resolves at all. Then it will fall back to respect a branch\u2019s pushremote, and if that isn\u2019t set, finally look for a repo\u2019s remote.pushDefault config settings.\n\nWhat this means is that the CLI is assuming your branch\u2019s pullRef is the pull request\u2019s baseRef and the branch\u2019s pushRef is the pull requests headRef. In other words, if you\u2019ve configured git pull and git push to work, then gh pr commands should just work. The diagram below, a general version of Figure 5, demonstrates this nicely:\n\nFigure 14: the triangular workflow supported by the GitHub CLI with respect to a branch\u2019s pullRef and pushRef. This is the generalized version of Figure 5\n\nConclusion\n\nWe\u2019re constantly working to improve the GitHub CLI, and we\u2019d like the behavior of the GitHub CLI to reasonably reflect the behavior of Git. This was a team effort\u2014everyone contributed to understanding, reviewing, and testing the code to enable this enhanced gh pr command set functionality.\n\nIt also couldn\u2019t have happened without the support of our contributors, so we extend our thanks to them:\n\nCLI native support for triangular workflows was 4.5 years in the making, and we\u2019re proud to have been able to provide this update for the community.\n\nThe GitHub CLI Team\n\n@andyfeller , @babakks , @bagtoad , @jtmcg , @mxie , @RyanHecht , and @williammartin\n\nTags:", "label": 0}
{"title": "A world run by tools", "url": "https://tommorris.org/posts/2024/a-world-run-by-tools/", "content": "Tantek just wrote an interesting post about writing tools, part of an IndieWeb Carnival for last month on tools.\n\nThis is partly a response to Tantek\u2019s post with some suggestions, but also explaining how I pick tools, and explaining a few I use.\n\nZuhandenheit\n\nIf I\u2019m trying to think, or learn something important or come up with ideas, my first port of call is to take notes by hand. There\u2019s some scientific evidence that writing by hand is better if you want to learn stuff, but I also just like writing by hand more. It\u2019s calming and it feels nice. Computers are filled with bugs, security vulnerabilities, bad software, corproate unpleasantness, and endless distractions. None of the things I dislike most about computers are present in pens and notebooks.\n\nI use hardbound notebooks (I\u2019ve tried a variety of brands, but I prefer 120gsm paper and pre-printed page numbers), and black uni-ball Eye UB-157 pens with 0.7mm nibs. I have some of the 1.0mm ones too, but they\u2019re a bit much. I used fountain pens at school and have no particular desire to go back to that particular mess. I used to use another brand of gel pens where you could replace the ink rather than having to dispose of the whole thing when it runs out, which would be a plus.\n\nThe stuff I write down may end up being typed up and stored in a digital system, but about 80% of the time, what I write starts on paper.\n\nThose with an optimizing mindset might find it a bit strange to duplicate effort in this way. Write stuff by hand and then type it into a digital system? Why?\n\nFor me, writing notes and ideas by hand is an important first stage. There are fewer distractions in a notebook. If you write digitally, you go immediately into editing mode - rewriting words, cutting and pasting, chopping and changing. Sometimes that\u2019s important, but there\u2019s an advantage to just letting the thoughts sit around for a bit before getting the scalpel out.\n\n\u201cSure, but what about an iPad with a Apple Pencil? Or a reMarkable? Or just scanning your notebook and using handwriting recognition?\u201d asks a conveniently invented interlocuter.\n\nIf those work for you, great. I have tried taking notes on an iPad with a stylus, and it\u2019s\u2026 fine. I was curious about the reMarkable, but then they added bloody subscription fees. I already have a notebook and a pen. Spending hundreds of pounds on another gadget, plus having to deal with tech nonsense like susbcription fees, and USB chargers - and, for what? I already have a notebook. It\u2019s good.\n\nThe second second stage of typing it up into a digital system isn\u2019t a problem that needs solving, it\u2019s a purposeful decision. The typing up stage is where I add context, reorder and restructure the ideas to make them more coherent, work out what needs to happen with the information\u2014see how things hang together in the broadest possible sense of the term, that kinda thing. Often times, the typing up stage will involve digging out links, finding references, double-checking stuff.\n\nSeparating thinking and writing into separate stages works for me. It takes time, but if the end result is better ideas or clearer understanding, that\u2019s time well spent.\n\nMinimally cursed technology\n\nIn terms of how I pick digital tools, I\u2019m not an free/open source purist, but my starting point is I want stable forever software, free of corporate control, that uses formats that are open, widely-implemented and simple. If it is open source, all the better. If it isn\u2019t, I treat it as considerably more disposable than I do free software.\n\nThere\u2019s commercial software I use which is pay once, or where there\u2019s an \u201cannual pass\u201d type approach that unlocks new features. I avoid SaaS and subscriptionware, not because of the cost, but because of the control. I don\u2019t trust stuff to not go away.\n\nThere are morally disgusting elements in the software industry\u2014Adobe just referred to Creative Cloud users as heroin addicts, and I want no part of that. But there is an incredibly pragmatic, commercial argument for preferring software that\u2019s open source (or at least closer to the Platonic ideal): it seems incredibly risky to hand over control of the tools one relies on to work effectively to companies whose commercial incentives do not necessarily align with your own. They can screw you over at any time, and you\u2019ve got no practical recourse. This is true of everybody whose job involves using technology, which, these days, is practically everyone. It\u2019s true of knowledge workers\u2014scientists, designers, software developers, lawyers, civil servants, journalists\u2014having control over the tools of their work, but it is also just as true of rural farmers being able to repair their tractors or doctors being able to fix ventilators during a global pandemic (they ended up getting more help from a Polish hacker than from the medical device manufacturers).\n\nOn open data, the near guaranteed way to achieve this is plain text. That\u2019s it. The closer to plain text it is, the better.\n\nI\u2019ve worked in companies where our beautifully curated bug tracker\u2014which, along with the commit history, is the closest thing to an oral history of a software project\u2014disappeared because someone decides that they prefer a different system. God only knows why\u2014 it\u2019s 40\u00a2 per user per month cheaper, the sales guy took them for a nice lunch, the icon is a nicer shade of blue. Nobody transferred the data. All the links to the bugs are broken. The consequence? The developers have no idea what happened before last week because it\u2019s all disappeared. If it\u2019s not in text/plain or something close to it (Markdown, HTML, etc.), it dies. If it is in plain text or something close to it, you can version control it, which is handy.\n\nI cannot emphasise enough how important plain text is. It\u2019s unreasonably effective, one might say. The fact that in knowledge-based workplaces and professions, only programmers seem to grasp the significance of this is incredibly strange.\n\nImplementation\n\nIn terms of electronic tooling, my first ports of call for writing are Emacs and Obsidian. Emacs is mostly for code, Obsidian is mostly for prose, but there\u2019s some overlap.\n\nFor Emacs, I use the Doom distro. I used Vim for a long time, and evil-mode gives me Vim but I can customise it more to my needs. In practice, I don\u2019t customise very much, but Emacs Lisp is preferable to Vimscript. And my Emacs setup looks pretty.\n\nWhen it comes to programming, my go-to languages these days are Python and Rust (plus, I guess, Elisp).\n\nAs for Obsidian, it\u2019s where pretty much everything that\u2019s not code and is of longer term importance ends up. It\u2019s a personal wiki, basically. Some people like to call them \u201czettelkastens\u201d or \u201cpersonal knowledge management systems\u201d but I just say \u201cpersonal wiki\u201d. Everything goes in there - notes on interesting things I read, hard-fought lessons, lists of films and music to try.\n\nIt is a genuine extended mind though. The main benefit is thought not going to waste. I\u2019ve been in discussions with people, and we\u2019ve been trying to understand something - I can pull up information from my personal wiki and refresh myself on what did I think when I encountered this thing in the past, and why.\n\nI don\u2019t love Obsidian (it not being open source being one problem). Logseq is promising (and actually open source), but I\u2019ve been too busy to switch over. I\u2019ve tried org-mode and I like it, but Org mode keybindings are weird and I\u2019m too lazy to change them. Plus there isn\u2019t really good mobile tooling in Org-mode world.\n\nBoth Obsidian and Logseq are just plain text which I trust far more than whatever rat-infested proprietary crap is popular until it gets sunsetted or pivoted or screwed up.\n\nIn theory, how you sync Obsidian (and Logseq) is up to you. In practice, if you\u2019re on macOS/iOS, you\u2019ve got a couple of options:\n\niCloud Drive - it\u2019s built into the OS after all\n\nDropbox / Google Drive / whatever - I\u2019ve moved away from them\n\nsome self-hosted thing - Resilio Sync, Syncthing, WebDAV etc.\n\nObsidian\u2019s paid for sync service which is probably lovely but it is $20 a month, no thanks\n\nGit - it\u2019s plain text, version control it\n\nThe latter is great but fiddly if you want to sync between iOS and desktop. This tutorial explains exactly how to do it correctly with Working Copy. Doing it any other way is a recipe for pain and unpleasantness.\n\n(The other way to do it is to have a Git copy, and then some kind of way to sync it to a cloud storage system without the Git repo, then pull the changes back in. Essentially that\u2019s what the Working Copy/Shortcuts trick is doing on iOS.)\n\nI\u2019ve bounced back and forth between Apple Notes and Bear for more trivial, throw-away stuff. Ultimately, that is for short-lived stuff - the important stuff gets checked into the personal wiki, because text/plain wins.\n\nIn terms of this blog, it uses Hugo. Why? Dunno. It\u2019s alright. I chose it because it\u2019s a single binary, and I had previously gotten utterly bored of RubyGems and Python virtualenvs and all that dependency management stuff. There\u2019s stuff I don\u2019t like about it, but I don\u2019t really have time to fix all the issues now.\n\nGo make stuff\n\nWriting and reading about tools is fun, but it is a whole lot of empty calories.\n\nPick the least bad option, try to exercise control over your tools rather than cede it to unpleasant people who want to turn everything on your computer into a satellite TV subscription, with you as a disempowered consumer.\n\nUse the tools you\u2019ve settled on to do things you value: learning stuff, writing, make art, watch cat videos, and your job (optional). That is all.", "label": 1}
{"title": "interview with Jack corbett", "url": "https://lifeofpablo.com/blog/interview-with-jack-corbett", "content": "interview with Jack corbett\n\nThis post was written in English (en_US).\n\nFrom Madison, WI to Los Angeles, CA.\n\nI had the opportunity to interview someone who has been working on a lot of projects during the last year or so. He has come a long way from living in Wyoming to going to school in Nebraska. He decided to take a different route and packed his things and moved to Los Angeles. I am really excited for the great things he is doing to become \"\"different\"\" and not living a normal life. He tells us about his journey and some tips to do more in life. I see great things for you Jack!\n\nPablo: Jack tell me about yourself?\n\nJack: Where to start? My name is Jack Corbett, 22 years old. I am from Madison, WI. I've always been a kid who has been in love with sports his whole life. The only thing I really wanted to do was to play football in NFL. That all changed when I got a concussion after my third year in college playing football. Things really changed after this. The concussion was very eye opening for me. I felt so messed up and out if it. I decided to take another route after football.\n\nP: What gave you that spark to get you into music?\n\nJ: When I was in high school I took a Hip-Hop class studies. I did a freestyle rap in class on the spot. Alongside I had used a beat production program, also referred as a digital audio workstation. Everyone had to present and the class had to pick their favorite beat that they liked. They ended up choosing mine. Everyone really seemed to love it! It got me super pumped. The feedback I got was good. Music is about impressing your fans. Might as well do it and I just love it. That\u2019s simply how it is. At first when starting out, I did not take music production seriously. I had always wanted to be different in how I wanted my life to be \u201cHow could I be different from the typical 9 to 5 lifestyle?\u201d Music was the way to go to do this. I turned this around and I created an independent record label, [NAME?] Records.\n\nJ: I started modeling when I got to to Los Angeles. I decided to do an open call audition for Maverick Modeling in LA. After I had done my audtion, they were interested in me but I needed to fix a few things to get things going. They wanted me to get more in shape. I had to workout quite a bit and check in with them every week for 4 weeks. Then it really worked out.\n\nP: What kind of modeling or related jobs have you had so far?\n\nJ: Oh, man! I've had a few jobs so far! I've been doing stuff with H&M, commercials, print & digital publications and some things in YouTube. Everything counts! I've had so much fun!\n\nP: How did you build contacts and/or clients?\n\nJ: The agency handles modeling projects (business aspect) for me to do. I hired a photographer that I met through a few friends. [She] is from London. I have met people from back home. You meet so many people in LA. Making beats, you get to know other people as well. I have met with a new group of rappers called FRIO. I have also made friends with Ponts De Leon. He is really making it out here. I got to hangout in his studio. Then I checked out Altrium Sudios and met label executive, Quincy Howard. I've been back there a few times with some rapper friends and Quincy has been there every time. Through meeting him and liking my vibe. I got an interview to be an intern. Today is my third day here. I am really loving every minute of it I'm grateful for all of the people I've met so far.\n\nP: What was the biggest challenge moving to this new place that you call home?\n\nJ: My biggest challenge would be financials. Everything out here is just much more expensive in every aspect Getting to LA was something else I rode for 30 hours in about 2 days. I had around 10 hours of rest before I had to start my new job Monday morning. Now I have a car. I didn\u2019t have a car for six months. I made it though LA was a bit shocking for me. It was over stimulating to be here at first. It's just different from what I was used to.\n\nP: What gave you the motivation to stay with it?\n\nJ: There would be a lot of days just sitting in your room. It is really hard to stay motivated. I love producing music! Sometimes I would be bored out of my mind not being able to show anyone my beats. It is seriously a good time even if I am by myself. At the same time, It is addicting like a video game. Sometimes I can't stop. As you get better, it is all about time. More people start to notice once you start getting better. I have so much more fun when I feel confident that I am getting better.\n\nP: Did you have any doubts in your mind that you were not going to be successful?\n\nJ: Everyone has lots of ups and down that they face. You never know when something crazy might happen!? A variety of things can make you question yourself. Things like when no one is buying your beats or not getting calls for audtions. Football has taught me to work hard. I just don't give up! I can't. .\"", "label": 1}
{"title": "Building Private Processing for AI tools on WhatsApp", "url": "https://engineering.fb.com/2025/04/29/security/whatsapp-private-processing-ai-tools/", "content": "We are inspired by the possibilities of AI to help people be more creative, productive, and stay closely connected on WhatsApp, so we set out to build a new technology that allows our users around the world to use AI in a privacy-preserving way.\n\nWe\u2019re sharing an early look into Private Processing, an optional capability that enables users to initiate a request to a confidential and secure environment and use AI for processing messages where no one \u2014 including Meta and WhatsApp \u2014 can access them.\n\nTo validate our implementation of these and other security principles, independent security researchers will be able to continuously verify our privacy and security architecture and its integrity.\n\nAI has revolutionized the way people interact with technology and information, making it possible for people to automate complex tasks and gain valuable insights from vast amounts of data. However, the current state of AI processing \u2014 which relies on large language models often running on servers, rather than mobile hardware \u2014 requires that users\u2019 requests are visible to the provider. Although that works for many use cases, it presents challenges in enabling people to use AI to process private messages while preserving the level of privacy afforded by end-to-end encryption.\n\nWe set out to enable AI capabilities with the privacy that people have come to expect from WhatsApp, so that AI can deliver helpful capabilities, such as summarizing messages, without Meta or WhatsApp having access to them, and in the way that meets the following principles:\n\nOptionality: Using Meta AI through WhatsApp, including features that use Private Processing, must be optional.\n\nTransparency: We must provide transparency when our features use Private Processing.\n\nUser control: For people\u2019s most sensitive chats that require extra assurance, they must be able to prevent messages from being used for AI features like mentioning Meta AI in chats, with the help of WhatApp\u2019s Advanced Chat Privacy feature.\n\nIntroducing Private Processing\n\nWe\u2019re excited to share an initial overview of Private Processing, a new technology we\u2019ve built to support people\u2019s needs and aspirations to leverage AI in a secure and privacy-preserving way. This confidential computing infrastructure, built on top of a Trusted Execution Environment (TEE), will make it possible for people to direct AI to process their requests \u2014 like summarizing unread WhatsApp threads or getting writing suggestions \u2014 in our secure and private cloud environment. In other words, Private Processing will allow users to leverage powerful AI features, while preserving WhatsApp\u2019s core privacy promise, ensuring no one except you and the people you\u2019re talking to can access or share your personal messages, not even Meta or WhatsApp.\n\nTo uphold this level of privacy and security, we designed Private Processing with the following foundational requirements:\n\nConfidential processing: Private Processing must be built in such a way that prevents any other system from accessing user\u2019s data \u2014 including Meta, WhatsApp or any third party \u2014 while in processing or in transit to Private Processing.\n\nEnforceable guarantees: Attempts to modify that confidential processing guarantee must cause the system to fail closed or become publicly discoverable via verifiable transparency.\n\nVerifiable transparency: Users and security researchers must be able to audit the behavior of Private Processing to independently verify our privacy and security guarantees.\n\nHowever, we know that technology platforms like ours operate in a highly adversarial environment where threat actors continuously adapt, and software and hardware systems keep evolving, generating unknown risks. As part of our defense-in-depth approach and best practices for any security-critical system, we\u2019re treating the following additional layers of requirements as core to Private Processing on WhatsApp:\n\nNon-targetability: An attacker should not be able to target a particular user for compromise without attempting to compromise the entire Private Processing system.\n\nStateless processing and forward security: Private Processing must not retain access to user messages once the session is complete to ensure that the attacker can not gain access to historical requests or responses.\n\nThreat modeling for Private Processing\n\nBecause we set out to meet these high-security requirements, our work to build Private Processing began with developing a threat model to help us identify potential attack vectors and vulnerabilities that could compromise the confidentiality, integrity, or availability of user data. We\u2019ve worked with our peers in the security community to audit the architecture and our implementation to help us continue to harden them.\n\nBuilding in the open\n\nTo help inform our industry\u2019s progress in building private AI processing, and to enable independent security research in this area, we will be publishing components of Private Processing, expanding the scope of our Bug Bounty program to include Private Processing, and releasing a detailed security engineering design paper, as we get closer to the launch of Private Processing in the coming weeks.\n\nWhile AI-enabled processing of personal messages for summarization and writing suggestions at users\u2019 direction is the first use case where Meta applies Private Processing, we expect there will be others where the same or similar infrastructure might be beneficial in processing user requests. We will continue to share our learnings and progress transparently and responsibly.\n\nHow Private Processing works\n\nPrivate Processing creates a secure cloud environment where AI models can analyze and process data without exposing it to unauthorized parties.\n\nHere\u2019s how it works:\n\nAuthentication: First, Private Processing obtains anonymous credentials to verify that the future requests are coming from authentic WhatsApp clients.\n\nThird-party routing and load balancing: In addition to these credentials, Private Processing fetches HPKE encryption public keys from a third-party CDN in order to support Oblivious HTTP (OHTTP).\n\nWire session establishment: Private Processing establishes an OHTTP connection from the user\u2019s device to a Meta gateway via a third-party relay which hides requester IP from Meta and WhatsApp.\n\nApplication session establishment: Private Processing establishes a Remote Attestation + Transport Layer Security (RA-TLS) session between the user\u2019s device and the TEE. The attestation verification step cross-checks the measurements against a third-party ledger to ensure that the client only connects to code which satisfies our verifiable transparency guarantee.\n\nRequest to Private Processing: After the above session is established, the device makes a request to Private Processing (e.g., message summarization request), that is encrypted end-to-end between the device and Private Processing with an ephemeral key that Meta and WhatsApp cannot access. In other words, no one except the user\u2019s device or the selected TEEs can decrypt the request.\n\nPrivate Processing: Our AI models process data in a confidential virtual machine (CVM), a type of TEE, without storing any messages, in order to generate a response. CVMs may communicate with other CVMs using the same RA-TLS connection clients use to complete processing.\n\nResponse from Private Processing: The processed results are then returned to the user\u2019s device, encrypted with a key that only the device and the pre-selected Private Processing server ever have access to. Private Processing does not retain access to messages after the session is completed.\n\nThe threat model\n\nIn designing any security-critical system, it is important to develop a threat model to guide how we build its defenses. Our threat model for Private Processing includes three key components:\n\nAssets : The sensitive data and systems that we need to protect.\n\nThreat actors : The individuals or groups that may attempt to compromise our assets.\n\nThreat scenarios : The ways in which our assets could be compromised, including the tactics, techniques, and procedures (TTPs) that threat actors might use.\n\nAssets\n\nIn the context of applying Private Processing to summarizing unread messages or providing writing suggestions at users\u2019 direction, we will use Private Processing to protect messaging content, whether they have been received by the user, or still in draft form. We use the term \u201cmessages\u201d to refer to these primary assets in the context of this blog.\n\nIn addition to messages, we also include additional, secondary assets which help support the goal of Private Processing and may interact with or directly process assets: the Trusted Computing Base (TCB) of the Confidential Virtual Machine (CVM), the underlying hardware, and the cryptographic keys used to protect data in transit.\n\nThreat actors\n\nWe have identified three threat actor types that could attack our system to attempt to recover assets.\n\nMalicious or compromised insiders with access to our infrastructure. A third party or supply chain vendor with access to components of the infrastructure. Malicious end users targeting other users on the platform.\n\nThreat scenarios\n\nWhen building Private Processing to be resilient against these threat actors, we consider relevant threat scenarios that may be pursued against our systems, including (but not limited to) the following:\n\nExternal actors directly exploit the exposed product attack surface or compromise the services running in Private Processing CVMs to extract messages.\n\nAnywhere the system processes untrusted data, there is potentially an attack surface for a threat actor to exploit. Examples of these kinds of attacks include exploitation of zero-day vulnerabilities or attacks unique to AI such as prompt injection.\n\nPrivate Processing is designed to reduce such an attack surface through limiting the exposed entry points to a small set of thoroughly reviewed components which are subject to regular assurance testing. The service binaries are hardened and run in a containerized environment to mitigate the risks of code execution and limit a compromised binary\u2019s ability to exfiltrate data from within the CVM to an external party.\n\nInternal or external attackers extract messages exposed through the CVM.\n\nObservability and debuggability remains a challenge in highly secure environments as they can be at odds with the goal of confidential computing, potentially exposing side channels to identify data and in the worst case accidentally leaking messages themselves. However, deploying any service at scale requires some level of observability to identify failure modes, since they may negatively impact many users, even when the frequency is uncommon. We implement a log-filtering system to limit export to only allowed log lines, such as error logs.\n\nLike any complex system, Private Processing is built of components to form a complex supply chain of both hardware and software. Internally, our CVM build process occurs in restricted environments that maintain provenance and require multi-party review. Transparency of the CVM environment, which we\u2019ll provide through publishing a third-party log of CVM binary digests and CVM binary images, will allow external researchers to analyze, replicate, and report instances where they believe logs could leak user data.\n\nInsiders with physical or remote access to Private Processing hosts interfere with the CVM at boot and runtime, potentially bypassing the protections in order to extract messages.\n\nTEE software exploitation is a growing area of security research, and vulnerability researchers have repeatedly demonstrated the ability to bypass TEE guarantees. Similarly, physical attacks on Private Processing hosts may be used to defeat TEE guarantees or present compromised hosts as legitimate to an end user.\n\nTo address these unknown risks, we built Private Processing on the principle of defense-in-depth by actively tracking novel vulnerabilities in this space, minimizing and sanitizing untrusted inputs to the TEE, minimizing attack surface through CVM hardening and enabling abuse detection through enhanced host monitoring.\n\nBecause we know that defending against physical access introduces significant complexity and attack surface even with industry-leading controls, we continuously pursue further attack surface hardening. In addition, we reduce these risks through measures like encrypted DRAM and standard physical security controls to protect our datacenters from bad actors.\n\nTo further address these unknown risks, we seek to eliminate the viability of targeted attacks via routing sessions through a third-party OHTTP relay to prevent an attacker\u2019s ability to route a specific user to a specific machine.\n\nDesigning Private Processing\n\nHere is how we designed Private Processing to meet these foundational security and privacy requirements against the threat model we developed.\n\n(Further technical documentation and security research engagements updates are coming soon).\n\nConfidential processing\n\nData shared to Private Processing is processed in an environment which does not make it available to any other system. This protection is further upheld by encrypting data end-to-end between the client and the Private Processing application, so that only Private Processing, and no one in between \u2013 including Meta, WhatsApp, or any third-party relay \u2013 can access the data.\n\nTo prevent possible user data leakage, only limited service reliability logs are permitted to leave the boundaries of CVM.\n\nSystem software\n\nTo prevent privileged runtime access to Private Processing, we prohibit remote shell access, including from the host machine, and implement security measures including code isolation. Code isolation ensures that only designated code in Private Processing has access to user data. Prohibited remote shell access ensures that neither the host nor a networked user can gain access to the CVM shell.\n\nWe defend against potential source control and supply chain attacks by implementing established industry best practices. This includes building software exclusively from checked-in source code and artifacts, where any change requires multiple engineers to modify the build artifacts or build pipeline.\n\nAs another layer of security, all code changes are auditable. This allows us to ensure that any potential issues are discovered \u2014 either through our continuous internal audits of code, or by external security researchers auditing our binaries.\n\nSystem hardware\n\nPrivate Processing utilizes CPU-based confidential virtualization technologies, along with Confidential Compute mode GPUs, which prevent certain classes of attacks from the host operating system, as well as certain physical attacks.\n\nEnforceable guarantees\n\nPrivate Processing utilizes CPU-based confidential virtualization technologies which allow attestation of software based in a hardware root of trust to guarantee the security of the system prior to each client-server connection. Before any data is transmitted, Private Processing checks these attestations, and confirms them against a third-party log of acceptable binaries.\n\nStateless and forward secure service\n\nWe operate Private Processing as a stateless service, which neither stores nor retains access to messages after the session has been completed.\n\nAdditionally, Private Processing does not store messages to disk or external storage, and thus does not maintain durable access to this data.\n\nAs part of our data minimization efforts, requests to Private Processing only include data that is useful for processing the prompt \u2014 for example, message summarization will only include the messages the user directed AI to summarize.\n\nNon-targetability\n\nPrivate Processing implements the OHTTP protocol to establish a secure session with Meta routing layers. This ensures that Meta and WhatsApp do not know which user is connecting to what CVM. In other words, Meta and WhatsApp do not know the user that initiated a request to Private Processing while the request is in route, so that a specific user cannot be routed to any specific hardware.\n\nPrivate Processing uses anonymous credentials to authenticate users over OHTTP. This way, Private Processing can authenticate users to the Private Processing system, but remains unable to identify them. Private Processing does not include any other identifiable information as part of the request during the establishment of a system session. We limit the impact of small-scale attacks by ensuring that they cannot be used to target the data of a specific user.\n\nVerifiable transparency\n\nTo provide users visibility into the processing of their data and aid in validation of any client-side behaviors, we will provide capabilities to obtain an in-app log of requests made to Private Processing, data shared with it, and details of how that secure session was set up.\n\nIn order to provide verifiability, we will make available the CVM image binary powering Private Processing. We will make these components available to researchers to allow independent, external verification of our implementation.\n\nIn addition, to enable deeper bug bounty research in this area, we will publish source code for certain components of the system, including our attestation verification code or load bearing code.\n\nWe will also be expanding the scope of our existing Bug Bounty program to cover Private Processing to enable further independent security research into Private Processing\u2019s design and implementation.\n\nFinally, we will be publishing a detailed technical white paper on the security engineering design of Private Processing to provide further transparency into our security practices, and aid others in the industry in building similar systems.\n\nGet Involved\n\nWe\u2019re deeply committed to providing our users with the best possible messaging experience while ensuring that only they and the people they\u2019re talking to can access or share their personal messages. Private Processing is a critical component of this commitment, and we\u2019re excited to make it available in the coming weeks.\n\nWe welcome feedback from our users, researchers, and the broader security community through our security research program:", "label": 0}
{"title": "Introducing the Stripe CLI", "url": "https://stripe.com/blog/stripe-cli", "content": "Building and testing a Stripe integration can require frequent switching between the terminal, your code editor, and the Dashboard. Today, we\u2019re excited to launch the Stripe command-line interface (CLI). It lets you interact with Stripe right from the terminal and makes it easier to build, test, and manage your integration.\n\nTo start, the CLI will let you test webhooks, tail real-time API logs, and create or update API objects. Here\u2019s a preview of some of the features:\n\nSimplify webhook setup and testing\n\nStripe sends a variety of webhooks, which let you listen and respond to specific events programmatically. Run stripe listen with the CLI to forward webhooks to your local web server during development\u2014no third-party tunneling tools required. You can also trigger and test specific webhook events with stripe trigger .\n\nDebug faster with real-time logs\n\nWhile integrating, it can be useful to look at logs to fix any issues. You can now use stripe logs tail to stream API request logs in real time in the terminal in addition to viewing these logs from the Dashboard. Quickly inspect parameters or JSON responses and debug errors as they happen.\n\nSpeed up common tasks and workflows\n\nYou can now create, retrieve, update, or delete any Stripe object directly from the CLI in both test and live mode. For example, you can use stripe customers create and specify parameters for properties inline.\n\nSince you can pipe results into other commands, this can be a simple and powerful way to automate tasks. Here\u2019s an example:\n\n$ stripe subscriptions list \\ --live \\ --status past_due \\ --expand data.customer | \\ jq -r \".data[] | [.customer.name, .customer.email] | @csv\" ~\n\nThe above command uses the CLI to list live subscriptions that are past due, pipes the JSON response to jq to extract the customer name and email, and exports the data in CSV format.\n\nTo see a full list of supported commands, run stripe help or visit the docs to learn more.\n\nGetting started\n\nThe Stripe CLI natively supports macOS, Windows, and Linux. You can also pull our Docker image to use in automated testing or a continuous integration setup.", "label": 0}
{"title": "December 2018", "url": "https://lifeofpablo.com/blog/published:2018-12", "content": "en\n\nPablo Morales-Garcia\n\nNew Student Enrollment Leader\n\nPhone: (308) 865-8526 | Email: BeALoper@unk.edu\n\nFavorite Place On Campus\n\nI really love the campus in its entirety from the art to the history of building and monuments. One of my favorite features on campus are the trails and sidewalks that connect you to all of campus but also to all of the bike trails in the city of Kearney. It is so easy to get to Cottonmill or Yanney Park on the nice trails. Around campus, I really enjoying riding around on my longboard! I can use my longboard to get point A to point B in a snap! You'll thank me later when you're running behind for class! I really enjoy the social aspect by meeting fellow longboarders.\n\n\n\nFavorite UNK Tradition\n\nMy favorite UNK Tradition would be Big Blue and Gold Week. It is such a good way to get started on the new school year and it is such an opportunity to get to know people by just having fun. One of my favorite events during BBGW is going to destination downtown and watch the food eating competition. One of my fondest memories is being able to ride on a float during and just having a blast! Not every day do you have the chance to ride on a float during a parade, cool, eh?\n\n\n\nWhat are you passionate about?\n\nI have always been passionate about travel and languages since a young age. My views of the world have expanded so much ever since I traveled when I was a kid and even more when I studied abroad in France. Traveling gives a person experiences that you cannot get in your backyard or in your daily routine!\n\nI want to spread the importance of learning more than one language. I want to share my passion of being open minded about the world and embrace languages. Just by learning another language opens the door for you in many ways you could never ever imagine!\n\n\n\nI Chose UNK Because...\n\nWhen I came for a tour of UNK, I instantly became in love with the campus. Just being here I didn't feel overwhelmed being on the campus. Everyone I ran across was so nice! I felt such a personal connection especially when I met with the chair, advisor, and a professor of my program. Of all the school visits I had gone to the past, not one had offered me to meet with my potential advisor to do this. At that moment I realized that \"This is the school I want to attend!\" It all came down to the little things that would make such a difference in picking a school! After that....the rest is history! 2018 Staff\n\nJennifer Garcia\n\nMary Dworak\n\nNoah Journey\n\nOdwuar Quinonez\n\nPablo Morales-Garcia\n\nShelby Hoffmann\n\nValeria Lozano\n\nTaylor Janicek\n\nUniversity of Nebraska at Kearney\n\n\"\n\n!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>!>", "label": 1}
{"title": "The Many Hats of the Korean Convenience Stores", "url": "https://lifeofpablo.com/blog/the-many-hats-of-the-korean-convenience-stores", "content": "The Many Hats of the Korean Convenience Stores\n\nThis post was written in English (en_US).\n\nThis is part of a three or four part blog series. These posts will be interactive on my website (https://lifeofpablo.com) . I am in the process of rebuilding my website so stay tuned.\n\nHere I am in South Korea getting a snack at a convenience store. Everytime I go to one, I reflect on convenience stores back in the United States. Convenience stores in the United States are usually gas stations at the same time. Living in California you see convenience stores as standalone locations more often. Convenience stores back home get a bad wrap because they sell not so healthy food or are simply over priced. It's not so much in Korea. Yes, there are things that are overpriced but not as bad as you would see in the United States. Korean Convenience Stores are a whole different experience..\n\n\u2665\ufe0fKOREA IS LIVING IN THE FUTURE\u2665\ufe0f\n\nCommon convenience stores are GS25, Nice to CU and 7/11. They are all over the place. It's hard not to walk from one corner of a block to another corner of the same block without passing a convenience store. This is pretty consistent in most of Korea. The cities of Seoul and Chuncheon are good examples. Two or three years since the pandemic, many did stay open pretty late or open 24 hours. They are just there whenever you need something. You can buy a snack, a beer (or many), toiletries and more! I've even found socks after my shoes got soaked in the rain. Prices are not overly expensive taking in consideration that the exchange rate between USD and WON are in the favor of the American consumer such as myself.\n\nThe experience at every convenience store is different. You never know what you'll find. I'm not sure how it is decided on what is carried in each store. I do love the element of surprise.\n\nKorean Convenience Stores will fill the void in many aspects of life. They are considered important in Korean Culture and Socialization. You'll often find tables inside and outside of the store. I find it hard to find a normal bar establishment. Even if I did find a bar, it could be expensive or hard to get into. Convenience Stores are a meeting point to socialize especially if there is seating available at the location you visit. I really enjoy getting a beer or a few with friends and just talk about our day, our hopes & desires, and just having tipy conversations. I've had some really deep moments here with others. One of my favorite moments was finding a convenience store in a large park and making my ramen right then and there!\n\nWhat really shocked me was the Seoul WorldCup Stadium when I went to a soccer match. There I found a GS25 with prices similar to one you find on a random street.\n\nOverall, I'm happy with convenience stores here! I sure will miss them!\n\nI will share this information in a lesson in the near future with others.\n\nHere are my favorite items I get from the convenience store:\n\nSmall pack of coffee\n\nA cup of ice - use it for any drink you need to make cold (or colder.)\n\nGimbap\n\nBeer\n\nRam\n\nSnacks such as fried\n\nRamen\n\nWhat is nice about convenience stores is being able to cook instant ramen in-store seconds right after you buy it.\n\nSnacks\n\nThere are so many options. Here are a few!", "label": 1}
{"title": "HDR10+ Now Streaming on Netflix", "url": "https://netflixtechblog.com/hdr10-now-streaming-on-netflix-c9ab1f4bd72b?source=collection_home---4------0-----------------------", "content": "HDR10+ Now Streaming on Netflix Netflix Technology Blog 5 min read \u00b7 Mar 24, 2025 -- 13 Listen Share\n\nRoger Quero, Liwei Guo, Jeff Watts, Joseph McCormick, Agata Opalach, Anush Moorthy\n\nWe are excited to announce that we are now streaming HDR10+ content on our service for AV1-enabled devices, enhancing the viewing experience for certified HDR10+ devices, which previously only received HDR10 content. The dynamic metadata included in our HDR10+ content improves the quality and accuracy of the picture when viewed on these devices.\n\nDelighting Members with Even Better Picture Quality\n\nNearly a decade ago, we made a bold move to be a pioneering adopter of High Dynamic Range (HDR) technology. HDR enables images to have more details, vivid colors, and improved realism. We began producing our shows and movies in HDR, encoding them in HDR, and streaming them in HDR for our members. We were confident that it would greatly enhance our members\u2019 viewing experience, and unlock new creative visions \u2014 and we were right! In the last five years, HDR streaming has increased by more than 300%, while the number of HDR-configured devices watching Netflix has more than doubled. Since launching HDR with season one of Marco Polo, Netflix now has over 11,000 hours of HDR titles for members to immerse themselves in.\n\nWe continue to enhance member joy while maintaining creative vision by adding support for HDR10+. This will further augment Netflix\u2019s growing HDR ecosystem, preserve creative intent on even more devices, and provide a more immersive viewing experience.\n\nWe enabled HDR10+ on Netflix using the AV1 video codec that was standardized by the Alliance for Open Media (AOM) in 2018. AV1 is one of the most efficient codecs available today. We previously enabled AV1 encoding for SDR content, and saw tremendous value for our members, including higher and more consistent visual quality, lower play delay and increased streaming at the highest resolution. AV1-SDR is already the second most streamed codec at Netflix, behind H.264/AVC, which has been around for over 20 years! With the addition of HDR10+ streams to AV1, we expect the day is not far when AV1 will be the most streamed codec at Netflix.\n\nTo enhance our offering, we have been adding HDR10+ streams to both new releases and existing popular HDR titles. AV1-HDR10+ now accounts for 50% of all eligible viewing hours. We will continue expanding our HDR10+ offerings with the goal of providing an HDR10+ experience for all HDR titles by the end of this year\u00b9.\n\nIndustry Adopted Formats\n\nToday, the industry recognizes three prevalent HDR formats: Dolby Vision, HDR10, and HDR10+. For all three HDR Formats, metadata is embedded in the content, serving as instructions to guide the playback device \u2014 whether it\u2019s a TV, mobile device, or computer \u2014 on how to display the image.\n\nHDR10 is the most widely adopted HDR format, supported by all HDR devices. HDR10 uses static metadata that is defined once for the entire content detailing aspects such as the maximum content light level (MaxCLL), maximum frame average light level (MaxFALL), as well as characteristics of the mastering display used for color grading. This metadata only allows for a one-size-fits-all tone mapping of the content for display devices. It cannot account for dynamic contrast across scenes, which most content contains.\n\nHDR10+ and Dolby Vision improve on this with dynamic metadata that provides content image statistics on a per-frame basis, enabling optimized tone mapping adjustments for each scene. This achieves greater perceptual fidelity to the original, preserving creative intent.\n\nHDR10 vs. HDR10+\n\nThe figure below shows screen grabs of two AV1-encoded frames of the same content displayed using HDR10 (top) and HDR10+ (bottom).\n\nPhotographs of devices displaying the same frame with HDR10 metadata (top) and HDR10+ metadata (bottom). Notice the preservation of the flashlight detail in the HDR10+ capture, and the over-exposure of the region under the flashlight in the HDR10 one\u00b2.\n\nAs seen in the flashlight on the table, the highlight details are clipped in the HDR10 content, but are recovered in HDR10+. Further, the region under the flashlight is overexposed in the HDR10 content, while HDR10+ renders that region with greater fidelity to the source. The reason HDR10+, with its dynamic metadata, shines in this example is that the scenes preceding and following the scene with this frame have markedly different luminance statistics. The static HDR10 metadata is unable to account for the change in the content. While this is a simple example, the dynamic metadata in HDR10+ demonstrates such value across any set of scenes. This consistency allows our members to stay immersed in the content, and better preserves creative intent.\n\nReceiving HDR10+\n\nAt the time of launch, these requirements must be satisfied to receive HDR10+:\n\n1.Member must have a Netflix Premium plan subscription\n\n2. Title must be available in HDR10+ format\n\n3. Member device must support AV1 & HDR10+. Here are some examples of compatible devices:\n\nSmartTVs, mobile phones, and tablets that meet Netflix certification for HDR10+\n\nSource device (such as set-top boxes, streaming devices, MVPDs, etc.) that meets Netflix certification for HDR10+, connected to an HDR10+ compliant display via HDMI\n\n4. For TV or streaming devices, ensure that the HDR toggle is enabled in our Netflix application settings: https://help.netflix.com/en/node/100220\n\nAdditional guidance: https://help.netflix.com/en/node/13444\n\nSummary\n\nMore HDR content is watched every day on Netflix. Expanding the Netflix HDR ecosystem to include HDR10+ increases the accessibility of HDR content with dynamic metadata to more members, improves the viewing experience, and preserves the creative intent of our content creators. The commitment to innovation and quality underscores our dedication to delivering an immersive and authentic viewing experience for all our members.\n\nAcknowledgements\n\nLaunching HDR10+ was a collaborative effort involving multiple teams at Netflix, and we are grateful to everyone who contributed to making this idea a reality. We would like to extend our thanks to the following teams for their crucial roles in this launch:\n\nFootnotes", "label": 0}
{"title": "Reply to: Where Do You Call Home? by Jason Kottke", "url": "https://lifeofpablo.com/blog/reply-to-where-do-you-call-home-by-jason-kottke", "content": "In Reply to Where Do You Call Home? by Jason Kottke\n\nJason Kottke asks the following question and starts mentioning his struggle with the concept of home. He also asks his readers to answer the question\n\nWhere do you call home? And why? I\u2019ve been struggling with the concept of home for the past several years.\n\nOn the surface, this seems like a very simple question to answer but the answer can be be complex. For me, I take the complex route. I too have been struggling with this question. Home is where major moments have occured in my life.\n\nI grew up in Nebraska which was home for 25+ years. I say this is one place I call home is due to the fact this is where I grew up. It's where I learned the norms of the Midwest and how it influenced my persona. Nebraska is home because this is where many of my friends I made in university still reside. I have many deep connections with others. I somehow always end up here such as the holidays and other important events. It's a very special place to me but it is not the place I cull my current home.\n\nMexico is also my home for the fact I've lived there on an off though out the years. I learned what is was like to be around non-Americans. Living in Mexico, I got meet so many family members. I learned more of the traditions of my people and how live in a community. I learned that this was a place of rich culture and rich gastronomy. I learned to not take things for granted. It's a place I call home because it allows me to be different person compared to my American self. For the record, I don't go to resorts, I stay in rural Mexico. This is a home I return to often. This home is important to my identity.\n\nThe home I consider currently now is in California. This is the first place I moved away to where I didn't go back to my parents' home after an extended leave such as a trip or going to university. I work here now and I have built out new relationships. I'm still pretty new here. Living in California has allowed me to be more of myself and allow me to express myself. This is home because I get to work on myself as well. When people ask me where am I based out of or where is my home is, I always say California.\n\nI also ask the same question, \"Where do you call home? And why?\"", "label": 1}
{"title": "Imagen 4 is now available in the Gemini API and Google AI Studio", "url": "https://developers.googleblog.com/en/imagen-4-now-available-in-the-gemini-api-and-google-ai-studio/", "content": "We're thrilled to bring Imagen 4, our best text-to-image model yet, to paid preview in the Gemini API and for limited free testing in Google AI Studio. Imagen 4 offers significantly improved text rendering over our prior image models and pushes the boundaries of text-to-image generation quality.\n\n\n\nThe Imagen 4 Family: Imagen 4 and Imagen 4 Ultra\n\nWe\u2019re introducing two models within the Imagen 4 family, built to serve a variety of creative needs:\n\n\n\nImagen 4: Your go-to for most tasks\n\nThis is our flagship text-to-image model designed to handle a wide range of image generation tasks with significant improvements in quality, particularly for text generation, over Imagen 3. Imagen 4 is priced at $0.04 per output image.\n\n\n\nImagen 4 Ultra: Precision for your prompts\n\nWhen you need your images to precisely follow instructions, Imagen 4 Ultra is the model for you. It's designed to produce outputs that are more highly aligned with your text prompts, achieving strong results compared to other leading image generation models. Imagen 4 Ultra is priced at $0.06 per output image.\n\nWe will introduce additional billing tiers in the coming weeks. In the meantime, you can request higher rate limits for Imagen 4 and 4 Ultra.\n\n\n\nSee Imagen 4 in action\n\nTo give you a glimpse of Imagen 4's capabilities, here are some examples of what you can create. Created using Imagen 4 Ultra, the prompts below showcase the model's versatility across various styles and content.\n\nPrompt: A 3-panel cosmic epic comic. Panel 1: Tiny 'Stardust' in nebula; radar shows anomaly (text 'ANOMALY DETECTED'), hull text 'stardust'. Pilot whispers. Panel 2: Bioluminescent leviathan emerges; console red text 'WARNING!. Panel 3: Leviathan chases ship through asteroids; console re text 'SHIELD CRITICAL!', screen text 'EVADE!'. Pilot screams, SFX 'CRUNCH!', 'ROOOOAAARR!'.", "label": 0}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2014-07", "content": "en\n\n\"Well today was the end of the great World Leaders Camp hosted at UNK.\n\nI had breakfast Had some scrambled eggs, sausage and hash-brown with a cup of OJ. I was just pondering about what the day was to bring... Mixed feelings all around.\n\nWe had a Global Networking Lecture from Prof. Amundson. It was very amusing. I personally learned a lot. His point was to meet people and network. He said, \"\" Your best experiences are through networking.\"\" I 100% agree on this quote. Study abroad is really something I would enjoy.\n\nYou can not end a great camp without a great game of Jeopardy. Man was my team being beat. Colin was just spitting out answers like no other. I was getting nervous.\n\nThen we got our certificates\n\nThis is my first certificate in college. I hope for many more. I was happy when I got it.\n\nI want to thank all of the people who were in the WLC.\n\nKatherine A Berke ;\n\nAshley M Bruha\n\nLydia J Crocker\n\nSarah C Haack\n\nSierra Hirth\n\nAllison N Kiolbasa\n\nNoemi L Liscano\n\nDanielle I Merrill\n\nBrittany K Mrkvicka\n\nPaige E Phillips ;\n\nColin T Stiles\n\nCatiana Urrutia\n\nI'm glad to have met all of you guys. I consider all of you like family. I'll always be excited to see you guys and say hello even from a distance.\n\nand of course Lisa for being our advisor for the Camp!! She's pretty cool I would say so myself. We could not forget Ann Marie also!!\n\nI'll see everyone on Campus!! GO LOPERS!!\"", "label": 1}
{"title": "30", "url": "https://lifeofpablo.com/blog/30", "content": "30\n\nPablo doing the peace sign with fingers.\n\nThis post was written in English (en_US).\n\nToday, I turn 30 years old. Woohoo! I guess I\u2019m no longer a 20-something-year-old. These last 10 years have flown by quickly but man, have they been memorable. It\u2019s a tad bitter sweet. The positive overtakes the negative by a long shot. I can\u2019t stop the aging process. If only it were that easy!\n\nHonestly, at this very moment, I don\u2019t feel any different. I still feel energetic and craving adventure. I think these song lyrics from T-Swizzle describe how I feel.\n\n(Hey!) I don't know about you But I'm feeling twenty-two\n\nIn hindsight, looking at myself in the mirror more recently, I notice my facial hair is starting to turn white. Nothing like a fresh shave that still makes me look young and boyish. I must being doing something right with my skin routine (and maybe good genes from my parents)!\n\nReflections\n\nI reflect on the growth I\u2019ve had since I entered my 20s. My 20s were about\n\nLearning\n\nFalling and rising\n\nGoing on adventures.\n\nLoved ones\n\nI learned some great lessons along the way. Some lessons were not so easy to swallow. Man, did I learn. There are so many things I know now that I wish a 20-year-old me knew. One of the things I wish he knew was, \u201cLove yourself more!\u201d I do love myself more than ever. I\u2019m happier with who I\u2019ve become. I\u2019m not done learning who I become. I don\u2019t think that ever stops. There were many moments in life when I hit my absolute lowest. Those are moments I\u2019m not proud of. While I did manage to get myself out of them, I did come back stronger. Another lesson here - ask for help and use the resources available to you. You\u2019re not alone, I promise.\n\nTake risks! Take more! I'm thrilled to have moved to California. I still can\u2019t believe that this guy from Nebraska moved to San Francisco. I was grateful to have gone on so many adventures. I learned so much about myself getting away. Some of my best learning moments have been from strangers and experiences with myself in another country. Who knew you could learn something so new about yourself at 3 am while eating a d\u00f5ner kebab on the streets of Berlin?\n\nWhen I was in my early 20s, I said something along the lines of, \u201cI get to be away from my parents! Freedom!\u201d When I was in college, I didn\u2019t want to come home as often, or if I went home to visit, I would not stay for too long. Now more than ever, I miss my parents. Now, I live far away from them. I only get to have home-cooked meals a few times a year. It honestly makes me sad watching them get older. I still need my parents. I do my best to call every day. My relationship with them has been stronger than ever. I am thankful for all the things they\u2019ve done for me and my siblings\n\nWho\u2019s cutting the internet onions on here?\n\nTravel\n\nI\u2019m very fortunate that I traveled a lot in my 20s. I want to thank my parents for taking us to Mexico as a young kid.\n\nI\u2019ve traveled to Canada, all over Europe, South Korea, Mexico, and various parts of the United States.\n\nTravel Buddies\n\nI got to travel with my favorite travel buddies, Sammy and Manny! We've have gone on some wild adventures. I\u2019m very happy we stayed friends. Best travel buddies ever!\n\nBest Birthday Ever! - Czech Republic\n\nOne of my favorite birthdays was when I was studying abroad in Europe. I visited my friends in the Czech Republic.\n\nPablo\u2019s 22nd birthday in 2017 in Czech Republic]\n\nFriends\n\nI\u2019ve made an amazing community of friends. People, I consider my family away from home. Learning from people whom I loved, making friends wherever I go! I\u2019ve made so many friends.\n\nWhat goals do I have in my 30s?\n\nThere are a lot of things I want to accomplish during my 30s. I have a vision for the next few years! I\u2019m hopeful I can find the right path to reach my goals\n\nEducation\n\nI truly value education. It got me where I am today! I was very fortunate to receive a great public school education. Others also need to have access to a great public school education. We must fight against the privatization of the education system. I want to continue to fight for public education. Public education provided me with so many opportunities.\n\nI am currently doing my master\u2019s degree at the University of Nebraska Kearney. My goal one day is to teach a class or two at the university level. It would be awesome to guide future teachers and help them be confident with technology use and go beyond 21st-century learning. I do miss being in the classroom.\n\nI would like to receive a PhD.\n\nI also want to improve my programming skills.\n\nHealth\n\nI\u2019m working towards getting in better shape and thinking of long-term health. As I age, I know I won\u2019t bounce back from injury as quickly. I know I must do everything in my power to take control of my health. Preventative measures are what will keep the hospital away. Mental health is just as important.\n\nAdventures\n\nI want to continue traveling. I want to visit more parts of the world. Ever since I last renewed my passport, the visa pages have been blank. The passport I had in my early twenties was pretty full. It\u2019s so satisfying to look at the stamps and stickers on each page. I want to take my parents on a fun adventure as well\n\nStartup\n\nI\u2019ve been helping a friend with a startup and launching an idea that we have. I am hopeful this year will be great!\n\nFinancial Independence\n\nI want to make smarter choices with money. I\u2019ve been reading up on financial literature to become better with money. Times are changing, and I must keep up with inflation, changing markets, AI, etc.\n\nLooking Forward\n\nOverall, things are finally clicking. I\u2019m excited for what\u2019s to come and all the things to learn. I will reevaluate and reprioritize many things in life.\n\nWhat have you learned in your 30s?\n\nHow am I celebrating my birthday?\n\nLuckily, I live in San Francisco, so there\u2019s a lot to do! I will be celebrating the entire weekend with my friends in San Francisco! So stoked to watch Drag Disco with Trixie Mattel! Let\u2019s put on some pink clothes and dance into my 30s!", "label": 1}
{"title": "Introducing Focus, a new open source Gradle plugin", "url": "https://dropbox.tech/mobile/introducing-focus-a-new-open-source-gradle-plugin", "content": "Working with large projects in Gradle can be a pain. As a project grows larger over time, configuration and build times tend to grow with it. Breaking up large projects into lots of modules can help mitigate this, but unless you meticulously follow the seemingly ever-changing advice about project structure and build script configuration, it\u2019s easy to end up with a build system that\u2019s doing far more than it needs to. And while Gradle is doing that extra work, you have to sit and wait. At Dropbox, like many companies, we use Gradle to build our Android apps. Our monorepo contains almost 600 projects, most of which aren\u2019t used by our engineers on a day-to-day basis. This means that Android Studio has to load many more modules than required, and we waste time waiting for our tools. When we\u2019re working on features within large projects, our time is generally spent within a specific feature module. By using reasonable abstractions within our modules we are able to develop code and write tests without depending on most of the other projects in our monorepo. But wouldn\u2019t it be great if we could easily tell Gradle exactly where we\u2019re working and what dependencies we need, and have it ignore the rest? To answer that question, we recently set out to find an easy way for our engineers to specify the module in which they\u2019re working, and let Gradle and Android Studio ignore the rest.\n\nThe manual way\n\nIn the past, we\u2019ve been able to trim the number of projects that Gradle loads by manually adjusting the projects that are included in the settings.gradle.kts file. The quick and dirty way is to simply comment out the projects that you don\u2019t need:\n\nCopy include(\":sample:app1\") //include(\":sample:app2\") include(\":sample:lib1a\") include(\":sample:lib1b\") //include(\":sample:lib2a\") //include(\":sample:lib2b\") //include(\":sample:lib2c\") include(\":sample:lib-shared\") include(\":sample:moved\") project(\":sample:moved\").projectDir = File(\"sample/lib-moved\")\n\nHowever, this approach quickly becomes unwieldy as a project grows, and the interdependency of your modules grows more complex. Another approach is to extract all of those include(\"...\") statements into a separate file. This way, you can more easily switch between modules by simply choosing which files you want to apply:\n\nCopy //apply(from = File(\"project-1.settings.gradle.kts\") //apply(from = File(\"project-2.settings.gradle.kts\") //apply(from = File(\"project-3.settings.gradle.kts\") apply(from = File(\"settings-all.gradle.kts\"))\n\nBut with all of the possible permutations, it\u2019s easy for this to become unmanageable too. On top of that, making sure those extra settings files stay in sync can be a nightmare. Since Gradle already knows about the dependency tree of your project, wouldn\u2019t it be great if there was an easy way to have Gradle create these files for you?\n\nEnter Focus\n\nFocus allows you to do just that. Our Focus Gradle Plugin evaluates your project configuration and creates a unique settings.gradle file for the module you want to focus on. This file only includes the project dependencies that a specific module requires, allowing you to easily ignore the rest. The plugin also creates a .focus file which identifies which module should currently be focused. With these files in place, Gradle will only configure the modules you need when you sync your project. Deleting the .focus file\u2014which can be done manually, or by using the clearFocus task\u2014will revert to including all of your modules. For example, while working on design systems at Dropbox, the Focus plugin allows us to easily focus on our UI Components Playground project (a sample app in our monorepo for working on design system components). This reduces the IDE sync time from 1 minute to 15 seconds. To start, we simply run the following command:\n\nCopy ./gradlew :applications:uicomponents_playground:focus\n\nThis creates a focus.settings.gradle file in the module\u2019s build directory\u2014which only includes the projects that are required to build the uicomponents_playground module\u2014and a .focus file at the root of the project which points to the newly created Gradle settings file. Clicking the Sync Elephant icon in Android Studio will only load the required modules, improving the performance of both the IDE and Gradle. If we want to spend some time in a different module\u2014perhaps a dependency that needs updating\u2014we can simply focus on that other module and sync.\n\nCopy ./gradlew :dsys:components:focus\n\nThis way, we\u2019re easily able to trim the number of modules loaded in the IDE and configured by Gradle. When we\u2019re ready to go back to building the entire project, we can simply remove the .focus file in the root directory of the project, or run the following Gradle task:\n\nCopy ./gradlew clearFocus\n\nWith Focus, our engineers are able to iterate faster throughout the day, allowing them to spend more time focused on the quality of the code they\u2019re developing, and less time waiting for their tools to sync.\n\nAdding the Focus plugin\n\nFocus is a settings plugin, so it should be applied in your settings.gradle(.kts) file. Since we currently publish the plugin to Maven Central, you\u2019ll have to add that as a repository in your pluginsManagement block:\n\nCopy // settings.gradle(.kts) pluginsManagement { repositories { mavenCentral() gradlePluginPortal() } } plugins { id(\"com.dropbox.focus\") version \"0.4.0\" }\n\nNext, move all of your include statements into a settings-all.gradle file:\n\nCopy // settings-all.gradle(.kts) include ':sample:app2' include ':sample:lib2c' include ':sample:lib-shared' // ... include ':sample:moved' project(':sample:moved').projectDir = new File(\"sample/lib-moved\")\n\nOptionally, you can configure the plugin within your settings.gradle.kts file:\n\nCopy // settings.gradle(.kts) focus { // The name of the settings file allSettingsFileName = \"settings-all.gradle\" // Default // The name of the pointer file that tells Focus which module to focus on // This should be added to your .gitignore file. focusFileName = \".focus\" // Default }\n\nLastly, don\u2019t forget to add your Focus file\u2014.focus by default\u2014to your .gitignore file. Once those steps are complete, you can use the focus Gradle tasks in each subproject to reduce the amount of time you have to sit around and wait.\n\nOpen source at Dropbox", "label": 0}
{"title": "Simplifying JVM App Development with Heroku\u2019s Buildpack Magic", "url": "https://www.heroku.com/blog/simplifying-jvm-app-development-herokus-buildpack-magic/", "content": "Heroku\u2019s commitment to developer productivity shines through in its powerful buildpack system. They handle the heavy lifting of building your app, letting you focus on what matters most: writing code. A prime example is the Heroku Java buildpack, a versatile tool that simplifies deploying Java applications, especially those built with popular frameworks like Spring Boot, Quarkus, and Micronaut.\n\nOne of the core strengths of Heroku buildpacks is their automatic nature. They intelligently detect your application\u2019s language and framework, fetching the necessary build tools and configuring the Heroku platform to run your app seamlessly. This means no more wrestling with server configurations or deployment scripts \u2013 Heroku handles it all.\n\nBeyond just building your application, our Java Buildpacks go a step further by understanding the nuances of different Java frameworks and tools. They automatically inject framework-specific configurations, such as database connection details for Postgres, eliminating the need for manual setup. This deep integration significantly reduces the friction of deploying complex Java applications. You don\u2019t have to teach Heroku how to run your Spring Boot, Quarkus, or Micronaut app, and in some cases you don\u2019t have to teach these frameworks how to interact with Heroku services either. In many cases, even a Procfile becomes optional! Let\u2019s take a closer look at how the Java Buildpack supports these popular development frameworks.\n\nThe Maven or Gradle buildpack recognizes your Spring Boot project by inspecting your build definition, for example your pom.xml file. It automatically packages your app into an executable JAR, and configures the environment to run it using the embedded web server. It also helps out with Spring specific environment variables, ensuring your Spring Boot app behaves as expected when working with databases. Database connections are automatically configured using SPRING_ (such as SPRING_DATASOURCE_URL ), so Spring automatically detects your use of the Heroku Postgres add-on. This is also true for our Heroku Key Value Store add-on, whereby the SPRING_REDIS_URL environment variable is automatically set. In many cases, a Procfile isn\u2019t necessary since the buildpack can determine the main JAR file automatically and adds a default process for your application such as: web: java -Dserver.port=$PORT $JAVA_OPTS -jar $jarFile .\n\nWe recently added support for Quarkus, known for its focus on developer joy. The Java (Maven) or Java (Gradle) buildpacks recognize your Quarkus project by inspecting your build definition. You can omit the usual Procfile and Heroku will default to Quarkus\u2019 runner JAR automatically: java -Dquarkus.http.port=$PORT $JAVA_OPTS -jar build/quarkus-app/quarkus-run.jar .\n\nMicronaut, another framework designed for speed and efficiency, also benefits from the Java Buildpack\u2019s intelligent automation. Just like with Spring Boot and Quarkus, database connections via DATABASE_URL and JDBC_DATABASE_URL and other environment-specific settings are handled automatically. You can omit the usual Procfile and Heroku will default to this automatically: java -Dmicronaut.server.port=$PORT $JAVA_OPTS -jar build/libs/*.jar .\n\nHeroku\u2019s Language Runtime Metrics provide JVM metrics for your application, displayed in the Heroku Dashboard. This feature complements our existing system-level metrics by offering insights specific to your application\u2019s execution, such as memory usage and garbage collection. These more granular metrics offer a clearer picture of your code\u2019s behavior.\n\nHeroku automatically configures your application to collect these metrics via a light-weight JVM agent. No configuration necessary.\n\nApart from offering excellent support for building Java applications, Heroku offers support for additional JVM languages in Scala and Clojure. The buildpacks for those languages offer a similar suite of features backed by the sbt and Leiningen build tools.\n\nLooking through our Heroku customer stories we can see that our customers are enjoying our Java support, building engagement apps, helping with cloud adoption and driving growth by leveraging Heroku\u2019s ability to elastically scale compute intensive workloads.\n\neCommerce Site & business platform : Improve user or employee engagement, and retention.\n\nCustomer Story: Goodshuffle Pro\n\n: Improve user or employee engagement, and retention. Customer Story: Goodshuffle Pro Cloud Adoption : Replatforming legacy back-end services.\n\nCustomer Story: Dovetail\n\n: Replatforming legacy back-end services. Customer Story: Dovetail Engines & APIs: Project customer growth.\n\nCustomer Story: PensionBee\n\nYes, and in fact, with any language supported by Heroku, it\u2019s possible to extend your Flow, Apex, and Agentforce experiences with code, frameworks, and tools you\u2019re familiar with from the Java ecosystem. Even if you haven\u2019t used Java before, you\u2019ll find its syntax similar to that of Apex. Check out our latest Heroku Eventing and AppLink pilot samples written in Java to find out more!\n\nHeroku\u2019s Java buildpacks are powerful tools that significantly simplify deploying JVM applications. By automating the build process, injecting framework-specific configurations, and handling runtime setup, it lets developers focus on writing code, not managing framework configuration. Here are some useful articles the Heroku DevCenter site:\n\nTo submit feedback on your favorite JVM language, framework, or packaging tool, please connect with us via the Heroku GitHub roadmap. We welcome your ideas and suggestions.", "label": 0}
{"title": "Day 1 \u2013 UNK World Leaders Camp", "url": "https://lifeofpablo.com/blog/day-1-unk-world-leaders-camp", "content": "Day 1 \u2013 UNK World Leaders Camp\n\nThis post was written in English (en_US).\n\nYesterday was a great first day. I checked in at UNK around 11am to put my stuff and away. Then I met my awesome roommate Collin. Him and i got to stick together. We are the only guys in the camp. Next we introduced ourselves to the ladies. They are some nice people. The laugh a lot. Like nonstop. Lol.\n\nWe went to go grab some lunch at the diner. The food was great. Im going to love this school. I like the ice cream the best .\n\nOur first activity was about culture. We were asked about describing our culture. Its was quite the open question. It wws alson hard at the same time. It shocked me that many of the participants didn\u2019t know where there culture was from or any inform about it. It was a learning experience for all of us.\n\nThen we did another activity comparing different cultures in different countries or groups of people. it was interesting seeing the differences.\n\nAfter all this we had dinner. Didn\u2019t meat much. I had the option to go to the swimming pools or go do something else. So Colin and i decided to do manly things like excersise. The girls went to the pool instead. I ran a out two miles. While running, Colin worked on his football workout. He is going to be a kicker for the UNK football team. Which is pretty awesome.\n\nI then after we played apples to apples. It was quite intense. Then it got crazy funny. The girls were just losing it with nonstop laughing. It was fun. Hey! We had pizza!\n\nOverall I\u2019m enjoying this camp. I\u2019m making some great friends and being around people who have great interests as I do.", "label": 1}
{"title": "Streamlining LLM Inference at the Edge with TFLite", "url": "https://developers.googleblog.com/en/streamlining-llm-inference-at-the-edge-with-tflite/", "content": "Optimizing Time to First Token and Peak Memory Usage with a Smarter Cache for XNNPack\n\nXNNPack is the default TensorFlow Lite CPU inference engine for all models. It delivers game changing speedups across mobile, desktop, and Web platforms. One of the optimizations employed in XNNPack is repacking the static weights of the Convolution, Depthwise Convolution, Transposed Convolution, and Fully Connected operators into an internal layout optimized for inference computations. During inference, the repacked weights are accessed in a sequential pattern that is friendly to the processors\u2019 pipelines. The inference latency reduction comes at a cost: repacking essentially creates an extra copy of the weights inside XNNPack. Previous efforts have been made to reduce that cost by adding an in-memorycache to XNNPack. This cache allows sharing the packed weights between independent TFLite interpreters that would run the same model independently. TFLite XNNPack delegate implementation has been improved to address some of the shortcomings of the existing cache.\n\n1. The cache lives in anonymous memory, which incurs swapping to disk in case of memory pressure, leading to poor performance. 2. It requires repacking the initial weights every time a process is started. 3. Because repacking reads the original TFLite weights and writes to a new buffer, this leads to a high peak memory usage during the packing. 4. It requires tedious steps and careful lifecycle management to properly enable caching through XNNPack delegate. 5. It doesn\u2019t allow sharing the weights across processes.\n\n.\n\nThe New XNNPack Cache Provider Interface XNNPack has been updated and provides an interface that lets you implement a weight cache provider. A weight cache provider behaves as a dictionary that XNNPack will fill and query in order to access packed buffers. Here are its main functions. look_up looks up a packed buffer key and returns a unique identifier (or a special identifier reserved for NotFound) that may be later used to retrieve the buffer address. reserve_space reserves a buffer that may be used to store information of a given size. That buffer then needs to be committed using look_up_or_insert . look_up_or_insert checks if a buffer matching the given key exists in the cache provider. If not, the given data is committed to the cache provider. This function also returns the identifier that may be used to retrieve the buffer address. offset_to_addr returns the buffer address from the identifier returned by look_up and look_up_or_insert . The interactions between XNNPack and the weight cache provider are illustrated in the following diagram.\n\n.\n\nLoading the Cache From Disk with MMAP in the TFLite Delegate The TFLite Delegate now uses this new interface and has its own weight cache provider. This provider is capable of saving and loading the packed weights directly to / from disk. TFLite has been leveraging flatbuffer and file-backed memory mapping for a long time. We are filling the gap here by leveraging the same technique, for the following advantages.\n\nIt eliminates the repacking overhead. Persisting packed weights on disk bypasses the costly repacking process each time a model is loaded. This translates to a significant reduction in both startup latency and peak memory usage. Even for the initial building, this offers packed data deduplication and further improves packing performance by avoiding repacking the same data again.\n\nIt improves memory management. mmap leverages the operating system's virtual memory management allowing it to optimize overall system memory usage and performance. In our case, this is especially advantageous for random access bulky read-only file access, like a neural network\u2019s operation\u2019s constant weights for instance. With packed data stored on disk, the XNNPack cache no longer relies on anonymous memory which can be prone to performance issues under memory pressure. Instead, it leverages the operating system's virtual memory management for smoother operation. By eliminating the need to copy data between the file system and memory, mmap significantly reduces overhead and speeds up access times. You can find more information about file mappings and memory usage directly from mmap\u2019s man page and other interesting reads.\n\nIt allows cross-process collaboration. mmap -based file loading opens the door for seamless weight sharing between multiple processes as each process\u2019 virtual address space maps to the same physical memory pages. This not only reduces the overall memory footprint as multiple processes share the same memory but also accelerates model loading across the board.\n\n.\n\nIt simplifies the user facing API. Instead of requiring the user to setup and manage the cache object throughout the application lifetime, they can simply provide a path to the cache file.\n\nstd::unique_ptr<tflite::Interpreter> interpreter; // Setup the options for the XNNPack delegate. TfLiteXNNPackDelegateOptions xnnpack_options = TfLiteXNNPackDelegateOptionsDefault(); xnnpack_options.weight_cache_file_path = \"/tmp/cache_file.xnn_cache\"; // Create and apply the XNNPack delegate to a TFLite interpreter. // Static weights will be packed and written into weights_cache on the first run. // They will be automatically loaded for all other runs. TfLiteDelegate* delegate = TfLiteXNNPackDelegateCreate(&xnnpack_options); interpreter->ModifyGraphWithDelegate(delegate); C++ Copied\n\nMaintaining Cache Integrity To guarantee accurate and efficient inference, it's crucial to invalidate the XNNPack cache under specific conditions: Model Evolution: if your model's weights or structure change, the cached data becomes outdated and must be invalidated. This means removing the file at the provided cache path. XNNPack Upgrades: updates to XNNPack's internal packing algorithm may result in incompatible cached weights, requiring the cache to be recomputed. Fortunately XNNPack is capable of detecting this and will replace the existing cache automatically. In essence, any modification that could impact the way weights are packed or utilized by XNNPack should trigger a cache invalidation.\n\nBenchmarks The session initialisation is dominated by the weight packing. For LLMs several subgraphs are reusing the same weights. Building the cache is faster because the deduplication functionality avoids packing those same weights multiple times. For more standard models, like stable diffusion, there is no deduplication and the slightly higher initialisation time is due to saving the cache to disk. Reloading the cache (from the 2nd run on) brings the initialisation down to a fraction of the previous time in all the cases. The session initialisation improvement naturally affects the time to the first token for LLMs, roughly dividing it by 2 in the benchmarks. The memory gains brought by the cache implementation can also be seen. The peak Resident Set Size is lowered for LLMs thanks to the deduplication. For other models that don\u2019t benefit from the deduplication, there is no change. Reloading the cache brings the peak RSS even further down because the TFLite original models aren\u2019t read anymore and therefore never get pulled into memory.\n\nGemma 2B on a Pixel 8 Pro\n\n.\n\nPhi2 on a Pixel 8 Pro\n\n.\n\nStable Diffusion on a Pixel 8 Pro\n\n.", "label": 0}
{"title": "Lessons learned: Using a cybersecurity vendor to check for malicious links", "url": "https://dropbox.tech/security/changing-how-we-identify-malicious-urls-in-shared-documents", "content": "Dropbox employs numerous industry-standard measures to prevent our services from being used for malicious purposes. This includes working with trusted third-party vendors to help us identify viruses, malware, and phishing attempts. One of these trusted vendors* previously helped us identify malicious URLs embedded within documents shared using Dropbox. However, we recently discovered that the URLs we submitted were made visible to our vendor\u2019s other paid subscribers and partners. As soon as we became aware of the situation, we immediately stopped submitting URLs to the vendor and worked with them to successfully remove the URLs from their database. To be clear: no files were ever submitted. Our investigation found 0.5% of registered Dropbox users and 10% of registered DocSend users were affected. We have no evidence that these URLs were ever exploited by malicious actors.\n\nWhat happened\n\nOn February 28, 2023, based on a report submitted to our bug bounty program, we became aware that URLs originating from Dropbox and DocSend were present in a database used to check for potential malware by the vendor\u2019s paid subscribers and partners. In response, we immediately stopped submitting URLs and began to investigate. We soon found that, due to an implementation error on our part, URLs\u2014and only the URLs\u2014embedded within a document shared using Dropbox or uploaded to DocSend were visible to the vendor\u2019s paid subscribers and partners. Neither the document itself, or any other information within it, were ever submitted.\n\nIn addition, any access controls on the embedded URLs\u2014such as password protection, authentication measures, or other restrictions\u2014remain intact. Out of an abundance of caution, we worked with our vendor to successfully remove the URLs from their database.\n\nOur tools enable collaboration\u2014but unfortunately, malicious actors often try to use the same tools to trick Dropbox customers and the community into downloading malicious content or redirecting them to malicious sites to steal their data. To help keep everyone safe online, we have safeguards in place when people use Dropbox to share documents that contain embedded URLs. Checking URLs for malware and phishing is a standard practice across the industry, and using this vendor to check whether URLs in shared Dropbox documents are safe was one of our techniques.\n\nWhat we\u2019re doing next", "label": 0}
{"title": "AI transformations for sustainability", "url": "https://blogs.microsoft.com/on-the-issues/2025/01/16/ai-transformations-for-sustainability/", "content": "Today, Microsoft published a new paper, Accelerating Sustainability with AI: Innovations for a Better Future. You can read the foreword below and explore the paper in its entirety.\n\nThroughout history, societal transformations have been driven by the emergence of general-purpose technologies that reshaped entire economies, industries, and ways of life.\n\nThe steam engine, the printing press, electricity, and the internet have each marked pivotal social and economic shifts, leading to lasting changes in how we live and work. Today, AI stands as the latest\u2014and potentially most powerful\u2014general-purpose technology, offering an unprecedented opportunity to drive the societal transformations we urgently need to achieve the world\u2019s sustainability goals.\n\nIn 2023, we published Accelerating Sustainability with AI: A Playbook, in which we highlighted that AI has three game-changing capabilities that make it an essential tool for accelerating sustainability. AI can enhance our ability to predict and optimize complex systems, accelerate the development and deployment of sustainable solutions, and empower the workforce to learn and achieve more\u2014equipping society with the means to drive sustainability progress at a speed and scale previously beyond reach.\n\nOver the last year, we have seen the potential of AI for sustainability in action, empowering the world with new tools for tackling the climate crisis and sustainability challenges more broadly. For example, earlier this year, Microsoft collaborated with Pacific Northwest National Laboratory to use AI in discovering a new battery material requiring less lithium\u2014a breakthrough achieved in weeks rather than the years that traditional research and development would have required. Reducing lithium dependence is crucial to decarbonization, as global demand for lithium is projected to outpace supply, potentially limiting the growth of the energy storage systems needed for the shift to electrification and renewable energy.\n\nAI\u2019s transformative capabilities extend far beyond sustainability, the world has an opportunity to harness AI to enhance both productivity and prosperity. By enabling smarter resource use, optimizing systems for efficiency, and fostering innovations in carbon-free energy and conservation, the AI economy also has the potential to advance both economic growth and environmental stewardship.\n\n\n\nAt Microsoft, we believe the world needs AI that is broadly accessible and trustworthy. This includes addressing the sustainability challenges associated with this technology. The five plays outlined in our AI and sustainability playbook reflect the targeted actions needed to unlock the full potential of AI for accelerating sustainability progress globally.\n\nAcross our sustainability work, we regularly assess our progress and adjust our strategies for greater impact. One lesson from this last year is that minimizing the sustainability impact of AI operations requires more than minimizing resource use in datacenter operations; it also requires supporting the communities where datacenters are located and expanding access to zero-carbon electricity. Global electricity demand is growing rapidly, at an estimated average annual rate of 3\u20134%. While AI currently consumes less than 0.3% of global electricity demand\u2014and, according to the International Energy Agency (IEA), is expected to remain a small portion in the decade ahead\u2014rapid growth in certain regions can strain local grids.\n\nIn light of these realities, we have updated the third play of our playbook to include enhancing access to carbon-free energy on electricity grids and supporting local communities where we operate datacenters. In support of these expanded goals, we are expanding our effort to build and operate digital infrastructure that addresses societal challenges and creates benefits for communities.\n\nThis report highlights Microsoft\u2019s innovations and actions to advance each of the five plays. Examples of our efforts across the five plays include:\n\nPlay 1: Invest in AI for sustainability\n\nMicrosoft is investing in building AI tools, such as MatterGen and MatterSim, which enable researchers to design and test materials with tenfold greater accuracy and significantly faster performance, while also predicting global weather and atmospheric processes with increased accuracy and at speeds up to 5,000 times greater than current forecasting systems. We are also building AI-enabled tools to empower stakeholders to more effectively and efficiently manage agriculture and water resources and to expedite the licensing process for carbon-free electricity.\n\nPlay 2: Develop digital and data infrastructure for the inclusive use of AI for sustainability\n\nWe are creating tools to fill critical data gaps, which can enhance AI models for better measuring and predicting complex systems such as biodiversity and climate. For instance, SPARROW captures images and acoustic recordings to gather data on biodiversity and ecosystem health in remote areas. Additionally, we are partnering with G42 on a $1 billion digital ecosystem initiative in Kenya.\n\nPlay 3: Minimize resource use, expand access to carbon-free electricity, and support local communities\n\nMicrosoft is innovating datacenter development with low-carbon materials like cross-laminated timber. Through an agreement with Brookfield, we aim to add 10.5 gigawatts (GW) of renewable energy to the grid.\n\nPlay 4: Advance AI policy principles and governance for sustainability\n\nWe advocated for policies that accelerate grid decarbonization, including Federal Energy Regulatory Commission (FERC) transmission rules and provisions in the Inflation Reduction Act in the United States. In addition, we continue to advance AI governance within Microsoft and globally.\n\nPlay 5: Build workforce capacity to use AI for sustainability\n\nMicrosoft Philanthropies\u2019 Skills for Social Impact program trained over 14 million people in digital and AI skills to support a workforce ready to deploy AI for sustainability. As the window for achieving global sustainability goals narrows, the urgency for action intensifies. The world needs every tool at its disposal, and the potential of AI to accelerate sustainability is already being realized. Sustainability is not a journey that can be taken alone, and unlocking the full potential of AI for climate progress requires continued partnerships to combine expertise, technology, and innovation. As we continue to explore the ways AI can advance sustainability, we invite others to join us in this journey.\n\nRead the full report at https://aka.ms/AcceleratingSustainabilitywithAI2025\n\nTags: AI, Environmental Sustainability, Innovation, Responsible AI, sustainability, Workforce", "label": 0}
{"title": "A new level unlocked", "url": "https://blogs.microsoft.com/blog/2025/02/19/a-new-level-unlocked/", "content": "Muse, the first World and Human Action Model, could facilitate interdisciplinary collaboration, for example, when exploring gameplay ideas.\n\nToday Microsoft released Muse, a first-of-its-kind generative AI model that we are applying to gaming. But it\u2019s so much more than that. What we\u2019re sharing today is a huge step forward for gameplay ideation. And what\u2019s even more exciting is what this breakthrough represents in our journey of building and using generative AI, and what industries, developers and creators of all interests will be enabled to do next.\n\nThe impressive abilities we first witnessed with ChatGPT and GPT-4 to learn human language are now being matched by AI\u2019s abilities to learn the mechanics of how things work, in effect developing a practical understanding of interactions in the world. As a computer scientist, this ability to understand and model a 3D world is something I and many other great researchers have pursued for over 10 years and, personally, I was not sure that it could be made possible with such speed and quality.\n\nIn the case of Muse, just from observing human gameplay, this model develops a deep understanding of the environment, including its dynamics and how it evolves over time in response to actions. This unlocks the ability to rapidly iterate, remix and create in video games so developers can eventually create immersive environments and unleash their full creativity.\n\nBeyond gaming, I\u2019m excited by the potential of this capability to enable AI assistants that understand and help visualize things, from reconfiguring the kitchen in your home to redesigning a retail space to building a digital twin of a factory floor to test and explore different scenarios. All these things are just now becoming possible with AI. From the perspective of computer science research, it\u2019s pretty amazing, and the future applications of this are likely to be transformative for creators.\n\n\u2014\n\nAt Microsoft, we have a long history of collaboration between research and engineering. Today, as we release Muse, we are also announcing Azure AI Foundry Labs, where the AI community can explore the latest from Microsoft Research. Azure AI Foundry Labs will help accelerate the transition from research to solutions, bringing new ideas to the broader community to help shape the future of AI. Learn more.\n\nTags: AI, Azure AI Foundry Labs, ChatGPT, GPT-4", "label": 0}
{"title": "How engineers can use one-on-ones with their manager to accelerate career growth", "url": "https://github.blog/developer-skills/career-growth/how-engineers-can-use-one-on-ones-with-their-manager-to-accelerate-career-growth/", "content": "One-on-one meetings with your manager are one of the most valuable tools you have for career growth, problem-solving, and unlocking new opportunities. So if you\u2019re only using them to provide status updates, you\u2019re leaving a lot on the table.\n\nI didn\u2019t fully realize this potential until I mentioned in a one-on-one that I was interested in mentorship and growing my leadership skills. Not long after, I was asked to co-lead a project with an intern to build an internal tool that helped surface enterprise configuration details. This gave me the opportunity to take technical ownership on a project while mentoring someone in a real-world context\u2014both of which pushed me outside my comfort zone in the best way. That experience made it clear: When used intentionally, one-on-ones can open doors you didn\u2019t even know were there.\n\nMany engineers treat one-on-ones as a low-stakes standup: reporting work, mentioning blockers, and getting general feedback. While that can be useful, it barely scratches the surface of what these meetings can accomplish. Instead, think of them as a system design review for your role\u2014a time to debug challenges, optimize your workflow, and align on long-term career goals.\n\nReframing your perception of what a one-on-one can accomplish\n\nA well-structured one-on-one meeting with your manager isn\u2019t just a check-in, it\u2019s an opportunity to shape your work environment and career trajectory. You wouldn\u2019t build a system without evaluating its constraints, dependencies, and long-term maintainability. Why approach your career any differently?\n\nStart by shifting your mindset: These meetings are not status updates. Your manager already sees your pull requests, sprint velocity, and planning docs. Instead, use this time to highlight what matters\u2014what you\u2019ve shipped, the value it\u2019s delivered, and where the friction is.\n\nYou can also use this space to validate decisions and gather context. If you\u2019re weighing different paths forward, don\u2019t just ask for approval\u2014frame the conversation in terms of trade-offs:\n\n\u201cHere are the pros and cons of refactoring this service now versus later. How does this align with our broader business goals?\u201d\n\nTreat your manager like a decision-making API: Feed in the relevant signals, surface what\u2019s unclear, and work together on an informed response.\n\nUse one-on-ones for career versioning (even before you\u2019re \u201cready\u201d)\n\nOne-on-one meetings are a great time to discuss your long-term career growth\u2014even if you\u2019re not actively seeking a promotion. Instead of waiting until promotion season, start having these conversations early to build clarity, direction, and momentum over time.\n\nIf you\u2019re more than a year away from seeking a promotion, start talking to your manager about: Where am I already meeting expectations? Where should I focus on strengthening my skills?\n\nIf you\u2019re approaching the next level or considering going up for promotion soon, try focusing the conversation on: What kind of work would demonstrate readiness for the next level? Are there specific opportunities I can take on to grow my scope or visibility?\n\n\n\nBy treating growth as an iterative process rather than an all-or-nothing milestone, you can continuously improve and course-correct based on early feedback.\n\nA useful framework for structuring these discussions is the Three Circles of Impact:\n\nIndividual Contributions \u2013 The direct value of your work. Collaboration \u2013 How you work with and support others across the team. Enabling Others \u2013 Mentorship, knowledge sharing, or improving systems and tooling for your peers.\n\nIf you\u2019re not sure how to show impact across all three, your one-on-one is a great place to explore it. The key is surfacing your goals early so your manager can help guide you toward the kinds of work that will stretch your skills and broaden your influence.\n\nThe more you shape your contributions around these areas, the clearer your readiness for growth becomes\u2014and the easier it is for your manager to advocate on your behalf.\n\nYour manager can\u2019t debug what they don\u2019t see\n\nManagers don\u2019t have full visibility into your day-to-day experience, so one-on-ones are the right time to highlight persistent blockers and unclear expectations.\n\nFor instance, I once brought up a latency issue I was chasing down. The endpoint\u2019s performance was slightly above our service level objective (SLO) target, and I had already spent a good chunk of time optimizing it. But in that conversation, my manager offered a different lens:\n\n\u201cAre we optimizing for the right thing? We control the SLO. If the extra latency is due to how the system is designed (and if users aren\u2019t impacted) maybe the right move is to revisit the threshold instead of squeezing more performance out of it.\u201d\n\nThat single conversation saved me hours and helped me reframe the problem entirely. Sometimes, the fix isn\u2019t in your code\u2014it\u2019s in how you\u2019re measuring success.\n\nMake your one-on-ones work for you\n\nYour one-on-ones will become far more effective\u2014and lead to real growth\u2014when you treat them as time to think strategically, not just check in. Reframing these meetings around your goals, your environment, and your long-term development puts you in a much stronger position to advocate for yourself and your work.\n\nStart thinking about your career progression earlier than feels natural. Come prepared. Bring in what\u2019s going well, what\u2019s stuck, and where you want to grow. And remember: your manager can\u2019t fix what they don\u2019t know about, and they can\u2019t support your goals if you never share them.\n\nIf this shift feels unfamiliar, you\u2019re not alone. The Engineer\u2019s Survival Guide helped me reframe my thinking around one-on-ones.\n\nHere are a few ideas that stuck with me:\n\nYour manager isn\u2019t a mind reader.\n\nYou can\u2019t expect guidance if you don\u2019t come with a direction.\n\nYour growth is a shared effort, but it starts with you.\n\nThe earlier you see one-on-ones as a tool for impact and growth, the more value you\u2019ll get from them.\n\nTags:", "label": 0}
{"title": "The secret life of DNS packets: investigating complex networks", "url": "https://stripe.com/blog/secret-life-of-dns", "content": "DNS is a critical piece of infrastructure used to facilitate communication across networks. It\u2019s often described as a phonebook: in its most basic form, DNS provides a way to look up a host\u2019s address by an easy-to-remember name. For example, looking up the domain name stripe.com will direct clients to the IP address 53.187.159.182, where one of Stripe\u2019s servers is located. Before any communication can take place, one of the first things a host must do is query a DNS server for the address of the destination host. Since these lookups are a prerequisite for communication, maintaining a reliable DNS service is extremely important. DNS issues can quickly lead to crippling, widespread outages, and you could find yourself in a real bind.\n\nIt\u2019s important to establish good observability practices for these systems so when things go wrong, you can clearly understand how they\u2019re failing and act quickly to minimize any impact. Well-instrumented systems provide visibility into how they operate; establishing a monitoring system and gathering robust metrics are both essential to effectively respond to incidents. This is critical for post-incident analysis when you\u2019re trying to understand the root cause and prevent recurrences in the future.\n\nIn this post, I\u2019ll describe how we monitor our DNS systems and how we used an array of tools to investigate and fix an unexpected spike in DNS errors that we encountered recently.\n\nDNS infrastructure at Stripe\n\nAt Stripe, we operate a cluster of DNS servers running Unbound, a popular open-source DNS resolver that can recursively resolve DNS queries and cache the results. These resolvers are configured to forward DNS queries to different upstream destinations based on the domain in the request. Queries that are used for service discovery are forwarded to our Consul cluster. Queries for domains we configure in Route 53 and any other domains on the public Internet are forwarded to our cluster\u2019s VPC resolver, which is a DNS resolver that AWS provides as part of their VPC offering. We also run resolvers locally on every host, which provides an additional layer of caching.\n\nUnbound runs locally on every host as well as on the DNS servers.\n\nUnbound exposes an extensive set of statistics that we collect and feed into our metrics pipeline. This provides us with visibility into metrics like how many queries are being served, the types of queries, and cache hit ratios.\n\nWe recently observed that for several minutes every hour, the cluster\u2019s DNS servers were returning SERVFAIL responses for a small percentage of internal requests. SERVFAIL is a generic response that DNS servers return when an error occurs, but it doesn\u2019t tell us much about what caused the error.\n\nWithout much to go on initially, we found another clue in the request list depth metric. (You can think of this as Unbound\u2019s internal todo list, where it keeps track of all the DNS requests it needs to resolve.)\n\nAn increase in this metric indicates that Unbound is unable to process messages in a timely fashion, which may be caused by an increase in load. However, the metrics didn\u2019t show a significant increase in the number of DNS queries, and resource consumption didn\u2019t appear to be hitting any limits. Since Unbound resolves queries by contacting external nameservers, another explanation could be that these upstream servers were taking longer to respond.\n\nTracking down the source\n\nWe followed this lead by logging into one of the DNS servers and inspecting Unbound\u2019s request list.\n\n$ unbound-control dump_requestlist thread #0 # type cl name seconds module status 0 A IN s3.amazonaws.com. - iterator wait for 10.0.0.2 1 PTR IN 3.8.25.104.in-addr.arpa. - iterator wait for 10.0.0.2 2 PTR IN 5.101.25.104.in-addr.arpa. - iterator wait for 10.0.0.2 3 PTR IN 5.156.25.104.in-addr.arpa. - iterator wait for 10.0.0.2 4 PTR IN 123.71.25.104.in-addr.arpa. - iterator wait for 10.0.0.2 5 PTR IN 212.28.24.104.in-addr.arpa. - iterator wait for 10.0.0.2 6 PTR IN 18.81.25.104.in-addr.arpa. - iterator wait for 10.0.0.2 7 PTR IN 103.78.24.104.in-addr.arpa. - iterator wait for 10.0.0.2 8 PTR IN 22.43.25.104.in-addr.arpa. - iterator wait for 10.0.0.2 9 PTR IN 24.17.25.104.in-addr.arpa. - iterator wait for 10.0.0.2 10 PTR IN 21.100.25.104.in-addr.arpa. - iterator wait for 10.0.0.2 ... ... ~\n\nThis confirmed that requests were accumulating in the request list. We noticed some interesting details: most of the entries in the list corresponded to reverse DNS lookups (PTR records) and they were all waiting for a response from 10.0.0.2, which is the IP address of the VPC resolver.\n\nWe then used tcpdump to capture the DNS traffic on one of the servers to get a better sense of what was happening and try to identify any patterns. We wanted to make sure we captured the traffic during one of these spikes, so we configured tcpdump to write data to files over a period of time. We split the files across 60 second collection intervals to keep file sizes small, which made it easier to work with them.\n\n# Capture all traffic on port 53 (DNS traffic) # Write data to files in 60 second intervals for 30 minutes # and format the filenames with the current time $ tcpdump -n -tt -i any -W 30 -G 60 -w '%FT%T.pcap' port 53 ~\n\nThe packet captures revealed that during the hourly spike, 90% of requests made to the VPC resolver were reverse DNS queries for IPs in the 104.16.0.0/12 CIDR range. The vast majority of these queries failed with a SERVFAIL response. We used dig to query the VPC resolver with a few of these addresses and confirmed that it took longer to receive responses.\n\nBy looking at the source IPs of clients making the reverse DNS queries, we noticed they were all coming from hosts in our Hadoop cluster. We maintain a database of when Hadoop jobs start and finish, so we were able to correlate these times to the hourly spikes. We finally narrowed down the source of the traffic to one job that analyzes network activity logs and performs a reverse DNS lookup on the IP addresses found in those logs.\n\nOne more surprising detail we discovered in the tcpdump data was that the VPC resolver was not sending back responses to many of the queries. During one of the 60-second collection periods the DNS server sent 257,430 packets to the VPC resolver. The VPC resolver replied back with only 61,385 packets, which averages to 1,023 packets per second. We realized we may be hitting the AWS limit for how much traffic can be sent to a VPC resolver, which is 1,024 packets per second per interface. Our next step was to establish better visibility in our cluster to validate our hypothesis.\n\nCounting packets\n\nAWS exposes its VPC resolver through a static IP address relative to the base IP of the VPC, plus two (for example, if the base IP is 10.0.0.0, then the VPC resolver will be at 10.0.0.2). We need to track the number of packets sent per second to this IP address. One tool that can help us here is iptables, since it keeps track of the number of packets matched by a rule.\n\nWe created a rule that matches traffic headed to the VPC resolver IP address and added it to the OUTPUT chain, which is a set of iptables rules that are applied to all packets sent from the host. We configured the rule to jump to a new chain called VPC_RESOLVER and added an empty rule to that chain. Since our hosts could contain other rules in the OUTPUT chain, we added this rule to isolate matches and make it a little easier to parse the output.\n\nListing the rules, we see the number of packets sent to the VPC resolver in the output:\n\n$ iptables -L -v -n -x Chain OUTPUT (policy ACCEPT 41023 packets, 2569001 bytes) pkts bytes target prot opt in out source destination 41023 2569001 VPC_RESOLVER all -- * * 0.0.0.0/0 10.0.0.2 Chain VPC_RESOLVER (1 references) pkts bytes target prot opt in out source destination 41023 2569001 all -- * * 0.0.0.0/0 0.0.0.0/0 ~\n\nWith this, we wrote a simple service that reads the statistics from the VPC_RESOLVER chain and reports this value through our metrics pipeline.\n\nwhile : do PACKET_COUNT=$(iptables -L VPC_RESOLVER 1 -x -n -v | awk '{ print $1 }') report-metric $PACKET_COUNT \"vpc_resolver.packet_count\" sleep 1 done ~\n\nOnce we started collecting this metric, we could see that the hourly spikes in SERVFAIL responses lined up with periods where the servers were sending too much traffic to the VPC resolver.\n\nTraffic amplification\n\nThe data we saw from iptables (the number of packets per second sent to the VPC resolver) indicated a significant increase in traffic to the VPC resolvers during these periods, and we wanted to better understand what was happening. Taking a closer look at the shape of the traffic coming into the DNS servers from the Hadoop job, we noticed the clients were sending the request five times for every failed reverse lookup. Since the reverse lookups were taking so long or being dropped at the server, the local caching resolver on each host was timing out and continually retrying the requests. On top of this, the DNS servers were also retrying requests, leading to request volume amplifying by an average of 7x.\n\nSpreading the load\n\nOne thing to remember is that the VPC resolver limit is imposed per network interface. Instead of performing the reverse lookups solely on our DNS servers, we could instead distribute the load and have each host contact the VPC resolver independently. With Unbound running on each host we can easily control this behavior. Unbound allows you to specify different forwarding rules per DNS zone. Reverse queries use the special domain in-addr.arpa , so configuring this behavior was a matter of adding a rule that forwards requests for this zone to the VPC resolver.\n\nWe knew that reverse lookups for private addresses stored in Route 53 would likely return faster than reverse lookups for public IPs that required communication with an external nameserver. So we decided to create two forwarding configurations, one for resolving private addresses (the 10.in-addr.arpa. zone) and one for all other reverse queries (the .in-addr.arpa. zone). Both rules were configured to send requests to the VPC resolver. Unbound calculates retry timeouts based on a smoothed average of historical round trip times to upstream servers and maintains separate calculations per forwarding rule. Even if two rules share the same upstream destination the retry timeouts are computed independently, which helps isolate the impact of inconsistent query performance on timeout calculations.\n\nAfter applying the forwarding configuration change to the local Unbound resolvers on the Hadoop nodes we saw that the hourly load spike to the VPC resolvers had gone away, eliminating the surge of SERVFAILS we were seeing:\n\nAdding the VPC resolver packet rate metric gives us a more complete picture of what\u2019s going on in our DNS infrastructure. It alerts us if we approach any resource limits and points us in the right direction when systems are unhealthy. Some other improvements we\u2019re considering include collecting a rolling tcpdump of DNS traffic and periodically logging the output of some of Unbound\u2019s debugging commands, such as the contents of the request list.\n\nVisibility into complex systems\n\nWhen operating such a critical piece of infrastructure like DNS, it\u2019s crucial to understand the health of the various components of the system. The metrics and command line tools that Unbound provides gives us great visibility into one of the core components of our DNS systems. As we saw in this scenario, these types of investigations often uncover areas where monitoring can be improved, and it\u2019s important to address these gaps to better prepare for incident response. Gathering data from multiple sources allows you to see what\u2019s going on in the system from different angles, which can help you narrow in on the root cause during an investigation. This information will also identify if the remediations you put in place have the intended effect. As these systems grow to handle more scale and increase in complexity, how you monitor them must also evolve to understand how different components interact with each other and build confidence that your systems are operating effectively.", "label": 0}
{"title": "Meeting at a Diner", "url": "https://lifeofpablo.com/blog/meeting-at-a-diner", "content": "Meeting at a Diner\n\nPablo & James at a Diner\n\nThis post was written in English (en_US).\n\nJames & Pablo at Pinecrest Diner in San Francisco.\n\nI love meeting with friends at diners. This is especially true when I return to Nebraska. When I land, I'll go with the person who picked me up at the diner after midnight. Going to the diner is a fun adventure when it's snowing, raining, sunny, hot, chilly, or raining cats and dogs!\n\nDiners are also places to meet people who have only met through a virtual medium such as video calls and online groups. Rarely do you meet people in real life (IRL) you get to converse weekly online.\n\nA few weeks ago in July, I met with James G. We both actively participate in the IndieWeb. (You should check it out!) James is a coffee lover and also enjoys writing. He was telling me about a coffee mojito.\n\nJames mentioned a while ago that he was visiting San Francisco for work during Homebrew Website Club. I was very excited to get a coffee and walk around the city. San Francisco is one of my favorite cities\n\nYou're wondering, \"So you two are getting coffee and talking? What 'bout the diner?\" Originally, getting coffee and chatting was the original plan. Then, I remembered that James loves diners and this got my brain juices flowing! WE SHOULD GO TO THE DINER! This is not a drill! I love a good side trip to spice up the adventure. Food and coffee unite people.\n\nI love food and getting caffeinated\n\nDiners are lovely! James loves diners. I've never heard of someone loving diners as much as he does! It makes me very happy!\n\nWe decided to eat at a fine establishment called the Pinecrest Diner. This place is an institution in San Francisco! Imagine a traditional diner restaurant with yummy breakfast food but in SF! The food is so heavenly people wait in line. This place truly hits the spot. They make the food in front of you due to the openness of the space.\n\nWhile eating, we caught up and we discussed various topics of interest. We discussed our visions for our respective personal websites, our current projects, Mozilla, and things to do in San Francisco such as checking out Noisebridge. I also recommended The Museum of Art and Digital Entertainment in Oakland. I enjoyed the discussions and ideas bounced off each other.\n\nThank you James for hanging out! I had a blast!\n\nI hope to see James when I'm on his side of the water. Should I see you at IndieWebCamp San Francisco?", "label": 1}
{"title": "Realities, perceived and presented, and their relationship to Truth \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/12/realities-perceived-and-presented-and-their-relationship-to-truth/", "content": "I\u2019ve been thinking a lot lately about how knowledge doesn\u2019t equal understanding\u2014there are things you can know to be intellectually true, but that doesn\u2019t mean your body or heart has caught up yet. \u2026 You have to prove to your nervous system, choice by choice, that this new way of thinking is actually safe\u2026 \u2014 Dipa Halder\n\n+\n\nCrises are moments of intense unpredictability. A crisis is, by definition, a rupture with the recent past, making any projection of the likely future a fool\u2019s game. Screens promise to relieve the discomfort this brings about: the unending stream, the servile virtual assistant, the fully optimized day\u2014all are designed to make things predictable, to resolve the anxiety of not knowing what could happen next. \u2014 Mandy Brown\n\n+\n\nwe long for the corporeal in a landscape governed by hypotheticals that may or may not evolve into literals. \u2014 Kyle Raymond Fitzpatrick\n\n+\n\nAmerica has no memory. Pioneering homesteaders, Gold Rushers, post-war company men with green lawns, Steve Jobs, ALL OUR GREAT MYTHOLOGIES, REALLY, NECESSITATE A BREAK FROM THE PAST. Deleting what came before you in favor of \u201cthe miraculous now\u201d\u2026 America wants to be a vacuum of time and space, a suspended hologram of perfection, unmolested by history\u2014 and unadulterated by nature. \u2014 Steven Phillips-Horst\n\n+\n\nThe complaint that real humans \u201ctalk in circles for hours\u201d and engage with \u201csubjective nuance\u201d reflects something all too real about our social skills in these times. As a society, we\u2019ve been socially deskilled in countless ways. The performance-based dynamics of social media, the biases and bigotries that divide us, and the loss of third spaces have eroded our capacity to relate to one another. \u2014 Kelly Hayes\n\n+\n\n[The phone] knows the parts of ourselves which we don\u2019t know, and cannot really understand the parts of ourselves which we are actually most interested in. \u2014 Aidan Walker\n\n+\n\nBeijing Watches Fake Sunrise On Video Screen Amid Smog Emergency (2014)\n\n+\n\n\u201cTasted a little tear gas\u2014 tasted like fascism\u201d\n\n\u201cTasted a little tear gas\u2014 tasted like fascism\u201d pic.twitter.com/o7SLl4ZWnV \u2014 Acyn (@Acyn) June 8, 2025\n\nThinking about how narrative can coopt \u2014 and eventually shape \u2014 reality in relation to the LA protests this week, and how they are being portrayed as chaos by the government and media when on-the-ground reports are that it was pretty chill till the cops showed up and started shooting people. It seems to make little difference what is true when Republicans can paint every act as a step towards the reality they desire. The troops did in fact get deployed to LA, whether or not it was legal or justified, law enforcement did in fact shoot people\u2026 that those they shot were journalists!!! and protesters practicing their First Amendment rights means nothing when MAGA can handwave them off as paid insurgents (\ud83e\udd28 the critical thinking skills have been shut down here). The occupying administration is synthesizing a false \u201ctruth\u201d from segments of reality that do not represent Truth.\n\nThe saying goes that \u201ccinema is truth 24 times per second,\u201d but the medium of film bears little relation to truth in itself, as the scene excludes whatever is cropped from the frame, the temporal context of events preceding and following, and all sensory elements besides sight and sound. Fact-checking the quote, I discovered it has a second part: \u201cand every cut is a lie.\u201d I finally read the Walter Benjamin essay \ud83e\uddbe and he writes that film inherently separates viewer from truth because of the camera intermediary: \u201cHis creation is by no means all of a piece; it is composed of many separate performances.\u201d Semblances of reality may be montaged together to create a seamless imitation of some desired reality.\n\nSee also:\n\nCreating our own unrealities\n\nCeding the work of interpretation\n\nComplementary: what is real?\n\nOn conformity and control", "label": 1}
{"title": "Singapore engineering hub", "url": "https://stripe.com/blog/singapore-eng-office", "content": "Stripe launched in Singapore in 2016. Since then, we\u2019ve seen strong traction, and are proud to work with some of the fastest-growing companies in the region, including Grab, Mobike, and Carousell. Today, we\u2019re increasing our investment: we\u2019re very excited to announce that Singapore is joining Seattle, Dublin, and San Francisco to become Stripe\u2019s fourth global engineering hub.\n\nIn the years ahead, we plan to hire hundreds of people to help us expand our infrastructure, build completely new products, and ensure that Stripe\u2019s product suite works just as well in Southeast Asia as it does in Europe and North America.\n\nMore than 200 million Southeast Asians will come online in the next two years, and the region\u2019s vibrant internet economy\u2014growing to more than $200 billion by 2025\u2014comprises more than 7,000 high-growth startups. We think that this is just the beginning, and that the region will see vast innovation in the years ahead.\n\nOur Singapore hub will include all of the core Stripe functions: product and engineering teams based here will work on expanding the geographic footprint of Stripe\u2019s existing global payments and treasury network, help build completely new products, and further develop the underlying infrastructure powering Stripe. Today, Stripe works with more than 1 million companies in more than 100 countries around the world. Our Singapore team will help those companies be significantly more successful in the world\u2019s fastest-growing internet region.\n\nWe\u2019re proud to be building this presence in Singapore. The country has long understood the importance of global integration and technological advancement, leading the world in everything from containerization to commodities. Singapore embodies a positive-sum economic mindset that serves as an example for the world.\n\nIf you\u2019re interested in working with us to help establish Stripe\u2019s engineering hub in Singapore, please get in touch!", "label": 0}
{"title": "Shepherd: How Stripe adapted Chronon to scale ML feature development", "url": "https://stripe.com/blog/shepherd-how-stripe-adapted-chronon-to-scale-ml-feature-development", "content": "Machine learning (ML) is a foundation underlying nearly every facet of Stripe\u2019s global operations, optimizing everything from backend processing to user interfaces. Applications of ML at Stripe add hundreds of millions of dollars to the internet economy each year, benefiting millions of businesses and customers worldwide. Developing and deploying ML models is a complex multistage process, and one of the hardest steps is feature engineering.\n\nBefore a feature\u2014an input to an ML model\u2014can be deployed into production, it typically goes through multiple iterations of ideation, prototyping, and evaluation. This is particularly challenging at Stripe\u2019s scale, where features have to be identified among hundreds of terabytes of raw data. As an engineer on the ML Features team, my goal is to build infrastructure and tooling to streamline ML feature development. The ideal platform needs to power ML feature development across huge datasets while meeting strict latency and freshness requirements.\n\nIn 2022 we began a partnership with Airbnb to adapt and implement its platform, Chronon, as the foundation for Shepherd\u2014our next-generation ML feature engineering platform\u2014with a view to open sourcing it. We\u2019ve already used it to build a new production model for fraud detection with over 200 features, and so far the Shepherd-enabled model has outperformed our previous model, blocking tens of millions of dollars of additional fraud per year. While our work building Shepherd was specific to Stripe, we are generalizing the approach by contributing optimizations and new functionality to Chronon that anyone can use.\n\nThis blog discusses the technical details of how we built Shepherd and how we are expanding the capabilities of Chronon to meet Stripe\u2019s scale.\n\nML feature engineering at Stripe scale\n\nIn a previous blog post, we described how ML powers Stripe Radar, which allows good charges through while blocking bad ones. Fraud detection is adversarial, and Stripe needs to improve models quickly\u2014fraud patterns change as malicious actors evolve their attacks, and Stripe needs to move even faster.\n\nML feature development is the process of defining the inputs (features) that a model uses to make its predictions. For example, a feature for a fraud prediction model could be the total number of charges processed by a business on Stripe over the last seven days.\n\nTo identify and deploy new features that would address rapidly changing fraud trends, we needed a feature engineering platform that would allow us to move quickly through the lifecycle of feature development.\n\nEffectively deploying ML models in the Stripe environment also requires meeting strict latency and feature freshness requirements.\n\nLatency: A measure of the time required to retrieve features during model inference. This is important because models such as the ones powering Radar are also used in processing payments, and the time required to retrieve features directly impacts the overall payment API latency\u2014lower latency means faster payments and a better overall customer experience for businesses.\n\nA measure of the time required to retrieve features during model inference. This is important because models such as the ones powering Radar are also used in processing payments, and the time required to retrieve features directly impacts the overall payment API latency\u2014lower latency means faster payments and a better overall customer experience for businesses. Feature freshness: A measure of the time required to update the value of features. This is important because Stripe needs to react quickly to changes in fraud patterns. For example, if there is an unusual spike in transactions for one business, feature values must quickly be updated to reflect the pattern so models can incorporate the new information in their predictions for other businesses.\n\nThere are trade-offs between latency and feature freshness. For example, we can improve latency at the expense of freshness by performing more precomputation when new data arrives, while we can prioritize freshness over latency by performing more of the feature computation during serving. Stripe\u2019s strict requirements for both low latency and feature freshness across the billions of transactions we process create a unique set of constraints on our feature platform.\n\nShepherd: Stripe\u2019s next-generation ML feature platform\n\nAs Stripe grew, so did our ambitions for applying ML to hard problems. To accelerate our feature engineering work, we evaluated several options, including revamping our existing platform, building from scratch, and implementing proprietary or open-source options. One particularly appealing option was an invitation we received from Airbnb to become early external adopters of Chronon, which Airbnb had developed to power its ML use cases.\n\nAirbnb wanted to integrate the platform with an external partner prior to open sourcing, and Chronon met all of our requirements: an intuitive Python- and SQL-based API, efficient windowed aggregations, support for online and offline computation of features, and built-in consistency monitoring. At the same time, we couldn\u2019t just use it off-the-shelf. We knew we would need to adapt Chronon to Stripe\u2019s unique scale, where training data can include thousands of features and billions of rows. It was going to be a significant engineering challenge, but we were confident that it was a strong foundational building block.\n\nAdapting Chronon\n\nChronon supports batch and streaming features in both online and offline contexts. To be able to use Chronon as the foundation for Shepherd, we needed to make sure the offline, online, and streaming components could all meet Stripe\u2019s scale.\n\nML engineers use Chronon to define their features with a Python- and SQL-based API, and Chronon provides the offline, streaming, and online components to compute and serve the features. Integrating with Chronon involves setting up each of these components and providing an implementation for the key-value (KV) store used to store feature data for serving. When integrating with Chronon, we needed to make sure each of the components could meet our feature freshness and latency requirements.\n\nKV store implementation\n\nThe KV store is responsible for storing data required to serve features. Offline jobs compute and write historical feature data to the store, and streaming jobs write feature updates. To cost-efficiently scale our KV store, we split it into two implementations: a lower-cost store optimized for bulk uploads that is write-once and read-many, and a higher-cost distributed memcache-based store that is optimized for write-many and read-many. With this dual KV store implementation, we lowered the cost of storing and serving data while still meeting our latency and feature freshness requirements.\n\nStreaming jobs\n\nChronon streaming jobs consume event streams and write the events to the KV store. The events can be thought of as updates to features. The default Chronon implementation writes events into the KV store with no preaggregation. Storing individual events into the KV store would not allow us to meet our latency requirements for features with a large number of events. We needed to choose a streaming platform that could achieve low latency updates and allow us to implement a more scalable write pattern.\n\nWe chose Flink as the streaming platform because of its low latency stateful processing. Since the Chronon API is a combination of Python and Spark SQL, maintaining consistency between offline and online computation meant we needed a way to run Spark SQL expressions in Flink. Fortunately, the Spark SQL expressions used in Chronon\u2019s feature definitions only require maps and filters. These are narrow transformations\u2014with no shuffling of data\u2014and can be applied to individual rows.\n\nWe implemented support for Spark SQL expressions applied to Flink rows. With Flink now powering our feature updates, we achieved p99 feature freshness of 150ms.\n\nUntiled Architecture\n\nTiled Architecture\n\nFlink-based streaming architecture allowed us to meet our feature freshness requirements; that left latency targets. To achieve those, we needed to modify how Chronon stores events in the KV store. When events are stored individually, computing features requires retrieving events for the feature and aggregating them together. If there are a large number of events for the feature, this is time-consuming and increases latency.\n\nRather than store individual events, we decided to maintain the state of preaggregated feature values in the Flink app, and periodically flush those values out to the KV store. We call each of these preaggregated values a \u201ctile.\u201d With tiling, computing a feature only requires retrieving and aggregating the tiles for the feature rather than all the individual events. For features with a large number of events, this is a much smaller amount of data and significantly decreases latency. We contributed both the Flink and tiling implementations back to Chronon, along with documentation on how to get started with them.\n\nMeeting Stripe\u2019s offline requirements\n\nThe Chronon offline algorithm produces both offline training data for models and batch-only use cases. Offline jobs are also required to compute historical data used for serving GroupBys. The offline jobs are configured using the same Python- and Spark SQL-based API as the online jobs, allowing developers to define their features once and compute both online and offline features.\n\nStripe\u2019s scale for offline jobs is larger than previous use cases of Chronon, just as it was for streaming and online components. Although the offline algorithm is designed to be robust, with support for handling skewed data, we needed to verify that it would scale to the size of Stripe\u2019s training sets. As a first step to integrating with Chronon\u2019s offline jobs, we performed benchmarks of training dataset generation and found the algorithm to be scalable with predictable tuning knobs.\n\nAfter verifying its scalability, we needed to integrate Chronon\u2019s offline jobs with Stripe\u2019s data orchestration system. We built a custom integration for scheduling and running jobs that worked with our highly customized Airflow setup. We designed the integration so users only need to mark their GroupBys as online or set an offline schedule in their Join definitions, after which the required offline jobs are automatically scheduled.\n\nWe also needed to integrate Chronon with Stripe\u2019s data warehouse. Chronon assumes data sources are all partitioned Hive tables. Not all data sources at Stripe meet these requirements. For example, many of the data sources required for batch features are unpartitioned snapshot tables.\n\nWe built support into our Chronon integration for defining features with a wider variety of data sources, and for writing features to Stripe\u2019s data warehouse using customized Iceberg writers. Fully integrating with our data warehouse provides feature engineers the flexibility to define features using any data source, and to consume features in downstream batch jobs for use cases including model training and batch scoring.\n\nOur implementation for more flexible data source support was Stripe-specific, but we plan to generalize the approach and contribute it to Chronon.\n\nBuilding a SEPA fraud model on Shepherd\n\nOur first use case for Shepherd was a partnership with our Local Payment Methods (LPM) team to create an updated ML model for detecting SEPA fraud. SEPA, which stands for Single Euro Payments Area, enables people and businesses to make cashless euro payments\u2014via credit transfer and direct debit\u2014anywhere in the European Union. The LPM team initially planned on combining new Shepherd-created features with existing features from our legacy feature platform, but found development on Shepherd so easy that they created all new features and launched a Shepherd-only model.\n\nOur new SEPA fraud model consists of over 200 features, including a combination of batch-only and streaming features. As we built the model, we also developed support for modeling delay in the offline training data so we could accurately represent the delay of batch data in training data to avoid training-serving skew\u2014when the feature values that a model is trained on are not reflective of the feature values used to make predictions.\n\nAs part of the new SEPA fraud model, we also built monitoring and alerting for Shepherd\u2014including integrating with Chronon\u2019s online offline consistency monitoring. As we mentioned at the start of this post, the new model blocks tens of millions of dollars of additional fraud a year.\n\nSupporting the Chronon community\n\nAs a co-maintainer of Chronon with Airbnb, we\u2019re excited to grow and support this open-source community while continuing to expand the capabilities of the project. We also designed the new Chronon logo, a subtle nod to the fabric of time.\n\nOver the coming months, we\u2019ll contribute new functionality and additional optimizations to Chronon, and we\u2019ll share more details about how teams at Stripe are adopting Shepherd.\n\nTo get started with Chronon, check out the GitHub repository, read the documentation at Chronon.ai, and drop into our community Discord channel.\n\nAnd if you\u2019re interested in building ML infrastructure at Stripe\u2014or developing ML features for Stripe products\u2014consider joining our engineering team.", "label": 0}
{"title": "Generative AI and the Business Borg aesthetic \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/02/generative-ai-and-the-business-borg-aesthetic/", "content": "This feels like a sister piece to Ed Zitron\u2019s essay Era of the Business Idiots and Mandy Brown\u2019s essay Toolmen. Fair warning, this is a 5000 word post; I\u2019ve been working on this for weeks, pulling together what I\u2019ve learned about generative AI and culture over the past two years, so I hope it is worth your time \ud83d\ude04 Bonus: it doubles as a playlist \ud83c\udfb6\n\n\u201c\u2018Real power\u2019 is achieved when a technology \u2018[leaves] mythology and [enters] banality,'\u201d Marion Fourcade and Kieran Healy quote Vincent Mosco in The Ordinal Society. We\u2019ve had the mythology stage \u2014 the world tour with grandiose prophecies of imminent AGI \u2014 but now the race to normalize generative AI* is on: tech corporations are attempting to inure people to generative AI, an expression of the Business Borg aesthetic that currently carries a negative stigma outside of tech.\n\n*(My rule of thumb: if something is described as AI, it\u2019s probably predatory and/or bullshit; if it\u2019s described as machine learning, it probably does something useful. Not always true but a helpful predictor.)\n\nIn general, people like what we recognize better than what we don\u2019t \u2014 we prefer cultural works we can categorize to the unfamiliar and undefinable \u2014 and we are facing an inescapable shock-and-awe barrage of genAI graphics across the web to inundate our synapses with uncanny synthetic renderings.\n\nCurrently, generative AI is shunned by many artists and writers, the traditional arbiters of good taste and culture, because it has been developed through the theft of their labor. But tech CEOs stand to make (even bigger) fortunes if they can convince people that genAI doesn\u2019t signify bad taste, or make it seem like an irrevocable fact of life, like spam emails and text scammers. It\u2019s being deployed upon us with the same lockstep corporate solidarity that forced us to pay fees for checked luggage on flights (younger folks, before 2008 your bag used to be included with your ticket! Stowing your carry-on wasn\u2019t a competitive sport back in the day.).\n\nThe aesthetics of generative AI\n\n\u201cI have become momentarily obsessed with scrolling down the homepage of the MetaAI tool and seeing the infinite feed of what people have been asking of The Machine. The outputs are horribly banal, but the requests are a weird window into THE (NORMIE) HUMAN SOUL AND ITS DESIRES: www.meta.ai\u201d\n\n\u2014 Matt Muir, May 6, 2025 at 8:51 AM\n\n(via)\n\nBy its nature, generative AI produces the most likely image that meets the brief, which devolves to insipid Pictionary-style visual communication: chonky = cat, kebab = food = French chef\u2019s hat. These graphics are iconographic pablum, the uninspired result of gee-whiz curiosity about a new \u201ctool\u201d (toy)(trap) in an environment that discourages personal taste and cultural literacy.\n\nOriginally, I wrote up why I find these particular graphics tacky and visually uninteresting, but realized that ultimately, what they look like doesn\u2019t matter \u2014 it\u2019s the beliefs beneath the appearance that matter. As corporate models are trained further, Generative AI will probably continue to get better, rendering more attractive and/or plausible outputs, but it won\u2019t matter to me how good it gets because I reject the values it represents.\n\n(NB: I don\u2019t want you to feel bad about yourself if you use generative AI, because there are myriad reasons to have experimented with it, including being forced to for your employment; I want you to recognize what it symbolizes when you enthusiastically use this technology today, and make an informed choice about whether those are values you wish to signal to others.)\n\n\u201cWhat is wrong with a counterfeit is not what it is like, but how it was made.\u201d\n\n\u2014 Harry G. Frankfurt, On Bullshit\n\nAesthetics are looks that signal values\n\nWitching Hour by Ladytron\n\nAn aesthetic is an expression of taste for shared values, commonly communicated through a distinct style. We think of aesthetics as surface appearance only, but the formation of an aesthetic\u2019s conventions reflects the why and the how underneath the what. Just as the medium of a thing carries a message, so does its aesthetic. Aesthetics are visually and verbally encoded value systems.\n\nIdeology is a value system, independent of appearances. Aesthetics are the appearance of an ideology, which grow from its values. Subcultures form around ideologies, with members signaling their participation through aesthetics. \u201cGood taste\u201d is aesthetics that express those values.\n\nThough it\u2019s now often reduced to a visual style, the Arts and Crafts movement of the late 1800s and early 1900s was cross-disciplinary and united instead by an ethos \u2014 namely, the nobility of craftsmanship*:\n\n\u201cFor it is not the material, but the absence of human labor, which makes the thing worthless, and a piece of terracotta, or plaster of paris, which has been wrought by the human hand, is worth all the stone and Carrara cut by machinery.\u201d \u2014 John Ruskin, The Lamp of Truth from The Seven Lamps of Architecture\n\nAccording to the Arts and Crafts aesthetic, what is made should signal how it is made \u2014 the aesthetic\u2019s value system weights how something is made to be as important, if not more, as what is made. Surface appearance is borne of the decisions this taste for craft produces.\n\n*(People might instead think of this William Morris quote as the quintessential perspective of Arts and Crafts \u2014 \u201cHave nothing in your house that you do not know to be useful, or believe to be beautiful.\u201d \u2014 and I suspect there\u2019s a reason we\u2019ve been taught to recall a philosophy centered on material possession instead of labor.)\n\nLooking at a Kelmscott Press book (William Morris\u2019 printing company) reveals the printer\u2019s respect for handicraft: they designed custom drop letters and frames, included original illustrations by fine artists like Edward Burne-Jones, used original type modeled after typefaces used by printers like Aldus Manutius in the early days of the printing press, printed with a richer black ink than was standard at the time, and employed a heavy hand in letterpress printing so the design and type would be impressed into the page. Thoughtful, intentional ornamentation is embraced. The artifact itself is a thing to be appreciated as much as its contents; these are books that honor the integrity of all creative workers involved in their production.\n\nEmily Amick applies this analytical lens to reveal the tradwife aesthetic\u2019s underlying values:\n\nPrairie-core. Domestic bliss. Big sleeves. Bigger sourdough starters. And beneath it all, the subtle (or not-so-subtle) message: a woman\u2019s place is in the kitchen. But not because she wants to be there, because that\u2019s where God or her husband or some TikTok algorithm put her. The tradwife aesthetic promises comfort, but it delivers control. It\u2019s softness as a strategy. It\u2019s anti-feminism with a floral filter. It\u2019s nostalgia for a time when women were property, romanticized by influencers who want brand deals from butter. [\u2026] The tradwife says: give up your autonomy and someone else will take care of everything.\n\nWe adopt aesthetics based on aspirational values\n\nJessica Cullen writes that (emphasis mine): \u201cAesthetics aren\u2019t always about who we currently are but rather who we want to be.\u201d People adopt an aesthetic to say something about themselves to others. We intentionally adopt a particular subculture\u2019s aesthetics to convey our belonging and raise our status within the subculture. As Alec Leach puts it, \u201ca lot of the modern taste economy is actually the status economy.\u201d\n\nSometimes, we perceive only the surface level of an aesthetic, its appearance without the values \u2014 as Amick notes, the tradwife aesthetic \u201clooks so damn pretty and nourishing. And we are tired.\u201d \u2014 but whenever we adopt an aesthetic, we endorse (intentionally or not) the underlying values it represents. Amick continues dissecting the tradwife aesthetic and how it serves as conservative propaganda:\n\nWhat looks like innocent lifestyle content is actually part of an organized political movement designed to make patriarchy look cozy and appealing. Because politics is downstream from culture. The vibes that influence how we act and live.\n\nAesthetics matter more than ever because we act in accordance with our chosen aesthetics. As we get more and more of our \u201ccultural\u201d content on corporate silos, politics and purchases have subsumed a lot of cultural tastemaking. In this TikTok, Jamelle Bouie describes how politicians use aesthetic signaling to appeal to voters. Richard Sennett identifies that politicians come to embody \u201cintentions, desires, values, beliefs, tastes \u2013 an emphasis which has again the effect of divorcing power from responsibility.\u201d\n\nAnu Atluru argues that \u201cAesthetics are the modern units of cultural currency\u2014stores of value and instruments of power, capable of appreciating and being monetized at scale. Owning an aesthetic is owning influence.\u201d (emphasis mine)\n\nInterrogating the Business Borg aesthetic\n\nGlass Lux by Glass Lux\n\nThe Arts and Craft movement\u2019s respect for labor inspired stylistic choices that highlighted craftwork as well as the decisions in what goods to produce and how. So how does the Business Borg aesthetic reveal its values?\n\nTo define the Business Borg aesthetic, I\u2019m looking at:\n\nthe values I see expressed through generative media,\n\nI see expressed through generative media, the actions of the corporations behind generative AI,\n\nof the corporations behind generative AI, who buys into generative AI, and\n\nbuys into generative AI, and what else they like that reflects the same underlying values.\n\nWhat genAI is better at than a human artist is being cheap and instant.\n\nThe Business Borg aesthetic uses technology to signal wealth and power. Generative AI is not the only visual expression of the Business Borg aesthetic, just its most recognizable. The aesthetic is also signified visually by CGI-heavy blockbuster franchises, NFT art, and the Cybertruck; and in text by LinkedIn corporate thirst traps, X braggadocio, SEO word vomit, and generated \u201canswers\u201d to search results.\n\nAs political bedpartners, there is overlap between the Business Borg aesthetic and the MAGA aesthetic, but they\u2019re distinct viewpoints. Both share dominance as a core value, decry empathy, center patriarchy, and admire performance \u2014 but MAGA also signals Judeo-Christian morality, traditional beauty standards / traditional gender roles, hyper masculinity / violence, and nationalism. MAGA borrows aesthetics from golden pasts, like Neo-Classical architecture, tradwives, and, as Kate Wagner brands it, Regional Car Dealership Rococo; Business Borg prefer the more modern tones of cyberpunk, solarpunk, and minimalism. Business Borg are regulatory libertarians who envision themselves as the rightful leaders of society, Kings of techno-city-states; MAGA are Christian nationalists who want to use the power of the state to impose their beliefs on others.\n\nElon is a Business Borg at heart but wielded a chainsaw to appeal to the more violent MAGA aesthetic. Zuckerberg is a Business Borg but got a MAGA makeover with masculine stubble and bling.\n\nWhy am I naming this after the Borg? Like Star Trek\u2019s Borg, this is an aesthetic rooted in extractive consumption, assimilationist dominance, neo-colonial expansionism, self-righteous conviction, reductionist thinking, and proclamations of inevitability. It idolizes technology, often inspired by older science-fiction, and draws on cyberpunk aesthetics. The Silicon Valley Collective values groupthink and believes themselves superior to \u201cthe other.\u201d\n\nWho embraces this aesthetic\n\nNot all users of Generative AI embrace the Business Borg aesthetic. I think a lot of people are experimenting with generative AI out of neophilic curiosity, productivity imperatives, nihilistic determinism, and corporate fiat. Aspiring billionaires adopt the Business Borg aesthetic to signal their belonging in the cohort of the techno-rich.\n\nGenAI evangelists seem to be the same type of person who was into passive income and supplements fifteen years ago, then Soylent and SEO ten years ago, then NFTs and macro diets five years ago, now genAI and X blue checks.\n\nThe Business Borg aesthetic combines tech-centered neophilia, a hustle mindset, an obsession with optimization, evangelical fervor, and fake-it-till-you-make-it showmanship.\n\nThe Elon fanclub are Business Borg. Ed Zitron\u2019s Business Idiot shares a lot of characteristics with Business Borg (and may even be the same group, but I think feels a little different?).\n\nThe subculture emphasizes high profile demonstrations of \u201cwinning\u201c \u2014 using a $10k NFT as their X profile pic, bragging about SEO heists ripping off a competitor, generosity stunts a la Mr Beast, rubbing it in Miyazaki\u2019s face that there will be thousands of shitty knockoffs vaguely reminiscent of his work across the web.\n\nBusiness Borg signal their busyness \u2014 and importance \u2014 by broadcasting how little they sleep, how much they work, and how little they read. The only fiction they like is (old) sci-fi because they read it as non-fiction, not fiction \u2014 a source of \u201cinspiration\u201d stripped of context and commentary. Using GenAI signals their adoption of cutting edge technology, the synthetic smoothness emphasizing its nonhuman origin. They care a lot about IQ, a supposedly impartial measure of intelligence that rewards their backgrounds and thinking styles, and idolize \u201cgenius.\u201d They\u2019re not actually neurodivergent, but play on stereotypes of autistic savants to cover for their pathological greed and lack of empathy.\n\nTheir visual and linguistic taste is mid because taste is not valued in the Business Borg aesthetic. In fact, there\u2019s a certain pride in prompting things without having any skill, an almost gleeful snub to the perceived cultural gatekeepers \u2014 artists and writers and other creative workers \u2014 whose opinions the Business Borg disrespect because they believe that authority derives from money, not knowledge. They believe artists have wasted their time learning skills and developing taste. Academics have wasted their time studying things when information is just a click away. Business Borg don\u2019t care about anything besides making money, and don\u2019t care much how they spend it because the point is to have it, and show off that they have it.\n\nInto The Water by Ritual Howls\n\nGenAI True Believers often resemble the CEOs at the head of tech companies: wealthy, male, and white. These are also the people who are least at threat from the widespread use of generative AI, which reinforces racial and gender stereotypes and purports neutrality while serving up right-wing biases and corporate and foreign propaganda. Audrey Watters points out:\n\n\u201cComputing (in general and ed-tech specifically) has long been the bastion of white male privilege; and while there had been efforts to change that \u2013 in pipelines and on panels and whatnot \u2013 AI is clearly a re-entrenchment of that power, explicitly so with the Trump Administration\u2019s dismantling of civil rights protections, echoed by the tech industry\u2019s dismantling of its own DEI initiatives.\u201d\n\nAnil Dash describes \u201cAI-first\u201d as this year\u2019s \u201cReturn to Office.\u201d Managers didn\u2019t care about in-office culture until it was made clear that workers could carry on just fine without them; managers care about AI only insomuch as it permits controlling \u2014 and firing \u2014 workers.\n\nWhat the Business Borg aesthetic represents is more important than its appearance; it represents the dominance of ordinal thinking and the ability of moneyed power to do as it wishes without regard to law or morality \u2014 in short, the hierarchical worldview that some people are better than others and that their preferences trump their lesser\u2019s needs.\n\n\u201cWe will add your biological and technological distinctiveness to our own. Your culture will adapt to service us. Resistance is futile.\u201d \u2014 the Borg\n\nValues driving the Business Borg aesthetic\n\nTeri Kanefield breaks down Leor Zmigrod\u2019s book on ideology, explaining that \u201cAll ideologies seek a utopia.\u201d The Business Borg utopia puts billionaires and their ilk high atop society, in control via the technology they own.\n\nCore values I see uniting the Business Borg aesthetic are:\n\nonly the output matters efficiency is king quantity over quality appearance trumps reality \u201cprogress\u201d cannot be stopped\n\nValue: Only the output matters\n\nGenerative AI is being marketed to businesses as a low-cost replacement for workers that cuts steps \u2014 and collaboration \u2014 out of the process. This is a box-checking culture; all that matters is that an email was sent, a presentation was created, the newspaper had a summer reading insert, no matter the books on it were imaginary.\n\nFoundational beliefs\n\nprocess does not add value and wastes time\n\nthe world is reducible to data , and every question has one objectively correct answer\n\n, and every question has communication and collaboration are a waste of time (\u201cemail jobs\u201d)\n\n(\u201cemail jobs\u201d) experience is irrelevant\n\nOutcome: Tech reduces the complex to input and output\n\nIn contrast with the Arts and Crafts movement, the Business Borg aesthetic actively conceals human labor and venerates the wisdom of the machine. Generative text and graphics simulate a performance of human-less \u2014 cost-less \u2014 automation. Generative answers encourage a reliance on the machine to synthesize on one\u2019s behalf \u2014 and it doesn\u2019t matter to search engines that the \u201canswers\u201d their AI has provided cite sources incorrectly.\n\nHumans are perceived as sources of inefficiency under the Business Borg ideology, because they must be compensated in accordance with their skills and how much time they spend working. Generating material is rooted in devaluing both skill and process. The invented summer reading list was the result of forcing a single contractor to prepare an impossible quantity of work; generating content was the only way for the poor bloke to produce the content on budget. No one reviewed it, because Business Borg only care that the product exists.\n\nEd Zitron describes the evolution of the Business Idiot, personified by middle managers who are completely dissociated from the product they\u2019re selling and explicitly do not do work (emphasis mine):\n\n[Business Idiots] see every part of our lives as a series of inputs and outputs. They boast about how many books they\u2019ve read rather than the content of said books, about how many hours they work (even though they never, ever work that many), about high level they are in a video game they clearly don\u2019t play, about the money they\u2019ve raised and the scale they\u2019ve raised it at, and about how expensive and fancy their kitchen gadgets are. Everything is dominance, acquisition, growth and possession over any lived experience, because their world is one where the journey doesn\u2019t matter, because their journeys are riddled with privilege and the persecution of others in the pursuit of success. These people don\u2019t want to automate work, they want to automate existence. They fantasize about hitting a button and something happening, because experiencing \u2014 living! \u2014 is beneath them, or at least your lives and your wants and your joy are.\n\nValue: Efficiency is king\n\nGenerative AI produces endless content for low cost. Corporations are using Generative AI as an excuse to lay off workers and intensify the jobs of those remaining.\n\nReady Aim Fire (Owl Vision Remix) [Single] by Blue Stahli\n\nFoundational beliefs\n\nthe more mechanized a process is, the more efficient it becomes because humans are naturally inefficient\n\nOutcome: GenAI performs \u201cefficiency\u201d\n\nGenerative AI need not actually reduce work or cost to represent efficiency when mechanization is always favored over people. The Business Borg aesthetic perceives automation as efficient \u2014 hence situations where workers are paid to simulate chatbots simulating human agents on customer service platforms, Microsoft devs handhold Copilot, and GM\u2019s Cruise \u201cautonomous\u201d taxis needed remote human intervention every 4-5 miles!!!\n\nEfficiency is a code word for shareholders, just like \u201ccost-cutting,\u201d who know that these phrases mean putting the boot on workers\u2019 necks for short-term profits. This efficiency aesthetic is used to justify outrageously profitable companies continuing to slash workers *cough Microsoft* It plays out as Hollywood demolishing the screenwriter profession to save a buck on writing rooms and self-cannibalizing the development of future acting talent by forcing extras to be body scanned so they can be reproduced by AI.\n\nJeremy comments on the use of genAI in coding, noting that it\u2019s justified by claims like \u201cworking code wins\u201d \u2014 as in, what it looks like under the hood and how it\u2019s constructed don\u2019t matter. I\u2019m not a coder, but I\u2019ve seen enough HTML produced by PageMaker and other CMSs to be skeptical of the quality of any generated code \u2014 is genAI producing the coding equivalent of tables for web layout? \ud83e\udd14 The Business Borg aesthetic accepts mediocrity without understanding; easy and fast is always best. Good enough is always good enough.\n\nValue: Quantity over quality\n\nGenerative AI is billed as a good-enough tool that will speed up production. Cory Doctorow writes of the managerial push to automate with AI:\n\nThe point of using automation to weaken labor isn\u2019t just cheaper products \u2013 it\u2019s cheaper, defective products, inflicted on the unsuspecting and defenseless public who are no longer protected by workers\u2019 professionalism and pride in their jobs.\n\nEnormous by LLgL TNDR\n\nFoundational beliefs\n\nprofit today trumps tomorrow\u2019s concerns \u2014 someone else will have to fix \u2018that problem\u2019\n\n\u2014 someone else will have to fix \u2018that problem\u2019 money is authority \u2014 individual rankings can be quantified by wealth\n\n\u2014 individual rankings can be quantified by wealth anything that cannot be quantified is not important\n\nOutcome: Algorithms produce culture-like media, not culture\n\nGenAI and corporate web algorithms are intended to absorb attention like black absorbs light; they are designed to maximize engagement at minimal cost. Spotify benefits from degradation of culture. Netflix is a chum machine, built to spew content that people watch in the background. Kyle Raymond Fitzpatrick laments that \u201cSo much of culture is edgeless and soft, intended for us to astroglide through it without any friction or doubt as we half-watch in 1.5x speed, to consume as if we really are incapable of critical thoughts, all to appeal to everyone and no one at the same time.\u201d Internet Age capitalism produces entertainment that is culture-like, writes Nicholas Carr:\n\n\u201cWhat\u2019s really being tested here is human taste. Will we accept a simulacrum of a work of art or craft as a satisfactory substitute for the real thing?\u201d\n\nSo long as people accept cheap low-quality cultural media, businesses have little incentive to pay for higher quality. Generative AI becomes an attack on culture because it drowns out human-made art and writing so it\u2019s impossible to find amidst the Great Social Media Garbage Patch. Aidan Walker describes all this as \u2018 slop capitalism \u2018: \u201can economic and cultural system in which the primary product is slop and the primary activity is the destruction of value rather than its creation.\u201d\n\nValue: Appearances trump reality\n\nGenerative AI produces plausible graphics that we interpret as real-adjacent and plausible combinations of text that we interpret as communication.\n\nFoundational Beliefs\n\nperformance of dominance builds and reinforces real power\n\nOutcome: GenAI supersedes reality with performance and symbolism\n\nThe Business Borg aesthetic celebrates audacious performances of infinite wealth and indefinite power: adopters and evangelists for genAI also embrace filling streets with \u201cself-driving\u201d cars over the protests of residents and first responders, raining space debris onto inhabited areas from slapdash rocket ships, paying a fortune for a banana taped to a wall as conceptual art and eating it, paying women to have their IVF babies so they can seem fertile without fucking. While the culture at large has shifted towards inconspicuous consumption, luxury that requires knowledge to see, Business Borg signal their wealth blatantly.\n\nA repeating theme of the Business Borg aesthetic is replacing reality with life-like hyperreality: the simulation of conversation and connection with chatbots, the renderings of war and disaster mimicking photojournalism to push political narratives, the \u201cresurrected\u201d extinct species, the green screen action sequences that don\u2019t track. A photograph of reality may seem less real than a generated image if it does not abide by our expectations. (See also: reading \u201chuman vibes\u201d into LLM responses)\n\nLook at the snoozefest kayfabe of modern MMA: it\u2019s as much about the smack talk at weigh-ins as the fights themselves, athletes winning by points for \u201ccontrolling the octagon\u201d instead of fighting to win by KO or submission \u2014 look no further than that embarrassment of a so-called fight between Mike Tyson and the YouTuber, who danced around an old man till he got tired so he could win by decision and say he\u2019d beaten a legend \ud83d\ude34\n\nEffective altruism performs charity that can never be disproven, despite its claims of data-driven decisions, because it pretends to think at such long-term scales that known, existing suffering pales in comparison to the imagined future suffering they claim to be protecting against. It cosplays rationalism. Wannabe Foundation shit.\n\nThe Business Borg aesthetic is expressed as cheap cruft disguised as something of substance: words that look like writing, but are not; images that look like art, but are not; fights that look fighting, but are not.\n\nAs Ed Zitron sums up, it\u2019s a \u201csymbolic economy:\u201d\n\nThe sweeping changes we\u2019ve seen, both in our economy and in our society, has led to an unprecedented, gilded age of bullshit where nothing matters, and things \u2014 things of actual substance \u2014 matter nothing.\n\nTraditionally, we have been wary of those wearing a mask, explains Dan Fox in Pretentiousness, suspecting they are something other than what they present themselves as \u2014 but authenticity has faded as a cultural value. Business Borg embrace performance as more true than reality; what someone wants to be is more important than what they are now. What a technology could be is more important than what it currently is.\n\nValue: \u201cProgress\u201d cannot be stopped\n\nThe rapacious assimilation of copyrighted material to train models, the dismissal of AI\u2019s environmental cost and induced demand, and PR campaign reorienting the conversation around AI to its possible future harms as a distraction from the harms happening today all build from the same foundation: theoretically, scaling towards the pursuit of AGI \u2014 but more likely securing the funding to inextricably embed AI into our society and infrastructure.\n\nFoundational Beliefs\n\ntechnology always represents progress\n\nthe ends justify the means\n\nOutcome: GenAI is full speed ahead, no matter what\n\nAdvocates of generative AI are pursuing a brute force, fear-mongering approach to deregulation and preventing regulation. They are defying legality in obtaining training data while declaring the technology a fait accompli: resistance is futile. Their web crawlers ignore robots.txt, breaking the common courtesy of the web. They dismiss complaints about bias, claiming that they can fix that in the future. (And what incentive will there be to ever go back and fix it, once we can\u2019t avoid using it?) They\u2019re hoping enough of us will get hooked on it \u2014 and government and corporations will integrate enough of it too deeply in their processes \u2014 to allow their legally-suspect models to be shut down.\n\nRyan Broderick observes:\n\nWhat the AI arms race has actually done is codified and automated all of the failures of the previous internet era. Extremism, misinformation, harassment, non-consensual sexual material, and scams \u2014 all the content that tech companies promised to fix, but never could at scale \u2014 are now trapped in some AI\u2019s black box of data.\n\nAGI, which has been \u201cthree to five years away\u201d for years, is a Bezzle. As Cory Doctorow quotes JK Gabraith, a Bezzle is \u201cthe magic interval when a confidence trickster knows he has the money he has appropriated but the victim does not yet understand that he has lost it.\u201d John Kay expands that \u201cThe joy of the bezzle is that two people \u2013 each ignorant of the other\u2019s existence and role \u2013 can enjoy the same wealth.\u201d\n\nScams and pyramid schemes are seductive to Business Borg, like Elizabeth Holmes and Sam Bankman-Fried, because the only things they value are money and power, not making things that actually work (see: Cybertrucks falling apart, SpaceX rockets exploding); the trick is not to get caught. The goal is to surf the edge of profit as long as possible.\n\nRight now the whole stock market is bloated by outrageous NVIDIA, Google, Microsoft and Meta valuations based on the potential of generative AI to create a new \u201cessential\u201d utility, a service that everyone will need to subscribe to, forever \u2014 and so many people have bought into the grift so hard they\u2019ll do anything they can to make it a success, or at least rake in the cash for as long as they can. Then when the bubble pops, the corporations will get bailed out on the taxpayer dollar, while we workers resign ourselves to working until we die since we\u2019ve privatized retirement \ud83d\ude43\n\nGenAI is being deployed to control\n\nArtists are under attack, culturally and economically, so it is only fair that they point out the quintessential thing that artists offer that generative AI cannot: taste. Professional (and amateur) artists have devoted a great deal of time to developing their taste. The oligarchs who run Silicon Valley are steeped in their own rightness and devalue anything that isn\u2019t their expertise; if they don\u2019t know about it, it must not be important. To the Business Borg, taste is not an essential component of production.\n\nMandy Brown describes how genAI is used to undercut expertise (emphasis mine):\n\nIt\u2019s instructive that one of the mechanisms for perpetuating this ideology are chattering bots that speak both fact and falsehood in the same servile and confident tone, their makers unconcerned with the difference. In fact, their makers seem entirely concerned with obviating that difference, with disappearing distinctions between knowledge and ignorance, without which truth becomes entirely a product of power. [\u2026] [I]f those in power cannot prove that a great many people are already inferior then they will bring that inferiority about by forcing them to use a tool that diminishes their intellectual and creative capacity.\n\nBusiness Borg like generative AI because it grants them cultural power that they have not been able to dominate on their own. They lack skill, so they devalue skill. They need content, so they make an infinite content machine and conscript users as unwitting factory workers to provide free labor. The relentless promotion of GenAI is an attempt by corporations to capture cultural value by siphoning off value from human-made aesthetics. Generative AI is billionaires punching down on artists and the working class.\n\n[\u2026] I think Miyazaki\u2019s style is still valuable But it is now, in a day, valuable in a different way. What was once valuable in the awareness of painstaking labor, beautiful stories, and coherent aesthetic across the previous two qualities. PLUS our reception to it. Is now valuable PURELY in our reception to, and reproduction of, the aesthetic. [\u2026] \u2014 Reggie James (@HipCityReg) March 27, 2025\n\nGenerative AI has intentionally been molded to attack artists and diminish cultural literacy. Aidan Walker argues (read this whole piece if you liked my post):\n\nAI doesn\u2019t have to be an antagonist to schools, work, and civil society \u2014 they\u2019ve just designed and trained it that way\u2026 There could be guardrails in place, they could pay the producers of their training data, they could give the people a say in how the models are made and deployed \u2014 we could do a thousand things differently than the way they\u2019re being done now.\n\nGenerative AI \u2014 both imagery and text \u2014 is inextricable from the corporate vision for its use: a world in which workers are powerless and worthless, replaced by \u201cfree\u201d generated material. Corporate GenAI cannot be separated from the purpose for its use or the billionaires and billionaire-wannabes who shill for it. The Business Borg aesthetic imbues a sheen of venality.\n\nFurther reading:\n\nGenAI is Our Polyester by W. David Marx\n\nEconomics & labor rights in AI skepticism by Henry from online\n\nYou don\u2019t hate AI; You hate\u2026 : a collection by Mita Williams\n\nDispatch from the Trenches of the Butlerian Jihad by ADH\n\nThe other way the [Butlerian Jihad] metaphor is proving apt is the deep-seated, almost spiritual nature of anti-AI sentiment. It\u2019s not just more Luddism. Many people \u2014 though hardly all, given the popularity of AI products \u2014 sense that there is something grotesque about these simulacra, the people who push them on us, this whole affair. That aversion to the technological profane holds even when various stated objections to AI are supposedly addressed or nitpicked to death.\n\nSee also:\n\nWe need solidarity across creative industries", "label": 1}
{"title": "Part 3: A Survey of Analytics Engineering Work at Netflix", "url": "https://netflixtechblog.com/part-3-a-survey-of-analytics-engineering-work-at-netflix-e67f0aa82183?source=collection_home---4------5-----------------------", "content": "Part 3: A Survey of Analytics Engineering Work at Netflix Netflix Technology Blog 9 min read \u00b7 Jan 6, 2025 -- 3 Listen Share\n\nThis article is the last in a multi-part series sharing a breadth of Analytics Engineering work at Netflix, recently presented as part of our annual internal Analytics Engineering conference. Need to catch up? Check out Part 1, which detailed how we\u2019re empowering Netflix to efficiently produce and effectively deliver high quality, actionable analytic insights across the company and Part 2, which stepped through a few exciting business applications for Analytics Engineering. This post will go into aspects of technical craft.\n\nDashboard Design Tips\n\nRina Chang, Susie Lu\n\nWhat is design, and why does it matter? Often people think design is about how things look, but design is actually about how things work. Everything is designed, because we\u2019re all making choices about how things work, but not everything is designed well. Good design doesn\u2019t waste time or mental energy; instead, it helps the user achieve their goals.\n\nWhen applying this to a dashboard application, the easiest way to use design effectively is to leverage existing patterns. (For example, people have learned that blue underlined text on a website means it\u2019s a clickable link.) So knowing the arsenal of available patterns and what they imply is useful when making the choice of when to use which pattern.\n\nFirst, to design a dashboard well, you need to understand your user.\n\nTalk to your users throughout the entire product lifecycle. Talk to them early and often, through whatever means you can.\n\nUnderstand their needs, ask why, then ask why again. Separate symptoms from problems from solutions.\n\nPrioritize and clarify \u2014 less is more! Distill what you can build that\u2019s differentiated and provides the most value to your user.\n\nHere is a framework for thinking about what your users are trying to achieve. Where do your users fall on these axes? Don\u2019t solve for multiple positions across these axes in a given view; if that exists, then create different views or potentially different dashboards.\n\nSecond, understanding your users\u2019 mental models will allow you to choose how to structure your app to match. A few questions to ask yourself when considering the information architecture of your app include:\n\nDo you have different user groups trying to accomplish different things? Split them into different apps or different views.\n\nWhat should go together on a single page? All the information needed for a single user type to accomplish their \u201cjob.\u201d If there are multiple jobs to be done, split each out onto its own page.\n\nWhat should go together within a single section on a page? All the information needed to answer a single question.\n\nDoes your dashboard feel too difficult to use? You probably have too much information! When in doubt, keep it simple. If needed, hide complexity under an \u201cAdvanced\u201d section.\n\nHere are some general guidelines for page layouts:\n\nChoose infinite scrolling vs. clicking through multiple pages depending on which option suits your users\u2019 expectations better\n\nLead with the most-used information first, above the fold\n\nCreate signposts that cue the user to where they are by labeling pages, sections, and links\n\nUse cards or borders to visually group related items together\n\nLeverage nesting to create well-understood \u201cscopes of control.\u201d Specifically, users expect a controller object to affect children either: Below it (if horizontal) or To the right of it (if vertical)\n\nThird, some tips and tricks can help you more easily tackle the unique design challenges that come with making interactive charts.\n\nTitles: Make sure filters are represented in the title or subtitle of the chart for easy scannability and screenshot-ability.\n\nTooltips: Core details should be on the page, while the context in the tooltip is for deeper information. Annotate multiple points when there are only a handful of lines.\n\nAnnotations: Provide annotations on charts to explain shifts in values so all users can access that context.\n\nColor: Limit the number of colors you use. Be consistent in how you use colors. Otherwise, colors lose meaning.\n\nOnboarding: Separate out onboarding to your dashboard from routine usage.\n\nFinally, it is important to note that these are general guidelines, but there is always room for interpretation and/or the use of good judgment to adapt them to suit your own product and use cases. At the end of the day, the most important thing is that a user can leverage the data insights provided by your dashboard to perform their work, and good design is a means to that end.\n\nLearnings from Deploying an Analytics API at Netflix\n\nDevin Carullo\n\nAt Netflix Studio, we operate at the intersection of art and science. Data is a tool that enhances decision-making, complementing the deep expertise and industry knowledge of our creative professionals.\n\nOne example is in production budgeting \u2014 namely, determining how much we should spend to produce a given show or movie. Although there was already a process for creating and comparing budgets for new productions against similar past projects, it was highly manual. We developed a tool that automatically selects and compares similar Netflix productions, flagging any anomalies for Production Finance to review.\n\nTo ensure success, it was essential that results be delivered in real-time and integrated seamlessly into existing tools. This required close collaboration among product teams, DSE, and front-end and back-end developers. We developed a GraphQL endpoint using Metaflow, integrating it into the existing budgeting product. This solution enabled data to be used more effectively for real-time decision-making.\n\nWe recently launched our MVP and continue to iterate on the product. Reflecting on our journey, the path to launch was complex and filled with unexpected challenges. As an analytics engineer accustomed to crafting quick solutions, I underestimated the effort required to deploy a production-grade analytics API.\n\nFig 1. My vague idea of how my API would work\n\nFig 2: Our actual solution\n\nWith hindsight, below are my key learnings.\n\nMeasure Impact and Necessity of Real-Time Results\n\nBefore implementing real-time analytics, assess whether real-time results are truly necessary for your use case. This can significantly impact the complexity and cost of your solution. Batch processing data may provide a similar impact and take significantly less time. It\u2019s easier to develop and maintain, and tends to be more familiar for analytics engineers, data scientists, and data engineers.\n\nAdditionally, if you are developing a proof of concept, the upfront investment may not be worth it. Scrappy solutions can often be the best choice for analytics work.\n\nExplore All Available Solutions\n\nAt Netflix, there were multiple established methods for creating an API, but none perfectly suited our specific use case. Metaflow, a tool developed at Netflix for data science projects, already supported REST APIs. However, this approach did not align with the preferred workflow of our engineering partners. Although they could integrate with REST endpoints, this solution presented inherent limitations. Large response sizes rendered the API/front-end integration unreliable, necessitating the addition of filter parameters to reduce the response size.\n\nAdditionally, the product we were integrating into was using GraphQL, and deviating from this established engineering approach was not ideal. Lastly, given our goal to overlay results throughout the product, GraphQL features, such as federation, proved to be particularly advantageous.\n\nAfter realizing there wasn\u2019t an existing solution at Netflix for deploying python endpoints with GraphQL, we worked with the Metaflow team to build this feature. This allowed us to continue developing via Metaflow and allowed our engineering partners to stay on their paved path.\n\nAlign on Performance Expectations\n\nA major challenge during development was managing API latency. Much of this could have been mitigated by aligning on performance expectations from the outset. Initially, we operated under our assumptions of what constituted an acceptable response time, which differed greatly from the actual needs of our users and our engineering partners.\n\nUnderstanding user expectations is key to designing an effective solution. Our methodology resulted in a full budget analysis taking, on average, 7 seconds. Users were willing to wait for an analysis when they modified a budget, but not every time they accessed one. To address this, we implemented caching using Metaflow, reducing the API response time to approximately 1 second for cached results. Additionally, we set up a nightly batch job to pre-cache results.\n\nWhile users were generally okay with waiting for analysis during changes, we had to be mindful of GraphQL\u2019s 30-second limit. This highlighted the importance of continuously monitoring the impact of changes on response times, leading us to our next key learning: rigorous testing.\n\nReal-Time Analysis Requires Rigorous Testing\n\nLoad Testing: We leveraged Locust to measure the response time of our endpoint and assess how the endpoint responded to reasonable and elevated loads. We were able to use FullStory, which was already being used in the product, to estimate expected calls per minute.\n\nFig 3. Locust allows us to simulate concurrent calls and measure response time\n\nUnit Tests & Integration Tests: Code testing is always a good idea, but it can often be overlooked in analytics. It is especially important when you are delivering live analysis to circumvent end users from being the first to see an error or incorrect information. We implemented unit testing and full integration tests, ensuring that our analysis would return correct results.\n\nThe Importance of Aligning Workflows and Collaboration\n\nThis project marked the first time our team collaborated directly with our engineering partners to integrate a DSE API into their product. Throughout the process, we discovered significant gaps in our understanding of each other\u2019s workflows. Assumptions about each other\u2019s knowledge and processes led to misunderstandings and delays.\n\nDeployment Paths: Our engineering partners followed a strict deployment path, whereas our approach on the DSE side was more flexible. We typically tested our work on feature branches using Metaflow projects and then pushed results to production. However, this lack of control led to issues, such as inadvertently deploying changes to production before the corresponding product updates were ready and difficulties in managing a test endpoint. Ultimately, we deferred to our engineering partners to establish a deployment path and collaborated with the Metaflow team and data engineers to implement it effectively.\n\nFig 4. Our current deployment path\n\nWork Planning: While the engineering team operated on sprints, our DSE team planned by quarters. This misalignment in planning cycles is an ongoing challenge that we are actively working to resolve.\n\nLooking ahead, our team is committed to continuing this partnership with our engineering colleagues. Both teams have invested significant time in building this relationship, and we are optimistic that it will yield substantial benefits in future projects.\n\nExternal Speaker: Benn Stancil\n\nIn addition to the above presentations, we kicked off our Analytics Summit with a keynote talk from Benn Stancil, Founder of Mode Analytics. Benn stepped through a history of the modern data stack, and the group discussed ideas on the future of analytics.", "label": 0}
{"title": "Are You a Dalia? How We Created Data Science Personas for Spotify\u2019s Analytics Platform", "url": "https://engineering.atspotify.com/2024/9/are-you-a-dalia-how-we-created-data-science-personas-for-spotifys-analytics-platform", "content": "On Spotify\u2019s Analytics Platform, we\u2019re dedicated to building products that empower data practitioners to discover, analyze, and share insights \u2014 think notebooks, dashboards, and metrics. To create solutions that connect with our users, it\u2019s critical for us to first understand who our users are. However, the evolving landscape of data roles presents a challenge.\n\nIn the past decade, we\u2019ve observed an industry trend toward consolidating diverse roles under the Data Scientist umbrella. Yet not all data scientists have the same needs, and many employees working with data don\u2019t even hold that title. While we adopted the term \u201cdata practitioner\u201d as a workaround, it proved too broad to effectively guide our product development. So we built a framework to better understand who our data practitioners are. Enter personas.\n\nTo create personas, we took a mixed-methods approach, meaning we blended qualitative insights from in-depth interviews with quantitative analysis of employee data, allowing us to identify clusters of shared behaviors. We pushed for personas that were both interpretable and applicable so that product leaders could easily grasp them and use them to make tactical decisions.\n\nThis work resulted in six distinct personas that compose the data practitioner universe at Spotify, along with several learnings relevant to all insights practitioners.\n\nCurious to see the personas? Skip to the end!\n\nWhat did we learn along the way?\n\nDon\u2019t just facilitate collaboration between Data Science and Research by lowering barriers. Instead, erase the barriers between data science and user research. While mixed-methods research sometimes involves one method following another, we found success in having an iterative loop \u2014 using the qualitative work to inform the quantitative work, then the quantitative work to inform the qualitative work, and so on, until we\u2019re satisfied with the end result. After a series of user interviews, we created our first version of the quantitative rules that would inform persona classification. We then continued to interview additional users to help us validate our classifications, add depth to our knowledge of the archetypes, and break ties to improve the rules we were developing. This became an iterative process of qualitative and quantitative validation, where each user interview we conducted resulted in either validation or tweaks to the proposed quantitative rules.\n\nDon\u2019t just share findings and send the deck around. Create momentum by openly discussing tailored recommendations with your stakeholders and connecting them to product initiatives. A common challenge to foundational work can be activating the work and actioning on the findings. Stakeholders may be wondering: What do we do now that we know this ? To encourage action, we created dedicated slides in our share-outs highlighting the personas for each team to focus on. These recommendations were open for discussion but gave a good starting point to engage with the content. We also offered suggestions on how the personas could inform future product strategy. With these slides as primers, we witnessed our teams learn a common language and understanding of who we were building for. Teammates used the new vocabulary to talk about our users during sprints and planning. Over the first quarter, the framework went from being a conversation starter to becoming the foundation for how we set goals and metrics.\n\nDon\u2019t just write the research plan and go off to work. Engage with your stakeholders from start to finish. For anything but the smallest questions, data scientists and user researchers have a tendency to withdraw into the research process and emerge at last with final answers, detached from changing requirements during the research period. Keeping our stakeholders involved throughout the project reduced this risk. Before kicking off the project, we aligned on the background and scope of the work and remained open to stakeholder feedback. To easily follow along during the project, we created a dedicated Slack channel for the research, to share updates and anecdotes on the fly. We also opened user interviews for observation across the entire product area, regardless of squad and role. We also found that foundational work with longer timelines benefitted from sharing early learnings at key milestones. Midway through the project, we shared \u201cPostcards from the Field\u201d \u2014 raw, unsynthesized qualitative data that kept stakeholders up to speed with the research. We also shared previews of the personas with our product area leads at later milestones to gather feedback.\n\nDon\u2019t take a complex quantitative approach for its own sake. Keep your approach simple and clear. We used a rule-based approach to classify Spotifiers into personas. (For example: \u201cEmployee has used dashboarding tools more than 90% of data scientists.\u201d) Keeping the approach simple made our results easily interpretable, transparent, and trustworthy to our stakeholders.\n\nDon\u2019t just move on to another project. Instead, find opportunities to extend the work beyond the original scope of the project. To ensure the personas would be used among our teams going forward, we built a dataset to classify every internal user at Spotify into a persona on a daily basis. This dataset has not only become helpful for recruitment for future initiatives, but also adds more color to our product area metrics. We are now able to segment all our key results by these persona types and explore user behaviors for each group. Because this work is large in scale and built foundational knowledge across our product area, we wanted to make sure that the outcome was easy to refer to and share. To make the personas more memorable, we collaborated with our design partners to create Spotify-approved illustrations, adding a face to each one. Building strong visual artifacts encouraged greater longevity and shareability.\n\nYour turn: Which persona are you?\n\nIf you work in the field, you may be asking yourself: So which data persona am I?\n\nPaola the Product Strategist: Paolas are the most common data science persona at Spotify. They analyze and tell stories with data to help inform strategy. They typically partner closely with non-technical stakeholders. Outside Spotify they hold titles like Product Analyst or Data Scientist, Analytics.\n\nEli the Extensive Explorer: Elis are the second-most-common persona type at Spotify. These are data scientists, machine learning engineers, or research scientists who go deep on data to deliver complex research and models. This persona brings together the traditional data science role with MLE and similar roles.\n\nIvan the Influencer: Ivans are leaders who support their data teams and have influence over best practices and processes. They may be team leads or managers.\n\nDaryl the Data Viz Artist: Daryls use data to tell interactive, visual stories and spend most of their time creating dashboards to share with non-technical stakeholders. This role is a niche at many companies and may not even have its own title.\n\nSigrid the Systems Engineer: Sigrids are engineers who create infrastructure to help technical stakeholders leverage data for analysis. Data Engineer or Analytics Engineer are common titles.\n\nDalia the Data Dabbler: Dalias are not data scientists by title but work with data to perform analysis or make decisions. They typically explore straightforward data questions, often with technical support from the other personas.\n\n\u2013\n\nDid one of the personas resonate with you? We found that most Spotify data practitioners related closely to one persona, but some employees reported fitting a hybrid of two personas.", "label": 0}
{"title": "How It\u2019s Made: Little Language Lessons uses Gemini\u2019s multilingual capabilities to personalize language learning", "url": "https://developers.googleblog.com/en/how-its-made-little-language-lessons-to-personalize-learning/", "content": "As an engineer, I\u2019ve always been fascinated by languages\u2014both the kind we code in and the kind we speak. Learning a new programming language typically begins by building something tangible, instantly putting theory into practice. Learning a new spoken language, on the other hand, often happens in a vacuum\u2014through textbooks or exercises that feel strangely disconnected from the situations where language actually matters. As is the case with programming, language is best learned through meaningful contexts: the conversations we have, the objects around us, the moments we find ourselves in. Unlike traditional learning tools, AI can adapt to a learner\u2019s context, making it uniquely suited to help us practice languages in ways that feel more natural and personal. This led me, along with a small group of colleagues, to experiment with the Gemini API, which enables developers to access the latest generative models from Google. The result is Little Language Lessons: a collection of three bite-sized learning experiments, all powered by Google\u2019s Gemini models.\n\nExperiment 1, Tiny Lesson: Learning what you need, when you need it One of the most frustrating parts about learning a language is finding yourself in a situation where you need a specific word or phrase\u2014and it\u2019s one that you haven\u2019t learned yet. That\u2019s the idea behind Tiny Lesson. You describe a situation\u2014maybe it\u2019s \u201casking for directions\u201d or \u201cfinding a lost passport\u201d\u2014and receive useful vocabulary, phrases, and grammar tips tailored to that context.\n\nSorry, your browser doesn't support playback for this video\n\nWe were able to accomplish this using a simple prompt recipe. The prompt begins with a persona-setting preamble that looks like this:\n\nYou are a(n) {target language} tutor who is bilingual in {target language} and {source language} and an expert at crafting educational content that is custom-tailored to students' language usage goals. Markdown Copied\n\nIn this prompt and in all of the prompts to come, we took advantage of Gemini\u2019s ability to provide outputs as structured JSON, defining desired result as a list of keys in an object:\n\nFor the given usage context, provide a JSON object containing two keys: \"vocabulary\" and \"phrases\". The value of \"vocabulary\" should be an array of objects, each containing three keys: \"term\", \u201ctransliteration\u201d, and \"translation\". The value of \"term\" should be a {target language} word that is highly relevant and useful in the given context. If the language of interest is ordinarily written in the Latin script, the value of \u201ctransliteration\u201d should be an empty string. Otherwise, the value of \u201ctransliteration\u201d should be a transliteration of the term. The value of \"translation\" should be the {source language} translation of the term. ... Markdown Copied\n\nIn total, each lesson is the result of two calls to the Gemini API. One prompt handles generating all of the vocabulary and phrases, and the other deals with generating relevant grammar topics. And the end of each prompt, we interpolate the user\u2019s desired usage context as follows:\n\nINPUT (usage context): {user input} Markdown Copied\n\nExperiment 2, Slang Hang: Learning to sound less like a textbook There\u2019s a moment in the journey of learning a language when you start feeling comfortable. You can hold conversations, express yourself, and mostly get by. But then you realize, you still sound\u2026 off. Too formal. Stiff. We built Slang Hang to help address this. The idea is simple: generate a realistic conversation between native speakers and let users learn from it. You can watch the dialogue unfold, revealing one message at a time and unpacking unfamiliar terms as they appear.\n\nSorry, your browser doesn't support playback for this video\n\nThe preamble for the Slang Hang prompt looks like this:\n\nYou are a screenwriter who is bilingual in {source language} and {target language} and an expert and crafting captivating dialogues. You are also a linguist and highly attuned to the cultural nuances that shape natural speech. Markdown Copied\n\nAlthough users can only reveal messages one at a time, everything\u2014the setting, the conversation, the explanations for highlighted terms\u2014is generated from a single call to the Gemini API. We define the structure of the JSON output as follows:\n\nGenerate a short scene that contains two interlocutors speaking authentic {target language}. Give the result as a JSON object that contains two keys: \"context\" and \"dialogue\". The value of \"context\" should be a short paragraph in {SOURCE LANGUAGE} that describes the setting of the scene, what is happening, who the speakers are, and speakers' relationship to each other. The value of \"dialogue\" should be an array of objects, where each object contains information about a single conversational turn. Each object in the \"dialogue\" array should contain four keys: \"speaker\", \"gender\", \"message\", and \"notes\". ... Markdown Copied\n\nThe dialogue is generated in the user\u2019s target language, but users can also translate messages into their native language (a functionality powered by the Cloud Translation API). One of the more interesting aspects of this experiment is the element of emergent storytelling. Each scene is unique and generated on the fly\u2014it could be a street vendor chatting with a customer, two coworkers meeting on the subway, or even a pair of long-lost friends unexpectedly reuniting at an exotic pet show. That said, we found that this experiment is somewhat susceptible to accuracy errors: it occasionally misuses certain expressions and slang, or even makes them up. LLMs still aren\u2019t perfect, and for that reason it\u2019s important to cross-reference with reliable sources.\n\nExperiment 3, Word Cam: Learning from your surroundings Sometimes, you just need words for the things in front of you. It can be extremely humbling to realize just how much you don\u2019t know how to say in your target language. You know the word for \u201cwindow\u201d, but how do you say \u201cwindowsill\u201d? Or \u201cblinds\u201d? Word Cam turns your camera into an instant vocabulary helper. Snap a photo, and Gemini will detect objects, label them in your target language, and give you additional words that you can use to describe them.\n\nSorry, your browser doesn't support playback for this video\n\nThis experiment leverages Gemini\u2019s vision capabilities for object detection. We send the model an image and ask it for the bounding box coordinates of the different objects in that image:\n\nProvide insights about the objects that are present in the given image. Give the result as a JSON object that contains a single key called \"objects\". The value of \"objects\" should be an array of objects whose length is no more than the number of distinct objects present in the image. Each object in the array should contain four keys: \"name\", \"transliteration\", \"translation\", and \"coordinates\". ... The value of \"coordinates\" should be an integer array representing the coordinates of the bounding box for the object. Give the coordinates as [ymin, xmin, ymax, xmax]. Markdown Copied\n\nOnce the user selects an object, we send the cropped image to Gemini in a separate prompt and ask it to generate descriptors for that object in the user\u2019s target language:\n\nFor the object represented in the given image, provide descriptors that describe the object. Give the result as a JSON object that contains a single key called \"descriptors\". The value of \"descriptors\" should be an array of objects, where each object contains five keys: \"descriptor\", \"transliteration\", \"translation\", \"exampleSentence\", \"exampleSentenceTransliteration\", and \"exampleSentenceTranslation\". ... Markdown Copied", "label": 0}
{"title": "Not Smiling Every Single Moment", "url": "https://lifeofpablo.com/blog/not-smiling-every-single-moment", "content": "Not Smiling Every Single Moment\n\nThis post was written in English (en_US).\n\nEver since I was young, I was the guy who always had a smile on his face. I mean like 90% of the time. Any where I went, any where I was spotted, any encounter. I, Pablo Morales was the smiling kid.\n\nFa\u00e7ade\n\nSmiling would help me put on a fa\u00e7ade or a filter to mask the many emotions I feel at any given moment.\n\nAbroad\n\nWhen I visit Mexico, I've learned not to smile as much. I just experience life differently. I try not to stand out. I am always happy to see the people I love.\n\nWhen I was studying in Europe, I truly learned not to smile. Many Europeans don't smile when they are out and about with their day. During studying abroad, I made it a goal to not stand out as an \"American\", especially during the political turmoil happening in the United States.\n\nEven when I reverted back to my \"typical smiling,\" I never truly reverted back.\n\nIt's okay not to smile\n\nBy not smiling, I am learning to express myself more. I can be in a great mood but not smile? Yes, I can! By smiling all the time, it taught me I wasn't true to myself. I wasn't being fair to myself. I was defrauding myself. How can I be a real person if I can't express myself?\n\nAm I happy?\n\nYou're asking yourself this question? Is Pablo Morales, actually happy with himself? Yes, I am happy.\n\nHas it been hard to love myself? Yes. Overall, happiness does not fall into one size fits all scenario.\n\nThere is a side of me that I am afraid to explore. I have a battle ahead. I am still looking for this battle with a smile or frown.", "label": 1}
{"title": "How GitHub uses CodeQL to secure GitHub", "url": "https://github.blog/engineering/how-github-uses-codeql-to-secure-github/", "content": "GitHub\u2019s Product Security Engineering team writes code and implements tools that help secure the code that powers GitHub. We use GitHub Advanced Security (GHAS) to discover, track, and remediate vulnerabilities and enforce secure coding standards at scale. One tool we rely heavily on to analyze our code at scale is CodeQL.\n\nCodeQL is GitHub\u2019s static analysis engine that powers automated security analyses. You can use it to query code in much the same way you would query a database. It provides a much more robust way to analyze code and uncover problems than an old-fashioned text search through a codebase.\n\nThe following post will detail how we use CodeQL to keep GitHub secure and how you can apply these lessons to your own organization. You will learn why and how we use:\n\nCustom query packs (and how we create and manage them).\n\nCustom queries.\n\nVariant analysis to uncover potentially insecure programming practices.\n\nEnabling CodeQL at scale\n\nWe employ CodeQL in a variety of ways at GitHub.\n\nDefault setup with the default and security-extended query suites\n\nDefault setup with the default and security-extended query suites meets the needs of the vast majority of our over 10,000 repositories. With these settings, pull requests automatically get a security review from CodeQL. Advanced setup with a custom query pack\n\nA few repositories, like our large Ruby monolith, need extra special attention, so we use advanced setup with a query pack containing custom queries to really tailor to our needs. Multi-repository variant analysis (MRVA)\n\nTo conduct variant analysis and quick auditing, we use MRVA. We also write custom CodeQL queries to detect code patterns that are either specific to GitHub\u2019s codebases or patterns we want a security engineer to manually review.\n\nThe specific custom Actions workflow step we use on our monolith is pretty simple. It looks like this:\n\n- name: Initialize CodeQL uses: github/codeql-action/init@v3 with: languages: ${{ matrix.language }} config-file: ./.github/codeql/${{ matrix.language }}/codeql-config.yml\n\nOur Ruby configuration is pretty standard, but advanced setup offers a variety of configuration options using custom configuration files. The interesting part is the packs option, which is how we enable our custom query pack as part of the CodeQL analysis. This pack contains a collection of CodeQL queries we have written for Ruby, specifically for the GitHub codebase.\n\nSo, let\u2019s dive deeper into why we did that\u2014and how!\n\nPublishing our CodeQL query pack\n\nInitially, we published CodeQL query files directly to the GitHub monolith repository, but we moved away from this approach for several reasons:\n\nIt required going through the production deployment process for each new or updated query.\n\nQueries not included in a query pack were not pre-compiled, which slowed down CodeQL analysis in CI.\n\nOur test suite for CodeQL queries ran as part of the monolith\u2019s CI jobs. When a new version of the CodeQL CLI was released, it sometimes caused the query tests to fail because of changes in the query output, even when there were no changes to the code in the pull request. This often led to confusion and frustration among engineers, as the failure wasn\u2019t related to their pull request changes.\n\nBy switching to publishing a query pack to GitHub Container Registry (GCR), we\u2019ve simplified our process and eliminated many of these pain points, making it easier to ship and maintain our CodeQL queries. So while it\u2019s possible to deploy custom CodeQL query files directly to a repository, we recommend publishing CodeQL queries as a query pack to the GCR for easier deployment and faster iteration.\n\nCreating our query pack\n\nWhen setting up our custom query pack, we faced several considerations, particularly around managing dependencies like the ruby-all package.\n\nTo ensure our custom queries remain maintainable and concise, we extend classes from the default query suite, such as the ruby-all library. This allows us to leverage existing functionality rather than reinventing the wheel, keeping our queries concise and maintainable. However, changes to the CodeQL library API can introduce breaking changes, potentially deprecating our queries or causing errors. Since CodeQL runs as part of our CI, we wanted to minimize the chance of this happening, as this can lead to frustration and loss of trust from developers.\n\nWe develop our queries against the latest version of the ruby-all package, ensuring we\u2019re always working with the most up-to-date functionality. To mitigate the risk of breaking changes affecting CI, we pin the ruby-all version when we\u2019re ready to release, locking it in the codeql-pack.lock.yml file. This guarantees that when our queries are deployed, they will run with the specific version of ruby-all we\u2019ve tested, avoiding potential issues from unintentional updates.\n\nHere\u2019s how we manage this setup:\n\nIn our qlpack.yml, we set the dependency to use the latest version of ruby-all\n\nDuring development, this configuration pulls in the latest version) of ruby-all when running codeql pack init , ensuring we\u2019re always up to date. // Our custom query pack's qlpack.yml library: false name: github/internal-ruby-codeql version: 0.2.3 extractor: 'ruby' dependencies: codeql/ruby-all: \"*\" tests: 'test' description: \"Ruby CodeQL queries used internally at GitHub\"\n\nwhen running , ensuring we\u2019re always up to date. Before releasing, we lock the version in the codeql-pack.lock.yml file, specifying the exact version to ensure stability and prevent issues in CI. // Our custom query pack's codeql-pack.lock.yml lockVersion: 1.0.0 dependencies: ... codeql/ruby-all: version: 1.0.6\n\nThis approach allows us to balance developing against the latest features of the ruby-all package while ensuring stability when we release.\n\nWe also have a set of CodeQL unit tests that exercise our queries against sample code snippets, which helps us quickly determine if any query will cause errors before we publish our pack. These tests are run as part of the CI process in our query pack repository, providing an early check for issues. We strongly recommend writing unit tests for your custom CodeQL queries to ensure stability and reliability.\n\nAltogether, the basic flow for releasing new CodeQL queries via our pack is as follows:\n\nOpen a pull request with the new query.\n\nWrite unit tests for the new query.\n\nMerge the pull request.\n\nIncrement the pack version in a new pull request.\n\nRun codeql pack init to resolve dependencies.\n\nto resolve dependencies. Correct unit tests as needed.\n\nPublish the query pack to the GitHub Container Registry (GCR).\n\nRepositories with the query pack in their config will start using the updated queries.\n\nWe have found this flow balances our team\u2019s development experience while ensuring stability in our published query pack.\n\nConfiguring our repository to use our custom query pack\n\nWe won\u2019t provide a general recommendation on configuration here, given that it ultimately depends on how your organization deploys code. We opted against locking our pack to a particular version in our CodeQL configuration file (see above). Instead, we chose to manage our versioning by publishing the CodeQL package in GCR. This results in the GitHub monolith retrieving the latest published version of the query pack. To roll back changes, we simply have to republish the package. In one instance, we released a query that had a high number of false positives and we were able to publish a new version of the pack that removed that query in less than 15 minutes. This is faster than the time it would have taken us to merge a pull request on the monolith repository to roll back the version in the CodeQL configuration file.\n\nOne of the problems we encountered with publishing the query pack in GCR was how to easily make the package available to multiple repositories within our enterprise. There are several approaches we explored.\n\nGrant access permissions for individual repositories. On the package management page, you can grant permissions for individual repositories to access your package. This was not a good solution for us since we have too many repositories for it to be feasible to do manually, yet there is not currently a way to configure programmatically using an API.\n\nOn the package management page, you can grant permissions for individual repositories to access your package. This was not a good solution for us since we have too many repositories for it to be feasible to do manually, yet there is not currently a way to configure programmatically using an API. Mint a personal access token for the CodeQL action runner. We could have minted a personal access token (PAT) that has access to read all packages for our organization and added that to the CodeQL action runner. However, this would have required managing a new token, and it seemed a bit more permissive than we wanted because it could read all of our private packages rather than ones we explicitly allow it to have access to.\n\nWe could have minted a personal access token (PAT) that has access to read all packages for our organization and added that to the CodeQL action runner. However, this would have required managing a new token, and it seemed a bit more permissive than we wanted because it could read all of our private packages rather than ones we explicitly allow it to have access to. Provide access permissions via a linked repository. We ended up implementing the third solution that we explored. We link a repository to the package and allow the package to inherit access permissions from the linked repository.\n\nCodeQL query pack queries\n\nWe write a variety of custom queries to be used in our custom query packs. These cover GitHub-specific patterns that aren\u2019t included in the default CodeQL query pack. This allows us to tailor the analysis to patterns and preferences that are specific to our company and codebase. Some of the types of things we alert on using our custom query pack include:\n\nHigh-risk APIs specific to GitHub\u2019s code that can be dangerous if they receive unsanitized user input.\n\nUse of specific built-in Rails methods for which we have safer, custom methods or functions.\n\nRequired authorization methods not being used in our REST API endpoint definitions and GraphQL object/mutation definitions.\n\nREST API endpoints and GraphQL mutations that require engineers to define access control methods to determine which actors can access them. (Specifically, the query detects the absence of this method definition to ensure that the actors\u2019 permissions are being checked for these endpoints.)\n\nUse of signed tokens so we can nudge engineers to include Product Security as a reviewer when using them.\n\nCustom queries can be used more for educational purposes rather than being blockers to shipping code. For example, we want to alert engineers when they use the ActiveRecord::decrypt method. This method should generally not be used in production code, as it will cause an encrypted column to become decrypted. We use the recommendation severity in the query metadata so these alerts are treated as more of an informational alert. That means this may trigger an alert in a pull request, but it won\u2019t cause the CodeQL CI job to fail. We use this lower severity level to allow engineers to assess the impact of new queries without immediate blocking. Additionally, this alert level isn\u2019t tracked through our Fundamentals program, meaning it doesn\u2019t require immediate action, reflecting the query\u2019s maturity as we continue to refine its relevance and risk assessment.\n\n/** * @id rb/github/use-of-activerecord-decrypt * @description Do not use the .decrypt method on AR models, this will decrypt all encrypted attributes and save * them unencrypted, effectively undoing encryption and possibly making the attributes inaccessible. * If you need to access the unencrypted value of any attribute, you can do so by calling my_model.attribute_name. * @kind problem * @severity recommendation * @name Use of ActiveRecord decrypt method * @tags security * github-internal */ import ruby import DataFlow import codeql.ruby.DataFlow import codeql.ruby.frameworks.ActiveRecord /** Match against .decrypt method calls where the receiver may be an ActiveRecord object */ class ActiveRecordDecryptMethodCall extends ActiveRecordInstanceMethodCall { ActiveRecordDecryptMethodCall() { this.getMethodName() = \"decrypt\" } } from ActiveRecordDecryptMethodCall call select call, \"Do not use the .decrypt method on AR models, this will decrypt all encrypted attributes and save them unencrypted.\n\nAnother educational query is the one mentioned above in which we detect the absence of the `control_access` method in a class that defines a REST API endpoint. If a pull request introduces a new endpoint without `control_access`, a comment will appear on the pull request saying that the `control_access` method wasn\u2019t found and it\u2019s a requirement for REST API endpoints. This will notify the reviewer of a potential issue and prompt the developer to fix it.\n\n/** * @id rb/github/api-control-access * @name Rest API Without 'control_access' * @description All REST API endpoints must call the 'control_access' method, to ensure that only specified actor types are able to access the given endpoint. * @kind problem * @tags security * github-internal * @precision high * @problem.severity recommendation */ import codeql.ruby.AST import codeql.ruby.DataFlow import codeql.ruby.TaintTracking import codeql.ruby.ApiGraphs // Api::App REST API endpoints should generally call the control_access method private DataFlow::ModuleNode appModule() { result = API::getTopLevelMember(\"Api\").getMember(\"App\").getADescendentModule() and not result = protectedApiModule() and not result = staffAppApiModule() } // Api::Admin, Api::Staff, Api::Internal, and Api::ThirdParty REST API endpoints do not need to call the control_access method private DataFlow::ModuleNode protectedApiModule() { result = API::getTopLevelMember([\"Api\"]) .getMember([\"Admin\", \"Staff\", \"Internal\", \"ThirdParty\"]) .getADescendentModule() } // Api::Staff::App REST API endpoints do not need to call the control_access method private DataFlow::ModuleNode staffAppApiModule() { result = API::getTopLevelMember([\"Api\"]).getMember(\"Staff\").getMember(\"App\").getADescendentModule() } private class ApiRouteWithoutControlAccess extends DataFlow::CallNode { ApiRouteWithoutControlAccess() { this = appModule().getAModuleLevelCall([\"get\", \"post\", \"delete\", \"patch\", \"put\"]) and not performsAccessControl(this.getBlock()) } } predicate performsAccessControl(DataFlow::BlockNode blocknode) { accessControlCalled(blocknode.asExpr().getExpr()) } predicate accessControlCalled(Block block) { // the method `control_access` is called somewhere inside `block` block.getAStmt().getAChild*().(MethodCall).getMethodName() = \"control_access\" } from ApiRouteWithoutControlAccess api select api.getLocation(), \"The control_access method was not detected in this REST API endpoint. All REST API endpoints must call this method to ensure that the endpoint is only accessible to the specified actor types.\"\n\nVariant analysis\n\nVariant analysis (VA) refers to the process of searching for variants of security vulnerabilities. This is particularly useful when we\u2019re responding to a bug bounty submission or a security incident. We use a combination of tools to do this, including GitHub\u2019s code search functionality, custom scripts, and CodeQL. We will often start by using code search to find patterns similar to the one that caused a particular vulnerability across numerous repositories. This is sometimes not good enough, as code search is not semantically aware, meaning that it cannot determine whether a given variable is an Active Record object or whether it is being used in an `if` expression. To answer those types of questions we turn to CodeQL.\n\nWhen we write CodeQL queries for variant analysis we are much less concerned about false positives, since the goal is to provide results for security engineers to analyze. The quality of the code is also not quite as important, as these queries will only be used for the duration of the VA effort. Some of the types of things we use CodeQL for during VAs are:\n\nWhere are we using SHA1 hashes?\n\nOne of our internal API endpoints was vulnerable to SQLi according to a recent bug bounty report. Where are we passing user input to that API endpoint?\n\nThere is a problem with how some HTTP request libraries in Ruby handle the proxy setting. Can we look at places we are instantiating our HTTP request libraries with a proxy setting?\n\nOne recent example involved a subtle vulnerability in Rails. We wanted to detect when the following condition was present in our code:\n\nA parameter was used to look up an Active Record object.\n\nThat parameter is later reused after the Active Record object is looked up.\n\nThe concern with this condition is that it could lead to an insecure direct object reference (IDOR) vulnerability because Active Record finder methods can accept an array. If the code looks up an Active Record object in one call to determine if a given entity has access to a resource, but later uses a different element from that array to find an object reference, that can lead to an IDOR vulnerability. It would be difficult to write a query to detect all vulnerable instances of this pattern, but we were able to write a query that found potential vulnerabilities that gave us a list of code paths to manually analyze. We ran the query against a large number of our Ruby codebases using CodeQL\u2019s MRVA.\n\nThe query, which is a bit hacky and not quite production grade, is below:\n\n/** * @name wip array query * @description an array is passed to an AR finder object */ import ruby import codeql.ruby.AST import codeql.ruby.ApiGraphs import codeql.ruby.frameworks.Rails import codeql.ruby.frameworks.ActiveRecord import codeql.ruby.frameworks.ActionController import codeql.ruby.DataFlow import codeql.ruby.Frameworks import codeql.ruby.TaintTracking // Gets the \"final\" receiver in a chain of method calls. // For example, in `Foo.bar`, this would give the `Foo` access, and in // `foo.bar.baz(\"arg\")` it would give the `foo` variable access private Expr getUltimateReceiver(MethodCall call) { exists(Expr recv | recv = call.getReceiver() and ( result = getUltimateReceiver(recv) or not recv instanceof MethodCall and result = recv ) ) } // Names of class methods on ActiveRecord models that may return one or more // instances of that model. This also includes the `initialize` method. // See https://api.rubyonrails.org/classes/ActiveRecord/FinderMethods.html private string staticFinderMethodName() { exists(string baseName | baseName = [\"find_by\", \"find_or_create_by\", \"find_or_initialize_by\", \"where\"] and result = baseName + [\"\", \"!\"] ) // or // result = [\"new\", \"create\"] } private class ActiveRecordModelFinderCall extends ActiveRecordModelInstantiation, DataFlow::CallNode { private ActiveRecordModelClass cls; ActiveRecordModelFinderCall() { exists(MethodCall call, Expr recv | call = this.asExpr().getExpr() and recv = getUltimateReceiver(call) and ( // The receiver refers to an `ActiveRecordModelClass` by name recv.(ConstantReadAccess).getAQualifiedName() = cls.getAQualifiedName() or // The receiver is self, and the call is within a singleton method of // the `ActiveRecordModelClass` recv instanceof SelfVariableAccess and exists(SingletonMethod callScope | callScope = call.getCfgScope() and callScope = cls.getAMethod() ) ) and ( call.getMethodName() = staticFinderMethodName() or // dynamically generated finder methods call.getMethodName().indexOf(\"find_by_\") = 0 ) ) } final override ActiveRecordModelClass getClass() { result = cls } } class FinderCallArgument extends DataFlow::Node { private ActiveRecordModelFinderCall finderCallNode; FinderCallArgument() { this = finderCallNode.getArgument(_) } } class ParamsHashReference extends DataFlow::CallNode { private Rails::ParamsCall params; // TODO: only direct element references against `params` calls are considered ParamsHashReference() { this.getReceiver().asExpr().getExpr() = params } string getArgString() { result = this.getArgument(0).asExpr().getConstantValue().getStringlikeValue() } } class ArrayPassedToActiveRecordFinder extends TaintTracking::Configuration { ArrayPassedToActiveRecordFinder() { this = \"ArrayPassedToActiveRecordFinder\" } override predicate isSource(DataFlow::Node source) { source instanceof ParamsHashReference } override predicate isSink(DataFlow::Node sink) { sink instanceof FinderCallArgument } string getParamsArg(DataFlow::CallNode paramsCall) { result = paramsCall.getArgument(0).asExpr().getConstantValue().getStringlikeValue() } // this doesn't check for anything fancy like whether it's reuse in a if/else // only intended for quick manual audit filtering of interesting candidates // so remains fairly broad to not induce false negatives predicate paramsUsedAfterLookups(DataFlow::Node source) { exists(DataFlow::CallNode y | y instanceof ParamsHashReference and source.getEnclosingMethod() = y.getEnclosingMethod() and source != y and getParamsArg(source) = getParamsArg(y) // we only care if it's used again AFTER an object lookup and y.getLocation().getStartLine() > source.getLocation().getStartLine()) } } from ArrayPassedToActiveRecordFinder config, DataFlow::Node source, DataFlow::Node sink where config.hasFlow(source, sink) and config.paramsUsedAfterLookups(source) select source, sink.getLocation()\n\nConclusion\n\nCodeQL can be very useful for product security engineering teams to detect and prevent vulnerabilities at scale. We use a combination of queries that run in CI using our query pack and one-off queries run through MRVA to find potential vulnerabilities and communicate them to engineers. CodeQL isn\u2019t only useful for finding security vulnerabilities, though; it is also useful for detecting the presence or absence of security controls that are defined in code. This saves our security team time by surfacing certain security problems automatically, and saves our engineers time by detecting them earlier in the development process.\n\nWriting custom CodeQL queries\n\nTips for getting started\n\nWe have a large number of articles and resources for writing custom CodeQL queries. If you haven\u2019t written custom CodeQL queries before, here are some resources to help get you started:\n\nImprove the security of your applications today by enabling CodeQL for free on your public repositories, or try GitHub Advanced Security for your organization.\n\nMichael Recachinas, GitHub Staff Security Engineer, also contributed to this blog post.", "label": 0}
{"title": "Publish with pride", "url": "https://shellsharks.com/notes/2025/01/29/publish-with-pride", "content": "Mike Sass\n\n@shellsharks\n\nPhew \ud83e\udd75, what a flurry. Made a ton of changes to the site. The home page now features a mixed feed of all of my posts, notes, logs and other content types. I\u2019ve also started a link blog to share cool links I come across. Finally, I\u2019ve figured out how to paginate a TON of stuff, to make some of my pages more performant (my Activity page is one good example of how pagination is making that page usable on mobile now).\n\nWith respect to my home page, I used to be kinda precious about only exposing my formal \u201cblog posts\u201d, which are supposed to be longer-form, and in some undefinable way, \u201cbetter\u201d than other things I write and publish on the site. Inspired by others in the Indieweb community, who proudly feature everything they write, I decided to try out the same. To distinguish between different content types, i\u2019ve added a splash of color with icons corresponding to different post types. I also hope it can call attention (for those who are interested) to all the other things I publish. For so long, I had these hidden away in pages you could find via my \u201chamburger\u201d menu, but you had to go exploring to really find most of it. And given the number of things I link to in that menu, I doubt many people had the patience or interest to really hunt down other things. So, here we are!\n\nLet me know what ya think! I\u2019m still tinkering around with the format, theme and other general aesthetics of the site and various pages, but I like it well enough at this point.", "label": 1}
{"title": "February 2016", "url": "https://lifeofpablo.com/blog/published:2016-02", "content": "en\n\n\"Yesterday was quite the cold adventure! It involved snow but I mean A LOT of it! I love winter. Having snow days in colllege is the best thing that could happen to sleep-deprived students. I managed to get some extra shut eye. Seriously though, these snowdays make me feel like a little kid again. You do not know the excitement when I found out UNK canceled clasess to have not one but two snowdays in a row? Man I LOVE COLLEGE! Its music to my ears.\n\nInstead of being a lazy bum and staying in bed all day, I decided to spend my day outside in the good ol' Nebraska cold winter. My friends and I spend the day outside in the beautiful Nebraska weather, winter. The goal for the day was to make an igloo. We started right after lunch. My best friend, Sammy and I spent the day carving out a snow pile to make an igloo. Of course we could not go without throwing the occasional snowball at each other and others that walked by! We bad! ?We used our inner architecture and construction skills as well as our inner child to good use to make the perfect one! It was fun to work on it together!\n\nHere is the end result with me in it.\n\nQuestion of the day!??\n\nDid you spend your day outside? Shoot me a commnet or an email of your pictures or links at\n\nI decided to make an igloo today with my number 1 @officialsamharp . The outdoors are better! #friendshipgoals #outdoors #nebraska #lopers ?credz @audrey_irene_ A photo posted by Pablo Morales (@pmorales18) on Feb 2, 2016 at 4:59pm PST\n\n\"", "label": 1}
{"title": "What\u2019s new with Robinhood, our in-house load balancing service", "url": "https://dropbox.tech/infrastructure/robinhood-in-house-load-balancing-service", "content": "Robinhood is the internal Dropbox load balancing service we deployed in 2020. It is responsible for routing all our internal traffic between servers to balance service load. Before we built Robinhood, most Dropbox services suffered from uneven load distribution among backends. Hardware differences throughout our fleet and the limitations of our prior load balancing algorithms led to reliability issues due to overloaded instances. In order to solve this problem, we often had to over-provision a service\u2019s fleet, which inevitably increased our hardware spend\u2014a pricey and avoidable headache. Robinhood solves these long-standing load balancing problems at Dropbox scale, across our entire global data center footprint. Last year, we introduced the latest iteration to Robinhood: By leveraging proportional\u2013integral\u2013derivative (PID) controllers, Robinhood can now more quickly and effectively manage load imbalances. This has not only improved the reliability of our infrastructure, but yielded significant hardware cost savings. And with an increase in AI workloads that power our latest intelligent features, effectively managing demands on our GPU resources is more critical to the business than ever.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images\u2014plus generative AI capabilities across even more connected apps. See what's new \u2192\n\nThe challenge of load balancing at Dropbox\n\nOur in-house service discovery system can scale to hundreds of thousands of hosts across multiple data centers around the globe. Some Dropbox services have millions of clients; however, we cannot allow each client to create connections to every server instance. This approach puts too much memory pressure on servers, and TLS handshakes during server restarts can overwhelm servers. Instead, we can use a service discovery system, which gives each client a subset of servers to connect to. Without other information, the best load balancing strategy a client can use is a round-robin of the list of addresses given by our service discovery system. However, the load on each server instance can be quite imbalanced with this method1. While increasing the subset size is an easy mitigation, it won\u2019t completely eliminate the imbalance and would just give service owners another parameter to deal with. And there\u2019s another, deeper issue. Even if we send the same number of requests to each server, the underlying hardware might differ from one server to the next. In other words, a request would consume a different amount of resources on different hardware classes.\n\nAt its core, the issue is that clients do not have visibility into server load. Years ago, we attempted to solve the problem by having servers attach the load in the response headers. Clients could then perform load balancing themselves by picking the least-loaded endpoint in the subset of addresses. The results were promising, but there were still several downsides. The approach required code changes to servers and clients for the special load header, which was hard to adopt globally. More importantly, the results were good, but they weren\u2019t good enough. In 2019, we officially decided to build Robinhood. This new service, built on top of our existing in-house service discovery system, collects load information from servers and attaches it to the routing information. Robinhood leverages Envoy\u2019s Endpoint Discovery Service, which incorporates the load information into endpoint weights so that clients can perform weighted round-robin. Now that the gRPC community is adopting the Envoy xDS protocol, Robinhood is compatible with both our Envoy and gRPC clients2. Another reason to build a new service was that, to our knowledge, there was no existing load balancing solution that met our needs at the time. After a few years in production, the results have been promising. We successfully reduced fleet size by 25% for some of our largest services, resulting in substantial hardware cost savings each year. Reliability has also improved, thanks to fewer over-utilized processes. Below, we break down the architecture of Robinhood to illustrate how we created a far superior system for balancing service load.\n\nThe architecture of Robinhood\n\nA deployment of Robinhood within a single datacenter\n\nAs shown in the illustration above, a Robinhood instance is deployed to each of our data centers and consists of three parts: the load balancing service, a proxy, and routing database. Load balancing service (LBS) This is the heart of Robinhood. The LBS is responsible for collecting load information and generating routing information with endpoint weights. Since we have multiple instances updating the routing info for a service concurrently, we use our in house shard manager to assign the primary worker for each service. In addition, each service is independent, so we can shard the LBS by service and scale it horizontally. Proxy The proxy is responsible for routing a service\u2019s load information to the corresponding LBS partition within the data center. A nice side effect of this setup is that the proxy also reduces the number of connections directly to LBS processes. Without a proxy, every LBS process would have to be connected to all nodes within our infrastructure. Instead, LBS processes are only connected to the proxy, which greatly reduces the memory pressure on the LBS. Also, because the proxy is only connected by nodes within the same data center, it can be scaled horizontally. This pattern is used in many parts of our infrastructure to protect services from receiving too many TLS connections. Routing database The routing database is a ZooKeeper/etcd-based database that stores routing information for services, such as hostname, IP address, weights generated by the LBS, etc. ZooKeeper and etcd can notify all watchers in real time of any node/key changes, and it scales pretty well for our read-heavy service discovery use case. The eventual consistency guaranteed by ZooKeeper/etcd is good enough for service discovery as well.\n\nA closer look at the load balancing service\n\nThe goal of load balancing is to ensure that the utilization of every node is equal to the average utilization. We use a PID controller to keep the utilization of each node almost the same as the average utilization. The LBS creates a PID controller for each node and uses the average utilization as the setpoint. The LBS then uses the output of the PID controller as the delta to the endpoint weight and normalizes the weight among all endpoints of the service. While it takes a couple of adjustments for a new node to converge on the average utilization, the PID controller works quite well for load balancing. The LBS is designed to handle a variety of scenarios that can affect load balancing, from node restarts to missing load reports. To maintain optimal performance, the LBS has implemented several strategies to handle these edge cases, which are detailed below. LBS start up. The LBS keeps load information and PID controller states in memory. During an LBS restart (which can occur due to a normal push, node rotation, hardware failure, etc.), the LBS does not immediately start updating weights but rather waits a short period of time for load reports to come in. For PID controller weights, LBS restores them by reading the endpoint weights from the routing database.\n\nThe LBS keeps load information and PID controller states in memory. During an LBS restart (which can occur due to a normal push, node rotation, hardware failure, etc.), the LBS does not immediately start updating weights but rather waits a short period of time for load reports to come in. For PID controller weights, LBS restores them by reading the endpoint weights from the routing database. Cold start node. New nodes frequently join the service fleet, and so it\u2019s important we prevent thundering herd issues. Since a new node typically has an initial utilization of 0, LBS sets the weight of the new node to a low endpoint weight and lets the PID controller ramp up the node to the average utilization.\n\nNew nodes frequently join the service fleet, and so it\u2019s important we prevent thundering herd issues. Since a new node typically has an initial utilization of 0, LBS sets the weight of the new node to a low endpoint weight and lets the PID controller ramp up the node to the average utilization. Missing load reports. Failures are common in distributed system environments. For example, load reports of some nodes might be delayed or never actually arrive because of network congestion or hardware failures. LBS skips these nodes during weight updates, meaning endpoint weights stay the same for those nodes since it doesn\u2019t know whether to increase or decrease the weight of those nodes. However, if a large portion of load reports are missing\u2014currently configured at 15%\u2014the average utilization calculation can be off, so it might not have an accurate setpoint to adjust node weights. For safety, LBS skips the weight update phase entirely in this case.\n\nFailures are common in distributed system environments. For example, load reports of some nodes might be delayed or never actually arrive because of network congestion or hardware failures. LBS skips these nodes during weight updates, meaning endpoint weights stay the same for those nodes since it doesn\u2019t know whether to increase or decrease the weight of those nodes. However, if a large portion of load reports are missing\u2014currently configured at 15%\u2014the average utilization calculation can be off, so it might not have an accurate setpoint to adjust node weights. For safety, LBS skips the weight update phase entirely in this case. Utilization metric. CPU utilization is the most popular metric for load balancing at Dropbox. For services not bottlenecked by CPU, the number of in-flight requests is a good alternate measurement. Therefore, we implemented LBS to support load balancing based on CPU and/or in-flight requests. 3\n\nCPU utilization is the most popular metric for load balancing at Dropbox. For services not bottlenecked by CPU, the number of in-flight requests is a good alternate measurement. Therefore, we implemented LBS to support load balancing based on CPU and/or in-flight requests. Limitations. The PID controller constructs a feedback loop to keep the utilization of the node close to the target value (the average utilization, in our case). This means that if there is little feedback\u2014for example, in the case of a very low traffic service, or very high-latency requests measured in minutes\u2014the load balancing won\u2019t be as effective. We argue that services with high latency requests should be asynchronous.\n\nRouting across data centers\n\nAn LBS instance handles load balancing within the data center. For cross-data center routing, there are different considerations. For example, we want to route requests to the closest data center to reduce the round trip time for the requests. Therefore, we've introduced a locality config for defining traffic splits between destination data centers:\n\nCopy { # client data center -> traffic split between destination data centers zone_1: { \"zone_1\": 100, } zone_2: { \"zone_2\": 50, \"zone_1\": 50, } }\n\nThis example config indicates that for clients in zone_1, 100% of requests are sent to zone_1, and for clients in zone_2, requests are evenly split between zone_1 and zone_2. The service discovery system utilizes this config to build Endpoint Discovery Service responses for clients. gRPC clients and Envoy perform two layers of weighted round-robin. The load balancer first selects the zone and then selects endpoints within that zone. Additionally, we support hot reload for changes to the locality config, allowing service owners to perform real-time failovers between data centers.\n\nEvaluating the performance of our load balancer\n\nWe measure load balancing performance with a max/avg ratio. For example, if the service owner chooses to load balance based on CPU, we use maxCPU/avgCPU as the indicator of performance. The reason is that service owners usually provision their service based on the maximum utilization among nodes, and the main purpose of load balancing is to reduce the size of the fleet. The PID controller load balancing strategy can achieve a max/avg ratio close to 1.\n\nThis graph shows the max/avg CPU and p95/avg CPU of one of our biggest Envoy proxy clusters. After enabling PID controller-based load balancing, the two metrics dropped close to 1. The max/avg ratio dropped from 1.26 to 1.01, showing a 20% (1.01/1.26 ~ 0.8) improvement.\n\nThe graph above shows the quantile breakdown of CPU utilization per node. After enabling PID controller-based load balancing, the max, p95, avg, and p5 almost consolidated into a single line. Let\u2019s look at another good example:\n\nNow, this graph shows the max/avg CPU and p95/avg CPU of another one of our biggest database frontend clusters. After enabling PID controller-based load balancing, the two metrics dropped close to 1. The max/avg ratio dropped from 1.4 to 1.05, showing a 25% improvement.\n\nFinally, this graph shows the quantile breakdown of CPU utilization per node. After enabling PID controller-based load balancing, the max, p95, avg, and p5 almost consolidated into a single line once again.\n\nWhy we built a config aggregator\n\nRobinhood exposes several options for service owners to choose from, and can even apply changes dynamically. Service owners create and update the Robinhood config for their services from within their service directory in the codebase. We then store these settings in our config management service, a convenient library that receives any changes to Robinhood\u2019s config in real-time. However, we cannot simply build and push Robinhood\u2019s mega config regularly from the codebase due to several problems: If a breaking change is introduced by a config push, it's risky to press the rollback button because we don\u2019t know how many other services have also made changes since the last push.\n\nThe team that owns Robinhood is also responsible for each mega config push. This means that the Robinhood team would have to get involved in every breaking config push\u2014which is a waste of engineering time, since most incidents can be resolved by the service owner.\n\nEach push takes hours to deploy to multiple data centers in order to minimize potential risks. To address these problems, we build another small service: the config aggregator.\n\nInstead of storing one Robinhood mega config in the config management service, we break the config into per-service configs. Each per-service config only contains the configuration for that particular service. This way, service owners can update and deploy their changes at any time without worrying about being affected by changes in other services. In the event of a breaking change, service owners can also roll back config pushes or apply fixes during incidents without having to involve the Robinhood team. To simplify the LBS and keep it focused on its primary task, we built another service\u2014the config aggregator\u2014which collects all the per-service configs and construct the mega config for LBS to consume. The config aggregator watches per-service configs and propagates the changes to the mega config in real-time. The config aggregator also provides a tombstone feature to prevent accidental deletion of a service's Robinhood config. When a service owner pushes a change to delete a service from the Robinhood config, the config aggregator puts a tombstone mark on the entry of the service instead of removing it immediately. The actual removal happens after several days. This feature also solves a race condition that could result from the different push cadences between the Robinhood config and other routing configs (e.g., Envoy config). One downside of our config management service is that it's not currently versioned. We periodically backup the mega config in case we need to revert the LBS config back to a known good state.\n\nA quick note on our migration strategy\n\nIt can be risky to switch load balancing strategies all at once. This is why we enable service owners to configure multiple load balancing strategies for a service in Robinhood. The LBS writes the weighted endpoints list generated by different strategies into different entries in the routing database. At Dropbox, we have a percentage-based feature gate, so we implement a mixed strategy where clients use the weighted sum of the weights generated by two load balancing strategies as the endpoint weight. For example, endpoint A might be weighted at 100 based on PID-based load balancing and 200 based on simple round-robin strategy. If we set the feature gate to 30% for PID-based load balancing, the weight of endpoint A becomes 100*0.3 + 200*0.7 = 170. This way, we can ensure that every client sees the same weight assignment for endpoints while gradually migrating to the new load balancing strategy.\n\nWhat we learned\n\nIn designing and implementing Robinhood, we learned several key lessons about what works and what doesn't. By prioritizing simplicity, minimizing client changes, and planning for migration from the outset, we were able to streamline the LBS's development and deployment, and avoid costly pitfalls. Configuration should be as simple as possible. Robinhood introduces many options for services owner to configure. However, for most cases what they need is a provided default setting. A good, simple default config\u2014or even better, zero config\u2014can save tons of engineering time.\n\nRobinhood introduces many options for services owner to configure. However, for most cases what they need is a provided default setting. A good, simple default config\u2014or even better, zero config\u2014can save tons of engineering time. Keep client changes simple, too. It can take several months to roll out changes to internal clients; although most deployments are pushed weekly, many are deployed only once a month, or not at all, for years. We learned that the more changes we could shift to the LBS, the better. For example, we decided early on to use weighted round robin for our client design, and we have not changed it since\u2014which has significantly accelerated our progress. Limiting most of our changes to the LBS also reduces reliability risks. This is because we can roll back changes in the LBS within minutes if necessary.\n\nIt can take several months to roll out changes to internal clients; although most deployments are pushed weekly, many are deployed only once a month, or not at all, for years. We learned that the more changes we could shift to the LBS, the better. For example, we decided early on to use weighted round robin for our client design, and we have not changed it since\u2014which has significantly accelerated our progress. Limiting most of our changes to the LBS also reduces reliability risks. This is because we can roll back changes in the LBS within minutes if necessary. Migration should be planned at project design phase. A migration takes a huge amount of engineering time. There are also reliability risks to consider. It\u2019s not fun, but it\u2019s important work. When designing a new service, think about how to smoothly migrate existing use cases onto the new service as early as possible. The more you ask of service owners, the more migration becomes a nightmare\u2014especially for fundamental infrastructure components. The migration process for Robinhood was not well-designed from the very beginning, so we ended up spending much more time than expected reimplementing the process and redesigning the configuration. The amount of engineering time required for a migration should be a key metric for success. After roughly a year in production, it\u2019s safe to say that the latest iteration of Robinhood effectively addresses our long-standing load balancing challenges. The PID controller algorithm at its core has yielded promising results\u2014showcasing significant performance improvements in our largest services\u2014and we\u2019ve gained valuable insights into the design and operation of load balancing services at Dropbox-scale. Special thanks to Mikhail Dolinin, Sai Teja Duthuluri, Nikitha Girish, Jared Hance, Itsik Hefez, Pardhu Konakanchi, Andrew Lazarev, Miguel Michel, Ruslan Nigmatullin, Pranesh Pandurangan, Yi-Shu Tai, Jialin Xu, and all past and present members of the runtime team and services platform teams for their contributions.", "label": 0}
{"title": "How I got Into Personal Websites", "url": "https://lifeofpablo.com/blog/how-i-got-into-personal-websites", "content": "I've had a presence since I was a young kid. I've been very fortunate to be around computers since i was three with these interactions been in school.\n\nHaving access to computers was game changing for me and has helped me. I would say I signed up for MySpace when I was ten. I know I am such a rule breaker. When I discovered personal websites, my mind exploded . . . metaphorically.\n\nCuriosity didn't kill the cat\n\nI remember when my parents upgraded from dial-up to DSL. I remember the lady at the phone company explaining all the details of the features included in our internet plan. Something that struck me was the FTP storage space. I thought, \"Wait, I can create a my website and have it hosted for free?\" This was in 2003 when I started my first personal website. I remember using basic HTML and clip art. I cringe just thinking about it. This was the stepping stone for me building and hosting websites. Using FTP was pretty advanced for an eight year old. I would stay up late just tinkering my site to make it perfect with poor HTML skills. It worked somehow?\n\nExpressing Myself\n\nI've always wanted to find ways to express myself. Since I was big into computers and tech, I figured I would use the website I built to share the things I enjoyed. I wanted to share who I was as a person. I had an idea who I was but I was still developing.\n\nInspired by others.\n\nI would look at other people who would blog and that would make me super gitty. I remember stumbling upon Matt Mullenweb back in 2003 and being inspired by him. He and many others taught me a lot about blogging. Learning by (browsing the internet) doing.\n\nBeing Resourceful.\n\nI would create new websites over the years. I couldn't afford to a pay for a website domain because I was eight years old and I wasn't going to ask my parents to spend money on something when I was more than grateful to have a computer and fast internet. I would use any free service I could until I could pay for it on my own.\n\nServices I'd use\n\nI would use a site in 2010 called Altervista where I officially used Wordpress for the first time. I made a big leap from HTML to the most popular CMS (of course I had no idea at the time). I decided this was the moment I would start expanding from a basic site to start blogging even if the posts had a few sentences. I used:\n\nAltervista\n\nDynDNS\n\nWordpress.com\n\nFreewebhosting\n\nToday\n\nOften, I really feel like that 8 year old who was discovering how to build things on the web.", "label": 1}
{"title": "Read This Will Be Fun \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/21/read-this-will-be-fun/", "content": "The Princess Bride meets People We Meet on Vacation in this cozy quest romantasy about a group of friends who once defended their magical land together but haven\u2019t spoken since, reuniting to attend a royal wedding, and ending up on a new adventure to save the realm\u2014and hopefully themselves.\n\nTen years ago, they saved the realm. It ruined their lives.\n\nEveryone in Mythria knows the story of how best friends Beatrice and Elowen, handsome ex-bandit Clare, and valiant leader Galwell the Great defended the land from darkness. It\u2019s a tale beloved by all\u2014except the former heroes. They haven\u2019t spoken in a decade, devastated by what their quest cost them.\n\nBut when they receive an invitation to the queen of Mythria\u2019s wedding, it\u2019s a summons they can\u2019t refuse . . . and a reunion for the ages, with Clare secretly not over his long-ago fling with Beatrice, Beatrice fighting the guilt she feels over how everything ended, Elowen unprepared for the return of her former flame (the cunning Vandra), and all of them lost without Galwell\u2019s presence. And if reuniting with old friends and lovers wasn\u2019t perilous enough, dark forces from their past have returned, plotting a domination that only Mythria\u2019s one-time defenders can stop. Maybe.\n\nDusting off old weapons and old instincts, they face undead nemeses, crystal caves, enchanted swords, coffee shops, games of magical Truth or Dare, and, hardest of all, their past\u2014rife with wounds never healed and romances never forgotten.\n\nThis time around, will their story end in happily ever after?", "label": 0}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2023", "content": "en\n\nWhat a year 2023 has been! I've learned so much about myself and hit so many milestones. This will be my first year in review I ever post. I've written brief year in reviews on paper in the past. My year in review for 2023.\n\nI started a Company\n\nI filed and started my own company called, Pabs Tech, LLC. This has been something that had in the back of my mind for a long time. I only did very tiny projects with my company.\n\nI'd like to have more projects where I can help others suceed in there projects.\n\nHelping Friends\n\nI started to help my friends build a small game.\n\nHelp friends with their projects.\n\nBlogging\n\n100 Blog posts! This blog post is number 100 for 2023. This is the most I've written on a personal site. I'm really proud of myself. This year I got back into blogging. I am so happy I did. I wrote a blog post on why I blog.\n\nThe IndieWeb\n\nI joined the IndieWeb this year! I've meet some amazing people who have motivated me in so many ways. I find them inspiring. This is a great communithy to join. I only wish I would have done it sooner.\n\nI got to meet up with an IndieWeb community member while I was visiting Los Angeles.\n\nI've attended almost every Homebrew Website Club (remote) since March.\n\nI started building an IndieAuth extension for Datenstrom Yellow\n\nPodcast\n\nStarted a podcast on my website called Pablo's Thought Podcast on things that I want to share.\n\nPhotography\n\nI really explored photography and videography. I made some major leaps and bounds in both analog and digital photography. My camera visited new cities.\n\nI posted a Photography Year in Review of my favorite shots.\n\nOutdoors\n\nI've gone on 10 hikes this year! Darn it! I live in California and I should make more use of the natural facilities.\n\nTraveling Around\n\nI did some traveling this year. I've been telling myself that I need to travel more of the United States this year even if I have visited these places before. I went to new places in California and went to places in California I've been to in the past. I even went to the same place on the East Coast Again.\n\nI visited two Buzzfeed Offices in Los Angeles and New York City.\n\nI attended NYC Climate Week for the first time\n\nI visited wine country for the first ime\n\nCities:\n\nSan Francisco\n\nLos Angeles\n\nMexico City\n\nNapa Valley (Wine Country)\n\nNew York\n\nOaxaca, Mexico\n\nSan Luis Reservoir, California\n\nThings I have learned in 2023\n\nI'm learned to start loving myself. I started going to therapy. I'm starting to unravel and start understanding why I've seen myself in such a negative light.\n\nIt's okay to make mistakes. Learning from them is key to improve.\n\nFeel less guilty in enjoying time for myself.\n\nBeing active consistently is key. It helps me stay motivated knowing I don't always have to go hard. Consistency is key.\n\nWhat about 2024 ?\n\nI have a lot planned for 2024. I will continue to travel with friends but also go places on my own. I will need to go on more artist dates.\n\nI'm planning to host an IndieWebCamp in Sacramento.\n\nI'd like to run a marathon.\n\nI'd like to learn how to dance.\n\nHere's to a great 2024!", "label": 1}
{"title": "The ultimate guide to developer happiness", "url": "https://github.blog/engineering/engineering-principles/the-ultimate-guide-to-developer-happiness/", "content": "In today\u2019s rapidly evolving landscape, where AI is reshaping industries and transforming workflows, the role of developers has never been more critical. As business leaders, fostering an environment where developers feel valued, motivated, and empowered is essential to harnessing their full potential and keeping your business profitable and innovative.\n\nIn this blog post, we\u2019ll explore actionable tips and strategies to supercharge developer happiness, ensuring your team remains productive, engaged, and ahead of the AI curve. We\u2019ll walk you through ways to secure your code with AI, how to increase productivity with a strong developer experience, and, of course, invite you to join us at GitHub Universe 2024 to see the very best of the latest AI tooling in action.\n\nBoost productivity with a great developer experience\n\nDeveloper experience is more than just a buzzword\u2014it\u2019s a critical factor in driving productivity and collaboration within software development teams. A seamless developer experience allows developers to get into the flow state more easily, where their productivity and creativity can peak. This flow state\u2014characterized by uninterrupted concentration and a deep sense of involvement in the task\u2014is crucial for tackling complex coding challenges.\n\nThis work environment needs to be built intentionally, and the research backs it up. Developers who carve out time for deep work enjoy 50% more productivity, while those that get work they find engaging are 30% more productive.\n\nHow does this impact businesses? Well, because a developer that can significantly reduce their context-switching and mental load can also produce code faster and at a higher quality.\n\nWhen developers understand their code, they\u2019re 42% more productive. When developers are able to get faster turnaround times, they are 20% more innovative. These are tangible, individual benefits that in turn directly impact the output of developer teams.\n\nNow is the time for leaders to invest in creating a great developer experience. By prioritizing the developer experience, you\u2019re setting your team up to harness the full potential of the latest AI and platform engineering advances, ensuring your business stays ahead of the curve. Curious to learn more? Then dive into how a great developer experience fuels productivity with our latest research.\n\nUse AI to secure your code\n\nHistorically, developers and security teams have found themselves at odds due to competing business goals. Shifting security left incorporates security earlier in the software development lifecycle, but in practice it has primarily shifted responsibility to developers without necessarily giving them the required expertise.\n\nThis, combined with the context switching inherent in development work, makes addressing security concerns particularly challenging. With AI, developers now have powerful tools at their disposal to enhance code security. AI can:\n\nImprove detection rates\n\nProvide near-instant fixes with context\n\nEnable application security (AppSec) at scale\n\nThese three improvements make it easier for developers to integrate robust security measures without sacrificing productivity, and transform the relationship between developers and security teams into a collaborative partnership.\n\nIntroducing a new security tool doesn\u2019t have to be a daunting task either. By following a few simple steps, organizations can ensure a smooth transition and broad adoption.\n\nDocument the tool\u2019s features and usage to set the foundation and set realistic expectations to help align goals across teams.\n\nthe tool\u2019s features and usage to set the foundation and set realistic expectations to help align goals across teams. Recognize and celebrate successes to showcase the value of the new tool.\n\nand celebrate successes to showcase the value of the new tool. Adopt a go-with-the-flow approach and organize hackathons to further drive engagement and interest.\n\na go-with-the-flow approach and organize hackathons to further drive engagement and interest. Listen to developer feedback continuously improve and refine security practices.\n\nAI-powered security tools not only enhance the efficiency and effectiveness of AppSec, but also empower developers to take a proactive role in securing their code. This shift not only improves overall security posture, but also fosters a culture of shared responsibility and continuous learning, ultimately leading to more secure and resilient applications.\n\nSee exactly why security should be built into the developer workflow. \ud83d\udc47\n\nCustomize your LLMs\n\nOrganizations that take AI a step further and customize their AI tools are poised to lead the pack.\n\nLarge language models (LLMs) are trained on vast amounts of text data and can perform a variety of natural language processing tasks like translation, summarization, question-answering, and text generation. Customizing a pre-trained LLM goes beyond mere training\u2014it involves adapting the model to perform specific tasks relevant to the organization\u2019s needs. This level of customization helps developers maintain their flow state and significantly boost productivity and efficiency.\n\nCustomization techniques like retrieval-augmented generation (RAG), in-context learning, and fine-tuning enable LLMs to deliver more accurate and contextually appropriate responses:\n\nRAG combines retrieval-based and generation-based approaches in natural language processing. It enhances LLMs by integrating information retrieval techniques, where relevant documents or snippets are retrieved from a vector database to assist in generating more accurate and contextually appropriate responses. This approach allows the model to access and utilize external knowledge, making the generated output more informed and relevant to the user\u2019s query.\n\ncombines retrieval-based and generation-based approaches in natural language processing. It enhances LLMs by integrating information retrieval techniques, where relevant documents or snippets are retrieved from a vector database to assist in generating more accurate and contextually appropriate responses. This approach allows the model to access and utilize external knowledge, making the generated output more informed and relevant to the user\u2019s query. In-context learning refers to a model\u2019s ability to adapt and respond to new tasks or inputs based on the context provided in the input prompt without requiring additional training. The model leverages its pre-trained knowledge and the context given in the input to perform tasks effectively.\n\nrefers to a model\u2019s ability to adapt and respond to new tasks or inputs based on the context provided in the input prompt without requiring additional training. The model leverages its pre-trained knowledge and the context given in the input to perform tasks effectively. Fine-tuning, on the other hand, is a process in which an LLM is further trained on a specific dataset to adapt it to a particular task or domain. During fine-tuning, the model\u2019s parameters are adjusted based on the new dataset, which typically involves supervised learning with labeled data. This process allows the model to specialize and improve its performance on specific tasks, (such as text classification, question answering, or machine translation), by leveraging the general knowledge acquired during its initial pre-training phase.\n\nBy implementing these customization strategies, businesses can unlock the full potential of their AI tools. Customized LLMs not only improve developer productivity\u2014they also enhance the quality and relevance of AI-generated content.\n\nPrepare your repository for teamwork\n\nFostering collaboration doesn\u2019t just make software development faster, it also helps teams build better products and boost job satisfaction. By making your repository as collaborative as possible, you\u2019ll optimize success. This includes focusing on:\n\nRepository settings : properly configuring repository settings to control visibility, access, and contribution workflows lays the foundation for collaboration.\n\n: properly configuring repository settings to control visibility, access, and contribution workflows lays the foundation for collaboration. Repository contents : including essential files like README.md, LICENSE.md, CONTRIBUTING.md, CODEOWNERS, and CODE_OF_CONDUCT.md helps collaborators understand the project, its purpose, and how to contribute.\n\n: including essential files like README.md, LICENSE.md, CONTRIBUTING.md, CODEOWNERS, and CODE_OF_CONDUCT.md helps collaborators understand the project, its purpose, and how to contribute. Automation and checks : implementing automation tools such as linters, continuous integration (CI), and continuous deployment (CD) pipelines streamlines the development process, ensures code quality, and enables immediate feedback.\n\n: implementing automation tools such as linters, continuous integration (CI), and continuous deployment (CD) pipelines streamlines the development process, ensures code quality, and enables immediate feedback. Security practices : enforcing role-based access control, managing secrets securely, and scanning code for vulnerabilities can foster trust and protect the project from vulnerabilities.\n\n: enforcing role-based access control, managing secrets securely, and scanning code for vulnerabilities can foster trust and protect the project from vulnerabilities. Issue templates : providing structured issue templates guides contributors in providing necessary information and context when reporting bugs.\n\n: providing structured issue templates guides contributors in providing necessary information and context when reporting bugs. Community engagement: engaging with the project\u2019s community through meetups, project blogs, discussions, and other channels fosters belonging and builds relationships.\n\nInvest in your team\u2019s learning opportunities\n\nWhen you signal to your team that you value their career growth and exposure to learning opportunities, it can boost happiness and job satisfaction, leading to increased productivity, collaboration, and better problem solving.\n\nEncouraging your developer teams to attend conferences like GitHub Universe 2024 is a strategic investment in their professional growth and your business\u2019 success. Our global developer event provides an unparalleled platform for the best in software development to gather and expand their knowledge, stay updated on the latest AI-powered tools, and bring fresh ideas back to their teams.\n\nHere are a few highlights of what you and your team can expect:\n\nHelp your developers get in the flow and stay there with sessions, demos, panels, and more on the powerful tools and techniques that enhance productivity and satisfaction.\n\nand stay there with sessions, demos, panels, and more on the powerful tools and techniques that enhance productivity and satisfaction. Connect with other technical leaders to share experiences, challenges, and best practices. Expand your network with valuable industry contacts.\n\nto share experiences, challenges, and best practices. Expand your network with valuable industry contacts. Get a first look at GitHub\u2019s product roadmap and see how upcoming features and enhancements can help you stay ahead in a competitive landscape.\n\nand see how upcoming features and enhancements can help you stay ahead in a competitive landscape. Gain technical skills with GitHub certifications and workshops designed to enhance your expertise in a rapidly evolving industry.\n\nwith GitHub certifications and workshops designed to enhance your expertise in a rapidly evolving industry. Learn the latest on GitHub Copilot and stay ahead with the latest coding practices and techniques.\n\nGet your tickets today. You can take advantage of our group discount and get four tickets for the price of three. (That\u2019s a 25% savings!)\n\nIf you\u2019re flying solo, you can also use our Early Bird discount and save 20% off one in-person ticket, only until September 3.\n\nReach new levels of creativity and efficiency\n\nIncorporating these five business strategies can transform your development process and increase developer happiness. By investing in these areas, you empower your team, foster a culture of continuous learning, and position your organization for success in the rapidly evolving tech landscape.", "label": 0}
{"title": "Introducing Netflix\u2019s Key-Value Data Abstraction Layer", "url": "https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30?source=collection_home---4------14-----------------------", "content": "Introducing Netflix\u2019s Key-Value Data Abstraction Layer Netflix Technology Blog 13 min read \u00b7 Sep 18, 2024 -- 10 Listen Share\n\nVidhya Arvind, Rajasekhar Ummadisetty, Joey Lynch, Vinay Chella\n\nIntroduction\n\nAt Netflix our ability to deliver seamless, high-quality, streaming experiences to millions of users hinges on robust, global backend infrastructure. Central to this infrastructure is our use of multiple online distributed databases such as Apache Cassandra, a NoSQL database known for its high availability and scalability. Cassandra serves as the backbone for a diverse array of use cases within Netflix, ranging from user sign-ups and storing viewing histories to supporting real-time analytics and live streaming.\n\nOver time as new key-value databases were introduced and service owners launched new use cases, we encountered numerous challenges with datastore misuse. Firstly, developers struggled to reason about consistency, durability and performance in this complex global deployment across multiple stores. Second, developers had to constantly re-learn new data modeling practices and common yet critical data access patterns. These include challenges with tail latency and idempotency, managing \u201cwide\u201d partitions with many rows, handling single large \u201cfat\u201d columns, and slow response pagination. Additionally, the tight coupling with multiple native database APIs \u2014 APIs that continually evolve and sometimes introduce backward-incompatible changes \u2014 resulted in org-wide engineering efforts to maintain and optimize our microservice\u2019s data access.\n\nTo overcome these challenges, we developed a holistic approach that builds upon our Data Gateway Platform. This approach led to the creation of several foundational abstraction services, the most mature of which is our Key-Value (KV) Data Abstraction Layer (DAL). This abstraction simplifies data access, enhances the reliability of our infrastructure, and enables us to support the broad spectrum of use cases that Netflix demands with minimal developer effort.\n\nIn this post, we dive deep into how Netflix\u2019s KV abstraction works, the architectural principles guiding its design, the challenges we faced in scaling diverse use cases, and the technical innovations that have allowed us to achieve the performance and reliability required by Netflix\u2019s global operations.\n\nThe Key-Value Service\n\nThe KV data abstraction service was introduced to solve the persistent challenges we faced with data access patterns in our distributed databases. Our goal was to build a versatile and efficient data storage solution that could handle a wide variety of use cases, ranging from the simplest hashmaps to more complex data structures, all while ensuring high availability, tunable consistency, and low latency.\n\nData Model\n\nAt its core, the KV abstraction is built around a two-level map architecture. The first level is a hashed string ID (the primary key), and the second level is a sorted map of a key-value pair of bytes. This model supports both simple and complex data models, balancing flexibility and efficiency.\n\nHashMap<String, SortedMap<Bytes, Bytes>>\n\nFor complex data models such as structured Records or time-ordered Events , this two-level approach handles hierarchical structures effectively, allowing related data to be retrieved together. For simpler use cases, it also represents flat key-value Maps (e.g. id \u2192 {\"\" \u2192 value} ) or named Sets (e.g. id \u2192 {key \u2192 \"\"} ). This adaptability allows the KV abstraction to be used in hundreds of diverse use cases, making it a versatile solution for managing both simple and complex data models in large-scale infrastructures like Netflix.\n\nThe KV data can be visualized at a high level, as shown in the diagram below, where three records are shown.\n\nmessage Item (\n\nBytes key,\n\nBytes value,\n\nMetadata metadata,\n\nInteger chunk\n\n)\n\nDatabase Agnostic Abstraction\n\nThe KV abstraction is designed to hide the implementation details of the underlying database, offering a consistent interface to application developers regardless of the optimal storage system for that use case. While Cassandra is one example, the abstraction works with multiple data stores like EVCache, DynamoDB, RocksDB, etc\u2026\n\nFor example, when implemented with Cassandra, the abstraction leverages Cassandra\u2019s partitioning and clustering capabilities. The record ID acts as the partition key, and the item key as the clustering column:\n\nThe corresponding Data Definition Language (DDL) for this structure in Cassandra is:\n\nCREATE TABLE IF NOT EXISTS <ns>.<table> (\n\nid text,\n\nkey blob,\n\nvalue blob,\n\nvalue_metadata blob,\n\n\n\nPRIMARY KEY (id, key))\n\nWITH CLUSTERING ORDER BY (key <ASC|DESC>)\n\nNamespace: Logical and Physical Configuration\n\nA namespace defines where and how data is stored, providing logical and physical separation while abstracting the underlying storage systems. It also serves as central configuration of access patterns such as consistency or latency targets. Each namespace may use different backends: Cassandra, EVCache, or combinations of multiple. This flexibility allows our Data Platform to route different use cases to the most suitable storage system based on performance, durability, and consistency needs. Developers just provide their data problem rather than a database solution!\n\nIn this example configuration, the ngsegment namespace is backed by both a Cassandra cluster and an EVCache caching layer, allowing for highly durable persistent storage and lower-latency point reads.\n\n\"persistence_configuration\":[\n\n{\n\n\"id\":\"PRIMARY_STORAGE\",\n\n\"physical_storage\": {\n\n\"type\":\"CASSANDRA\",\n\n\"cluster\":\"cassandra_kv_ngsegment\",\n\n\"dataset\":\"ngsegment\",\n\n\"table\":\"ngsegment\",\n\n\"regions\": [\"us-east-1\"],\n\n\"config\": {\n\n\"consistency_scope\": \"LOCAL\",\n\n\"consistency_target\": \"READ_YOUR_WRITES\"\n\n}\n\n}\n\n},\n\n{\n\n\"id\":\"CACHE\",\n\n\"physical_storage\": {\n\n\"type\":\"CACHE\",\n\n\"cluster\":\"evcache_kv_ngsegment\"\n\n},\n\n\"config\": {\n\n\"default_cache_ttl\": 180s\n\n}\n\n}\n\n]\n\n\n\nKey APIs of the KV Abstraction\n\nTo support diverse use-cases, the KV abstraction provides four basic CRUD APIs:\n\nPutItems \u2014 Write one or more Items to a Record\n\nThe PutItems API is an upsert operation, it can insert new data or update existing data in the two-level map structure.\n\nmessage PutItemRequest (\n\nIdempotencyToken idempotency_token,\n\nstring namespace,\n\nstring id,\n\nList<Item> items\n\n)\n\nAs you can see, the request includes the namespace, Record ID, one or more items, and an idempotency token to ensure retries of the same write are safe. Chunked data can be written by staging chunks and then committing them with appropriate metadata (e.g. number of chunks).\n\nGetItems \u2014 Read one or more Items from a Record\n\nThe GetItems API provides a structured and adaptive way to fetch data using ID, predicates, and selection mechanisms. This approach balances the need to retrieve large volumes of data while meeting stringent Service Level Objectives (SLOs) for performance and reliability.\n\nmessage GetItemsRequest (\n\nString namespace,\n\nString id,\n\nPredicate predicate,\n\nSelection selection,\n\nMap<String, Struct> signals\n\n)\n\nThe GetItemsRequest includes several key parameters:\n\nNamespace : Specifies the logical dataset or table\n\n: Specifies the logical dataset or table Id : Identifies the entry in the top-level HashMap\n\n: Identifies the entry in the top-level HashMap Predicate : Filters the matching items and can retrieve all items ( match_all ), specific items ( match_keys ), or a range ( match_range )\n\n: Filters the matching items and can retrieve all items ( ), specific items ( ), or a range ( ) Selection : Narrows returned responses for example page_size_bytes for pagination, item_limit for limiting the total number of items across pages and include / exclude to include or exclude large values from responses\n\n: Narrows returned responses for example for pagination, for limiting the total number of items across pages and / to include or exclude large values from responses Signals: Provides in-band signaling to indicate client capabilities, such as supporting client compression or chunking.\n\nThe GetItemResponse message contains the matching data:\n\nmessage GetItemResponse (\n\nList<Item> items,\n\nOptional<String> next_page_token\n\n)\n\nItems : A list of retrieved items based on the Predicate and Selection defined in the request.\n\n: A list of retrieved items based on the and defined in the request. Next Page Token: An optional token indicating the position for subsequent reads if needed, essential for handling large data sets across multiple requests. Pagination is a critical component for efficiently managing data retrieval, especially when dealing with large datasets that could exceed typical response size limits.\n\nDeleteItems \u2014 Delete one or more Items from a Record\n\nThe DeleteItems API provides flexible options for removing data, including record-level, item-level, and range deletes \u2014 all while supporting idempotency.\n\nmessage DeleteItemsRequest (\n\nIdempotencyToken idempotency_token,\n\nString namespace,\n\nString id,\n\nPredicate predicate\n\n)\n\n\n\nJust like in the GetItems API, the Predicate allows one or more Items to be addressed at once:\n\nRecord-Level Deletes (match_all) : Removes the entire record in constant latency regardless of the number of items in the record.\n\n: Removes the entire record in constant latency regardless of the number of items in the record. Item-Range Deletes (match_range) : This deletes a range of items within a Record. Useful for keeping \u201cn-newest\u201d or prefix path deletion.\n\n: This deletes a range of items within a Record. Useful for keeping \u201cn-newest\u201d or prefix path deletion. Item-Level Deletes (match_keys): Deletes one or more individual items.\n\nSome storage engines (any store which defers true deletion) such as Cassandra struggle with high volumes of deletes due to tombstone and compaction overhead. Key-Value optimizes both record and range deletes to generate a single tombstone for the operation \u2014 you can learn more about tombstones in About Deletes and Tombstones.\n\nItem-level deletes create many tombstones but KV hides that storage engine complexity via TTL-based deletes with jitter. Instead of immediate deletion, item metadata is updated as expired with randomly jittered TTL applied to stagger deletions. This technique maintains read pagination protections. While this doesn\u2019t completely solve the problem it reduces load spikes and helps maintain consistent performance while compaction catches up. These strategies help maintain system performance, reduce read overhead, and meet SLOs by minimizing the impact of deletes.\n\nComplex Mutate and Scan APIs\n\nBeyond simple CRUD on single Records, KV also supports complex multi-item and multi-record mutations and scans via MutateItems and ScanItems APIs. PutItems also supports atomic writes of large blob data within a single Item via a chunked protocol. These complex APIs require careful consideration to ensure predictable linear low-latency and we will share details on their implementation in a future post.\n\nDesign Philosophies for reliable and predictable performance\n\nIdempotency to fight tail latencies\n\nTo ensure data integrity the PutItems and DeleteItems APIs use idempotency tokens, which uniquely identify each mutative operation and guarantee that operations are logically executed in order, even when hedged or retried for latency reasons. This is especially crucial in last-write-wins databases like Cassandra, where ensuring the correct order and de-duplication of requests is vital.\n\nIn the Key-Value abstraction, idempotency tokens contain a generation timestamp and random nonce token. Either or both may be required by backing storage engines to de-duplicate mutations.\n\nmessage IdempotencyToken (\n\nTimestamp generation_time,\n\nString token\n\n)\n\nAt Netflix, client-generated monotonic tokens are preferred due to their reliability, especially in environments where network delays could impact server-side token generation. This combines a client provided monotonic generation_time timestamp with a 128 bit random UUID token . Although clock-based token generation can suffer from clock skew, our tests on EC2 Nitro instances show drift is minimal (under 1 millisecond). In some cases that require stronger ordering, regionally unique tokens can be generated using tools like Zookeeper, or globally unique tokens such as a transaction IDs can be used.\n\nThe following graphs illustrate the observed clock skew on our Cassandra fleet, suggesting the safety of this technique on modern cloud VMs with direct access to high-quality clocks. To further maintain safety, KV servers reject writes bearing tokens with large drift both preventing silent write discard (write has timestamp far in past) and immutable doomstones (write has a timestamp far in future) in storage engines vulnerable to those.\n\nHandling Large Data through Chunking\n\nKey-Value is also designed to efficiently handle large blobs, a common challenge for traditional key-value stores. Databases often face limitations on the amount of data that can be stored per key or partition. To address these constraints, KV uses transparent chunking to manage large data efficiently.\n\nFor items smaller than 1 MiB, data is stored directly in the main backing storage (e.g. Cassandra), ensuring fast and efficient access. However, for larger items, only the id, key, and metadata are stored in the primary storage, while the actual data is split into smaller chunks and stored separately in chunk storage. This chunk storage can also be Cassandra but with a different partitioning scheme optimized for handling large values. The idempotency token ties all these writes together into one atomic operation.\n\nBy splitting large items into chunks, we ensure that latency scales linearly with the size of the data, making the system both predictable and efficient. A future blog post will describe the chunking architecture in more detail, including its intricacies and optimization strategies.\n\nClient-Side Compression\n\nThe KV abstraction leverages client-side payload compression to optimize performance, especially for large data transfers. While many databases offer server-side compression, handling compression on the client side reduces expensive server CPU usage, network bandwidth, and disk I/O. In one of our deployments, which helps power Netflix\u2019s search, enabling client-side compression reduced payload sizes by 75%, significantly improving cost efficiency.\n\nSmarter Pagination\n\nWe chose payload size in bytes as the limit per response page rather than the number of items because it allows us to provide predictable operation SLOs. For instance, we can provide a single-digit millisecond SLO on a 2 MiB page read. Conversely, using the number of items per page as the limit would result in unpredictable latencies due to significant variations in item size. A request for 10 items per page could result in vastly different latencies if each item was 1 KiB versus 1 MiB.\n\nUsing bytes as a limit poses challenges as few backing stores support byte-based pagination; most data stores use the number of results e.g. DynamoDB and Cassandra limit by number of items or rows. To address this, we use a static limit for the initial queries to the backing store, query with this limit, and process the results. If more data is needed to meet the byte limit, additional queries are executed until the limit is met, the excess result is discarded and a page token is generated.\n\nThis static limit can lead to inefficiencies, one large item in the result may cause us to discard many results, while small items may require multiple iterations to fill a page, resulting in read amplification. To mitigate these issues, we implemented adaptive pagination which dynamically tunes the limits based on observed data.\n\nAdaptive Pagination\n\nWhen an initial request is made, a query is executed in the storage engine, and the results are retrieved. As the consumer processes these results, the system tracks the number of items consumed and the total size used. This data helps calculate an approximate item size, which is stored in the page token. For subsequent page requests, this stored information allows the server to apply the appropriate limits to the underlying storage, reducing unnecessary work and minimizing read amplification.\n\nWhile this method is effective for follow-up page requests, what happens with the initial request? In addition to storing item size information in the page token, the server also estimates the average item size for a given namespace and caches it locally. This cached estimate helps the server set a more optimal limit on the backing store for the initial request, improving efficiency. The server continuously adjusts this limit based on recent query patterns or other factors to keep it accurate. For subsequent pages, the server uses both the cached data and the information in the page token to fine-tune the limits.\n\nIn addition to adaptive pagination, a mechanism is in place to send a response early if the server detects that processing the request is at risk of exceeding the request\u2019s latency SLO.\n\nFor example, let us assume a client submits a GetItems request with a per-page limit of 2 MiB and a maximum end-to-end latency limit of 500ms. While processing this request, the server retrieves data from the backing store. This particular record has thousands of small items so it would normally take longer than the 500ms SLO to gather the full page of data. If this happens, the client would receive an SLO violation error, causing the request to fail even though there is nothing exceptional. To prevent this, the server tracks the elapsed time while fetching data. If it determines that continuing to retrieve more data might breach the SLO, the server will stop processing further results and return a response with a pagination token.\n\nThis approach ensures that requests are processed within the SLO, even if the full page size isn\u2019t met, giving clients predictable progress. Furthermore, if the client is a gRPC server with proper deadlines, the client is smart enough not to issue further requests, reducing useless work.\n\nIf you want to know more, the How Netflix Ensures Highly-Reliable Online Stateful Systems article talks in further detail about these and many other techniques.\n\nSignaling\n\nKV uses in-band messaging we call signaling that allows the dynamic configuration of the client and enables it to communicate its capabilities to the server. This ensures that configuration settings and tuning parameters can be exchanged seamlessly between the client and server. Without signaling, the client would need static configuration \u2014 requiring a redeployment for each change \u2014 or, with dynamic configuration, would require coordination with the client team.\n\nFor server-side signals, when the client is initialized, it sends a handshake to the server. The server responds back with signals, such as target or max latency SLOs, allowing the client to dynamically adjust timeouts and hedging policies. Handshakes are then made periodically in the background to keep the configuration current. For client-communicated signals, the client, along with each request, communicates its capabilities, such as whether it can handle compression, chunking, and other features.\n\nKV Usage @ Netflix\n\nThe KV abstraction powers several key Netflix use cases, including:\n\nStreaming Metadata : High-throughput, low-latency access to streaming metadata, ensuring personalized content delivery in real-time.\n\n: High-throughput, low-latency access to streaming metadata, ensuring personalized content delivery in real-time. User Profiles : Efficient storage and retrieval of user preferences and history, enabling seamless, personalized experiences across devices.\n\n: Efficient storage and retrieval of user preferences and history, enabling seamless, personalized experiences across devices. Messaging : Storage and retrieval of push registry for messaging needs, enabling the millions of requests to flow through.\n\n: Storage and retrieval of push registry for messaging needs, enabling the millions of requests to flow through. Real-Time Analytics: This persists large-scale impression and provides insights into user behavior and system performance, moving data from offline to online and vice versa.\n\nFuture Enhancements\n\nLooking forward, we plan to enhance the KV abstraction with:\n\nLifecycle Management : Fine-grained control over data retention and deletion.\n\n: Fine-grained control over data retention and deletion. Summarization : Techniques to improve retrieval efficiency by summarizing records with many items into fewer backing rows.\n\n: Techniques to improve retrieval efficiency by summarizing records with many items into fewer backing rows. New Storage Engines : Integration with more storage systems to support new use cases.\n\n: Integration with more storage systems to support new use cases. Dictionary Compression: Further reducing data size while maintaining performance.\n\nConclusion\n\nThe Key-Value service at Netflix is a flexible, cost-effective solution that supports a wide range of data patterns and use cases, from low to high traffic scenarios, including critical Netflix streaming use-cases. The simple yet robust design allows it to handle diverse data models like HashMaps, Sets, Event storage, Lists, and Graphs. It abstracts the complexity of the underlying databases from our developers, which enables our application engineers to focus on solving business problems instead of becoming experts in every storage engine and their distributed consistency models. As Netflix continues to innovate in online datastores, the KV abstraction remains a central component in managing data efficiently and reliably at scale, ensuring a solid foundation for future growth.\n\nAcknowledgments: Special thanks to our stunning colleagues who contributed to Key Value\u2019s success: William Schor, Mengqing Wang, Chandrasekhar Thumuluru, Rajiv Shringi, John Lu, George Cambell, Ammar Khaku, Jordan West, Chris Lohfink, Matt Lehman, and the whole online datastores team (ODS, f.k.a CDE).", "label": 0}
{"title": "Re: 15 Books with the Most Impact", "url": "https://lifeofpablo.com/blog/re-15-books-with-the-most-impact", "content": "Re: 15 Books with the Most Impact\n\nBooks\n\nThis post was written in English (en_US).\n\nTracy nominated me a while ago (more like July) to share my list(s) of books that have impacted me. Reiterating what Tracy wrote, many of these books had their moment with me and others still hold strong to me.\n\nI won't go too in-depth for some of these for various reasons. I also don't wan't to give too many spoilers.\n\nBooks:\n\nA Thousand Splendid Suns - Khaled Hosseini\n\nMaus / Maus II - Art Spiegelman\n\nA graphic novel where Spiegelman interviews his father about his experiences as a Polish Jew and Holocaust survivor. *I really enjoyed the various themes in the book.\n\nThis is a must read\n\n1984 - George Orwell\n\nThis taught me many lessons and how eerie this period of time is currently. I'm pretty scared for what's to come.\n\nThe Odyssey - Homer\n\nMany of the themes taught me some important lessons.\n\nIn The Hot Zone - Kevin Sites\n\nThis wasn't my favorite book but it made me aware of various wars and situations happening beyond Iraq.\n\nLady of the Lake - Sir Walter Scott\n\nYeruldelgger - Ian Manook (French)\n\nThis book me realize I can actually read French.\n\nRyan White My Own Story - Ryan White\n\nRyan White has always been an inspiration to me as I have in various periods of my life been involved with bringing awareness about HIV/AIDS. It fascinated me how well-versed Ryan was and his outlook on life as he was very unlucky getting the virus from tainted blood products.\n\nLe Gone du Cha\u00e2ba - Azouz Begag (French)\n\nLe Gone du Cha\u00e2ba (The Kid of the Chaaba), translated into English as Shantytown Kid by Naima Wolf.\n\nI love this book as it shows the struggles of balancing two different cultures - the country you live in and the culture at home. This book understood me in various ways as I struggled to balance US life and my Mexican culture.\n\nMy French Professor gave me this book as a graduation gift.\n\nRemaking Manhood: Stories From the Front Lines of Change - Mark Greene\n\nI read this book right before I graduated college. Being a male teacher is very important as many students don't have a good male role model at home. I made it a goal in my teaching to help my students not fall for certain behaviors.\n\nThis book also helped me reevaluate my relationships and look deep inside of me.\n\nHere's a good link related to Remaking Manhood\n\nWinning the War On War - Joshua S. Goldstein\n\nI minored in International Studies and this book really opened me up to deeper issues that continue to exist.\n\nConstructive Living - David K. Reynolds\n\nAlthough this book wasn't the solution to many of my problems during some very dark times, it has helped me address them and find the right direction. It has helped me get out of the rut.\n\nJourney To The Future: A Roadmap for Success for Youth - Consuelo Castillo Kickbusch (Author)\n\nThis book has helped me in various ways to get where I am today. Consuela is a great presenter!\n\nThe New Koreans: The Story of a Nation - Michael Breen", "label": 1}
{"title": "Finding leaked passwords with AI: How we built Copilot secret scanning", "url": "https://github.blog/engineering/platform-security/finding-leaked-passwords-with-ai-how-we-built-copilot-secret-scanning/", "content": "In October 2024, we announced the general availability of Copilot secret scanning, leveraging AI to detect generic passwords in users\u2019 codebases. This post describes how Copilot secret scanning works under the hood, the challenges we ran into when developing it, and the framework we use for testing and iteration.\n\nWhat is Copilot secret scanning?\n\nCopilot secret scanning is a feature of GitHub Secret Protection, which protects millions of repositories on GitHub by detecting hundreds of pattern types through our partner program. The precision of these detections is paramount for security teams and developers when dealing with security alerts. Historically, our detection approach has relied on regular expressions, which is an effective method for identifying secrets with strict, provider-minted formats. However, this method struggles with the nuanced and varied structures of generic passwords, often generating excessive noise for security teams and developers.\n\nWe now detect generic passwords with GitHub Copilot, using AI to analyze context\u2014such as the usage and location of a potential secret\u2014to limit noise and deliver relevant alerts that are critical to the health and security of your repositories.\n\nGetting to the point where we were confident in our password precision was a journey over many test cases, prompt iterations, and model changes. Let\u2019s dive in to explore what we learned along the way and find out where we\u2019re going.\n\nThe private preview highlighted a problem early on: unconventional file types and structures\n\nAt the core of Copilot secret scanning lies a request to a large language model (LLM), expressed through an LLM prompt consisting of:\n\nGeneral information about the type of vulnerability, in this case passwords.\n\nThe source code location and contents of the file where we believe the vulnerability may exist.\n\nA strict JSON format specification for the model output, to allow for automated processing.\n\nOur first iteration of the prompt used the few-shot prompting technique, which provides the LLM with example inputs and outputs to demonstrate how to perform the task. We wanted a resource-effective model to run the detections at scale and landed on GPT-3.5-Turbo. In parallel, we developed a basic offline evaluation framework, including manually curated test cases with both positive and negative findings, to help us validate that our approach was sound before deploying it to customers.\n\nWe deployed this first iteration to our private preview participants and immediately noticed a problem. While it worked reasonably well at identifying credentials in our offline evaluation, it would fail spectacularly in some customer repositories. The model had difficulty interpreting file types and structures not typically seen in the conventional coding languages and patterns that LLMs train on.\n\nThis experience revealed the complexity of the problem and the limiting nature of LLMs. We had to reevaluate our approach.\n\nThe road to public preview: Improving offline evaluation and prompting\n\nIn response to these initial results, we enhanced the offline evaluation framework in a few key ways. First, we added reports from private preview participants to increase the diversity of our test cases. Next, we enhanced the framework so that we could visually identify and analyze deviations resulting from model or prompt changes. This allowed us to better see the impact of customizing different steps in our prompting strategy. Finally, we leveraged the GitHub Code Security team\u2019s evaluation processes to create a data collection pipeline, and used GPT-4 to create our own test cases based on learnings from existing secret scanning alerts in open source repositories.\n\nThis improved offline evaluation and gave us the breadth needed to measure both precision and recall. Precision is the ability to find secrets more accurately, with concerns to the false positive rate, while recall is the ability to find secrets more reliably, with concerns to the false negative rate.\n\nFrom here, we ran a series of experiments to evaluate detection quality:\n\nWhat if we tried a different model?\n\nWhat if we ran the prompt multiple times and somehow combined the responses?\n\nWhat if we ran two different prompts on two different models in sequence?\n\nHow do we better handle the nondeterministic nature of LLM responses?\n\nMore specifically, we started experimenting with a few different mechanisms to improve our detection with the LLM.\n\nWe tried voting (asking the model the same question many times), which allowed for more deterministic responses but had no material impact on our precision.\n\nWe also tried using a larger model (GPT-4) trained on a larger set of parameters as a confirming scanner, to validate the accuracy of candidates found by GPT-3.5-Turbo. This helped improve precision without reducing our recall, but was also more resource intensive.\n\nWe also tried a few different prompting strategies, such as Fill-in-the-Middle, Zero-Shot, and Chain-of-Thought. We ended up collaborating with our colleagues at Microsoft and used their MetaReflection technique, a novel offline reinforcement learning technique that allows experiential learnings from past trials to come up with a hybrid Chain of Thought (CoT) and few-shot prompt that improves precision with a small penalty in recall.\n\nWe ultimately ended up using a combination of all these techniques and moved Copilot secret scanning into public preview, opening it widely to all GitHub Secret Protection customers. This brings us to our next hurdle: scale.\n\nScaling out capacity for a public preview\n\nSecret scanning not only scans incoming Git pushes, but also your entire Git history on all branches. With each new customer, the necessary resources increase linearly. Rather than simply expanding LLM capacity, we focused on striking the most effective balance between value and cost to ensure optimal performance and efficiency. Before tackling how we managed the resources, we tried to find ways to reduce resource usage itself by:\n\nIdentifying and excluding a class of changes from scanning (such as media files or language files that contain \u201ctest,\u201d \u201cmock,\u201d or \u201cspec\u201d in the filepath), because we expected they would never contain credentials or they would be incomprehensible to the model.\n\nExperimenting with newer models, such as GPT-4-Turbo and GPT-4o-mini, that were expected to be less resource intensive without compromising on performance and latency.\n\nExperimenting with different context windows to find one that reduced resources without significantly increasing latency for the LLM to respond to our queries.\n\nMaking improvements to how we tokenize the content we want to scan, including retaining some memory of previous tokenizations while processing new parts of a file.\n\nWhile some of these efforts proved fruitful, such as limiting the content we scanned, other efforts were less effective. For example, breaking down content into smaller pieces didn\u2019t have much of an impact, while using a more powerful model did.\n\nUltimately, the most impactful change came from creating a workload-aware request management system that allowed us to maximize and equitably share LLM capacity against the variety of different workloads we run during scans.\n\nIn building the system, we noticed a fundamental problem that needed addressing in our capacity management: assigning specific rate limits to individual workloads (such as scanning incoming Git commits or scanning the full history) was suboptimal. As each workload was tied to specific traffic patterns\u2014Git commits, for example, tend to correlate with working hours, while full history scanning correlates with discrete events like a security manager or administrator enabling the feature on a new organization\u2014it was easy to land in a situation where an individual workload could run into rate limits within its operational context, leaving additional resources available elsewhere unused.\n\nWe drew significant inspiration from existing solutions in this space, such as Doorman, GitHub\u2019s own Freno, and various other weighted, fair-priority, queue-related algorithms. We came up with an algorithm that allows us to set a range of limits for each workload, preventing the workload from completely overwhelming the LLM, while allowing it to tap into resources from other workloads going unused at the moment. This strategy was so effective at maximizing utilization that we ended up using it within Copilot Autofix and security campaigns as well.\n\nMirror testing our way to general availability\n\nAchieving confidence in detection quality was crucial for moving Copilot secret scanning to general availability. We implemented a mirror testing framework that ran our prompt and filtering changes against a subset of repositories that participated in our public preview. Rescanning these repositories with our latest improvements allowed us to assess the change in real alert volumes and false positive resolutions, without impacting users.\n\nWe found a huge drop in detections and false positives with very few missing real passwords. In some cases, we saw a 94% reduction in false positives across organizations! This before-and-after comparison indicated that all the different changes we made during private and public preview led to increased precision without sacrificing recall, and that we were ready to provide a reliable and efficient detection mechanism to all GitHub Secret Protection customers.\n\nLessons for the future\n\nCopilot secret scanning is now detecting passwords on nearly 35% of all GitHub Secret Protection repositories. We\u2019re continuing to monitor performance and apply lessons learned as we leverage the tooling we created along the way:\n\nA focus on precision: Security and development teams need accurate and actionable alerts without the noise\u2014this is always our primary goal.\n\nSecurity and development teams need accurate and actionable alerts without the noise\u2014this is always our primary goal. Including diverse test cases: We continue to incorporate examples based on learnings from customer feedback into our test bed as we refine our detection capabilities.\n\nWe continue to incorporate examples based on learnings from customer feedback into our test bed as we refine our detection capabilities. Effective resource management: We always need to balance scalability with performance.\n\nWe always need to balance scalability with performance. Collaborative innovation: Partnering with other GitHub and Microsoft teams helps us push the boundaries of what Copilot can achieve.\n\nThese learnings are also shared across Copilot Autofix, which continues to expand coverage for code scanning alerts and helps development teams remediate code scanning alerts quickly.\n\nSince our general availability launch, enablement for Copilot secret scanning has been included in security configurations, allowing you to control which repositories are detecting secrets across your organizations or enterprise. We\u2019re dedicated to continuous improvement through ongoing monitoring, mirror testing, and approach refinement based on customer feedback and detection trends. Copilot secret scanning serves as a critical component for robust application security and will evolve to meet the dynamic needs of our users.\n\nCopilot secret scanning is a feature of GitHub Secret Protection, which offers enterprise-ready solutions for preventing accidental secret exposure in your repositories. GitHub Secret Protection is available to purchase starting April 1, 2025.", "label": 0}
{"title": "Meta Open Source: 2024 by the numbers", "url": "https://engineering.fb.com/2025/04/02/open-source/meta-open-source-by-the-numbers/", "content": "Open source has played an essential role in the tech industry and beyond. Whether in the AI/ML, web, or mobile space, our open source community grew and evolved while connecting people worldwide.\n\nAt Meta Open Source, 2024 was a year of growth and transformation. Our open source initiatives addressed the evolving needs and challenges of developers\u2014powering breakthroughs in AI and enabling the creation of innovative, user-focused applications and experiences. In close collaboration with the open source community, we shared knowledge, introduced new projects, and enhanced existing ones.\n\nIn this post, we look at our portfolio of open source projects through numbers to give a better view of the scale of the community we interact with daily.\n\nAt Meta, we have several GitHub organizations where we publish new open source projects, maintain existing ones, and hold already archived projects. They include various tools, frameworks, and platforms for web, mobile, AI/ML, and hardware industries.\n\nBy the end of last year, we launched 256 brand-new repositories, bringing active public projects to 944. This number excludes archived repositories and projects that we moved to foundations.\n\nIn 2024, our open source codebases grew at an impressive pace, reaching 189,719 total commits in just one year. Community contributors accounted for 71,018, while Meta employees made the remaining 118,701.\n\nOpen source cannot exist without people collaborating, sharing, and innovating. A total of 4,274 external contributors helped bring our community to 7,144 strong. This remarkable community is what fuels the ongoing evolution of Meta Open Source.\n\nBeyond individual contributions, our projects on GitHub accumulated an additional 151,380 stars, bringing the total to a staggering 1.8 million. This growth in engagement shows strong interest and excitement for Meta Open Source projects.\n\nThank you to the open source community\n\nAt Meta, we believe open source accelerates the pace of innovation in the world. By sharing our technologies, we aim to move the industry forward while allowing other companies and individuals to use our solutions to scale more quickly and build great products.\n\nAt the same time, Meta Open Source projects are made possible by contributions from developers like you. Pull requests, documentation updates, social media posts, and everything in between are what build connections in our communities. Thank you all for another great year for open source.\n\nTo learn more about Meta Open Source, visit our open source site, subscribe to our YouTube channel, or follow us on Facebook, Threads, X, and LinkedIn.", "label": 0}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2020", "content": "en\n\nToday, 30 years ago on April 8th, 1990 Ryan White passed away. Today, he would have been 48 years old. I have always been an advocate for people with HIV/AIDS. I learned about Ryan when I was in high school. Ryan White contracted HIV through contaminated blood transfusions for hemophilia. He was the poster child during a time that many people didn't know what HIV/AIDS was as a disease. He spoke out against all adversaries. Ryan was forced out of school many times. He did \"The Fight to Go to School.\" So much ignorance occurred such as thinking it could be passed through casual contact. Ryan became a national spokesman. Many famous people such as Michael Jackson, befriended Ryan in a time that nobody wanted to be near him. Having people of high status led the way in showing that this disease is not transmitted just by shaking someone's hand, giving somebody a kiss or giving somebody a kiss. Ryan didn't stay quiet about his disease, he helped inform all of us that this disease can affect anyone, not just certain groups of people. This disease doesn't discriminate your gender, race, sexual orientation, etc. We all need to bond together to find a cure and stop the hatred. The Ryan White Act was created in 1990 to help people who have been diagnosed with this disease. Today the battle continues against HIV. We must remember those who fought to help educate us. RIP Ryan White", "label": 1}
{"title": "BoyMeetsWorld - Band Review", "url": "https://lifeofpablo.com/blog/boymeetsworld-band-review", "content": "BoyMeetsWorld - Band Review\n\nThis post was written in English (en_US).\n\nTopic of today is music. Importantly bands! As some of you may know, I went to the Vans Warped Tour '15 and discovered a lot of bands. It was probably the best day of my life so far!\n\nI want to introduce you guys to a band I discovered this summer: BoyMeetsWorld. A band of five guys from Ohio.\n\nThey are a really good rock/alternative band. For my die-hard alternative fans. This band is for you!! Those who want to expand their music taste, I also encourage you to listen to them. I really like their music. I can really relate to them.\n\n\"\"The fivesome are out to embody their message and encourage others through their music as they too have faced some of life's tough decisions and overcame them, making their relatability to their fans truly something special.\"\" -According to Vans Warped Tour Site\n\nThese are one of the nicest guys you will ever meet! One can really relate to their music! They never disappoint their growing fan base. Thanks guys for autographing my stuff. This was definitely put in the books.\n\nIt is nice to see a band grow and see them be even more successful as time goes on! I see great things for you guys!\n\nBand Members:\n\nRyan Sulken on drums\n\nBrad Sulken on bass,\n\nDrew Ritcher and\n\nDrew Thomason on guitar,\n\nSupport these guys! You wont regret them!\n\nHey BoyMeetsWorld do you want to come to my college for a concert in the future!? Please?\n\nFind them on BandCamp, Spotify, Google Play Music, etc! This would mean a lot for them! and Twitter (@OfficialBMWBand)You wont regret it!\n\nSources: VansWarpedTour.com , Spotify, BandCamp, Soundcloud.\"", "label": 1}
{"title": "What I Had for Dinner Tonight 16-Aug-2023", "url": "https://lifeofpablo.com/blog/what-i-had-for-dinner-tonight-16-aug-2023", "content": "Recent Trip\n\nOn my most recent trip to Oaxaca I brought back lots of delicious foods with me. One food item that always comes back with me is tlayudas.\n\nThis is a Tlayuda\n\nA tlayuda is a big tortilla tortilla that is a staple food in Oaxaca made out of corn and water. They are huge and twice the size of my head. These are no regular tortillas. This has been a food I've been eating since I was a young kid with my parents, grandparents, other relatives, and many friends. Even in moments of low funds, I have always been able to rely on these to meet my needs.\n\nOften times the tlayuda itself is the plate which you eat on. What I mean by this that often times you don't use an actual plate on the table to hold your tlayuda but the tlayuda is your plate.\n\nHow It's Eaten?\n\nThere are many ways on how you eat one but here are the ways I eat them:\n\nYou rip the tlayuda piece by piece until there is no more\n\nYou can roll it up and just starting biting into it.\n\nWhat's on this Tlayuda\n\nThis one is a simple Tlayuda. For the toppings, I used:\n\nTlayuda as the base\n\nHomemade refried beans\n\nQuesillo (Oaxacan String Cheese)\n\nChapulines (Grasshoppers). Yes, you read that correctly.\n\nWhy I Chose These Ingredients\n\nI brought back plenty of these ingredients with me from Oaxaca since I don't get to have these often or it's hard to find around here. Even when i do find them here, it doesn't have the same taste or the ingredients are old. I'm trying to eat as much of them as fast as I can.\n\nI love quesillo. Even though my body I don't agree with dairy much but quesillo is something that I never have a problem with. It probably the way it's cultured.\n\nI also haven't had much of an appetite the last few days and I haven't been craving much meat either. The chapulines provide me a substantial amount of protein and nutrients. It's common in Oaxaca to go multiple meals without eating meat. There are times where I don't eat meat for weeks at a time\n\nKeep Connected with My Roots\n\nAs a first generation Mexican-American and Oaxacan, food is important to me to help me stay connected with my culture.", "label": 1}
{"title": "Generated content is an invasive species in the online ecosystem \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2024/01/09/generated-content-is-an-invasive-species-in-the-online-ecosystem/", "content": "Invasive species disrupt ecosystems because they did not evolve in balance with the other species. Native species have adapted to fill specific niches, but the constraints they\u2019ve accepted to fit that niche in the ecosystem do not also bind invasive species. Not limited by the same factors, they reproduce faster and crowd out the native species. Time and again, we\u2019ve seen invasive species introduced to an ecosystem outcompete the more specialized native species, sometimes even driving them to extinction.\n\nLikewise, generated imagery and text is not bound by human limitations of productivity. As generated material rapaciously populates the Internet, human-created artworks will be outcompeted by generated graphics on social media platforms by virtue of volume.\n\nAnd corporations are also trying to argue that their products should not be bound by the same legalities that human artists and writers are bound by. Their products only work with copyrighted material, and that means it\u2019s only economically viable if they steal the training data. Like invasive species, they don\u2019t play by the same rules: the rest of us peons must wait 95 years to play with fucking Steamboat Willie, but they get to gobble down anything they want for free instantly and use it to (try to) drive us out of work.\n\nLet\u2019s starve out this species invasion before it collapses our information ecosystem \u270a\n\nSee also:\n\nA 21st century collage tool\n\nWe need solidarity across creative industries\n\nGenerative AI is intellectual sharecropping", "label": 1}
{"title": "Journey to 1000 models: Scaling Instagram\u2019s recommendation system", "url": "https://engineering.fb.com/2025/05/21/production-engineering/journey-to-1000-models-scaling-instagrams-recommendation-system/", "content": "In this post, we explore how Instagram has successfully scaled its algorithm to include over 1000 ML models without sacrificing recommendation quality or reliability.\n\nWe delve into the intricacies of managing such a vast array of models, each with its own performance characteristics and product goals.\n\nWe share insights and lessons learned along the way\u2014from the initial realization that our infrastructure maturity was lagging behind our ambitious scaling goals, to the innovative solutions we implemented to bridge these gaps.\n\nIn the ever-evolving landscape of social media, Instagram serves as a hub for creative expression and connection, continually adapting to meet the dynamic needs of its global community. At the heart of this adaptability lies a web of machine learning (ML) models, each playing a crucial role in personalizing experiences. As Instagram\u2019s reach and influence has grown, so too has the complexity of its algorithmic infrastructure. This growth, while exciting, presents a unique set of challenges, particularly in terms of reliability and scalability.\n\nJoin us as we uncover the strategies and tools that have enabled Instagram to maintain its position at the forefront of social media innovation, ensuring a seamless and engaging experience for billions of users worldwide.\n\nAre there really that many ML models in Instagram?\n\nThough what shows up in Feed, Stories, and Reels is personally ranked, the number of ranked surfaces goes much deeper\u2014to which comments surface in Feed, which notifications are \u201cimportant,\u201d or whom you might tag in a post. These are all driven by ML recommendations.\n\nWithin a given surface, we\u2019ll have different layers of the ranking funnel: sourcing (retrieval), early-stage ranking (ESR), and late-stage ranking (LSR). We operate on fewer candidates as we progress through the funnel, as the underlying operations grow more expensive (see Figure 1 below):\n\nWithin each surface and layer, there is constant experimentation, and these permutations create a severe infrastructure challenge. We need to allow room for our ML engineers to experiment with changes such as adjusting weights for a given prediction. The net result, depicted below in Figure 2, is a large number of models serving user traffic in production:\n\nHow did we realize infra maturity wasn\u2019t going to catch up?\n\nIdentified risks\n\nWe identified several risks associated with scaling our algorithm, rooted in complaints about ML productivity and repeating patterns of issues:\n\nDiscovery: Even as a team focused on one app \u2014 Instagram \u2014 we couldn\u2019t stay on top of the growth, and product ML teams were maintaining separate sources of truth, if any, for their models in production.\n\nRelease: We didn\u2019t have a consistent way to launch new models safely, and the process was slow, impacting ML velocity and, therefore, product innovation.\n\nHealth: We lacked a consistent definition of model prediction quality, and with the diversity of surfaces and subtlety of degraded ranking, quality issues went unnoticed.\n\nSolution overview\n\nTo address these risks, we implemented several solutions:\n\nModel registry: We built a registry that serves as a ledger for production model importance and business function foremost, among other metadata. This registry serves as our foundational source of truth, upon which we can leverage automation to uplevel system-wide observability, change management, and model health.\n\nModel launch tooling: We developed a more ideal flow for launching new models that includes estimation, approval, prep, scale-up, and finalization. This process is now automated, and we\u2019ve reduced the time it takes to launch a new model from days to hours.\n\nModel stability: We defined and operationalized model stability, a pioneering metric that measures the accuracy of our model predictions. We\u2019ve leveraged model stability to produce SLOs for all models in the model registry, which enables simple understanding of the entire product surface\u2019s ML health.\n\nModel registry\n\nWhat did model investigations look like prior to the registry?\n\nBefore we created the model registry, the investigation process was a time-consuming and error-prone experience for on-call engineers and model owners. An on-call engineer had to ask multiple questions to model owners to gather information, as depicted Figure 3 below, about the context of what this model does in the stack and to clarify how important it is to the business.\n\nUnderstanding this context is extremely important to the operational response: Depending on the importance of the model and the criticality of the surface it\u2019s supporting, the response is going to differ in kind. When a model is an experiment serving a small percentage of the traffic, an appropriate response can be to end the experiment and reroute the traffic back to the main model (the baseline). But if there\u2019s a problem with the baseline model that needs to be handled with urgency, it\u2019s not possible to \u201cjust turn it off.\u201d The engineer on call has to loop in the model owner, defeating the purpose of having a dedicated on-call.\n\nTo avoid holding up an operational response on a single POC, we needed a central source of truth for model importance and business function. What if the model is not available? What if 10 of these issues happen concurrently?\n\nWith the development of the model registry, we standardized the collection of model importance and business function information, ensuring most of our operational resources were going towards the most important models.\n\nWhat problems did the model registry solve?\n\nThe model registry is a system of record built on top of Configerator, Meta\u2019s distributed configuration suite . This schematized ledger (see an example in Figure 4 and detailed further below) provides read-and-write access to operational data based on the inventory of production models. It\u2019s a flexible and extensible foundation upon which one can build automation and tools to solve problems that are specific to individual organizations within Meta that are not served by the general tooling.\n\nAs Instagram scaled its investment in AI through rapid innovation in content recommendations, the number of models and AI assets grew; as a result, it has been increasingly important \u2014 but also increasingly difficult \u2014 to maintain a minimum standard for all of our models, as we lacked an authoritative source for the business context as well as for a model\u2019s importance.\n\nIn creating the model registry, we set out to provide a structured interface for collecting business context via model types, importance via criticality, and additional metadata that would enable model understanding. Below, we\u2019ll get into the model types, criticality, and automation we\u2019ve built for this purpose.\n\nModel types\n\nAt a high level, model type describes the purpose for the ML workload where it represents a category or class of models that share a common purpose or are used in similar contexts. For example, we have \u201cig_stories_tray_mtml\u201d which is a string attached to training flows, model checkpoints, inference services, and more. Put simply, a model type identifies for the reader this model\u2019s purpose in the ranking funnel.\n\nLet\u2019s break it down:\n\n\u201cig_stories_tray_mtml\u201d \u2192 \u201cig\u201d \u201cstories\u201d \u201ctray\u201d \u201cmtml\u201d\n\n\u201c ig \u201d: This model is an \u201cig\u201d model as opposed to \u201cfb\u201d or \u201cwhatsapp\u201d.\n\n\u201c stories \u201d: This model serves IG Stories.\n\n\u201c tray \u201d: This model serves in the main IG Stories tray (as opposed to stories in some other surface).\n\n\u201cmtml\u201d: This model is a multi-task-multi-label model, commonly used in late-stage ranking.\n\nWe can then use these model type strings to tag AI assets, and since they serve as proxies for business context, we can use them also for asset management, policy enforcement, analytics, and more.\n\nThe metadata entries in the model registry are anchored on two main types that describe model instances (ModelMetadata) as well as model types (ModelTypeMetadata). These types are made up of \u201ccore\u201d attributes that are universally applicable, as well as \u201cextended\u201d attributes that allow different teams to encode their opinions about how these entries will inform operations. For example, in Instagram our extended attributes encode \u201cbaseline\u201d and \u201choldout\u201d model IDs, which are used in our ranking infrastructure to orchestrate ranking funnel execution.\n\nCriticality\n\nIn addition to defining business function, we had to establish clear guidelines for model importance. Within Meta, SEVs and services have a unified-importance tier system where the Global Service Index (GSI) records a criticality from TIER0 to TIER4 based on the maximum incident severity level the service can cause, from SEV0 as the most critical to SEV4 as simply a \u201cheads up.\u201d Since GSI criticality had social proof at the company, and infra engineers were familiar with this system, we adopted these criticalities for models and now annotate them at the model type and model level.\n\nNo longer would each team decide to raise their own model services to TIER1 for themselves, increasing the burden on all teams that support these models. Teams needed to provide an immediate response (available 24/7) on call and be able to prove that their models contributed meaningfully to critical business metrics to qualify for elevated monitoring.\n\nConfiguration structure as a foundation for automation\n\nOnce we had onboarded a critical mass of Instagram models to the model registry, we could begin to fully integrate with our monitoring and observability suite using our Meta-wide configuration solution, Configerator. With this, we could now have model performance monitoring and alerts that are fully automated and integrated with our tooling for SLIs called SLICK, dashboards that allow us to monitor models across many time series dimensions, and a suite of alerting specific to the model that is driven from the entries in the model registry.\n\nThis provided all our teams confidence that our monitoring coverage was complete and automated.\n\nLaunching\n\nWhile a point-in-time snapshot of models in production is great for static systems, Instagram\u2019s ML landscape is constantly shifting. With the rapid increase of iteration on the recommendation system driving an increased number of launches, it became clear our infrastructure support to make this happen was not adequate. Time-to-launch was a bottleneck in ML velocity, and we needed to drive it down.\n\nWhat did the process look like?\n\nConventionally, services were longstanding systems that had engineers supporting them to tune. Even when new changes would introduce new capacity regression risks, we could gate this behind change safety mechanisms.\n\nHowever, our modeling and experimentation structure was unique in that we were planning for more rapid iteration, and our options were insufficient. To safely test the extent of load a new service could support, we would clone the entire service, send shadow traffic (i.e., cloned traffic that isn\u2019t processed by our clients), and run multiple overload tests until we found a consistent peak throughput. But this wasn\u2019t a perfect science. Sometimes we didn\u2019t send enough traffic, and sometimes we\u2019d send too much, and the amount could change throughout the day due to variations in global user behavior.\n\nThis could easily take two days to get right, including actually debugging the performance itself when the results weren\u2019t expected. Once we got the result, we\u2019d then have to estimate the final cost. Below (in Figure 5) is the formula we landed on.\n\nThe actual traffic shifting portion was tedious as well. For example, when we managed to fully estimate that we needed 500 replicas to host the new service, we might not actually have 500 spares lying around to do a full replacement, so launching was a delicate process of partially sizing up by approximately 20%, sending 20% of traffic over, and then scaling down the old service by 20% to reclaim and recycle the capacity. Rinse, repeat. Inefficient!\n\nAnd by the time we got to the end of this arduous process, the ordeal still wasn\u2019t over. Each team was responsible for correctly setting up new alerts for their baseline in a timely fashion, or else their old models could and did trigger false alarms.\n\nHow does forcing virtual pools aid product growth?\n\nOne of the prerequisites for fixing competition for resources and unblocking productivity was to put up guardrails. Prior to this, it was \u201cfirst come first served,\u201d with no clear way to even \u201creserve\u201d future freed capacity. It was also hard to reason about fairness from an infra perspective: Would it make sense to give each team equal pools, or give each individual person a maximum limit?\n\nAs it turned out, not all MLEs are experimenting at the same time, due to staggered progress on their work, so individual (per-engineer) limits were not ideal. One member might be in the experimentation stage and another might be training. So our solution was to provide bandwidth to each team.\n\nOnce each team \u2014 and therefore product \u2014 had quotas distributed, their launch policy became more clear cut. Some teams established free launching as long as the team was within quota. Others required no regressions in capacity usage. But mostly this unlocked our ability to run launches in parallel, since each one required much less red tape, and prioritization was no longer done at the org level.\n\nWhat other tooling improved launching?\n\nAs mentioned earlier, preplanning with capacity estimations was critical to understanding cost and ensuring reliability. We were often asked, Why not let autoscaling take care of everything? The problem was that each service could be configured slightly differently than a previously optimized service, or some architectural change could have affected the performance of the model. We didn\u2019t have an infinite amount of supply to work with, so by the time we fully traffic-shifted everything over, we might find that we didn\u2019t have enough supply. Reverting is costly, taking hours to get through each stage.\n\nBy doing capacity estimations in advance, this also allowed us and each team to accurately evaluate metric improvement versus cost. It might be worthwhile to double our costs if something would increase time spent on the app by 1%, but likely not for a 0.05% improvement where we could better spend that capacity funding another initiative.\n\nWith partners in AI Infra, we developed two major solutions to this process: offline performance evaluation and an automated launching platform.\n\nWe simplified determining performance of a new service using recorded traffic. Pre-recorded traffic was continuously collected into a data warehouse that the benchmarker could read from, and we\u2019d spin up temporary jobs with this automation. One job would replay different levels of traffic continuously and send it to another job that was a clone of the existing experiment. By putting stoppers on desired latency and error rates, the tooling would eventually output a converged stable number that we could understand as the max load (see Figure 6).\n\nThe launch platform itself would input the numbers we captured from these tests, automatically collect demand data as defined, and run that same formula to calculate a cost. The platform would then perform the upscaling/downscaling cycle for teams as we shifted traffic.\n\nAnd finally, by leveraging the model registry, we were able to land this model change in code (see example in Figure 6), to help us better maintain and understand the 1000+ models within our fleet. Likewise, this bolstered our trust in the model registry, which was now directly tied to the model launch lifecycle.\n\nThis suite of launch automation has dramatically reduced the class of SEVs related to model launches, improved our pace of innovation from a few to more than 10 launches per week, and reduced the amount of time engineers spend conducting a launch by more than two days.\n\nModel stability\n\nAs the number of models in production increased, our organization started to feel the effects of an inconsistent measure of model health. While ranking models are run like any other distributed backend system (receive a request, produce a response), one may think a universal SLO that measures request success rate can suffice to capture holistic health. This is not the case for ranking models, as the accuracy of recommendations received carries significant importance to the end-user experience. If we consider a user who is a huge fan of golf but does not enjoy cooking content (see the \u201cavailable & irrelevant\u201d case in Figure 8 below), we see an example of this inaccuracy in practice. This is precisely what the model stability metric sought to capture.\n\nWhy is measuring ranking model reliability unique?\n\nRanking models, unlike traditional idempotent request/response backends, produce scores predicting user action given a set of candidates (PLIKE, PCOMMENT, PFOLLOW, etc.). These scores then combine and are used to determine which candidates are most relevant to an end user. It\u2019s important that these scores accurately reflect user interest, as their accuracy is directly correlated to user engagement. If we recommend irrelevant content, user engagement suffers. The model stability metric was designed to make it easy to measure this accuracy and detect inaccuracy at our scale.\n\nLet\u2019s discuss how this works.\n\nDefining model stability\n\nModels are complex, and they produce multiple output predictions. Let\u2019s take a simplified example (shown in Figure 9 below) of a multi-task-multi-label (MTML) model predicting three actions:\n\nFor us to claim this model is stable, we must also claim that each underlying prediction is stable.\n\nWhen evaluating the accuracy of a ranking model\u2019s predictions, we typically look at two metrics:\n\nModel calibration , which is based on observed real-world outcomes and answers the question, \u201cAre we over- or under-predicting user action?\u201d It is calculated as a ratio of predicted click-through-rate (CTR) and empirical CTR. A perfect predictor will have calibration centered at 1.\n\nModel normalized entropy (NE), which measures the discriminative power of a predictor, and answers the question, \u201cHow well can this predictor separate action from inaction?\u201d It is calculated as a ratio of the average log-loss per impression to what the average log-loss per impression would be if we always predicted the empirical CTR. With NE, lower values are better, and an NE of 1 is equivalent to random predictions.\n\n(For more information regarding our choice of prediction evaluation metrics, please refer to the paper, \u201cPractical Lessons from Predicting Clicks on Ads at Facebook.\u201d)\n\nA model\u2019s predictions are unstable when either calibration or NE are out of their expected healthy ranges. To determine what a healthy range is, we must look at each metric in real time, and Figure 10 below shows what these time series can look like:\n\nBy observing the trend of a healthy prediction, we can apply thresholds for our evaluation metrics. When these thresholds are breached, the underlying prediction is considered unstable.\n\nFrom here, we can define model stability as a binary indicator across a model\u2019s predictions. It is 1 if all underlying predictions are stable, and 0 if any prediction is unstable. This is an extremely powerful method of reacting to real-time prediction instability as well as a tool for understanding trends in predictive health per model or across distinct products ranking funnels.\n\nOperationalizing model stability\n\nWith a real-time view on model predictive health, we can leverage this unified definition of model stability and apply it to all of our models in production, once again leveraging the model registry as a ledger to hold this important data. In Figure 11 below, we can see the addition of model stability metric metadata after we determined the expected thresholds.\n\nGiven the large number of models in production, each producing many predictions, building a portable definition of model health applicable to all of our ranking models represented an important milestone toward upleveling Instagram\u2019s ML infrastructure maturity. This has unlocked our ability to build generic alerting to guarantee detection of our most important models becoming unstable, thereby moving us closer to mitigation when our recommendation system is at risk.\n\nSince the addition of these metrics and alerting, ML teams have discovered previously hidden issues within their models and addressed them faster than before, leading to higher-quality recommendations.\n\nKey takeaways\n\nIn our journey to scale Instagram\u2019s algorithm to manage over 1000 models, we have learned several critical lessons that have shaped our approach and infrastructure. These takeaways not only highlight the challenges we faced but also underscore the strategies that led to our success.\n\nInfra understanding is the foundation to building the right tools\n\nA unified understanding of our infrastructure footprint was essential in developing the right tools to support our scaling efforts. By identifying the gaps and potential risks in our existing systems, we were able to implement solutions such as the model registry that significantly improved our operational efficiency and reliability posture.\n\nHelping colleagues move fast means we all move faster\n\nBy addressing the model iteration bottleneck, we enabled our teams to innovate more rapidly. Our focus on creating a seamless, self-service process for model iteration empowered client teams to take ownership of their workflows. This not only accelerated their progress but also reduced the operational burden on our infrastructure team. As a result, the entire organization benefited from increased agility and productivity.\n\nReliability must consider quality\n\nEnsuring the reliability of our models required us to redefine how we measure and maintain model quality. By operationalizing model stability and establishing clear metrics for model health, we were able to proactively manage the performance of our models. This approach enables us to maintain high standards of quality across our recommendation systems, ultimately enhancing user engagement and satisfaction.\n\nOur experience in scaling Instagram\u2019s recommendation system has reinforced the importance of infrastructure understanding, collaboration, and a focus on quality. By building robust tools and processes, we have not only improved our own operations but also empowered our colleagues to drive innovation and growth across the platform.", "label": 0}
{"title": "From idea to app: Introducing Stitch, a new way to design UIs", "url": "https://developers.googleblog.com/en/stitch-a-new-way-to-design-uis/", "content": "That's precisely the problem Stitch aims to solve \u2013 Stitch is a new experiment from Google Labs that allows you to turn simple prompt and image inputs into complex UI designs and frontend code in minutes.\n\nBuilding great applications always comes down to a powerful partnership between design and development. Designers envision the user experience, crafting intuitive and engaging interfaces. Developers then bring those designs to life with functional code. Traditionally, connecting design ideas to working code took a lot of manual effort and back-and-forth.\n\nStitch was born of an idea between a designer and an engineer, both looking to build a product that optimized their respective workflows. It leverages the multimodal capabilities of Gemini 2.5 Pro to create a more fluid and integrated workflow between design and development. And, with an option to refine your design with image inputs, an interactive chat, theme selectors, and a paste to Figma function, Stitch lets you truly hone in on your creative designs and development needs.\n\n\n\nHere\u2019s what Stitch offers today to enhance your design and development process:\n\n\n\nGenerate UI from natural language\n\nDescribe the application you want to build in plain English, including details like color palettes or desired user experience. Stitch can generate a visual interface tailored to your description.\n\n\n\nGenerate UI from images or wireframes\n\nHave a design sketch on a whiteboard, a screenshot of a compelling UI, or a rough wireframe? Upload it to Stitch. Stitch processes the image to produce a corresponding digital UI, bridging your initial visual ideas to a functional design.\n\n\n\nRapid iteration and design exploration\n\nDesign is an iterative process, and Stitch facilitates this by allowing you to generate multiple variants of your interface. Experiment with different layouts, components, and styles to achieve the desired look and feel.\n\n\n\nSeamless transition to development\n\nOnce you're satisfied with your design, Stitch provides crucial bridges to the development workflow:\n\nPaste to Figma: Your generated design can be seamlessly pasted to Figma for easy further refinement, collaboration with design teams, and integration into existing design systems.\n\nExport front-end code: Stitch generates clean, functional front-end code based on your design, so you have a fully functional UI ready to go.\n\n\n\nStitch is about unlocking the magic of app creation for everyone. We're thrilled to bring this experiment to you and can't wait to see what you'll build with it.\n\nTry out Stitch at stitch.withgoogle.com and let us know what you think!", "label": 0}
{"title": "Mexico Journal Entries in French", "url": "https://lifeofpablo.com/blog/mexico-journal-entries-in-french", "content": "Mexico Journal Entries in French\n\nIn a mountain in Oaxaca\n\nThis post was written in Fran\u00e7ais (fr_FR).\n\nGoing on a hike in Oaxaca - 2015\n\nThis is some writing I wrote in French while visiting my grandparents in 2015\n\nJour 1\n\nJe suis arriv\u00e9 en Mexique.\n\nJour 2\n\nLe second jour de mes vacances \u00e9tait ennuyeux. Je me l\u00e8ve t\u00f4t. Je me suis lev\u00e9 en retard. Je prends une douche. Le jour commence. La derni\u00e8re fois que je suis venu c'\u00e9tait il y a cinq jours avec ma m\u00e8re et mes deux s\u0153urs. Nous \u00e9tions ici partout en \u00e9t\u00e9. Je n'ai vraiment pas fait beaucoup, juste aider ma grand-m\u00e8re \u00e0 cause de sa sant\u00e9. Elle est tr\u00e8s malade.\n\nJour 3\n\nJe crois que j'ai une nouvelle routine quand je me l\u00e8ve. Ceci est \u00e0 cause du fait que je suis dans un nouvel endroit g\u00e9ographique. Je dois recommencer le forme d'habitude que les gens font tous les jours. Pour le petit-d\u00e9jeuner, je mange beaucoup de pain. A c\u00f4t\u00e9 de \u00e7a je bois du caf\u00e9. C' est un repas traditionnel. Il est vite mang\u00e9. Apr\u00e8s \u00e7a nous avons tu\u00e9 une dinde pour manger un autre plat traditionnel. J'ai un nouvel ami. Il s'appelle Bryan. Ouais c'est un nom am\u00e9ricain. Il va \u00e0 l'\u00e9cole. Il est maintenant en vacances. C' est un gar\u00e7on aimable. Les parents de Bryan sont vendeurs de glaces d\u00e9licieuses. Ils vendent aussi du bois de beaucoup parfums. Je passe du temps avec lui. Je vais \u00e0 sa maison. Nous regardons la t\u00e9l\u00e9vision. Chez mes grands-parents n'ont pas de t\u00e9l\u00e9. Nous regardons les programmes am\u00e9ricain en espagnol. J'aime aussi le programme mexicaine aussi. C' est un cool type!\n\nJour 4\n\nJe me suis encore lev\u00e9. La m\u00eame routine comme toujours. Ma routine ici est presque la m\u00eame comme celle- l\u00e0, aux \u00c9tats-Unis. J'ai fait la cuisine pour le d\u00e9jeuner. J'ai pr\u00e9par\u00e9 un repas italien. C'\u00e9tait \"chicken Alfredo\" avec pasta. J'ai enseign\u00e9 mes tantes \u00e0 faire. C'\u00e9tait d\u00e9licieux. Mon grand-p\u00e8re et mes tantes l'aiment. Mais ma grand-m\u00e8re ne l'aime pas. Elle est plus traditionnelle. Elle n'aime pas le change. Oh la la! Elle est compliqu\u00e9e. Vous n'avez pas id\u00e9e. :'( . Apr\u00e8s \u00e7a je suis sorti au centre du village. Je fais du shopping. Je rach\u00e8te d\u00e8s nourriture. Oh la la il faisait chaud. Je transpire beaucoup. Les tortas sont la version mexicaine d'un sandwich en comparaison entre les am\u00e9ricain. Ils sont d\u00e9licieux. Comme j'aime la nourriture mexicaine.", "label": 1}
{"title": "Supercharged Developer Portals", "url": "https://engineering.atspotify.com/2024/4/supercharged-developer-portals", "content": "Today, we announced Spotify\u2019s latest products and services for companies adopting Backstage, the open source framework for building internal developer portals (IDPs). Whether your company needs a highly customized IDP built from scratch or an out-of-the-box solution that\u2019s ready to go ASAP, we want to make it easy for anyone to maximize the value they get from their Backstage developer portal. You can watch today\u2019s Spotify for Backstage roadmap webinar below to see how our latest tools help other companies build like Spotify \u2014 by prioritizing developer experience and developer productivity.\n\nWatch Spotify\u2019s Backstage team introduce our latest enterprise developer tools.\n\nEvery company is a tech company\n\nNo matter what business you\u2019re in, you\u2019re building software to run it and software to innovate it \u2014 so you\u2019re really in the software business now. Whether you\u2019re an online retailer or an airline or a bank, every company is a tech company. And all tech companies are facing the same problem: software development is getting more and more complex.\n\nWhy has it become so complex?\n\nYesterday, it was the proliferation of microservices, the explosion of cloud tooling, and the rise of DevOps \u2014 where you\u2019re responsible for everything you build, from how a service is deployed to how secure it is, to how much it costs to run. Today, it\u2019s generative A.I. and code generation tools that give teams the power to build faster than ever, while also increasing software sprawl at a previously unimaginable rate. And tomorrow? There will be a new technology, just as exciting and transformative as microservice, clouds, and LLMs, and just as likely to spur on more sprawl.\n\nBut it doesn\u2019t even have to be something as sophisticated or world-changing as A.I. It could simply be that your parent company acquired another company, and so your engineering org just doubled in size and inherited an entirely separate tech stack. Suddenly, you have to make sense of a thousand new components built in a hundred new ways by a hundred new engineers \u2014 increasing the cognitive load of all your teams and testing the limits of context-switching and their patience.\n\nSo what\u2019s the solution? This is where Backstage comes in\u2026\n\nTaming the chaos and empowering teams with a Backstage IDP\n\nBackstage began life at Spotify as a software catalog \u2014 a directory of all the components our teams were building, from backend services to websites and libraries, emphasizing ownership and discoverability. And then we just kept adding to it. Because the platform had a simple, extensible plugin architecture, Backstage could evolve to tame chaos and complexity wherever it found it \u2014 until eventually it became a hub for everything engineering: aka, an internal developer portal (or IDP).\n\nAs a single pane of glass for all your tech infrastructure, a Backstage IDP streamlines software development \u2014 promoting the healthy parts of teams building software (ownership, tech standards, knowledge sharing, self-service, and collaboration), while minimizing the frustrating parts (silos, fragmentation, lack of discovery and documentation, dependency bottlenecks, and unclear standards).\n\nAt Spotify, we saw how much Backstage could improve developer effectiveness firsthand. Not only did Spotify\u2019s frequent Backstage users build faster, but they also created higher-quality software compared to other developers. According to that internal study, our Backstage users are 2.3x more active in GitHub, create 2x as many code changes in 17% less cycle time, and deploy software 2x as often \u2014 and their software is deployed for 3x as long. We were onto something.\n\nAn open platform that prioritizes developer experience\n\nSince open sourcing the Backstage framework and donating it to the CNCF, the wider tech community has experienced the benefits of improved developer experience and productivity for themselves. As part of last month\u2019s KubeCon in Paris, the CNCF held its third BackstageCon \u2014 a testament to the platform\u2019s continued growth and popularity. The open source framework has over 2,200 project contributors and over 3,000 adopting companies. Based on that, we estimate over two million people are using our homegrown developer portal today.\n\nThe third-party Backstage ecosystem is thriving as well. Consulting companies like Frontside, Thoughtworks, and Adaptavist provide a range of services and support for Backstage adopters. Popular tech services continue to build plugins for Backstage, including new ones coming soon from Rootly, DX, LinearB, Snyk, and Swimm. And new companies are adopting Backstage every week.\n\nBut we want to see this ecosystem grow even further. And we think Spotify has a lot more to offer Backstage adopters of all shapes and sizes.\n\nDevEx at scale: Spotify\u2019s secret sauce\n\nDeveloper experience has long been a main ingredient of Spotify\u2019s success. And that has put us in a unique position to become a pioneer in the field of DevOps and platform engineering. Very few companies get to test what they build in a real-world environment like ours, at the global scale we operate in.\n\nWe build developer tools based on the insights we gain from:\n\nOur internal R&D community of demanding Spotifiers. We have hundreds of teams who depend on this software every day to get their jobs done, whether they\u2019re shipping innovative features like AI DJ or fighting hordes of spam bots.\n\nA diverse open source community of contributors and end users. Spotify\u2019s engineers maintain the open source framework together with a global community of thousands of contributors, while also supporting a Discord channel with over 13,000 members.\n\nOur enterprise customers and third-party partners. Other companies have been using our bundle of Spotify-built plugins and support services. So, we learn by seeing our tools and methods being applied to companies that can sometimes be very different from our own.\n\nAll of this feedback, experience, and understanding goes into what we ship. We think this unique perspective adds unique value to the Backstage ecosystem, something no one else can offer. And that\u2019s what you\u2019ll see in the products we announced today.\n\nSay hello to Spotify for Backstage\n\nDifferent companies need different kinds of IDPs. There are the DIY companies who need to build a highly customized IDP from the ground up, those who need an IDP that\u2019s quick to set up and easy to maintain right out of the box, and many others who fall somewhere in between.\n\nOur products for Backstage are designed to supercharge your developer experience, no matter what kind of business you\u2019re in, how big or small your company is, or where you are in your DevEx journey.\n\nIn the webinar, you\u2019ll hear Spotify\u2019s Backstage team talk about:\n\nSpotify Plugins for Backstage : Level up your custom portal with our newly updated bundle of proven, Spotify-built plugins, including Soundcheck, Role-Based Access Control, Skill Exchange, and Insights.\n\nSpotify Enterprise Support for Backstage : Let our Backstage experts help you build, maintain, or customize your portal with our personalized support and consulting services.\n\nSpotify Portal for Backstage: Get a full-featured IDP designed by Spotify, that\u2019s both fast to get up and running, and easy to maintain. How fast? Watch the live demo of how you can set up Portal in less than five minutes, with no coding required.\n\nTo learn more about all of these products, watch today\u2019s roadmap webinar or head to backstage.spotify.com to get started.", "label": 0}
{"title": "IndieWeb Carnival October 2023 - Self Care and Routine", "url": "https://lifeofpablo.com/blog/indieweb-carnival-october-2023", "content": "This month I am hosting this month's IndieWeb Carnival on self-care and routine. Anyone and everyone is welcome to participate in the carnival.\n\nWhat is self-care ?\n\n\"a multidimensional, multifaceted process of purposeful engagement in strategies that promote healthy functioning and enhance well-being.\" [1]\n\n\"Self-care means taking care of yourself so that you can be healthy, you can be well, you can do your job, you can help and care for others, and you can do all the things you need to and want to accomplish in a day.\" [2]\n\nThere is no one size fits all model for everyone. People's backgrounds such as culture, spiritual beliefs, life experiences, etc influence how self-care is practices. It could also be proactive or reactive. There is no right or wrong way of doing this.\n\nSelf-Care in the Digital Age\n\nWe live in age where we are always connected online. This also adds complexities balancing our lives. Setting boundaries isn't just limited to people. Being always connected to the web is also taxing on our mental health.\n\nHow Am I performing Self-Care ?\n\nLearning self-care has been something I've been working on a lot this year. It has been something I neglected for many, many years. It has helped me be more aligned and be more connected with myself. Finding a self-care routine isn't exactly a straight line to follow. I'm still finding ways to better improve my routine. It's been important to adjust as I go because it's a continious learning process. Some things worked a few months ago and now it isn't working as well before.\n\nSetting boundaries with myself and people\n\nActually find time for myself and appreciate alone time.\n\nActually address insecurities\n\nExcercise such as running or biking.\n\nDisconnect from digital devices as needed\n\nPicking up new hobbies such as hacky sack.\n\nI invite you to write a post on self-care.\n\nHere are some prompts to help you get started or to build off of this post.\n\nWhat type of self-care routines do partake in?\n\nWhat are some hard realizations once you started to take care of yourself?\n\nHow do you incorporate self-care in difficult times?\n\nWhat has been your journey in reaching self-care and your routines.\n\nHow does your self-care routine differ during the week vs the weekend/going on holiday?\n\nHow do you take care of yourself.\n\nDo you check in with yourself too make sure your self-care routine is keeping up with your needs?\n\nI will create a roundup post on the 1 November 2023 on all the responses I recieve. I will post on Indieweb News\n\nSend me your responses via:\n\nWebmention\n\nEmail - pablo@lifeofpablo.com\n\nIf you would like to host a future monthly IndieWeb Carnival, please check out the details on the IndieWeb Wiki.\n\nThis post has been syndicated to IndieWeb News.", "label": 1}
{"title": "Explore the latest updates on Google Wallet", "url": "https://developers.googleblog.com/en/explore-the-latest-updates-google-wallet-io-25/", "content": "Last year, we were thrilled to expand access to Google Wallet for users in more than 90 countries and territories. Recently we expanded Google Wallet adding +50 more countries, allowing users to view and use digital passes in the app and on the web. And we've worked hard to make Google Wallet even more robust, flexible, and ultimately, more feature-rich for you, our developer community.\n\nLet\u2019s dive into the exciting new capabilities we\u2019ve announced at our I/O session this year.\n\n\n\nDigital IDs: A foundation of trust, ease, and interoperability\n\nDigital IDs in Google Wallet are live today, built with trust, ease of use, and interoperability as our top priorities. We're committed to bringing more digital IDs to Google Wallet to support our users across the globe. Now, it\u2019s easier to prove age and identity with Google Wallet.\n\nExpanding Availability: Residents in Arkansas, Montana, Puerto Rico and West Virginia will soon be able to save their government-issued digital IDs to Google Wallet. And in Arizona, Georgia, Maryland and New Mexico, users will also be able to use their mobile IDs at the DMV for improved and streamlined customer experiences.\n\nU.K. Passport Support: U.K. passport holders will soon be able to create digital ID passes with their U.K. passports and securely and conveniently store them in Google Wallet. At launch, we\u2019re partnering with Rail Delivery Group, which will offer train travellers the opportunity to use their digital ID to verify that they meet the eligibility criteria for select Railcards on its Railcard retailing platform railcard.co.uk.\n\nNew use cases: New use cases are on the way in collaboration with our strong partner ecosystem. Soon, you'll be able to use your digital ID to recover Amazon accounts, access online health services with CVS Health and MyChart by Epic, verify profiles on platforms like Uber and more.\n\nIntroducing the Digital Credentials API: To empower you to leverage these digital IDs, we collaborated with ecosystem partners in the W3C to develop our Digital Credentials API. This unified and secure framework allows apps and websites to request verifiable proof of age or identity directly from any digital wallet on a user's device.\n\n\n\nConnecting with families: Google Wallet enabled for kids\n\nWe're very excited to provide parents and guardians a way to allow their children to access Google Wallet with appropriate supervision.\n\nParents and guardians in select countries can now allow their kids to tap and pay in stores, plus keep supported passes like event tickets, library cards, and gift cards, all in one place, on their Android devices. Safety is key, so parents have controls with Family Link. They will get notified by email about every transaction, and can easily track recent purchases, remove payment cards, and turn off passes access in Family Link.\n\nBest of all, there are no changes in the Google Wallet API to support this enhancement.\n\n\n\nElevating user engagement: more granular notifications\n\nLast year, we expanded mobile push notifications for the Google Wallet API, empowering you to deliver timely updates to your users. This year, we're taking it a step further with the introduction of field update notifications.\n\nImagine being able to trigger a push notification not just on a general update, but specifically when a particular field within a pass changes. For example, when a user's points balance crosses a threshold, or when their tier status is upgraded, a push notification can be triggered which will notify them that the data on their pass has changed. This granularity allows for a much more engaging and user-centric experience, driving higher engagement and utility on the pass.\n\n\n\nProximity power: introducing Nearby Passes Notifications\n\nSpeaking of notifications, we\u2019re really excited to announce that we will support Nearby Passes notifications for the Google Wallet API. If enabled by the user, this feature is designed to provide timely and relevant information by alerting users about pertinent passes, such as loyalty cards, offers, boarding passes, or event tickets, when they approach a designated point of interest.\n\nThe Google Wallet app sends a contextual notification when the user is near a specific location. This notification serves as a direct gateway, allowing users to seamlessly access the associated pass with a single tap. This direct access promotes a more fluid and intuitive interaction with the stored passes, encouraging users to leverage the wallet's capabilities more effectively.\n\nTo ensure user control and flexibility, we\u2019ve introduced two new toggles to help users control their notification experience. The first is on the pass details screen, that allows users to turn on or off notifications from that specific pass. This applies to all notifications related to your pass, including field updates, and nearby passes notifications. The second is through the Nearby Passes notifications channel that allows users to control whether they receive nearby passes notifications. This empowers users to tailor their notification settings based on their specific needs and preferences.\n\n\n\nBeyond transactions: unlocking Value Added Opportunities\n\nEngaging users goes beyond just notifications. To address this, we\u2019re introducing Value Added Opportunities which empowers you to integrate personalized modules directly into your passes, showcasing relevant deals, promotions, and additional services.\n\nThis is a significant step towards transforming passes into a more dynamic engagement surface for you and your users. By highlighting these value-added benefits, such as exclusive offers or upgrade options, you can guide users back to your app or website, creating a dynamic gateway for ongoing user interaction.\n\n\n\nBridging the gap: introducing the Pass Upgrade experience\n\nUsers have saved millions of passes by manually adding their loyalty cards to Google Wallet. These passes are unlinked from their merchant accounts, and, as a developer, you can\u2019t update, or engage users on these passes. We\u2019ve now introduced a Pass Upgrade experience that prompts users to sign into their merchant account and save a linked version of the pass. All you need to do is integrate with the Wallet API\u2019s User Loyalty Program Enrollment feature, and ensure that users can save the linked pass once they successfully sign in. We\u2019ll be expanding on this feature in the future to enable users who have added a pass using our \u201ceverything else\u201d feature to link to the merchant account as well.\n\n\n\nSeamless journeys: enhancements for travel\n\nAt last year's I/O, we announced Auto Linked Passes, allowing you to add an additional related pass automatically to your users\u2019 Google Wallet provided they already have an existing pass issued by you. Now, we're happy to announce an expansion of this feature for airlines. Developers from airlines that integrate loyalty cards for their frequent flyer programs with the Google Wallet API can automatically push boarding passes to their users\u2019 wallets once they check in for a flight.\n\nGoogle Wallet users already benefit from streamlined travel on open-loop EMV transit systems, using tokenized payment methods for seamless fare transactions. Our solution further enhances this experience by providing riders with detailed journey and fare construction details. The tokenized open loop payment card acts as a bridge between user payments and transit systems, facilitating this communication.\n\nBuilding on this foundation, we're excited to announce the upcoming expansion of Google Transit Insights to support specific pass types sales, such as season passes, directly linked to the users' tokenized open loop payment card. With this new capability, developers will soon be able to leverage the Google Wallet APIs to implement seamless pass purchasing and management, eliminating the need for separate transit cards.\n\nUsers of Google Wallet will now see real-time transit pass updates, such as on-time or delayed train status, directly on their passes. This enhanced experience is powered by the Google Wallet API's new live status support and a seamless integration with Google Maps. Train operators can enable this feature by ensuring their tickets contain the required fields and providing a real-time GTFS feed, making it accessible to major operators worldwide.\n\n\n\nPersonalization and security: Secure Private Images\n\nFor some use cases, it\u2019s important for you to create a pass that includes a profile photo of the user you\u2019re distributing the pass to. To achieve this, we\u2019re enabling Secure Private Images on passes.\n\nWith this feature, you\u2019re able to define passes that include images that are only accessible to the holder of the pass. For example, this allows you to create digital business cards, membership passes, or event tickets with the user profile image on it.\n\nPlease note that this feature can\u2019t be used for official identity verification.\n\n\n\nLearn more and build with Google Wallet\n\nGoogle Wallet continues to evolve, offering developers powerful new capabilities to create richer, more engaging user experiences. From expanding digital identity features like the Digital Credentials API and secure private images, to enhancing user engagement with granular notifications and Value Added Opportunities, and streamlining travel with expanded Auto Linked Passes and real-time transit updates, these features are designed to help you build innovative solutions that connect more deeply with your users and unlock new value within the Google Wallet platform.\n\nPlease take a look at the following resources to learn more:\n\nLearn everything about Google Wallet in the developer documentation website.\n\nStay tuned with upcoming and past events at our events page.\n\nTry one of our codelabs to have a hands-on code experience.\n\nUse one of our client libraries for your favorite language/platform.", "label": 0}
{"title": "Scaling Real-Time SignalR Applications on Heroku", "url": "https://www.heroku.com/blog/scaling-real-time-signalr-applications-on-heroku/", "content": "SignalR makes it easy to add real-time functionality to .NET web applications\u2014things like live chat, instant notifications, or interactive dashboards. But what happens when your app starts to grow? A single server can only take you so far. At some point, you\u2019ll need to scale out.\n\nIn this post, we\u2019ll walk through what it takes to scale a SignalR app to run across multiple servers. We\u2019ll start with the basics, then show you how to use Redis as a backplane and enable sticky sessions to keep WebSocket connections stable. And we\u2019ll deploy it all to Heroku. If you\u2019re curious about what it takes to run a real-time app across multiple dynos, this guide is for you.\n\nIntroduction to our app\n\nFor my demo application, I started with Microsoft\u2019s tutorial project on building a real-time application using SignalR, found here. Because we\u2019re focusing on how to scale a SignalR application, we won\u2019t spend too much time covering how to build the original application.\n\nYou can access the code used for this demo in our GitHub repository. I\u2019ll briefly highlight a few pieces.\n\nI used .NET 9.0 ( 9.0.203 at the time of writing). To start, I created a new web application:\n\n~$ dotnet new webapp -o SignalRChat The template \"ASP.NET Core Web App (Razor Pages)\" was created successfully. This template contains technologies from parties other than Microsoft, see https://aka.ms/aspnetcore/9.0-third-party-notices for details. Processing post-creation actions... Restoring /home/user/SignalRChat/SignalRChat.csproj: Restore succeeded\n\nThen, I installed LibMan to get the JavaScript client library for our SignalR project.\n\n~/SignalRChat$ dotnet tool install -g Microsoft.Web.LibraryManager.Cli ~/SignalRChat$ libman install @microsoft/signalr@latest \\ -p unpkg \\ -d wwwroot/js/signalr \\ --files dist/browser/signalr.js\n\nWith my dependencies in place, I created the following files:\n\nhubs/ChatHub.cs : The hub class that serves as a high-level pipeline and handles client-server communication.\n\n: The hub class that serves as a high-level pipeline and handles client-server communication. Pages/Index.cshtml : The main Razor file, combining HTML and embedded C# with Razor syntax.\n\n: The main Razor file, combining HTML and embedded C# with Razor syntax. wwwroot/js/chat.js : The chat logic for the application.\n\nLastly, I had the main application code in Program.cs :\n\nusing SignalRChat.Hubs; var builder = WebApplication.CreateBuilder(args); // Add services to the container. builder.Services.AddRazorPages(); builder.Services.AddSignalR(); var app = builder.Build(); // Configure the HTTP request pipeline. if (!app.Environment.IsDevelopment()) { app.UseExceptionHandler(\"/Error\"); // The default HSTS value is 30 days. You may want to change this for production scenarios, see https://aka.ms/aspnetcore-hsts. app.UseHsts(); } app.UseHttpsRedirection(); app.UseStaticFiles(); app.UseRouting(); app.UseAuthorization(); app.MapRazorPages(); app.MapHub(\"/chatHub\"); app.Run();\n\nYou\u2019ll notice in this initial version that I\u2019ve added SignalR, but I haven\u2019t configured it to use a Redis backplane yet. We\u2019ll iterate and get there soon.\n\nFor a sanity check, I tested my application.\n\n~/SignalRChat$ dotnet build Restore complete (0.2s) SignalRChat succeeded (3.1s) \u2192 bin/Debug/net9.0/SignalRChat.dll Build succeeded in 3.7s ~/SignalRChat$ dotnet run Using launch settings from /home/user/SignalRChat/Properties/launchSettings.json... Building... info: Microsoft.Hosting.Lifetime[14] Now listening on: http://localhost:5028 info: Microsoft.Hosting.Lifetime[0] Application started. Press Ctrl+C to shut down. info: Microsoft.Hosting.Lifetime[0] Hosting environment: Development info: Microsoft.Hosting.Lifetime[0] Content root path: /home/user/SignalRChat\n\nIn one browser, I navigated to http://localhost:5028 . Then, with a different browser, I navigated to the same page.\n\nI verified that both browsers had WebSocket connections to my running application, and I posted a message from each browser.\n\nIn real time, the messages posted in one browser were displayed in the other. My app was up and running.\n\nNow, it was time to scale.\n\nHow To scale SignalR\n\nScaling a SignalR app isn\u2019t as simple as just adding more servers. Out of the box, each server maintains its own list of connected clients. That means if a user is connected to server A, and a message is sent through server B, that user won\u2019t receive it\u2014unless there\u2019s a mechanism to synchronize messages across all servers. This is where scaling gets tricky.\n\nTo pull this off, you need two things:\n\nBackplane : The backplane handles message coordination between servers. It ensures that when one instance of your app sends a message, all other instances relay that message to their connected clients. Redis is commonly used for this purpose because it\u2019s fast, lightweight, and supported natively by SignalR.\n\n: The backplane handles message coordination between servers. It ensures that when one instance of your app sends a message, all other instances relay that message to their connected clients. Redis is commonly used for this purpose because it\u2019s fast, lightweight, and supported natively by SignalR. Sticky sessions: WebSockets are long-lived connections, and if your app is spread across multiple servers, you can\u2019t have a user\u2019s connection bouncing between them. Sticky sessions make sure all of a user\u2019s requests are routed to the same server, which keeps WebSocket connections stable and prevents dropped connections during scale-out.\n\nBy combining these two techniques, you set your SignalR app up to handle real-time communication at scale. Let\u2019s walk through how I did this.\n\nUsing Redis as a backplane\n\nThe first task in scaling up meant modifying my application to use Redis as a backplane. First, I added the StackExchange.Redis package for .NET.\n\n~/SignalRChat$ dotnet add package \\ Microsoft.AspNetCore.SignalR.StackExchangeRedis\n\nThen, I modified Program.cs , replacing the original builder.Services.AddSignalR(); line with the following:\n\nvar redisUrl = Environment.GetEnvironmentVariable(\"REDIS_URL\") ?? \"localhost:6379\"; if (redisUrl == \"localhost:6379\") { builder.Services.AddSignalR().AddStackExchangeRedis(redisUrl, options => { options.Configuration.ChannelPrefix = RedisChannel.Literal(\"SignalRChat\"); options.Configuration.Ssl = redisUrl.StartsWith(\"rediss://\"); options.Configuration.AbortOnConnectFail = false; }); } else { var uri = new Uri(redisUrl); var userInfoParts = uri.UserInfo.Split(':'); if (userInfoParts.Length != 2) { throw new InvalidOperationException(\"REDIS_URL is not in the expected format ('redis://user:password@host:port')\"); } var configurationOptions = new ConfigurationOptions { EndPoints = { { uri.Host, uri.Port } }, Password = userInfoParts[1], Ssl = true, }; configurationOptions.CertificateValidation += (sender, cert, chain, errors) => true; builder.Services.AddSignalR(options => { options.ClientTimeoutInterval = TimeSpan.FromSeconds(60); // default is 30 options.KeepAliveInterval = TimeSpan.FromSeconds(15); // default is 15 }).AddStackExchangeRedis(redisUrl, options => { options.Configuration = configurationOptions; }); }\n\nThe above code configures the SignalR application to use Redis, connecting via a default address ( localhost:6379 ) or through a connection string in the environment variable, REDIS_URL . Using REDIS_URL is an example of me thinking ahead, as I plan to deploy this application to Heroku with the Heroku Key-Value Store add-on.\n\nFor how to set up the Redis connection between my .NET application and my Heroku Key-Value Store add-on, I took my cues from here.\n\nWith Program.cs modified to use Redis as a backplane, I tested my application locally again.\n\n~/SignalRChat$ dotnet run\n\nThis time, with my two browser windows open, I also opened a terminal and connected to my local Redis instance, running on port 6379 . I listed the Pub/Sub channels and then subscribed to the main ChatHub channel.\n\n127.0.0.1:6379> pubsub channels 1) \"SignalRChat__Booksleeve_MasterChanged\" 2) \"SignalRChatSignalRChat.Hubs.ChatHub:internal:ack:demo_b3204c22a84c9\" 3) \"SignalRChatSignalRChat.Hubs.ChatHub:internal:return:demo_b3204c22a84c9\" 4) \"SignalRChatSignalRChat.Hubs.ChatHub:all\" 5) \"SignalRChatSignalRChat.Hubs.ChatHub:internal:groups\" 127.0.0.1:6379> subscribe SignalRChatSignalRChat.Hubs.ChatHub:all Reading messages... (press Ctrl-C to quit) 1) \"subscribe\" 2) \"SignalRChatSignalRChat.Hubs.ChatHub:all\" 3) (integer) 1\n\nIn one browser, I sent a message. Then, in the other, I sent a reply. Here\u2019s what came across in my Redis CLI:\n\n1) \"message\" 2) \"SignalRChatSignalRChat.Hubs.ChatHub:all\" 3) \"\\x92\\x90\\x81\\xa4json\\xc4W{\\\"type\\\":1,\\\"target\\\":\\\"ReceiveMessage\\\",\\\"arguments\\\":[\\\"Chrome User\\\",\\\"This is my message.\\\"]}\\x1e\" 1) \"message\" 2) \"SignalRChatSignalRChat.Hubs.ChatHub:all\" 3) \"\\x92\\x90\\x81\\xa4json\\xc4Y{\\\"type\\\":1,\\\"target\\\":\\\"ReceiveMessage\\\",\\\"arguments\\\":[\\\"Firefox User\\\",\\\"And this is a reply.\\\"]}\\x1e\"\n\nI successfully verified that my SignalR application was using Redis as its backplane. Scaling task one of two was complete!\n\nMoving onto sticky sessions, I would need to scale. For that, I needed to deploy to Heroku.\n\nDeploying to Heroku\n\nDeploying my Redis-backed application to Heroku was straightforward. Here were the steps:\n\nStep #1: Login\n\n~/SignalRChat$ heroku login\n\nStep #2: Create app\n\n~/SignalRChat$ heroku create signalr-chat-demo Creating \u2b22 signalr-chat-demo... done https://signalr-chat-demo-b49ac4212f6d.herokuapp.com/ | https://git.heroku.com/signalr-chat-demo.git\n\nStep #3: Add the Heroku Key-Value Store add-on\n\n~/SignalRChat$ heroku addons:add heroku-redis Creating heroku-redis on \u2b22 signalr-chat-demo... ~$0.004/hour (max $3/month) Your add-on should be available in a few minutes. ! WARNING: Data stored in essential plans on Heroku Redis are not persisted. redis-solid-16630 is being created in the background. The app will restart when complete... Use heroku addons:info redis-solid-16630 to check creation progress Use heroku addons:docs heroku-redis to view documentation\n\nI waited a few minutes for Heroku to create my add-on. After this was completed, I had access to REDIS_URL .\n\n~/SignalRChat$ heroku config === signalr-chat-demo Config Vars REDIS_URL: rediss://:pcbcd9558e402ff2615a4484ac5ca9ac373f811e53bcb17f81ada3c243f8a11cc@ec2-52-20-254-181.compute-1.amazonaws.com:8150\n\nStep #4: Add a Procfile\n\nNext, I added a file called Procfile to my root project folder. The Procfile tells Heroku how to start up my app. It has one line:\n\nweb: cd bin/publish; ./SignalRChat --urls http://*:$PORT\n\nStep #5: Push code to Heroku\n\n~/SignalRChat$ git push heroku main \u2026 remote: -----> Building on the Heroku-24 stack remote: -----> Using buildpack: heroku/dotnet remote: -----> .NET app detected remote: -----> SDK version detection remote: Detected .NET project: `/tmp/build_ad246347/SignalRChat.csproj` remote: Inferring version requirement from `/tmp/build_ad246347/SignalRChat.csproj` remote: Detected version requirement: `^9.0` remote: Resolved .NET SDK version `9.0.203` (linux-amd64) remote: -----> SDK installation remote: Downloading SDK from https://builds.dotnet.microsoft.com/dotnet/Sdk/9.0.203/dotnet-sdk-9.0.203-linux-x64.tar.gz ... (0.7s) remote: Verifying SDK checksum remote: Installing SDK remote: -----> Publish app \u2026 remote: -----> Launching... remote: Released v4 remote: https://signalr-chat-demo-b49ac4212f6d.herokuapp.com/ deployed to Heroku remote: remote: Verifying deploy... done.\n\nStep #6: Test Heroku app\n\nIn my two browser windows, I navigated to my Heroku app URL (in my case, https://signalr-chat-demo-b49ac4212f6d.herokuapp.com/ ) and tested sending messages to the chat.\n\nI also had a terminal window open, connecting to my Heroku Key-Value Store add-on via heroku redis:cli . Just like I did when testing locally, I subscribed to the main chat channel. As I sent messages, they came across in Redis.\n\nredis:8150> subscribe SignalRChat.Hubs.ChatHub:all 1) subscribe 2) SignalRChat.Hubs.ChatHub:all 3) 2 redis:8150> 1) message 2) SignalRChat.Hubs.ChatHub:all 3) ''''json'R{\"type\":1,\"target\":\"ReceiveMessage\",\"arguments\":[\"Chrome User\",\"I'm on Heroku!\"]} redis:8150> 1) message 2) SignalRChat.Hubs.ChatHub:all 3) ''''json'M{\"type\":1,\"target\":\"ReceiveMessage\",\"arguments\":[\"Firefox User\",\"So am I!\"]}\n\nAs another sanity check, I looked in my developer tools console in my browser. Looking in the Network Inspector, I saw a stable WebSocket connection ( wss:// ) as well as the inbound and outbound connection data.\n\nI had successfully deployed to Heroku, using Redis as my backplane. I hadn\u2019t scaled up to multiple dynos just yet, but everything was looking smooth so far.\n\nScaling with Multiple Dynos\n\nNext, I needed to scale up to use multiple dynos. With Heroku, this is simple. However, you can\u2019t scale up with Eco or Basic dynos. So, I needed to change my dyno type to the next level up: standard-1x .\n\n~/SignalRChat$ heroku ps:type web=standard-1x Scaling dynos on signalr-chat-demo... done === Process Types Type Size Qty Cost/hour Max cost/month \u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 web Standard-1X 1 ~$0.035 $25 === Dyno Totals Type Total \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500 Standard-1X 1\n\nWith my dyno type set, I could scale up to use multiple dynos. I went with three.\n\n~/SignalRChat$ heroku ps:scale web=3 Scaling dynos... done, now running web at 3:Standard-1X\n\nMaintaining WebSocket Connections with Sticky Sessions\n\nI reloaded the application in my browser. Now, my inspector console showed an issue:\n\nHere\u2019s the error:\n\nError: Failed to start the transport 'WebSockets': Error: WebSocket failed to connect. The connection could not be found on the server, either the endpoint may not be a SignalR endpoint, the connection ID is not present on the server, or there is a proxy blocking WebSockets. If you have multiple servers check that sticky sessions are enabled.\n\nThat\u2019s a pretty helpful error message. Just as we had expected, our real-time SignalR application would run into issues once we scaled up to multiple dynos. What was the solution? Sticky sessions with Heroku\u2019s session affinity feature.\n\nEnabling Heroku session affinity\n\nThis feature from Heroku works to keep all HTTP requests coming from a client consistently routed to a single dyno. It\u2019s easy to set up, and it would solve our multi-dyno WebSocket connection issue.\n\n~/SignalRChat$ heroku features:enable http-session-affinity Enabling http-session-affinity for \u2b22 signalr-chat-demo... done\n\nThat was it. With sticky sessions enabled, I was ready to test again.\n\nTesting with sticky sessions on multiple dynos\n\nI reloaded the application in both browsers. This time, my network inspector showed no errors. It looked like I had a stable WebSocket connection.\n\nReal-time chat messages were sent and received without any problems.\n\nSuccess!\n\nWrapping Up\n\nWith Redis as a backplane and sticky sessions enabled, our SignalR app scaled seamlessly across multiple dynos on Heroku. It delivered real-time messages smoothly, and the WebSocket connections remained stable even under a scaled-out setup.\n\nThe takeaway? You don\u2019t need a complicated setup to scale SignalR, just the right combination of tooling and configuration. Whether you\u2019re building chat apps, live dashboards, or collaborative tools, you now have a tested approach to scale real-time experiences with confidence.\n\nReady to build and deploy your own scalable SignalR application? Check out the .NET Getting Started guide for foundational knowledge. For a visual walkthrough of deploying .NET applications to Heroku, watch our Deploying .NET Applications on Heroku video.", "label": 0}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2023-03", "content": "en\n\nIntroduction\n\nI started messing with AI a lot more. One of my students got me doing more with it! Thanks Ryan! I started out with ChatGPT from OpenAI. I've been thinking about tv show crossovers. Crossovers are fun! Especially on animated shows.\n\nEvery since I was a kid I've really enjoyed two animated shows, Digimon Frontier and Code Lyoko. They both involved the digital/virtual world and the real world.\n\nCode Lyoko tends to show a lot more regarding how \"AI\" can go wrong. I will write a blog post on Code Lyoko very soon!\n\nBackground on Code Lyoko and Digimon Frontier\n\nCode Lyoko is a French animated television series that was produced by MoonScoop Group and aired from 2003 to 2007. It follows the adventures of a group of four students - Jeremie, Ulrich, Odd, and Yumi - who discover a virtual world called Lyoko. In Lyoko, they fight against an evil artificial intelligence named XANA that seeks to take over both the virtual and real worlds.\n\nDigimon Frontier is the fourth season of the Digimon anime series. It aired in Japan from 2002 to 2003 and was dubbed into English and aired in North America in 2004. The show follows a group of five children - Takuya, Koji, Zoe, J.P., and Tommy - who are transported to the Digital World to fight against an evil force known as the Royal Knights.\n\nUnlike previous seasons of Digimon, the children in Digimon Frontier transform into Digimon themselves and fight alongside their partner Digimon. Each of the children has a unique spirit that allows them to transform into a different Digimon, and they must learn to master their powers in order to save the Digital World.\n\nThe show is known for its action-packed battles, complex characters, and intricate\n\nChatGPT Combining Code Lyoko and Digimon Frontier\n\nI ask ChatGPT to create a story line combining both worlds and I got this. I really like this.!\n\nCombining Digimon Frontier and Code Lyoko would create an interesting story that combines the elements of both shows. The concept of a virtual world threatened by an evil force is a common thread between the two shows, and could serve as a basis for a crossover.\n\nIn this hypothetical scenario, the characters from Digimon Frontier would find themselves transported to the world of Code Lyoko, where they must team up with the Lyoko warriors to defeat XANA and save both the digital and real worlds from destruction. The Digimon would bring their unique powers and abilities to the fight, while the Lyoko warriors would provide their expertise in hacking and computer programming.\n\nTogether, the two groups would explore the virtual world of Lyoko, battling monsters and uncovering XANA's plans. The conflict between XANA and the Digimon's arch-nemesis, the evil Digimon Emperor, could also be woven into the story to add an additional layer of complexity.\n\nOverall, a crossover between Digimon Frontier and Code Lyoko could create an exciting and action-packed adventure that would appeal to fans of both shows.\n\nQuestion\n\nDo you think we will ever get to this point where we need to fight the evils of the digital/virtual worlds? Honestly, I think we will someday. I'd love to hear from you all.", "label": 1}
{"title": "Noisy Neighbor Detection with eBPF", "url": "https://netflixtechblog.com/noisy-neighbor-detection-with-ebpf-64b1f4b3bbdd?source=collection_home---4------16-----------------------", "content": "The sched_wakeup and sched_wakeup_new hooks are invoked when a process changes state from 'sleeping' to 'runnable.' They let us identify when a process is ready to run and is waiting for CPU time. During this event, we generate a timestamp and store it in an eBPF hash map using the process ID as the key.\n\nstruct {\n\n__uint(type, BPF_MAP_TYPE_HASH);\n\n__uint(max_entries, MAX_TASK_ENTRIES);\n\n__uint(key_size, sizeof(u32));\n\n__uint(value_size, sizeof(u64));\n\n} runq_enqueued SEC(\".maps\");\n\n\n\nSEC(\"tp_btf/sched_wakeup\")\n\nint tp_sched_wakeup(u64 *ctx)\n\n{\n\nstruct task_struct *task = (void *)ctx[0];\n\nu32 pid = task->pid;\n\nu64 ts = bpf_ktime_get_ns();\n\n\n\nbpf_map_update_elem(&runq_enqueued, &pid, &ts, BPF_NOEXIST);\n\nreturn 0;\n\n}\n\nConversely, the sched_switch hook is triggered when the CPU switches between processes. This hook provides pointers to the process currently utilizing the CPU and the process about to take over. We use the upcoming task's process ID (PID) to fetch the timestamp from the eBPF map. This timestamp represents when the process entered the queue, which we had previously stored. We then calculate the run queue latency by simply subtracting the timestamps.\n\nSEC(\"tp_btf/sched_switch\")\n\nint tp_sched_switch(u64 *ctx)\n\n{\n\nstruct task_struct *prev = (struct task_struct *)ctx[1];\n\nstruct task_struct *next = (struct task_struct *)ctx[2];\n\nu32 prev_pid = prev->pid;\n\nu32 next_pid = next->pid;\n\n\n\n// fetch timestamp of when the next task was enqueued\n\nu64 *tsp = bpf_map_lookup_elem(&runq_enqueued, &next_pid);\n\nif (tsp == NULL) {\n\nreturn 0; // missed enqueue\n\n}\n\n\n\n// calculate runq latency before deleting the stored timestamp\n\nu64 now = bpf_ktime_get_ns();\n\nu64 runq_lat = now - *tsp;\n\n\n\n// delete pid from enqueued map\n\nbpf_map_delete_elem(&runq_enqueued, &next_pid);\n\n....\n\nOne of the advantages of eBPF is its ability to provide pointers to the actual kernel data structures representing processes or threads, also known as tasks in kernel terminology. This feature enables access to a wealth of information stored about a process. We required the process's cgroup ID to associate it with a container for our specific use case. However, the cgroup information in the process struct is safeguarded by an RCU (Read Copy Update) lock.\n\nTo safely access this RCU-protected information, we can leverage kfuncs in eBPF. kfuncs are kernel functions that can be called from eBPF programs. There are kfuncs available to lock and unlock RCU read-side critical sections. These functions ensure that our eBPF program remains safe and efficient while retrieving the cgroup ID from the task struct.\n\nvoid bpf_rcu_read_lock(void) __ksym;\n\nvoid bpf_rcu_read_unlock(void) __ksym;\n\n\n\nu64 get_task_cgroup_id(struct task_struct *task)\n\n{\n\nstruct css_set *cgroups;\n\nu64 cgroup_id;\n\nbpf_rcu_read_lock();\n\ncgroups = task->cgroups;\n\ncgroup_id = cgroups->dfl_cgrp->kn->id;\n\nbpf_rcu_read_unlock();\n\nreturn cgroup_id;\n\n}\n\nOnce the data is ready, we must package it and send it to userspace. For this purpose, we chose the eBPF ring buffer. It is efficient, high-performing, and user-friendly. It can handle variable-length data records and allows data reading without necessitating extra memory copying or syscalls. However, the sheer number of data points was causing the userspace program to use too much CPU, so we implemented a rate limiter in eBPF to sample the data.\n\nstruct {\n\n__uint(type, BPF_MAP_TYPE_RINGBUF);\n\n__uint(max_entries, RINGBUF_SIZE_BYTES);\n\n} events SEC(\".maps\");\n\n\n\nstruct {\n\n__uint(type, BPF_MAP_TYPE_PERCPU_HASH);\n\n__uint(max_entries, MAX_TASK_ENTRIES);\n\n__uint(key_size, sizeof(u64));\n\n__uint(value_size, sizeof(u64));\n\n} cgroup_id_to_last_event_ts SEC(\".maps\");\n\n\n\nstruct runq_event {\n\nu64 prev_cgroup_id;\n\nu64 cgroup_id;\n\nu64 runq_lat;\n\nu64 ts;\n\n};\n\n\n\nSEC(\"tp_btf/sched_switch\")\n\nint tp_sched_switch(u64 *ctx)\n\n{\n\n// ....\n\n// The previous code\n\n// ....\n\n\n\nu64 prev_cgroup_id = get_task_cgroup_id(prev);\n\nu64 cgroup_id = get_task_cgroup_id(next);\n\n\n\n// per-cgroup-id-per-CPU rate-limiting\n\n// to balance observability with performance overhead\n\nu64 *last_ts =\n\nbpf_map_lookup_elem(&cgroup_id_to_last_event_ts, &cgroup_id);\n\nu64 last_ts_val = last_ts == NULL ? 0 : *last_ts;\n\n\n\n// check the rate limit for the cgroup_id in consideration\n\n// before doing more work\n\nif (now - last_ts_val < RATE_LIMIT_NS) {\n\n// Rate limit exceeded, drop the event\n\nreturn 0;\n\n}\n\n\n\nstruct runq_event *event;\n\nevent = bpf_ringbuf_reserve(&events, sizeof(*event), 0);\n\n\n\nif (event) {\n\nevent->prev_cgroup_id = prev_cgroup_id;\n\nevent->cgroup_id = cgroup_id;\n\nevent->runq_lat = runq_lat;\n\nevent->ts = now;\n\nbpf_ringbuf_submit(event, 0);\n\n// Update the last event timestamp for the current cgroup_id\n\nbpf_map_update_elem(&cgroup_id_to_last_event_ts, &cgroup_id,\n\n&now, BPF_ANY);\n\n\n\n}\n\n\n\nreturn 0;\n\n}\n\nOur userspace application, developed in Go, processes events from the ring buffer to emit metrics to our metrics backend, Atlas. Each event includes a run queue latency sample with a cgroup ID, which we associate with containers running on the host. We categorize it as a system service if no such association is found. When a cgroup ID is associated with a container, we emit a percentile timer Atlas metric ( runq.latency ) for that container. We also increment a counter metric ( sched.switch.out ) to monitor preemptions occurring for the container's processes. Access to the prev_cgroup_id of the preempted process allows us to tag the metric with the cause of the preemption, whether it's due to a process within the same container (or cgroup), a process in another container, or a system service.\n\nIt's important to highlight that both the runq.latency metric and the sched.switch.out metrics are needed to determine if a container is affected by noisy neighbors, which is the goal we aim to achieve \u2014 relying solely on the runq.latency metric can lead to misconceptions. For example, if a container is at or over its cgroup CPU limit, the scheduler will throttle it, resulting in an apparent spike in run queue latency due to delays in the queue. If we were only to consider this metric, we might incorrectly attribute the performance degradation to noisy neighbors when it's actually because the container is hitting its CPU quota. However, simultaneous spikes in both metrics, mainly when the cause is a different container or system process, clearly indicate a noisy neighbor issue.\n\nA Noisy Neighbor Story", "label": 0}
{"title": "Meta Open Source: 2024 by the numbers", "url": "https://engineering.fb.com/2025/04/02/open-source/meta-open-source-by-the-numbers/", "content": "Open source has played an essential role in the tech industry and beyond. Whether in the AI/ML, web, or mobile space, our open source community grew and evolved while connecting people worldwide.\n\nAt Meta Open Source, 2024 was a year of growth and transformation. Our open source initiatives addressed the evolving needs and challenges of developers\u2014powering breakthroughs in AI and enabling the creation of innovative, user-focused applications and experiences. In close collaboration with the open source community, we shared knowledge, introduced new projects, and enhanced existing ones.\n\nIn this post, we look at our portfolio of open source projects through numbers to give a better view of the scale of the community we interact with daily.\n\nAt Meta, we have several GitHub organizations where we publish new open source projects, maintain existing ones, and hold already archived projects. They include various tools, frameworks, and platforms for web, mobile, AI/ML, and hardware industries.\n\nBy the end of last year, we launched 256 brand-new repositories, bringing active public projects to 944. This number excludes archived repositories and projects that we moved to foundations.\n\nIn 2024, our open source codebases grew at an impressive pace, reaching 189,719 total commits in just one year. Community contributors accounted for 71,018, while Meta employees made the remaining 118,701.\n\nOpen source cannot exist without people collaborating, sharing, and innovating. A total of 4,274 external contributors helped bring our community to 7,144 strong. This remarkable community is what fuels the ongoing evolution of Meta Open Source.\n\nBeyond individual contributions, our projects on GitHub accumulated an additional 151,380 stars, bringing the total to a staggering 1.8 million. This growth in engagement shows strong interest and excitement for Meta Open Source projects.\n\nThank you to the open source community\n\nAt Meta, we believe open source accelerates the pace of innovation in the world. By sharing our technologies, we aim to move the industry forward while allowing other companies and individuals to use our solutions to scale more quickly and build great products.\n\nAt the same time, Meta Open Source projects are made possible by contributions from developers like you. Pull requests, documentation updates, social media posts, and everything in between are what build connections in our communities. Thank you all for another great year for open source.\n\nTo learn more about Meta Open Source, visit our open source site, subscribe to our YouTube channel, or follow us on Facebook, Threads, X, and LinkedIn.", "label": 0}
{"title": "Why we built a custom Rust library for Capture", "url": "https://dropbox.tech/application/why-we-built-a-custom-rust-library-for-capture", "content": "In some respects, it was an easy decision. Dropbox has a thriving community of developers building Rust into our products. Rust is at the heart of our desktop client\u2019s recently re-written sync engine , which is what makes the Dropbox folder on your computer work like magic. We also use it for file compression , in our crash reporting infrastructure , and in Magic Pocket\u2014our exabyte scale custom storage infrastructure\u2014to optimize the storage of file data .\n\nThere were a lot of ways we could have solved these problems\u2014perhaps more TypeScript, or C++\u2014but in the end we decided to go with Rust.\n\nIdeally, we wanted streamlined a codebase that could target multiple platforms painlessly, consistently, and that was easy for our developers to build. We also wanted more control over our ability to take screen captures and recordings, better error handling, and faster performance behind the scenes. In fact, we were looking for something that would give us more control at every layer\u2014that didn\u2019t require us to jump through quite so many hoops to call native code\u2014and would better support the new features we wanted to build.\n\nOne of our team\u2019s guiding principles is \u201cbe a Margherita pizza.\u201d Just as a Margherita pizza is perfect in its simplicity\u2014what more do you need than tomato sauce, mozzarella, and basil?\u2014we\u2019ve tried to keep Capture\u2019s ingredients as simple and straightforward as possible. We knew early on that Electron and Node would make it easy to build a cross-platform TypeScript app for both macOS and Windows. But finding the right third ingredient that would enable us to quickly, simply, and reliably call native OS-level code took a bit more experimentation.\n\nDropbox Capture is a new visual communication tool designed to make it easy for teams to asynchronously share their work using screen recordings, video messages, screenshots, or GIFs. There's no formal onboarding required, and you can start sharing your ideas in seconds. In fact, simplicity is key to the Capture experience, and it's a value that also extends down to the development of Capture\u2019s underlying code.\n\nFind, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images\u2014plus generative AI capabilities across even more connected apps.\n\nIt turned out that Rust was perfect for our needs, too. Building a custom Rust library helped unlock higher-quality screen recording from 720p through 4K, made screenshots and screen recordings available to share more quickly, and dramatically improved our error handling capabilities, allowing us to offer a more reliable experience for our users.\n\nFrom Hack Week to here\n\nCapture began as an internal Hack Week project where rapid iteration was key. In early versions, we used a handful of third-party libraries to do things like take screenshots and process GIFs. Cobbling together bits of preexisting code helped us quickly develop a prototype, test out our initial assumptions, and experiment with new features. But when we considered the long-term health of Capture\u2019s codebase\u2014and all the complexity these third party libraries introduced\u2014we knew we would eventually have to pay our early technical debt down.\n\nThe third-party libraries we used were usually shell-based applications; Capture would send commands to each shell app and receive stderr and/or stdout responses in return. This meant spinning up an application each time we wanted to complete certain tasks, or in some cases having an application running continuously and awaiting input\u2014not exactly ideal.\n\nMore importantly, it also meant there was some inherent brittleness in the way Capture communicated with native code. Each line of output from the shell application had to be parsed. If a line failed to parse, we assumed it was an error, and if it was an error, the issue was likely masked and we wouldn\u2019t know exactly what broke in the native code. As you might expect, this made monitoring and handling errors difficult!\n\nFrom a developer standpoint, the libraries posed other challenges. We found APIs could be quite different between macOS and Windows\u2014even within the same cross-platform library\u2014which added complexity when developing for the two platforms. And while some libraries were well maintained but missing features that we needed, other libraries had everything we wanted but were not as well maintained. Each presented tradeoffs we had to work around, some more easily than others.\n\nFor example, if we wanted to make any changes to the individual libraries we\u2019d have to have the institutional knowledge of how to build each one and then build them into Capture. Case in point: It took one of our engineers hours of valuable development time to learn how to build the Windows screen recording library just to fix a single parsing bug!\n\n\n\nRust to the rescue\n\nBecause Rust was new to the Capture team, our early efforts were extremely incremental. Initially we focused on re-writing simple functions that otherwise required a third-party library. For example, activate-windows was previously a macOS-only library that let us bring a window to the forefront and only record that window. We were quickly able to port the feature to Rust on macOS, and then bring the feature to Windows where it didn\u2019t previously exist.\n\nThese early successes gave us the confidence to try more ambitious things. The more we learned, the more features we moved to our custom Rust library, which benefitted Capture in a handful of ways:\n\nNo overhead. With Neon-bindings we could now easily make calls to native OS code from TypeScript without any overhead (and more reliably, too). In other words, we no longer had to spin up separate shell applications to complete certain tasks. Taking screenshots, for example, which was once asynchronous\u2014requiring us to wait for a response from the shell application\u2014was now immediate and fast.\n\nBetter error handling. Rust also dramatically improved our ability to handle errors. Once most of Capture\u2019s code was running within our own library, and with a consistent API across both macOS and Windows, we were able to add more robust logging and monitoring. No more trying to interpret the output from shell apps! Having all of our code in one place gave us more insight into how our app actually was actually behaving.\n\nMore control. Ownership of the library meant fixes and enhancements could be made more quickly. Having all our code in one place also made it easier to tackle more nebulous issues\u2014like the instability we kept encountering when taking captures or recordings with more than three screens. It resulted in a simpler build pipeline across platforms, too.\n\nA smaller footprint. Not having to include third-party libraries also reduced the overall size of our app. Around 17MB of Swift libraries were no longer needed on macOS, for example, after re-writing those capabilities in Rust. And now that we could simply call functions as needed\u2014instead of having shell applications running in the background at all times\u2014we also needed less memory than before.\n\n\n\nNew features. As we found early on with activate-windows, moving to Rust also allowed us to do things we just couldn\u2019t do before. We were able to bring functionality to Windows that previously only existed on macOS. We were also able to introduce a new crop tool, new recording controls, and add new recording types like audio or camera only, as well as increase recording quality to 720p/1080p/4K. It\u2019s not so much that we couldn\u2019t have built these things with another language, but rather, Rust allowed us to build them faster and with less effort than before.\n\n\n\nWhat\u2019s next for Capture and Rust\n\nCapture is available now in Beta, and if you haven\u2019t already you should try it out.\n\n\n\nOne of our biggest surprises developing Capture was how easy it was to get started with Rust. In just a few weeks we were pushing Rust code to production, and we benefitted greatly from Dropbox\u2019s supportive community of Rust-savvy engineers who offered help and guidance whenever we got stuck. And as our team grows, there\u2019ll be less of a learning curve for new developers. Instead of wrestling with multiple libraries the way we used to, now we have a really simple document that basically says \u201chere\u2019s how to build everything using this single command.\u201d\n\nWith Rust, our developers know exactly what to expect\u2014just like our beloved Margherita pizza.\n\nIn time, we expect to move more features to our in-house library. The macOS screen recorder has already been re-written in Rust, and a similar re-write of the Windows recorder is on the way. We\u2019ve been moving features like GIF creation and other OS-level integrations into our Rust library, too. And we\u2019re especially excited for the future of Rust on Windows, which just recently reached v0.21.0.\n\nBut it\u2019s also not an all-or-nothing approach. We\u2019ve configured Rust so that we can still call third-party libraries using the old shell process approach if needed. This means we can be intentional about which features we choose to re-write and when. Of course, if we had struggled with Rust it would have been easy to turn back. But our enthusiasm and excitement turned out to be justified given the benefits Rust has unlocked.\n\n\n\nWe\u2019re hiring!\n\nDo you love Rust? Do you want to grow as an engineer? Dropbox is hiring!\n\nMany of our teams are solving problems big and small with Rust and other new technologies\u2014and Capture is no different. We're always on the lookout for clever, curious front-end engineers who want to learn new things, take on bigger challenges, and build products our customers love. If you're a front-end engineer with a talent, passion, and enthusiasm for Rust, we'd love to have you at Dropbox. Visit our careers page to apply.\n\nSpecial thanks to the Capture team (Youcef Es-skouri, Noga Raviv, Joey Diab, Kyle Shay, Andy Liu, Will Hall, Alex Pelan, Alan Chu, Mike Boht, Karan Khanna, Lien Chung, and Lily Lee) as well as Parker Timmerman and the rest of Dropbox\u2019s internal Rust developer community.", "label": 0}
{"title": "Foundation Model for Personalized Recommendation", "url": "https://netflixtechblog.com/foundation-model-for-personalized-recommendation-1a0bd8e02d39?source=collection_home---4------1-----------------------", "content": "Foundation Model for Personalized Recommendation Netflix Technology Blog 11 min read \u00b7 Mar 21, 2025 -- 36 Listen Share\n\nBy Ko-Jen Hsiao, Yesu Feng and Sudarshan Lamkhede\n\nMotivation\n\nNetflix\u2019s personalized recommender system is a complex system, boasting a variety of specialized machine learned models each catering to distinct needs including \u201cContinue Watching\u201d and \u201cToday\u2019s Top Picks for You.\u201d (Refer to our recent overview for more details). However, as we expanded our set of personalization algorithms to meet increasing business needs, maintenance of the recommender system became quite costly. Furthermore, it was difficult to transfer innovations from one model to another, given that most are independently trained despite using common data sources. This scenario underscored the need for a new recommender system architecture where member preference learning is centralized, enhancing accessibility and utility across different models.\n\nParticularly, these models predominantly extract features from members\u2019 recent interaction histories on the platform. Yet, many are confined to a brief temporal window due to constraints in serving latency or training costs. This limitation has inspired us to develop a foundation model for recommendation. This model aims to assimilate information both from members\u2019 comprehensive interaction histories and our content at a very large scale. It facilitates the distribution of these learnings to other models, either through shared model weights for fine tuning or directly through embeddings.\n\nThe impetus for constructing a foundational recommendation model is based on the paradigm shift in natural language processing (NLP) to large language models (LLMs). In NLP, the trend is moving away from numerous small, specialized models towards a single, large language model that can perform a variety of tasks either directly or with minimal fine-tuning. Key insights from this shift include:\n\nA Data-Centric Approach: Shifting focus from model-centric strategies, which heavily rely on feature engineering, to a data-centric one. This approach prioritizes the accumulation of large-scale, high-quality data and, where feasible, aims for end-to-end learning. Leveraging Semi-Supervised Learning: The next-token prediction objective in LLMs has proven remarkably effective. It enables large-scale semi-supervised learning using unlabeled data while also equipping the model with a surprisingly deep understanding of world knowledge.\n\nThese insights have shaped the design of our foundation model, enabling a transition from maintaining numerous small, specialized models to building a scalable, efficient system. By scaling up semi-supervised training data and model parameters, we aim to develop a model that not only meets current needs but also adapts dynamically to evolving demands, ensuring sustainable innovation and resource efficiency.\n\nData\n\nAt Netflix, user engagement spans a wide spectrum, from casual browsing to committed movie watching. With over 300 million users at the end of 2024, this translates into hundreds of billions of interactions \u2014 an immense dataset comparable in scale to the token volume of large language models (LLMs). However, as in LLMs, the quality of data often outweighs its sheer volume. To harness this data effectively, we employ a process of interaction tokenization, ensuring meaningful events are identified and redundancies are minimized.\n\nTokenizing User Interactions: Not all raw user actions contribute equally to understanding preferences. Tokenization helps define what constitutes a meaningful \u201ctoken\u201d in a sequence. Drawing an analogy to Byte Pair Encoding (BPE) in NLP, we can think of tokenization as merging adjacent actions to form new, higher-level tokens. However, unlike language tokenization, creating these new tokens requires careful consideration of what information to retain. For instance, the total watch duration might need to be summed or engagement types aggregated to preserve critical details.\n\nFigure 1.Tokenization of user interaction history by merging actions on the same title, preserving important information.\n\nThis tradeoff between granular data and sequence compression is akin to the balance in LLMs between vocabulary size and context window. In our case, the goal is to balance the length of interaction history against the level of detail retained in individual tokens. Overly lossy tokenization risks losing valuable signals, while too granular a sequence can exceed practical limits on processing time and memory.\n\nEven with such strategies, interaction histories from active users can span thousands of events, exceeding the capacity of transformer models with standard self attention layers. In recommendation systems, context windows during inference are often limited to hundreds of events \u2014 not due to model capability but because these services typically require millisecond-level latency. This constraint is more stringent than what is typical in LLM applications, where longer inference times (seconds) are more tolerable.\n\nTo address this during training, we implement two key solutions:\n\nSparse Attention Mechanisms: By leveraging sparse attention techniques such as low-rank compression, the model can extend its context window to several hundred events while maintaining computational efficiency. This enables it to process more extensive interaction histories and derive richer insights into long-term preferences. Sliding Window Sampling: During training, we sample overlapping windows of interactions from the full sequence. This ensures the model is exposed to different segments of the user\u2019s history over multiple epochs, allowing it to learn from the entire sequence without requiring an impractically large context window.\n\nAt inference time, when multi-step decoding is needed, we can deploy KV caching to efficiently reuse past computations and maintain low latency.\n\nThese approaches collectively allow us to balance the need for detailed, long-term interaction modeling with the practical constraints of model training and inference, enhancing both the precision and scalability of our recommendation system.\n\nInformation in Each \u2018Token\u2019: While the first part of our tokenization process focuses on structuring sequences of interactions, the next critical step is defining the rich information contained within each token. Unlike LLMs, which typically rely on a single embedding space to represent input tokens, our interaction events are packed with heterogeneous details. These include attributes of the action itself (such as locale, time, duration, and device type) as well as information about the content (such as item ID and metadata like genre and release country). Most of these features, especially categorical ones, are directly embedded within the model, embracing an end-to-end learning approach. However, certain features require special attention. For example, timestamps need additional processing to capture both absolute and relative notions of time, with absolute time being particularly important for understanding time-sensitive behaviors.\n\nTo enhance prediction accuracy in sequential recommendation systems, we organize token features into two categories:\n\nRequest-Time Features: These are features available at the moment of prediction, such as log-in time, device, or location. Post-Action Features: These are details available after an interaction has occurred, such as the specific show interacted with or the duration of the interaction.\n\nTo predict the next interaction, we combine request-time features from the current step with post-action features from the previous step. This blending of contextual and historical information ensures each token in the sequence carries a comprehensive representation, capturing both the immediate context and user behavior patterns over time.\n\nConsiderations for Model Objective and Architecture\n\nAs previously mentioned, our default approach employs the autoregressive next-token prediction objective, similar to GPT. This strategy effectively leverages the vast scale of unlabeled user interaction data. The adoption of this objective in recommendation systems has shown multiple successes [1\u20133]. However, given the distinct differences between language tasks and recommendation tasks, we have made several critical modifications to the objective.\n\nFirstly, during the pretraining phase of typical LLMs, such as GPT, every target token is generally treated with equal weight. In contrast, in our model, not all user interactions are of equal importance. For instance, a 5-minute trailer play should not carry the same weight as a 2-hour full movie watch. A greater challenge arises when trying to align long-term user satisfaction with specific interactions and recommendations. To address this, we can adopt a multi-token prediction objective during training, where the model predicts the next n tokens at each step instead of a single token[4]. This approach encourages the model to capture longer-term dependencies and avoid myopic predictions focused solely on immediate next events.\n\nSecondly, we can use multiple fields in our input data as auxiliary prediction objectives in addition to predicting the next item ID, which remains the primary target. For example, we can derive genres from the items in the original sequence and use this genre sequence as an auxiliary target. This approach serves several purposes: it acts as a regularizer to reduce overfitting on noisy item ID predictions, provides additional insights into user intentions or long-term genre preferences, and, when structured hierarchically, can improve the accuracy of predicting the target item ID. By first predicting auxiliary targets, such as genre or original language, the model effectively narrows down the candidate list, simplifying subsequent item ID prediction.\n\nUnique Challenges for Recommendation FM\n\nIn addition to the infrastructure challenges posed by training bigger models with substantial amounts of user interaction data that are common when trying to build foundation models, there are several unique hurdles specific to recommendations to make them viable. One of unique challenges is entity cold-starting.\n\nAt Netflix, our mission is to entertain the world. New titles are added to the catalog frequently. Therefore the recommendation foundation models require a cold start capability, which means the models need to estimate members\u2019 preferences for newly launched titles before anyone has engaged with them. To enable this, our foundation model training framework is built with the following two capabilities: Incremental training and being able to do inference with unseen entities.\n\nIncremental training : Foundation models are trained on extensive datasets, including every member\u2019s history of plays and actions, making frequent retraining impractical. However, our catalog and member preferences continually evolve. Unlike large language models, which can be incrementally trained with stable token vocabularies, our recommendation models require new embeddings for new titles, necessitating expanded embedding layers and output components. To address this, we warm-start new models by reusing parameters from previous models and initializing new parameters for new titles. For example, new title embeddings can be initialized by adding slight random noise to existing average embeddings or by using a weighted combination of similar titles\u2019 embeddings based on metadata. This approach allows new titles to start with relevant embeddings, facilitating faster fine-tuning. In practice, the initialization method becomes less critical when more member interaction data is used for fine-tuning. Dealing with unseen entities : Even with incremental training, it\u2019s not always guaranteed to learn efficiently on new entities (ex: newly launched titles). It\u2019s also possible that there will be some new entities that are not included/seen in the training data even if we fine-tune foundation models on a frequent basis. Therefore, it\u2019s also important to let foundation models use metadata information of entities and inputs, not just member interaction data. Thus, our foundation model combines both learnable item id embeddings and learnable embeddings from metadata. The following diagram demonstrates this idea.\n\nFigure 2. Titles are associated with various metadata, such as genres, storylines, and tones. Each type of metadata could be represented by averaging its respective embeddings, which are then concatenated to form the overall metadata-based embedding for the title.\n\nTo create the final title embedding, we combine this metadata-based embedding with a fully-learnable ID-based embedding using a mixing layer. Instead of simply summing these embeddings, we use an attention mechanism based on the \u201cage\u201d of the entity. This approach allows new titles with limited interaction data to rely more on metadata, while established titles can depend more on ID-based embeddings. Since titles with similar metadata can have different user engagement, their embeddings should reflect these differences. Introducing some randomness during training encourages the model to learn from metadata rather than relying solely on ID embeddings. This method ensures that newly-launched or pre-launch titles have reasonable embeddings even with no user interaction data.\n\nDownstream Applications and Challenges\n\nOur recommendation foundation model is designed to understand long-term member preferences and can be utilized in various ways by downstream applications:\n\nDirect Use as a Predictive Model The model is primarily trained to predict the next entity a user will interact with. It includes multiple predictor heads for different tasks, such as forecasting member preferences for various genres. These can be directly applied to meet diverse business needs.. Utilizing embeddings The model generates valuable embeddings for members and entities like videos, games, and genres. These embeddings are calculated in batch jobs and stored for use in both offline and online applications. They can serve as features in other models or be used for candidate generation, such as retrieving appealing titles for a user. High-quality title embeddings also support title-to-title recommendations. However, one important consideration is that the embedding space has arbitrary, uninterpretable dimensions and is incompatible across different model training runs. This poses challenges for downstream consumers, who must adapt to each retraining and redeployment, risking bugs due to invalidated assumptions about the embedding structure. To address this, we apply an orthogonal low-rank transformation to stabilize the user/item embedding space, ensuring consistent meaning of embedding dimensions, even as the base foundation model is retrained and redeployed. Fine-Tuning with Specific Data The model\u2019s adaptability allows for fine-tuning with application-specific data. Users can integrate the full model or subgraphs into their own models, fine-tuning them with less data and computational power. This approach achieves performance comparable to previous models, despite the initial foundation model requiring significant resources.\n\nScaling Foundation Models for Netflix Recommendations\n\nIn scaling up our foundation model for Netflix recommendations, we draw inspiration from the success of large language models (LLMs). Just as LLMs have demonstrated the power of scaling in improving performance, we find that scaling is crucial for enhancing generative recommendation tasks. Successful scaling demands robust evaluation, efficient training algorithms, and substantial computing resources. Evaluation must effectively differentiate model performance and identify areas for improvement. Scaling involves data, model, and context scaling, incorporating user engagement, external reviews, multimedia assets, and high-quality embeddings. Our experiments confirm that the scaling law also applies to our foundation model, with consistent improvements observed as we increase data and model size.\n\nFigure 3. The relationship between model parameter size and relative performance improvement. The plot demonstrates the scaling law in recommendation modeling, showing a trend of increased performance with larger model sizes. The x-axis is logarithmically scaled to highlight growth across different magnitudes.\n\nConclusion\n\nIn conclusion, our Foundation Model for Personalized Recommendation represents a significant step towards creating a unified, data-centric system that leverages large-scale data to increase the quality of recommendations for our members. This approach borrows insights from Large Language Models (LLMs), particularly the principles of semi-supervised learning and end-to-end training, aiming to harness the vast scale of unlabeled user interaction data. Addressing unique challenges, like cold start and presentation bias, the model also acknowledges the distinct differences between language tasks and recommendation. The Foundation Model allows various downstream applications, from direct use as a predictive model to generate user and entity embeddings for other applications, and can be fine-tuned for specific canvases. We see promising results from downstream integrations. This move from multiple specialized models to a more comprehensive system marks an exciting development in the field of personalized recommendation systems.\n\nAcknowledgements\n\nContributors to this work (name in alphabetical order): Ai-Lei Sun Aish Fenton Anne Cocos Anuj Shah Arash Aghevli Baolin Li Bowei Yan Dan Zheng Dawen Liang Ding Tong Divya Gadde Emma Kong Gary Yeh Inbar Naor Jin Wang Justin Basilico Kabir Nagrecha Kevin Zielnicki Linas Baltrunas Lingyi Liu Luke Wang Matan Appelbaum Michael Tu Moumita Bhattacharya Pablo Delgado Qiuling Xu Rakesh Komuravelli Raveesh Bhalla Rob Story Roger Menezes Sejoon Oh Shahrzad Naseri Swanand Joshi Trung Nguyen Vito Ostuni Wei Wang Zhe Zhang\n\nReference", "label": 0}
{"title": "Memories with People", "url": "https://lifeofpablo.com/blog/memories-with-people", "content": "Memories with People\n\nPicture of Luke Korns holding a disposible camera.\n\nThis post was written in English (en_US).\n\nI've watched this video by Luke Korns numerous times. Everytime, I watch it, it brings me joy thinking about my experiences traveling. Traveling is life changing and it's been life changing for me. It's taught me things, I wouldn't learn staying in one place. Learning about yourself is key for growth. Often, while learning we also learn from people. These people you learn from are strangers. I remember hanging out with strangers in Rome for an entire day. I've roamed around the city of Seoul, Korea with people I met on the bus. You get the idea. Meeting strangers is fun.\n\nI've learned so much from the people I've interacted on my travels. They've shown me their perspective on various topics and provided some life advice as well. All of these people have been strangers. I made a connection with them even if was sitting with them, eating lunch or spending the day with them. I saw their perspective of what it was like to grow up in their city. I've experience them providing a message to their city. It waa beautiful to witness them speaking in their native language.", "label": 1}
{"title": "Accelerating GPU indexes in Faiss with NVIDIA cuVS", "url": "https://engineering.fb.com/2025/05/08/data-infrastructure/accelerating-gpu-indexes-in-faiss-with-nvidia-cuvs/", "content": "Meta and NVIDIA collaborated to accelerate vector search on GPUs by integrating NVIDIA cuVS into Faiss v1.10 , Meta\u2019s open source library for similarity search.\n\nThis new implementation of cuVS will be more performant than classic GPU-accelerated search in some areas.\n\nFor inverted file (IVF) indexing, NVIDIA cuVS outperforms classical GPU-accelerated IVF build times by up to 4.7x; and search latency is reduced by as much as 8.1x.\n\nFor graph indexing, CUDA ANN Graph (CAGRA) outperforms CPU Hierarchical Navigable Small World graphs (HNSW) build times by up to 12.3x; and search latency is reduced by as much as 4.7x.\n\nThe Faiss library\n\nThe Faiss library is an open source library, developed by Meta FAIR, for efficient vector search and clustering of dense vectors. Faiss pioneered vector search on GPUs, as well as the ability to seamlessly switch between GPUs and CPUs. It has made a lasting impact in both research and industry, being used as an integrated library in several databases (e.g., Milvus and OpenSearch), machine learning libraries, data processing libraries, and AI workflows. Faiss is also used heavily by researchers and data scientists as a standalone library, often paired with PyTorch.\n\nCollaboration with NVIDIA\n\nThree years ago, Meta and NVIDIA worked together to enhance the capabilities of vector search technology and to accelerate vector search on GPUs. Previously, in 2016, Meta had incorporated high performing vector search algorithms made for NVIDIA GPUs: GpuIndexFlat; GpuIndexIVFFlat; GpuIndexIVFPQ. After the partnership, NVIDIA rapidly contributed GpuIndexCagra, a state-of-the art graph-based index designed specifically for GPUs. In its latest release, Faiss 1.10.0 officially includes these algorithms from the NVIDIA cuVS library.\n\nFaiss 1.10.0 also includes a new conda package that unlocks the ability to choose between the classic Faiss GPU implementations and the newer NVIDIA cuVS algorithms, making it easy for users to switch between GPU and CPU.\n\nBenchmarking\n\nThe following benchmarks were conducted using the cuVS-bench tool.\n\nWe measured:\n\nA tall, slender image dataset: A subset of 100 million vectors from the Deep1B dataset by 96 dimensions.\n\nA short, wide dataset of text embeddings: 5 million vector embeddings, curated using the OpenAI text-embedding-ada-002 model .\n\nTests for index build times and search latency were conducted on an NVIDIA H100 GPU and compared to an Intel Xeon Platinum 8480CL system. Results are reported in the tables below at 95% recall along the pareto frontiers for k=10 nearest neighbors.\n\nBuild time (95% recall@10)\n\nIndex Embeddings\n\n100M x 96\n\n(seconds) Embeddings\n\n5M x 1536\n\n(seconds) Faiss Classic Faiss cuVS Faiss Classic Faiss cuVS Faiss Classic Faiss cuVS IVF Flat IVF Flat 101.4 37.9 (2.7x) 24.4 15.2 (1.6x) IVF PQ IVF PQ 168.2 72.7 (2.3x) 42.0 9.0 (4.7x) HNSW (CPU) CAGRA 3322.1 518.5 (6.4x) 1106.1 89.7 (12.3x)\n\nTable 1: Index build times for Faiss-classic and Faiss-cuVS in seconds (with NVIDIA cuVS speedups in parentheses).\n\nSearch latency (95% recall@10)\n\nIndex Embeddings\n\n100M x 96\n\n(milliseconds) Embeddings\n\n5M x 1536\n\n(milliseconds) Faiss Classic Faiss cuVS Faiss Classic Faiss cuVS Faiss Classic Faiss cuVS IVF Flat IVF Flat 0.75 0.39 (1.9x) 1.98 1.14 (1.7x) IVF PQ IVF PQ 0.49 0.17 (2.9x) 1.78 0.22 (8.1x) HNSW (CPU) CAGRA 0.56 0.23 (2.4x) 0.71 0.15 (4.7x)\n\nTable 2: Online (i.e., one at a time) search query latency for Faiss-classic and Faiss-cuVS in milliseconds (with NVIDIA cuVS speedups in parentheses).\n\nLooking forward\n\nThe emergence of state-of-the-art NVIDIA GPUs has revolutionized the field of vector search, enabling high recall and lightning-fast search speeds. The integration of Faiss and cuVS will continue to incorporate state-of-the-art algorithms, and we look forward to unlocking new innovations in this partnership between Meta and NVIDIA.\n\nRead here for more details about NVIDIA cuVS.", "label": 0}
{"title": "Ledger: Stripe\u2019s system for tracking and validating money movement", "url": "https://stripe.com/blog/ledger-stripe-system-for-tracking-and-validating-money-movement", "content": "Last Black Friday to Cyber Monday, Stripe processed 300 million transactions with a total payment volume of $18.6B\u2014and the Stripe API maintained greater than 99.999% availability. Underlying these metrics is our Global Payments and Treasury Network (GPTN) that manages the complexity of accepting payments, money storage, and money movement. Today, Stripe supports more than 135 currencies and payment methods through partnerships with local banks and financial networks in 185 countries. These entities provide different interfaces, data models, and behaviors, and Stripe continually manages this complexity so developers can quickly integrate the GPTN into their businesses.\n\nInternally, Stripe needs to guarantee that what we expect to happen during payment processing actually happens for internal customers and external auditors of our data. We built Ledger, an immutable and auditable log, as a trustworthy system of record for all of our financial data. Ledger standardizes our representation of money movement, and it serves as the scalable foundation for our automated Data Quality (DQ) Platform\u2014guaranteeing Stripe faithfully manages money for users.\n\nMany existing systems provide primitives for accurate accounting, but the real world is imperfect, incomplete, and constantly changing. We witness basic and obvious failures like malformed reports or propagated errors from banking or network partners, and also broad macroeconomic changes such as currencies ceasing to exist or large banks collapsing overnight. While we aspire to an orderly ideal, at Stripe scale, that\u2019s impossible\u2014instead we built a system that keeps these imperfections manageable and bounded.\n\nLedger models internal data-producing systems with common patterns, and it relies on proactive alerting to surface issues and proposed solutions. Each day, Ledger sees five billion events and 99.99% of our dollar volume is fully ingested and verified within four days. Of that activity, 99.999% is monitored, categorized, and triaged through rich investigative tooling\u2014while the remaining long-tail is reliably handled through manual analysis. Together, Ledger and the DQ Platform ensure over 99.9999% explainability of money movement, even as Stripe\u2019s data volume has grown 10x.\n\nIn this blog post, we\u2019ll share technical details on how we built this state-of-the-art money movement tracking system, and describe how teams at Stripe interact with the data quality metrics that underlie our global payments network.\n\nHow Stripe processes payments\n\nThe GPTN in part is a payment processing network consisting of customer business calls to Stripe\u2019s API and Stripe\u2019s interactions with a variety of banks and payment methods. There is complexity in tracking the requests Stripe makes to partners, the physical money movement between financial partners, and the reporting Stripe receives back. We make this multifaceted problem tractable by segmenting the Stripe platform into discrete services, databases, and APIs/gRPC interfaces, which lets us solve individual problems without getting overwhelmed by the broader system.\n\nThe challenge with this approach is that there is no intrinsic mechanism forcing these systems to represent or deliver data in the same way. Some might operate in real time, while others may operate on a monthly cadence with vastly different data volumes; some producers generate billions of events per day, while others may only generate a few hundred. Moreover, each system might have its own definitions of correctness or reliability. We require a mechanism that can deal with these variations and prove that these individual systems are collectively modeling our financials correctly.\n\nA simplified summary view of Stripe\u2019s interactions with external entities\n\nHow we designed Ledger\n\nThe Stripe services mentioned above have independent responsibilities, but they collaborate to solve a large federated problem. An ideal solution provides a mental model for correctness\u2014supported by trustworthy statistics\u2014that easily generalizes to new use cases. Further, we want to represent all activity on the Stripe platform in a common data structure that can be analyzed by a single system.\n\nThis is the way we approach it:\n\nLedger encodes a state machine representation of producer systems, and models its behavior as a logical fund flow\u2014the movement of balances (events) between accounts (states).\n\nLedger computes all account balances to evaluate the health of the system, grouped by various subdivisions to generate comprehensive statistics.\n\nThis approach abstracts individual differences between underlying systems and provides mathematical evidence that they are functioning correctly.\n\nLedger as a semantic data store\n\nLedger is a faithful representation of the underlying state of all payment processes on the Stripe platform. Instead of computing a derived dataset based on incoming data pipelines, Ledger models the actual work of producer systems, recording each operation as a transaction. Ledger modeling may diverge from upstream data, but we guard against these cases explicitly with data completeness checks.\n\nCombined with our other data quality metrics, we can safely rely on Ledger\u2019s data representation to monitor external systems. If we instrument Ledger, we indirectly instrument the data-producing pipelines. And, if we identify a problem, we alert our internal users to which part of their data pipeline is broken\u2014and exactly how they can fix it.\n\nProcessing a charge with a creation event for a pending charge, and a release event for completion\n\nInside of Ledger, we represent this activity as a movement of balances between two discrete states (creation and release), turning the above process into an observable state machine.\n\nProcessing a charge in Ledger, represented by a creation event for a pending charge and a release event for completion\n\nSystem abstraction\n\nLedger also abstracts producer systems. Instead of separately monitoring handoffs between data pipelines, we model systems as connected fund flows moving money between accounts. Because Ledger is a transaction-level system of record, we can prove that even complex multisystem pipelines with multiple stages of handoff are working correctly. We also model data consistency between otherwise disconnected systems, and we track individual transactions through their entire lifecycle. We call this tracing, and, at our scale, this totals to billions of daily transactions.\n\nUnifying separate systems with fund flows\n\nConsider an abstract end-to-end fund flow: for example, a business adding funds to its balance. This requires moving funds between banks, reconciling money movement with third-party reporting, and matching regulatory reporting with financial reporting. The fund flow spans multiple internal team boundaries, with discrete events published to different systems at different times. If we model this fund flow with logical constructs, Ledger can unify this data across separate systems and monitor its correctness.\n\nImmutability\n\nAt its core, Ledger is an immutable log of events. Transactions previously published into Ledger cannot be deleted or modified, and we can always reconstruct past state by processing all events up to that point. All constructs\u2014balances, fund flows, data quality controls, and so on\u2014are transformations of the static underlying structure. Ledger\u2019s immutability ensures we can audit and reproduce any data point at any time. Immutability justifies our data quality measures by guaranteeing that we can explain and analyze the exact problematic data.\n\nHow we designed the Data Quality (DQ) Platform\n\nLedger is the foundation for our Data Quality (DQ) Platform, which unifies detection of money movement issues and response tooling. Empirically, the DQ Platform ensures reliable and timely reporting across Stripe\u2019s key lines of business: we maintained a 99.999% readiness target, even as data volume grew 10x.\n\nTransaction-level fund flows give us powerful tools to reason about complex interconnected subcomponents. We analyze these abstractions with a set of trustworthy DQ metrics that measure the health of a fund flow. These metrics are based on a common set of questions across all fund flows. For a specific cross-section of data, evaluated at time X, we look at:\n\nClearing: Did the fund flow complete correctly?\n\nDid the fund flow complete correctly? Timeliness: Did the data arrive on time?\n\nDid the data arrive on time? Completeness: Do we have a complete representation of the underlying data system?\n\nWe then compose DQ metrics on individual fund flows to provide scoring and targeted guidance for technical experts. These measurements roll up to create a unified DQ score\u2014a system with a 99.99% data quality score is extremely unlikely to hide major problems\u2014turning a complex distributed analysis problem into a straightforward tabulation exercise. Technical users can likewise trust that improving DQ scores reflect true improvement in underlying system behavior and accuracy.\n\nClearing\n\nLedger is based on double-entry bookkeeping, a standard method for guaranteeing that all money in a system is fully accounted for by balancing credits and debits. Grounding our analysis in this construct gives us a mathematical proof of correctness. If you\u2019ve never encountered this term before, a helpful explainer is \u201cAn Engineer\u2019s Guide to Double-Entry Bookkeeping.\u201d\n\nUsing double-entry bookkeeping to validate money movement is similar to analyzing a flow of water through a network of pipes (processes) ending in reservoirs (balance sheets). At steady state, terminal (nonclearing) reservoirs are full, and intermediate (clearing) pipes are empty. If there is water stuck in the pipes, then you have a problem\u2014in other words, unresolved balances on the balance sheet.\n\nTraditionally, bookkeeping is purely an accounting construct, but we apply these ideas in a novel way. Rather than just tabulating cash flow in and out, we\u2019re simultaneously modeling internal data system behaviors that may have nothing to do with physical movement of money\u2014for example, currency conversion, report parsing, estimation, or billing analysis. We can use the same bookkeeping concepts to reason about those systems and evaluate their correctness in a much more general way.\n\nDetecting problems\n\nClearing measures the fraction of Ledger that is appropriately zeroed out at steady state. Consider an example that models two steps of a flow: charge creation (potential money movement) and release (funds becoming available). As you follow the flow, keep in mind these definitions:\n\nAccounts are buckets of money distinguished by their type (e.g., charge_unsubmitted ) and properties (e.g., id , business ).\n\nare buckets of money distinguished by their type (e.g., ) and properties (e.g., , ). Events move money between accounts (e.g., charge.creation and charge.release ).\n\nAt time T0 , the charge.creation event sets up a balance in the undisbursed account; then at T1 , charge.release completes the flow and moves the funds to the business_balance account.\n\nIt is important to note that the creation and release events are completely independent. Even if they arrive out of order, or are created by different sources, Ledger maintains accurate fund flows through the identifier for business and id . But, if the release event is never published or has the wrong id , Ledger would not clear the balance in the associated charge_undisbursed account, and it would instead hold the balance in a different instance of charge_undisbursed .\n\nExample clearing issue\n\nConsider next how a wrong value ( business: B vs. business: A ) results in two clearing accounts with nonzero balance. Instead of having one reservoir of money for business: A , we wind up with two\u2014one for business: A and one for business: B .\n\nGeneralizing from this example, we repeat this process for every fund flow, account type, and property-based subdivision inside of Ledger. Even when we have billions of transactions, a single missing, late, or incorrect transaction immediately creates a detectable accuracy issue with a simple query\u2014for example, \u201cFind the clearing Accounts with nonzero balance.\u201d\n\nTimeliness\n\nClearing prevents persistent problems, but we also need to guarantee data arrives on time for time-sensitive functions such as monthly report generation. Producers create time stamps when integrating with Ledger, and we measure the delta between when data first enters the Stripe platform and when it reaches Ledger. We set a hard threshold on the data delivery window, and we create headroom for subsequent reporting, analysis, and manipulations to guarantee 99.999% timeliness.\n\nCompleteness\n\nWe guarantee data completeness and guard against missing data from upstream systems with explicit cross-system checks alongside automated anomaly detection. For example, we ensure that every ID in a producer database has a matching Ledger event. We also run statistical modeling on data availability. We have models for every account type that use historical trends to calculate expected data arrival time and, if events do not appear, we interpret this as potentially missing data.\n\nHow teams at Stripe explore DQ metrics\n\nOn top of the DQ Platform, we built hierarchical automated alerting and rich tooling. We combine interactive metric displays with analysis and guidance. The experience for internal leaders and team members focuses on proactive feedback, simple manipulation of data, and meaningful metrics. We also provide use-case-specific context that depends on which part of the business is using it. For example, consider how we show team-level DQ metrics for our periodic financial reporting, which we call Accounting Close. Note: some details are blocked out for privacy.\n\nThe topline view is generally in a good state, but there are areas for improvement at the team level within the Payment Engineering group. For example, the 50% score for Aging Balances means that some clearing issues have persisted over time:\n\nA single team-level view of data quality metrics\n\nThis team-level view shows DQ metrics alongside a call to action including auto-generated tickets, relevant resources, and tool links\u2014everything required for self-service. For leaders, this view provides the exact dollar impact of DQ issues.\n\nTactical views\n\nDQ scores drop when a problem is observed in Ledger. Although Ledger is a projection of underlying systems, Ledger problems are not usually problems of transcription or data modeling in Ledger. They primarily reveal real problems with system implementations, integrations, or physical money movement. In these cases, we provide tactical views to trace issues back to their root cause inside Stripe platforms or external systems.\n\nConsider an uncleared balance of a specific account type\u2014a processing fee that must be invoiced and paid. At steady state, the invoice should be paid and the balance is zero, but over time we observe a nonclearing balance.\n\nInvestigation and attribution\n\nClicking on a point in the graph generates SQL queries in Presto (our ad-hoc SQL query engine) and surfaces relevant data: reference keys, metadata, ownership, and tips. If a Ledger user is unable to debug and publish a correction\u2014perhaps because the root cause is related to an infrastructure or third-party incident outside their control\u2014they can reassign ownership to the right internal stakeholders and exclude it from alerting.\n\nWhen issues are attributed to a known incident, we can retroactively analyze the impact to DQ metrics across teams to fully understand how Stripe was affected:\n\nCombined, we have the ability to measure and analyze data quality, identify root-cause problems, and flexibly interact with the underlying data constructs to manage our problem load over time. In this case, fixing problems in Ledger may involve republishing data from source systems.\n\nData correction\n\nLedger is our system of record and must remain an evergreen representation of truth. Persistent problems reduce visibility into new problems and may result in incorrect reporting or derived datasets. Because Ledger is an immutable log of events, we can\u2019t run simple queries to mutate the state; instead, we have to revert and reprocess prior operations. If an incident occurs, we need a tool for correcting data at scale.\n\nWe built a supporting utility to create and safely execute migrations, protected by a data quality tool that generates out-of-band reports on the production impact of proposed changes. Together, these tools approximate a CI pipeline for ad-hoc data repair operations. All operations must go through a two-phase review and commit of the data\u2014and its associated DQ impact.\n\nFewer data problems, more reliable reporting\n\nOur systems need to operate within a messy reality, but the innovations described in this blog post drive us towards a trustworthy and explainable operational model. Likewise, as businesses and mechanisms for money movement inevitably evolve, Stripe is empowered to keep pace with that change.\n\nThe DQ Platform ensures reliable and timely reporting across all Stripe business lines. The combination of clearing, timeliness, and completeness metrics ensures that internal stakeholders can make sound judgments about the correctness of underlying data systems without worrying about maintaining complex specialized knowledge.\n\nThe digital economy will continue to accelerate, and our focus is on building robust and scalable systems to power it. In the future, we want to improve timeliness to minute-level analysis and response\u2014offering lower latency processing, which will strengthen fraud detection and increase available response time to address possible financial problems.\n\nWe are also investing in advanced enrichment capabilities that allow us to declaratively compose new datasets and reporting interfaces while guaranteeing that they meet our data quality bar. This work safely evolves the complexity of our internal systems alongside Stripe\u2019s growth.\n\nWe\u2019re excited to continue to solve hard, important problems. If you are too, consider joining our engineering team.", "label": 0}
{"title": "Alone Time: Solo Adventures", "url": "https://lifeofpablo.com/blog/alone-time-solo-adventures", "content": "Alone Time: Solo Adventures\n\nThis post was written in English (en_US).\n\nWhen was the last time you did something alone and by yourself outside of your place of living? Do you think about how as a person, it's hard to think about times you are truly by yourself? This has been something that has been on my mind a lot.\n\nToday, I went on a run today and drank a matcha latte at a coffee shop. It was nice just being in my thoughts and I appreciated being with myself and only myself. My mind was saying (or saying to itself?) \"This is relaxing!\"\n\nThis initiated me to start refecting on the activites I've done this year so far. I went through my phone's photo's apps, looked at my check-ins and any other history I had available. hen I started filtering out all things I've done alone and not with people. This is what I was able to recall the things I did or initiated by myself.\n\nSitting in a Coffee\n\nGoing to Noisebridge (San Francisco)\n\nGoing swimming in the river\n\nSeeing two films\n\nA few day trips?\n\nI ask myself the following questions: \"Am I not having a enough solo time?\" or \"Am I having a healthy amount of solo time?\"\n\nOften times, activites outside of home involve people in my day to day. I am not complaining about having a social circle. I just wondering for myself, \"What is a good balance?\", between activities that social and alone?\n\nAm I afraid to venture out on my own? Probably? Being at home is comfortable and I feel safe. Does my lack of knowing what is happening in my community add to this? More than likely. Does becoming a home body also play a roll. Yes!\n\nI should address these to make progress.\n\nFor those on the web, what is your take on this? Send me a webmention or email?", "label": 1}
{"title": "Nature Appreciation", "url": "https://shellsharks.com/blog-challenge-nature-appreciation", "content": "A Blog Questions Challenge\n\nThis week\u2019s Blog Questions Challenge is called \u201cNature Appreciation\u201d.\n\nHere are the questions\u2026\n\nSilliest Animal I\u2019ve Seen\n\nThe Bactrian Camel was the first come to mind. Camels have pretty silly (and grumpy) personalities as it is, but the Bactrian variety have wildly ridiculous camel humps. I got up close and personal with some at a wild live reserve kinda\u2019 thing once.\n\nPlant Superpower\n\nHonestly I did some web searching to find cool \u201cabilities\u201d I would want to steal from a plant and came up kinda empty. Fire resistance, regeneration, growing super tall\u2014these are all things that are kinda cool, but nothing specific stood out to me. So, as my pick, I\u2019ve decided to go with the wise old Oak Tree. For any particular ability? Not really. I just like the idea of being a chill old Oak Tree that animals love to hang out with.\n\nFavorite Nature Sound\n\nI\u2019d say it\u2019s a close tie between the sound of a mountain stream and that of a rain storm with low, rolling thunder.\n\nThe Perfect Nature Spot\n\nEasy. Western-facing, mountain-side cabin. Conifers as far as the eye can see. A mountain creek babbles nearby. The glow of a camp fire flickers across my face as I watch the sun melt behind the distant peaks.\n\nThanks for reading!", "label": 1}
{"title": "How to make Storybook Interactions respect user motion preferences", "url": "https://github.blog/engineering/user-experience/how-to-make-storybook-interactions-respect-user-motion-preferences/", "content": "Recently, while browsing my company\u2019s Storybook, I came across something that seemed broken: a flickering component that appeared to be re-rendering repeatedly. The open source tool that helps designers, developers, and others build and use reusable components was behaving weirdly. As I dug in, I realized I was seeing the unintended effects of the Storybook Interactions addon, which allows developers to simulate user interactions within a story, in action.\n\nStorybook Interactions can be a powerful tool, enabling developers to simulate and test user behaviors quickly. But if you\u2019re unfamiliar with Interactions\u2014especially if you\u2019re just looking to explore available components\u2014the simulated tests jumping around on the screen can feel disorienting.\n\nThis can be especially jarring for users who have the prefers-reduced-motion setting enabled in their operating system. When these users encounter a story that includes an interaction, their preferences are ignored and they have no option to disable or enable it. Instead, the Storybook Interaction immediately plays on page load, regardless. These rapid screen movements can cause disorientation for users or in some cases can even trigger a seizure.\n\nKnowledge share Operating systems allow users to set a motion preference. Adhering to this setting can be critical for some users. For example, for users who have photosensitive epilepsy or vertigo, even a small animation can be life-threatening. This explicit preference for a reduced motion experience can be used by browsers, applications, and websites to reduce unnecessary animations and motions via the prefers-reduced-motion CSS media feature.\n\nAt this time, Storybook does not have built-in capabilities to toggle interactions on or off. Until this feature can be baked in I am hoping this blog will provide you with an alternative way to make your work environment more inclusive. Now, let\u2019s get into building an addon that respects user\u2019s motion preferences and allows users to toggle interactions on and off.\n\nGoals\n\nUsers with prefers-reduced-motion enabled MUST have interactions off by default. Users with prefers-reduced-motion enabled MUST have a way to toggle the feature on or off without altering their operating system user preferences. All users SHOULD have a way to toggle the feature on or off without altering their user preferences.\n\nLet\u2019s get started\n\nStep 1: Build a Storybook addon\n\nStorybook allows developers to create custom addons. In this case, we will create one that will allow users to toggle Interactions on or off, while respecting the prefers-reduced-motion setting.\n\nAdd the following code to a file in your project\u2019s .storybook folder:\n\nimport React, {useCallback, useEffect} from 'react' import {IconButton} from '@storybook/components' import {PlayIcon, StopIcon} from '@storybook/icons' export const ADDON_ID = 'toggle-interaction' export const TOOL_ID = `${ADDON_ID}/tool` export const INTERACTION_STORAGE_KEY = 'disableInteractions' export const InteractionToggle = () => { const [disableInteractions, setDisableInteractions] = React.useState( window?.localStorage.getItem(INTERACTION_STORAGE_KEY) === 'true', ) useEffect(() => { const reducedMotion = matchMedia('(prefers-reduced-motion)') if (window?.localStorage.getItem(INTERACTION_STORAGE_KEY) === null && reducedMotion.matches) { window?.localStorage?.setItem(INTERACTION_STORAGE_KEY, 'true') setDisableInteractions(true) } }, []) const toggleMyTool = useCallback(() => { window?.localStorage?.setItem(INTERACTION_STORAGE_KEY, `${!disableInteractions}`) setDisableInteractions(!disableInteractions) // Refreshes the page to cause the interaction to stop/start window.location.reload() }, [disableInteractions, setDisableInteractions]) return ( <IconButton key={TOOL_ID} aria-label=\"Disable Interactions\" onClick={toggleMyTool} defaultChecked={disableInteractions} aria-pressed={disableInteractions} > {disableInteractions ? <PlayIcon /> : <StopIcon />} Interactions </IconButton> ) }\n\nCode breakdown\n\nThis addon stores user preferences for Interactions using window.localStorage . When the addon first loads, it checks whether the preference is already set and, if so, it defaults to the user\u2019s preference.\n\nconst [disableInteractions, setDisableInteractions] = React.useState( window?.localStorage.getItem(INTERACTION_STORAGE_KEY) === 'true', )\n\nThis useEffect hook checks if a user has their motion preferences set to prefers-reduced-motion and ensures that Interactions are turned off if the user hasn\u2019t already set a preference in Storybook.\n\nuseEffect(() => { const reducedMotion = matchMedia('(prefers-reduced-motion)') if (window?.localStorage.getItem(INTERACTION_STORAGE_KEY) === null && reducedMotion.matches) { window?.localStorage?.setItem(INTERACTION_STORAGE_KEY, 'true') setDisableInteractions(true) } }, [])\n\nWhen a user clicks the toggle button, preferences are updated and the page is refreshed to reflect the changes.\n\nconst toggleMyTool = useCallback(() => { window?.localStorage?.setItem(INTERACTION_STORAGE_KEY, `${!disableInteractions}`) setDisableInteractions(!disableInteractions) // Refreshes the page to cause the interaction to stop/start window.location.reload() }, [disableInteractions, setDisableInteractions])\n\nStep 2: Register your new addon with Storybook\n\nIn your .storybook/manager file, register your new addon:\n\naddons.register(ADDON_ID, () => { addons.add(TOOL_ID, { title: 'toggle interaction', type: types.TOOL as any, match: ({ viewMode, tabId }) => viewMode === 'story' && !tabId, render: () => <InteractionToggle />, }) })\n\nThis adds the toggle button to the Storybook toolbar, which will allow users to change their Storybook Interaction preferences.\n\nStep 3: Add functionality to check user preferences\n\nFinally, create a function that checks whether Interactions should be played and add it to your interaction stories:\n\nimport {INTERACTION_STORAGE_KEY} from './.storybook/src/InteractionToggle' export const shouldInteractionPlay = () => { const disableInteractions = window?.localStorage?.getItem(INTERACTION_STORAGE_KEY) return disableInteractions === 'false' || disableInteractions === null } export const SomeComponentStory = { render: SomeComponent, play: async ({context}) => { if (shouldInteractionPlay()) { ... } }) }\n\nWith this custom addon, you can ensure your workplace remains accessible to users with motion sensitivities while benefiting from Storybook\u2019s Interactions. For those with prefers-reduced-motion enabled, motion will be turned off by default and all users will be able to toggle interactions on or off.", "label": 0}
{"title": "Data-driven marketing starts with developers", "url": "https://developers.googleblog.com/en/data-driven-marketing-starts-with-developers/", "content": "To build a great marketing campaign in today\u2019s landscape, data needs to be steering your strategy, not just measuring success. Developers play a key role in implementing the tools that analyze and process this data, turning it into insights, smarter strategies, and better results.\n\nUnlock the power in your marketing data with these three developer-friendly MarTech solutions. From gathering data with unparalleled transparency and control, to transforming raw data into structured insights, or using automated A/B testing for optimal performance, here\u2019s how developers can transform what marketing data can do.\n\n\n\nsGTM Pantheon\n\nGain more control and transparency over your marketing data\n\nFrom buttons clicked to pages scrolled, knowing how people interact with your website or app is crucial to optimizing performance. Server-side Google Tag Manager (sGTM) makes this process easier by measuring traffic and managing data flow\u2014while opening the doors to better privacy, performance, control, and productivity.\n\nsGTM Pantheon is a toolbox of easy-to-deploy solutions that complement the existing capabilities of sGTM in different ways:\n\nImprove reporting, bidding, audience management, and data pipeline processes.\n\nReceive unparalleled transparency and control over website and app data.\n\nAccess data from external APIs and cloud-based customer, product, and business data in real time.\n\nOffer real-time website personalization and conversion rate optimization.\n\nAccess advanced analytics and reporting using cloud databases.\n\n\n\nDevelopers have the flexibility to mix and match solutions to create a single pipeline that can be integrated with both Google and non-Google platforms. And because sGTM Pantheon uses a server environment, the solutions run in a private, first-party cloud-secure environment.\n\n\n\nWhat will you find in the sGTM Pantheon toolbox?\n\nTo gather data:\n\nSoteria: Calculates bid to profit for online transactions without exposing data.\n\nPhoebe: Calls Vertex AI in real time for Lifetime Value (LTV) bidding and lead scoring.\n\nArtemis: Gets customer data from Firestore for audience segmentation.\n\nApollo: Retrieves data from a Google Sheet to generate lead gen value for lead scoring.\n\nCerberus: Integrates reCAPTCHA to filter bot-generated events and suspicious activity.\n\nDioscuri: Offers personalization with quick access to Gemini.\n\n\n\nTo send data:\n\nHephaestus: Advances bidding, audience, analytics, and marketing data pipeline automation.\n\nDeipeus: Sends first-party data back to the website for personalization.\n\nChaos: Drives advanced analytics, data recovery, and audience creation.\n\nHermes: Simplifies the sending of data in data pipelines.\n\n\n\nTo manage data:\n\nArgos: Monitors critical gTag settings.\n\n\n\nsGTM Pantheon is a living solution and is continually growing. Want to see more tools? Explore the full sGTM Pantheon on GitHub.\n\n\n\nGA4 Dataform\n\nTransform BigQuery data into accessible insights with GA4 Dataform\n\nYour Google Analytics 4 (GA4) marketing data holds untold stories, powerful insights, and new ways to connect with your audience\u2014but deciphering it isn\u2019t always easy.\n\nGA4 Dataform is a data transformation tool that organizes raw BigQuery data into clear, modular tables, such as events, items, sessions, transactions, and more\u2014so users of all technical skill levels can analyze data and steer data-driven campaigns. Offering both depth and simplicity, GA4 Dataform gives you the power to go beyond default settings, build your own data models, and find new ways to engage with customers.\n\n\n\nHow do I integrate GA4 Dataform with BigQuery?\n\nGA4 Dataform is a Google Cloud Dataform project that provides SQL data models for transforming raw GA4 BigQuery exports. The code is essentially a starter pack to help you build models on top of the GA4 raw data exports for data-driven marketing insights.", "label": 0}
{"title": "Data-driven marketing starts with developers", "url": "https://developers.googleblog.com/en/data-driven-marketing-starts-with-developers/", "content": "To build a great marketing campaign in today\u2019s landscape, data needs to be steering your strategy, not just measuring success. Developers play a key role in implementing the tools that analyze and process this data, turning it into insights, smarter strategies, and better results.\n\nUnlock the power in your marketing data with these three developer-friendly MarTech solutions. From gathering data with unparalleled transparency and control, to transforming raw data into structured insights, or using automated A/B testing for optimal performance, here\u2019s how developers can transform what marketing data can do.\n\n\n\nsGTM Pantheon\n\nGain more control and transparency over your marketing data\n\nFrom buttons clicked to pages scrolled, knowing how people interact with your website or app is crucial to optimizing performance. Server-side Google Tag Manager (sGTM) makes this process easier by measuring traffic and managing data flow\u2014while opening the doors to better privacy, performance, control, and productivity.\n\nsGTM Pantheon is a toolbox of easy-to-deploy solutions that complement the existing capabilities of sGTM in different ways:\n\nImprove reporting, bidding, audience management, and data pipeline processes.\n\nReceive unparalleled transparency and control over website and app data.\n\nAccess data from external APIs and cloud-based customer, product, and business data in real time.\n\nOffer real-time website personalization and conversion rate optimization.\n\nAccess advanced analytics and reporting using cloud databases.\n\n\n\nDevelopers have the flexibility to mix and match solutions to create a single pipeline that can be integrated with both Google and non-Google platforms. And because sGTM Pantheon uses a server environment, the solutions run in a private, first-party cloud-secure environment.\n\n\n\nWhat will you find in the sGTM Pantheon toolbox?\n\nTo gather data:\n\nSoteria: Calculates bid to profit for online transactions without exposing data.\n\nPhoebe: Calls Vertex AI in real time for Lifetime Value (LTV) bidding and lead scoring.\n\nArtemis: Gets customer data from Firestore for audience segmentation.\n\nApollo: Retrieves data from a Google Sheet to generate lead gen value for lead scoring.\n\nCerberus: Integrates reCAPTCHA to filter bot-generated events and suspicious activity.\n\nDioscuri: Offers personalization with quick access to Gemini.\n\n\n\nTo send data:\n\nHephaestus: Advances bidding, audience, analytics, and marketing data pipeline automation.\n\nDeipeus: Sends first-party data back to the website for personalization.\n\nChaos: Drives advanced analytics, data recovery, and audience creation.\n\nHermes: Simplifies the sending of data in data pipelines.\n\n\n\nTo manage data:\n\nArgos: Monitors critical gTag settings.\n\n\n\nsGTM Pantheon is a living solution and is continually growing. Want to see more tools? Explore the full sGTM Pantheon on GitHub.\n\n\n\nGA4 Dataform\n\nTransform BigQuery data into accessible insights with GA4 Dataform\n\nYour Google Analytics 4 (GA4) marketing data holds untold stories, powerful insights, and new ways to connect with your audience\u2014but deciphering it isn\u2019t always easy.\n\nGA4 Dataform is a data transformation tool that organizes raw BigQuery data into clear, modular tables, such as events, items, sessions, transactions, and more\u2014so users of all technical skill levels can analyze data and steer data-driven campaigns. Offering both depth and simplicity, GA4 Dataform gives you the power to go beyond default settings, build your own data models, and find new ways to engage with customers.\n\n\n\nHow do I integrate GA4 Dataform with BigQuery?\n\nGA4 Dataform is a Google Cloud Dataform project that provides SQL data models for transforming raw GA4 BigQuery exports. The code is essentially a starter pack to help you build models on top of the GA4 raw data exports for data-driven marketing insights.", "label": 0}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2018-04", "content": "en\n\n\"\n\nI am currently taking an ESL course and we had the opportunity to interview an ESL teacher. I interviewed Denise Teetor who is the ESL Teacher at Hastings Senior High School. She told me about her experiences getting into this subject field and how not always was she the ESL Teacher. I learned so much from this opportunity. This interview has me really excited to be a teacher. All the information here will hopefully help those people who want to become teachers. Getting the insights from another teacher lets you have a feeling of what you are getting yourself into. I am not trying to scare anyone, I'm just showing it how it is. Enjoy the interview! I will have the audio podcast available soon! Click the picture below to download a copy of the document to your device.\n\n\"\n\nI am currently taking an ESL course and we had the opportunity to interview an ESL teacher. I interviewed Denise Teetor who is the ESL Teacher at Hastings Senior High School. She told me about her experiences getting into this subject field and how not always was she the ESL Teacher. I learned so much from this opportunity. This interview has me really excited to be a teacher. All the information here will hopefully help those people who want to become teachers. Getting the insights from another teacher lets you have a feeling of what its like to be in the classroom. Enjoy the interview! I will have the audio podcast available soon! Click here to download a copy of the document to your device.\n\nDenise Teetor: ESL at Hastings Senior High School\n\nInterviewed by\n\nPablo MORALES GARCIA\n\nMrs. Denise Teetor is an English as a Second Language Teacher at Hastings Senior High School in Hastings Nebraska. She has been teaching at the high school for more than 30 years. She has quite a different perspective of the world since she has traveled outside of the United States plenty of times. She has seen so many changes in ESL over the years. She has quite the story to tell in which she becomes the ESL teacher she is today.\n\nMrs. Teetor was not an ESL teacher from the beginning. She use to teach physical education and coach for a long time. She decided to pursue a different area of teaching and left behind being an PE teacher and decided to teach English in Japan. She wanted to do something different with her time during the summers instead of wasting it. She saw an advertisement to teach in Japan for six weeks and thought \u201cthat sounds pretty cool\u201d, and that\u2019s how she got involved with Japan. She really enjoyed it and had so much fun with it.\n\nAfter one summer, she decided to pursue her ESL endorsements. She was thinking ahead that she could teach ESL at the high school level at some point down the road. One summer as she was getting ready to go back to Japan, she knew that the ELL teacher at Hastings High would be leaving that summer, even though she had not filed her resignation letter. Mrs. Teetor took some steps before the other teacher quit. She let the school know that she was interested in the position. The teacher did end up leaving and she received a phone call asking if she was interested in the ELL position and she said \u201cYes\u201d. This is how she moved from physical education to ELL. Japan really peaked her interest working with foreign kids.\n\nShe is happy with how much ESL/ELL has evolved over the years, especially since she saw there was not a lot of curriculum that existed. The number of students has changed over the years. The program development has come a long way. There was curriculum that really existed. They were fortunate when they were able to find books related to ELL/ESL. Now there are whole curriculums that one can use. That has been an advancement of ELL programs. It is like all other curriculums in schools such as science where you have books, notebooks and a foundation to use. You start at the foundational levels and move through the different stages of it. It has been one of the biggest tools available so teachers don\u2019t have to go out and make their own material.\n\nShe describes the program at Hastings Senior High School. She gets all different levels of ESL students in each class period. She explains that in an ideal world, students are grouped by their level such as all level 1 students are together, level 2 are together, and so forth. Unfortunately, she doesn\u2019t have that luxury at the high school. She works with each level of students individually so each group of students can be differentiated as needed. She splits the groups up between her and Mrs. Brenneman, the para educator, to help each leveled group. The program at HHS is not a program that is like those implemented in other schools. The program isn\u2019t a dual language or other type. They just work based on the situation they have or a melting pot as she states. She is very proud of the graduation rate that has increased. If the kids start with the program, they tend to finish with the program unless they move away. She is proud that the ELL students have been able to take all the required classes or core classes needed to graduate. Even though they might not have a high level of English, \u201cthey do pretty well\u201d. Even though teachers provide differentiation, they learn all the same material as regular students. The students are able to get into the classroom right away. This is good for them since they can get \u201creal world\u201d experience and immerse themselves in the culture and listen to how students talk. No one ever follows grammar rules when the speak.\n\nMrs. Teetor mentions that students have to take a test to see if they need ESL services based on responses on intake forms when transferring into the district. She uses the newly implemented ELPA to screen prospective and current students who might or are using ESL services to measure their level of English. Newly arriving students fill out a questionnaire that askes the language they spoke first, what language is spoken primarily at home, and what language they want to be communicated with. If any of these are not English, they need to be tested. Even though a student knows English, they still have to take the test if their language spoken at home is not English. She uses a computer screener test. Anyone that is four (4) or above, they do not need to be in the program but anything below that, students should be enrolled in ESL/ELL. Things are done differently at the elementary level compared to the high school level.\n\nMrs. Teetor works closely with other teachers. She also likes to know what the teachers are teaching so she can meet the needs of students such as preparing a presentation or report. With American History, the students have no background especially with the foreign students. American history pertains to the United States. Math is math, science is science, these things are the same all the way across cultures. She enjoys how things are taught at HHS. Students are in ELL for at least 2 class periods. She tells us about an example student who doesn\u2019t speak a lot of but is very bright. Looking at her transcripts, she is a very well-rounded person. She has taken courses in psychology to advanced math. She was top of her class before moving. This student is \u201cbrilliant\u201d, according to Mrs. Teetor. She sees how this student can feel out of place due to the language barrier. She sees another barrier with math leaning towards story-problems. It intimidates students since it involves reading instead of involving direct math problems. What is truly moving is that she truly advocates for students to get them in the regular classroom where they will have to think, perform in the real thing. She knows it will be hard but it would be a \u201cdisservice\u201d if she does not.\n\nShe hasn\u2019t faced any teachers who have rejected or negated students. This at least hasn\u2019t occurred during her time her but she says that \u201cit might have happened before I started.\u201d She believes this hasn\u2019t happened because of her strong personality and she knows the teachers will be supportive at Hastings Senior High School. She hopes that the next person who replaces her will continue on standing up for the students. She also knows which teachers will be best for her students and she know the teachers who will not be a good fit. She moves students as necessary to put them in the right classrooms. She would rather do something else with the student if the teacher is not a good fit for the students. For a new teacher, this is something that he or she will not know and this will take time to figure out. She is hopeful the other teacher will be the best advocate for the ESL students.\n\nShe has great advice for new ESL teachers especially for the teacher replacing her. The best advice would be just to have fun with these students. They do need to learn but also everything is not so rigid that they can\u2019t enjoy. Simply cramming information gets them discouraged. It has been interested and fun for them to know. Everyone has some kid within each other. We need to promote this with the kids. She gets a lot out of the kids when they have fun. If they are sitting bored, they are not going anywhere. This is her overall advice.\n\nThank you, Mrs. Teetor for allowing me to interview you. It was such a great insight of ESL and very helpful for me for my future career as an ESL Teacher. I wish you the best in luck with retirement and hope to hear about your adventures in Japan! Cheers!\n\nA podcast of this will be available on my SoundCloud page soon!", "label": 1}
{"title": "One Year Left: Apple\u2019s Long Goodbye For Intel Macs", "url": "https://tedium.co/2025/06/09/apple-wwdc-intel-mac-support-ending/", "content": "As you probably know after all this time, Tedium is obsessed with the closing frame, the end of the story. And today, we learned that Apple is finally ending its 20-year run of Intel-based Macs.\n\nThat\u2019s the bad news. The good news is that they gave the public one more year of new versions, along with the promise of potential security fixes, avoiding an uncomfortable rug-pull like the one that many PowerPC users experienced with Snow Leopard in 2009. That OS came out a mere three years after the discontinuation of the last PowerPC Mac, and users had to figure out the cutoff was happening by reading Apple rumor sites.\n\nWhile some Mac models did get short shrift (owners of the 2020 Intel MacBook Air have some angry skeets to write), for the most part, the company did not try to force this transition to happen faster than it needed to.\n\nThe commercial for the first Intel Mac, dating to 2006. If you\u2019ve ever seen the video for The Postal Service\u2019s \u201cSuch Great Heights\u201d and think it looks very familiar, there\u2019s a reason for that.\n\nIt was as if the company wanted to bury the blow as much as possible, so it didn\u2019t even mention it during the main WWDC keynote, which is the one that the average person cares about. It was instead buried nearly 55 minutes into the 57-minute Platforms State of the Union, where Apple Senior Director of Developer Relations Matthew Firlik dropped the news like this:\n\nMetal 4 is a great example of the tight integration of our software with Apple silicon, creating a whole new class of experiences. In fact, since we began the transition to Apple silicon over five years ago, we\u2019ve been able to add incredible features like Apple Intelligence, Game Mode, Presenter Overlay, and more. We completed the transition to Apple silicon across our entire product lineup two years ago. So your apps can now depend on and build upon these features too. Apple silicon enables us all to achieve things that were previously unimaginable. And it\u2019s time to put all of our focus and innovation there. And so, macOS Tahoe will be the final release for Intel Macs. So if you\u2019ve not done so already, now is a great time to help your users migrate to the Apple silicon versions of your apps.\n\nThis sort of finality\u2014a one-year pre-announcement from an official Apple source\u2014is useful for any old users who have been holding off for whatever reason. But it\u2019s also great for developers, who now have the OK to transition towards an upgrade if they haven\u2019t already. And certainly, Apple\u2019s ARM-based chips are some of the best processors ever made, based on their balance of speed and energy efficiency, which has made the M1 MacBook Air (nearly a 5-year-old machine!) perhaps the greatest goldilocks machine ever created.\n\nBut still, even with all that lead-up, this decision still stings, because it feels unnecessary to put all that good hardware to pasture. As I wrote back in April, a similar decision to put an end of life on Windows 10 is ultimately unnecessary\u2014and it would lead to a lot of good hardware ending up in landfills. That\u2019s the downside, and one we should not ignore.", "label": 0}
{"title": "Microsoft Build 2025: The age of AI agents and building the open agentic web", "url": "https://blogs.microsoft.com/blog/2025/05/19/microsoft-build-2025-the-age-of-ai-agents-and-building-the-open-agentic-web/", "content": "TL;DR? Hear the news as an AI-generated audio overview made using Microsoft 365 Copilot. You can read the transcript here.\n\nWe\u2019ve entered the era of AI agents. Thanks to groundbreaking advancements in reasoning and memory, AI models are now more capable and efficient, and we\u2019re seeing how AI systems can help us all solve problems in new ways.\n\nFor example, 15 million developers are already using GitHub Copilot, and features like agent mode and code review are streamlining the way they code, check, deploy and troubleshoot.\n\nHundreds of thousands of customers are using Microsoft 365 Copilot to help research, brainstorm and develop solutions, and more than 230,000 organizations \u2014 including 90% of the Fortune 500 \u2014 have already used Copilot Studio to build AI agents and automations.\n\nCompanies like Fujitsu and NTT DATA are using Azure AI Foundry to build and manage AI apps and agents that help prioritize sales leads, speed proposal creation and surface client insights. Stanford Health Care is using Microsoft\u2019s healthcare agent orchestrator to build and test AI agents that can help alleviate the administrative burden and speed up the workflow for tumor board preparation.\n\nDevelopers are at the center of it all. For 50 years Microsoft has been empowering developers with tools and platforms to turn their ideas into reality, accelerating innovation at every stage. From AI-driven automation to seamless cloud integration and more, it\u2019s exciting to see how developers are fueling the next generation of digital transformation.\n\nSo, what\u2019s next?\n\nWe envision a world in which agents operate across individual, organizational, team and end-to-end business contexts. This emerging vision of the internet is an open agentic web, where AI agents make decisions and perform tasks on behalf of users or organizations.\n\nAt Microsoft Build we\u2019re showing the steps we\u2019re taking to make this vision a reality through our platforms, products and infrastructure. We\u2019re putting new models and coding agents in the hands of developers, introducing enterprise-grade agents, making our platforms like Azure AI Foundry, GitHub and Windows the best places to build, embracing open protocols and accelerating scientific discovery with AI, all so that developers and organizations can go invent the next big thing.\n\nHere\u2019s a glimpse at just a few of the announcements today:\n\nReimagining the software development lifecycle with AI\n\nAI is fundamentally shifting how code is written, deployed and maintained. Developers are using AI to stay in the flow of their environment longer and to shift their focus to more strategic tasks. And as the software development lifecycle is being transformed, we\u2019re providing new features across platforms including GitHub, Azure AI Foundry and Windows that enable developers to work faster, think bigger and build at scale.\n\nGitHub Copilot coding agent and new updates to GitHub Models: GitHub Copilot is evolving from an in-editor assistant to an agentic AI partner with a first-of-its-kind asynchronous coding agent integrated into the GitHub platform. We\u2019re adding prompt management, lightweight evaluations and enterprise controls to GitHub Models so teams can experiment with best-in-class models, without leaving GitHub. Microsoft is also open-sourcing GitHub Copilot Chat in VS Code. The AI-powered capabilities from GitHub Copilot extensions will now be part of the same open-source repository that drives the world\u2019s most popular development tool. As the home of over 150 million developers, this reinforces our commitment to open, collaborative, AI-powered software development. Learn more about GitHub Copilot updates.\n\nIntroducing Windows AI Foundry: For developers, Windows remains one of the most open and widely used platforms available, with scale, flexibility and growing opportunity. Windows AI Foundry offers a unified and reliable platform supporting the AI developer lifecycle across training and inference. With simple model APIs for vision and language tasks, developers can manage and run open source LLMs via Foundry Local or bring a proprietary model to convert, fine-tune and deploy across client and cloud. Windows AI Foundry is available to get started today. To learn more visit our Windows Developer Blog.\n\nAzure AI Foundry Models and new tools for model evaluation: Azure AI Foundry is a unified platform for developers to design, customize and manage AI applications and agents. With Azure AI Foundry Models, we\u2019re bringing Grok 3 and Grok 3 mini models from xAI to our ecosystem, hosted and billed directly by Microsoft. Developers can now choose from more than 1,900 partner-hosted and Microsoft-hosted AI models, while managing secure data integration, model customization and enterprise-grade governance. We\u2019re also introducing new tools like the Model Leaderboard, which ranks the top-performing AI models across different categories and tasks, and the Model Router, designed to select an optimal model for a specific query or task in real-time. Read more about Azure AI Foundry Models.\n\nMaking AI agents more capable and secure\n\nAI agents are not only changing how developers build, but how individuals, teams and companies get work done. At Build, we\u2019re unveiling new pre-built agents, custom agent building blocks, multi-agent capabilities and new models to help developers and organizations build and deploy agents securely to help increase productivity in meaningful ways.\n\nWith the general availability of Azure AI Foundry Agent Service, Microsoft is bringing new capabilities to empower professional developers to orchestrate multiple specialized agents to handle complex tasks, including bringing Semantic Kernel and AutoGen into a single, developer-focused SDK and Agent-to-Agent (A2A) and Model Context Protocol (MCP) support. To help developers build trust and confidence in their AI agents, we\u2019re announcing new features in Azure AI Foundry Observability for built-in observability into metrics for performance, quality, cost and safety, all incorporated alongside detailed tracing in a streamlined dashboard. Learn more about how to deploy enterprise-grade AI agents in Azure AI Foundry Service.\n\nDiscover, protect and govern in Azure AI Foundry: With Microsoft Entra Agent ID, now in preview, agents that developers create in Microsoft Copilot Studio or Azure AI Foundry are automatically assigned unique identities in an Entra directory, helping enterprises securely manage agents right from the start and avoid \u201cagent sprawl\u201d that could lead to blind spots. Apps and agents built with Foundry further benefit from Purview data security and compliance controls. Foundry also offers enhanced governance tools to set risk parameters, run automated evaluations and receive detailed reports. Learn more about Microsoft Entra Agent ID and Azure AI Foundry integrations with Microsoft Purview Compliance Manager.\n\nIntroducing Microsoft 365 Copilot Tuning and multi-agent orchestration: With Copilot Tuning, customers can use their own company data, workflows and processes to train models and create agents in a simple, low-code way. These agents perform highly accurate, domain-specific tasks securely from within the Microsoft 365 service boundary. For example, a law firm can create an agent that generates documents aligned with its organization\u2019s expertise and style. Additionally, new multi-agent orchestration in Copilot Studio connects multiple agents, allowing them to combine skills and tackle broader, more complex tasks. Check out the Microsoft 365 blog to learn how to access these new tools as well as the Microsoft 365 Copilot Wave 2 spring release, which has moved to general availability and begins rolling out today.\n\nSupporting the open agentic web\n\nTo realize the future of AI agents, we\u2019re advancing open standards and shared infrastructure to provide unique capabilities for customers.\n\nSupporting Model Context Protocol (MCP): Microsoft is delivering broad first-party support for Model Context Protocol (MCP) across its agent platform and frameworks, spanning GitHub, Copilot Studio, Dynamics 365, Azure AI Foundry, Semantic Kernel and Windows 11. In addition, Microsoft and GitHub have joined the MCP Steering Committee to help advance secure, at-scale adoption of the open protocol and announced two new contributions to the MCP ecosystem, an updated authorization specification, which enables people to use their existing trusted sign-in methods to give agents and LLM-powered apps access to data and services such as personal storage drives or subscription services, and the design of an MCP server registry service, which allows anyone to implement public or private, up-to-date, centralized repositories for MCP server entries. Check out the GitHub repository. As we expand our MCP capabilities, our top priority is to ensure we\u2019re building upon a secure foundation. To learn more about this approach see: Securing the Model Context Protocol: Building a Safe Agentic Future on Windows.\n\nA new open project called NLWeb: Microsoft is introducing NLWeb, which we believe can play a similar role to HTML for the agentic web. NLWeb makes it easy for websites to provide a conversational interface for their users with the model of their choice and their own data, allowing users to interact directly with web content in a rich, semantic manner. Every NLWeb endpoint is also an MCP server, so websites can make their content easily discoverable and accessible to AI agents if they choose. Learn more here.\n\nAccelerating scientific discovery with AI\n\nScience may be one of the most important applications of AI, helping to tackle humanity\u2019s most pressing challenges, from drug discovery to sustainability. At Build we\u2019re introducing Microsoft Discovery, an extensible platform built to empower researchers to transform the entire discovery process with agentic AI, helping research and development departments across various industries accelerate the time to market for new products and accelerate and expand the end-to-end discovery process for all scientists. Learn more here.\n\nThis is only a small selection of the many exciting features and updates we will be announcing at Build. We\u2019re looking forward to connecting with those who have registered to join us virtually and in-person, for keynote sessions, live code deep dives, hack sessions and more \u2014 much of which will be available on demand.\n\nPlus, you can get more on all these announcements by exploring the Book of News, the official compendium of all today\u2019s news.\n\nTags: AI, Azure AI, Azure AI Foundry, Book of News, GitHub, GitHub Copilot, Microsoft 365 Copilot, Microsoft Copilot, Microsoft Purview", "label": 0}
{"title": "Bringing AI-powered answers and summaries to file previews on the web", "url": "https://dropbox.tech/machine-learning/bringing-ai-powered-answers-and-summaries-to-file-previews-on-the-web", "content": "Dropbox offers a handful of features that use machine learning to understand content much like a human would. For example, Dropbox can generate summaries and answer questions about files when those files are previewed on the web. Instead of asking a coworker for the gist of last week\u2019s all-hands meetings, Dropbox can provide a summary of the video and a user can ask questions about its contents\u2014all from the file preview. We recently expanded AI-powered summarization and Q&A to handle multiple files simultaneously, too. (As part of our commitment to responsibly using AI, Dropbox abides by a set of AI principles; you can visit our Help Center to learn more. These features are still in early access, and not yet available to all users. These features are also optional, and can be turned on or off for you or your team.) Both our summarization and Q&A features leverage large language models (LLMs) to find, compare, and consolidate the content of the file. An LLM works by ingesting content as text, transforming the ideas contained within it into a numerical representation, and comparing those numerical representations against both the input query and an internal corpus of knowledge to answer the question. This effectively enables a computer to consume and compare information semantically, rather than lexically. For knowledge workers suffering from information overload, we can use machine learning to get them the answers they need\u2014without them having to remember exactly how a piece of information was worded, or where it might be contained within a file. This is what we\u2019ve done with file previews on the web.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images\u2014plus generative AI capabilities across even more connected apps. See what's new \u2192\n\nExtracting text and embeddings with Riviera\n\nThe first part of this process is, of course, retrieving the text. Luckily, we already have a framework for transforming basically any file type to text. Dropbox is capable of turning complex file types like CAD drawings into formats that are easily consumable by web browsers, such as PDF. Historically we have used this system for file previews, but we also use it to power features like transcription and Dropbox Replay. Internally, we call this system Riviera. At a high level, Riviera consists of a frontend which routes requests through one or more plugins that convert a file from one type to another. Each plugin runs in a jail, which is a container designed to run third party code and tools safely and in an isolated manner. The framework maintains a graph of possible conversions, and is capable of chaining together multiple plugins into a multi-step pipeline to perform even more complex transformations. We currently support conversions between about 300 file types, and the system crunches through about 2.5 billion requests\u2014totalling nearly an exabyte, or one billion gigabytes of data\u2014per day. In order to apply machine learning to file previews, the conversions we are interested are those that convert any file type to raw text. In the case of video, text extraction might looks something like:\n\nCopy Video (.mp4) -> Audio (.aac) -> Transcript (.txt)\n\nSome of the conversions in Riviera can be quite expensive to compute on-the-fly, so it also includes a sophisticated caching layer that allows us to reuse conversions between plugins. Each state of the pipeline is cached, so intermediate states can be reused. In the world of LLMs, the mathematical representation of the semantic meaning of text is called the embedding. Riviera treats embeddings like any other file conversion, so in the pipeline above, we can append:\n\n\n\nCopy Video (.mp4) -> Audio (.aac) -> Transcript (.txt) -> AIEmbedding\n\nBy separating the embedding generation from the actual summary generation we can reuse the cache features built into Riviera. If a user wants to summarize a video, then ask some follow-up questions, we only have to generate the transcript and embeddings once.\n\nCopy Video (.mp4) -> Audio (.aac) -> Transcript (.txt) -> AIEmbedding --> Summary | --> Q&A\n\nThe input for the embeddings plugin typically consists of text data extracted from various file types such as documents, videos, or audio files. In the case of video content, for example, the plugin may receive the transcript generated from the audio track of the video. The process of converting text into embeddings involves using advanced language models that take the text input and produce a vector representing that text. In our implementation, we split the text into paragraph-sized chunks and calculate an embedding for each chunk. By doing this, we effectively increase the granularity of the information stored within a file. Instead of having just a single embedding for an entire file, we have multiple embeddings for different sections of the text. This higher granularity, or bit depth, allows us to capture more detailed and nuanced information. We apply the same chunking and embedding method for both summaries and Q&A, ensuring they share the same embedding cache within Riviera. While the actual LLM processing happens inside the summary and Q&A plugins, treating embeddings, summaries, and queries as file conversions inside Riviera allows us to operate these features at Dropbox scale.\n\nThe high level architecture of our file previews surface, with new machine learning components highlighted\n\nThe summarization plugin\n\nA common use case for LLMs is to concisely summarize large amounts of text. When negotiating a contract, for example, a person might copy the text from a contract PDF, paste it into an LLM-powered chat prompt, and ask the LLM to summarize it in understandable terms. While adding this feature into Dropbox may have been useful as-is, we decided that we didn\u2019t want to stop there. Dropbox users store long documents, videos, and other rich media files in Dropbox, and a summarization feature only gets more useful as the length of the file increases. We wanted to unlock this feature for all of our users' files, no matter the format or length. First, we needed to define the qualities of a good summary. A contract to purchase a home might include many different concepts. There might be a long description of payment terms, wire instructions, good-faith deposits, and escrow accounts. It also might have a long description of contingencies and when they can be triggered. A good summary might simply say \u201cthis document is an agreement to purchase a home for X amount, and it has finance and inspection contingencies.\u201d In other words, we defined a good summary as one that can identify all the different ideas or concepts in a document and give the reader the gist of each. Language models enable this to be implemented algorithmically using embeddings, which allow passages to be compared semantically. King and Queen might be relatively close together (they are both regal), King and Dog might be somewhere in the middle (they are both living beings, perhaps), while King and Toaster have very little in common. Language model embeddings allow us to compare passages on thousands of dimensions that are all learned through a long training process. These embeddings in turn enable efficient summarization of large files of essentially unlimited length. Our summarization plugin takes the chunks and associated embeddings from the embeddings plugin and uses k-means clustering to group the text chunks from the file into clusters in this multi-dimensional embedding space. With this method, we can organize data into distinct groups, or clusters, based on their characteristics, so that chunks with similar content are placed in the same cluster. We then identify major clusters (the main ideas of the file) and concatenate a representative chunk from the corresponding text from each cluster into one blob\u2014the context. Finally, we generate a summary of that context via an LLM. We found k-means clustering was better than alternatives such as a summary-of-summaries approach in a couple of ways: Higher diversity of topics. Many individual summaries before reaching the final summary of summaries often repeat similar information. Combining these summaries results in a significant loss of overall file content. Using k-means clustering, we discovered that the summaries covered a broader range of topics\u2014approximately 50% more than with map-reduce, since we search for chunks that are semantically dissimilar to one another.\n\nMany individual summaries before reaching the final summary of summaries often repeat similar information. Combining these summaries results in a significant loss of overall file content. Using k-means clustering, we discovered that the summaries covered a broader range of topics\u2014approximately 50% more than with map-reduce, since we search for chunks that are semantically dissimilar to one another. Lower chance of hallucinations. When the LLM receives the entire file in one go, its likelihood of hallucinating decreases significantly. Conversely, each call made to the LLM presents a chance for hallucination, making the problem exponentially worse when summarizing summaries. The map-reduce approach lacks the context provided by other chunks, compounding the issue. Using the k-means technique, pinpointing errors in the final output\u2014especially when comparing between LLMs or models\u2014becomes much easier because there is just a single LLM call to evaluate.\n\nThe Q&A plugin\n\nOur Q&A plugin works in a similar manner to the summarization plugin, but in a somewhat opposite way. The Q&A plugin takes in the embeddings and text chunks from the embedding plugin and generates a new embedding for the user question. Then for each chunk of file text it computes the distance to the query text embedding. By calculating the closeness of each chunk to the query in vector space, the most relevant chunks are selected. In the summarization plugin the text chunks were selected for dissimilarity, while in the Q&A plugin they were selected for similarity to the query text. These text chunks, along with the query, are sent to the language model for generating the answer. A language model uses the context to generate answers by analyzing the provided text chunks and the query to understand their meaning and relationships. When a query is received, the model first interprets the question and identifies key elements and intents. It then examines the text chunks, which serve as additional context, to extract relevant information. The model employs sophisticated algorithms to detect patterns, correlations, and nuances within the text, allowing it to discern how different pieces of information fit together. By integrating the context from the text chunks with the specifics of the query, the language model can produce a coherent and accurate response that is tailored to the query's requirements. This process involves leveraging large-scale language patterns learned during training, enabling the model to generate answers that are both contextually appropriate and informative. The relevant chunk locations are then returned to the user as sources, allowing them to reference the specific parts of the file that contributed to the answer. As an add on feature to both the summarization and Q&A plugins, we also request context-relevant follow-up questions from the LLM. In testing we found that follow-up questions allow the user to more naturally learn about a file and the topic they are interested in. To gather these follow-up questions, we utilize function calling and structured outputs to request follow-up questions from the LLM at the same time we request the summary or an answer to the initial question.\n\nExpanding to multiple files\n\nThe first iteration of intelligent summaries and Q&A was limited to one file at a time\u2014but we knew we could do better for our customers. We wanted to make LLM-powered understanding possible across collections of files, and not just individual documents. Expanding our understanding capabilities to multiple files within Dropbox involved a significant evolution of our capabilities, infrastructure, UI, and algorithms. Building on our existing file processing pipeline, we expanded Riviera\u2019s capabilities to handle multiple files simultaneously inside of a single plugin. The embeddings plugin would still be separate for every file, but the final summarization or Q&A plugin call would need to take in multiple files. The question was: which subset of files selected by the user would we extract the relevant information from? When testing this feature, we found that some questions were quite direct (\u201cWhat is Dropbox?\u201d) while some were quite broad (\u201cCan you explain this contract like I\u2019m 5?\u201d). For best results we found that we needed to tailor our response accordingly. Direct questions can often be answered in a single sentence, while the answers to broader questions are typically longer and potentially sourced from a wider context. The challenge was determining which type of answer was required. Put another way: How should we determine the number of relevant chunks or files to use when answering a request or query? We eventually came to the conclusion that this was a trick question. You cannot determine if a question is direct or broad based just on the question itself. You also need the context the answer is pulled from. The question \u201cWhat is Dropbox?\u201d could both direct if asked about a list of tech companies, but also broad if asked about the Dropbox Wikipedia page. To solve this question, we took advantage of power law dynamics to determine the number of relevant chunks to send to the LLM.\n\nLine A is a direct question, while line B is a broad question\n\nOur solutions takes the max and min relevance scores from the top 50 text chunks related to the user query, as calculated through the embeddings, and cuts off the bottom 20% of that spread. This works well because, as shown in the diagram above, direct questions have a steeper power law curve than broad questions. For Example A above, lets say that the most relevant chunk has a score of 0.9 and the least is 0.2. In this case, everything below a 0.34 score is discarded (the 20th percentile score). Since the slope is steep, over half of the chunks will be discarded, leaving about 15 left. For Example B\u2014a more broad question\u2014let\u2019s say that the most relevant chunk has a score of 0.5, and the least is 0.2. In this case, everything below a 0.26 would be discarded, leaving about 40 left. Since the slope is flatter, more chunks are chosen to send to the LLM. Another example could be a quarterly earnings report. For the question \u201cwhat were the financial results?\u201d a lot of the chunks would have medium relevance, resulting in something similar to the B curve above, since it is a broad question. For a question like \u201chow much was spent on real estate?\u201d there would be some very relevant chunks and a lot of non-relevant chunks\u2014like the A curve above, since it is a more direct question. The first question would require more chunks to answer the question fully versus the second question. This algorithm allowed us to strategically determine which files and chunks within those files were relevant, and thus, which context to send to the LLM. Direct questions get less but more relevant context, while broad questions are given more context to expand on the topic.\n\nWhat we learned", "label": 0}
{"title": "September 2024", "url": "https://lifeofpablo.com/blog/published:2024-09", "content": "fr\n\nGoing on a hike in Oaxaca - 2015\n\nThis is some writing I wrote in French while visiting my grandparents in 2015\n\nJour 1\n\nJe suis arriv\u00e9 en Mexique.\n\nJour 2\n\nLe second jour de mes vacances \u00e9tait ennuyeux. Je me l\u00e8ve t\u00f4t. Je me suis lev\u00e9 en retard. Je prends une douche. Le jour commence. La derni\u00e8re fois que je suis venu c'\u00e9tait il y a cinq jours avec ma m\u00e8re et mes deux s\u0153urs. Nous \u00e9tions ici partout en \u00e9t\u00e9. Je n'ai vraiment pas fait beaucoup, juste aider ma grand-m\u00e8re \u00e0 cause de sa sant\u00e9. Elle est tr\u00e8s malade.\n\nJour 3\n\nJe crois que j'ai une nouvelle routine quand je me l\u00e8ve. Ceci est \u00e0 cause du fait que je suis dans un nouvel endroit g\u00e9ographique. Je dois recommencer le forme d'habitude que les gens font tous les jours. Pour le petit-d\u00e9jeuner, je mange beaucoup de pain. A c\u00f4t\u00e9 de \u00e7a je bois du caf\u00e9. C' est un repas traditionnel. Il est vite mang\u00e9. Apr\u00e8s \u00e7a nous avons tu\u00e9 une dinde pour manger un autre plat traditionnel. J'ai un nouvel ami. Il s'appelle Bryan. Ouais c'est un nom am\u00e9ricain. Il va \u00e0 l'\u00e9cole. Il est maintenant en vacances. C' est un gar\u00e7on aimable. Les parents de Bryan sont vendeurs de glaces d\u00e9licieuses. Ils vendent aussi du bois de beaucoup parfums. Je passe du temps avec lui. Je vais \u00e0 sa maison. Nous regardons la t\u00e9l\u00e9vision. Chez mes grands-parents n'ont pas de t\u00e9l\u00e9. Nous regardons les programmes am\u00e9ricain en espagnol. J'aime aussi le programme mexicaine aussi. C' est un cool type!\n\nJour 4\n\nJe me suis encore lev\u00e9. La m\u00eame routine comme toujours. Ma routine ici est presque la m\u00eame comme celle- l\u00e0, aux \u00c9tats-Unis. J'ai fait la cuisine pour le d\u00e9jeuner. J'ai pr\u00e9par\u00e9 un repas italien. C'\u00e9tait \"chicken Alfredo\" avec pasta. J'ai enseign\u00e9 mes tantes \u00e0 faire. C'\u00e9tait d\u00e9licieux. Mon grand-p\u00e8re et mes tantes l'aiment. Mais ma grand-m\u00e8re ne l'aime pas. Elle est plus traditionnelle. Elle n'aime pas le change. Oh la la! Elle est compliqu\u00e9e. Vous n'avez pas id\u00e9e. :'( . Apr\u00e8s \u00e7a je suis sorti au centre du village. Je fais du shopping. Je rach\u00e8te d\u00e8s nourriture. Oh la la il faisait chaud. Je transpire beaucoup. Les tortas sont la version mexicaine d'un sandwich en comparaison entre les am\u00e9ricain. Ils sont d\u00e9licieux. Comme j'aime la nourriture mexicaine.", "label": 1}
{"title": "Incident Report: Spotify Outage on April 16, 2025", "url": "https://engineering.atspotify.com/2025/5/incident-report-spotify-outage-on-april-16-2025", "content": "On April 16, Spotify experienced an outage that affected users worldwide. Here is what happened and what we are going to do about it.\n\nContext\n\nWe use Envoy Proxy for our networking perimeter systems. The perimeter is the first piece of our software that receives users (your!) network traffic. It then distributes that traffic to other services. We use cloud regions to distribute that traffic sensibly across the globe.\n\nTo enhance Envoy's capabilities, we develop and integrate our own custom filters. A specific example is our filter for rate limiting, which we discussed in detail during our recent talk at EnvoyCon 2025.\n\nWhat happened?\n\nOn April 16 2025, between 12:20 and 15:45 UTC, we experienced an outage, affecting the majority of our users worldwide. During the incident most traffic was disrupted, except to our Asia Pacific region due to timezone differences. The graph below shows the amount of successful requests on our perimeter, the purple line is the unaffected Asia Pacific region.\n\nWhat caused this outage?\n\nOn the day of the incident we changed the order of our Envoy filters. This change was deemed low risk and as such we applied it to all regions at the same time. Changing the order triggered a bug in one of our filters which in turn caused Envoy to crash. Unlike typical isolated crashes, this crash happened simultaneously on all Envoy instances.\n\nThe immediate restart of all Envoy instances, combined with client side application retry logic, created an unprecedented load spike for the perimeter. The sudden surge in traffic then exposed a misconfiguration. Envoy instances were continuously cycled by Kubernetes as the Envoy max heap size was set higher than the allowed memory limit. As soon as any new Envoy instance started up it received a very large amount of traffic, which in turn caused it to use more than the allowed Kubernetes memory limit. Kubernetes then automatically shut down the instance and the cycle repeated.\n\nLower traffic in our Asia Pacific region at the time of the incident, due to the difference in timezone and time of day, meant the regional Envoy memory usage never reached the kubernetes limit, which is why this region was unaffected.\n\nThe outage was mitigated by increasing the total perimeter server capacity which in turn allowed the Envoy servers to drop under the Kubernetes memory limits. This in turn stopped the continuous cycling of servers.\n\nTimeline\n\n12:18 UTC - Envoy filter order changed and all Envoy instances crash\n\n12:20 UTC - Alarms are triggered indicating a significant drop of incoming traffic\n\n12:28 UTC - Situation escalated, no traffic worldwide except for the Asia Pacific region\n\n14:20 UTC - Traffic fully recovered in the European regions\n\n15:10 UTC - Traffic fully recovered in the US regions\n\n15:40 UTC - All traffic patterns back to normal\n\nWhere do we go from here?\n\nWe recognize the impact such outages can have, and we\u2019re committed to learning from it. Here are the steps we\u2019re taking to improve our systems and prevent similar issues in the future;\n\nWe have fixed the bug causing Envoy to crash\n\nWe have fixed the configuration mismatch between Envoy heap size and Kubernetes memory limits\n\nWe will improve how we roll out configuration changes to our perimeter\n\nWe will improve our monitoring capabilities to be able to catch these issues sooner\n\nAs we have in the past, we will continue to provide transparency on similar occasions so as to hold ourselves accountable and support ongoing improvements to our services.", "label": 0}
{"title": "February 2025", "url": "https://lifeofpablo.com/blog/published:2025-02", "content": "en\n\nThis month's IndieWeb Carnival focuses on self-expression. Everyone and anyone is welcome to participate! I love reading posts in different languages!\n\nSend me a webmention or email me at pablo[at]lifeofpablo.com to submit your posts. I will create a round up post and i will update this blog post with the replies I receive!\n\nWhat is self-expression?\n\nThat depends\n\nself-expression (noun)\n\nself-ex\u00b7\u200bpres\u00b7\u200bsion \u02ccself-ik-\u02c8spre-sh\u0259n\n\nThe expression of one's personality : assertion of one's individual traits\n\nSelf-expression comes in various forms. It manifests differently in every person. There is no single route to how people develop how they become. People express themselves loudly, quietly, non-verbally, or in any other way. Many things influence us in our lives. The environment we grew up in influences how we present to the world. Our cultural backgrounds, the countries we've lived in, the languages we speak, or the type of supportive friends we have in our lives are all the ingredients that build the recipe for who we become and how we share our personalities with the world.\n\nFor me, I love speaking languages! I love playing games in French and being super duper competitive. I love reading in Spanish!\n\nI grew up in a Mexican household which has greatly influenced my ways of self-expression. I love speaking with more emotion at home and being surrounded by people with my cultural background. An example of this is when I am angry, I will cuss in Spanish like no other. I feel so much better after letting it all out. This Instagram reel in Spanish will tell you what I mean.\n\nThe Mexican culture influences my American side and vice versa. I would consider myself very expressive in English. Ever since I was a kid, I loved saying, \"That's so cute\" or \"That flower is so pretty.\" Sharing your emotions is very beautiful. Don't be afraid of saying things out loud like this. Fuck that toxic masculinity garbage!\n\nI love blogging and journaling. I love having a journal - getting lost in my thoughts while jotting them down on beautiful lined or dotted paper on a park bench or sitting crisscross applesauce on the top of a building in San Francisco. I love sharing my thoughts publicly blogging or quietly on paper. Not all forms of expression have to be public. Self-expression also includes time when it's you and only you involved. This includes recording audio notes, coding your new blog engine, or building an arcade cabinet. Alone time is self-expression.\n\nAs I've become older, I've stopped caring as much about what people perceive of me (or what I thought people perceived of me) in the things that interest me. Self-expression is about how comfortable you can be with yourself. It comes down to loving yourself more every day. I love dressing in bright colors, especially on a nice sunny day. It makes me feel great and brightens my mood. Heck, find me wearing the brightest pink shirt out in public in the streets of San Francisco.\n\nHow you view self-expression is open to interpretation. How do you self-express?\n\nHere are some questions to get you started to get the juices flowing:\n\nHow do you present yourself to the world?\n\nWhat does self-expression do for you?\n\nDo you use self-expression as a coping mechanism?\n\nWhat has been your journey to becoming a more expressive version of yourself?\n\nIf your family culture is different from the country that you live in, how do you balance your self-expression? Are they intertwined?\n\nAlso posted on IndieNews\n\nSubmissions", "label": 1}
{"title": "How enterprise engineering teams can successfully adopt AI", "url": "https://github.com/resources/whitepapers/how-enterprise-engineering-teams-can-successfully-adopt-ai", "content": "Afghanistan ( +93 ) \u00c5land ( +358 ) Albania ( +355 ) Algeria ( +213 ) American Samoa ( +1 ) Andorra ( +376 ) Angola ( +244 ) Anguilla ( +1 ) Antigua and Barbuda ( +1 ) Argentina ( +54 ) Armenia ( +374 ) Aruba ( +297 ) Australia ( +61 ) Austria ( +43 ) Azerbaijan ( +994 ) Bahamas ( +1 ) Bahrain ( +973 ) Bangladesh ( +880 ) Barbados ( +1 ) Belarus ( +375 ) Belgium ( +32 ) Belize ( +501 ) Benin ( +229 ) Bermuda ( +1 ) Bhutan ( +975 ) Bolivia ( +591 ) Bonaire, Sint Eustatius and Saba ( +599 ) Bosnia and Herzegovina ( +387 ) Botswana ( +267 ) Brazil ( +55 ) British Indian Ocean Territory ( +246 ) Brunei Darussalam ( +673 ) Bulgaria ( +359 ) Burkina Faso ( +226 ) Burundi ( +257 ) Cambodia ( +855 ) Cameroon ( +237 ) Canada ( +1 ) Cape Verde ( +238 ) Cayman Islands ( +1 ) Central African Republic ( +236 ) Chad ( +235 ) Chile ( +56 ) China ( +86 ) Christmas Island ( +61 ) Cocos (Keeling) Islands ( +61 ) Colombia ( +57 ) Comoros ( +269 ) Congo (Brazzaville) ( +242 ) Congo (Kinshasa) ( +243 ) Cook Islands ( +682 ) Costa Rica ( +506 ) C\u00f4te d'Ivoire ( +225 ) Croatia ( +385 ) Cura\u00e7ao ( +599 ) Cyprus ( +357 ) Czech Republic ( +420 ) Denmark ( +45 ) Djibouti ( +253 ) Dominica ( +1 ) Dominican Republic ( +1 ) Ecuador ( +593 ) Egypt ( +20 ) El Salvador ( +503 ) Equatorial Guinea ( +240 ) Eritrea ( +291 ) Estonia ( +372 ) Ethiopia ( +251 ) Falkland Islands ( +500 ) Faroe Islands ( +298 ) Fiji ( +679 ) Finland ( +358 ) France ( +33 ) French Guiana ( +594 ) French Polynesia ( +689 ) Gabon ( +241 ) Gambia ( +220 ) Georgia ( +995 ) Germany ( +49 ) Ghana ( +233 ) Gibraltar ( +350 ) Greece ( +30 ) Greenland ( +299 ) Grenada ( +1 ) Guadeloupe ( +590 ) Guam ( +1 ) Guatemala ( +502 ) Guernsey ( +44 ) Guinea ( +224 ) Guinea-Bissau ( +245 ) Guyana ( +592 ) Haiti ( +509 ) Honduras ( +504 ) Hong Kong ( +852 ) Hungary ( +36 ) Iceland ( +354 ) India ( +91 ) Indonesia ( +62 ) Iran ( +98 ) Iraq ( +964 ) Ireland ( +353 ) Isle of Man ( +44 ) Israel ( +972 ) Italy ( +39 ) Jamaica ( +1 ) Japan ( +81 ) Jersey ( +44 ) Jordan ( +962 ) Kazakhstan ( +7 ) Kenya ( +254 ) Kiribati ( +686 ) Korea, South ( +82 ) Kuwait ( +965 ) Kyrgyzstan ( +996 ) Laos ( +856 ) Latvia ( +371 ) Lebanon ( +961 ) Lesotho ( +266 ) Liberia ( +231 ) Libya ( +218 ) Liechtenstein ( +423 ) Lithuania ( +370 ) Luxembourg ( +352 ) Macau ( +853 ) Macedonia ( +389 ) Madagascar ( +261 ) Malawi ( +265 ) Malaysia ( +60 ) Maldives ( +960 ) Mali ( +223 ) Malta ( +356 ) Marshall Islands ( +692 ) Martinique ( +596 ) Mauritania ( +222 ) Mauritius ( +230 ) Mayotte ( +262 ) Mexico ( +52 ) Micronesia ( +691 ) Moldova ( +373 ) Monaco ( +377 ) Mongolia ( +976 ) Montenegro ( +382 ) Montserrat ( +1 ) Morocco ( +212 ) Mozambique ( +258 ) Myanmar ( +95 ) Namibia ( +264 ) Nauru ( +674 ) Nepal ( +977 ) Netherlands ( +31 ) New Caledonia ( +687 ) New Zealand ( +64 ) Nicaragua ( +505 ) Niger ( +227 ) Nigeria ( +234 ) Niue ( +683 ) Norfolk Island ( +672 ) Northern Mariana Islands ( +1 ) Norway ( +47 ) Oman ( +968 ) Pakistan ( +92 ) Palau ( +680 ) Palestine ( +970 ) Panama ( +507 ) Papua New Guinea ( +675 ) Paraguay ( +595 ) Peru ( +51 ) Philippines ( +63 ) Poland ( +48 ) Portugal ( +351 ) Puerto Rico ( +1 ) Qatar ( +974 ) Reunion ( +262 ) Romania ( +40 ) Rwanda ( +250 ) Saint Barth\u00e9lemy ( +590 ) Saint Helena ( +290 ) Saint Kitts and Nevis ( +1 ) Saint Lucia ( +1 ) Saint Martin (French part) ( +590 ) Saint Pierre and Miquelon ( +508 ) Saint Vincent and the Grenadines ( +1 ) Samoa ( +685 ) San Marino ( +378 ) Sao Tome and Principe ( +239 ) Saudi Arabia ( +966 ) Senegal ( +221 ) Serbia ( +381 ) Seychelles ( +248 ) Sierra Leone ( +232 ) Singapore ( +65 ) Sint Maarten (Dutch part) ( +1 ) Slovakia ( +421 ) Slovenia ( +386 ) Solomon Islands ( +677 ) Somalia ( +252 ) South Africa ( +27 ) South Sudan ( +211 ) Spain ( +34 ) Sri Lanka ( +94 ) Sudan ( +249 ) Suriname ( +597 ) Svalbard and Jan Mayen Islands ( +47 ) Swaziland ( +268 ) Sweden ( +46 ) Switzerland ( +41 ) Taiwan ( +886 ) Tajikistan ( +992 ) Tanzania ( +255 ) Thailand ( +66 ) Timor-Leste ( +670 ) Togo ( +228 ) Tokelau ( +690 ) Tonga ( +676 ) Trinidad and Tobago ( +1 ) Tunisia ( +216 ) T\u00fcrkiye ( +90 ) Turkmenistan ( +993 ) Turks and Caicos Islands ( +1 ) Tuvalu ( +688 ) Uganda ( +256 ) Ukraine ( +380 ) United Arab Emirates ( +971 ) United Kingdom ( +44 ) United States of America ( +1 ) Uruguay ( +598 ) Uzbekistan ( +998 ) Vanuatu ( +678 ) Vatican City ( +39 ) Venezuela ( +58 ) Vietnam ( +84 ) Virgin Islands, British ( +1 ) Virgin Islands, U.S. ( +1 ) Wallis and Futuna Islands ( +681 ) Yemen ( +967 ) Zambia ( +260 ) Zimbabwe ( +263 )\n\n+1", "label": 0}
{"title": "Generative AI is intellectual sharecropping \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2023/09/25/generative-ai-is-intellectual-sharecropping/", "content": "Replied to Digital sharecropping by Nicholas Carr ( roughtype.com ) One of the fundamental economic characteristics of Web 2.0 is the distribution of production into the hands of the many and the concentration of the economic rewards into the hands of the few. It\u2019s a sharecropping system, but the sharecroppers are generally happy because their interest lies in self-expression or socializing, not in making money, and, besides, the economic value of each of their individual contributions is trivial. It\u2019s only by aggregating those contributions on a massive scale \u2013 on a web scale \u2013 that the business becomes lucrative. To put it a different way, the sharecroppers operate happily in an attention economy while their overseers operate happily in a cash economy.\n\nI encountered this (old) analogy for social media platforms as digital sharecropping and thought it also fit generative AI. Generative AI companies steal our intellectual property then license it back to us. We can\u2019t be compensated reasonably for our individual contributions to the model because they\u2019ve stolen from so many of us and each individual\u2019s work represents a miniscule portion of the entire model. Whatever we generate with their models can\u2019t be copyrighted and used to make money for *us* without significant human contributions \u2014 but generated works are in direct competition with the creators whose works built the model. These powerful, well-funded companies want businesses to fire their employees and pay them instead, making businesses reliant on an opaque, unpredictable service that demands vast amounts of natural resources that may be in short \u2014 and shortening \u2014 supply.\n\nBut unlike social media, which rewards users emotionally rather than financially for their labor, creators aren\u2019t getting anything out of having our work used to train generative AI models. And so we\u2019re fighting back earlier in the cycle than with social media \u2014 maybe before it can become entrenched. AI evangelists speak as if the technology\u2019s supremacy is inevitable, but that\u2019s propaganda to get us to shut up and hand over our creations and our jobs.\n\nRecent AI shenanigans in the news:\n\nChatGPT caught giving horrible advice to cancer patients by Sharon Adarlo (Neoscope)\n\nWhy Silicon Valley\u2019s biggest AI developers are hiring poets by Andrew Deck (rest of world)\n\nIowa Is Using ChatGPT to Take Out Banned Books by Alejandra Gularte (Vulture)\n\nGenerative AI at work", "label": 1}
{"title": "Multilingual innovation in LLMs: How open models help unlock global communication", "url": "https://developers.googleblog.com/en/unlock-global-communication-gemma-projects/", "content": "We are thrilled to celebrate the incredible contributions of the community to the Unlock Global Communication with Gemma competition on Kaggle! Developers tackled the critical challenge in AI of adapting state-of-the-art large language models (LLMs) for diverse cultural and linguistic contexts.\n\nModels often exhibit a bias towards high-resource languages due to the predominant language of their training and evaluation datasets. This can lead to a performance gap, where the latest AI advancements may not be realized in lower-resourced languages. Additionally, these models may not only lack understanding of the language, but also culturally-relevant context that would make these models helpful for the communities.\n\nWe were incredibly impressed by the community's creative solutions for translation of languages, lyrics, old texts, and more.\n\n\n\nHonoring the innovators\n\nThrough hundreds of submissions, developers demonstrated how to bring the transformative power of LLMs to languages everywhere. Projects leveraged custom datasets and efficient post-training methods to adapt Gemma for instruction following, translation, and specific domains. We encourage you to explore the notebooks on Kaggle to see these techniques in action and apply them to your own multilingual projects.\n\nThe first place project adapted Gemma for Swahili understanding, opening up new possibilities to reach 200+ million language speakers. Gemma models were fine-tuned using parameter-efficient fine-tuning techniques for the 2B, 9B, and 27B parameter sizes.\n\nA key aspect of their tuning was Gemma\u2019s \u201cremarkable flexibility in instruction-response formatting,\u201d which allowed the models to parse instructions with minimal structural constraints and generate coherent responses across different input formats.\n\nKnowledge Yielding Adaptive Retrieval Augmentation (Kyara) explored retrieval processes for LLM fine-tuning, demonstrating how to enhance Gemma\u2019s ability to generate informed responses in Traditional Chinese.\n\nThe project focused on building high-quality question & answer (Q&A) datasets using a graph-based approach to knowledge retrieval, inspired on how humans learn by connecting concepts.\n\nThe project fine-tuned Gemma for Arabic language tasks, including translation, summarization, storytelling, and dialogue generation.\n\nAs a language with a rich historical past, the project also aimed to enhance comprehension of older forms of Arabic used in literary texts and art, employing multiple techniques to bridge tasks between Modern Standard Arabic and Classical Arabic.\n\nThis project focused on improving Italian language understanding for Gemma using a cost-effective post-training approach that addresses pitfalls such as hallucinations and catastrophic forgetting.\n\nThe 2B and 9B model sizes were fine-tuned on a mix of data, including a new instruction tuning dataset created using LLM-as-a-judge to ensure the quality of translations.\n\nThis project developed an \u201cAncient Chinese Expert\u201d using Gemma to understand and generate translations for ancient Chinese texts, highlighting the potential of LLMs for historical cultural preservation.\n\nThe model was fine-tuned on a comprehensive dataset to improve linguistic understanding, and post-training included techniques to improve instruction following.\n\nThis project tackled nuanced challenges specific to AI-driven lyric translation, enhancing Gemma\u2019s sensitivity to cultural references and symbolic language, while also ensuring rhythmic fidelity to the original song.\n\nA multilingual dataset contained lyric translations annotated to capture crucial cultural context, emotional tone, and rhythmic features, enabling the model to grasp and replicate the artistic depth of lyrical content.\n\nThis project adapted Gemma 2 JPN to generate Yomigana/Furigana, a reading aid for Japanese text and assist language learners or readers encountering complex Kanji.\n\nWhile other rule-based tools currently exist, LLMs can recognize rare Kanji better and \u201cinterpret the context of a sentence, enabling accurate disambiguation of polyphonic Kanji\u201d. The notebook also noted that conversational capabilities had degraded due to training on the singular translation task.\n\nThis project enhances Gemma\u2019s mathematical and logical understanding in Hindi numeric words, which presents a challenge for models to interpret given complex word formations, for example \u201c\u0926\u094b \u0938\u094c\u201d for \u201c200\u201d or \u201c\u0922\u093e\u0908\u201d for \u201c2.5\u201d.\n\nThe 9B model was fine-tuned on a curated and human expert-verified dataset featuring a wide array of question types, unlocking uses for AI-driven educational tools, automated tutoring, and localized content\n\nThis project fine-tuned the Gemma 2 9B model for translation tasks in Kazakh. A language written in three distinct scripts (Cyrillic, Latin, and Arabic), the Cyrillic version requires approximately twice as many tokens as English, presenting a challenge for training with limited resources.\n\nModel performance showed better benchmarks than the 27B Gemma variant and Google Translate, demonstrating how to adapt LLMs for underrepresented languages using a cost-effective approach.\n\nThis project enables Gemma to understand and translate Old English, the earliest recorded form of the English language. A custom dataset with Old English-Modern English language pairs was created to help tackle the challenge of working with historical languages and limited publicly available data.\n\nThe notebook also features a bonus audio generation component, based on an open-source Icelandic text-to-speech model, offering an approximation of how speech might have sounded.\n\n\n\n10 more awesome projects\n\nGemma 2 Reasoning for Japanese Math: This project created reasoning variants to perform chain-of-thought processes and handle complex problems.\n\nMultitask Gemma2 Agents - Summarise & Translate: This project focused on developing agents capable of multiple tasks.\n\nKorean AI Doctor Gemma2: This project adapted Gemma for medical applications in Korean.\n\nGemma Fine-Tuning for Ru-En Medical Translations: This project enhanced Gemma translation accuracy in ophthalmology.\n\nGemma PT: This project fine-tuned the ShieldGemma content classifier to detect prejudice and disinformation in Portuguese.\n\nHow to Fine-tune Gemma 2 for Advanced Reasoning: This project enhanced Gemma reasoning capabilities by implementing the Coconut (Chain of Continuous Thought) paradigm.\n\nFinetune Gemma Turkish Chat: This project fine-tuned on Gemma on a Q&A dataset to improve accuracy and conversational ability.\n\nFinetuning Gemma2 Customized Dataset: This project fine-tuned Gemma for English-Arabic translation and medical understanding.\n\nGemma-2 Finetuning on Telugu News Dataset: This project adapted Gemma to generate Telugu headlines from news articles.\n\nFinetuned Gemma2 9B Math Reasoning Model Russian: This project enhanced Gemma performance for math problems in Russian.\n\n\n\nLooking ahead with Gemma 3\n\nWith over 7,000 languages spoken worldwide, the potential for AI to bridge communication gaps is immense. The Gemma open model family provides a powerful foundation for developers to adapt high-performing models to low-resource languages.\n\nThe innovation and dedication demonstrated by the Kaggle community in adapting Gemma 2 for various languages are truly inspiring. As we continue to build a future where AI empowers global communication for everyone, we're excited for Gemma 3, which brings pretrained support for over 140 languages, making it a great foundation to build on.\n\nWe encourage developers to explore the possibilities of Gemma, to share their datasets and models with others, and continue to advance multilingual AI together.", "label": 0}
{"title": "Enhancing the Python ecosystem with type checking and free threading", "url": "https://engineering.fb.com/2025/05/05/developer-tools/enhancing-the-python-ecosystem-with-type-checking-and-free-threading/", "content": "Meta and Quansight have improved key libraries in the Python Ecosystem. There is plenty more to do and we invite the community to help with our efforts.\n\nWe\u2019ll look at two key efforts in Python\u2019s packaging ecosystem to make packages faster and easier to use:\n\n\ud83d\ude80 Unlock performance wins for developers through free-threaded Python \u2013 where we leverage Python 3.13\u2019s support for concurrent programming (made possible by removing the Global Interpreter Lock (GIL)).\n\n\u2705 Increase developer velocity in the IDE with improved type annotations.\n\nEnhancing typed Python in the Python scientific stack\n\nType hints, introduced in Python 3.5 with PEP-484, allow developers to specify variable types, enhancing code understanding without affecting runtime behavior. Type-checkers validate these annotations, helping prevent bugs and improving IDE functions like autocomplete and jump-to-definition. Despite their benefits, adoption is inconsistent across the open source ecosystem, with varied approaches to specifying and maintaining type annotations.\n\nThe landscape of open source software is fractured with respect to how type annotations are specified, maintained, and distributed to end users. Some projects have in-line annotations (types directly declared in the source code directly), others keep types in stub files, and many projects have no types at all, relying on third party repositories such as typeshed to provide community-maintained stubs. Each approach has its own pros and cons, but application and maintenance of them has been inconsistent.\n\nMeta and Quansight are addressing this inconsistency through:\n\nDirect contributions: We have improved the type coverage for pandas-stubs and numpy, and are eager to expand the effort to more packages. Community engagement: Promoting type annotation efforts to encourage community involvement, listen to feedback and create actionable ways to improve the ecosystem. Tooling and automation: Developing tools to address common challenges adding types and keeping the types up-to-date with the source code.\n\nImproved type annotations in pandas\n\nTL;DR: Pandas is the second most downloaded package from the Python scientific stack. We improved pandas-stubs package type annotation coverage from 36% to over 50%.\n\nBackground\n\nThe pandas community maintains its own stubs in a separate repository, which must be installed to obtain type annotations. While these stubs are checked separately from the source code, it allows the community to use types with their own type checking and IDE.\n\nImproving type coverage\n\nWhen we began our work in pandas-stubs, coverage was around 36%, as measured by the percentage of parameters, returns, and attributes that had a complete type annotation (the annotation is present and all generics have type arguments). After several weeks of work and about 30 PRs, type completeness is now measured at over 50%. The majority of our contributions involved adding annotations to previously-untyped parameters, adding type arguments to raw generic types, and removing deprecated/undocumented interfaces. We also improved several inaccurate annotations and updated others to match the inline annotations in the pandas source code.\n\nKey introductions\n\nTwo key introductions significantly increased coverage:\n\nReplacing raw Series types with UnknownSeries , a new type aliased to Series[Any] . When applied to return type annotations, this reduces the number of type checker false-positives when the function is called.\n\nImproving types of core Dataframe operations like insert, combine, replace, transpose, and assign, as well as many timestamp and time-zone related APIs.\n\nTooling development\n\nIn addition to improving coverage directly, we developed tooling to catalog public interfaces missing annotations. We also augmented our tools for measuring type coverage to handle the situation where stubs are distributed independently, rather than being packaged into the core library wheel.\n\nWhat is free-threaded Python ?\n\nFree-threaded Python (FTP) is an experimental build of CPython that allows multiple threads to interact with the VM in parallel. Previously, access to the VM required holding the global interpreter lock (GIL), thereby serializing execution of concurrently running threads. With the GIL becoming optional, developers will be able to take full advantage of multi-core processors and write truly parallel code.\n\nBenefits of free-threaded Python\n\nThe benefits of free-threaded Python are numerous:\n\nTrue parallelism in a single process : With the GIL removed, developers can write Python code that takes full advantage of multi-core processors without needing to use multiple processes. CPU-bound code can execute in parallel across multiple cores.\n\nImproved performance: By allowing multiple threads to execute Python code simultaneously, work can be effectively distributed across multiple threads inside a single process.\n\nSimplified concurrency: Free-threading provides developers with a more ergonomic way to write parallel programs in Python. Gone are the days of needing to use multiprocessing.Pool and/or resorting to custom shared memory data structures to efficiently share data between worker processes.\n\nGetting Python\u2019s ecosystem ready for FTP\n\nThe ecosystem of Python packages must work well with free-threaded Python in order for it to be practically useful; application owners can\u2019t use free-threading unless their dependencies work well with it. To that end, we have been taking a \u201cbottoms up\u201d approach to tackle the most difficult/popular packages in the ecosystem. We\u2019ve added free-threading support to many of the most popular packages used for scientific computing (e.g. numpy, scipy, scikit-learn) and language bindings (e.g. Cython, nanobind, pybind, PyO3).\n\nJust getting started\n\nTogether, we made substantial progress in improving type annotations and free-threading compatibility in Python libraries. We couldn\u2019t have done it without the Python community and are asking others to join our efforts. Whether it\u2019s further updates to the type annotations or preparing your code for FTP, we value your help moving the Python ecosystem forward!\n\nTo learn more about Meta Open Source, visit our open source site, subscribe to our YouTube channel, or follow us on Facebook, Threads, X and LinkedIn.", "label": 0}
{"title": "December 2023", "url": "https://lifeofpablo.com/blog/published:2023-12", "content": "en\n\nWhat a year 2023 has been! I've learned so much about myself and hit so many milestones. This will be my first year in review I ever post. I've written brief year in reviews on paper in the past. My year in review for 2023.\n\nI started a Company\n\nI filed and started my own company called, Pabs Tech, LLC. This has been something that had in the back of my mind for a long time. I only did very tiny projects with my company.\n\nI'd like to have more projects where I can help others suceed in there projects.\n\nHelping Friends\n\nI started to help my friends build a small game.\n\nHelp friends with their projects.\n\nBlogging\n\n100 Blog posts! This blog post is number 100 for 2023. This is the most I've written on a personal site. I'm really proud of myself. This year I got back into blogging. I am so happy I did. I wrote a blog post on why I blog.\n\nThe IndieWeb\n\nI joined the IndieWeb this year! I've meet some amazing people who have motivated me in so many ways. I find them inspiring. This is a great communithy to join. I only wish I would have done it sooner.\n\nI got to meet up with an IndieWeb community member while I was visiting Los Angeles.\n\nI've attended almost every Homebrew Website Club (remote) since March.\n\nI started building an IndieAuth extension for Datenstrom Yellow\n\nPodcast\n\nStarted a podcast on my website called Pablo's Thought Podcast on things that I want to share.\n\nPhotography\n\nI really explored photography and videography. I made some major leaps and bounds in both analog and digital photography. My camera visited new cities.\n\nI posted a Photography Year in Review of my favorite shots.\n\nOutdoors\n\nI've gone on 10 hikes this year! Darn it! I live in California and I should make more use of the natural facilities.\n\nTraveling Around\n\nI did some traveling this year. I've been telling myself that I need to travel more of the United States this year even if I have visited these places before. I went to new places in California and went to places in California I've been to in the past. I even went to the same place on the East Coast Again.\n\nI visited two Buzzfeed Offices in Los Angeles and New York City.\n\nI attended NYC Climate Week for the first time\n\nI visited wine country for the first ime\n\nCities:\n\nSan Francisco\n\nLos Angeles\n\nMexico City\n\nNapa Valley (Wine Country)\n\nNew York\n\nOaxaca, Mexico\n\nSan Luis Reservoir, California\n\nThings I have learned in 2023\n\nI'm learned to start loving myself. I started going to therapy. I'm starting to unravel and start understanding why I've seen myself in such a negative light.\n\nIt's okay to make mistakes. Learning from them is key to improve.\n\nFeel less guilty in enjoying time for myself.\n\nBeing active consistently is key. It helps me stay motivated knowing I don't always have to go hard. Consistency is key.\n\nWhat about 2024 ?\n\nI have a lot planned for 2024. I will continue to travel with friends but also go places on my own. I will need to go on more artist dates.\n\nI'm planning to host an IndieWebCamp in Sacramento.\n\nI'd like to run a marathon.\n\nI'd like to learn how to dance.\n\nHere's to a great 2024!", "label": 1}
{"title": "September 2016", "url": "https://lifeofpablo.com/blog/published:2016-09", "content": "en\n\n\"Dear Friends and family,\n\nToday marks for an important announcement about my studies. Since many of you know, I have declared a French Education Major with an ESL endorsement very recently. Everything is starting to look up. So time for the juicy part of my post. Hold on tight!\n\nI would like to tell everyone that I will be studying abroad in France for this upcoming spring semester. Studying in France has been a dream that I have put on hold for quite some time. The desire to study in this country all began when I visited it over 3 years ago. That trip changed me greatly. This was the first time I had faced such culture. Deep down, I knew deep down inside that this is where I would want to live for a portion of my life. Going there for 6 months will be the experience to see a new part of France that I have never been too. I will be at the Universit\u00e9 de Strasbourg 'Institut International d'Etudes Fran\u00e7aises (IIEF UNISTRA).\n\nWanting to become French Teacher, this 6 month trip will greatly benefit me but most importantly, the influence I will have on my future students. I want to tell them what it is like living in another country. This trip will be a major milestone in my life as the want for has made me realize what I really want to do in my life. . As mentioned in a past post, I realized that teaching was actually my true calling. Je suis contente! Going through the process has been a lot of work. I am not even done yet, though! I still have to go to Chicago at some point in the semester to get my student visa. I will also get to see other parts of Europe! I am blessed to have such opportunity to explore outside of my comfort zone.\n\nI only see good things happening. This is only the beginning. Again thank you for the support! I will keep you all updated!\n\nBest Regards,\n\nPablo Morales\"", "label": 1}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2016-08", "content": "en\n\n\"Junior year of college is the turning point of most people's education. It is the time where the most people are deep into their major. You know, the point where it seems too late to turn back. This is not the case for me.\n\nI've changed my major various times to in the last two years. It has not been the easiest process. In most cases it revolved around me jumping back and forth between different areas of science or healthcare related fields to humanities. Not deciding what I wanted in my life has had an effect on my greatly. Many friends know to what extent this led too. Thinking back on this tossing and turning made me realize how I truly did not have a passion for the sciences. You know what it really showed me? I am a person who is very stubborn. I was in denial. Deep down inside, there was no want for me to continue in the sciences. My mindset was not open to explore other options. I felt like a failure if I did not become something in the sciences or be something in healthcare. Avoiding disappointment and failure had become the way my life driving towards.\n\nThere comes a time where you have to wake up and come to reality that you really do not have interest in what you are shooting for after attempting various attempts. I should have followed my heart from the very beginning. My advice to you,\n\n\"\"Do not let anyone tell you what you should study. Only y0u know what you want in life.\"\"\n\nI did that mistake and did not follow my dreams right away. It took a lot of courage for me to stand up to myself. It is sort of shocking, huh? The mind is is your worst enemy. I am glad that I did what I did.\n\nNow I am following my passion. A passion that I have always known to have since the first time I took this class in high school. There is this a major sense of relief. I announce that I will fully commit to become a teacher and strive for what I feel that I will do best\n\nThank you for the support.\n\nWith much love,\n\nPablo Morales\n\n\"", "label": 1}
{"title": "GitHub Enterprise Cloud with data residency: How we built the next evolution of GitHub Enterprise using GitHub", "url": "https://github.blog/engineering/engineering-principles/github-enterprise-cloud-with-data-residency/", "content": "Today, we announced that GitHub Enterprise Cloud will offer data residency, starting with the European Union (EU) on October 29, 2024, to address a critical desire from customers and enable an optimal, unified experience on GitHub for our customers.\n\nData residency and what it means for developers\n\nWe\u2019ve heard for years from enterprises that being able to control where their data resides is critical for them. With data residency, organizations can now store their GitHub code and repository data in their preferred geographical region. With this need met, even more developers across the globe can build on the world\u2019s AI-powered developer platform.\n\nEnterprise Cloud with data residency provides enhanced user control and unique namespaces on ghe.com isolated from the open source cloud on github.com. It\u2019s built on the security, business continuity, and disaster recovery capabilities of Microsoft Azure.\n\nThis is a huge milestone for our customers and for GitHub\u2013a multi-year effort that required extensive time, effort, and dedication across the company. We\u2019re excited to share a behind-the-scenes look at how we leveraged GitHub to develop the next evolution of Enterprise Cloud.\n\nDesigning the architecture for the next evolution of GitHub Enterprise\n\nThis effort started in summer of 2022 with a proof of concept (PoC) and involved teams across GitHub. We carefully considered which architecture would enable us to be successful. After iterating with different approaches, we decided to build the new offering as a feature set that extends Enterprise Cloud. This approach would allow us to be consistently in sync with features on github.com and provide the performance, reliability, and security that our customers expect. For hosting, we effectively leveraged Microsoft Azure\u2019s scale, security, and regional footprint to produce a reliable and secured product with data residency built-in, without having to build new data centers ourselves.\n\nAs the home for all developers, developer experience is critically important for us. We recognized early on that consistency was important, so we sought to minimize differences in developing for Enterprise Cloud and Enterprise Cloud with data residency. To this end, the architecture across both is very similar, reducing complexity, risk, and development costs. The deployment model is familiar to our developers: it builds off of GitHub Actions. Also, changes to github.com and Enterprise Cloud with data residency are deployed minutes apart as part of a unified pipeline.\n\nTo accomplish this, we had to organize the work, modify our build and deployment systems, and validate the quality of the platform. We were able to do all three of these by using GitHub.\n\nOrganizing with GitHub Issues and Projects\n\nTo organize the project, we used GitHub Issues and Projects, taking advantage of multiple views to effectively drive work across multiple projects, more than 100 teams, and over 2,000 issues. Different stakeholders and teams could take advantage of these views to focus on the information most relevant to them. Our talented technical project management team helped coordinate updates and used the filtering and slicing capabilities of Projects to present continuously updated information for each milestone in an easily consumable way.\n\nWe also used upcoming features like issues hierarchy to help us understand relationships between issues, and issue types to help clearly classify issues across repositories. As part of using these features internally we were able to give feedback to the teams working on them and refine the final product. Keep an eye out for future announcements for issues hierarchy and issue types coming soon!\n\nAll of these powerful features helped us keep the initiative on track. We were able to clearly understand potential risk areas and partner across multiple teams to resolve blockers and complex dependencies, keeping the project effectively moving forward across multiple years.\n\nBuilding Enterprise Cloud with data residency using GitHub\n\nGitHub has always been built using GitHub. We wanted to continue this practice to set ourselves up for success with the new data residency feature. To this end, we continued leveraging GitHub Codespaces for development and GitHub Actions for continuous integration (CI). In addition, we added deployment targets for new regions. This produced a development, testing, and CI model that required no changes for our developers and a deployment process that was tightly integrated into the existing flow.\n\nWe have previously discussed our deploy then merge model, where we deploy branches before merging into the main branch. We expanded this approach to include successful deployments to Enterprise Cloud data residency targets before changes could be merged and considered complete, continuing to use the existing GitHub merge queue. A visualization of our monolithic deployment pipeline is shown in the figure below.\n\nWe start by deploying to environments used by GitHub employees in parallel. This includes the internal environment for Enterprise Cloud with data residency discussed more in the next section. As we use GitHub every day to build GitHub, this step helps us catch issues as employees use the product before it impacts our customers. After automated and manual testing, we proceed to roll out to \u201cCanary.\u201d Canary is the name for the stage where we configure our load balancers to gradually direct an increasing percentage of github.com traffic to the updated version of the code in a staged manner. Additional testing occurs in between each stage. Once we successfully deploy the updated version of github.com to all users, we then deploy and validate Enterprise Cloud with data residency in the EU before finishing the process and merging the pull request.\n\nEnsuring all deployments are successful before we merge means changes are deployed in sync across all Enterprise Cloud environments and monitored effectively. Note that in addition to deployments, we also use feature flags to gradually roll out changes to groups of customers to reduce risk. If a deployment to any target fails, we roll back the change completely. Once we have understood the failure and are ready to deploy again, the entire process starts from the beginning with the merge queue.\n\nFinally, to maintain consistency across all teams and services, we created automation to generate deployment pipelines for over 100 services so, as new targets are introduced, each service automatically deploys to the new environment in a consistent order.\n\nUsing Enterprise Cloud with data residency ourselves\n\nTo create the best possible product, we also prioritized using it ourselves and stood up an isolated environment for this purpose. Using our GitHub migration tooling, we moved the day-to-day development for the team working on GitHub Enterprise Importer to that environment, and invested in updating our build, deploy, and development environments to support working from the data resident environment. Since its creation, we have deployed to this environment over 8,000 times. This gave us invaluable feedback about the experience of working in the product with issues, pull requests, and actions that we were able to address early in the development process. We were also able to iterate on our status page tooling and internal Service Level Objective (SLO) process with the new environment in mind. The team is continuing to work in this environment today and runs over 1,000 actions jobs a month. This is a testament to the stability and quality we\u2019ve been able to deliver and our commitment to this feature.\n\nWhat\u2019s next\n\nWe are proud that we\u2019ve been able to evolve Enterprise Cloud to offer data residency while using GitHub to organize, build, deploy, and test it. We\u2019re excited to unlock GitHub for even more developers and for you to experience what we have built, starting on October 29, 2024 in the EU, with more regions on the way.\n\nIf you\u2019re excited about Enterprise Cloud with data residency, please join us at GitHub Universe 2024 to learn more and hear from other companies how they\u2019ve used this to accelerate software development and innovation.\n\nTags:", "label": 0}
{"title": "How to debug code with GitHub Copilot", "url": "https://github.blog/ai-and-ml/github-copilot/how-to-debug-code-with-github-copilot/", "content": "Debugging is an essential part of a developer\u2019s workflow\u2014but it\u2019s also one of the most time consuming. What if AI could streamline the process, helping you analyze, fix, and document code faster? Enter GitHub Copilot, your AI-powered coding assistant.\n\nGitHub Copilot isn\u2019t just for writing code\u2014it\u2019s also a powerful tool for debugging. Whether you\u2019re troubleshooting in your IDE, using Copilot Chat\u2019s slash commands like /fix , or reviewing pull requests (PR) on github.com, GitHub Copilot offers flexible, intelligent solutions to speed up your debugging process. And with the free version of GitHub Copilot, available to all personal GitHub accounts, you can start exploring these features today.\n\nIn this guide, we\u2019ll explore how to debug code with GitHub Copilot, where to use it in your workflow, and best practices to get the most out of its capabilities. Whether you\u2019re new to GitHub Copilot or looking to deepen your skills, this guide has something for you.\n\nStart using GitHub Copilot \ud83c\udf1f GitHub Copilot Free includes 2,000 code completions, 50 Copilot Chat messages per month, multi-file edits, and model options like GPT-4o or Claude 3.5 Sonnet, with native support in VS Code and on GitHub.\n\nDebugging code with GitHub Copilot: surfaces and workflows\n\nDebugging code with GitHub Copilot can help you tackle issues faster while enhancing your understanding of the codebase. Whether you\u2019re fixing syntax errors, refactoring inefficient code, or troubleshooting unexpected behavior, GitHub Copilot can provide valuable insights in your debugging journey.\n\nSo, how exactly does this work? \u201cGitHub Copilot is recognizing patterns and suggesting solutions based on what it has learned,\u201d says Christopher Harrison, Senior Developer Advocate. \u201cOnce you\u2019ve identified the problem area, you can turn to GitHub Copilot and ask, \u2018I\u2019m giving this input but getting this output\u2014what\u2019s wrong?\u2019 That\u2019s where GitHub Copilot really shines.\u201d\n\nLet\u2019s explore how GitHub Copilot can help you debug your code across different surfaces, from your IDE to github.com and even pull requests.\n\n1. In Copilot Chat\n\nCopilot Chat acts as an interactive AI assistant, helping you debug issues with natural language queries. And with Copilot Free, you get 50 chat messages per month. With Copilot Chat, you can:\n\nGet real-time explanations: Ask \u201cWhy is this function throwing an error?\u201d and Copilot Chat will analyze the code and provide insights.\n\nAsk \u201cWhy is this function throwing an error?\u201d and Copilot Chat will analyze the code and provide insights. Use slash commands for debugging: Try /fix to generate a potential solution or /explain for a step-by-step breakdown of a complex function. (More on this later!)\n\nTry to generate a potential solution or for a step-by-step breakdown of a complex function. (More on this later!) Refactor code for efficiency: If your implementation is messy or inefficient, Copilot Chat can suggest cleaner alternatives. Christopher explains, \u201cRefactoring improves the readability of code, making it easier for both developers and GitHub Copilot to understand. And if code is easier to understand, it\u2019s easier to debug and spot problems.\u201d\n\nIf your implementation is messy or inefficient, Copilot Chat can suggest cleaner alternatives. Christopher explains, \u201cRefactoring improves the readability of code, making it easier for both developers and GitHub Copilot to understand. And if code is easier to understand, it\u2019s easier to debug and spot problems.\u201d Walk through errors interactively: Describe your issue in chat and get tailored guidance without ever having to leave your IDE.\n\n\ud83d\udd0e How to find GitHub Copilot Chat Look for the GitHub Copilot icon or prompt to start a conversation in your IDE, on github.com, or within pull requests. Simply click on the icon to get started with code suggestions, unit tests, suggested code fixes, and more!\n\n2. In your IDE\n\nWhen working in popular IDEs like VS Code or JetBrains, GitHub Copilot offers real-time suggestions as you type. It helps by:\n\nFlagging issues: For example, if you declare a variable but forget to initialize it, GitHub Copilot can suggest a correction.\n\nFor example, if you declare a variable but forget to initialize it, GitHub Copilot can suggest a correction. Code fixes: Encounter a syntax error? GitHub Copilot can suggest a fix in seconds, ensuring your code stays error-free.\n\nEncounter a syntax error? GitHub Copilot can suggest a fix in seconds, ensuring your code stays error-free. Contextual assistance: By analyzing your workspace, GitHub Copilot provides solutions tailored to your codebase and project structure.\n\n\ud83d\udd0e How to find GitHub Copilot in VS Code To open up the chat view, head over to the VS Code title bar and select, \u201cUse AI features with Copilot for Free.\u201d\n\nSign in with your GitHub account by clicking on \u201cSign in to use Copilot for Free.\u201d\n\nDon\u2019t have a GitHub Copilot subscription yet? Then follow the browser steps to sign up for your Copilot Free plan. Learn how to install GitHub Copilot in JetBrains, Azure Data Studio, and more >\n\n3. On github.com\n\nGitHub Copilot extends beyond your IDE, offering debugging assistance directly on github.com via Copilot Chat, particularly in repositories and discussions. With this feature, you can:\n\nTroubleshoot code in repositories: Open a file, highlight a problematic section, and use Copilot Chat to analyze it.\n\nOpen a file, highlight a problematic section, and use Copilot Chat to analyze it. Generate test cases: If you\u2019re unsure how to verify a function, GitHub Copilot can suggest test cases based on existing code.\n\nIf you\u2019re unsure how to verify a function, GitHub Copilot can suggest test cases based on existing code. Understand unfamiliar code: Reviewing an open-source project or a teammate\u2019s PR? Ask GitHub Copilot to summarize a function or explain its logic.\n\n\ud83d\udd0e How to find GitHub Copilot on github.com\n\n4. For pull request assistance\n\nGitHub Copilot can also streamline debugging within PRs, ensuring code quality before merging.\n\nSuggest improvements in PR comments: GitHub Copilot can review PRs and propose fixes directly in the conversation.\n\nGitHub Copilot can review PRs and propose fixes directly in the conversation. Generate PR summaries: Struggling to describe your changes? Greg Larkin, Senior Service Delivery Engineer, says, \u201cI use GitHub Copilot in the PR creation process to generate a summary of the changes in my feature branch compared to the branch I\u2019m merging into. That can be really helpful when I\u2019m struggling to figure out a good description, so that other people understand what I did.\u201d\n\nStruggling to describe your changes? Greg Larkin, Senior Service Delivery Engineer, says, \u201cI use GitHub Copilot in the PR creation process to generate a summary of the changes in my feature branch compared to the branch I\u2019m merging into. That can be really helpful when I\u2019m struggling to figure out a good description, so that other people understand what I did.\u201d Explain diffs: Not sure why a change was made? Ask GitHub Copilot to summarize what\u2019s different between commits.\n\nNot sure why a change was made? Ask GitHub Copilot to summarize what\u2019s different between commits. Catch edge cases before merging: Use /analyze to identify potential issues and /tests to generate missing test cases.\n\nUse to identify potential issues and to generate missing test cases. Refactor on the fly: If a PR contains redundant or inefficient code, GitHub Copilot can suggest optimized alternatives.\n\nBy integrating Copilot into your PR workflow, you can speed up code reviews while maintaining high-quality standards. Just be sure to pair it with peer expertise for the best results.\n\n\ud83d\udd0e How to find GitHub Copilot in pull requests\n\n5 slash commands in GitHub Copilot for debugging code\n\nSlash commands turn GitHub Copilot into an on-demand debugging assistant, helping you solve issues faster, get more insights, and improve your code quality. Here are some of the most useful slash commands for debugging:\n\n1. Use /help to get guidance on using GitHub Copilot effectively\n\nThe /help slash command provides guidance on how to interact with GitHub Copilot effectively, offering tips on structuring prompts, using slash commands, and maximizing GitHub Copilot\u2019s capabilities.\n\nHow it works : Type /help in Copilot Chat to receive suggestions on your current task, whether it\u2019s debugging, explaining code, or generating test cases.\n\n: Type in Copilot Chat to receive suggestions on your current task, whether it\u2019s debugging, explaining code, or generating test cases. Example: Need a refresher on what GitHub Copilot can do? Use /help to access a quick guide to slash commands like /fix and /explain .\n\n2. Use /fix to suggest and apply fixes\n\nThe /fix command is a go-to tool for resolving code issues by allowing you to highlight a block of problematic code or describe an error.\n\nHow it works: Select the code causing issues, type /fix , and let Copilot Chat generate suggestions.\n\nSelect the code causing issues, type , and let Copilot Chat generate suggestions. Example: If you have a broken API call, use /fix to get a corrected version with appropriate headers or parameters.\n\n3. Use /explain to understand code and errors\n\nThe /explain command breaks down complex code or cryptic error messages into simpler, more digestible terms.\n\nHow it works: Highlight the code or error message you want clarified, type /explain , and Copilot Chat will provide an explanation. It will explain the function\u2019s purpose, how it processes the data, potential edge cases, and any possible bugs or issues.\n\nHighlight the code or error message you want clarified, type , and Copilot Chat will provide an explanation. It will explain the function\u2019s purpose, how it processes the data, potential edge cases, and any possible bugs or issues. Example: Encounter a \u201cNullPointerException\u201d? Use /explain to understand why it occurred and how to prevent it.\n\n4. Use /tests to generate tests\n\nTesting is key to identifying bugs, and the /tests command helps by generating test cases based on your code.\n\nHow it works: Use /tests on a function or snippet, and Copilot Chat will generate relevant test cases.\n\nUse on a function or snippet, and Copilot Chat will generate relevant test cases. Example: Apply /tests to a sorting function, and Copilot Chat might generate unit tests for edge cases like empty arrays or null inputs.\n\n5. Use /doc to generate or improve documentation\n\nThere are long-term benefits to having good text documentation\u2014for developers and GitHub Copilot, which can draw context from it\u2014because it makes your codebase that much more searchable. By using the /doc command with Copilot Free, you can even ask GitHub Copilot to write a summary of specific code blocks within your IDE.\n\nThe /doc command helps you create or refine documentation for your code, which is critical when debugging or collaborating with others. Clear documentation provides context for troubleshooting, speeds up issue resolution, and helps fellow developers understand your code faster.\n\nHow it works: Highlight a function, class, or file, type /doc and right-click to see the context menu, and Copilot Chat will generate comprehensive comments or documentation.\n\nHighlight a function, class, or file, type and right-click to see the context menu, and Copilot Chat will generate comprehensive comments or documentation. Example: Apply /doc to a function, and Copilot Chat will generate inline comments detailing its purpose, parameters, and expected output.\n\nBy mastering these commands, you can streamline your debugging workflow and resolve issues faster without switching between tools or wasting time on manual tasks.\n\nUsing shortcut keys: Quickly activate GitHub Copilot\u2019s debugging features in VS Code Start/continue debugging: F5\n\nStop debugging: Shift + F5\n\nStep over: F10\n\nStep into: F11\n\nStep out: Shift + F11\n\nToggle breakpoint: F9\n\nBest practices for debugging code with GitHub Copilot\n\nProvide clear context for better results\n\nProviding the right context helps GitHub Copilot generate even more relevant debugging suggestions. As Christopher explains, \u201cThe better that Copilot is able to understand what you\u2019re trying to do and how you\u2019re trying to do it, the better the responses are that it\u2019s able to give to you.\u201d\n\nSince GitHub Copilot analyzes your code within the surrounding scope, ensure your files are well structured and that relevant dependencies are included. If you\u2019re using Copilot Chat, reference specific functions, error messages, or logs to get precise answers instead of generic suggestions.\n\n\ud83d\udca1 Pro tip: Working across multiple files? Use the @workspace command to point GitHub Copilot in the right direction and give it more context for your prompt and intended goal.\n\nAsk, refine, and optimize in real time\n\nInstead of treating GitHub Copilot as a one-and-done solution, refine its suggestions by engaging in a back-and-forth process. Greg says, \u201cI find it useful to ask GitHub Copilot for three or four different options on how to fix a problem or to analyze for performance. The more detail you provide about what you\u2019re after\u2014whether it\u2019s speed, memory efficiency, or another constraint\u2014the better the result.\u201d\n\nThis iterative approach can help you explore alternative solutions you might not have considered, leading to more robust and efficient code.\n\nMaster the art of specific prompts\n\nThe more specific your prompt, the better GitHub Copilot\u2019s response. Instead of asking \u201cWhat\u2019s wrong with this function?\u201d try \u201cWhy is this function returning undefined when the input is valid?\u201d GitHub Copilot performs best when given clear, detailed queries\u2014this applies whether you\u2019re requesting a fix, asking for an explanation, or looking for test cases to verify your changes.\n\nBy crafting precise prompts and testing edge cases, you can use GitHub Copilot to surface potential issues before they become production problems.\n\nTry a structured approach with progressive debugging\n\nNext, try a step-by-step approach to your debugging process! Instead of immediately applying fixes, use GitHub Copilot\u2019s commands to first understand the issue, analyze potential causes, and then implement a solution. This structured workflow\u2014known as progressive debugging\u2014helps you gain deeper insights into your code while ensuring that fixes align with the root cause of the problem.\n\nFor example:\n\nStart with the slash command /explain on a problematic function to understand the issue. Use the slash command /startDebugging to help with configuring interactive debugging. Finally, apply the slash command /fix to generate possible corrections.\n\n\ud83d\udccc Use case: If a function in your React app isn\u2019t rendering as expected, start by running /explain on the relevant JSX or state logic, then use /debug to identify mismanaged props, and finally, apply /fix for a corrected implementation.\n\nCombine commands for a smarter workflow\n\nSome issues require multiple levels of debugging and refinement. By combining commands, you can move from diagnosis to resolution even faster.\n\nFor example:\n\nUse /explain + /fix to understand and resolve issues quickly.\n\nto understand and resolve issues quickly. Apply /fixTestFailure + /tests to find failing tests and generate new ones.\n\n\ud83d\udccc Use case:\n\nFixing a broken function: Run the slash command /explain to understand why it fails, then use the slash command /fix to generate a corrected version.\n\nRun the slash command to understand why it fails, then use the slash command to generate a corrected version. Improving test coverage: Use the slash command /fixTestFailure to identify and fix failing tests, then use the slash command /tests to generate additional unit tests for the highlighted code.\n\nRemember, slash commands are most effective when they\u2019re used in the appropriate context, combined with clear descriptions of the problem, are part of a systematic debugging approach, and followed up with verification and testing.\n\nGitHub Copilot is a powerful tool that enhances your workflow, but it doesn\u2019t replace the need for human insight, critical thinking, and collaboration. As Greg points out, \u201cGitHub Copilot can essentially act as another reviewer, analyzing changes and providing comments. Even so, it doesn\u2019t replace human oversight. Having multiple perspectives on your code is crucial, as different reviewers will spot issues that others might miss.\u201d\n\nBy combining GitHub Copilot\u2019s suggestions with human expertise and rigorous testing, you can debug more efficiently while maintaining high-quality, reliable code.\n\nReady to try the free version of GitHub Copilot?\n\nStart using GitHub Copilot today >\n\nYou can keep the learning going with these resources:\n\n* Debug your app with GitHub Copilot in Visual Studio\n\n* Example prompts for GitHub Copilot Chat\n\n* GitHub Copilot and VS Code tutorials", "label": 0}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2023-07", "content": "It's been a great year so far! Lot's of good things have happened. I've overcame a few things. I started my blog again. The little things add up. One of the major events was becoming part of the IndieWeb Community. It's been a great community that is welcoming. I have learned so much from the people in the community and at Homebrew Website Club. Every single one of them has been pretty amazing.\n\nSince it's Send a Friend a Webmention Day, I want to send a webmention to a few people.\n\nAngelo Gladding\n\nYou were the first person I interacted with at Homebrew Website Club. He gave me the run down and help me connect the pieces based on the information I knew already. Thanks for being so rad! Your bot trained with your voice is pretty tight alongside your mediasoup-based setup.\n\nTracy Durnell I really enjoy your style of writing! I have been looking for better ways to express myself through writing and to find my style of blogging, it's inspiring and I enjoy your content. I've been wanting to make the Apple crumb pie. I'm excited to try it soon! If you need some more information on Oaxacan cuisine, I'm your guy!\n\nJames G Everything you do is super cool especially the programming language you created! You'll have to try some Coffee (and hot chocolate) from Oaxaca. I really enjoyed this month's IndieWeb Carnival topic.\n\nBenji I love what you are doing with your site. I love the minimalist approach. The Sparkles is so rad! It works beautifully with one of my sites.\n\ngRegor Love I hope you picked the blue shirt! I still haven't seen the movie? Yay or nay? Thank you for pointing me in the right directions when it comes to marking up content in a different language. More of content in different languages coming soon.\n\nAlex Sirac Ton site, R\u00e9ussir Mes \u00c9tudes est super cool et informatif! J'aime le blog de ton site web principal. You've inspired me to write in French again!\n\nJo dead.garden Since I love languages, I started looking into toki pona. Thank you so much for sharing this. Hopefully we can speak soon.\n\nAnthony Ciccarello Thank you for helping me out microformats and \"likes\" and getting that squared away. Every time I see you post about your puppy I immediately want to pet but we haven't broken the virtual-physical barrier yet to pet dogs yet. Any ideas?\n\nAlso as a Midwesterner, I saw you have a recipe for Puppy Chow. I will probably be making a batch once I get home.\n\nColin Walker I'm really digging your site. I also stumbled on your Music and I'm really digging it! I'd love to talk about your music since I am teaching a sound engineering class. I've been making progress on the e-book as well.", "label": 1}
{"title": "Video annotator: a framework for efficiently building video classifiers using vision-language models and active learning", "url": "https://netflixtechblog.com/video-annotator-building-video-classifiers-using-vision-language-models-and-active-learning-8ebdda0b2db4?source=collection_home---4------23-----------------------", "content": "Video annotator: a framework for efficiently building video classifiers using vision-language models and active learning Netflix Technology Blog 6 min read \u00b7 Jun 19, 2024 -- 2 Listen Share\n\nAmir Ziai, Aneesh Vartakavi, Kelli Griggs, Eugene Lok, Yvonne Jukes, Alex Alonso, Vi Iyengar, Anna Pulido\n\nIntroduction\n\nProblem\n\nHigh-quality and consistent annotations are fundamental to the successful development of robust machine learning models. Conventional techniques for training machine learning classifiers are resource intensive. They involve a cycle where domain experts annotate a dataset, which is then transferred to data scientists to train models, review outcomes, and make changes. This labeling process tends to be time-consuming and inefficient, sometimes halting after a few annotation cycles.\n\nImplications\n\nConsequently, less effort is invested in annotating high-quality datasets compared to iterating on complex models and algorithmic methods to improve performance and fix edge cases. As a result, ML systems grow rapidly in complexity.\n\nFurthermore, constraints on time and resources often result in leveraging third-party annotators rather than domain experts. These annotators perform the labeling task without a deep understanding of the model\u2019s intended deployment or usage, often making consistent labeling of borderline or hard examples, especially in more subjective tasks, a challenge.\n\nThis necessitates multiple review rounds with domain experts, leading to unexpected costs and delays. This lengthy cycle can also result in model drift, as it takes longer to fix edge cases and deploy new models, potentially hurting usefulness and stakeholder trust.\n\nSolution\n\nWe suggest that more direct involvement of domain experts, using a human-in-the-loop system, can resolve many of these practical challenges. We introduce a novel framework, Video Annotator (VA), which leverages active learning techniques and zero-shot capabilities of large vision-language models to guide users to focus their efforts on progressively harder examples, enhancing the model\u2019s sample efficiency and keeping costs low.\n\nVA seamlessly integrates model building into the data annotation process, facilitating user validation of the model before deployment, therefore helping with building trust and fostering a sense of ownership. VA also supports a continuous annotation process, allowing users to rapidly deploy models, monitor their quality in production, and swiftly fix any edge cases by annotating a few more examples and deploying a new model version.\n\nThis self-service architecture empowers users to make improvements without active involvement of data scientists or third-party annotators, allowing for fast iteration.\n\nVideo understanding\n\nWe design VA to assist in granular video understanding which requires the identification of visuals, concepts, and events within video segments. Video understanding is fundamental for numerous applications such as search and discovery, personalization, and the creation of promotional assets. Our framework allows users to efficiently train machine learning models for video understanding by developing an extensible set of binary video classifiers, which power scalable scoring and retrieval of a vast catalog of content.\n\nVideo classification\n\nVideo classification is the task of assigning a label to an arbitrary-length video clip, often accompanied by a probability or prediction score, as illustrated in Fig 1.\n\nFig 1- Functional view of a binary video classifier. A few-second clip from \u201dOperation Varsity Blues: The College Admissions Scandal\u201d is passed to a binary classifier for detecting the \u201destablishing shots\u201d label. The classifier outputs a very high score (score is between 0 and 1), indicating that the video clip is very likely an establishing shot. In filmmaking, an establishing shot is a wide shot (i.e. video clip between two consecutive cuts) of a building or a landscape that is intended for establishing the time and location of the scene.\n\nVideo understanding via an extensible set of video classifiers\n\nBinary classification allows for independence and flexibility, allowing us to add or improve one model independent of the others. It also has the additional benefit of being easier to understand and build for our users. Combining the predictions of multiple models allows us a deeper understanding of the video content at various levels of granularity, illustrated in Fig 2.\n\nFig 2- Three video clips and the corresponding binary classifier scores for three video understanding labels. Note that these labels are not mutually exclusive. Video clips are from Operation Varsity Blues: The College Admissions Scandal, 6 Underground, and Leave The World Behind, respectively.\n\nVideo Annotator (VA)\n\nIn this section, we describe VA\u2019s three-step process for building video classifiers.\n\nStep 1 \u2014 search\n\nUsers begin by finding an initial set of examples within a large, diverse corpus to bootstrap the annotation process. We leverage text-to-video search to enable this, powered by video and text encoders from a Vision-Language Model to extract embeddings. For example, an annotator working on the establishing shots model may start the process by searching for \u201cwide shots of buildings\u201d, illustrated in Fig 3.\n\nFig 3- Step 1 \u2014 Text-to-video search to bootstrap the annotation process.\n\nStep 2 \u2014 active learning\n\nThe next stage involves a classic Active Learning loop. VA then builds a lightweight binary classifier over the video embeddings, which is subsequently used to score all clips in the corpus, and presents some examples within feeds for further annotation and refinement, as illustrated in Fig 4.\n\nFig 4- Step 2 \u2014 Active Learning loop. The annotator clicks on build, which initiates classifier training and scoring of all clips in a video corpus. Scored clips are organized in four feeds.\n\nThe top-scoring positive and negative feeds display examples with the highest and lowest scores respectively. Our users reported that this provided a valuable indication as to whether the classifier has picked up the correct concepts in the early stages of training and spot cases of bias in the training data that they were able to subsequently fix. We also include a feed of \u201cborderline\u201d examples that the model is not confident about. This feed helps with discovering interesting edge cases and inspires the need for labeling additional concepts. Finally, the random feed consists of randomly selected clips and helps to annotate diverse examples which is important for generalization.\n\nThe annotator can label additional clips in any of the feeds and build a new classifier and repeat as many times as desired.\n\nStep 3 \u2014 review\n\nThe last step simply presents the user with all annotated clips. It\u2019s a good opportunity to spot annotation mistakes and to identify ideas and concepts for further annotation via search in step 1. From this step, users often go back to step 1 or step 2 to refine their annotations.\n\nExperiments\n\nTo evaluate VA, we asked three video experts to annotate a diverse set of 56 labels across a video corpus of 500k shots. We compared VA to the performance of a few baseline methods, and observed that VA leads to the creation of higher quality video classifiers. Fig 5 compares VA\u2019s performance to baselines as a function of the number of annotated clips.\n\nFig 5- Model quality (i.e. Average Precision) as a function of the number of annotated clips for the \u201cestablishing shots\u201d label. We observe that all methods outperform the baseline, and that all methods benefit from additional annotated data, albeit to varying degrees.\n\nYou can find more details about VA and our experiments in this paper.\n\nConclusion\n\nWe presented Video Annotator (VA), an interactive framework that addresses many challenges associated with conventional techniques for training machine learning classifiers. VA leverages the zero-shot capabilities of large vision-language models and active learning techniques to enhance sample efficiency and reduce costs. It offers a unique approach to annotating, managing, and iterating on video classification datasets, emphasizing the direct involvement of domain experts in a human-in-the-loop system. By enabling these users to rapidly make informed decisions on hard samples during the annotation process, VA increases the system\u2019s overall efficiency. Moreover, it allows for a continuous annotation process, allowing users to swiftly deploy models, monitor their quality in production, and rapidly fix any edge cases.\n\nThis self-service architecture empowers domain experts to make improvements without the active involvement of data scientists or third-party annotators, and fosters a sense of ownership, thereby building trust in the system.\n\nWe conducted experiments to study the performance of VA, and found that it yields a median 8.3 point improvement in Average Precision relative to the most competitive baseline across a wide-ranging assortment of video understanding tasks. We release a dataset with 153k labels across 56 video understanding tasks annotated by three professional video editors using VA, and also release code to replicate our experiments.", "label": 0}
{"title": "Globalizing Productions with Netflix\u2019s Media Production Suite", "url": "https://netflixtechblog.com/globalizing-productions-with-netflixs-media-production-suite-fc3c108c0a22?source=collection_home---4------2-----------------------", "content": "Globalizing Productions with Netflix\u2019s Media Production Suite Netflix Technology Blog 12 min read \u00b7 Mar 31, 2025 -- 8 Listen Share\n\nJesse Korosi, Thijs van de Kamp, Mayra Vega, Laura Futuro, Anton Margoline\n\nThe journey from script to screen is full of challenges in the ever-evolving world of film and television. The industry has always innovated, and over the last decade, it started moving towards cloud-based workflows. However, unlocking cloud innovation and all its benefits on a global scale has proven to be difficult. The opportunity is clear: streamline complex media management logistics, eliminate tedious, non-creative task-based work and enable productions to focus on what matters most\u2013creative storytelling. With these challenges in mind, Netflix has developed a suite of tools by filmmakers for filmmakers: the Media Production Suite (MPS).\n\nWhat are we solving for?\n\nSignificant time and resources are devoted to managing media logistics throughout the production lifecycle. An average Netflix title produces around ~200 Terabytes of Original Camera Files (OCF), with outliers up to 700 Terabytes, not including any work-in-progress files, VFX assets, 3D assets, etc. The data produced on set is traditionally copied to physical tape stock like LTO. This workflow has been considered the industry norm for a long time and may be cost-effective, but comes with trade-offs. Aside from needing to physically ship and track all movement of the tape stock, storing media on a physical tape makes it harder to search, play and share media assets; slowing down accessibility to production media when needed, especially when titles need to collaborate with talent and vendors all over the world.\n\nEven when workflows are fully digital, the distribution of media between multiple departments and vendors can still be challenging. A lack of automation and standardization often results in a labour-intensive process across post-production and VFX with a lot of dependencies that introduce potential human errors and security risks. Many productions utilize a large variety of vendors, making this collaboration a large technical puzzle. As file sizes grow and workflows become more complex, these issues are magnified, leading to inefficiencies that slow down post-production and reduce the available time spent on creative work.\n\nMoving media into the cloud introduces new challenges for production and post ramping up to meet the operational and technological hurdles this poses. For some post-production facilities, it\u2019s not uncommon to see a wall of portable hard drives at their facility, with media being hand-carried between vendors because alternatives are not available. The need for a centralized, cloud-based solution that transcends these barriers is more pressing than ever. This results in a willingness to embrace new and innovative ideas, even if exploratory, and introduce drastic workflow changes to productions in pursuit of creative evolution.\n\nAt Netflix, we believe that great stories can come from anywhere, but we have seen that technical limitations in traditional workflows reduce access to media and restrict filmmakers\u2019 access to talent. Besides the need for robust cloud storage for their media, artists need access to powerful workstations and real-time playback. Depending on the market, or production budget, cutting-edge technology might not be available or affordable.\n\nWhat if we started charting a course to break free from many of these technical limitations and found ways to enhance creativity? Industry trade shows like the International Broadcast Convention (IBC) and the National Association of Broadcasters Show (NAB) highlight a strong global trend: instead of bringing media to the artist/applications (traditional workflow) we see the shift towards bringing people and applications to the media (cloud workflows and remote workstations). The concept of cloud-based workflows is not new, as many technology leaders in our industry have been experimenting in this space for more than a decade. However, executing this vision at a Netflix scale with hundreds of titles a year has not been done before\u2026\n\nThe challenge of building a global technology to solve this\n\nBuilding solutions at a global scale poses significant challenges. The art of making movies and series lacks equal access to technology, best practices, and global standardization. Different countries worldwide are at different phases of innovation based on local needs and nuances. While some regions boast over a century of cinematic history and have a strong industry, others are just beginning to carve their niche. This vast gap presents a unique challenge: developing global technology that caters to both established and emerging markets, each with distinct languages and workflows.\n\nThe large diversity of needs by talent and vendors globally creates a standardization challenge and can be seen when productions use a global talent pool. Many mature post-production and VFX facilities have built scripts and automation that flow between various artists and personnel within their facility; allowing a more streamlined workflow, even though the customization is time-consuming. E.g., Transcoding, or transcriptions that automatically run when files are dropped in a hot folder, with the expectation that certain sidecar metadata files will accompany them with a specific organizational structure. Embracing and integrating new workflows introduces the fear of disrupting a well-established process, increasing additional pressure on the profit margins of vendors. Small workflow changes that may seem arbitrary may actually have a large impact on vendors. Therefore, innovation should provide meaningful benefits to a title in order to get adopted at scale. Reliability, a proven track record, strong support, and an incredibly low tolerance for bugs, or issues are top of mind in well-established markets.\n\nIn developing this suite, we recognized the necessity of addressing the vast array of titles that flow through Netflix without the luxury of expanding into a massive operational entity. Consequently, automation became imperative. The intricacies of color and framing management, along with deliverables, must be seamlessly controlled and effortlessly managed by the user, without the need for manual intervention. Therefore, we cannot lean into humans configuring JSON files behind the scenes to map camera formats into deliverables. By embracing open standards, we not only streamline these processes but also facilitate smoother collaboration across diverse markets and countries, ensuring that our global productions can operate with unparalleled efficiency and cohesion. To ensure this, we\u2019ve decided to lean heavily into standards like ACES, AMF, ASC MHL, ASC FDL, and OTIO. ACES and AMF for color pipeline management. ASC MHL for any file management/verifications. ASC FDL will serve as our framing interoperability and OTIO for any timeline interchange. Leaning into standards like this means that many things can be automated at scale and more importantly, high-complexity workflows can be offered to markets or shows that don\u2019t normally have access to them. As an example, if a show is shot on various camera formats all framed and recorded at different resolutions, with different lenses and different safeties on each frame. The task of normalizing all of these for a VFX vendor into one common container with a normalized center extracted frame is often only offered to very high-end titles, considering it takes a human behind the curtain to create all of these mappings. But by leaning into a standard like the FDL, it means this can now easily be automated, and the control for these mappings, put directly in the hands of users.\n\nOur Answer \u2014 Content Hub\u2019s Media Production Suite (MPS)\n\nIntroducing Content Hub Media Production Suite video\n\nBuilding a global scalable solution that could be utilized in a diversity of markets has been an exciting challenge. We set out to provide customizable and feature-rich tooling for advanced users while remaining intuitive and streamlined enough for less experienced filmmakers. With collaboration from Netflix teams, vendors, and talent across the globe, we\u2019ve taken a bold step forward in enabling a suite of tools inside Netflix Content Hub that democratizes technology: the Media Production Suite. While leveraging our scale economies and access to resources, we can now unlock global talent pools for our productions, drastically reduce non-creative task-based work, streamline workflows, and level the playing field between our markets, ultimately maximizing the time available for what matters most; creative work!\n\nSo what is it?\n\n1. Netflix Hybrid Infrastructure: Netflix has invested in a hybrid infrastructure, a mix of cloud-based and physically distributed capabilities operating in multiple locations across the world and close to our productions to optimize user performance. This infrastructure is available for Netflix shows and is foundational under Content Hub\u2019s Media Production Suite tooling. Local storage and compute services are connected through the Netflix Open Connect network (Netflix Content Delivery Network) to the infrastructure of Amazon Web Services (AWS). The system facilitates large volumes of camera and sound media and is built for speed. In order to ensure that productions have sufficient upload speeds to get their media into the cloud, Netflix has started to roll out Content Hub Ingest Centers globally to provide high-speed internet connectivity where required. With all media centralized, MPS eliminates the need for physical media transport and reduces the risk of human error. This approach not only streamlines operations but also enhances security and accessibility.\n\n2. Automation and Tooling: In addition to the Netflix Hybrid infrastructure layer, MPS consists of a suite of tools that tap into the media in the Netflix ecosystem.\n\nFootage Ingest \u2014 An application that allows users to upload media/files into Content Hub.\n\nMedia Library \u2014 A central library that allows users to search, preview, share and download media.\n\nDailies \u2014 A workflow, backed by an operational team, offering automated Quality Control of your footage, sound sync, application of color, rendering, and delivering dailies directly to editorial.\n\nRemote Workstations \u2014 Offering access to remote editorial workstations and storage for post-production needs.\n\nVFX Pulls \u2014 An automated method for converting and delivering visual effects plates, associated color, and framing files to VFX vendors.\n\nConform Pulls \u2014 An automated method for consolidating, trimming, and delivering all OCF to picture-finishing vendors.\n\nMedia Downloader \u2014 An automated download tool that initiates a download once media has been made available in the Netflix cloud.\n\nWhile each of the individual tools within MPS is at different states of maturity, over 350 titles have made use of at least one of the tools noted above. Input has been taken from all over the world while developing, with users ranging from UCAN (United States/Canada), EMEA (Europe, Middle East, and Africa), SEA (South East Asia), LATAM (Latin America), and APAC (Asia Pacific).\n\nSenna: Early Adoption and Insightful Feedback Driving MPS Evolution\n\nMedia from the Brazilian-produced series \u2018Senna\u2019 being reviewed in MPS\n\nThe Brazilian-produced series Senna, which follows the life of legendary Formula 1 driver Ayrton Senna, utilized MPS to reshape their content creation workflow, overcome geographical barriers, and unlock innovation to support world-class storytelling for a global audience. Senna is a groundbreaking series, not just for its storytelling but for its production journey across Argentina, Uruguay, Brazil, and the United Kingdom. With editorial teams spread across Porto Alegre and Spain, and VFX studios collaborating across locations in Brazil, Canada, the United States, and India, all orchestrated by our subsidiary Scanline VFX. The series exemplifies the global nature of modern filmmaking and was the perfect fit for Netflix\u2019s new Content Hub Media Production Suite (MPS).\n\nAt the heart of Senna\u2019s workflow orchestration is MPS. While each of the tools within MPS is based on an opt-in model, in order to use many of the downstream services, the first step is ensuring that the original camera files (OCF) and original sound files (OSF) are uploaded. \u201cWe knew we were going to shoot in different places,\u201d said Post Supervisor Gabriel Queiroz,\u201cto have all this material cloud-based, it\u2019s definitely one of the most important things for us. It would be hard to bring all this media physically from Argentina or wherever to Brazil. It will take us a lot of time.\u201d With Senna shooting across locations, allowing production the capability of uploading their OCF and OSF resulted in no longer requiring shuttling hard drives on airplanes, creating LTO tapes, & managing physical shipments for their negative. And yes, you read that correctly; when utilizing MPS, we don\u2019t require LTO tapes to be written unless there are title-specific needs.\n\nWith Senna beginning production back in June of 2023, our investment in MPS was still very early stages, and the tooling was considered beta. However, with the help, feedback, and partnership from this production, it was quickly realized that the investment was worth doubling down on. Since the early version used on Senna, Netflix has been spinning up ingest centers around the world, where drives can be dropped off, and within a matter of hours, all original camera files are uploaded into the Netflix ecosystem. While creating the ability to upload is not a novel concept, behind the scenes, it\u2019s far from simple. Once a drive has been plugged in and our Netflix Footage Ingest application is opened, a validation is run, ensuring all expected media from set is on the drive. After media has been uploaded and checksums are run validating media integrity, all media is inspected, metadata is extracted, and assets are created for viewing/sharing/downloading with playable proxies. All media is then automatically backed up to a second tier of cloud-based storage for the final archive.\n\nTraditionally, if you wanted to check in with your post vendor on how things are going for each of these media management steps noted above, or whether or not you can clear on set camera cards if you haven\u2019t gotten a completion notification, you would have to pick up the phone and call your vendor. For Senna, anyone who wanted visibility on progress, simply logged in to Content Hub and could see any activity in the Footage Ingest dashboard, as well as look up any information needed on past uploads.\n\nRemote monitoring media being uploaded and archived using the MPS Footage Ingest workflow\n\nWhile many services in MPS are available once media has been uploaded, Senna\u2019s use of MPS focused on VFX. With Senna shooting a high volume of footage and the show having a high volume of VFX shots, according to Post Supervisor Gabriel Queiroz \u201cUsing MPS was basically a no-brainer, [having] used the tool before, I knew what it could bring to the project. And to be honest, with the amount of footage that we have, it was just so much material and with the amount of vendors we have, knowing that we would have to deliver all this footage to all these kinds of vendors, including outside of Brazil and to different parts of the world.\u201d\n\nWith a traditional workflow, utilizing available resources in Latin America, VFX Pulls would have been done manually. This process is prone to human error and more importantly, for a show like Senna, too slow and would have resulted in different I/O methods for every vendor.\n\nIllustrating a traditional VFX Editor having to manage various I/O methods\n\nBy utilizing MPS, the Assistant Editor was able to log into Content Hub, upload an EDL, and have their VFX Pulls automatically transcoded, color files consolidated and all media placed into a Google Drive style folder built directly in Content Hub (called Workspaces). The VFX Editor was able to make any additional tweaks they wanted to the directory before farming out each of the shots to whichever vendor they were meant for. When it came time for the VFX vendors to then send shots back to editorial or DI, this was also done through MPS. Having one standard method for I/O for all VFX file sharing meant that Editorial and DI did not have to manage a different file transfer/workflow for every single vendor that was onboarded.\n\nIllustrating a more streamlined workflow for VFX vendors when using MPS\n\nAfter picture was locked and it was time for Senna to do their Online, the DI facility Quanta was able to utilize the Conform Pull service within MPS. The Conform Pull service allowed their team to upload an EDL, which ran a QC on all of the media from within their edit to ensure a smooth conform and then consolidated, trimmed, and packaged up all of the media they needed for the online. Since this early beta and thanks to learnings from many shows like Senna, advancements have been made in the system\u2019s ability to match back to source media for both Conform and VFX Pulls. Rather than requiring an exact match between EDL and source OCF, there are several variations of fuzzy matching that can take place, as well as a current investigation in using one of our perceptual matching algorithms, allowing for a perceptual conform using computer vision, instead of solely relying on metadata.\n\nInside Senna with Content Hub Media Production Suite video\n\nConclusion\n\nThe Media Production Suite (MPS) represents a transformative leap in how we approach media production at Netflix. By embracing open standards, we have crafted a scalable solution that not only makes economic sense but also democratizes access to advanced production tools across the globe. This approach allows us to eliminate tedious tasks, enabling our teams to focus on what truly matters: creative storytelling. By fostering global collaboration and leveraging the power of cloud-based workflows, we\u2019re not just enhancing efficiency but also elevating the quality of our productions. As we continue to innovate and refine our processes, we remain committed to breaking down barriers and unlocking the full potential of creative talent worldwide. The future of filmmaking is here, and with MPS, we are leading the charge toward a more connected and creatively empowered industry.", "label": 0}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2015-10", "content": "en\n\nTopic of today is music. Importantly bands! As some of you may know, I went to the Vans Warped Tour '15 and discovered a lot of bands. It was probably the best day of my life so far!\n\nI want to introduce you guys to a band I discovered this summer: BoyMeetsWorld. A band of five guys from Ohio.\n\nThey are a really good rock/alternative band. For my die-hard alternative fans. This band is for you!! Those who want to expand their music taste, I also encourage you to listen to them. I really like their music. I can really relate to them.\n\n\"\"The fivesome are out to embody their message and encourage others through their music as they too have faced some of life's tough decisions and overcame them, making their relatability to their fans truly something special.\"\" -According to Vans Warped Tour Site\n\nThese are one of the nicest guys you will ever meet! One can really relate to their music! They never disappoint their growing fan base. Thanks guys for autographing my stuff. This was definitely put in the books.\n\nIt is nice to see a band grow and see them be even more successful as time goes on! I see great things for you guys!\n\nBand Members:\n\nRyan Sulken on drums\n\nBrad Sulken on bass,\n\nDrew Ritcher and\n\nDrew Thomason on guitar,\n\nSupport these guys! You wont regret them!\n\nHey BoyMeetsWorld do you want to come to my college for a concert in the future!? Please?\n\nFind them on BandCamp, Spotify, Google Play Music, etc! This would mean a lot for them! and Twitter (@OfficialBMWBand)You wont regret it!\n\nSources: VansWarpedTour.com , Spotify, BandCamp, Soundcloud.\"", "label": 1}
{"title": "How AI code generation works", "url": "https://github.blog/ai-and-ml/generative-ai/how-ai-code-generation-works/", "content": "Generative AI coding tools are changing software production for enterprises. Not just for their code generation abilities\u2014from vulnerability detection and facilitating comprehension of unfamiliar codebases, to streamlining documentation and pull request descriptions, they\u2019re fundamentally reshaping how developers approach application infrastructure, deployment, and their own work experience.\n\nWe\u2019re now witnessing a significant turning point. As AI models get better, refusing adoption would be like \u201casking an office worker to use a typewriter instead of a computer,\u201d says Albert Ziegler, principal researcher and member of the GitHub Next research and development team.\n\nIn this post, we\u2019ll dive into the inner workings of AI code generation, exploring how it functions, its capabilities and benefits, and how developers can use it to enhance their development experience while propelling your enterprise forward in today\u2019s competitive landscape.\n\nHow to use AI to generate code\n\nAI code generation refers to full or partial lines of code that are generated by machines instead of human developers. This emerging technology leverages advanced machine learning models, particularly large language models (LLMs), to understand and replicate the syntax, patterns, and paradigms found in human-generated code.\n\nThe AI models powering these tools, like ChatGPT and GitHub Copilot, are trained on natural language text and source code from publicly available sources that include a diverse range of code examples. This training enables them to understand the nuances of various programming languages, coding styles, and common practices. As a result, the AI can generate code suggestions that are syntactically correct and contextually relevant based on input from developers.\n\nFavored by 55% of developers, our AI-powered pair programmer, GitHub Copilot, provides contextualized coding assistance based on your organization\u2019s codebase across dozens of programming languages, and targets developers of all experience levels. With GitHub Copilot, developers can use AI to generate code in three ways:\n\n1. Type code and AI can autocomplete the code\n\nAutocompletions are the earliest version of AI code generation. John Berryman, a senior researcher of ML on the GitHub Copilot team, explains the user experience: \u201cI\u2019ll be writing code and taking a pause to think. While I\u2019m doing that, the agent itself is also thinking, looking at surrounding code and content in neighboring tabs. Then it pops up on the screen as gray \u2018ghost text\u2019 that I can reject, partially accept, or fully accept and then, if necessary, modify.\u201d\n\nWhile every developer can reap the benefits of using AI coding tools, experienced programmers can often feel these gains even more so. \u201cIn many cases, especially for experienced programmers in a familiar environment, this suggestion speeds us up. I would have written the same thing. It\u2019s just faster to hit \u2018tab\u2019 (thus accepting the suggestion) than it is to write out those 20 characters by myself,\u201d says Johan Rosenkilde, principal researcher for GitHub Next.\n\nWhether developers are new or highly skilled, they\u2019ll often have to work in less familiar languages, and code completion suggestions using GitHub Copilot can lend a helping hand. \u201cUsing GitHub Copilot for code completion has really helped speed up my learning experience,\u201d says Berryman. \u201cI will often accept the suggestion because it\u2019s something I wouldn\u2019t have written on my own since I don\u2019t know the syntax.\u201d\n\nUsing an AI coding tool has become an invaluable skill in itself. Why? Because the more developers practice coding with these tools, the faster they\u2019ll get at using them.\n\nFor experienced developers in unfamiliar environments, tools like GitHub Copilot can even help jog their memories.\n\nLet\u2019s say a developer imports a new type of library they haven\u2019t used before, or that they don\u2019t remember. Maybe they\u2019re looking to figure out the standard library function or the order of the argument. In these cases, it can be helpful to make GitHub Copilot more explicitly aware of where the developer wants to go by writing a comment.\n\n\u201cIt\u2019s quite likely that the developer might not remember the formula, but they can recognize the formula, and GitHub Copilot can remember it by being prompted,\u201d says Rosenkilde. This is where natural language commentary comes into play: it can be a shortcut for explaining intent when the developer is struggling with the first few characters of code that they need.\n\nIf developers give specific names to their functions and variables, and write documentation, they can get better suggestions, too. That\u2019s because GitHub Copilot can read the variable names and use them as an indicator for what that function should do.\n\nSuddenly that changes how developers write code for the better, because code with good variable and function names are more maintainable. And oftentimes the main job of a programmer is to maintain code, not write it from scratch.\n\n\u201cWhen you push that code, someone is going to review it, and they will likely have a better time reviewing that code if it\u2019s well named, if there\u2019s even a hint of documentation in it, and so on,\u201d says Rosenkilde. In this sense, the symbiotic relationship between the developer and the AI coding tool is not just beneficial for the developer, but for the entire team.\n\n3. Chat directly with AI\n\nWith AI chatbots, code generation can be more interactive. GitHub Copilot Chat, for example, allows developers to interact with code by asking it to explain code, improve syntax, provide ideas, generate tests, and modify existing code\u2014making it a versatile ally in managing coding tasks.\n\nRosenkilde uses the different functionalities of GitHub Copilot:\n\n\u201cWhen I want to do something and I can\u2019t remember how to do it, I type the first few letters of it, and then I wait to see if Copilot can guess what I\u2019m doing,\u201d he says. \u201cIf that doesn\u2019t work, maybe I delete those characters and I write a one liner in commentary and see whether Copilot can guess the next line. If that doesn\u2019t work, then I go to Copilot Chat and explain in more detail what I want done.\u201d\n\nTypically, Copilot Chat returns with something much more verbose and complete than what you get from GitHub Copilot code completion. \u201cNamely, it describes back to you what it is you want done and how it can be accomplished. It gives you code examples, and you can respond and say, oh, I see where you\u2019re going. But actually I meant it like this instead,\u201d says Rosenkilde.\n\nBut using AI chatbots doesn\u2019t mean developers should be hands off. Mistakes in reasoning could lead the AI down a path of further mistakes if left unchecked. Berryman recommends that users should interact with the chat assistant in much the same way that you would when pair programming with a human. \u201cGo back and forth with it. Tell the assistant about the task you are working on, ask it for ideas, have it help you write code, and critique and redirect the assistant\u2019s work in order to keep it on the right track.\u201d\n\nThe importance of code reviews\n\nGitHub Copilot is designed to empower developers to execute their ideas. As long as there is some context for it to draw on, it will likely generate the type of code the developer wants. But this doesn\u2019t replace code reviews between developers.\n\nCode reviews play an important role in maintaining code quality and reliability in software projects, regardless of whether AI coding tools are involved. In fact, the earlier developers can spot bugs in the code development process, the cheaper it is by orders of magnitude.\n\nOrdinary verification would be: does the code parse? Do the tests work? With AI code generation, Ziegler explains that developers should, \u201cScrutinize it in enough detail so that you can be sure the generated code is correct and bug-free. Because if you use tools like that in the wrong way and just accept everything, then the bugs that you introduce are going to cost you more time than you save.\u201d\n\nRosenkilde adds, \u201cA review with another human being is not the same as that, right? It\u2019s a conversation between two developers about whether this change fits into the kind of software they\u2019re building in this organization. GitHub Copilot doesn\u2019t replace that.\u201d\n\nThe advantages of using AI to generate code\n\nWhen developer teams use AI coding tools across the software development cycle, they experience a host of benefits, including:\n\nFaster development, more productivity\n\nAI code generation can significantly speed up the development process by automating repetitive and time-consuming tasks. This means that developers can focus on high-level architecture and problem-solving. In fact, 88% of developers reported feeling more productive when using GitHub Copilot.\n\nRosenkilde reflects on his own experience with GitHub\u2019s AI pair programmer: \u201c95% of the time, Copilot brings me joy and makes my day a little bit easier. And this doesn\u2019t change the code I would have written. It doesn\u2019t change the way I would have written it. It doesn\u2019t change the design of my code. All it does is it makes me faster at writing that same code.\u201d And Rosenkilde isn\u2019t alone: 60% of developers feel more fulfilled with their jobs when using GitHub Copilot.\n\nMental load alleviated\n\nThe benefits of faster development aren\u2019t just about speed: they\u2019re also about alleviating the mental effort that comes with completing tedious tasks. For example, when it comes to debugging, developers have to reverse engineer what went wrong. Detecting a bug can involve digging through an endless list of potential hiding places where it might be lurking, making it repetitive and tedious work.\n\nRosenkilde explains, \u201cSometimes when you\u2019re debugging, you just have to resort to creating print statements that you can\u2019t get around. Thankfully, Copilot is brilliant at print statements.\u201d\n\nA whopping 87% of developers reported spending less mental effort on repetitive tasks with the help of GitHub Copilot.\n\nLess context switching\n\nIn software development, context switching is when developers move between different tasks, projects, or environments, which can disrupt their workflow and decrease productivity. They also often deal with the stress of juggling multiple tasks, remembering syntax details, and managing complex code structures.\n\nWith GitHub Copilot developers can bypass several levels of context switching, staying in their IDE instead of searching on Google or jumping into external documentation.\n\n\u201cWhen I\u2019m writing natural language commentary,\u201d says Rosenkilde, \u201cGitHub Copilot code completion can help me. Or if I use Copilot Chat, it\u2019s a conversation in the context that I\u2019m in, and I don\u2019t have to explain quite as much.\u201d\n\nGenerating code with AI helps developers offload the responsibility of recalling every detail, allowing them to focus on higher-level thinking, problem-solving, and strategic planning.\n\nBerryman adds, \u201cWith GitHub Copilot Chat, I don\u2019t have to restate the problem because the code never leaves my trusted environment. And I get an answer immediately. If there is a misunderstanding or follow-up questions, they are easy to communicate with.\u201d\n\nBefore you implement any AI into your workflow, you should always review and test tools thoroughly to make sure they\u2019re a good fit for your organization. Here are a few considerations to keep in mind.\n\nCompliance\n\nRegulatory compliance . Does the tool comply with relevant regulations in your industry?\n\n. Does the tool comply with relevant regulations in your industry? Compliance certifications. Are there attestations that demonstrate the tool\u2019s compliance with regulations?\n\nSecurity\n\nEncryption . Is the data transmission and storage encrypted to protect sensitive information?\n\n. Is the data transmission and storage encrypted to protect sensitive information? Access controls . Are you able to implement strong authentication measures and access controls to prevent unauthorized access?\n\n. Are you able to implement strong authentication measures and access controls to prevent unauthorized access? Compliance with security standards . Is the tool compliant with industry standards?\n\n. Is the tool compliant with industry standards? Security audits. Does the tool undergo regular security audits and updates to address vulnerabilities?\n\nPrivacy\n\nData handling . Are there clear policies for handling user data and does it adhere to privacy regulations like GDPR, CCPA, etc.?\n\n. Are there clear policies for handling user data and does it adhere to privacy regulations like GDPR, CCPA, etc.? Data anonymization. Does the tool support anonymization techniques to protect user privacy?\n\nPermissioning\n\nRole-based access control . Are you able to manage permissions based on user roles and responsibilities?\n\n. Are you able to manage permissions based on user roles and responsibilities? Granular permissions . Can you control access to different features and functionalities within the tool?\n\n. Can you control access to different features and functionalities within the tool? Opt-in/Opt-out mechanisms. Can users control the use of their data and opt out if needed?\n\nPricing\n\nUnderstand the pricing model . is it based on usage, number of users, features, or other metrics?\n\n. is it based on usage, number of users, features, or other metrics? Look for transparency . Is the pricing structure clear with no hidden costs?\n\n. Is the pricing structure clear with no hidden costs? Scalability. Does the pricing scale with your usage and business growth?\n\nAdditionally, consider factors such as customer support, ease of integration with existing systems, performance, and user experience when evaluating AI coding tools. Lastly, it\u2019s important to thoroughly assess how well the tool aligns with your organization\u2019s specific requirements and priorities in each of these areas.\n\nVisit the GitHub Copilot Trust Center to learn more around security, privacy, and other topics.\n\nCan AI code generation be detected?\n\nThe short answer here is: maybe.\n\nLet\u2019s first give some context to the question. It\u2019s never really the case that a whole code base is generated with AI, because large chunks of AI-generated code are very likely to be wrong. The standard code review process is a good way to avoid this, since large swaths of completely auto-generated code would stand out to a human developer as simply not working.\n\nFor smaller amounts of AI-generated code, there is no way at the moment to detect traces of AI in code with true confidence. There are offerings that purport to classify whether content has AI-generated text, but there are limited equivalents for code, since you\u2019d need a dedicated model to do it. Ziegler explains, \u201cComputer generated code is good enough that it doesn\u2019t leave any particular traces and normally has no clear tells.\u201d\n\nAt GitHub, the Copilot team makes use of a duplicate detection filter that detects exact duplicates in code. So, if you\u2019re writing code and it\u2019s an exact copy of something that exists elsewhere, then it\u2019ll flag it.\n\nIs AI code generation secure?\n\nAI code generation is not any more insecure than human generated code. A combination of testing, manual code reviews, scanning, monitoring, and feedback loops can produce the same quality of code as your human-generated code.\n\nWhen it comes to code generated by GitHub Copilot, developers can use tools like code scanning, which actively reviews your code for potential security issues in real-time and seamlessly integrates the findings into the developer workflow.\n\nUltimately, AI code generation will have vulnerabilities\u2014but so does code written by human developers. As Ziegler explains, \u201cIt\u2019s unclear whether computer generated code does particularly worse. So, the answer is not if you have GitHub Copilot, use a vulnerability checker. The answer is always use a vulnerability checker.\u201d\n\nWatch this video for more tips and words of advice around secure coding best practices with AI.\n\nEmpower your enterprise with AI code generation\n\nWhile the benefits to using AI code generation tools can be significant, it\u2019s important to note that human oversight remains crucial to ensure that the generated code aligns with project goals, coding standards, and business needs.\n\nTech leaders should embrace the use of AI code generation\u2014not only to streamline development, but also to empower developer teams to collaborate, drive meaningful business outcomes, and deliver exceptional value to customers.\n\nLearn more or get started with the world\u2019s most widely adopted AI developer tool.\n\nWant to learn how GitHub can help your organization do more with AI? At GitHub Galaxy 2024, we\u2019ll explore cutting-edge research and best practices in the rapidly evolving world of AI\u2014empowering your business to maximize productivity and innovate at scale. Register now >", "label": 0}
{"title": "Google Cloud announces general availability of APIM Operator for Apigee", "url": "https://developers.googleblog.com/en/google-cloud-announces-apim-operator-for-apigee-general-availability/", "content": "We're excited to announce the general availability of the Apigee APIM Operator, a new feature that brings lightweight API Management and API Gateway capabilities to your GKE environment. This release marks a significant step in our strategy to enable Apigee for API management on any gateway, anywhere.\n\n\n\nWhat does this mean for you?\n\nDeveloper-Native Tooling: For the many cloud-native businesses using CNCF-standardized tooling, you can now configure API management using Kubernetes-like YAML, eliminating the need to switch between different tools.\n\nReduced Friction: By supporting APIM with Kubernetes and CNCF toolchains, we're reducing the conceptual and operational friction for service developers and platform administrators.\n\nPolicy Management: Admins can create APIM template rules with RBAC, allowing different groups to use different sets of policies based on their needs. Users and admins can also add various Apigee policies to APIM templates, achieving a comparable level of functionality to Apigee Hybrid.\n\n\n\nKey Features and Capabilities\n\nThe GA release enables customers to configure a GKE cluster and GKE Gateway to use an Apigee Hybrid instance for API management through a traffic extension (ext-proc callout). It also provides API lifecycle management using YAML-based policies operated through the Kubernetes/CNCF toolchain and offers factory-built-in starter defaults for Day-Zero with workload tailoring.\n\n\n\nAddressing Customer Needs\n\nThis feature addresses the growing need for developer-friendly tooling that simplifies API management. Apigee, with its perceived complexity and the requirement to switch from kubectl to other tooling, was seen as less agile. The APIM Operator is our response to this feedback, providing a more streamlined and efficient way to manage APIs.\n\n\n\nLooking Ahead\n\nBuilding on the strong foundation of this GA release, we are exploring potential future enhancements such as gRPC and GraphQL support to accommodate a wider range of API types. We are also evaluating ways to address current limitations concerning the number of Gateway resources and policy attachments, and will update the community as new features and support become available.\n\nWe believe that the APIM Operator will significantly improve the developer experience and streamline API management for our customers. We are excited to see the innovative ways you will use this feature to build and deploy your applications.", "label": 0}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2022", "content": "en\n\nThe Metro Experience with a Jingle This is part of a three or four part blog series. These posts will be interactive on my website (https://lifeofpablo.com) . I am in the process of rebuilding my website so stayed tuned.\n\nMy name is Pablo Morales. I used to teach within Omaha Public Schools but now I teach in Sacramento, CA. I still teach French & Spanish to middle schoolers.\n\nI have always been a fan of public transportation! Within the United States, commuting to work seems painfully dreadful. We are a nation dominated by cars and bad policies. Our public transportation infrastructure is not good for a very \u201cdeveloped\u201d country. Our roads are suffering, and everything else in-between isn\u2019t pleasant either. This isn't the case in South Korea. I'm only a week or so in since arriving in South Korea. I am absolutely M-I-N-D blown on what I am seeing and hearing.. My brain can't handle this!\n\nCommuting in Seoul, Korea is not about trying to catch your bus or train, you are there for the experience of being in a station. You're probably wondering, \"Pablo, Isn't the topic over public transportation\u2026.you know boring?\" I say, \"Absolutely Not!\" I've been to multiple cities who don't even come close to the Korean experience of public transportation. My favorite part about using the Seoul Metro System is that you get to hear all the cute and relaxing rings and jingles offered at all metro stations and some bus stations.\n\nCommuting is hard on the body when traveling far. It's hot and humid out, or you're just simply tired. Those little jingles give you a glimmer of hope that you are getting closer to your final destination. You stay motivated instead of only hearing the usual \"This train is departing.\" or departing. It brightens the mood. My two favorite jingles or sounds are: The Trumpet Link A steelpan Link Do you see what I mean? It definitely made me smile. Everyone seems so happy. I knew traveling within Seoul was going to be fun but who knew these little jingles are what make the experience of the metro in Seoul.\n\nHere is a handpicked few of my favorite jingles. Not all are in circulation at the moment.\n\nLink\n\nLink\n\nBeing a minority in the US and still being a minority in South Korea.\n\nSomeone who speaks 3 different languages. Learning the Korean language is easy in some aspects and difficult in others. I teach French & Spanish. The one common thing between these two languages is the alphabet - romanized letters. The Korean Language is written in Hangul.\n\nI was an ELL student. Even. I now understand what it is like to be in my student\u2019s mind.", "label": 1}
{"title": "Ten Pointless Facts about Me", "url": "https://forkingmad.blog/ten-pointless-facts-about-me/", "content": "Ten Pointless Facts about Me\n\n23 Apr, 2025\n\nI love doing blogging challenges or participating in blogging prompts. It's fun, and a great way to get a little insight into other people's lives.\n\nI thought I'd create my own, so here it is. Why not do your own blog with your answers and share it. Encourage others. Drop me a note, or comment at the end, if you participate and I will add a link at the bottom of this page. Use the tag #Pointless10 if you're on the socials.\n\nDo you floss your teeth? No! Pointless. A good brush is enough. Tea, coffee, or water? Between the three, water - preferable still and ice-cold. I dabble in tea -- maybe one cup every other day. I've never had a coffee. I know, I'm odd. Did anyone ask about wine? ;-) Bare feet as much as possible, around the house mostly. Outdoors, generally a canvas shoe (Sketchers) . I don't have any sneakers. Favourite dessert? Lime Cheesecake. Not the American baked kind, but the set one. The first thing you do when you wake up? Check my phone (don't we all) then shower. Age you'd like to stick to? 26 seems a good age to be for body health, but I want to retain all the wisdom I've gained at 55. How many hats do you own? Three. All brimmed style hats, no caps. I love wearing a hat but don't do it enough. Describe the last photo you took A bottle of red wine I tried at a friend's house; didn't want to forget what it was. Worst TV show Any soap opera. Although they are a work of fiction, people seem to believe that's how we should live real life... As rude, disrespectful, argumentative twats. As a child, what was your aspiration for adulthood? To become a vet. Didn't happen. I became a Technologist.\n\nHere are the question again, for you to copy and paste:\n\nDo you floss your teeth?\n\nTea, coffee, or water?\n\nFootwear preference?\n\nFavourite dessert?\n\nThe first thing you do when you wake up?\n\nAge you'd like to stick at?\n\nHow many hats do you own?\n\nDescribe the last photo you took?\n\nWorst TV show?\n\nAs a child, what was your aspiration for adulthood?\n\n\n\nOther participating blog I am aware of\n\n\n\nAny Comments?", "label": 1}
{"title": "Making camera uploads for Android faster and more reliable", "url": "https://dropbox.tech/mobile/making-camera-uploads-for-android-faster-and-more-reliable", "content": "Camera uploads is a feature in our Android and iOS apps that automatically backs up a user\u2019s photos and videos from their mobile device to Dropbox. The feature was first introduced in 2012, and uploads millions of photos and videos for hundreds of thousands of users every day. People who use camera uploads are some of our most dedicated and engaged users. They care deeply about their photo libraries, and expect their backups to be quick and dependable every time. It\u2019s important that we offer a service they can trust.\n\nUntil recently, camera uploads was built on a C++ library shared between the Android and iOS Dropbox apps. This library served us well for a long time, uploading billions of images over many years. However, it had numerous problems. The shared code had grown polluted with complex platform-specific hacks that made it difficult to understand and risky to change. This risk was compounded by a lack of tooling support, and a shortage of in-house C++ expertise. Plus, after more than five years in production, the C++ implementation was beginning to show its age. It was unaware of platform-specific restrictions on background processes, had bugs that could delay uploads for long periods of time, and made outage recovery difficult and time-consuming. In 2019, we decided that rewriting the feature was the best way to offer a reliable, trustworthy user experience for years to come. This time, Android and iOS implementations would be separate and use platform-native languages (Kotlin and Swift respectively) and libraries (such as WorkManager and Room for Android). The implementations could then be optimized for each platform and evolve independently, without being constrained by design decisions from the other. This post is about some of the design, validation, and release decisions we made while building the new camera uploads feature for Android, which we released to all users during the summer of 2021. The project shipped successfully, with no outages or major issues; error rates went down, and upload performance greatly improved. If you haven\u2019t already enabled camera uploads, you should try it out for yourself.\n\n\n\nDesigning for background reliability\n\nThe main value proposition of camera uploads is that it works silently in the background. For users who don\u2019t open the app for weeks or even months at a time, new photos should still upload promptly.\n\nHow does this work? When someone takes a new photo or modifies an existing photo, the OS notifies the Dropbox mobile app. A background worker we call the scanner carefully identifies all the photos (or videos) that haven\u2019t yet been uploaded to Dropbox and queues them for upload. Then another background worker, the uploader, batch uploads all the photos in the queue. Uploading is a two step process. First, like many Dropbox systems, we break the file into 4 MB blocks, compute the hash of each block, and upload each block to the server. Once all the file blocks are uploaded, we make a final commit request to the server with a list of all block hashes in the file. This creates a new file consisting of those blocks in the user\u2019s Camera Uploads folder. Photos and videos uploaded to this folder can then be accessed from any linked device.\n\nOne of our biggest challenges is that Android places strong constraints on how often apps can run in the background and what capabilities they have. For example, App Standby limits our background network access if the Dropbox app hasn\u2019t recently been foregrounded. This means we might only be allowed to access the network for a 10-minute interval once every 24 hours. These restrictions have grown more strict in recent versions of Android, and the cross-platform C++ version of camera uploads was not well-equipped to handle them. It would sometimes try to perform uploads that were doomed to fail because of a lack of network access, or fail to restart uploads during the system-provided window when network access became available. Our rewrite does not escape these background restrictions; they still apply unless the user chooses to disable them in Android\u2019s system settings. However, we reduce delays as much as possible by taking maximum advantage of the network access we do receive. We use WorkManager to handle these background constraints for us, guaranteeing that uploads are attempted if, and only if, network access becomes available. Unlike our C++ implementation, we also do as much work as possible while offline\u2014for example, by performing rudimentary checks on new photos for duplicates\u2014before asking WorkManager to schedule us for network access.\n\nMeasuring interactions with our status banners helps us identify emerging issues in our apps, and is a helpful signal in our efforts to eliminate errors. After the rewrite was released, we saw users interacting with more \u201call done\u201d statuses than usual, while the number of \u201cwaiting\u201d or error status interactions went down. (This data reflects only paid users, but non-paying users show similar results.)\n\nTo further optimize use of our limited network access, we also refined our handling of failed uploads. C++ camera uploads aggressively retried failed uploads an unlimited number of times. In the rewrite we added backoff intervals between retry attempts, and also tuned our retry behavior for different error categories. If an error is likely to be transient, we retry multiple times. If it\u2019s likely to be permanent, we don\u2019t bother retrying at all. As a result, we make fewer overall retry attempts\u2014which limits network and battery usage\u2014and users see fewer errors.\n\n\n\nDesigning for performance\n\nOur users don\u2019t just expect camera uploads to work reliably. They also expect their photos to upload quickly, and without wasting system resources. We were able to make some big improvements here. For instance, first-time uploads of large photo libraries now finish up to four times faster. There are a few ways our new implementation achieves this.\n\nParallel uploads\n\nFirst, we substantially improved performance by adding support for parallel uploads. The C++ version uploaded only one file at a time. Early in the rewrite, we collaborated with our iOS and backend infrastructure colleagues to design an updated commit endpoint with support for parallel uploads. Once the server constraint was gone, Kotlin coroutines made it easy to run uploads concurrently. Although Kotlin Flows are typically processed sequentially, the available operators are flexible enough to serve as building blocks for powerful custom operators that support concurrent processing. These operators can be chained declaratively to produce code that\u2019s much simpler, and has less overhead, than the manual thread management that would\u2019ve been necessary in C++.\n\n\n\nCopy val uploadResults = mediaUploadStore .getPendingUploads() .unorderedConcurrentMap(concurrentUploadCount) { mediaUploader.upload(it) } .takeUntil { it != UploadTaskResult.SUCCESS } .toList()\n\nA simple example of a concurrent upload pipeline. unorderedConcurrentMap is a custom operator that combines the built-in flatMapMerge and transform operators.\n\nOptimizing memory use\n\nAfter adding support for parallel uploads, we saw a big uptick in out-of-memory crashes from our early testers. A number of improvements were required to make parallel uploads stable enough for production. First, we modified our uploader to dynamically vary the number of simultaneous uploads based on the amount of available system memory. This way, devices with lots of memory could enjoy the fastest possible uploads, while older devices would not be overwhelmed. However, we were still seeing much higher memory usage than we expected, so we used the memory profiler to take a closer look. The first thing we noticed was that memory consumption wasn\u2019t returning to its pre-upload baseline after all uploads were done. It turned out this was due to an unfortunate behavior of the Java NIO API. It created an in-memory cache on every thread where we read a file, and once created, the cache could never be destroyed. Since we read files with the threadpool-backed IO dispatcher, we typically ended up with many of these caches, one for each dispatcher thread we used. We resolved this by switching to direct byte buffers, which don\u2019t allocate this cache. The next thing we noticed were large spikes in memory usage when uploading, especially with larger files. During each upload, we read the file in blocks, copying each block into a ByteArray for further processing. We never created a new byte array until the previous one had gone out of scope, so we expected only one to be in-memory at a time. However, it turned out that when we allocated a large number of byte arrays in a short time, the garbage collector could not free them quickly enough, causing a transient memory spike. We resolved this issue by re-using the same buffer for all block reads. Parallel scanning and uploading\n\nIn the C++ implementation of camera uploads, uploading could not start until we finished scanning a user\u2019s photo library for changes. To avoid upload delays, each scan only looked at changes that were newer than what was seen in the previous scan. This approach had downsides. There were some edge cases where photos with misleading timestamps could be skipped completely. If we ever missed photos due to a bug or OS change, shipping a fix wasn\u2019t enough to recover; we also had to clear affected users\u2019 saved scan timestamps to force a full re-scan. Plus, when camera uploads was first enabled, we still had to check everything before uploading anything. This wasn\u2019t a great first impression for new users.\n\nIn the rewrite, we ensured correctness by re-scanning the whole library after every change. We also parallelized uploading and scanning, so new photos can start uploading while we\u2019re still scanning older ones. This means that although re-scanning can take longer, the uploads themselves still start and finish promptly.\n\nValidation\n\nA rewrite of this magnitude is risky to ship. It has dangerous failure modes that might only show up at scale, such as corrupting one out of every million uploads. Plus, as with most rewrites, we could not avoid introducing new bugs because we did not understand\u2014or even know about\u2014every edge case handled by the old system. We were reminded of this at the start of the project when we tried to remove some ancient camera uploads code that we thought was dead, and instead ended up DDOSing Dropbox\u2019s crash reporting service. \ud83d\ude43\n\nHash validation in production\n\nDuring early development, we validated many low-level components by running them in production alongside their C++ counterparts and then comparing the outputs. This let us confirm that the new components were working correctly before we started relying on their results. One of those components was a Kotlin implementation of the hashing algorithms that we use to identify photos. Because these hashes are used for de-duplication, unexpected things could happen if the hashes change for even a tiny percentage of photos. For instance, we might re-upload old photos believing they are new. When we ran our Kotlin code alongside the C++ implementation, both implementations almost always returned matching hashes, but they differed about 0.005% of the time. Which implementation was wrong? To answer this, we added some additional logging. In cases where Kotlin and C++ disagreed, we checked if the server subsequently rejected the upload because of a hash mismatch, and if so, what hash it was expecting. We saw that the server was expecting the Kotlin hashes, giving us high confidence the C++ hashes were wrong. This was great news, since it meant we had fixed a rare bug we didn\u2019t even know we had. Validating state transitions\n\nCamera uploads uses a database to track each photo\u2019s upload state. Typically, the scanner adds photos in state NEW and then moves them to PENDING (or DONE if they don\u2019t need to be uploaded). The uploader tries to upload PENDING photos and then moves them to DONE or ERROR. Since we parallelize so much work, it\u2019s normal for multiple parts of the system to read and write this state database simultaneously. Individual reads and writes are guaranteed to happen sequentially, but we\u2019re still vulnerable to subtle bugs where multiple workers try to change the state in redundant or contradictory ways. Since unit tests only cover single components in isolation, they won\u2019t catch these bugs. Even an integration test might miss rare race conditions. In the rewritten version of camera uploads, we guard against this by validating every state update against a set of allowed state transitions. For instance, we stipulate that a photo can never move from ERROR to DONE without passing back through PENDING. Unexpected state transitions could indicate a serious bug, so if we see one, we stop uploading and report an exception.\n\nThese checks helped us detect a nasty bug early in our rollout. We started to see a high volume of exceptions in our logs that were caused when camera uploads tried to transition photos from DONE to DONE. This made us realize we were uploading some photos multiple times! The root cause was a surprising behavior in WorkManager where unique workers can restart before the previous instance is fully cancelled. No duplicate files were being created because the server rejects them, but the redundant uploads were wasting bandwidth and time. Once we fixed the issue, upload throughput dramatically improved.\n\nRolling it out\n\nEven after all this validation, we still had to be cautious during the rollout. The fully-integrated system was more complex than its parts, and we\u2019d also need to contend with a long tail of rare device types that are not represented in our internal user testing pool. We also needed to continue to meet or surpass the high expectations of all our users who rely on camera uploads.\n\nTo reduce this risk preemptively, we made sure to support rollbacks from the new version to the C++ version. For instance, we ensured that all user preference changes made in the new version would apply to the old version as well. In the end we never ended up needing to roll back, but it was still worth the effort to have the option available in case of disaster. We started our rollout with an opt-in pool of beta (Play Store early access) users who receive a new version of the Dropbox Android app every week. This pool of users was large enough to surface rare errors and collect key performance metrics such as upload success rate. We monitored these key metrics in this population for a number of months to gain confidence it was ready to ship widely. We discovered many problems during this time period, but the fast beta release cadence allowed us to iterate and fix them quickly. We also monitored many metrics that could hint at future problems. To make sure our uploader wasn\u2019t falling behind over time, we watched for signs of ever-growing backlogs of photos waiting to upload. We tracked retry success rates by error type, and used this to fine-tune our retry algorithm. Last but not least, we also paid close attention to feedback and support tickets we received from users, which helped surface bugs that our metrics had missed. When we finally released the new version of camera uploads to all users, it was clear our months spent in beta had paid off. Our metrics held steady through the rollout and we had no major surprises, with improved reliability and low error rates right out of the gate. In fact, we ended up finishing the rollout ahead of schedule. Since we\u2019d front-loaded so much quality improvement work into the beta period (with its weekly releases), we didn\u2019t have any multi-week delays waiting for critical bug fixes to roll out in the stable releases.\n\nSo, was it worth it?\n\nRewriting a big legacy feature isn\u2019t always the right decision. Rewrites are extremely time-consuming\u2014the Android version alone took two people working for two full years\u2014and can easily cause major regressions or outages. In order to be worthwhile, a rewrite needs to deliver tangible value by improving the user experience, saving engineering time and effort in the long term, or both.\n\nWhat advice do we have for others who are beginning a project like this? Define your goals and how you will measure them. At the start, this is important to make sure that the benefits will justify the effort. At the end, it will help you determine whether you got the results you wanted. Some goals (for example, future resilience against OS changes) may not be quantifiable\u2014and that\u2019s OK\u2014but it\u2019s good to spell out which ones are and aren\u2019t.\n\nAt the start, this is important to make sure that the benefits will justify the effort. At the end, it will help you determine whether you got the results you wanted. Some goals (for example, future resilience against OS changes) may not be quantifiable\u2014and that\u2019s OK\u2014but it\u2019s good to spell out which ones are and aren\u2019t. De-risk it. Identify the components (or system-wide interactions) that would cause the biggest problems if they failed, and guard against those failures from the very start. Build critical components first, and try to test them in production without waiting for the whole system to be finished. It\u2019s also worth doing extra work up-front in order to be able to roll back if something goes wrong.\n\nIdentify the components (or system-wide interactions) that would cause the biggest problems if they failed, and guard against those failures from the very start. Build critical components first, and try to test them in production without waiting for the whole system to be finished. It\u2019s also worth doing extra work up-front in order to be able to roll back if something goes wrong. Don\u2019t rush. Shipping a rewrite is arguably riskier than shipping a new feature, since your audience is already relying on things to work as expected. Start by releasing to an audience that\u2019s just large enough to give you the data you need to evaluate success. Then, watch and wait (and fix stuff) until your data give you confidence to continue. Dealing with problems when the user-base is small is much faster and less stressful in the long run.\n\nShipping a rewrite is arguably riskier than shipping a new feature, since your audience is already relying on things to work as expected. Start by releasing to an audience that\u2019s just large enough to give you the data you need to evaluate success. Then, watch and wait (and fix stuff) until your data give you confidence to continue. Dealing with problems when the user-base is small is much faster and less stressful in the long run. Limit your scope. When doing a rewrite, it\u2019s tempting to tackle new feature requests, UI cleanup, and other backlog work at the same time. Consider whether this will actually be faster or easier than shipping the rewrite first and fast-following with the rest. During this rewrite we addressed issues linked to the core architecture (such as crashes intrinsic to the underlying data model) and deferred all other improvements. If you change the feature too much, not only does it take longer to implement, but it\u2019s also harder to notice regressions or roll back. In this case, we feel good about the decision to rewrite. We were able to improve reliability right away, and more importantly, we set ourselves up to stay reliable in the future. As the iOS and Android operating systems continue to evolve in separate directions, it was only a matter of time before the C++ library broke badly enough to require fundamental systemic changes. Now that the rewrite is complete, we\u2019re able to build and iterate on camera uploads much faster\u2014and offer a better experience for our users, too.\n\nAlso: We're hiring!", "label": 0}
{"title": "Sorbet: Stripe\u2019s type checker for Ruby", "url": "https://stripe.com/blog/sorbet-stripes-type-checker-for-ruby", "content": "By 2017, Stripe had grown to the point where hundreds of engineers had written millions of lines of code. Most of that code was\u2014and still is\u2014written in Ruby, which is famous for helping engineers iterate quickly (if somewhat notorious for encouraging inscrutable code). Unfortunately we were starting to see Ruby come apart at the seams: new engineers found it hard to learn the codebase, and existing engineers were scared to make sweeping changes. Everyone faced a constant tradeoff: run the fast, local tests which might not catch many breakages, or run all the tests, even the slow ones. Ruby was becoming a source of friction more than a source of productivity.\n\nWe set out to change that, with two goals in mind: make it easier to understand the code, while doubling down on what makes Ruby productive and delightful. This was the backdrop against which we decided to create and open source Sorbet, a fast, powerful type checker designed for Ruby. Sorbet statically analyzes a codebase, builds up an understanding of how each piece of code relates to every other piece, and then exposes that knowledge to the programmer via type errors, autocompletion results, documentation on hover, or jumps between definitions and usages.\n\nToday Sorbet runs over Stripe\u2019s entire Ruby codebase, currently amounting to over 15 million lines of code spread across 150,000 files. We can't take credit for pioneering the idea of adding static types to a dynamically typed language\u2014Microsoft and Facebook popularized the approach with TypeScript and Hack, respectively. However, we thought it was worth sharing how Sorbet has not just met but exceeded our goals in the almost four years since we first enabled it on our Ruby codebase.\n\nSorbet reinforces the delightful bits of Ruby while making engineers more productive. Not only has it made code easier to understand, it\u2019s even helped shape and reinforce Stripe's engineering culture as we've grown. But before we dive into what makes Sorbet\u2026 Sorbet, let\u2019s take a short step back in time to its origins at Stripe.\n\nA brief history of Sorbet inside Stripe\n\nType annotations arrived in Stripe's Ruby codebase as early as November 2016, almost a full year before work began on Sorbet. These annotations were born out of a desire to encourage engineers to write modular units with clear public interfaces. Here's an example test case from the pull request that introduced type annotations:\n\nNeither Sorbet nor any other static type checker existed to consume these type annotations yet; they existed only at runtime. The declare_method call above acted like a decorator on the def call method: it would check that the msg argument given to call was a String and that call returned a String on every invocation. Throughout the next year, these runtime-only annotations spread throughout Stripe's codebase.\n\nMonths prior we had added Flow, a static type checker for JavaScript, to our frontend codebase. Ruby developers quickly grew envious and kept asking us what it would take to get the same features for Ruby. We staffed an effort to figure out what it would take to either adopt one of the two in-progress Ruby type checkers\u2014RDL and TypedRuby\u2014or to build our own. RDL proved to be powerful, but too slow . TypedRuby was faster, but had bugs that would have required a near-rewrite to solve . So in November 2017, we began writing Sorbet from scratch. Six months later, in May 2018, Sorbet type checking became required in Stripe\u2019s automated test suite. After another year of internal adoption, we released Sorbet to the world in June 2019.\n\nIn all that time a lot has changed\u2014Sorbet has far more features today than we ever imagined back then. But there's been one constant driving force behind the project: building tools that make engineers working in Ruby more productive.\n\nSupercharged productivity in Ruby\n\nWhen we ask how Sorbet makes people more productive they tell us all sorts of things, but the most common theme is raw speed.\n\nSorbet gives near-instantaneous feedback while editing: for 80% of edits, it can finish reporting type errors in milliseconds, even in our multi-million line codebase. The longest error reporting wait times measure in seconds. Types aren't a replacement for tests, but few test suites are fast enough to run on every edit like Sorbet.\n\nBut there's more to it than just speed: Sorbet takes the toil out of understanding how code fits together.\n\nOn the day we rolled out the Sorbet-powered VS Code extension for Ruby, Justin Duke described the feeling better than anyone:\n\nHaving just spent the past few minutes clicking around VSCode like a kid on Christmas morning, I don't think it's an exaggeration to say that this might be the single largest improvement in my pay-server [Stripe's Ruby codebase] productivity since joining Stripe. Justin Duke, Satisfied Sorbet user\n\nIn a large codebase, Ruby can be uniquely hard to understand, even among other dynamically typed languages. What's worse is that it's hard to just, say, lint against the features that make Ruby hard to understand, because many of them are Ruby's most loved features. Here are some of the features that can make a Ruby codebase hard to unravel:\n\nRuby lacks import statements (like those in Python or JavaScript), which bind global names to file-scoped names. Instead, Ruby provides require statements, which merely run other Ruby code. This mechanism works kind of like #include statements in C and C++: a single require statement might hide implicit calls to hundreds of other require statements.\n\nBut this feature enables Rails\u2019 famous \u201cconvention over configuration\u201d approach to project layouts, which many people love about it, you don't have to import files in Rails, you can just reference the code you want to reference.\n\nRuby encourages factoring code into modules, which can then be mixed into classes or even other modules. When used well, modules can help organize code into composable, testable units.\n\nBut on the other hand, overuse of modules obscures where a method is defined behind a deep ancestor hierarchy. New Stripe engineers working in our codebase frequently struggled to find a method\u2019s definition when it came into scope from behind multiple layers of modules.\n\nRuby embraces metaprogramming, which is when methods and objects are dynamically created by code itself, instead of directly by the programmer. Concretely, this means that while some methods are written literally like def invoices; ...; end , others are defined dynamically by calling a library function like has_many(:invoices) . Metaprogramming as a way to share code is one of the biggest reasons why projects like Rails have been so successful.\n\nUnfortunately, metaprogramming is very opaque. It prevents simple regular expression searches from surfacing method definitions. Once a definition is found, the programmer still has to trace through code to know things like what arguments the method takes.\n\nWe built Sorbet to make it easy to navigate and understand a codebase without having to give up these features people love about Ruby. The key, more than just reporting type errors quickly, is to offer a powerful editor extension, which provides ever-present answers to common questions. The answer to \u201cwhere is this class defined?\u201d is a click away, not hidden behind multiple require statements. \u201cHow am I supposed to use this method?\u201d fades as a flick of the cursor reveals the method's types and documentation, replacing a lengthy crawl through a class's transitive mixins. Instant responses from Sorbet mean less time toiling and more time discovering.\n\nBuilding Sorbet in a way where it delivers type errors and IDE responses so fast comes from a set of design choices we made early on in its development. First, Sorbet is written in C++, not Ruby. To quote Nelson Elhage, one of the founding members of the team, \"Writing in C++ doesn't automatically make your program fast, and a program does not need to be written in C++ to be fast. However, using C++ well gives an experienced team a fairly unique set of tools to write high-performance software.\" C++ gives us great baseline performance and a lot of headroom for further improvement when we decide that it's critical to make a given component of Sorbet fast.\n\nAnother key element of why Sorbet is fast is that we deliberately chose a simple type inference algorithm. Specifically, Sorbet only does local type inference, so the result of type checking one method never affects the result of type checking another method. This inference algorithm is a pure function of the code inside a method and Sorbet's immutable indexes of what's defined where. Put this all together, and Sorbet's inference algorithm is embarrassingly parallel, scaling to as many cores as the machine has available while being able to use fast shared memory instead of copying large data structures.\n\nA bedrock for engineering values\n\nIn addition to the productivity boost, an unintentional benefit to come out of adopting Sorbet has been its cultural impact. In a fast-growing company, communicating and codifying cultural norms can be a full time job on its own! Sorbet lends concrete structure to some of Stripe's engineering norms.\n\nConsider the cultural norm \u201cStripe should grow more reliable over time.\u201d Despite our best efforts, production incidents happen\u2014our goal when an incident happens is to make sure the same one doesn't happen again. After years of using Sorbet, Stripe engineers reflexively reach for type annotations as a preventative tool when doing incident remediations.\n\nAs an aside, it's interesting to reflect on the classes of problems that simply don't happen at Stripe anymore (or if they do, they happen exceedingly rarely). For example: typos that used to manifest as NameError: uninitialized constant exceptions in production have been entirely replaced by static type errors. But even some more subtle problems are absent, like this one:\n\ndef update_invoice(invoice, paid) # ... end update_invoice('in_1KZ7eP2eZvKYlo2C3B98SLc9', true) # ??? ~\n\nDoes this method need to be passed a string invoice ID, or a full invoice object? Scanning the implementation for context clues can sometimes help, but type annotations replace guesswork with machine-checked assurances:\n\nsig {params(invoice: Invoice, paid: T::Boolean).void} def update_invoice(invoice, paid) # ... end # error: Expected `Invoice` but found `String` for argument `invoice` update_invoice('in_1KZ7eP2eZvKYlo2C3B98SLc9', true) ~\n\nView in the Sorbet Playground \u2192\n\nThis brings up another norm: \u201cpublic interfaces should have up-to-date documentation.\u201d For this we use a clever trick about how Sorbet's strictness levels work. Sorbet activates in files with # typed: true comments at the top of the file, but only in a \u201cbest-effort\u201d mode: type annotations aren't required and all methods behave as though their arguments were annotated with T.untyped . But by trading up to # typed: strict , Sorbet stops assuming T.untyped and instead requires signatures for all methods.\n\nTo encourage this, Stripe\u2019s continuous integration (CI) system looks through all code changes and leaves a \u201cStripe code quality score\u201d in a comment on the pull request, like this one:\n\nThe score is reported as a weighted sum of signals, where a smaller score is better. There are a lot of inputs to the score, and we hide the ones that don\u2019t change in a given pull request, but the one relevant to Sorbet in the picture above reads, \u201cNumber of non-test files which are not strictly typed (typed below strict ).\u201d This means both the author and reviewer get a heads up when new files aren't using # typed: strict , reminding them that at Stripe we really prefer all Ruby code to be type-annotated. After almost 4 years of Sorbet at Stripe, 85% of all non-test files opt into # typed: strict (and for that matter, over 95% of all files are # typed: true ).\n\nWe often say that most of Stripe's engineers haven't been hired yet. Tooling like Sorbet encodes lessons learned over the years and helps teach these lessons to new engineers in a hands-on environment. As we continue to grow, especially distributed around the globe, Sorbet will continue to serve as a concrete reference point for new and old coworkers to align on shared engineering values.\n\nRuby fits in alongside a handful of other languages in use at Stripe. Stripe is also deeply investing in building new product backends in Java, building delightful frontend experiences with TypeScript, and various pieces of infrastructure in Go. Stripe commits to staffing high quality development experiences across all of these languages, not just Ruby. Making strategic investments in tooling ensures engineers at Stripe write code that is safe and fast as we scale.\n\nAfter all this time, Sorbet is still gaining features, performance improvements, and bug fixes. We love that Sorbet lets us enhance Ruby's natural productivity while helping shape Stripe's code to be resilient and understandable as we grow. As we approach 5 years since Sorbet's conception, we can't wait to see where the next 5 years will lead!\n\nSorbet is written in C++ and compiles to WebAssembly, which means you can try it out in your browser. Below you\u2019ll find a link to the Sorbet Playground.", "label": 0}
{"title": "How It\u2019s Made: Little Language Lessons uses Gemini\u2019s multilingual capabilities to personalize language learning", "url": "https://developers.googleblog.com/en/how-its-made-little-language-lessons-to-personalize-learning/", "content": "As an engineer, I\u2019ve always been fascinated by languages\u2014both the kind we code in and the kind we speak. Learning a new programming language typically begins by building something tangible, instantly putting theory into practice. Learning a new spoken language, on the other hand, often happens in a vacuum\u2014through textbooks or exercises that feel strangely disconnected from the situations where language actually matters. As is the case with programming, language is best learned through meaningful contexts: the conversations we have, the objects around us, the moments we find ourselves in. Unlike traditional learning tools, AI can adapt to a learner\u2019s context, making it uniquely suited to help us practice languages in ways that feel more natural and personal. This led me, along with a small group of colleagues, to experiment with the Gemini API, which enables developers to access the latest generative models from Google. The result is Little Language Lessons: a collection of three bite-sized learning experiments, all powered by Google\u2019s Gemini models.\n\nExperiment 1, Tiny Lesson: Learning what you need, when you need it One of the most frustrating parts about learning a language is finding yourself in a situation where you need a specific word or phrase\u2014and it\u2019s one that you haven\u2019t learned yet. That\u2019s the idea behind Tiny Lesson. You describe a situation\u2014maybe it\u2019s \u201casking for directions\u201d or \u201cfinding a lost passport\u201d\u2014and receive useful vocabulary, phrases, and grammar tips tailored to that context.\n\nSorry, your browser doesn't support playback for this video\n\nWe were able to accomplish this using a simple prompt recipe. The prompt begins with a persona-setting preamble that looks like this:\n\nYou are a(n) {target language} tutor who is bilingual in {target language} and {source language} and an expert at crafting educational content that is custom-tailored to students' language usage goals. Markdown Copied\n\nIn this prompt and in all of the prompts to come, we took advantage of Gemini\u2019s ability to provide outputs as structured JSON, defining desired result as a list of keys in an object:\n\nFor the given usage context, provide a JSON object containing two keys: \"vocabulary\" and \"phrases\". The value of \"vocabulary\" should be an array of objects, each containing three keys: \"term\", \u201ctransliteration\u201d, and \"translation\". The value of \"term\" should be a {target language} word that is highly relevant and useful in the given context. If the language of interest is ordinarily written in the Latin script, the value of \u201ctransliteration\u201d should be an empty string. Otherwise, the value of \u201ctransliteration\u201d should be a transliteration of the term. The value of \"translation\" should be the {source language} translation of the term. ... Markdown Copied\n\nIn total, each lesson is the result of two calls to the Gemini API. One prompt handles generating all of the vocabulary and phrases, and the other deals with generating relevant grammar topics. And the end of each prompt, we interpolate the user\u2019s desired usage context as follows:\n\nINPUT (usage context): {user input} Markdown Copied\n\nExperiment 2, Slang Hang: Learning to sound less like a textbook There\u2019s a moment in the journey of learning a language when you start feeling comfortable. You can hold conversations, express yourself, and mostly get by. But then you realize, you still sound\u2026 off. Too formal. Stiff. We built Slang Hang to help address this. The idea is simple: generate a realistic conversation between native speakers and let users learn from it. You can watch the dialogue unfold, revealing one message at a time and unpacking unfamiliar terms as they appear.\n\nSorry, your browser doesn't support playback for this video\n\nThe preamble for the Slang Hang prompt looks like this:\n\nYou are a screenwriter who is bilingual in {source language} and {target language} and an expert and crafting captivating dialogues. You are also a linguist and highly attuned to the cultural nuances that shape natural speech. Markdown Copied\n\nAlthough users can only reveal messages one at a time, everything\u2014the setting, the conversation, the explanations for highlighted terms\u2014is generated from a single call to the Gemini API. We define the structure of the JSON output as follows:\n\nGenerate a short scene that contains two interlocutors speaking authentic {target language}. Give the result as a JSON object that contains two keys: \"context\" and \"dialogue\". The value of \"context\" should be a short paragraph in {SOURCE LANGUAGE} that describes the setting of the scene, what is happening, who the speakers are, and speakers' relationship to each other. The value of \"dialogue\" should be an array of objects, where each object contains information about a single conversational turn. Each object in the \"dialogue\" array should contain four keys: \"speaker\", \"gender\", \"message\", and \"notes\". ... Markdown Copied\n\nThe dialogue is generated in the user\u2019s target language, but users can also translate messages into their native language (a functionality powered by the Cloud Translation API). One of the more interesting aspects of this experiment is the element of emergent storytelling. Each scene is unique and generated on the fly\u2014it could be a street vendor chatting with a customer, two coworkers meeting on the subway, or even a pair of long-lost friends unexpectedly reuniting at an exotic pet show. That said, we found that this experiment is somewhat susceptible to accuracy errors: it occasionally misuses certain expressions and slang, or even makes them up. LLMs still aren\u2019t perfect, and for that reason it\u2019s important to cross-reference with reliable sources.\n\nExperiment 3, Word Cam: Learning from your surroundings Sometimes, you just need words for the things in front of you. It can be extremely humbling to realize just how much you don\u2019t know how to say in your target language. You know the word for \u201cwindow\u201d, but how do you say \u201cwindowsill\u201d? Or \u201cblinds\u201d? Word Cam turns your camera into an instant vocabulary helper. Snap a photo, and Gemini will detect objects, label them in your target language, and give you additional words that you can use to describe them.\n\nSorry, your browser doesn't support playback for this video\n\nThis experiment leverages Gemini\u2019s vision capabilities for object detection. We send the model an image and ask it for the bounding box coordinates of the different objects in that image:\n\nProvide insights about the objects that are present in the given image. Give the result as a JSON object that contains a single key called \"objects\". The value of \"objects\" should be an array of objects whose length is no more than the number of distinct objects present in the image. Each object in the array should contain four keys: \"name\", \"transliteration\", \"translation\", and \"coordinates\". ... The value of \"coordinates\" should be an integer array representing the coordinates of the bounding box for the object. Give the coordinates as [ymin, xmin, ymax, xmax]. Markdown Copied\n\nOnce the user selects an object, we send the cropped image to Gemini in a separate prompt and ask it to generate descriptors for that object in the user\u2019s target language:\n\nFor the object represented in the given image, provide descriptors that describe the object. Give the result as a JSON object that contains a single key called \"descriptors\". The value of \"descriptors\" should be an array of objects, where each object contains five keys: \"descriptor\", \"transliteration\", \"translation\", \"exampleSentence\", \"exampleSentenceTransliteration\", and \"exampleSentenceTranslation\". ... Markdown Copied", "label": 0}
{"title": "tech industry \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/tag/tech-industry/", "content": "This feels like a sister piece to Ed Zitron\u2019s essay Era of the Business Idiots and Mandy Brown\u2019s essay Toolmen. Fair warning, this is a 5000 word post; I\u2019ve been working on this for weeks, pulling together what I\u2019ve learned about generative AI and culture over the past two years, so I hope it is worth your time \ud83d\ude04 Bonus: it doubles as a playlist \ud83c\udfb6\n\n\u201c\u2018Real power\u2019 is achieved when a technology \u2018[leaves] mythology and [enters] banality,'\u201d Marion Fourcade and Kieran Healy quote Vincent Mosco in The Ordinal Society. We\u2019ve had the mythology stage \u2014 the world tour with grandiose prophecies of imminent AGI \u2014 but now the race to normalize generative AI* is on: tech corporations are attempting to inure people to generative AI, an expression of the Business Borg aesthetic that currently carries a negative stigma outside of tech.\n\n*(My rule of thumb: if something is described as AI, it\u2019s probably predatory and/or bullshit; if it\u2019s described as machine learning, it probably does something useful. Not always true but a helpful predictor.)\n\nIn general, people like what we recognize better than what we don\u2019t \u2014 we prefer cultural works we can categorize to the unfamiliar and undefinable \u2014 and we are facing an inescapable shock-and-awe barrage of genAI graphics across the web to inundate our synapses with uncanny synthetic renderings.\n\nCurrently, generative AI is shunned by many artists and writers, the traditional arbiters of good taste and culture, because it has been developed through the theft of their labor. But tech CEOs stand to make (even bigger) fortunes if they can convince people that genAI doesn\u2019t signify bad taste, or make it seem like an irrevocable fact of life, like spam emails and text scammers. It\u2019s being deployed upon us with the same lockstep corporate solidarity that forced us to pay fees for checked luggage on flights (younger folks, before 2008 your bag used to be included with your ticket! Stowing your carry-on wasn\u2019t a competitive sport back in the day.).", "label": 1}
{"title": "Introducing Gemma 3n: The developer guide", "url": "https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/", "content": "The first Gemma model launched early last year and has since grown into a thriving Gemmaverse of over 160 million collective downloads. This ecosystem includes our family of over a dozen specialized models for everything from safeguarding to medical applications and, most inspiringly, the countless innovations from the community. From innovators like Roboflow building enterprise computer vision to the Institute of Science Tokyo creating highly-capable Japanese Gemma variants, your work has shown us the path forward. Building on this incredible momentum, we're excited to announce the full release of Gemma 3n. While last month's preview offered a glimpse, today unlocks the full power of this mobile-first architecture. Gemma 3n is designed for the developer community that helped shape Gemma. It\u2019s supported by your favorite tools including Hugging Face Transformers, llama.cpp, Google AI Edge, Ollama, MLX, and many others, enabling you to fine-tune and deploy for your specific on-device applications with ease. This post is the developer deep dive: we'll explore some of the innovations behind Gemma 3n, share new benchmark results, and show you how to start building today.\n\nWhat\u2019s new in Gemma 3n? Gemma 3n represents a major advancement for on-device AI, bringing powerful multimodal capabilities to edge devices with performance previously only seen in last year's cloud-based frontier models.\n\nLink to Youtube Video (visible only when JS is disabled)\n\nMultimodal by design: Gemma 3n natively supports image, audio, video, and text inputs and text outputs. Optimized for on-device: Engineered with a focus on efficiency, Gemma 3n models are available in two sizes based on effective parameters: E2B and E4B. While their raw parameter count is 5B and 8B respectively, architectural innovations allow them to run with a memory footprint comparable to traditional 2B and 4B models, operating with as little as 2GB (E2B) and 3GB (E4B) of memory. Groundbreaking architecture: At its core, Gemma 3n features novel components like the MatFormer architecture for compute flexibility, Per Layer Embeddings (PLE) for memory efficiency, LAuReL and AltUp for architectural efficiency, and new audio and MobileNet-v5 based vision encoders optimized for on-device use cases. Enhanced quality: Gemma 3n delivers quality improvements across multilinguality (supporting 140 languages for text and multimodal understanding of 35 languages), math, coding, and reasoning. The E4B version achieves an LMArena score over 1300, making it the first model under 10 billion parameters to reach this benchmark.\n\nAchieving this leap in on-device performance required rethinking the model from the ground up. The foundation is Gemma 3n\u2019s unique mobile-first architecture, and it all starts with MatFormer.\n\nMatFormer: One model, many sizes At the core of Gemma 3n is the MatFormer (\ud83e\ude86Matryoshka Transformer) architecture, a novel nested transformer built for elastic inference. Think of it like Matryoshka dolls: a larger model contains smaller, fully functional versions of itself. This approach extends the concept of Matryoshka Representation Learning from just embeddings to all transformer components.\n\nDuring the MatFormer training of the 4B effective parameter (E4B) model, a 2B effective parameter (E2B) sub-model is simultaneously optimized within it, as shown in the figure above. This provides developers two powerful capabilities and use cases today: 1: Pre-extracted models: You can directly download and use either the main E4B model for the highest capabilities, or the standalone E2B sub-model which we have already extracted for you, offering up to 2x faster inference. 2: Custom sizes with Mix-n-Match: For more granular control tailored to specific hardware constraints, you can create a spectrum of custom-sized models between E2B and E4B using a method we call Mix-n-Match. This technique allows you to precisely slice the E4B model's parameters, primarily by adjusting the feed forward network hidden dimension per layer (from 8192 to 16384) and selectively skipping some layers. We are releasing the MatFormer Lab, a tool that shows how to retrieve these optimal models, which were identified by evaluating various settings on benchmarks like MMLU.\n\nMMLU scores for the pre-trained Gemma 3n checkpoints at different model sizes (using Mix-n-Match)\n\nLooking ahead, the MatFormer architecture also paves the way for elastic execution. While not part of today\u2019s launched implementations, this capability allows a single deployed E4B model to dynamically switch between E4B and E2B inference paths on the fly, enabling real-time optimization of performance and memory usage based on the current task and device load.\n\nPer-Layer Embeddings (PLE): Unlocking more memory efficiency Gemma 3n models incorporate Per-Layer Embeddings (PLE). This innovation is tailored for on-device deployment as it dramatically improves model quality without increasing the high-speed memory footprint required on your device's accelerator (GPU/TPU). While the Gemma 3n E2B and E4B models have a total parameter count of 5B and 8B respectively, PLE allows a significant portion of these parameters (the embeddings associated with each layer) to be loaded and computed efficiently on the CPU. This means only the core transformer weights (approximately 2B for E2B and 4B for E4B) need to sit in the typically more constrained accelerator memory (VRAM).\n\nWith Per-Layer Embeddings, you can use Gemma 3n E2B while only having ~2B parameters loaded in your accelerator.\n\nKV Cache sharing: Faster long-context processing Processing long inputs, such as the sequences derived from audio and video streams, is essential for many advanced on-device multimodal applications. Gemma 3n introduces KV Cache Sharing, a feature designed to significantly accelerate time-to-first-token for streaming response applications. KV Cache Sharing optimizes how the model handles the initial input processing stage (often called the \"prefill\" phase). The keys and values of the middle layer from local and global attention are directly shared with all the top layers, delivering a notable 2x improvement on prefill performance compared to Gemma 3 4B. This means the model can ingest and understand lengthy prompt sequences much faster than before.\n\nAudio understanding: Introducing speech to text and translation Gemma 3n uses an advanced audio encoder based on the Universal Speech Model (USM). The encoder generates a token for every 160ms of audio (about 6 tokens per second), which are then integrated as input to the language model, providing a granular representation of the sound context. This integrated audio capability unlocks key features for on-device development, including: Automatic Speech Recognition (ASR): Enable high-quality speech-to-text transcription directly on the device. Automatic Speech Translation (AST): Translate spoken language into text in another language. We've observed particularly strong AST results for translation between English and Spanish, French, Italian, and Portuguese, offering great potential for developers targeting applications in these languages. For tasks like speech translation, leveraging Chain-of-Thought prompting can significantly enhance results. Here\u2019s an example:", "label": 0}
{"title": "Cloud Efficiency at Netflix", "url": "https://netflixtechblog.com/cloud-efficiency-at-netflix-f2a142955f83?source=collection_home---4------9-----------------------", "content": "Cloud Efficiency at Netflix Netflix Technology Blog 5 min read \u00b7 Dec 17, 2024 -- 10 Listen Share\n\nBy J Han, Pallavi Phadnis\n\nContext\n\nAt Netflix, we use Amazon Web Services (AWS) for our cloud infrastructure needs, such as compute, storage, and networking to build and run the streaming platform that we love. Our ecosystem enables engineering teams to run applications and services at scale, utilizing a mix of open-source and proprietary solutions. In turn, our self-serve platforms allow teams to create and deploy, sometimes custom, workloads more efficiently. This diverse technological landscape generates extensive and rich data from various infrastructure entities, from which, data engineers and analysts collaborate to provide actionable insights to the engineering organization in a continuous feedback loop that ultimately enhances the business.\n\nOne crucial way in which we do this is through the democratization of highly curated data sources that sunshine usage and cost patterns across Netflix\u2019s services and teams. The Data & Insights organization partners closely with our engineering teams to share key efficiency metrics, empowering internal stakeholders to make informed business decisions.\n\nData is Key\n\nThis is where our team, Platform DSE (Data Science Engineering), comes in to enable our engineering partners to understand what resources they\u2019re using, how effectively and efficiently they use those resources, and the cost associated with their resource usage. We want our downstream consumers to make cost conscious decisions using our datasets.\n\nTo address these numerous analytic needs in a scalable way, we\u2019ve developed a two-component solution:\n\nFoundational Platform Data (FPD): This component provides a centralized data layer for all platform data, featuring a consistent data model and standardized data processing methodology. Cloud Efficiency Analytics (CEA): Built on top of FPD, this component offers an analytics data layer that provides time series efficiency metrics across various business use cases.\n\nFoundational Platform Data (FPD)\n\nWe work with different platform data providers to get inventory, ownership, and usage data for the respective platforms they own. Below is an example of how this framework applies to the Spark platform. FPD establishes data contracts with producers to ensure data quality and reliability; these contracts allow the team to leverage a common data model for ownership. The standardized data model and processing promotes scalability and consistency.\n\nCloud Efficiency Analytics (CEA Data)\n\nOnce the foundational data is ready, CEA consumes inventory, ownership, and usage data and applies the appropriate business logic to produce cost and ownership attribution at various granularities. The data model approach in CEA is to compartmentalize and be transparent; we want downstream consumers to understand why they\u2019re seeing resources show up under their name/org and how those costs are calculated. Another benefit to this approach is the ability to pivot quickly as new or changes in business logic is/are introduced.\n\n* For cost accounting purposes, we resolve assets to a single owner, or distribute costs when assets are multi-tenant. However, we do also provide usage and cost at different aggregations for different consumers.\n\nData Principles\n\nAs the source of truth for efficiency metrics, our team\u2019s tenants are to provide accurate, reliable, and accessible data, comprehensive documentation to navigate the complexity of the efficiency space, and well-defined Service Level Agreements (SLAs) to set expectations with downstream consumers during delays, outages or changes.\n\nWhile ownership and cost may seem straightforward, the complexity of the datasets is considerably high due to the breadth and scope of the business infrastructure and platform specific features. Services can have multiple owners, cost heuristics are unique to each platform, and the scale of infra data is large. As we work on expanding infrastructure coverage to all verticals of the business, we face a unique set of challenges:\n\nA Few Sizes to Fit the Majority\n\nDespite data contracts and a standardized data model on transforming upstream platform data into FPD and CEA, there is usually some degree of customization that is unique to that particular platform. As the centralized source of truth, we feel the constant tension of where to place the processing burden. Decision-making involves ongoing transparent conversations with both our data producers and consumers, frequent prioritization checks, and alignment with business needs as informed captains in this space.\n\nData Guarantees\n\nFor data correctness and trust, it\u2019s crucial that we have audits and visibility into health metrics at each layer in the pipeline in order to investigate issues and root cause anomalies quickly. Maintaining data completeness while ensuring correctness becomes challenging due to upstream latency and required transformations to have the data ready for consumption. We continuously iterate our audits and incorporate feedback to refine and meet our SLAs.\n\nAbstraction Layers\n\nWe value people over process, and it is not uncommon for engineering teams to build custom SaaS solutions for other parts of the organization. Although this fosters innovation and improves development velocity, it can create a bit of a conundrum when it comes to understanding and interpreting usage patterns and attributing cost in a way that makes sense to the business and end consumer. With clear inventory, ownership, and usage data from FPD, and precise attribution in the analytical layer, we aim to provide metrics to downstream users regardless of whether they utilize and build on top of internal platforms or on AWS resources directly.\n\nFuture Forward\n\nLooking ahead, we aim to continue onboarding platforms to FPD and CEA, striving for nearly complete cost insight coverage in the upcoming year. Longer term, we plan to extend FPD to other areas of the business such as security and availability. We aim to move towards proactive approaches via predictive analytics and ML for optimizing usage and detecting anomalies in cost.\n\nUltimately, our goal is to enable our engineering organization to make efficiency-conscious decisions when building and maintaining the myriad of services that allow us to enjoy Netflix as a streaming service.\n\nAcknowledgments\n\nThe FPD and CEA work would not have been possible without the cross functional input of many outstanding colleagues and our dedicated team building these important data assets.\n\n\u2014\n\nA bit about the authors:\n\nJHan enjoys nature, reading fantasy, and finding the best chocolate chip cookies and cinnamon rolls. She is adamant about writing the SQL select statement with leading commas.\n\nPallavi enjoys music, travel and watching astrophysics documentaries. With 15+ years working with data, she knows everything\u2019s better with a dash of analytics and a cup of coffee!", "label": 0}
{"title": "Ken", "url": "https://www.heroku.com/blog/author/ken-w-alger/feed/", "content": "As a Python developer constantly striving for smoother workflows and faster iterations, the buzz around uv has definitely caught my attention. So, let\u2019s roll up our sleeves and explore the benefits of using uv as your Python package manager, taking a look at where we\u2019ve come from and how uv stacks up. We\u2019ll even walk through setting up a project for Heroku deployment using this exciting new tool.\n\nA trip down memory lane: The evolution of Python package management\n\nTo truly appreciate what uv brings to the table, it\u2019s worth taking a quick stroll down memory lane and acknowledging the journey of Python package management.\n\nIn the early days, installing Python packages often involved manual downloads, unpacking, and running setup scripts. It was a far cry from the streamlined experience we have today. Then came Distutils, which provided a more standardized way to package and distribute Python software. While a significant step forward, it still lacked robust dependency resolution.\n\nEnter setuptools, which built upon Distutils and introduced features like dependency management and package indexing (the foundation for PyPI). For a long time, setuptools was the de facto standard, and its influence is still felt today.\n\nHowever, as the Python ecosystem grew exponentially, the limitations of the existing tools became more apparent. Dependency conflicts, slow installation times, and the complexities of managing virtual environments started to become significant pain points.\n\nThis paved the way for pip (Pip Installs Packages). Introduced in 2008, pip revolutionized Python package management. It provided a simple and powerful command-line interface for installing, upgrading, and uninstalling packages from PyPI and other indices. For over a decade, pip has been the go-to tool for most Python developers, and it has served us well.\n\nBut the increasing complexity of modern Python projects, with their often intricate web of dependencies, has exposed some of pip\u2019s performance bottlenecks. Resolving complex dependency trees can be time-consuming, and the installation process, while generally reliable, can sometimes feel sluggish.\n\nAnother challenge with the complexity of modern applications is package versioning. Lockfiles that pin project dependencies have become table stakes for package management. Many package management tools use them. Throughout the course of the evolution of package management in Python, we\u2019ve seen managers such as Poetry and Pipenv, just to name a few. However, many of these projects don\u2019t have dedicated teams. Sometimes this results in them not being able to keep up with the latest standards or the complex dependency trees of modern apps.\n\nThis is where the new generation of package management tools, like uv, comes into play, promising to address these very challenges, with a dedicated team behind them.\n\nEnter the speed demon: The benefits of using uv\n\nuv isn\u2019t just another package manager; it\u2019s built with a focus on speed and efficiency, leveraging modern programming languages and data structures to deliver a significantly faster experience. Here are some key benefits that have me, and many other Python developers, excited:\n\nBlazing Fast Installation: This is arguably uv\u2019s headline feature. Written in Rust from scratch using a thoughtful design approach uv significantly outperforms pip in resolving and installing dependencies, especially for large and complex projects. The difference can be dramatic, cutting down installation times from minutes to seconds in some cases. This speed boost translates directly into increased developer productivity and faster CI/CD pipelines. Efficient Dependency Resolution: uv employs sophisticated algorithms for dependency resolution, aiming to find compatible package versions quickly and efficiently. While pip has made improvements in this area, uv\u2019s underlying architecture allows it to handle complex dependency graphs with remarkable speed. This reduces the likelihood of dependency conflicts and streamlines the environment setup process. Drop-in Replacement for pip and venv : One of the most appealing aspects of uv is its ambition to be a seamless replacement for both pip and venv (Python\u2019s built-in virtual environment tool). It aims to handle package installation and virtual environment creation with a unified command-line interface. This simplifies project setup and management, reducing the cognitive load of juggling multiple tools. Compatibility with Existing Standards: uv adheres to existing Python packaging standards like pyproject.toml (PEP 621). This means that projects already using these standards can easily adopt uv without significant modifications. It reads and respects your existing pyproject.toml files, making the transition relatively smooth. uv is built with a strong emphasis on modern packaging practices, encouraging the adoption of pyproject.toml for declaring project dependencies and build system requirements. This aligns with the direction the Python packaging ecosystem is heading. Improved Error Messaging: While pip\u2019s error messages have improved over time, uv, being a newer tool, has the opportunity to provide more informative and user-friendly error messages, making debugging dependency issues easier. Potential for Future Enhancements: As a relatively new project with a dedicated development team, uv has the potential to introduce further optimizations and features that could significantly enhance the Python development experience. The active development and growing community support are promising signs.\n\nHow to use uv with Heroku\n\nNow, let\u2019s put some of this into practice. Imagine we\u2019re building a simple Python web application (using Flask, for instance) that we want to deploy to Heroku, and we want to leverage the speed and efficiency of uv in our development and deployment process.\n\nHere\u2019s how we can set up our project:\n\n1. Install uv\n\nThere are a variety of options to install uv, depending on your operating system. For a full list, take a look at the official Installation Guide site. I\u2019m going to install it using Homebrew:\n\n~/user$ brew install uv\n\n2. Create the project directory and initialize uv\n\n~/user$ uv init my-app ~/user$ cd my-app ~/user/my-app$ ls -a\n\nIn doing that, uv generates several project files\n\nmy-app/ \u251c\u2500\u2500 main.py \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 README.md \u2514\u2500\u2500 .python-version\n\nOur main.py looks like this:\n\ndef main(): print(\"Hello from my-app!\") if __name__ == \"__main__\": main()\n\nWe can run this with the uv run main.py command which does a few things for us. In addition to actually running main.py and generating the \u201cHello from my-app!\u201d output, uv also generates a virtual environment for the project and generates a uv.lock file which describes the project. More on that in a bit.\n\n3. Expanding the project\u2026 slightly.\n\nLet\u2019s take this project a bit further and turn it into a Flask app that we can deploy to Heroku. We\u2019ll need to specify our dependencies, Flask and Gunicorn for this example. We can do this using pyproject.toml .\n\nUsing pyproject.toml :\n\nThe uv generated pyproject.toml file looks like this:\n\n[project] name = \"my-app\" version = \"0.1.0\" description = \"Add your description here\" readme = \"README.md\" requires_python =\">=3.13\" dependencies = []\n\nTo add dependencies we use the uv add command.\n\n~/user/my-app$ uv add Flask ~/user/my-app$ uv add gunicorn\n\nThis accomplishes a couple of things:\n\nFirst, it adds those packages to the pyproject.toml file:\n\n[project] name = \"my-app\" version = \"0.1.0\" description = \"Add your description here\" readme = \"README.md\" requires_python =\">=3.13\" dependencies = [ \"Flask>=3.1.1\", \"gunicorn>=23.0.0\", ]\n\nSecond, it updates the uv.lock file for dependency management.\n\n4. Updating main.py\n\nLet\u2019s update the code in main.py to be a basic Flask web application\n\nfrom flask import Flask app = Flask(__name__) @app.route('/') def hello_world(): return \"Hello from uv on Heroku!\" if __name__ == '__main__': app.run(debug=True)\n\n5. Preparing for Heroku deployment:\n\nHeroku needs to know how to run your application. For a Flask application, we typically use Gunicorn as a production WSGI server. We\u2019ve already included it in our dependencies.\n\nWe\u2019ll need a Procfile in the root of our project to tell Heroku how to start our application:\n\nweb: gunicorn main:app\n\nHere, app refers to the name of our Flask application instance in main.py .\n\n6. Deploying to Heroku:\n\nNow, assuming you are in the project working directory, have the Heroku CLI installed, and have logged in, you can create a local git repository and Heroku application:\n\n~/user/my-app$ git init ~/user/my-app$ heroku create python-uv # Replace python-uv with your desired app name ~/user/my-app$ git add . ~/user/my-app$ git commit -m \"Initial commit with uv setup\"\n\nThe Heroku CLI will create a remote in your git repository, but you check to make sure it\u2019s there before you and push your code\n\n~/user/my-app$ git remote -v heroku https://git.heroku.com/python-uv.git (fetch) heroku https://git.heroku.com/python-uv.git (push) ~/user/my-app$ git push heroku main\n\nHeroku will detect your Python application, install the dependencies (based on .python-version , uv.lock and pyproject.toml ), and run your application using the command specified in the Procfile .\n\nThe future is bright (and fast!)\n\nWe\u2019re excited to announce that Heroku now natively supports uv for your Python development. By combining uv\u2019s performance with Heroku\u2019s fully managed runtime, teams can ship faster with greater confidence in their environment consistency. This reduces onboarding time, eliminates flaky builds, and improves pipeline performance.\n\nWhile uv is still relatively new, its potential to significantly improve the Python development workflow is undeniable. The focus on speed, efficiency, and modern packaging standards addresses some of the long-standing frustrations with existing tools.\n\nAs the project matures and gains wider adoption, we can expect even more features and tighter integration with other parts of the Python ecosystem. For now, even the significant speed improvements in local development are a compelling reason for Python developers to start exploring uv.\n\nThe journey of Python package management has been one of continuous improvement, and uv represents an exciting step forward. If you\u2019re a Python developer looking to boost your productivity and streamline your environment management, I highly recommend giving uv a try. You might just find your new favorite package manager!\n\nTry uv out on Heroku\n\nWhether you\u2019re modernizing legacy apps or spinning up new services, uv gives you the speed and flexibility you need\u2014now with first-class support on Heroku. Get started with uv on Heroku today.\n\nThe post Local Speed, Smooth Deploys: Heroku Adds Support for uv appeared first on Heroku.", "label": 0}
{"title": "Letter to Planning Commission re: upzoning in Juanita \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/12/letter-to-planning-commission-re-upzoning-in-juanita/", "content": "Two large sites in the northern part of my city are being proposed for upzoning to permit taller buildings, reduced parking requirements, as well as other changes. In general the changes seem good, but there is one proposed change that is concerning: allowing townhomes instead of apartments or condos on some large sites that ought to be developed as densely as possible (link to packet). There is a great deal of community opposition to these upzones, so I sent in a supportive note to the Planning Commission in advance of their evening meeting.\n\nIn support of higher density at Michael\u2019s and Goodwill sites\n\nTo the Kirkland Planning Commission:\n\nI am writing in favor of proposed zoning amendments to the Michael\u2019s and Goodwill sites to allow higher density, taller building heights, and reduced parking requirements. These rare anchor sites offer great potential to make the neighborhood denser and more walkable.\n\nI do not support allowing townhomes in this area \u2014 along arterial routes with transit access, with many goods and services within walking or biking distance, we ought to support as much housing density as feasible. Kirkland and the greater Seattle area are in desperate need of more housing, so I urge you to use the code to require higher densities in the BC1 zone. If we do not push for higher density where it makes sense \u2014 and it makes a great deal of sense here \u2014 then the overall amount of additional housing Kirkland can accommodate will be much lower than the need. Please require a minimum density for ADUs in this area as in Alternatives 1 or 2 from Appendix 5.", "label": 1}
{"title": "How we brought multimedia search to Dropbox Dash", "url": "https://dropbox.tech/infrastructure/multimedia-search-dropbox-dash-evolution", "content": "Knowledge workers routinely lose valuable time trying to find that thing\u2014the right images, videos, documents, or audio files\u2014across their dozens of apps and essential work tools. When we started building Dropbox Dash, our universal search and knowledge management product, we knew it had to do more than just speed up search. It also needed to scale beyond text. Because often, the challenge isn\u2019t just finding a file\u2014it\u2019s finding what\u2019s inside that file. And that gets tricky when things aren\u2019t labeled clearly, your team\u2019s folder structure breaks down, or you just can\u2019t remember where you saved what you need. Searching for multimedia content poses unique challenges. Images, for example, often come with cryptic names like IMG_6798 by default, and teams can quickly accumulate thousands of these unlabeled assets. Unlike documents, which usually contain metadata or readable content to help with discovery, media files frequently lack that context and require manual review. On top of that, they demand heavier compute resources and smarter ranking systems to deliver relevant results at speed. Supporting fast, accurate media search in Dash wasn\u2019t a matter of layering features on top\u2014it required fundamental changes across our infrastructure. We had to rethink how we indexed and ranked non-text files, how we rendered visual previews, and how we hydrated and surfaced metadata. We also had to reevaluate traditional document-search assumptions about relevance, latency, and even UI presentation. Our multimedia retrieval features were built to solve these exact problems, allowing users to find images, video, and audio just as easily as they find documents. What follows is a behind-the-scenes look at the engineering that made this possible: what we built, what we learned, and how we delivered a system that makes media as searchable as text.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images\u2014plus generative AI capabilities across even more connected apps. See what's new \u2192\n\nChallenges in supporting multimedia search\n\nSupporting search for multimedia\u2014images, video, and audio\u2014introduces a distinct set of technical hurdles. These files require significantly more processing power, have fewer textual cues for ranking, and often lack meaningful metadata. Delivering fast, relevant results means handling large file sizes efficiently, identifying new relevance signals, and optimizing how results are rendered for user review. That\u2019s why our universal search solution had to support seamless browsing, filtering, and previewing of media\u2014right inside Dash. Scaling search for this content meant facing higher storage and compute costs, tighter latency requirements, and adapting systems originally built for text-based retrieval. To understand what makes media search tricky, let\u2019s break down some key considerations. Storage cost\n\nMedia files are significantly larger than typical documents. On average, image files are about 3X larger, and video files are roughly 13X larger than non-media files in our system. These size differences directly increase storage demands and costs. Compute cost\n\nMultimedia files, such as images and videos, require more intensive processing to extract features both due to their larger size and the complexity of the features. Unlike text documents, we also generate previews of different resolutions for the images and videos, thereby significantly increasing the compute demands in our system. Relevance\n\nDash operates a multi-phase retrieval and ranking scheme, which was previously trained and optimized for textual content. Retrieving and ranking multimedia content requires having indexed any new multimedia-specific signals, formulating a query plan that leverages these signals, and handling any corner cases to avoid poorly ranked results. Responsiveness\n\nServing multimedia content introduces new latency challenges that are not present with text-based documents. We need previews for the multimedia search results to be meaningful, and we need them in multiple resolutions, including high-res formats, for a rich product experience. The larger resolutions add to the storage and compute costs. Only a small fraction of the indexed files are actually viewed during search interactions. As a result, precomputing previews at multiple resolutions for all media files would be wasteful and unnecessary. To balance responsiveness with resource efficiency, we generate previews on demand during the read path rather than upfront. This minimizes upfront compute and storage costs but introduces new latency concerns during user interactions, since we want to generate previews quickly to have a snappy user experience. With these challenges in mind, we designed a solution that integrates scalable infrastructure, smarter content understanding, and a preview system optimized for speed and accuracy.\n\nBuilding a multimedia search solution\n\nTo deliver a responsive and scalable experience in Dash, we had to rebuild key parts of our infrastructure to support search that\u2019s as smart and seamless for photos and videos as it is for documents. This work spans multiple layers of the stack, and it wasn\u2019t pulled off successfully without trial and error. We began by indexing lightweight metadata\u2014pulled from media blobs (the raw files like images, videos, or audio)\u2014to keep compute costs low. We extended our relevance models to handle location-aware queries and fuzzy file naming, and we optimized our preview generation pipeline to balance latency with cost. Along the way, we made frontend and backend updates to ensure media renders quickly and consistently across devices. The result is a robust multimedia search experience powered by smart metadata, just-in-time previews, and a UI that helps users find the right visual asset fast. Let\u2019s get into how we tackled it. Indexing media files by metadata To keep the compute costs low, we begin by indexing media files using available metadata, which is significantly cheaper to process than analyzing the full contents of media like images or videos. For example, we extract features such as file path, title, and EXIF. These metadata provide a lightweight foundation that enables basic search functionality with minimal processing overhead. As our capabilities evolve, we plan to build on this metadata-first approach by selectively incorporating deeper content analysis\u2014such as semantic embedding and/or OCR\u2014striking a balance between accuracy and cost.\n\nTo generate metadata features at scale, we leveraged Riviera, our internal compute framework that already powers Dropbox Search. Riviera processes tens of petabytes of data daily and includes mature business logic for metadata extraction. By reusing it, we benefited from proven scalability and consistency with existing Dropbox search infrastructure. Backfilling the index\n\nPrior to this initiative, we avoided downloading or storing raw media blobs in order to reduce storage and compute costs. As a result, our existing search index lacked the necessary features to support rich, media-specific search experiences. To address this gap, we added support for ingesting multimedia blob content to compute the required features. We retain the raw content for preview generation and to compute future features. Where possible, we download previews provided by third-party applications. These externally sourced previews are especially useful for design files like Canva, where we\u2019re unable to generate our own. Using them also helps us reduce compute costs. Storage optimizations\n\nDash optimizes the file sizes and MIME types ingested to balance storage cost and file availability. We currently ingest about 97% of media files and are working to address the remaining gaps with smarter lifecycle management techniques. Retrieving media files by metadata When a user searches for media, we configure the query to match their input against the metadata features extracted during indexing. This includes fields like filenames, file paths, and location data. To enhance location-based search, we also apply custom query logic for interpreting geographic references. Internally, we index a GPS location as a chain of IDs corresponding to the geographical hierarchy. For instance, we can look up the GPS coordinates of a photo to be from San Francisco in a process known as \u201creverse geocoding.\u201d Then, we would build a chain of IDs corresponding to San Francisco, California, and the United States, respectively, and place these IDs in the index for the photo. This allows us to retrieve the photo when the user wants to search for a photo taken in San Francisco, California or the entire United States, respectively.\n\nAt query time, we identify substrings of the query that may potentially be geographical locations, and then we determine whether they map to a valid location ID. In practice, because the number of known geographical locations has a manageably small cardinality, we retrieve the entire mapping upon the service startup and cache it. Lastly, in the course of building multimedia search, we realized that many multimedia files are named in particular ways. Many of them are files in the filesystem, e.g. PhotoShoot-Revised1234.jpg. To support better matching, we added logic to tokenize camel case, hyphenated strings, and numeric suffixes during both indexing and retrieval. Preview and metadata hydration at retrieval time Our system ingests data at a rate that\u2019s approximately three orders of magnitude higher than the query rate. This disparity makes it prohibitively expensive to generate and store previews for all multimedia files during ingestion, both in terms of compute and storage. To address this, we adopted a just-in-time approach, where previews are generated at query time. This strategy significantly reduces upfront costs while still supporting a responsive user experience. As part of our storage optimization efforts, we considered precomputing previews during ingestion to enable deletion of the raw content afterward. However, we ultimately decided against this approach for two key reasons. First, managing the lifecycle of these additional preview artifacts would introduce significant code complexity. Second, retaining the raw content ensures future flexibility, allowing us to compute new features later without having to re-ingest the original files.\n\nTo power the just-in-time approach, we rely on an internal previews service built on top of Riviera, a framework originally developed for Dropbox Search. The previews service is designed to be fast, scalable, and efficient. It incorporates intelligent caching strategies, storing previews for up to 30 days. This allows us to serve previews quickly when needed without repeatedly generating them for every request. During a search, we generate preview URLs for the relevant results, which are then passed to the frontend. The frontend fetches these URLs and displays the corresponding previews directly to the user. By reusing both the Riviera framework and the previews service, we also create the opportunity to reuse frontend components across both Dropbox and Dash. This ensures a consistent product experience across both platforms. To improve latencies, we create the preview URLs in parallel with other search operations such as ranking the results, performing permission checks, and fetching additional metadata needed to render complete search results. By handling these tasks in parallel, we minimize the overall response time and ensure a responsive user experience.\n\nSometimes, a user may want to enlarge a preview and view additional metadata, such as camera information. However, this is a less common operation, and sending all the extra metadata with every search result would be inefficient. When users request more detail\u2014such as camera metadata or timestamp\u2014we fetch it on-demand via a separate endpoint. This keeps the initial search response lean while still supporting deeper inspection when needed. User experience Searching through images and videos is a different experience than searching documents, especially since media files often have names like \u201cIMG_1234\u201d that don\u2019t tell you much. That\u2019s why fast, visual previews are essential\u2014they help users quickly decide which file is relevant without needing to open each one. We\u2019ve designed our preview system to load quickly and adapt to different shapes and sizes of media, whether an image is tall, wide, or an unusual shape. The layout avoids awkward cropping and keeps things easy to browse. When a user wants a closer look, they can open a full-size preview that also shows helpful EXIF details like when the photo was taken, what kind of camera was used, and where it was captured. Everything is built to feel smooth and fast, whether you\u2019re using Dash on a phone or computer. The interface stays out of the way and puts the focus on the content, making it easy to browse quickly or dive into a specific file when needed.\n\nLessons learned and future direction", "label": 0}
{"title": "February 2024", "url": "https://lifeofpablo.com/blog/published:2024-02", "content": "I enjoy making friends and making connections with people of all backgrounds and cultures. Growing up, I'd make friends with the international exchange students. They show up for a year or so and then leave. Many I've stayed in touch with and have gone to visit them in their home country. I would consider them close friends. We've kept this friendship even if they are across the world.\n\nAs a person, I would say I\u2019m a very affectionate person. I like to form bonds and form some level of intimacy. Intimacy can be physical, emotional, intellectual, spiritual or experiential. The idea of intimacy does not need to be sexual. I\u2019m very affectionate to those with whom I have a close relationship, such as my best friends. I would also consider them very affectionate as well. I guess we can say we are in touch with our emotions and be expressive towards each other. We\u2019ve shared so many moments together. That\u2019s why we\u2019re friends. As we\u2019ve become older and started our careers, our lives, our relationships, etc, we\u2019ve all relocated and don\u2019t see each other as often as we used to. We all find ourselves living in various parts of the United States. It\u2019s a sad reality growing up. Yet, here we are. It doesn\u2019t seem that our bonds have weakened.\n\nHaving these human relationships or interactions is the main piece of keeping our relationships so strong. Many of my relationships with people I have rely on digital relationships as well. It's how many of our relationships keep strong.\n\nMany of us desire a form of intimacy and emotional bond from those important to us. Oftentimes the only way to get these is through a virtual medium. We want to foster deep emotional connections with the absence of physical proximity. We want to share those important moments with a phone call or a video chat. We want to support each other in hard times. We want to replicate those movie nights when we were all living together. Virtual relationships allow us to provide emotional support from afar with our personal struggles, external stressors, or societal issues. This strengthens the emotional bond between individuals. Trust becomes a cornerstone that supports the vulnerability inherent in forming emotional bonds.\n\nIt's possible to have meaningful virtual relationships as we adapt with the changing times and remember the human in relationships.", "label": 1}
