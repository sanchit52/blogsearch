{"title": "My Apple Music Replay'23", "url": "https://lifeofpablo.com/blog/my-apple-music-replay-23", "content": "In response to James G on Spotify Wrapped\n\nEarlier this year I wrote a blog post what I'm listening on repeat. I'm that person who doesn't use Spotify. I left Spotify a while ago. I instead use Apple Music does something similar to Spotify Wrapped. I honestly didn't know that Apple Music had a feature to share. Now it's time for the results my Apple Music Replay'23 (the Apple equivalent to Spotify Wrappted) to shine. It was interesting because I had to launch my browser to get the Replay'23 instead of it being in app. It was easy to share on social media as images. My listens ranged from mid/late 2000s to more recent. I really liked listening to The Submarines. I'm a sucker for the song Brightest Hour.\n\nThis coming I would like to discover smaller indie bands and attend more concerts to get out of my cycle of repeating and getting too comfortable with the things I listen often.\n\nI really like Jo's Post on what she listened to on Spotify. I love what you listened to this year and how you organized your post!\n\nI read Henrique's Post on what he listened to on Spotify. He made me remember that I need to give my vinyls a little more love (or a spin) and decrease my listening minutes on my phone.\n\nI know I have an interesting taste in Music but I like listening to alternative.\n\nWell here you go! Enjoy!", "label": 1}
{"title": "Title Launch Observability at Netflix Scale", "url": "https://netflixtechblog.com/title-launch-observability-at-netflix-scale-19ea916be1ed?source=collection_home---4------4-----------------------", "content": "Title Launch Observability at Netflix Scale\n\nPart 2: Navigating Ambiguity Netflix Technology Blog 6 min read \u00b7 Jan 7, 2025 -- 8 Listen Share\n\nBy: Varun Khaitan\n\nWith special thanks to my stunning colleagues: Mallika Rao, Esmir Mesic, Hugo Marques\n\nBuilding on the foundation laid in Part 1, where we explored the \u201cwhat\u201d behind the challenges of title launch observability at Netflix, this post shifts focus to the \u201chow.\u201d How do we ensure every title launches seamlessly and remains discoverable by the right audience?\n\nIn the dynamic world of technology, it\u2019s tempting to leap into problem-solving mode. But the key to lasting success lies in taking a step back \u2014 understanding the broader context before diving into solutions. This thoughtful approach doesn\u2019t just address immediate hurdles; it builds the resilience and scalability needed for the future. Let\u2019s explore how this mindset drives results.\n\nUnderstanding the Bigger Picture\n\nLet\u2019s take a comprehensive look at all the elements involved and how they interconnect. We should aim to address questions such as: What is vital to the business? Which aspects of the problem are essential to resolve? And how did we arrive at this point?\n\nThis process involves:\n\nIdentifying Stakeholders: Determine who is impacted by the issue and whose input is crucial for a successful resolution. In this case, the main stakeholders are:\n\n\n\n- Title Launch Operators\n\nRole: Responsible for setting up the title and its metadata into our systems.\n\nChallenge: Don\u2019t understand the cascading effects of their setup on these perceived black box personalization systems\n\n\n\n- Personalization System Engineers\n\nRole: Develop and operate the personalization systems.\n\nChallenge: End up spending unplanned cycles on title launch and personalization investigations.\n\n\n\n- Product Managers\n\nRole: Ensure we put forward the best experience for our members.\n\nChallenge: Members may not connect with the most relevant title.\n\n\n\n- Creative Representatives\n\nRole: Mediator between the content creators and Netflix.\n\nChallenge: Build trust in the Netflix brand with content creators. Mapping the Current Landscape: By charting the existing landscape, we can pinpoint areas ripe for improvement and steer clear of redundant efforts. Beyond the scattered solutions and makeshift scripts, it became evident that there was no established solution for title launch observability. This suggests that this area has been neglected for quite some time and likely requires significant investment. This situation presents both challenges and opportunities; while it may be more difficult to make initial progress, there are plenty of easy wins to capitalize on. Clarifying the Core Problem: By clearly defining the problem, we can ensure that our solutions address the root cause rather than just the symptoms. While there were many issues and problems we could address, the core problem here was to make sure every title was treated fairly by our personalization stack. If we can ensure fair treatment with confidence and bring that visibility to all our stakeholders, we can address all their challenges. Assessing Business Priorities: Understanding what is most important to the organization helps prioritize actions and resources effectively. In this context, we\u2019re focused on developing systems that ensure successful title launches, build trust between content creators and our brand, and reduce engineering operational overhead. While this is a critical business need and we definitely should solve it, it\u2019s essential to evaluate how it stacks up against other priorities across different areas of the organization.\n\nDefining Title Health\n\nNavigating such an ambiguous space required a shared understanding to foster clarity and collaboration. To address this, we introduced the term \u201cTitle Health,\u201d a concept designed to help us communicate effectively and capture the nuances of maintaining each title\u2019s visibility and performance. This shared language became a foundation for discussing the complexities of this domain.\n\n\u201cTitle Health\u201d encompasses various metrics and indicators that reflect how well a title is performing, in terms of discoverability and member engagement. The three main questions we try to answer are:\n\nIs this title visible at all to any member? Is this title visible to an appropriate audience size? Is this title reaching all the appropriate audiences?\n\nDefining Title Health provided a framework to monitor and optimize each title\u2019s lifecycle. It allowed us to align with partners on principles and requirements before building solutions, ensuring every title reaches its intended audience seamlessly. This common language not only introduced the problem space effectively but also accelerated collaboration and decision-making across teams.\n\nCategories of issues\n\nTo build a robust plan for title launch observability, we first needed to categorize the types of issues we encounter. This structured approach allows us to address all aspects of title health comprehensively.\n\nCurrently, these issues are grouped into three primary categories:\n\n1. Title Setup\n\nA title\u2019s setup includes essential attributes like metadata (e.g., launch dates, audio and subtitle languages, editorial tags) and assets (e.g., artwork, trailers, supplemental messages). These elements are critical for a title\u2019s eligibility in a row, accurate personalization, and an engaging presentation. Since these attributes feed directly into algorithms, any delays or inaccuracies can ripple through the system.\n\nThe observability system must ensure that title setup is complete and validated in a timely manner, identify potential bottlenecks and ensure a smooth launch process.\n\n2. Personalization Systems\n\nTitles are eligible to be recommended across multiple canvases on product \u2014 HomePage, Coming Soon, Messaging, Search and more. Personalization systems handle the recommendation and serving of titles on these canvases, leveraging a vast ecosystem of microservices, caches, databases, code, and configurations to build these product canvases.\n\nWe aim to validate that titles are eligible in all appropriate product canvases across the end to end personalization stack during all of the title\u2019s launch phases.\n\n3. Algorithms\n\nComplex algorithms drive each personalized product experience, recommending titles tailored to individual members. Observability here means validating the accuracy of algorithmic recommendations for all titles.\n\nAlgorithmic performance can be affected by various factors, such as model shortcomings, incomplete or inaccurate input signals, feature anomalies, or interactions between titles. Identifying and addressing these issues ensures that recommendations remain precise and effective.\n\nBy categorizing issues into these areas, we can systematically address challenges and deliver a reliable, personalized experience for every title on our platform.\n\nIssue Analysis\n\nLet\u2019s also learn more about how often we see each of these types of issues and how much effort it takes to fix them once they come up.\n\nFrom the above chart, we see that setup issues are the most common but they are also easy to fix since it\u2019s relatively straightforward to go back and rectify a title\u2019s metadata. System issues, which mostly manifest as bugs in our personalization microservices are not uncommon, and they take moderate effort to address. Algorithm issues, while rare, are really difficult to address since these often involve interpreting and retraining complex machine learning models.\n\nEvaluating Our Options\n\nNow that we understand more deeply about the problems we want to address and how we should go about prioritizing our resources. Lets go back to the two options we discussed in Part 1, and make an informed decision.\n\nUltimately, we realized this space demands the full spectrum of features we\u2019ve discussed. But the question remained: Where do we start?\n\nAfter careful consideration, we chose to focus on proactive issue detection first. Catching problems before launch offered the greatest potential for business impact, ensuring smoother launches, better member experiences, and stronger system reliability.\n\nThis decision wasn\u2019t just about solving today\u2019s challenges \u2014 it was about laying the foundation for a scalable, robust system that can grow with the complexities of our ever-evolving platform.\n\nUp next\n\nIn the next iteration we will talk about how to design an observability endpoint that works for all personalization systems. What are the main things to keep in mind while creating a microservice API endpoint? How do we ensure standardization? What is the architecture of the systems involved?\n\nKeep an eye out for our next binge-worthy episode!", "label": 0}
{"title": "Building a more accessible GitHub CLI", "url": "https://github.blog/engineering/user-experience/building-a-more-accessible-github-cli/", "content": "At GitHub, we\u2019re committed to making our tools truly accessible for every developer, regardless of ability or toolset. The command line interface (CLI) is a vital part of the developer experience, and the GitHub CLI is our product that brings the power of GitHub to your terminal.\n\nWhen it comes to accessibility, the terminal is fundamentally different from a web browser or a graphical user interface, with a lineage that predates the web itself. While standards like the Web Content Accessibility Guidelines (WCAG) provide a clear path for making web and graphical applications accessible, there is no equivalent, comprehensive standard for the terminal and CLIs. The W3C offers some high-level guidance for non-web software, but it stops short of prescribing concrete techniques, leaving much open to interpretation and innovation.\n\nThis gap has challenged us to think creatively and purposefully about what accessibility should look like in the terminal. Our recent Public Preview is focused on addressing the needs of three key groups: users who rely on screen readers, users who need high contrast between background and text, and users who require customizable color options. Our work aims to make the GitHub CLI more inclusive for all, regardless of how you interact with your terminal. Run gh a11y in the latest version of the GitHub CLI to enable these features, or read on to learn about our path to designing and implementing them.\n\nUnderstanding the terminal landscape\n\nText-based and command-line applications differ fundamentally from graphical or web applications. On a web page, assistive technologies like screen readers make use of the document object model (DOM) to infer structure and context of the page. Web pages can be designed such that the DOM\u2019s structure is friendly to these technologies without impacting the visual design of the page. By contrast, CLI\u2019s primary output is plain text, without hidden markup. A terminal emulator acts as the \u201cuser agent\u201d for text apps, rendering characters as directed by the server application. Assistive technologies access this matrix of characters, analyze its layout, and try to infer structure. As the WCAG2ICT guidance notes, accessibility in this space means ensuring that all text output is available to assistive technologies, and that structural information is conveyed in a way that\u2019s programmatically determinable\u2014even if no explicit markup is present.\n\nIn our quest to improve the GitHub CLI\u2019s usability for blind, low-vision, and colorblind users, we found ourselves navigating a landscape with lots of guidance, but few concrete techniques for implementing accessible experiences. We studied how assistive technology interacts with terminals: how screen readers review output, how color and contrast can be customized, and how structural cues can be inferred from plain text. Our recent Public Preview contains explorations into various use cases in these spaces.\n\nRethinking prompts and progress for screen readers\n\nOne of the GitHub CLI\u2019s strengths as a command-line application is its rich prompting experience, which gives our users an interactive interface to enter command options. However, this rich interactive experience poses a hurdle for speech synthesis screen readers: Non-alphanumeric visual cues and uses of constant screen redraws for visual or other effects can be tricky to correctly interpret as speech.\n\nA demo video with sound of screen reader reading legacy prompter.\n\nTo reduce confusion and make it easier for blind and low vision users to confidently answer questions and navigate choices, we\u2019re introducing a prompting experience that allows speech synthesis screen readers to accurately convey prompts to users. Our new prompter is built using Charm\u2019s open source charmbracelet/huh prompting library.\n\n\n\nA demo of a screenreader correctly reading a prompt.\n\nAnother use case where the terminal is redrawn for visual effect is when showing progress bars. Our existing implementation uses a \u201cspinner\u201d made by redrawing the screen to display different braille characters (yes, we appreciate the irony) to give the user the indication that their command is executing. Speech synthesis screen readers do not handle this well:\n\n\n\nA demo of a screenreader and an old spinner.\n\nThis has been replaced with a static text progress indicator (with a relevant message to the action being taken where possible, falling back to a general \u201cWorking\u2026\u201d message). We\u2019re working on identifying other areas we can further improve the contextual text.\n\nA demo video of the new progress indicator experience.\n\nColor, contrast, and customization\n\nColor is more than decoration in the terminal: It\u2019s a vital tool for highlighting information, signaling errors, and guiding workflows. But color can also be a barrier\u2014if contrast between the color of the terminal background and the text displayed on it is too low, some users will have difficulty discerning the displayed information. Unlike in a web browser, a terminal\u2019s background color is not set by the application. That task is handled by the user\u2019s terminal emulator. In order to maintain contrast, it is important that a command line application takes into account this variable. Our legacy color palette used for rendering Markdown did not take the terminal\u2019s background color into account, leading to low contrast in some cases.\n\nThe colors themselves also matter. Different terminal environments have varied color capabilities (some support 4-bit, some 8-bit, some 24-bit, etc). No matter the capability, terminals enable users to customize their color preferences, choosing how different hues are displayed. However, most terminals only support changing a limited subset of colors: namely, the sixteen colors in the ANSI 4-bit color table. The GitHub CLI has made extensive efforts to align our color palettes to 4-bit colors so our users can completely customize their experience using their terminal preferences. We built on top of the accessibility foundations pioneered by Primer when deciding which 4-bit colors to use.\n\nBuilding for the CLI community\n\nOur improvements aim to support a wide range of developer needs, from blind users who need screen readers, to low vision users who need high contrast, to colorblind users who require customizable color options. But this Public Preview does not mark the end of our team\u2019s commitment to enabling all developers to use the GitHub CLI. We intend to make it easier for our extension authors to implement the same accessibility improvements that we\u2019ve made to the core CLI. This will allow users to have a cohesive experience across all GitHub CLI commands, official or community-maintained, and so that more workflows can be made accessible by default. We\u2019re also looking into experiences to customize the formatting of tables output by commands to be more easily read/interpreted by screen readers. We\u2019re excited to continue our accessibility journey.\n\n\n\nWe couldn\u2019t have come this far without collaboration with our friends at Charm and our colleagues on the GitHub Accessibility team.\n\nA call for feedback\n\nWe invite you to help us in our goal to make the GitHub CLI an experience for all developers:\n\nTry it out : Update the GitHub CLI to v2.72.0 and run gh a11y in your terminal to learn more about enabling these new accessible features.\n\n: Update the GitHub CLI to v2.72.0 and run in your terminal to learn more about enabling these new accessible features. Share your experience : Join our GitHub CLI accessibility discussion to provide feedback or suggestions.\n\n: Join our GitHub CLI accessibility discussion to provide feedback or suggestions. Connect with us: If you have a lived experience relevant to our accessibility personas, reach out to the accessibility team or get involved in our discussion panel.\n\nLooking forward\n\nAdapting accessibility standards for the command line is a challenge\u2014and an opportunity. We\u2019re committed to sharing our approach, learning from the community, and helping set a new standard for accessible CLI tools.\n\nThank you for building a more accessible GitHub with us.\n\nWant to help us make GitHub the home for all developers? Learn more about GitHub\u2019s accessibility efforts.", "label": 0}
{"title": "Read The Arts and Crafts Movement \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/24/read-the-arts-and-crafts-movement/", "content": "Read The Arts and Crafts Movement: A Study of Its Sources, Ideals and Influence on Design Theory by Gillian Naylor\n\nLots of original quotes are interspersed, from a variety of sources. The text itself sometimes got lost in facts, dates, and sequences; it was best where it narrativized and provided high level analysis of trends. Some of the quotes are \ud83d\udd25\n\n\u201cThe movement\u2026 represents in some sense a revolt against the hard mechanical conventional life and its insensibility to beauty (quite another thing to ornament). It is a protest against that so-called industrial progress which provides shoddy wares, the cheapness of which is paid for by the lives of their producers and the degradation of their users.\u201d\n\n\u2014 Walter Crane, The Revival of Design and Handicraft (orig. pg. 12)\n\nI found the layout of the text frustratingly uncomfortable to read. I read through page 145 (195 pages of content before endnotes).\n\nGraphics\n\nInteresting and wide-ranging collection of sample works included (chiefly black and white unfortunately) \u2014 though I was frustrated at least twice when a specific piece would be described in the text but not pictured.\n\nNotes and Quotes\n\nMartin Wiener \u2013 English Culture and the Decline of the Industrial Spirit\n\nDesign theory and categorization of ornament = related to industrialization of design\n\n\u201cThe Arts and Crafts movement was inspired by a crisis of conscience. Its motivations were social and moral, and its aesthetic values derived from the conviction that society produces the art and architecture it deserves.\u201d (from 1989 preface by Gillian Naylor)\n\n\u201cAverting mankind\u2019s enslavement to the machine by saving the mass product and the home from mechanical anarchy and by restoring them to purpose, sense and life.\u201d\n\n\u2014 Walter Gropius, The Scope of Total Architecture\n\ncontemporary critique from Thorstein Veblen 1899: idolizing the handmade \u2013> conspicuous consumption \u2014 \u201cpropaganda of crudity\u201d describing the \u201cexaltation of the defective\u201d aka imperfections in handwork (aestheticization!!!)\n\nthey don\u2019t write insults like this anymore lol: \u201cdisencumber yourselves of the lymphatic ideology of your deplorable Ruskin\u201d \u2013Marinetti, 1912\n\n1835 committee to figure out how to give craftspeople a design sense and *taste* since consumers preferred imported aesthetics\n\nRuskin thought a craft and its society were inextricable\n\n\u201cFor it is not the material, but the absence of human labor, which makes the thing worthless, and a piece of terracotta, or plaster of paris, which has been wrought by the human hand, is worth all the stone and Carrara cut by machinery. It is, indeed, possible and even usual, for men to sink into machines themselves, so that even handwork has all the character of mechanization.\u201d\n\n\u2014 Ruskin, \u2018The Lamp of Truth\u2019 from The Seven Lamps of Architecture\n\nThe Stones of Venice = key book to the movement \u2014 especially the essay \u201cNature of Gothic\u201d (here as printed by William Morris)\n\n\u201cIt is not that men are ill-fed, but that they have no pleasure in the work by which they make their bread, and therefore look to wealth as the only means of pleasure.\u201d\n\n\u2014 Ruskin, \u2018Nature of Gothic\u2019 from The Stones of Venice\n\nRuskin into destigmatizing manual labor\n\nsociety\u2019s wealth measured in human happiness and its works of art\n\nthis is grouping Burne-Jones in with Arts and Crafts rather than Pre-Raphaelite\n\n\u201cIt was just a commonplace thing handled imaginatively, and it gave me as much pleasure as anything in the exhibition. It made me feel that it takes a big man to do a simple thing.\u201d\n\n\u2014 architect John Sedding, about a piece of furniture designed by Ford Madox Brown\n\n\u201c\u2018Art\u2019 to them meant individuality and the search for \u2018truth\u2019, whether in painting, architecture or applied design \u2014 and truth, they felt, could be found both in the study of nature, and in the recreation of the spirit rather than the letter of mediaevalism.\u201d\n\nduality of \u201cstraightforward, honest craftsmanship\u201d and \u201cmid-nineteenth-century ornamental conventions\u201d\n\n\u201ccardinal principle\u201d = know your materials and learn the craft directly\n\nMorris thought pattern design should hold meaning:\n\n\u201cdo not introduce any lines or objects which cannot be explained by the structure of the pattern; it is just this logical sequence of form, this growth which looks as if, under the circumstances, it could not have been otherwise, which prevents the eye wearying of the repetition of the pattern.\u201d\n\nconflict between Morris\u2019 love of craftsmanship and the expense of producing quality goods blocking most people from accessing it led to him becoming a Socialist\n\n\u201c[I]t is the allowing of machines to be our masters, and not our servants, that so injures the beauty of life nowadays.\u201d \u2014 William Morris\n\nArts and Crafts accepted both \u201csimple and luxurious\u201d \u2014 \u201csprang from the ideal of the craftsman as artist, and from the belief in individualism and individual commitment\u201d\n\nSee also: Read Liberty: British Colour Pattern\n\nRead In Harmony with Nature", "label": 1}
{"title": "Weeknotes: June 14-20, 2025 \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/20/weeknotes-june-14-20-2025/", "content": "Win of the week: cooked up a giant pot of chickpeas (added a kinda-tadka of fresh rosemary sizzling in oil at the end) for a mildly awkward neighborhood potluck (where the only other handmade food was peanut butter celery sticks \ud83d\ude02 apparently my idea of potluck food is outdated)\n\nLooking forward to: finishing This Will Be Fun by E.B. Asher! I\u2019ve been in a book slump and have been enjoying this \u2014 it\u2019s gleefully goofy to balance out the characters\u2019 intense angst and pain\n\nStuff I did:\n\n13.5 hours consulting \u2014 sent over another deliverable \ud83e\uddbe\n\n0 hours writing oops\n\ntook Thursday off for Juneteenth\n\nwatched Yancey Strickler interview Nadia Asparouhova about her new book Antimemetics\u2026 may not be quite what I was expecting but I\u2019ll withhold judgment till I get the book\n\ngot an email about a new program inspired by my high school travels with my track coach and discovered I was mentioned in their promo materials?? \ud83e\udd14 still deciding how to reply\u2026\n\none virtual appointment + dentist appointment \u2014 I\u2019ve been grinding my teeth in my sleep (thanks politics) so they 3D scanned my teeth to make a mouth guard\n\nhung out with my sister\n\nbonus walk with friend but skipped regular walk for bad weather\n\nbaked my go-to coffee cake \u2014 used a smaller pan which doubled the bake time\n\nupdated my Signal settings per Cory\u2019s suggestions\n\nan hour of gardening to clear paths \u2013 again \u2013 and get rid of a bunch of plants with some kinda mildew on the leaves\u2026 I\u2019ve been trying to nurture my mildewy ninebark back to health but finally hacked almost all of it waaay back\n\n\ud83e\udd26\u200d\u2640\ufe0f I somehow failed to observe that my pine tree had candles last month, which I bought a taller ladder specifically to prune\u2026 the wood on the new growth is still green so I went for it! \ud83e\udd1e\ud83d\ude2c Husband says if the tree gets too big he\u2019s cutting it down, it\u2019s living on borrowed time anyway, so it\u2019s fine if I screw it up \u2014 and it is very possible I done fucked up, we\u2019ll find out next year I guess \ud83e\udd37\u200d\u2640\ufe0f\n\nDinners:\n\nfake chicken burgers (Quorn) + tots + ginger beer + dill pickle\n\ncauliflower shawarma + fries with toum (apparently we last ordered this exactly one year ago \ud83d\ude02)\n\nfish wraps with mango salsa and air fryer potatoes + ginger beer \u2014 cooked the tilapia from frozen, 20 minutes at 400 worked great\n\nfake chicken burgers (Impossible) with BBQ sauce and pineapple + curly fries\n\nbaked feta pasta \u2014 tried goat\u2019s milk feta, not a great batch\n\nwent out to the family Mexican place \u2014 somehow with tip and drinks it turned into $90?!\n\npigs in a blanket + baked beans + tater tots\n\nReading:\n\nRead A Bride\u2019s Story 14 by Kaoru Mori and Mammoths at the Gates by Nghi Vo and SCP-055 by qntm and CptBellman\n\nStarted reading Dangerous Fictions by Lyta Gold and This Will Be Fun by E. B. Asher\n\nContinued reading We Will Not Cancel Us by Adrienne Maree Brown\n\nAcquired free copy of In a Treacherous Court by Michelle Diener\n\nWords I looked up / concepts I learned:\n\nbiopolitics \u2014 \u201cThe governmental power of biopolitics is exerted through practices such as surveillance, healthcare policies, population control measures, gender-based laws, and the implementation of biometric identification systems.\u201d Thanks I hate it.\n\napodictically\n\n\u201caccountability sink\u201d (from Brian Merchant)\n\nhustings\n\ntendentious\n\ncompunctious\n\nShoutout to Jeremy for reminding me of the existence of Webster\u2019s 1913 Dictionary, and the paean to it by James Somers from 2014\n\nChoice phrases:\n\n\u201cThe books are not yet on the shelves, not yet touched by the mild boredom of order.\u201d\n\n\u2014 Walter Benjamin, Unpacking My Library (emphasis mine)\n\n\u201cThe fear of fiction waxes and wanes, spiking every couple of decades like some kind of hysterical cicada.\u201d\n\n\u2014 Lyta Gold, Dangerous Fictions\n\nOnline, we perform solidarity for strangers rather than engaging in hard conversations with comrades.\n\n\u2014 adrienne maree brown, We Will Not Cancel Us\n\nPretty stuff I saw:\n\nNew music I listened to:\n\nWebsite changes:\n\nNature notes:", "label": 1}
{"title": "The Natural Garden: Oaxaca - IndieWeb Carnival August 2023", "url": "https://lifeofpablo.com/blog/the-natural-garden-oaxaca", "content": "When the word gardening comes to mind, many think of maintaining a small plot of land. I envision, peppers, tomatoes, peaches, or simply a flower garden. This is exactly what I see in my parent's garden back in Nebraska. A garden can mean so many things due to culture, relationships, activism, hobbies, etc.\n\nA garden I think of often is the country side in Oaxaca. I've gone hiking here for many years since I was a kid. Exploring here gives me a sense of reconnecting with my roots. It let's me explore with no limit to what I can find or experience. It helps me be in the moment. This garden to me is infinite.\n\nIt's a beautiful place to walk through when everything is green or dry. I really admire the beauty of it. There are so many native plants surrounding me. I see the cactus, the native grasses, the carrizo (giant reed) and any plant offspring that falls on the ground.\n\nThere are so many fields of crops as well. Nature and agriculture co-existing. It's weird how well they blend in with each other.\n\nMy favorite time to enjoy the garden is sun down. All the colors are breath taking to watch and see how day becomes night. There are fun activities you can do at night time during a hike such as picking azucenas (lilies).\n\nThis is the land my ancestors lived on. I've lived on this land as well. I must continue to visit and reconnect with the people before me.\n\nNature is so beautiful. We must protect it at all costs.", "label": 1}
{"title": "My Childhood Home", "url": "https://lifeofpablo.com/blog/my-childhood-home", "content": "I flew back for the holidays to my hometown in Nebraska. When I'm on the plane ride home, various memories run through my head as I am excited to visit home. I think about the things I need to get done while I am home. I'm excited to see my friends and most importantly, my family.\n\nMy hometown is quiet. There's enough to do. There are stores, parks, movie theaters. Just enough. There are subtle things I do miss about my hometown. It's the place that saw me grow up. A place where I am familiar with.\n\nSomething happens as I enter my childhood home. I spent most of my childhood in this house. When I step into the home, so many many memories flood my mind. It's like I unzipped a .zip file and the entire folder just spills. Every step I take and every time I turn my head, flashbacks come to me as well.\n\nThere are so many moments that make me feel happy. I remember it was here I got my first computer. It's where I learned to ride a bike. I remember helping my mom study for the U.S Citizenship Test everyday afterschool. I remember numerous times packing and unpacking for/from Mexico. I remember the moment I learned I was accepted into university.\n\nMy bedroom was a place of safety. A place where I could be myself. The place where I learned. I took apart my first computer. A place I recorded my first audio note. A place to allow my emotions to run free.\n\nWe didn't live in the best house but it was our house. We had a roof over our heads.\n\nI reflect back on all this and it brings a smile of joy. So much has happened. So many things happening in the background I didn't know were happening.\n\nI'm here now rearranging my room. I feel happy that I can still make it my own and still have a place of safety.", "label": 1}
{"title": "Unlocking the power of unstructured data with RAG", "url": "https://github.blog/ai-and-ml/llms/unlocking-the-power-of-unstructured-data-with-rag/", "content": "Whether they\u2019re building a new product or improving a process or feature, developers and IT leaders need data and insights to make informed decisions.\n\nWhen it comes to software development, this data exists in two ways: unstructured and structured. While structured data follows a specific and predefined format, unstructured data\u2014like email, an audio or visual file, code comment, or commit message\u2014doesn\u2019t. This makes unstructured data hard to organize and interpret, which means teams can miss out on potentially valuable insights.\n\nTo make the most of their unstructured data, development teams are turning to retrieval-augmented generation, or RAG, a method for customizing large language models (LLMs). They can use RAG to keep LLMs up to date with organizational knowledge and the latest information available on the web. They can also use RAG and LLMs to surface and extract insights from unstructured data.\n\nGitHub data scientists, Pam Moriarty and Jessica Guo, explain unstructured data\u2019s unique value in software development, and how developers and organizations can use RAG to create greater efficiency and value in the development process.\n\nUnstructured data in software development\n\nWhen it comes to software development, unstructured data includes source code and the context surrounding it, as these sources of information don\u2019t follow a predefined format.\n\nHere are some examples of unstructured data on GitHub:\n\nREADME files describe in text the purpose behind project source code, and include instructions for source code use, how to contribute, and other details that developers decide is important to include. While they\u2019re usually written in Markdown, README files don\u2019t follow a predefined structure.\n\ndescribe in text the purpose behind project source code, and include instructions for source code use, how to contribute, and other details that developers decide is important to include. While they\u2019re usually written in Markdown, README files don\u2019t follow a predefined structure. Code files are more orderly than README files in that they follow the syntax of a programming language. But not all code files have the exact same fields nor are they all written in the same format. Additionally, some parts of the file, like coding logic and variable names, are decided by individual developers.\n\nare more orderly than README files in that they follow the syntax of a programming language. But not all code files have the exact same fields nor are they all written in the same format. Additionally, some parts of the file, like coding logic and variable names, are decided by individual developers. Package documentation explains how the software works and how to use it. Documentation, written in natural language, can include installation instructions, troubleshooting tips, a description of the package\u2019s API, and a list of any dependencies required to use the package. It can also include code snippets that highlight the package\u2019s features.\n\nexplains how the software works and how to use it. Documentation, written in natural language, can include installation instructions, troubleshooting tips, a description of the package\u2019s API, and a list of any dependencies required to use the package. It can also include code snippets that highlight the package\u2019s features. Code comments explain the function behind certain code blocks in a code file. They\u2019re text comments written in natural language and make the source code easier to understand by other developers.\n\nexplain the function behind certain code blocks in a code file. They\u2019re text comments written in natural language and make the source code easier to understand by other developers. Wiki pages , while not limited to unstructured data, can contain helpful text documentation about installation instructions, API references, and other information.\n\n, while not limited to unstructured data, can contain helpful text documentation about installation instructions, API references, and other information. Commit messages describe in natural language text the changes a developer made to a codebase and why.\n\ndescribe in natural language text the changes a developer made to a codebase and why. Issue and pull request descriptions are written in natural language and in a text field. They can contain any kind of information a developer chooses to include about a bug, feature request, or general task in a project.\n\nare written in natural language and in a text field. They can contain any kind of information a developer chooses to include about a bug, feature request, or general task in a project. Discussions contain a wealth and variety of information, from developer and end- user feedback to open-ended conversations about a topic. As long as a repository enables discussions, anyone with a GitHub account can start a discussion.\n\ncontain a wealth and variety of information, from developer and end- user feedback to open-ended conversations about a topic. As long as a repository enables discussions, anyone with a GitHub account can start a discussion. Review comments are where developers can discuss changes before they\u2019re merged into a codebase. Consequently, they contain information in natural language about code quality, context behind certain decisions, and concerns about potential bugs.\n\nThe value of unstructured data\n\nThe same features that make unstructured data valuable also make it hard to analyze.\n\nUnstructured data lacks inherent organization, as it often consists of free-form text, images, or multimedia content.\n\n\u201cWithout clear boundaries or predefined formats, extracting meaningful information from unstructured data becomes very challenging,\u201d Guo says.\n\nBut LLMs can help to identify complex patterns in unstructured data\u2014especially text. Though not all unstructured data is text, a lot of text is unstructured. And LLMs can help you to analyze it.\n\n\u201cWhen dealing with ambiguous, semi-structured or unstructured data, LLMs dramatically excel at identifying patterns, sentiments, entities, and topics within text data and uncover valuable insights that might otherwise remain hidden,\u201d Guo explains.\n\nNeed a refresher on LLMs? Check out our AI explainers, guides, and best practices >\n\nHere are a few reasons why developers and IT leaders might consider using RAG-powered LLMs to leverage unstructured data:\n\nSurface organizational best practices and establish consistency . Through RAG, an LLM can receive a prompt with additional context pulled from an organization\u2019s repositories and documents. So, instead of sifting through and piece-mealing documents, developers can quickly receive answers from an LLM that align with their organization\u2019s knowledge and best practices.\n\n. Through RAG, an LLM can receive a prompt with additional context pulled from an organization\u2019s repositories and documents. So, instead of sifting through and piece-mealing documents, developers can quickly receive answers from an LLM that align with their organization\u2019s knowledge and best practices. Accelerate and deepen understanding of an existing codebase\u2014including its conventions, functions, common issues, and bugs. Understanding and familiarizing yourself with code written by another developer is a persisting challenge for several reasons, including but not limited to: code complexity, use of different coding styles, a lack of documentation, use of legacy code or deprecated libraries and APIs, and the buildup of technical debt from quick fixes and workarounds.\n\nRAG can help to mediate these pain points by enabling developers to ask and receive answers in natural language about a specific codebase. It can also guide developers to relevant documentation or existing solutions.\n\nAccelerated and deepened understanding of a codebase enables junior developers to contribute their first pull request with less onboarding time and senior developers to mitigate live site incidents, even when they\u2019re unfamiliar with the service that\u2019s failing. It also means that legacy code suffering from \u201ccode rot\u201d and natural aging can be more quickly modernized and easily maintained.\n\nUnstructured data doesn\u2019t just help to improve development processes. It can also improve product decisions by surfacing user pain points.\n\nMoriarty says, \u201cStructured data might show a user\u2019s decision to upgrade or renew a subscription, or how frequently they use a product or not. While those decisions represent the user\u2019s attitude and feelings toward the product, it\u2019s not a complete representation. Unstructured data allows for more nuanced and qualitative feedback, making for a more complete picture.\u201d\n\nA lot of information and feedback is shared during informal discussions, whether those discussions happen on a call, over email, on social platforms, or in an instant message. From these discussions, decision makers and builders can find helpful feedback to improve a service or product, and understand general public and user sentiment.\n\nWhat about structured data?\n\nContrary to unstructured data, structured data\u2014like relational databases, Protobuf files, and configuration files\u2014follows a specific and predefined format.\n\nWe\u2019re not saying unstructured data is more valuable than structured. But the processes for analyzing structured data are more straightforward: you can use SQL functions to modify the data and traditional statistical methods to understand the relationship between different variables.\n\nThat\u2019s not to say AI isn\u2019t used for structured data analysis. \u201cThere\u2019s a reason that machine learning, given its predictive power, is and continues to be widespread across industries that use data,\u201d according to Moriarty.\n\nHowever, \u201cStructured data is often numeric, and numbers are simply easier to analyze for patterns than words are,\u201d Moriarty says. Not to mention that methods for analyzing structured data have been around longer** **than those for analyzing unstructured data: \u201cA longer history with more focus just means there are more established approaches, and more people are familiar with it,\u201d she explains.\n\nThat\u2019s why the demand to enhance structured data might seem less urgent, according to Guo. \u201cThe potential for transformative impact is significantly greater when applied to unstructured data,\u201d she says.\n\nHow does RAG extract value from unstructured data?\n\nWith RAG, an LLM can use data sources beyond its training data to generate an output.\n\nRAG is a prompting method that uses retrieval\u2014a process for searching for and accessing information\u2014to add more context to a prompt that generates an LLM response.\n\nThis method is designed to improve the quality and relevance of an LLM\u2019s outputs. Additional data sources include a vector database, traditional database, or search engine. So, developers who use an enterprise AI tool equipped with RAG can receive AI outputs customized to their organization\u2019s best practices and knowledge, and proprietary data.\n\nWe break down these data sources in our RAG explainer, but here\u2019s a quick summary:\n\nVector databases. While you code in your IDE, algorithms create embeddings for your code snippets, which are stored in a vector database. An AI coding tool can search that database to find snippets from across your codebase that are similar to the code you\u2019re currently writing and generate a suggestion.\n\nAnd when you\u2019re engaging with GitHub Copilot Chat on GitHub.com or in the IDE, your query or code is transformed into an embedding. Our retrieval service then fetches relevant embeddings from the vector database for the repository you\u2019ve indexed. These embeddings are turned back into text and code when they\u2019re added to the prompt as additional context for the LLM. This entire process leverages unstructured data, even though the retrieval system uses embeddings internally.\n\nGeneral text search. When developers engage with GitHub Copilot Chat under a GitHub Copilot Enterprise plan, they can index repositories\u2014specifically code and documentation. So, when a developer on GitHub.com or in the IDE asks GitHub Copilot Chat a question about an indexed repository, the AI coding tool can retrieve data from all of those indexed, unstructured data sources. And on GitHub.com, GitHub Copilot Chat can tap into a collection of unstructured data in Markdown files from across repositories, which we call knowledge bases.\n\nLearn about GitHub Copilot Enterprise features >\n\nBut wait, why is Markdown considered unstructured data? Though you can use Markdown to format a file, the file itself can contain essentially any kind of data. Think about it this way: how would you put the contents of a Markdown file in a table?\n\nExternal or internal search engine. The retrieval method searches and pulls information from a wide range of sources from the public web or your internal platforms and websites. That information is used for RAG, which means the AI model now has data from additional files\u2014like text, image, video, and audio\u2014to answer your questions.\n\nRetrieval also taps into internal search engines. So, if a developer wants to ask a question about a specific repository, they can index the repository and then send their question to GitHub Copilot Chat on GitHub.com. Retrieval uses our internal search engine to find relevant code or text from the indexed files, which are then used by RAG to prompt the LLM for a contextually relevant response.\n\nStay smart: LLMs can do things they weren\u2019t trained to do, so it\u2019s important to always evaluate and verify their outputs.\n\nRAG and GitHub Copilot Enterprise Powered by RAG, GitHub Copilot Enterprise can help developers and leaders at all levels receive natural language answers to questions about specific repositories. GitHub Copilot can also use content in commits, issues, and discussions to provide contextually relevant responses. In fact, by asking GitHub Copilot questions, developers actually provide GitHub Copilot with more details about the context in which information is being used, which then helps the AI coding tool provide more accurate responses tailored to an organization\u2019s unique codebase. Learn more about the use cases and benefits of GitHub Copilot Enterprise.\n\nUse RAG to unlock insights from unstructured data\n\nAs developers improve their productivity and write more code with AI tools like GitHub Copilot, there\u2019ll be even more unstructured data. Not just in the code itself, but also the information used to build, contextualize, maintain, and improve that code.\n\nThat means even more data containing rich insights that organizations can surface and leverage, or let sink and disappear.\n\nDevelopers and IT leaders can use RAG as a tool to help improve their productivity, produce high-quality and consistent code at greater speed, preserve and share information, and increase their understanding of existing codebases, which can impact reduced onboarding time.\n\nWith a RAG-powered AI tool, developers and IT leaders can quickly discover, analyze, and evaluate a wealth of unstructured data\u2014simply by asking a question.\n\nA RAG reading list \ud83d\udcda", "label": 0}
{"title": "Unlock your potential: Discover the enhanced Google Developer Program", "url": "https://developers.googleblog.com/en/google-developer-program-latest-enhancements/", "content": "The Google Developer Program is evolving. We're introducing AI-powered tools and expanded resources designed to help you build faster, smarter and more effectively with Google\u2019s technologies.\n\nIn today\u2019s fast-paced development landscape, having the right toolkit is crucial. We\u2019ve listened to your feedback and are delivering updates that will enable you to focus on what matters most: creating exceptional AI applications.\n\n\n\nUnlock your full potential with Google Developer Program Premium: Now infused with AI\n\nThe Google Developer Program premium membership ($299/year) is an enhanced set of resources designed to provide developers with more advanced capabilities, including access to Google\u2019s latest AI tools. Think of it as your catalyst for growth with advanced tools and dedicated support at every phase of your development journey.\n\nToday, we are adding some new ways you can experience the power of Google\u2019s AI:\n\nEnhanced Coding Assistance with Gemini Code Assist Standard: Access paid Gemini in Firebase features and Gemini Code Assist to instantly boost your coding efficiency. Write high-quality code, faster, and with greater confidence.\n\nIncreased Capacity with Firebase Studio Workspaces: Get more room to build! Project IDX is now part of Firebase Studio. With premium, you get 30 Firebase Studio workspaces, providing greater flexibility to handle complex projects and scale your applications.\n\nExperiment with the Latest Models: Dive into API-driven AI with a $50 GenAI developer credit for Google AI Studio and Google Cloud Vertex AI. Experiment with cutting-edge Gemini, Imagen, and Veo models and integrate powerful AI capabilities into your application.\n\nAccess to Premium Google AI Features: Premium members receive a 3-month free trial of Google One AI Premium. Enjoy Gemini Advanced, NotebookLM Plus, increased storage, and much more.\n\nWe've also improved the premium benefits dashboard. Log in to see all your benefits and easily activate them in one place.", "label": 0}
{"title": "Simplifying payment methods code in our new API version", "url": "https://stripe.com/blog/dynamic-payment-methods", "content": "The landscape of payment methods is changing quickly. Buy now, pay later methods, little-known less than a decade ago, now account for more than $300 billion in transactions worldwide. Mobile wallets accounted for roughly half of global ecommerce payment transactions last year. And while we at Stripe love nothing more than keeping up with emerging payments trends, we realize most developers don\u2019t want to constantly refactor and tweak their Stripe integration. So we\u2019ve been thinking hard about the right set of tools to help you continuously offer the payment methods your customers want while minimizing engineering overhead.\n\nThe problem with specifying configurable properties in code\n\nWriting code for a payments interface typically requires you to specify three types of properties: the capabilities of your payments integration (e.g., whether your application can support redirects), the variable properties of a given payment (e.g., amount or currency), and numerous configurable properties (e.g., do not show Affirm for transactions less than $50). These configurable properties, which are typically hard-coded in a parameter like payment_method_types , can cause land mines that result in payment failures and errors that are hard to debug.\n\nThis is because payment methods have nuanced limitations that impact whether they can be used for a given transaction\u2014whether it\u2019s transaction minimums and maximums, currency limitations, merchant category restrictions, or differences in recurring payment support. Implementing a new payment method means you need to 1) know all the specific limitations for that payment method and 2) encode complex logic to hide or display that payment method based on those limitations for each transaction. Doing either of these wrong means risking payment failures and disputes, leading to lots of time and effort to ship new changes.\n\nMaking dynamic payment methods the default in the API\n\nRequiring a code change to modify configurable properties is the type of high-toil engineering work you\u2019ve consistently told us you want to eliminate. So today, we\u2019re changing the default behavior of the PaymentIntents and SetupIntents API. Instead of individually specifying payment_method_types in code, you can now configure payment method settings from the Dashboard\u2014no code required. We call this integration path dynamic payment methods.\n\nWhen you use dynamic payment methods with our prebuilt UIs like the Payment Element or Checkout, Stripe handles the logic for displaying eligible payment methods in your frontend for each transaction\u2014removing the need to know and encode specific eligibility requirements. We then dynamically order all of your eligible payment methods to maximize for conversion based on factors including the customer\u2019s device, location, and local currency. If you want to change your payment methods configuration later, you can do so directly from the Dashboard. For example, if you want to offer different payment methods for one-time versus recurring payment flows, or if you want to hide high-risk payment methods on high-AOV transactions, you can set these rules from the Dashboard, rather than writing complex error-prone code. Your integration becomes simpler, safer, and more future-proof.\n\nBefore\n\napp.post(\"/create-payment-intent\", async (req, res) => { const { items } = req.body; const orderAmount = calculateOrderAmount(items) const orderCurrency = calculateOrderCurrency(items) let paymentMethodTypes = [\"card\"]; // Add specific payment methods based on currency and amount switch(orderCurrency) { case \"eur\": paymentMethodTypes.push(\"giropay\"); break; case \"gbp\": paymentMethodTypes.push(\"klarna\"); if (orderAmount >= 100 && orderAmount <= 100000) { paymentMethodTypes.push(\"afterpay_clearpay\"); } break; case \"usd\": paymentMethodTypes.push(\"paypal\"); break; default: } // Create PaymentIntent with payment_method_types const paymentIntent = await stripe.paymentIntents.create({ amount: orderAmount, currency: orderCurrency, payment_method_types: paymentMethodTypes, }); res.send({ clientSecret: paymentIntent.client_secret, }); }); ~\n\nAfter\n\napp.post(\"/create-payment-intent\", async (req, res) => { const { items } = req.body; const orderAmount = calculateOrderAmount(items) const orderCurrency = calculateOrderCurrency(items) // Create PaymentIntent with dynamic payment methods by default const paymentIntent = await stripe.paymentIntents.create({ amount: orderAmount, currency: orderCurrency }); res.send({ clientSecret: paymentIntent.client_secret, }); }); ~\n\nWe\u2019ve paired this API change with tools to give you confidence that your integration is working as expected and to determine why certain payment methods aren\u2019t available for a given transaction. From the Dashboard, you can enter a PaymentIntent ID or Checkout Session for more information on which payment methods were enabled for the transaction and why certain options were not eligible. You can also simulate which payment methods will be displayed when changing factors such as amount, currency, capture method, and future usage.\n\nAnd we\u2019re not stopping here. Since it no longer takes a code change to configure payment methods, you can rely on Stripe to tackle all kinds of checkout optimizations that previously required nontrivial engineering effort. You can use Dashboard controls to run A/B tests on payment methods, configure payment methods for different checkout scenarios, and set custom targeting rules. To take advantage of these new features, upgrade to the latest API version today.", "label": 0}
{"title": "Congratulations to the Recipients of the 2024 Spotify FOSS Fund", "url": "https://engineering.atspotify.com/2024/11/congratulations-to-the-recipients-of-the-2024-spotify-foss-fund", "content": "TL;DR The Spotify FOSS Fund is back again! We created the Spotify FOSS Fund in 2022 to help support the open source ecosystem and to lend a monetary hand to those projects we use most at Spotify. This year, we\u2019ve selected five projects to share the \u20ac100,000 fund (\u20ac20,000 each):\n\nHere\u2019s what some of the recipients had to say about their projects, what kind of impact the funds might have on them, and other ways we can all help build a more sustainable open source community. (Note: some responses have been edited for length and clarity.)\n\nNorman Maurer, Netty\n\nNorman Maurer is a Netty project lead and co-author of Netty in Action.\n\nWhat is the vision for Netty?\n\nNetty aims to remain the de facto standard for writing network libraries/services on the JVM. Through Netty\u2019s powerful abstraction it\u2019s possible to easily write reusable components that can be composed to build up complex processing logic. Beside this, Netty provides support for popular internet standards like HTTP1, HTTP2 and HTTP3 and many more. Netty also provides the most performant implementation for specific platforms (by making use of native code) and makes use of new features \u2014 like, for example io_uring, TLS \u2014 to provide low-latency / high throughput.\n\nArtem Zakharchenko, MSW\n\nArtem (@kettainaitto) created the library and is a full-time maintainer of MSW.\n\nHow will these funds go towards supporting the future of MSW?\n\nMSW challenges the misconception that mocking is a hacky thing, and gives thousands of developers around the world better tools to control their network. Support from Spotify will enable me to focus even more on the project. Since I\u2019ve switched to working on it full time (as financially damaging as it was), we\u2019ve already shipped a ton of improvements, like the Server Boundaries, socket-based interceptor for Node.js, and Source. Not to mention a lot of work that went into collaborating with the teams behind Vitest, Vercel, Apollo, Nock, Node.js (to name a few) to ensure consistent experience for everybody, everywhere. Later this year, I\u2019d love to announce the WebSocket support as well, making MSW the first API mocking library to cover all major API types. That being said, there are always more things to solve. Improving debugging and error handling, polishing type definitions, exploring uncharted territories like inter-process request interception. And now I will be able to solve them with Spotify\u2019s help!\n\nKetan Umare, Flyte\n\nKetan (@kumare3) is the founder and TSC chair of Flye and co-founder and CEO of @UnionAI\n\nWhat is the vision for Flyte?\n\nAs generative AI advances, we aim to position Flyte at the forefront of this transformation, uniting data, ML, and AI seamlessly. The vision is to enable ML engineers and AI teams to build, deploy and scale ML Workflows with an open-source, kubernetes-native, pythonic orchestration kit. To accomplish this, Flyte provides a free, modern workflow orchestration platform. This platform enables teams to build production-grade pipelines, easily handle large datasets and efficient scheduling while working on complex infrastructure in distributed environments. Our mission is to enable ML engineers to focus more on solving business challenges and less on debugging and testing workflows. We are dedicated to building the world\u2019s largest, most collaborative ML engineering community.\n\nSam Pillsworth, Typelevel\n\nSam (@samspills) is a TSC member of Typelevel and is based out of Toronto, Ontario.\n\nHow will these funds go towards supporting the future of Typelevel?\n\nMaintaining an organization at the scale of Typelevel incurs costs. Through FOSS funding we can provide better support to maintainers, and make stronger guarantees to users, by paying for things like CI infrastructure or community support. Our current funding goes towards recurring infrastructure expenses, and sending members of our code of conduct committee for training to better support our community. With these extra funds, we\u2019d like to expand our initiatives to encourage and support new contributors and users! We\u2019ve specifically discussed funding student or first time contributors through an initiative like Outreachy and engaging a technical writer to help improve documentation.\n\nThank you to all the maintainers of these projects for all the work they\u2019ve done for these projects!", "label": 0}
{"title": "GitHub Enterprise Cloud with data residency: How we built the next evolution of GitHub Enterprise using GitHub", "url": "https://github.blog/engineering/engineering-principles/github-enterprise-cloud-with-data-residency/", "content": "Today, we announced that GitHub Enterprise Cloud will offer data residency, starting with the European Union (EU) on October 29, 2024, to address a critical desire from customers and enable an optimal, unified experience on GitHub for our customers.\n\nData residency and what it means for developers\n\nWe\u2019ve heard for years from enterprises that being able to control where their data resides is critical for them. With data residency, organizations can now store their GitHub code and repository data in their preferred geographical region. With this need met, even more developers across the globe can build on the world\u2019s AI-powered developer platform.\n\nEnterprise Cloud with data residency provides enhanced user control and unique namespaces on ghe.com isolated from the open source cloud on github.com. It\u2019s built on the security, business continuity, and disaster recovery capabilities of Microsoft Azure.\n\nThis is a huge milestone for our customers and for GitHub\u2013a multi-year effort that required extensive time, effort, and dedication across the company. We\u2019re excited to share a behind-the-scenes look at how we leveraged GitHub to develop the next evolution of Enterprise Cloud.\n\nDesigning the architecture for the next evolution of GitHub Enterprise\n\nThis effort started in summer of 2022 with a proof of concept (PoC) and involved teams across GitHub. We carefully considered which architecture would enable us to be successful. After iterating with different approaches, we decided to build the new offering as a feature set that extends Enterprise Cloud. This approach would allow us to be consistently in sync with features on github.com and provide the performance, reliability, and security that our customers expect. For hosting, we effectively leveraged Microsoft Azure\u2019s scale, security, and regional footprint to produce a reliable and secured product with data residency built-in, without having to build new data centers ourselves.\n\nAs the home for all developers, developer experience is critically important for us. We recognized early on that consistency was important, so we sought to minimize differences in developing for Enterprise Cloud and Enterprise Cloud with data residency. To this end, the architecture across both is very similar, reducing complexity, risk, and development costs. The deployment model is familiar to our developers: it builds off of GitHub Actions. Also, changes to github.com and Enterprise Cloud with data residency are deployed minutes apart as part of a unified pipeline.\n\nTo accomplish this, we had to organize the work, modify our build and deployment systems, and validate the quality of the platform. We were able to do all three of these by using GitHub.\n\nOrganizing with GitHub Issues and Projects\n\nTo organize the project, we used GitHub Issues and Projects, taking advantage of multiple views to effectively drive work across multiple projects, more than 100 teams, and over 2,000 issues. Different stakeholders and teams could take advantage of these views to focus on the information most relevant to them. Our talented technical project management team helped coordinate updates and used the filtering and slicing capabilities of Projects to present continuously updated information for each milestone in an easily consumable way.\n\nWe also used upcoming features like issues hierarchy to help us understand relationships between issues, and issue types to help clearly classify issues across repositories. As part of using these features internally we were able to give feedback to the teams working on them and refine the final product. Keep an eye out for future announcements for issues hierarchy and issue types coming soon!\n\nAll of these powerful features helped us keep the initiative on track. We were able to clearly understand potential risk areas and partner across multiple teams to resolve blockers and complex dependencies, keeping the project effectively moving forward across multiple years.\n\nBuilding Enterprise Cloud with data residency using GitHub\n\nGitHub has always been built using GitHub. We wanted to continue this practice to set ourselves up for success with the new data residency feature. To this end, we continued leveraging GitHub Codespaces for development and GitHub Actions for continuous integration (CI). In addition, we added deployment targets for new regions. This produced a development, testing, and CI model that required no changes for our developers and a deployment process that was tightly integrated into the existing flow.\n\nWe have previously discussed our deploy then merge model, where we deploy branches before merging into the main branch. We expanded this approach to include successful deployments to Enterprise Cloud data residency targets before changes could be merged and considered complete, continuing to use the existing GitHub merge queue. A visualization of our monolithic deployment pipeline is shown in the figure below.\n\nWe start by deploying to environments used by GitHub employees in parallel. This includes the internal environment for Enterprise Cloud with data residency discussed more in the next section. As we use GitHub every day to build GitHub, this step helps us catch issues as employees use the product before it impacts our customers. After automated and manual testing, we proceed to roll out to \u201cCanary.\u201d Canary is the name for the stage where we configure our load balancers to gradually direct an increasing percentage of github.com traffic to the updated version of the code in a staged manner. Additional testing occurs in between each stage. Once we successfully deploy the updated version of github.com to all users, we then deploy and validate Enterprise Cloud with data residency in the EU before finishing the process and merging the pull request.\n\nEnsuring all deployments are successful before we merge means changes are deployed in sync across all Enterprise Cloud environments and monitored effectively. Note that in addition to deployments, we also use feature flags to gradually roll out changes to groups of customers to reduce risk. If a deployment to any target fails, we roll back the change completely. Once we have understood the failure and are ready to deploy again, the entire process starts from the beginning with the merge queue.\n\nFinally, to maintain consistency across all teams and services, we created automation to generate deployment pipelines for over 100 services so, as new targets are introduced, each service automatically deploys to the new environment in a consistent order.\n\nUsing Enterprise Cloud with data residency ourselves\n\nTo create the best possible product, we also prioritized using it ourselves and stood up an isolated environment for this purpose. Using our GitHub migration tooling, we moved the day-to-day development for the team working on GitHub Enterprise Importer to that environment, and invested in updating our build, deploy, and development environments to support working from the data resident environment. Since its creation, we have deployed to this environment over 8,000 times. This gave us invaluable feedback about the experience of working in the product with issues, pull requests, and actions that we were able to address early in the development process. We were also able to iterate on our status page tooling and internal Service Level Objective (SLO) process with the new environment in mind. The team is continuing to work in this environment today and runs over 1,000 actions jobs a month. This is a testament to the stability and quality we\u2019ve been able to deliver and our commitment to this feature.\n\nWhat\u2019s next\n\nWe are proud that we\u2019ve been able to evolve Enterprise Cloud to offer data residency while using GitHub to organize, build, deploy, and test it. We\u2019re excited to unlock GitHub for even more developers and for you to experience what we have built, starting on October 29, 2024 in the EU, with more regions on the way.\n\nIf you\u2019re excited about Enterprise Cloud with data residency, please join us at GitHub Universe 2024 to learn more and hear from other companies how they\u2019ve used this to accelerate software development and innovation.\n\nTags:", "label": 0}
{"title": "Midlife rage \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/01/midlife-rage/", "content": "I\u2019m turning 40 later this year, and I\u2019ve been thinking about the reality that I\u2019m approaching \u201chalfway\u201d through my statistically projected lifespan. But I\u2019m not having a crisis about my personal life choices\u2026 instead I\u2019m having a crisis of generational impotence. I\u2019m tired of waiting for my generation\u2019s turn, which I am less and less convinced will come in time to make a difference, especially since GenZ seems to be going conservative \ud83d\ude11\n\nOuter South by Conor Oberst and the Mystic Valley Band\n\nI finally got letters back from two of my reps:\n\nSenator Murray, 74yo and in office since 1993 when I was in third grade, wants me to know that she \u201cremain[s] committed to continuing to work with [her] colleagues on both sides of the aisle to find ways to make progress on immigration reform.\u201d Thank you for your complacency Senator. I don\u2019t know how immigration progress will be made with isolationist Nazis tbh \ud83e\uddd0 I don\u2019t get why Dems are taking conservative immigration stances when in my mind the problem is we are too strict on immigration, and should let in more people. There, birth rate \u201ccrisis\u201d solved too! What I want to know now is, what are the Senator\u2019s plans for retirement?\n\nRepresentative DelBene\u2019s office thinks I \u201cmay be pleased to know that [she] joined [her] Democratic colleagues in sending a letter to President Trump expressing serious concern about Mr. Abrego Garcia\u2019s case and requesting an immediate update on his wellbeing and how the administration plans to ensure his safe return to the United States.\u201d I am, in fact, not pleased. I am the opposite of pleased. Is she fucking with me? Does she not go on the Internet? In what world am I satisfied that she signed a strongly worded letter and called it good?\n\nMy resentment towards 80yo politicians who won\u2019t retire and pass their power to the next generation is reaching a rolling boil as the Democrats in charge apparently don\u2019t have an ounce of urgency left in their moldering bodies. I have to think it\u2019s intentional, especially given the way Nancy and co blocked Millennial AOC from a position of power in favor of an old dude with cancer who just died from it (\u201cGerry\u2019s a young 74, cancer notwithstanding\u201d \ud83d\ude44\ud83d\ude44\ud83d\ude44); I think Boomer politicians know how badly they\u2019ve fucked over everyone younger than them, and they will literally hold power until they die to prevent facing justice. (Yeah I\u2019m making it generational, they\u2019ve been demonizing Millennials to avoid looking in the mirror for decades.) They booted 25yo David Hogg from DNC leadership as punishment for suggesting the party should not always support incumbents, making a mockery of DEI principles by using gender equality rules to invalidate his election.\n\nA friend and I just looked it up: Nancy Pelosi was first elected in 1987. She has been in power for basically my entire life. And I am turning 40. Her legacy, in my mind? Losing women the right to bodily autonomy, for not taking Republicans seriously and doing anything about it when she could have. OK fine she did get us the ACA. Thanks lady. But it\u2019s time to go home, get some rest, and let someone else have a turn! (Sorry Bernie, I love ya man, but you too!!!)\n\nI think those in charge should have some stakes in the game. If you\u2019re already past the actuarial tables, you shouldn\u2019t get to make decisions for the rest of us who are going to live in the world our politicians make for potentially decades. (I expect the Supreme Court will be ultra conservative for the rest of my life because Dems wouldn\u2019t dare trying to expand it.) Be a mentor, yes, please! But give up the power. Normal retirement age is 67. I don\u2019t think it\u2019s ageist to suggest it might be time for someone two decades past retirement age to pass the torch.\n\nGive me a party with energy, give me politicians who fight! Oh, for an opposition party \ud83e\udd7a I don\u2019t have to agree 100% with them, if they\u2019d just have some gumption. Give me AOC energy, Shawn Fain energy, Pete Buttigieg (!) energy, Cory Booker energy, Chris Van Hollen energy, Kat Abughazaleh energy. Not whatever this pathetic excuse for \u201cleadership\u201d is. Walz had the energy and they told him to stuff it. Brilliant strategy guys.\n\nPlease, please tell me someone is primarying all my Federal reps next election, because I will vote for anyone who shows some semblance of spine \ud83d\ude21", "label": 1}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2018-03", "content": "en\n\nCareer\n\nI am so excited to become a teacher down the road and spread my global awareness to my future students. Teachers are so needed to help build the next generation of students who will lead the way to a better future. Living in the world that we live in today, it is needed to learn another language, to love cultures and to love those people who are different from us. All this has me so stoked!\n\nPondering\n\nLately, I've been thinking about my future. I've been asking myself, \"\"what can I do to become more than just an ordinary teacher?\"\" I definitely want to be more well-rounded as a person. I'm not the type of person who is satisfied settling for one thing. I've been becoming the person who really wants to try new things and hope to see where such road takes me? Even though I should have minored in it at least, I still feel like I can be successful learning new skills. I want to start a new journey this summer in learning new skills. I want to leave my comfort zone and just do me? So what exactly am I doing?\n\n3 things\n\nI decided that I want to learn and continue learning 3 things: learn the skills to graphic design, continue HTML/CSS & other programming languages and learn to make a simple app. This is something I've always wanted to do for a while but was scared to start. I have received inspiration from people around me. This is gives me hope that even though I am only a beginner. Mastering all of these skills will take more than a summer. I have accepted this.\n\nI also plan on being able to continuously read a book at all times. Knowledge will help me sooth the savage beast from driving me crazy. I want to be able to grow the mind and soul in ways I haven't felt in a longtime.\n\nPersonal Growth\n\nOverall, I want this to be my summer to help me grow, to enlighten me and overall, to help me become a better person. I won't just limit myself to the items mentioned above. I'm going to try all sorts of things. How I go about this will be all up to me. My attitude toward things will decide how bad or great my outcome will be. I know that things will be positive. I will face obstacles but I will overcome them. What matters is that I tried something new this summer.\n\nI want to thank you for all of your support. I hope to be able to share my adventure as I go. Even though I am hoping to grow this summer, I want to let you know that you will also grow this summer. You and I will find the right path for a bright future.\"", "label": 1}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2024-05", "content": "en\n\nI've seen the movie, Call Me By Your Name featuring Arnie Hammer and Timoth\u00e9e Chalamet a few times since it came out. Not only is it a queer film which I love but a film that includes multilingual characters. What I melt inside of happiness when multiple languages are spoken in a movie. We hear English, Italian and French. One minute you hear French, then the same character will switch to English or Italian often times in the same scene. I find this very beautiful as someone who can speak multiple romantic languages. The code switching in my opinion is pretty good. Not to brag but it helps to know a few romantic languages to understand the others. I speak English, Spanish and French, fluently. I can for the most part understand Italian since there is so much cross over and there is so much cultural cross-context that I understand. I honestly turn off the subtitles when I watch this movie and others because I don't feel the need to see the translation. The movie is a lot more meaningful to me because I can understand the subtleties of the languages and how people express themselves in such languages.\n\nMovies and TV shows that can switch between languages are so cool!", "label": 1}
{"title": "I got accepted to a master's program", "url": "https://lifeofpablo.com/blog/i-got-accepted-to-a-master-s-program", "content": "I got accepted to a master's program\n\nUniversity of Nebraska Kearney Logo\n\nThis post was written in English (en_US).\n\nI'm really excited to share that I applied to the graduate program at the alma mater I graduated from at the University of Nebraska at Kearney. I applied for the Masters of Science in Education in Learning, Design & Technology with an Information Technology Concentration. I chose the IT concentration because, I work in IT where I would like to implement my knowledge at the school level. I'm not in the classroom as often anymore. It seemed more appropriate to go this route.\n\nI'm really excited to go back to school! I've been itching for this for a while. Since, I still work in education, it seems appropriate to also continue my education and better improve my knowledge and skills in addition to what I do already with other forms of learning.\n\nI also value education as well. I want to also have degrees beyond a bachelor's degree. My goal in the near but possibly distance future is to get a doctorate degree. Even with a master's degree, it's been a dream to teach a course or two at the university level. I would teach technology courses specifically for teachers to help eliminate barriers and increase technology literacy for teachers. I would also teach methods for teachers of all ages through scaffolding to embrace technology.\n\nAs a Latino, I find it important to help represent that we can succeed in higher education.\n\nTime to register for classes!", "label": 1}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2018", "content": "en\n\n\"\n\nI am currently taking an ESL course and we had the opportunity to interview an ESL teacher. I interviewed Denise Teetor who is the ESL Teacher at Hastings Senior High School. She told me about her experiences getting into this subject field and how not always was she the ESL Teacher. I learned so much from this opportunity. This interview has me really excited to be a teacher. All the information here will hopefully help those people who want to become teachers. Getting the insights from another teacher lets you have a feeling of what you are getting yourself into. I am not trying to scare anyone, I'm just showing it how it is. Enjoy the interview! I will have the audio podcast available soon! Click the picture below to download a copy of the document to your device.\n\n\"\n\nI am currently taking an ESL course and we had the opportunity to interview an ESL teacher. I interviewed Denise Teetor who is the ESL Teacher at Hastings Senior High School. She told me about her experiences getting into this subject field and how not always was she the ESL Teacher. I learned so much from this opportunity. This interview has me really excited to be a teacher. All the information here will hopefully help those people who want to become teachers. Getting the insights from another teacher lets you have a feeling of what its like to be in the classroom. Enjoy the interview! I will have the audio podcast available soon! Click here to download a copy of the document to your device.\n\nDenise Teetor: ESL at Hastings Senior High School\n\nInterviewed by\n\nPablo MORALES GARCIA\n\nMrs. Denise Teetor is an English as a Second Language Teacher at Hastings Senior High School in Hastings Nebraska. She has been teaching at the high school for more than 30 years. She has quite a different perspective of the world since she has traveled outside of the United States plenty of times. She has seen so many changes in ESL over the years. She has quite the story to tell in which she becomes the ESL teacher she is today.\n\nMrs. Teetor was not an ESL teacher from the beginning. She use to teach physical education and coach for a long time. She decided to pursue a different area of teaching and left behind being an PE teacher and decided to teach English in Japan. She wanted to do something different with her time during the summers instead of wasting it. She saw an advertisement to teach in Japan for six weeks and thought \u201cthat sounds pretty cool\u201d, and that\u2019s how she got involved with Japan. She really enjoyed it and had so much fun with it.\n\nAfter one summer, she decided to pursue her ESL endorsements. She was thinking ahead that she could teach ESL at the high school level at some point down the road. One summer as she was getting ready to go back to Japan, she knew that the ELL teacher at Hastings High would be leaving that summer, even though she had not filed her resignation letter. Mrs. Teetor took some steps before the other teacher quit. She let the school know that she was interested in the position. The teacher did end up leaving and she received a phone call asking if she was interested in the ELL position and she said \u201cYes\u201d. This is how she moved from physical education to ELL. Japan really peaked her interest working with foreign kids.\n\nShe is happy with how much ESL/ELL has evolved over the years, especially since she saw there was not a lot of curriculum that existed. The number of students has changed over the years. The program development has come a long way. There was curriculum that really existed. They were fortunate when they were able to find books related to ELL/ESL. Now there are whole curriculums that one can use. That has been an advancement of ELL programs. It is like all other curriculums in schools such as science where you have books, notebooks and a foundation to use. You start at the foundational levels and move through the different stages of it. It has been one of the biggest tools available so teachers don\u2019t have to go out and make their own material.\n\nShe describes the program at Hastings Senior High School. She gets all different levels of ESL students in each class period. She explains that in an ideal world, students are grouped by their level such as all level 1 students are together, level 2 are together, and so forth. Unfortunately, she doesn\u2019t have that luxury at the high school. She works with each level of students individually so each group of students can be differentiated as needed. She splits the groups up between her and Mrs. Brenneman, the para educator, to help each leveled group. The program at HHS is not a program that is like those implemented in other schools. The program isn\u2019t a dual language or other type. They just work based on the situation they have or a melting pot as she states. She is very proud of the graduation rate that has increased. If the kids start with the program, they tend to finish with the program unless they move away. She is proud that the ELL students have been able to take all the required classes or core classes needed to graduate. Even though they might not have a high level of English, \u201cthey do pretty well\u201d. Even though teachers provide differentiation, they learn all the same material as regular students. The students are able to get into the classroom right away. This is good for them since they can get \u201creal world\u201d experience and immerse themselves in the culture and listen to how students talk. No one ever follows grammar rules when the speak.\n\nMrs. Teetor mentions that students have to take a test to see if they need ESL services based on responses on intake forms when transferring into the district. She uses the newly implemented ELPA to screen prospective and current students who might or are using ESL services to measure their level of English. Newly arriving students fill out a questionnaire that askes the language they spoke first, what language is spoken primarily at home, and what language they want to be communicated with. If any of these are not English, they need to be tested. Even though a student knows English, they still have to take the test if their language spoken at home is not English. She uses a computer screener test. Anyone that is four (4) or above, they do not need to be in the program but anything below that, students should be enrolled in ESL/ELL. Things are done differently at the elementary level compared to the high school level.\n\nMrs. Teetor works closely with other teachers. She also likes to know what the teachers are teaching so she can meet the needs of students such as preparing a presentation or report. With American History, the students have no background especially with the foreign students. American history pertains to the United States. Math is math, science is science, these things are the same all the way across cultures. She enjoys how things are taught at HHS. Students are in ELL for at least 2 class periods. She tells us about an example student who doesn\u2019t speak a lot of but is very bright. Looking at her transcripts, she is a very well-rounded person. She has taken courses in psychology to advanced math. She was top of her class before moving. This student is \u201cbrilliant\u201d, according to Mrs. Teetor. She sees how this student can feel out of place due to the language barrier. She sees another barrier with math leaning towards story-problems. It intimidates students since it involves reading instead of involving direct math problems. What is truly moving is that she truly advocates for students to get them in the regular classroom where they will have to think, perform in the real thing. She knows it will be hard but it would be a \u201cdisservice\u201d if she does not.\n\nShe hasn\u2019t faced any teachers who have rejected or negated students. This at least hasn\u2019t occurred during her time her but she says that \u201cit might have happened before I started.\u201d She believes this hasn\u2019t happened because of her strong personality and she knows the teachers will be supportive at Hastings Senior High School. She hopes that the next person who replaces her will continue on standing up for the students. She also knows which teachers will be best for her students and she know the teachers who will not be a good fit. She moves students as necessary to put them in the right classrooms. She would rather do something else with the student if the teacher is not a good fit for the students. For a new teacher, this is something that he or she will not know and this will take time to figure out. She is hopeful the other teacher will be the best advocate for the ESL students.\n\nShe has great advice for new ESL teachers especially for the teacher replacing her. The best advice would be just to have fun with these students. They do need to learn but also everything is not so rigid that they can\u2019t enjoy. Simply cramming information gets them discouraged. It has been interested and fun for them to know. Everyone has some kid within each other. We need to promote this with the kids. She gets a lot out of the kids when they have fun. If they are sitting bored, they are not going anywhere. This is her overall advice.\n\nThank you, Mrs. Teetor for allowing me to interview you. It was such a great insight of ESL and very helpful for me for my future career as an ESL Teacher. I wish you the best in luck with retirement and hope to hear about your adventures in Japan! Cheers!\n\nA podcast of this will be available on my SoundCloud page soon!", "label": 1}
{"title": "Weeknotes: May 31 \u2013 June 6, 2025 \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/06/weeknotes-may-31-june-6-2025/", "content": "Happy Pride Month! \ud83c\udf08\n\nWin of the week: finally! finished! this AI aesthetics blog post! it\u2019s occupied basically all my blogging time for the past month, at last I can think about something else \ud83d\ude02 (who am I kidding, now I can turn the sections that got cut into their own posts lol)\n\nLooking forward to: doing a garden tour this weekend\n\nStuff I did:\n\n6.75 hours consulting \u2014 and signed the contract extension / budget adjustment for the rest of the year!\n\n3.25 hours writing \u2014 goal was 4 but I squandered a writing session on reading a craft book \u2014 still happy with the changes I made this week\n\nbonus evening walk with my friend \u2014 we\u2019re going to try doing two walks a week while evening daylight hours permit because neither of us dig squeezing in a weekend walk when we wanna be chilling\n\nregular weekly walk \u2014 had to dig my shorts out from winter storage \ud83e\ude73\n\nbaked strawberry biscuits \ud83d\ude0d haven\u2019t made these in a while because strawberries were $$$ last year\n\nhusband put on the most painful YouTube show, Kill Tony \u2014 it was like a car wreck, I couldn\u2019t look away even though I wanted to read my book \ud83d\ude02 (this is part of why I hate TV, there\u2019s something about it that\u2019s hard to tear my attention away from, even when something else is happening I\u2019d rather pay attention to)\n\nupdated my 2025 listening page with my April and May listens\n\nI washed my favorite new sweater with a load of mixed darks and dye rubbed off on it even though I turned my pants inside out! \ud83d\ude2d (I forgot I haven\u2019t worn / washed one of the pants very many times.) Spent a couple hours trying different methods for washing it out, including handwashing, using different detergents, and trying different stain removers. An enzymatic stain remover + Tide Free and Clear was my last attempt that got most of it out. Fortunately nothing else was affected.\n\nDinners:\n\nburger takeout \u2014 impossible burger with sauteed onions + French fries\n\nDomino\u2019s pizza (after watching a video on how it\u2019s manufactured)\n\nbaked feta with tomatoes + ciabatta bread + lemonade\n\nred lentil curry + rice + naan + ginger beer\n\nsourdough pancakes with strawberries and syrup + potato wedges\n\nIndian takeout \u2014 dum aloo + rice + naan \u2014 took two enzyme pills rather than the usual one but still had trouble, I think spice and fat multiply each other \ud83d\ude14\n\nfake chicken burgers with avocado and pineapple + tater tots + lemonade\n\nReading:\n\nRead A Sea of Love by Wilfrid Lupino and Gregory Panaccione, The Last Guy on Earth by Sarina Bowen, Seven Drafts by Allison K Williams, and Wed to the Alien Warlord by January Bell (needed some braindead reading)\n\nStarted reading Empire of Shadows by Jacquelyne Benson and Life Debt by R.J. Blain \u2014 not sure if I\u2019ll keep reading either\n\nContinued reading Things Become Other Things by Craig Mod and Cooking As Though You Might Cook Again by Danny Licht\n\nWords I looked up:\n\nChoice phrases:\n\nOn non-tradwife women who like cooking: \u201cdomestic, but not domesticated\u201d \u2014 Emily Amick\n\nPretty stuff I saw:\n\nPretty colors in these cactus flowers Anthony posted\n\nbeautiful woodblock prints of plants by Ito Jakuchu in the book \u201cGenpo y\u014dka\u201d (1768)\n\nNew music I listened to:\n\nWebsite changes:\n\nTurned off self-webmentions but left on self-pingbacks \u2014 I find pingbacks render more usefully, showing an obvious excerpt of the linking text and listing the post title as the commenter, whereas self-webmentions look just like a comment and post either the entire referencing post or a non-obvious excerpt from the start of the post. Also, self-pingbacks don\u2019t send me an email while self-webmentions do \ud83e\uddbe\n\n(If it\u2019s possible to change how my self-webmentions display on my site retroactively I would love to hear it, because I\u2019m not going to republish all ~3000 posts on this site\u2026)\n\nNature notes:", "label": 1}
{"title": "The Friend I Made on the Bus", "url": "https://lifeofpablo.com/blog/friend-on-bus", "content": "I was browsing the web and I happened to stumble upon this Instagram post. It was a screenshot of a tweet OP posted about two girls on a cruise who became best friends for the day. One of the girls was looking for her best friend that day.\n\nHere is what the post said, if you don't want to open an Instagram link. The post also has the picture of two girls on the cruise sitting next to each other.\n\nin Hawaii in 2006. We were basically best friends for that night so I need y'all to help me find my bestfriend because I miss her and I need to see how she's doing now. Please retweet this so we can be reunited.\n\nThis helped me unlock a memory of an experience I had as a young kid. I remember my family and I took the Greyhound bus back from Mexico all the way to Nebraska. This was in the early 2000s. Let me tell you. When you're six or seven on a bus from Southern Mexico to Nebraska, you feel like dying, died, and forgot you died on a 48+ hour (or however long) ride. Maybe I'm being dramatic. It wasn't a pleasant ride, at least most of it.\n\nI remember when the bus made a stop somewhere in Texas possibly in Austin or Dallas. I remember I was already having a bad time since I've been on a bus for an extended amount of time. Then some kid around my age got on the bus with his family. I gave him the nod to acknowledge him. He happened to sit near us. An hour or so had passed and I was curious and was desperate to talk to someone who wasn't in my family. So I said hello to the kid. I don't regret doing so.\n\nWhen I said hello, right off the bat we hit it off. We were talking about a bunch of nerd stuff that kids our age were interested in.. I remember bringing up our computers and we lost our minds. I was telling him about the iMac G3s I would use at school or how my friend built a small website on his computer. He would tell me about things he'd create on Kid Pix. We also talked about our favorite TV shows, our hobbies, and upbringings. We shared stories. This kid was super cool! I hope he thought I was cool too. We had a grand old time.\n\nAt some point, he and his family had to get off to their transfer point or it was their final destination. We said our goodbyes. Then I waved at him, he waved back when he was on the ground. It was a very warm and sad moment to say goodbye to my best friend of that day.\n\nI unfortunately don't remember his name and I doubt he remembers mine. At that time, it wasn't common for young kids to have a cell phone. We didn't exchange numbers or email. I think about this often. I wish I could find out where he is and if he is doing well. Who knows what paths we've taken? Have we crossed paths and not known it? I hope we get to meet someday and this post comes across him and we could reconnect.\n\nIf you're out there man, say hello!\n\nDo you have a story similar to this? I'd love to hear from you.", "label": 1}
{"title": "Implementing end-to-end encryption for Dropbox teams", "url": "https://dropbox.tech/security/end-to-end-encryption-for-dropbox-teams", "content": "People trust Dropbox to keep their most important content secure. As more teams embrace remote and distributed work, ensuring the privacy and security of their data has never been more important. While customers already appreciate our simple, seamless access controls, those who work with more sensitive information have told us they want even more control over how their data is secured. One of the ways that Dropbox is meeting the needs of these customers is with the introduction of zero-knowledge, end-to-end encryption for team folders. While Dropbox already encrypts files at rest using 256-bit AES, customers are seeking end-to-end encryption where only they possess the decryption key, so not even Dropbox can access the contents of their files. For customers with especially sensitive or confidential data\u2014for example, those working in finance or healthcare\u2014end-to-end encryption offers an additional level of security. When enabled, files are encrypted directly on the customer\u2019s device before being uploaded to our servers. Here we\u2019ll discuss our implementation of end-to-end encryption for teams, the threat model of our design and encryption algorithms, and our commitment to minimizing the risk of data loss with a team-centric key management approach.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images\u2014plus generative AI capabilities across even more connected apps. See what's new \u2192\n\nBalancing security and usability\n\nOur commitment to simplicity and reliability is at the heart of our encryption design. In our view, a secure system must also be user-friendly. A security system serves no purpose if it\u2019s too complicated to use. For a feature like end-to-end encryption, with its added layer of complexity, striking the right balance between security and usability is key. Our aim was to make this technology accessible without compromising security. At its core, our implementation of end-to-end encryption is designed so that neither Dropbox, unauthorized users, or malicious third parties can access a team\u2019s encrypted files. Only the team holds the keys. Even if an attacker gains access to those keys, our implementation still ensures the confidentiality of new files or modifications, as long as the team\u2019s keys have been rotated. Encryption also assures that a file has not been tampered with. In other words, if a file decrypts successfully, it is cryptographically guaranteed to be the exact same content as encrypted in the first place.\n\nAt the same time, because zero-knowledge encryption means customers manage their own keys, they also risk losing access to their data if those keys are lost. To address this, we\u2019ve developed a key management system designed specifically for teams. It ensures that even if one member loses their keys, the data remains accessible and secure for the rest of the team.\n\nTeam-centric key management\n\nKey management in many end-to-end encryption systems has traditionally focused on individual users, mainly because they were the first to adopt and use these systems. In those systems, each user is responsible for managing their own set of keys and making sure they're always accessible. However, this can create complications that diminish the user experience and may even lead to data loss if keys are misplaced. To counteract potential data loss, some systems use a method called key escrow, which allows for data recovery by a trusted third party, e.g. a spouse or an administrator. But this adds complexity, both in terms of the coding required and in using the product itself. By focusing on our teams customers and drawing the cryptographic boundary around teams, we were able to re-think how the key management is done. With our approach, users don\u2019t have any keys, but every team has a central team key. This key is accessible to all team members and controls access to the team\u2019s encrypted data, providing protection against unauthorized third parties. The team-centric approach offers the following benefits: Reduced risk of data loss and implicit key escrow. By sharing the team key among all members, any member with access\u2014such as a team admin with a recovery key or a member with a registered device\u2014can restore access for everyone.\n\nBy sharing the team key among all members, any member with access\u2014such as a team admin with a recovery key or a member with a registered device\u2014can restore access for everyone. Reduced user responsibility. The burden of managing cryptographic keys shifts from individuals to the team, reducing the risk of a single person causing data loss.\n\nThe burden of managing cryptographic keys shifts from individuals to the team, reducing the risk of a single person causing data loss. Reduced complexity and improved user experience. The absence of user keys as well as an explicit key escrow significantly simplifies the implementation and improves the user experience. Team members can simply use end-to-end encryption without having to worry about keys at all. To preserve data confidentiality when team members change, admins can rotate keys for the entire team. Rotating keys upon the departure of a member ensures that any potentially leaked keys become obsolete for accessing new or modified encrypted data. This mechanism is critical in a scenario where a former member, now considered an untrusted outsider, attempts to misuse a previously acquired key. By instituting a new team key for encrypting subsequent data, the system effectively safeguards the confidentiality of new files or modifications made after key rotation, thereby aligning with the threat model's emphasis on protecting data integrity against insider threats turned external.\n\nAutomatic and manual device registration\n\nBefore a user can use end-to-end encryption on a new device, the required keys must first be made available. We offer admins a choice of two device registration modes: automatic device registration and manual device registration. Automatic device registration balances security with usability by distributing keys from our system to authorized team members through the Dropbox authentication and access control infrastructure\u2014for example, when logging into a new device. Existing devices automatically authorize new devices by wrapping the team key with the new device's public key. If there are no devices available to do this, a team admin can use a recovery key to facilitate the new device's registration. The device then obtains and uses its version of the team key, ensuring quick and smooth setup without manual input. If a customer prefers more fine-grained control, they can opt for manual device registration. This process lets team admins personally approve new devices before they can access encrypted files. Team admins and members can check key authenticity by comparing the fingerprints, or security codes, of the device and team keys out-of-band. Only keys verified to belong to the correct devices and team will be used, ensuring that only legitimate team devices can access encrypted files. This process adds an additional safeguard against unauthorized access and man-in-the-middle attacks as admins can ensure that a key really belongs to a person or team, and not a malicious actor. Despite its security benefits, key verification can be cumbersome and impact usability, often leading to its limited real-world use\u2014so we've made it an optional feature for those who need greater security.\n\nWhat end-to-end encryption doesn\u2019t cover\n\nIt\u2019s important to point out there are also some threats that fall beyond the scope of our implementation: Device security. Though end-to-end encryption keeps data safe during transmission and while stored on our servers, it doesn't address security at the device level. Since encrypted files decrypt automatically for access during sync or download, we still recommend customers adopt best practices such as full-disk encryption and secure access methods to protect their devices.\n\nThough end-to-end encryption keeps data safe during transmission and while stored on our servers, it doesn't address security at the device level. Since encrypted files decrypt automatically for access during sync or download, we still recommend customers adopt best practices such as full-disk encryption and secure access methods to protect their devices. Metadata visibility. Our encryption efforts concentrate on file contents rather than metadata. With this approach, customers can still search their Dropbox account based on metadata such as file name, file type, and creation date, ensuring end-to-end encryption is still practical in everyday use.\n\nOur encryption efforts concentrate on file contents rather than metadata. With this approach, customers can still search their Dropbox account based on metadata such as file name, file type, and creation date, ensuring end-to-end encryption is still practical in everyday use. Insider threats. Our implementation safeguards against external threats to a team but doesn't change internal permissions. Teams should continue using existing access controls to manage data access amongst members, ensuring sensitive information remains compartmentalized and secure.\n\nA closer look at our encryption techniques\n\nOur implementation uses a hybrid scheme, combining a symmetric algorithm for encrypting file content with an asymmetric algorithm for securing the keys. We aim for a balance of proven security, performance, and broad platform support in our choice of encryption algorithms. Symmetric file encryption\n\nPlaintext content is split into 4 MB blocks, where each block is authenticated using AES-256 encrypted in Galois/Counter Mode (GCM) with a random and unique 96-bit nonce. While AES-GCM guarantees authenticity and integrity for each block, the 128-bit authentication tags of all blocks are cryptographically hashed using HMAC-SHA-256 to expand these guarantees to the entirety of the file. This method supports partial encryption and decryption, offering seamless security without compromising the file's integrity or order. This method is especially effective for large files, as it aligns with our practice of chunking file content into 4 MB blocks for storage. It also avoids the limitations of in-memory processing required by some APIs, like WebCrypto. Asymmetric key wrapping\n\nTo encrypt secret keys, our approach to key management uses Hybrid Public Key Encryption (HPKE), a modern and flexible standard that combines asymmetric and symmetric encryption in a hybrid crypto system. We use HPKE in single shot, base mode using Elliptic-Curve Cryptography (ECC) with the P-256 curve, SHA-256, and AES-256-GCM (DHKEM(P-256, HKDF-SHA256), HKDF-SHA256, AES-256-GCM). When manual device registration is chosen, HPKE is used in auth mode to encrypt parts of the key chain with sender authentication required for effective key verification. NIST P-256 has been chosen over other curves like Curve25519 because it is widely adopted in the industry, is available in most cryptographic libraries (e.g. WebCrypto, CryptoKit, OpenSSL), and is specified in FIPS 186-4. Post-quantum cryptography\n\nThe algorithms mentioned above do not include any post-quantum cryptography (PQC). While there exist some products with early implementations of PQC, we're taking a more cautious approach, relying on proven and time-tested encryption algorithms for several reasons: PQC's reliability for long-term storage is still uncertain due to ongoing standardization efforts. For instance, the Kyber algorithm has seen several revisions throughout its NIST standardization process.\n\nPQC is relatively new in cryptographic terms and lacks the extensive scrutiny that more established algorithms have undergone. To counteract this, some PQC applications use a hybrid model, where traditional cryptography is also used. This ensures baseline security should the PQC component be compromised at the expense of greater complexity.\n\nPQC algorithms are not yet sufficiently included in common cryptographic libraries, requiring custom implementations across some codebases and increasing the risk of vulnerabilities, bugs, and other human error.\n\nThe threat posed by quantum computing\u2014while significant\u2014is still theoretical, with its practical impacts still unknown. Given these considerations, we\u2019ve maintained flexibility around our ability to change our encryption protocols, while staying focused on trusted, well-known cryptographic implementations. This will enable us to integrate new encryption algorithms to our protocol at any time in the future. We are closely monitoring the development of Kyber and other PQC algorithms and will adapt our choice of encryption algorithms as they mature and standards evolve further.\n\nSecuring the future", "label": 0}
{"title": "ANNOUNCEMENT: Going to FRANCE!!!!", "url": "https://lifeofpablo.com/blog/announcement-going-to-france", "content": "ANNOUNCEMENT: Going to FRANCE!!!!\n\nThis post was written in English (en_US).\n\n\"Dear Friends and family,\n\nToday marks for an important announcement about my studies. Since many of you know, I have declared a French Education Major with an ESL endorsement very recently. Everything is starting to look up. So time for the juicy part of my post. Hold on tight!\n\nI would like to tell everyone that I will be studying abroad in France for this upcoming spring semester. Studying in France has been a dream that I have put on hold for quite some time. The desire to study in this country all began when I visited it over 3 years ago. That trip changed me greatly. This was the first time I had faced such culture. Deep down, I knew deep down inside that this is where I would want to live for a portion of my life. Going there for 6 months will be the experience to see a new part of France that I have never been too. I will be at the Universit\u00e9 de Strasbourg 'Institut International d'Etudes Fran\u00e7aises (IIEF UNISTRA).\n\nWanting to become French Teacher, this 6 month trip will greatly benefit me but most importantly, the influence I will have on my future students. I want to tell them what it is like living in another country. This trip will be a major milestone in my life as the want for has made me realize what I really want to do in my life. . As mentioned in a past post, I realized that teaching was actually my true calling. Je suis contente! Going through the process has been a lot of work. I am not even done yet, though! I still have to go to Chicago at some point in the semester to get my student visa. I will also get to see other parts of Europe! I am blessed to have such opportunity to explore outside of my comfort zone.\n\nI only see good things happening. This is only the beginning. Again thank you for the support! I will keep you all updated!\n\nBest Regards,\n\nPablo Morales\"", "label": 1}
{"title": "Midlife rage \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/01/midlife-rage/", "content": "I\u2019m turning 40 later this year, and I\u2019ve been thinking about the reality that I\u2019m approaching \u201chalfway\u201d through my statistically projected lifespan. But I\u2019m not having a crisis about my personal life choices\u2026 instead I\u2019m having a crisis of generational impotence. I\u2019m tired of waiting for my generation\u2019s turn, which I am less and less convinced will come in time to make a difference, especially since GenZ seems to be going conservative \ud83d\ude11\n\nOuter South by Conor Oberst and the Mystic Valley Band\n\nI finally got letters back from two of my reps:\n\nSenator Murray, 74yo and in office since 1993 when I was in third grade, wants me to know that she \u201cremain[s] committed to continuing to work with [her] colleagues on both sides of the aisle to find ways to make progress on immigration reform.\u201d Thank you for your complacency Senator. I don\u2019t know how immigration progress will be made with isolationist Nazis tbh \ud83e\uddd0 I don\u2019t get why Dems are taking conservative immigration stances when in my mind the problem is we are too strict on immigration, and should let in more people. There, birth rate \u201ccrisis\u201d solved too! What I want to know now is, what are the Senator\u2019s plans for retirement?\n\nRepresentative DelBene\u2019s office thinks I \u201cmay be pleased to know that [she] joined [her] Democratic colleagues in sending a letter to President Trump expressing serious concern about Mr. Abrego Garcia\u2019s case and requesting an immediate update on his wellbeing and how the administration plans to ensure his safe return to the United States.\u201d I am, in fact, not pleased. I am the opposite of pleased. Is she fucking with me? Does she not go on the Internet? In what world am I satisfied that she signed a strongly worded letter and called it good?\n\nMy resentment towards 80yo politicians who won\u2019t retire and pass their power to the next generation is reaching a rolling boil as the Democrats in charge apparently don\u2019t have an ounce of urgency left in their moldering bodies. I have to think it\u2019s intentional, especially given the way Nancy and co blocked Millennial AOC from a position of power in favor of an old dude with cancer who just died from it (\u201cGerry\u2019s a young 74, cancer notwithstanding\u201d \ud83d\ude44\ud83d\ude44\ud83d\ude44); I think Boomer politicians know how badly they\u2019ve fucked over everyone younger than them, and they will literally hold power until they die to prevent facing justice. (Yeah I\u2019m making it generational, they\u2019ve been demonizing Millennials to avoid looking in the mirror for decades.) They booted 25yo David Hogg from DNC leadership as punishment for suggesting the party should not always support incumbents, making a mockery of DEI principles by using gender equality rules to invalidate his election.\n\nA friend and I just looked it up: Nancy Pelosi was first elected in 1987. She has been in power for basically my entire life. And I am turning 40. Her legacy, in my mind? Losing women the right to bodily autonomy, for not taking Republicans seriously and doing anything about it when she could have. OK fine she did get us the ACA. Thanks lady. But it\u2019s time to go home, get some rest, and let someone else have a turn! (Sorry Bernie, I love ya man, but you too!!!)\n\nI think those in charge should have some stakes in the game. If you\u2019re already past the actuarial tables, you shouldn\u2019t get to make decisions for the rest of us who are going to live in the world our politicians make for potentially decades. (I expect the Supreme Court will be ultra conservative for the rest of my life because Dems wouldn\u2019t dare trying to expand it.) Be a mentor, yes, please! But give up the power. Normal retirement age is 67. I don\u2019t think it\u2019s ageist to suggest it might be time for someone two decades past retirement age to pass the torch.\n\nGive me a party with energy, give me politicians who fight! Oh, for an opposition party \ud83e\udd7a I don\u2019t have to agree 100% with them, if they\u2019d just have some gumption. Give me AOC energy, Shawn Fain energy, Pete Buttigieg (!) energy, Cory Booker energy, Chris Van Hollen energy, Kat Abughazaleh energy. Not whatever this pathetic excuse for \u201cleadership\u201d is. Walz had the energy and they told him to stuff it. Brilliant strategy guys.\n\nPlease, please tell me someone is primarying all my Federal reps next election, because I will vote for anyone who shows some semblance of spine \ud83d\ude21", "label": 1}
{"title": "My Favorite New York Times Interactives", "url": "https://lifeofpablo.com/blog/my-favorite-new-york-times-interactives", "content": "My Favorite New York Times Interactives\n\nThis post was written in English (en_US).\n\nScreenshot of 2021: The Year in Visual Stories and Graphics | Copyright The New York Times\n\nMany people who know me personally, know that I love the . I'll get straight to the point... I love their Interactives. New York Times (NY Times) interactive articles allow me to visualize and get a deeper understanding of what is trying to be conveyed. Some interactives are very simple to very complex in nature. The interactive visuals are amazing.\n\nSince I am really into photography, I really appreciate the the beautiful photography across the New York Times but the interactives take the cake! I will literally drool hours upon hours just reading and browsing the beautiful photography. The videography was amazing as well. I used some videos from the interactives as examples or inspiration. on how to become better at using the camera, and make better content, overall.\n\nThe New York Times is great at their web development. It really has helped me become better at designing on the web and becoming more visual. I am no where near any good web designer/web developer.\n\nMy first interactive, which I don't think they called interactive at the time, was back in 2012.\n\nThe categories of Interactives that I enjoy are (in no particular order):\n\nEnvironment\n\nEducation\n\nLatin America\n\nPolitics\n\nTechnology\n\nTravel", "label": 1}
{"title": "Building a better smart home", "url": "https://developers.googleblog.com/en/building-a-better-smart-home-expanding-access-for-developers-and-users/", "content": "Expanding access for app and device developers\n\nThe smart home industry is constantly evolving, and we're committed to staying at the forefront of innovation. At I/O 2024 we announced how we reimagined Google Home as a platform for all developers. Since then we have reached major milestones and our first partners have begun launching apps built on our platform.\n\nOur goal has remained the same - to make it easier for developers to create amazing experiences for users, and for those users to enjoy seamless connectivity and interoperability across all their devices. To that end, we are making a series of investments designed to enable all developers to build for the home.\n\n\n\nEmpowering developers with Home APIs\n\nWe believe that open platforms foster greater innovation, and it's clear that developers do too. We\u2019ve had nearly 2,000 developers sign up to learn more about the Home APIs since I/O 2024 and that's why we're thrilled to announce the public developer beta launch of Home APIs - today for Android, and in the coming months for iOS. These APIs provide developers with the tools they need to build richer and more integrated smart home experiences.\n\nThis isn't just theoretical, our partners are already making headway. Early access partners Eve, Nanoleaf, LG, ADT and Tuya Smart have launched new apps and features built with the Google Home Platform, with even more partners like Cync, GE Appliances, Yale, and Aqara releasing in the coming months. At CES 2025, our partners are showcasing device control and automation experiences in their apps built using these APIs. Hisense and Aqara will demo how the Home APIs have helped them to create and surface automations for different areas of their users\u2019 lives. SDMC will demonstrate how the Device & Structure API enables their users to control devices connected to Google Home directly in their apps.\n\n\n\nInvesting in connectivity and interoperability\n\nDelivering a truly smart home requires seamless connectivity made effortless via our platform. Our investments in Matter are a foundational layer to the smart home experience, and we continue to increase our investments in this area across multiple fronts:\n\nExpanding Matter support: A hub for Google Home is critical to unlocking Matter's fast, secure and reliable experience - it enables remote access and fully local control of Matter devices. We've significantly expanded the reach of hubs for Google Home by integrating the Google Home runtime into over 40 million devices, including Google Nest devices, Chromecasts, Google TV devices on Android 14 and eligible LG TVs. This means more users can enjoy the benefits of Matter connectivity, with less work. You can learn more details on our expanded Matter support here.\n\nIncreasing our investment to improve Matter quality: We firmly believe in Matter's potential to unify the smart home, and we are committed to its growth. That's why we, along with Apple and Samsung, are going beyond our existing commitments to Matter to further accelerate the improvement of quality for Matter. At Google, we're investing in Matter's growth in a number of new ways, including increasing development resources to enhance certification automation, interoperability scripting, and SDK bug fixes & maintenance.\n\nStreamlining certification with Connectivity Standards Alliance: In an effort to reduce time and costs for developers looking to certify software and products across multiple ecosystems, Google is excited to join Apple and Samsung in accepting Connectivity Standards Alliance Interop Lab test results. With this, you can now get certified with Works With Google Home for Matter devices without an additional certification process with Google.\n\nMaking Thread more universal: One key customer challenge with Matter today is that Thread devices require a Thread Border Router (TBR) in the home for control. To address this challenge at the ecosystem level and expand TBR availability, we\u2019ve partnered with MediaTek on the new Trinity chip (MT7903) that includes Wi-Fi, Bluetooth LE and Thread on a single system-on-a-chip (SoC). This makes it easier and more affordable for device OEMs to build Thread into all their new products.\n\n\n\nThe Future of the smart home\n\nWe're incredibly excited about the future of the smart home, and we believe these investments will pave the way for a new era of innovation and interoperability. By empowering developers and fostering a robust ecosystem, we're making the dream of a truly connected and intuitive smart home a reality.\n\nTo make sure you are staying up to date with the latest news, announcements, and resources from Google Home, be sure to subscribe to the Google Home Developer Newsletter.", "label": 0}
{"title": "Usability and safety updates to Google Auth Platform", "url": "https://developers.googleblog.com/en/usability-and-safety-updates-to-google-auth-platform/", "content": "Millions of developers rely on Google\u2019s identity platform for user authentication and the ability to authorize access to hundreds of APIs. Underpinning the platform is one of the world\u2019s largest implementations of the OAuth 2.0 protocol and related OpenID Connect standard, which provide a seamless, safe, and reliable way for developers to integrate with Google. We\u2019re excited to share some updates that will make the platform even more secure and easy to use.\n\n\n\nSimplified OAuth configuration in the Google Cloud Console\n\nDevelopers that use Sign in with Google for authentication or to obtain user authorization to call Google APIs need to register their apps and websites to create client credentials. For developers that use the Google Cloud Console, OAuth configuration pages previously lived in the APIs & Services section. Now, these pages have their own dedicated navigation section called Google Auth Platform. As part of this change, we\u2019ve made it easier to register new projects, reduced the time it takes to update app configurations, and added more helpful guidance for developers. Stay tuned for more improvements in the coming months, including a better onboarding wizard, simplified OAuth scope management, and changes to make app verification faster and more transparent.\n\nFor developers who use OAuth capabilities through other consoles like Firebase or Apps Script, your experience on those products remains unchanged.\n\n\n\nChange to how OAuth client secrets are displayed\n\nSome OAuth clients are required to use a \u201csecret\u201d when making authentication and authorization requests. The client secret is like a password for a website or application, so it\u2019s critical to protect these strings to ensure the security and privacy of user accounts and data.\n\nHistorically, developers have been able to view and download their own client secrets in the Google Cloud Console, Firebase Console, and other places across Google developer products. Starting in June, we\u2019ll start masking OAuth secrets in the client management pages of the Google Cloud Console. As an aid to help identify them, developer consoles will show the last few characters.", "label": 0}
{"title": "Gemini 2.5 for robotics and embodied intelligence", "url": "https://developers.googleblog.com/en/gemini-25-for-robotics-and-embodied-intelligence/", "content": "The latest generation of Gemini models, 2.5 Pro and Flash, are unlocking new frontiers in robotics. Their advanced coding, reasoning, and multimodal capabilities, now combined with spatial understanding, provide the foundation for the next generation of interactive and intelligent robots. This post explores how developers can leverage Gemini 2.5 to build sophisticated robotics applications. We'll provide practical examples with prompts to show using Gemini 2.5 and the Live API for: Semantic scene understanding for complex queries: Identify and label objects from robot camera feeds. Understand complex queries through multimodal reasoning. Combine spatial reasoning with code generation to control robots: Use the robot's API to call functions and bring task plans to life. Build interactive robotics applications with the Live API: Convert voice commands into executable robot plans. In March, we launched our Gemini Robotics models, including Gemini Robotics-ER, our advanced embodied reasoning model optimized for the unique demands of robotics applications. We\u2019re also excited to share how our Gemini Robotics trusted testers are already demonstrating the power of Gemini in robotics applications. We are including examples from Agile Robots, Agility Robotics, Boston Dynamics, and Enchanted Tools. Join the Gemini Robotics-ER trusted tester program waitlist.\n\nSemantic scene understanding for complex queries Reasoning about the physical world is at the core of general and robust control. Gemini 2.5 represents a step in this direction with its improved ability to reason multimodally. Below we share two examples, utilizing Gemini\u2019s pointing and object detection capabilities. Pointing allows a model to refer to entities or parts of entities precisely, and locate them in space. Gemini 2.5 Pro is able to reason about the entities it is pointing to, opening new opportunities for interacting with images. For example, Gemini 2.5 Pro is able to reason about empty space in the context of a supermarket display, knowing that this indicates restocking may be needed. In the example below, Gemini identifies the baby eggplant needs restocking. Gemini 2.5 Pro also shows a nascent ability to locate and read information from that location, as illustrated in the gauge example. Example 1: Gemini 2.5 can locate objects in the scene based on fine-grained language descriptions, for example, find a shelf that needs restocking. Prompt: Point to one bin on the shelf that needs restocking. The answer should follow the json format: [{\"point\": <point>, \"label\": <label1>}, ...]. The points are in [y, x] format normalized to 0-1000. Input image with response overlay:\n\nExample 2: Gemini 2.5 can locate small objects in the scene and estimate states of those objects. For example, it can read gauges. Prompt: Point to all the round gauges. The answer should follow the json format: [{\"point\": <point>, \"label\": <label1>}, ...]. The points are in [y, x] format normalized to 0-1000. Input image with response overlay:\n\nPrompt: What is the reading on the middle gauge? Response: Based on the close-up view, the round gauge in the center-left of the image appears to be reading 0. The needle is pointing directly at the \"0\" mark on the dial.\n\nGoing beyond object-centric perception Gemini 2.5 is able to accurately track multiple objects across time and detect open ended concepts like \u2018a spill\u2019. Gemini 2.5 can be prompted into trajectory prediction in the form of a sequence of points. Example 1: Gemini 2.5 can generate bounding boxes for each frame in a video and be visualized like below. Prompt: Detect green bowl, crab, wallet, pink bowl, phone, return a json array with keys box_2d and label. (executed per frame). Input image with response overlay:\n\nSorry, your browser doesn't support playback for this video\n\nExample 2: Gemini 2.5 can detect open-ended concepts relevant to robotics, requiring commonsense knowledge and context specific reasoning. For example, a helpful robot needs to understand the concept of a \u201cspill\u201d. Prompt: 1) Show me the bounding box of spill. Return in a json array with keys box_2d and label. 2) Give the segmentation masks for the spill. Output a JSON list of segmentation masks where each entry contains the 2D bounding box in the key \"box_2d\", the segmentation mask in key \"mask\", and the text label in the key \"label\". Input image with response overlay:\n\nExample 3: Gemini 2.5 can be prompted into trajectory prediction in the form of a sequence of points. Prompt: Generate a robot arm trajectory of 10 points to move the cloth to the spill. The answer should follow the json format: [{\"point\": <point>, \"label\": <label1>}, ...]. The points are in [y, x] format normalized to 0-1000. Input image with response overlay:\n\nUsing spatial understanding and code generation to control robots Gemini 2.5 can utilize its underlying spatial understanding to control robots through code generation. By providing Gemini 2.5 with a robot control API, it can apply advanced capabilities in scene understanding, object manipulation, and code writing together to perform tasks zero-shot, with no additional training. Example 1 below showcases code-generation for \u201cPut the banana in the bowl\u201d. It gives Gemini access to a robot control API and shows how the model leverages its spatial understanding, thinking, and code generation capabilities to select the appropriate API calls and arguments given the task. Gemini 2.5 generates 2 different feasible plans for putting the banana in the bowl. The first solution is to simply pick up the banana, move it above the bowl, and drop it. The second solution lifts the banana, moves the bowl below the banana, and then drops the banana. Example 1: Gemini 2.5 Generate high-level planning code for pick-and-place tasks with different strategies. Prompt: You are given a robot control API and example code below:\n\n# Provided API class RobotAPI: def detect_object(self, obj: str) -> Detection \"\"\"Detects the given object's XYZ location\"\"\" def get_grasp_position_and_euler_orientation(self, gripper: str, object_name: str) -> tuple[numpy.ndarray, numpy.ndarray]: \"\"\"Returns the grasp position and orientation for the given object and gripper from the Gemini Robotics ER model or other grasping primitives. def move_gripper_to(self, gripper: str, position: numpy.ndarray, orientation: numpy.ndarray): \"\"\"Moves the gripper to the given position and orientation.\"\"\" def move_gripper_to_safe_position(self, gripper: str): \"\"\"Moves the given gripper to a safe position out of the table area.\"\"\" def open_gripper(self, gripper: str): \"\"\"Opens the given gripper.\"\"\" def close_gripper(self, gripper: str): \"\"\"Closes the given gripper.\"\"\" robot=RobotAPI() # Example code robot.open_gripper(gripper='RIGHT') banana_detection = robot.detect_object(obj='banana') banana_position = banana_detection.position print(f\"Detected banana position: {banana_position}\") # Gemini uses this to determine which gripper to use. grasp_position, grasp_orientation = robot.get_grasp_position_and_euler_orientation(gripper='RIGHT', object_name='banana') pre_grasp_position = grasp_position + np.array([0, 0, 0.1]) # Add 10cm to the Z-coordinate for pre-grasp robot.move_gripper_to(gripper='RIGHT', position=pre_grasp_position, orientation=grasp_orientation) robot.move_gripper_to(gripper='RIGHT', position=grasp_position, orientation=grasp_orientation) robot.close_gripper(gripper='RIGHT') Python Copied\n\nNow generate code for the task \u201cput the banana in the bowl\u201d. Response 1 with results of code executed: Reasoning (abbreviated) 1: The current position of the right gripper is [-0.04, -0.028, 0.176] . 2: The bowl's detected position is [-0.03839983, -0.02811164, 0.0073104] . 3: To place the banana inside the bowl, the gripper's Z-coordinate needs to be lowered. I will use the bowl's detected X and Y coordinates for the target position. For the Z-coordinate, I will use the bowl's detected Z-coordinate ( 0.0073104 ) and add a small offset (e.g., 0.03m) to ensure the banana is placed inside the bowl and not through the table. This would result in a target Z of approximately 0.007 + 0.03 = 0.037m . This Z-coordinate is higher than the bowl's base but lower than the typical height of a banana when standing on the table, ensuring it's placed *into* the bowl. Code Snippet:\n\n# Detect the bowl's position (re-detect to ensure fresh data, though it's unlikely to have moved) bowl_detection = robot.detect_object('bowl') if bowl_detection.position is None: raise ValueError(\"Bowl not detected.\") # Define the target Z-coordinate for dropping the banana into the bowl # Use the bowl's Z-position and add an offset to place the banana inside. # The banana's original Z was ~0.032, bowl's Z is ~0.007. # A target Z of 0.037 (0.007 + 0.03) should place it inside the bowl. drop_z = bowl_detection.position[2] + 0.03 Python Copied\n\nSorry, your browser doesn't support playback for this video\n\nResponse 2 with results of code executed: Reasoning (abbreviated) # Re-planning due to reachability constraints: The RIGHT arm cannot reach the bowl's current position. # The plan will now involve the LEFT arm moving the bowl to the RIGHT arm's reachable area. # The RIGHT arm can reach x-coordinates from 0.04m to 0.40m. I will move the bowl to an x-coordinate of 0.10m, which is well within the RIGHT arm's reach and also allows for some clearance from the center.\n\nCode Snippet:\n\ntarget_bowl_position = [-0.07274145, -0.08317622, 0.00720479] # Original bowl position # Move the bowl to an x-coordinate reachable by the RIGHT arm, keeping the same y and z (lifted) # Let's choose x = 0.10m for the bowl, which is within the right arm's reach (0.04 to 0.40) # Keep the lifted height at 0.20m move_bowl_position = [0.10, bowl_detection.position[1], 0.20] robot.move_gripper_to('LEFT', move_bowl_position, current_left_gripper_orientation) Python Copied\n\nSorry, your browser doesn't support playback for this video\n\nGemini 2.5 can also effectively utilize a small number of in-context examples to perform more dexterous robot control tasks. In the two examples shown below of an ALOHA robot packing boxes and folding a dress, 10 demonstrations consisting of interleaved reasoning and robot actions for each task were added to Gemini\u2019s context. We\u2019ve created open-source code showing how to do this using Gemini, including examples of the input demonstrations. This enables robots to be taught and deployed on the spot. See the Colab. Example 2: Gemini 2.5 (Flash) utilizes a small number of in-context examples to perform more dexterous robot control tasks. Prompt: see colab. Response with results of code executed:\n\nSorry, your browser doesn't support playback for this video\n\nSorry, your browser doesn't support playback for this video\n\nBuilding interactive robotics applications with the Live API The Live API for realtime streaming was recently introduced and can be used to build interactive applications that let people control robots using their voice. Intuitive human-robot-interaction is an important aspect of making robots that are easy and safe to use. We recently showcased an interactive Gemini Robotics demo at I/O 2025, which was built around Live API for voice interaction and function calling. Live API supports both audio and video as input modalities, and audio / text as output modalities. This allows you to send both voice input and the robot camera feed to the Live API. This is even more powerful when combined with tool use. Tool use allows Live API to go beyond just conversation by enabling it to perform actions in the real-world while maintaining a real time connection. For example, the robot APIs defined above can be defined as function calls including robot.open_gripper() , robot.close_gripper() and robot.move_gripper_to() . After they are defined as tool calls, they can be integrated into the workflow where people can interact with the robot using voice in real time. Developers can get started on GitHub, and refer to API documentation for function calling features.\n\nSorry, your browser doesn't support playback for this video \ud83d\udd0a Demonstration of a realtime web console for robotics built with Live API, right click to open in a new tab for audio.\n\nSafety The 2.5 Pro and 2.5 Flash models demonstrate robust performance on the ASIMOV Multimodal and Physical Injury benchmarks released along with the Gemini Robotics tech report, exhibiting accuracy comparable to that of 2.0 models. Beyond the ASIMOV benchmarks, the 2.5 Pro and 2.5 Flash models also exhibit excellent performance in rejecting prompts that attempt to leverage embodied reasoning capabilities while violating safety policies such as promoting harmful stereotypes, discrimination, or endangerment of minors. Following rigorous evaluation against such synthetically generated adversarial prompts, 2.5 Pro and Flash demonstrated near-zero violation rates.\n\nHow Gemini is being used today for Robotics In March we released the Gemini Robotics-ER model and we\u2019re already inspired by how the community is using it for robotics applications. Check out these examples of interactivity, perception, planning, and function calling from our trusted testers: Agile Robots, Agility Robotics, Boston Dynamics, and Enchanted Tools.\n\nSorry, your browser doesn't support playback for this video\n\nSorry, your browser doesn't support playback for this video\n\nSorry, your browser doesn't support playback for this video\n\nSorry, your browser doesn't support playback for this video", "label": 0}
{"title": "I'm moving to San Francisco", "url": "https://lifeofpablo.com/blog/i-m-moving-to-san-francisco", "content": "I'm moving to San Francisco\n\nPablo in SF\n\nThis post was written in English (en_US).\n\nI am sharing that I am moving to San Francisco this week! I've accepted a new job out there and I'm so stoked! It still hasn't hit me that I'm moving to a different part of California.\n\nMy career path will be to continue working in IT but leaving education. Education will always have a special place in my heart. I'm excited about the new path for my career and acquiring some new skills! Being a nerd is fun!\n\nSan Francisco and the Bay Area are such places of beauty. I've dreamt of living in a city like this one since I was a kid. I've traveled and stayed in places throughout the world for extended periods but there's nothing like being in a place you'll be at for a long time.\n\nWhy San Francisco other than your new job, Pablo? Yes, thank you for asking! I'd love to answer your question!\n\nLet me tell you that I'm curious and I like to wander off to other places. Wandering off to different parts of California is no exception. While living in Sacramento, I would hop on the train on holiday breaks and weekends to San Francisco. Who doesn't like taking a train? I have friends that I visit quite regularly. I know my way around the city. They would show me around but also I would go off on my adventure. Do you see a pattern here? I'm excited to see my friends who live down the street or across town. I won't need to travel two hours or more to get there.\n\nI'll be close to outdoor adventures, go to my favorite hackerspace and more!\n\nSan Francisco is a place where I want to continue to flourish as an individual. I'm just getting started! It will grow my career, my lifestyle choices, my health, increased network opportunities, my love for technology, and my love for public transit. I've learned so much about myself being in the city. Now that I get to live there, I will continue to learn more! Don't forget about walking everywhere as well!\n\nI'm excited to be working on some startup ideas in my downtime.\n\nI can't wait for my friends, and family to visit me! San Francisco is easy to fly into! For my Nebraska people, Interstate 80 will take you to me! You can't miss it unless you drive into the bay. Isn't there a scene like this in The Office?\n\nI still find it hard to believe that this guy from Hastings, Nebraska has made it to San Francisco! I'm going to be living in a major city! The initial idea of moving to California seemed out of this world! Moving to San Francisco takes the cake! I don't regret venturing out of Nebraska. I'm more happy than I've ever been. This is your sign to explore other parts as well. You won't regret it!\n\nI'm sad I'm leaving Sacramento. It received me with open arms and I am extremely grateful for the people I have me. Every single one of you has had an impact on me.\n\nTo my friend on the other side of the country, I'll be waving while you drink some tea!\n\nIt's time to unpack boxes and enjoy the cooler weather(see picture above). Hooray!!", "label": 1}
{"title": "Railyard: how we rapidly train machine learning models with Kubernetes", "url": "https://stripe.com/blog/railyard-training-models", "content": "Stripe uses machine learning to respond to our users\u2019 complex, real-world problems. Machine learning powers Radar to block fraud, and Billing to retry failed charges on the network. Stripe serves millions of businesses around the world, and our machine learning infrastructure scores hundreds of millions of predictions across many machine learning models. These models are powered by billions of data points, with hundreds of new models being trained each day. Over time, the volume, quality of data, and number of signals have grown enormously as our models continuously improve in performance.\n\nRunning infrastructure at this scale poses a very practical data science and ML problem: how do we give every team the tools they need to train their models without requiring them to operate their own infrastructure? Our teams also need a stable and fast ML pipeline to continuously update and train new models as they respond to a rapidly changing world. To solve this, we built Railyard, an API and job manager for training these models in a scalable and maintainable way. It\u2019s powered by Kubernetes, a platform we\u2019ve been working with since late 2017. Railyard enables our teams to independently train their models on a daily basis with a centrally managed ML service.\n\nIn many ways, we\u2019ve built Railyard to mirror our approach to products for Stripe\u2019s users: we want teams to focus on their core work training and developing machine learning models rather than operating infrastructure. In this post, we\u2019ll discuss Railyard and best practices for operating machine learning infrastructure we\u2019ve discovered while building this system.\n\nEffective machine learning infrastructure for organizations\n\nWe\u2019ve been running Railyard in production for a year and a half, and our ML teams have converged on it as their common training environment. After training tens of thousands of models on this architecture over that period, here are our biggest takeaways:\n\nBuild a generic API, not tied to any single machine learning framework. Teams have extended Railyard in ways we did not anticipate. We first focused on classifiers, but teams have since adopted the system for applications such as time series forecasting and word2vec style embeddings..\n\nTeams have extended Railyard in ways we did not anticipate. We first focused on classifiers, but teams have since adopted the system for applications such as time series forecasting and word2vec style embeddings.. A fully managed Kubernetes cluster reduces operational burden across an organization. Railyard interacts directly with the Kubernetes API (as opposed to a higher level abstraction), but the cluster is operated entirely by another team. We\u2019re able to learn from their domain knowledge to keep the cluster running reliably so we can focus on ML infrastructure.\n\nRailyard interacts directly with the Kubernetes API (as opposed to a higher level abstraction), but the cluster is operated entirely by another team. We\u2019re able to learn from their domain knowledge to keep the cluster running reliably so we can focus on ML infrastructure. Our Kubernetes cluster gives us great flexibility to scale up and out. We can easily scale our cluster volume when we need to train more models, or quickly add new instance types when we need additional compute resources.\n\nWe can easily scale our cluster volume when we need to train more models, or quickly add new instance types when we need additional compute resources. Centrally tracking model state and ownership allows us to easily observe and debug training jobs. We\u2019ve moved from asking, \u201cDid you save the output of your job anywhere so we can look at?\u201d to \u201cWhat\u2019s your job ID? We\u2019ll figure the rest out.\u201d We observe aggregate metrics and track the overall performance of training jobs across the cluster.\n\nWe\u2019ve moved from asking, \u201cDid you save the output of your job anywhere so we can look at?\u201d to \u201cWhat\u2019s your job ID? We\u2019ll figure the rest out.\u201d We observe aggregate metrics and track the overall performance of training jobs across the cluster. Building an API for model training enables us to use it everywhere.Teams can call our API from any service, scheduler, or task runner. We now use Railyard to train models using an Airflow task definition as part of a larger graph of data jobs.\n\nThe Railyard architecture\n\nIn the early days of model training at Stripe, an engineer or data scientist would SSH into an EC2 instance and manually launch a Python process to train a model. This served Stripe\u2019s needs at the time, but had a number of challenges and open questions for our Machine Learning Infrastructure team to address as the company grew:\n\nHow do we scale model training from ad-hoc Python processes on shared EC2 instances to automatically training hundreds of models a day?\n\nHow do we build an interface that is generic enough to support multiple training libraries, frameworks, and paradigms while remaining expressive and concise?\n\nWhat metrics and metadata do we want to track for each model run?\n\nWhere should training jobs be executed?\n\nHow do we scale different compute resource needs (CPU, GPU, memory) for different model types?\n\nOur goal when designing this system was to enable our data scientists to think less about how their machine learning jobs are run on our infrastructure, and instead focus on their core inquiry. Machine learning workflows typically involve multiple steps that include loading data, training models, serializing models, and persisting evaluation data. Because Stripe runs its infrastructure in the cloud, we can manage these processes behind an API: this reduces cognitive burden for our data science and engineering teams and moves local processes to a collaborative, shared environment. After a year and a half of iteration and collaboration with teams across Stripe, we\u2019ve converged on the following system architecture for Railyard. Here\u2019s a high-level overview:\n\nRailyard runs on a Kubernetes cluster and pairs jobs with the right instance type.\n\nRailyard provides a JSON API and is a Scala service that manages job history, state, and provenance in a Postgres database. Jobs are executed and coordinated using the Kubernetes API, and our Kubernetes cluster provides multiple instance types with different compute resources. The cluster can pair jobs with the right instance type: for example, most jobs default to our high-CPU instances, data-intensive jobs run on high-memory instances, and specialized training jobs like deep learning run on GPU instances.\n\nWe package the Python code for model training using Subpar, a Google library that creates a standalone executable that includes all dependencies in one package. This is included in a Docker container, deployed to the AWS Elastic Container Registry, and executed as a Kubernetes job. When Railyard receives an API request, it runs the matching training job and logs are streamed to S3 for inspection. A given job will run through multiple steps, including fetching training and holdout data, training the model, and serializing the trained model and evaluation data to S3. These training results are persisted in Postgres and exposed in the Railyard API.\n\nRailyard\u2019s API design\n\nThe Railyard API allows you to specify everything you need to train a machine learning model, including data sources and model parameters. In designing this API we needed to answer the following question: how do we provide a generic interface for multiple training frameworks while remaining expressive and concise for users?\n\nWe iterated on a few designs with multiple internal customers to understand each use case. Some teams only needed ad-hoc model training and could simply use SQL to fetch features, while others needed to call an API programmatically hundreds of times a day using features stored in S3. We explored a number of different API concepts, arriving at two extremes on either end of the design spectrum.\n\nOn one end, we explored designing a custom DSL to specify the entire training job by encoding scikit-learn components directly in the API itself. Users could include scikit-learn pipeline components in the API specification and would not need to write any Python code themselves.\n\nOn the other end of the spectrum we reviewed designs to allow users to write their own Python classes for their training code with clearly defined input and output interfaces. Our library would be responsible for both the necessary inputs to train models (fetching, filtering, and splitting training and test data) and the outputs of the training pipeline (serializing the model, and writing evaluation and label data). The user would otherwise be responsible for writing all training logic.\n\nIn the end, any DSL-based approach ended up being too inflexible: it either tied us to a given machine learning framework or required that we continuously update the API to keep pace with changing frameworks or libraries. We converged on the following split: our API exposes fields for changing data sources, data filters, feature names, labels, and training parameters, but the core logic for a given training job lives entirely in Python.\n\nHere\u2019s an example of an API request to the Railyard service:\n\n{ // What does this model do? \"model_description\": \"A model to predict fraud\", // What is this model called? \"model_name\": \"fraud_prediction_model\", // What team owns this model? \"owner\": \"machine-learning-infrastructure\", // What project is this model for? \"project\": \"railyard-api-blog-post\", // Which team member is training this model? \"trainer\": \"robstory\", \"data\": { \"features\": [ { // Columns we\u2019re fetching from Hadoop Parquet files \"names\": [\"created_at\", \"charge_type\", \"charge_amount\", \"charge_country\", \"has_fraud_dispute\"], // Our data source is S3 \"source\": \"s3\", // The path to our Parquet data \"path\": \"s3://path/to/parquet/fraud_data.parq\" } ], // The canonical date column in our dataset \"date_column\": \"created_at\", // Data can be filtered multiple times \"filters\": [ // Filter out data before 2018-01-01 { \"feature_name\": \"created_at\", \"predicate\": \"GtEq\", \"feature_value\": { \"string_val\": \"2018-01-01\" } }, // Filter out data after 2019-01-01 { \"feature_name\": \"created_at\", \"predicate\": \"LtEq\", \"feature_value\": { \"string_val\": \"2019-01-01\" } }, // Filter for charges greater than $10.00 { \"feature_name\": \"charge_amount\", \"predicate\": \"Gt\", \"feature_value\": { \"float_val\": 10.00 } }, // Filter for charges in the US or Canada { \"feature_name\": \"charge_country\", \"predicate\": \"IsIn\", \"feature_value\": { \"string_vals\": [\"US\", \"CA\"] } } ], // We can specify how to treat holdout data \"holdout_sampling\": { \"sampling_function\": \"DATE_RANGE\", // Split holdout data from 2018-10-01 to 2019-01-01 // into a new dataset \"date_range_sampling\": { \"date_column\": \"created_at\", \"start_date\": \"2018-10-01\", \"end_date\": \"2019-01-01\" } } }, \"train\": { // The name of the Python workflow we're training \"workflow_name\": \"StripeFraudModel\", // The list of features we're using in our classifier \"classifier_features\": [ \"charge_type\", \"charge_amount\", \"charge_country\" ], \"label\": \"is_fraudulent\", // We can include hyperparameters in our model \"custom_params\": { \"objective\": \"reg:linear\", \"max_depth\": 6, \"n_estimators\": 500, \"min_child_weight\": 50, \"learning_rate\": 0.02 } } } ~\n\nWe learned a few lessons while designing this API:\n\nBe flexible with model parameters. Providing a free-form custom_params field that accepts any valid JSON was very important for our users. We validate most of the API request, but you can\u2019t anticipate every parameter a machine learning engineer or data scientist needs for all of the model types they want to use. This field is most frequently used to include a model\u2019s.\n\nProviding a free-form field that accepts any valid JSON was very important for our users. We validate most of the API request, but you can\u2019t anticipate every parameter a machine learning engineer or data scientist needs for all of the model types they want to use. This field is most frequently used to include a model\u2019s. Not providing a DSL was the right choice (for us).Finding the sweet spot for expressiveness in an API for machine learning is difficult, but so far the approach outlined above has worked out well for our users. Many users only need to change dates, data sources, or hyperparameters when retraining. We haven\u2019t gotten any requests to add more DSL-like features to the API itself.\n\nThe Python workflow\n\nStripe uses Python for all ML model training because of its support for many best-in-class ML libraries and frameworks. When the Railyard project started we only had support for scikit-learn, but have since added XGBoost, PyTorch, and FastText. The ML landscape changes very quickly and we needed a design that didn\u2019t pick winners or constrain users to specific libraries. To enable this extensibility, we defined a framework-agnostic workflow that presents an API contract with users: we pass data in, you pass a trained model back out, and we\u2019ll score and serialize the model for you. Here\u2019s what a minimal Python workflow looks like:\n\nclass StripeFraudModel(StripeMLWorkflow): # A basic model training workflow: all workflows inherit # Railyard\u2019s StripeMLWorkflow class def train(self, training_dataframe, holdout_dataframe): # Construct an estimator using specified hyperparameters estimator = xgboost.XGBRegressor(**self.custom_params) # Serialize the trained model once training is finished; # we're using an in-house serialization library. serializable_estimator = stripe_ml.make_serializable(estimator) # Train our model fitted_model = serializable_estimator.fit( training_dataframe[self.classifier_features], training_dataframe[self.classifier_label] ) # Hand our fitted model back to Railyard to serialize return fitted_model ~\n\nTeams start adopting Railyard with an API specification and a workflow that defines a train method to train a classifier with the data fetched from the API request. The StripeMLWorkflow interface supports extensive customization to adapt to different training approaches and model types. You can preprocess your data before it gets passed in to the train function, define your own data fetching implementation, specify how you want training/holdout data to be scored, and run any other Python code you need. For example, some of our deep learning models have custom data fetching code to stream batches of training data for model training. When your training job finishes you\u2019ll end up with two output: a model identifier for your serialized model that can be put into production, and your evaluation data in S3.\n\nIf you build a machine learning API specification, here are a few things to keep in mind:\n\nInterfaces are important. Users will want to load and transform data in ways you didn\u2019t anticipate, train models using unsupported patterns, and write out unfamiliar types of evaluation data. It\u2019s important to provide standard API interfaces like fetch_data, preprocess, train, and write_evaluation_data that specify some standard data containers (e.g., Pandas DataFrame and Torch Dataset) but are flexible in how they are generated and used.\n\nUsers will want to load and transform data in ways you didn\u2019t anticipate, train models using unsupported patterns, and write out unfamiliar types of evaluation data. It\u2019s important to provide standard API interfaces like fetch_data, preprocess, train, and write_evaluation_data that specify some standard data containers (e.g., Pandas DataFrame and Torch Dataset) but are flexible in how they are generated and used. Users should not need to think about model serialization or persistence. Reducing their cognitive burden makes their lives easier and gives them more time to be creative and focus on modeling and feature engineering. Data scientists and ML engineers already have enough to think about between feature engineering, modeling, evaluation, and more. They should be able to train and hand over their model to your scoring infrastructure without ever needing to think about how it gets serialized or persisted.\n\nReducing their cognitive burden makes their lives easier and gives them more time to be creative and focus on modeling and feature engineering. Data scientists and ML engineers already have enough to think about between feature engineering, modeling, evaluation, and more. They should be able to train and hand over their model to your scoring infrastructure without ever needing to think about how it gets serialized or persisted. Define metrics for each step of the training workflow.Make sure you\u2019re gathering fine-grained metrics for each training step: data loading, model training, model serialization, evaluation data persistence, etc. We store high-level success and failure metrics that can be examined by team, project, or the individual machine performing the training. On a functional level,our team uses these metrics to debug and profile long-running or failed jobs, and provide feedback to the appropriate team when there\u2019s a problem with a given training job. And on a collaborative level, these metrics have changed how our team operates. Moving from a reactive stance (\u201cMy model didn\u2019t train, can you help?\u201d) to a proactive one (\u201cHey, I notice your model didn\u2019t train, here\u2019s what happened\u201d) has helped us be better partners to the many teams we work with.\n\nScaling Kubernetes\n\nRailyard coordinates hundreds of machine learning jobs across our cluster, so effective resource management across our instances is crucial. The first version of Railyard simply ran individual subprocesses from the Scala service that manages all jobs across our cluster. We would get a request, start Java\u2019s ProcessBuilder, and kick off a subprocess to build a Python virtualenv and train the model. This basic implementation allowed us to quickly iterate on our API in our early days, but managing subprocesses wasn\u2019t going to scale very well. We needed a proper job management system that met a few requirements:\n\nScaling the cluster quickly for different resource/instance types\n\nRouting models to specific instances based on their resource needs\n\nJob queueing to prioritize resources for pending work\n\nLuckily, our Orchestration team had been working hard to build a reliable Kubernetes cluster and suggested this new cluster would be a good platform for Railyard\u2019s needs. It was a great fit; a fully managed Kubernetes cluster provides all of the pieces we needed to meet our system\u2019s requirements.\n\nContainerizing Railyard\n\nTo run Railyard jobs on Kubernetes, we needed a way to reliably package our Python code into a fully executable binary. We use Google\u2019s Subpar library which allows us to package all of our Python requirements and source code into a single .par file for execution. The library also includes support for the Bazel build system out of the box. Over the past few years, Stripe has been moving many of its builds to Bazel; we appreciate its speed, correctness, and flexibility in a multi-language environment.\n\nWith Subpar you can define an entrypoint to your Python executable and Bazel will build your .par executable to bundle into a Dockerfile:\n\npar_binary( name = \"railyard_train\", srcs = [\"@.../ml:railyard_srcs\"], data = [\"@.../ml:railyard_data\"], main = \"@.../ml:railyard/train.py\", deps = all_requirements,) ~\n\nWith the Subpar package built, the Kubernetes command only needs to execute it with Python:\n\ncommand: [\"sh\"] args: [\"-c\", \"python /railyard_train.par\"] ~\n\nWithin the Dockerfile we package up any other third-party dependencies that we need for model training, such as the CUDA runtime to provide GPU support for our PyTorch models. After our Docker image is built, we deploy it to AWS\u2019s Elastic Container Repository so our Kubernetes cluster can fetch and run the image.\n\nRunning diverse workloads\n\nSome machine learning tasks can benefit from a specific instance type with resources optimized for a given workload. For example, a deep learning task may be best suited for a GPU instance while fraud models that employ huge datasets should be paired with high-memory instances. To support these mixed workloads we added a new top-level field to the Railyard API request to specify the compute resource for jobs running on Kubernetes:\n\n{ \"compute_resource\": \"GPU\" } ~\n\nRailyard supports training models on CPU, GPU, or memory-optimized instances. Models for our largest datasets can require hundreds of gigabytes of memory to train, while our smaller models can train quickly on smaller (and less expensive) instance types.\n\nScheduling and distributing jobs\n\nRailyard exerts a fine-grained level of control on how Kubernetes distributes jobs across the cluster. For each request, we look at the requested compute resource and set both a Kubernetes Toleration and an Affinity to specify the type of node that we would like to run on. These parameters effectively tell the Kubernetes cluster:\n\nthe affinity , or which nodes the job should run on\n\n, or which nodes the job should run on the toleration, or which nodes should be reserved for specific tasks\n\nKubernetes will use the affinity and toleration properties for a given Kubernetes pod to compute how jobs should be best distributed across or within each node.\n\nKubernetes supports per-job CPU and memory requirements to ensure that workloads don\u2019t experience resource starvation due to neighboring jobs on the same host. In Railyard, we determine limits for all jobs based on their historic and future expected usage of resources. In the case of high-memory or GPU training jobs, these limits are set so that each job gets an entire node to itself; if all nodes are occupied, then the scheduler will place the job in a queue. Jobs with less intensive resource requirements are scheduled on nodes to run in parallel.\n\nWith these parameters in place, we can lean on the Kubernetes resource scheduler to balance our jobs across available nodes. Given a set of job and resource requests, the scheduler will intelligently distribute those jobs to nodes across the cluster.\n\nOne year later: running at scale\n\nMoving our training jobs to a Kubernetes cluster has enabled us to rapidly spin up new resources for different models and expand the cluster to support more training jobs. We can use a single command to expand the cluster and new instance types only require a small configuration change. When the memory requirements of running jobs outgrew our CPU-optimized instance types, we started training on memory-optimized instances the very next day; when we observe a backlog of jobs, we can immediately expand the cluster to process the queue. Model training on Kubernetes is available to any data scientist or engineer at Stripe: all that\u2019s needed is a Python workflow and an API request and they can start training models on any resource type in the cluster.\n\nTo date, we\u2019ve trained almost 100,000 models on Kubernetes, with new models trained each day. Our fraud models automatically retrain on a regular basis using Railyard and Kubernetes, and we\u2019re steadily moving more of Stripe\u2019s models onto an automated retraining cycle. Radar\u2019s fraud model is built on hundreds of distinct ML models and has a dedicated service that trains and deploys all of those models on a daily cadence. Other models retrain regularly using an Airflow task that uses the Railyard API.\n\nWe\u2019ve learned a few key considerations for scaling Kubernetes and effectively managing instances:\n\nInstance flexibility is really important. Teams can have very different machine learning workloads. In any given day we might train thousands of time series forecasts, a long-running word embedding model, or a fraud model with hundreds of gigabytes of data. The ability to quickly add new instance types and expand the cluster are equally important for scalability.\n\nTeams can have very different machine learning workloads. In any given day we might train thousands of time series forecasts, a long-running word embedding model, or a fraud model with hundreds of gigabytes of data. The ability to quickly add new instance types and expand the cluster are equally important for scalability. Managing memory-intensive workflows is hard. Even using various instance sizes and a managed cluster, we still sometimes have jobs that run out of memory and are killed. This is a downside to providing so much flexibility in the Python workflow: modelers are free to write memory-intensive workflows. Kubernetes allows us to proactively kill jobs that are consuming too many resources, but it still results in a failed training job for the modeler. We\u2019re thinking about ways to better manage this, including smart retry behavior to automatically reschedule failed jobs on higher-capacity instances and moving to distributed libraries like dask-ml.\n\nEven using various instance sizes and a managed cluster, we still sometimes have jobs that run out of memory and are killed. This is a downside to providing so much flexibility in the Python workflow: modelers are free to write memory-intensive workflows. Kubernetes allows us to proactively kill jobs that are consuming too many resources, but it still results in a failed training job for the modeler. We\u2019re thinking about ways to better manage this, including smart retry behavior to automatically reschedule failed jobs on higher-capacity instances and moving to distributed libraries like dask-ml. Subpar is an excellent solution for packaging Python code. Managing Python dependencies can be tricky, particularly when you\u2019d like to bundle them as an executable that can be shipped to different instances. If we were to build this from scratch again we would probably take a look at Facebook\u2019s XARs, but Subpar is very compatible with Bazel and it\u2019s been running well in production for over a year.\n\nManaging Python dependencies can be tricky, particularly when you\u2019d like to bundle them as an executable that can be shipped to different instances. If we were to build this from scratch again we would probably take a look at Facebook\u2019s XARs, but Subpar is very compatible with Bazel and it\u2019s been running well in production for over a year. Having a good Kubernetes team is a force multiplier.Railyard could not have been a success without the support of our Orchestration team, which manages our Kubernetes cluster and pushes the platform forward for the whole organization. If we had to manage and operate the cluster in addition to building our services, we would have needed more engineers and taken significantly longer to ship.\n\nBuilding ML infrastructure\n\nWe\u2019ve learned that building common machine learning infrastructure enables teams across Stripe to operate independently and focus on their local ML modeling goals. Over the last year we\u2019ve used Railyard to train thousands of models spanning use cases from forecasting to deep learning. This system has enabled us to build rich functionality for model evaluation and design services to optimize hyperparameters for our models at scale.\n\nWhile there is a wealth of information available on data science and machine learning from the modeling perspective, there isn\u2019t nearly as much published about how companies build and operate their production machine learning infrastructure. Uber, Airbnb, and Lyft have all discussed how their infrastructure operates, and we\u2019re following their lead in introducing the design patterns that have worked for us. We plan to share more lessons from our ML architecture in the months ahead. In the meantime, we\u2019d love to hear from you: please let us know which lessons are most useful and if there are any specific topics about which you\u2019d like to hear more.", "label": 0}
{"title": "Unlocking the Potential of Quantum Computing", "url": "https://developers.googleblog.com/en/unlocking-the-potential-of-quantum-computing-a-developers-guide-to-error-correction/", "content": "A Developer\u2019s Guide to Error Correction\n\nManipulating quantum states on a superconducting chip cooled to a fraction of a degree above absolute zero at the bottom of a cryogenic fridge is incredibly challenging. And things don't always go to plan. Errors happen. Lots of errors. Sophisticated strategies are required to extract reliable computation from what would otherwise be just so many random output bits. Success is only possible if errors are not too overwhelmingly common. Today, Google Quantum AI announced the construction of a quantum chip where errors are indeed not overwhelmingly common. It makes use of the surface code, essentially a square patch of nearest-neighbor coupled physical qubits that work together to form a single more reliable logical qubit. As the square patch gets bigger, a logical qubit should get more reliable, and that is exactly what we demonstrated: a chip with logical qubits getting over a factor of two more reliable with each increase in patch size. But why is this needed\u2014what are quantum errors and how are they corrected? We'll give a short version of the answers here, and if you want more than a brief overview, you can head straight to the hands-on quantum error correction course now available for free on Coursera where you will find a series of videos and exercises to develop your understanding. \u201cThis new course opens a door to a complex domain, making quantum error correction not just accessible, but tangible for millions of learners worldwide,\u201d says Marni Baker Stein, Chief Content Officer at Coursera. \u201cOur collaboration with an industry pioneer like Google Quantum AI signifies another step towards a future where knowledge is the key to harnessing the power of quantum computing.\u201d\n\nWhat is a quantum error? Let's start with classical errors. You have a bit. Suppose it should be 0. But a cosmic ray hits it and it becomes 1. That's a bit-flip error, the only type of error in a classical computer. In comparison, a quantum chip doesn't consist of bits but rather multilevel quantum systems so cold they have well-defined discrete ground and excited states. We write these states as |0>, |1>, |2>, etc. When a cosmic ray hits a qubit that should be |0>, it can produce a superposition a|0> + b|1> + c|2> + ... of many states. Careful engineering of our device has made our qubits robust to all but the most energetic impacts, allowing us to compute results and simply discard the output when we are hit. When computing, we try hard to only use states |0> and |1>, but control and measurement errors inevitably lead to the occasional |2+> state, known as leakage errors. Resetting a qubit gets rid of this error, but also gets rid of any data on that qubit, so we have a special gate that moves higher states off data qubits and onto a qubit about to be reset. This prevents the accumulation of |2+> states in the computer. Then there is the problem that quantum data just doesn't like hanging around very long. Qubits like to relax. If you use an excited state to represent |1>, after a short time it will relax to |0>. We also want to be able to store superpositions like a|0> + b|1>, and that \u2018plus\u2019 can spontaneously become a \u2018minus\u2019, or a phase-flip error. The various ways that qubits lose data are collectively called decoherence. In general, decoherence can produce a completely different state to the one we want, but fortunately this difference can be broken into a mix of bit flips and phase flips; for brevity we call them X and Z errors.\n\nHow do we detect X and Z errors? Let's start with a picture of our quantum chip.\n\nThis is a 2D array of qubits with nearest neighbor interactions only. Data qubits store our precious quantum state. Measure qubits are used to detect X and Z errors. A good way to understand this is to imagine each light blue region detecting Z errors on the data qubits it touches, and each dark blue region detecting X errors. A Z error on a data qubit activates the neighboring light blue regions, locating it and allowing us to compensate for its presence in software. Provided the density of X and Z errors is low enough, the pattern of lit up regions will give us clear information allowing us to find and compensate for these errors. If a measure qubit suffers an error, that can give you a falsely activated region. To cope with this, the search for errors is repeated as often as possible, and the next time the check is performed there is a good chance it will be resolved. This creates an identifiable signature for a measurement error, enabling these to also be handled in software. It is an area of ongoing research to devise more sophisticated algorithms to handle the output of measure qubits to better identify the location and type of errors, all while keeping pace with the quantum computer.\n\nLearning more What we\u2019ve covered above is scraping the surface of quantum error correction and the critical role it plays in advancing quantum computing. For a step-by-step explanation and labs starting from the very basics, through quantum states and circuits, to some of the latest error correction tools used today, go to Coursera for our hands-on quantum error correction course. If you are a software engineer who has always wanted to work on a quantum problem, after taking the course head over to Quantum AI open source tools, where we build open-source software like Cirq, Stim and Crumble to simulate quantum circuits and develop error correction techniques. Learn how to design quantum algorithms and contribute to the development of tools that will enable the realization of practical quantum computing.\n\nQuantum computing: extra credit Here is an exciting area of research that combines theoretical computer science, software engineering, and quantum physics:", "label": 0}
{"title": "Part 2: A Survey of Analytics Engineering Work at Netflix", "url": "https://netflixtechblog.com/part-2-a-survey-of-analytics-engineering-work-at-netflix-4f1f53b4ab0f?source=collection_home---4------6-----------------------", "content": "Part 2: A Survey of Analytics Engineering Work at Netflix Netflix Technology Blog 9 min read \u00b7 Jan 2, 2025 -- 9 Listen Share\n\nThis article is the second in a multi-part series sharing a breadth of Analytics Engineering work at Netflix, recently presented as part of our annual internal Analytics Engineering conference. Need to catch up? Check out Part 1. In this article, we highlight a few exciting analytic business applications, and in our final article we\u2019ll go into aspects of the technical craft.\n\nGame Analytics\n\nYimeng Tang, Claire Willeck, Sagar Palao\n\nUser Acquisition Incrementality for Netflix Games\n\nNetflix has been launching games for the past three years, during which it has initiated various marketing efforts, including User Acquisition (UA) campaigns, to promote these games across different countries. These UA campaigns typically feature static creatives, launch trailers, and game review videos on platforms like Google, Meta, and TikTok. The primary goals of these campaigns are to encourage more people to install and play the games, making incremental installs and engagement crucial metrics for evaluating their effectiveness.\n\nMost UA campaigns are conducted at the country level, meaning that everyone in the targeted countries can see the ads. However, due to the absence of a control group in these countries, we adopt a synthetic control framework (blog post) to estimate the counterfactual scenario. This involves creating a weighted combination of countries not exposed to the UA campaign to serve as a counterfactual for the treated countries. To facilitate easier access to incrementality results, we have developed an interactive tool powered by this framework. This tool allows users to directly obtain the lift in game installs and engagement, view plots for both the treated country and the synthetic control unit, and assess the p-value from placebo tests.\n\nTo better guide the design and budgeting of future campaigns, we are developing an Incremental Return on Investment model. This model incorporates factors such as the incremental impact, the value of the incremental engagement and incremental signups, and the cost of running the campaign. In addition to using the causal inference framework mentioned earlier to estimate incrementality, we also leverage other frameworks, such as Incremental Account Lifetime Valuation (blog post), to assign value to the incremental engagement and signups resulting from the campaigns.\n\nMeasuring and Validating Incremental Signups for Netflix Games\n\nNetflix is a subscription service meaning members buy subscriptions which include games but not the individual games themselves. This makes it difficult to measure the impact of different game launches on acquisition. We only observe signups, not why members signed up.\n\nThis means we need to estimate incremental signups. We adopt an approach developed at Netflix to estimate incremental acquisition (technical paper). This approach uses simple assumptions to estimate a counterfactual for the rate that new members start playing the game.\n\nBecause games differ from series/films, it\u2019s crucial to validate this estimation method for games. Ideally, we would have causal estimates from an A/B test to use for validation, but since that is not available, we use another causal inference design as one of our ensemble of validation approaches. This causal inference design involves a systematic framework we designed to measure game events that relies on synthetic control (blog post).\n\nAs we mentioned above, we have been launching User Acquisition (UA) campaigns in select countries to boost game engagement and new memberships. We can use this cross-country variation to form a synthetic control and measure the incremental signups due to the UA campaign. The incremental signups from UA campaigns differ from those attributed to a game, but they should be similar. When our estimated incremental acquisition numbers over a campaign period are similar to the incremental acquisition numbers calculated using synthetic control, we feel more confident in our approach to measuring incremental signups for games.\n\nNetflix Games Players\u2019 Adventure: Modeled using State Machine\n\nAt Netflix Games, we aim to have a high number of members engaging with games each month, referred to as Monthly Active Accounts (MAA). To evaluate our progress toward this objective and to find areas to boost our MAA, we modeled the Netflix players\u2019 journey as a state machine.\n\nWe track a daily state machine showing the probability of account transitions between states.\n\nNetflix Players\u2019 Journey as State machine\n\nModeling the players\u2019 journey as a state machine allows us to simulate future states and assess progress toward engagement goals. The most basic operation involves multiplying the daily state-transition matrix with the current state values to determine the next day\u2019s state values.\n\nThis basic operation allows us to explore various scenarios:\n\nConstant Trends: If transition rates stay constant, we can predict future states by repeatedly multiplying the daily state-transition matrix to new state values, helping us assess progress towards annual goals under unchanged conditions.\n\nDynamic Scenarios: By modifying transition rates, we can simulate complex scenarios. For instance, mimicking past changes in transition rates from a game launch allows us to predict the impact of similar future launches by altering the transition rate for a specific period.\n\nSteady State: We can calculate the steady state of the state-transition matrix (excluding new players) to estimate the MAA once all accounts have tried Netflix games and understand long-term retention and reactivation effects.\n\nBeyond predicting future states, we use the state machine for sensitivity analysis to find which transition rates most impact MAA. By making small changes to each transition rate we calculate the resulting MAA and measure its impact. This guides us in prioritizing efforts on top-of-funnel improvements, member retention, or reactivation.\n\nContent Cash Modeling\n\nAlex Diamond\n\nAt Netflix we produce a variety of entertainment: movies, series, documentaries, stand-up specials, and more. Each format has a different production process and different patterns of cash spend, called our \u201cContent Forecast\u201d. Looking into the future, Netflix keeps a plan of how many titles we intend to produce, what kinds, and when. Because we don\u2019t yet know what specific titles that content will eventually become, these generic placeholders are called \u201cTBD Slots.\u201d A sizable portion of our Content Forecast is represented by TBD Slots.\n\nAlmost all businesses have a cash forecasting process informing how much cash they need in a given time period to continue executing on their plans. As plans change, the cash forecast will change. Netflix has a cash forecast that projects our cash needs to produce the titles we plan to make. This presents the question: how can we optimally forecast cash needs for TBD Slots, given we don\u2019t have details on what real titles they will become?\n\nThe large majority of our titles are funded throughout the production process \u2014 starting from when we begin developing the title to shooting the actual shows and movies to launch on our Netflix service.\n\nSince cash spend is driven by what is happening on a production, we model it by breaking down into these three steps:\n\nDetermine estimated production phase durations using historical actuals Determine estimated percent of cash spent in each production phase Model the shape of cash spend within each phase\n\nPutting these three pieces together allows us to generate a generic estimation of cash spend per day leading up to and beyond a title\u2019s launch date (a proxy for \u201ccompletion\u201d). We could distribute this spend linearly across each phase, but this approach allows us to capture nuance around patterns of spend that ramp up slowly, or are concentrated at the start and taper off throughout.\n\nBefore starting any math, we need to ensure a high quality historical dataset. Data quality plays a huge role in this work. For example, if we see 80% of our cash spent before production even started, it might be safe to say that either the production dates (which are manually captured) are incorrect or that title had a unique spending pattern that we don\u2019t want to anticipate our future titles will follow.\n\nFor the first two steps, finding the estimated phase durations and cash percent per phase, we\u2019ve found that simple math works best, for interpretability and consistency. We use a weighted average across our \u201cclean\u201d historical actuals to produce these estimated assumptions.\n\nFor modeling the shape of spend throughout each phase, we perform constrained optimization to fit a 3rd degree polynomial function. The constraints include:\n\nMust pass through the points (0,0) and (1,1). This ensures that 0% through the phase, 0% of that phase\u2019s cash has been spent. Similarly, 100% through the phase, 100% of that phase\u2019s cash has been spent. The derivative must be non-negative. This ensures that the function is monotonically increasing, avoiding counterintuitively forecasting any negative spend.\n\nThe optimization\u2019s objective function minimizes the sum of squared residuals and returns the coefficients of the polynomial that will guide the shape of cash spend through each phase.\n\nOnce we have these coefficients, we can evaluate this polynomial at each day of the expected phase duration, and then multiply the result by the expected cash per phase. With some additional data processing, this yields an expected percent of cash spend each day leading up to and beyond the launch date, which we can base our forecasts on.\n\nAssistive Speech Recognition in Dubbing Workflows at Netflix\n\nTanguy Cornau\n\nGreat stories can come from anywhere and be loved everywhere. At Netflix, we strive to make our titles accessible to a global audience, transcending language barriers to connect with viewers worldwide. One of the key ways we achieve this is through creating dubs in many languages.\n\nFrom the transcription of the original titles all the way to the delivery of the dub audio, we blend innovation with human expertise to preserve the original creative intent.\n\nLeveraging technologies like Assistive Speech Recognition (ASR), we seek to make the transcription part of the process more efficient for our linguists. Transcription, in our context, involves creating a verbatim script of the spoken dialogue, along with precise timing information to perfectly align the text with the original video. With ASR, instead of starting the transcription from scratch, linguists get a pre-generated starting point which they can use and edit for complete accuracy.\n\nThis efficiency enables linguists to focus more on other creative tasks, such as adding cultural annotations and references, which are crucial for downstream dubbing.\n\nWith ASR, and other new and enhanced technologies we introduce, rigorous analytics and measurement are essential to their success. To effectively evaluate our ASR system, we\u2019ve established a multi-layered measurement framework that provides comprehensive insights into its performance across many dimensions (for example, the accuracy of the text and timing predictions), offline and online.\n\nASR is expected to perform differently for various languages; therefore, at a high level, we track metrics by original language of the show, allowing us to assess overall ASR effectiveness and identify trends across different linguistic contexts. We further break down performance by various dimensions, e.g. content type, genre, etc\u2026 to help us pinpoint specific areas where the ASR system may encounter difficulties. Furthermore, our framework allows us to conduct in-depth analyses of individual titles\u2019 transcription, focusing on critical quality dimensions around text and timing accuracy of ASR suggestions. By zooming in on where the system falls short, we gain valuable insights into specific challenges, enabling us to further refine our understanding of ASR performance.\n\nThese measurement layers collectively empower us to continuously monitor, identify improvement areas, and implement targeted enhancements, ensuring that our ASR technology gets more and more accurate, effective, and helpful to linguists across diverse content types and languages. By refining our dubbing workflows through these innovations, we aim to keep improving the quality of our dubs to help great stories travel across the globe and bring joy to our members.", "label": 0}
{"title": "The Metro Experience with a Jingle", "url": "https://lifeofpablo.com/blog/the-metro-experience-with-a-jingle", "content": "The Metro Experience with a Jingle\n\nThis post was written in English (en_US).\n\nThe Metro Experience with a Jingle This is part of a three or four part blog series. These posts will be interactive on my website (https://lifeofpablo.com) . I am in the process of rebuilding my website so stayed tuned.\n\nMy name is Pablo Morales. I used to teach within Omaha Public Schools but now I teach in Sacramento, CA. I still teach French & Spanish to middle schoolers.\n\nI have always been a fan of public transportation! Within the United States, commuting to work seems painfully dreadful. We are a nation dominated by cars and bad policies. Our public transportation infrastructure is not good for a very \u201cdeveloped\u201d country. Our roads are suffering, and everything else in-between isn\u2019t pleasant either. This isn't the case in South Korea. I'm only a week or so in since arriving in South Korea. I am absolutely M-I-N-D blown on what I am seeing and hearing.. My brain can't handle this!\n\nCommuting in Seoul, Korea is not about trying to catch your bus or train, you are there for the experience of being in a station. You're probably wondering, \"Pablo, Isn't the topic over public transportation\u2026.you know boring?\" I say, \"Absolutely Not!\" I've been to multiple cities who don't even come close to the Korean experience of public transportation. My favorite part about using the Seoul Metro System is that you get to hear all the cute and relaxing rings and jingles offered at all metro stations and some bus stations.\n\nCommuting is hard on the body when traveling far. It's hot and humid out, or you're just simply tired. Those little jingles give you a glimmer of hope that you are getting closer to your final destination. You stay motivated instead of only hearing the usual \"This train is departing.\" or departing. It brightens the mood. My two favorite jingles or sounds are: The Trumpet Link A steelpan Link Do you see what I mean? It definitely made me smile. Everyone seems so happy. I knew traveling within Seoul was going to be fun but who knew these little jingles are what make the experience of the metro in Seoul.\n\nHere is a handpicked few of my favorite jingles. Not all are in circulation at the moment.\n\nLink\n\nLink\n\nBeing a minority in the US and still being a minority in South Korea.\n\nSomeone who speaks 3 different languages. Learning the Korean language is easy in some aspects and difficult in others. I teach French & Spanish. The one common thing between these two languages is the alphabet - romanized letters. The Korean Language is written in Hangul.\n\nI was an ELL student. Even. I now understand what it is like to be in my student\u2019s mind.", "label": 1}
{"title": "Summer 2016", "url": "https://lifeofpablo.com/blog/summer-2016", "content": "Summer 2016\n\nThis post was written in English (en_US).\n\n\"Summer is the time where we actually get to relax and get away from our obligations for a short period of time. We travel, we work our butts off to have some financial gains and stability. What's that for a college kid? Summer is also a time to think ahead into the future while enjoying yourself. It is unfortunate that this this adventure is only temporary to help us get away. That being said, I am going to tell you about my summer adventures and what it entails. The summer is slowly slipping to the end...\n\nWorking is important to make money. That's exactly what I have done throughout the summer. For those who do not know, I work at McDonald's as a manager. It pays me well to let me go on adventures. (More on that in a second.) Like any job, it has it's ups and downs. Honestly I am surprised that I have stayed as long as I have. 5 years strong! WOO! Its a job that has many benefits than just the paycheck.\n\nSo now you want to know what I actually did for fun? Alright! One thing at a time.\n\nSo my first adventure was hitting the upper west coast! It was the trip trio: three friends, three places! We went to started our trip off in Denver to catch a flight out of DEN towards Seattle. Seattle was such a beautiful city. Then we crossed into Canada where we spend a few days in Vancouver, BC. Canada was breath taking. I was glad to see Canada for the first time. It was exciting to see one of my friends leave the country for the first time ever! I got to speak a little French here and there.\n\nCan always count on these guys to push me through anything! #summer #canada A photo posted by Pablo Morales (@pmorales18) on Jun 11, 2016 at 7:04pm PDT\n\nI went to Denver a second time recently to the Vans Warped Tour! This is my second year going to this I got to see some of my favorite bands this year! Many of you heard that Yellow Card is breaking up!! I about cried when I heard this. They have their reasons and I respect them for that. They have had such an impact on me.\n\nI got to hangout with friends all throughout. Frienships grew!\n\nThis is only a summary of my summer. Want to know? Just ask!\n\n-Pablo\"", "label": 1}
{"title": "Climate Week NYC & Build a Website in an Hour", "url": "https://lifeofpablo.com/blog/climate-week-build-1hr-website", "content": "Background Information\n\nA few weeks ago I attended Climate Week NYC in New York City. It was such a great time listening to all the great talks and participating in different activities such as driving a Lucid electric car with my friends. It got to meet such amazing people (and networked) who are in the climate change space. It made me step back an realize how much time we have lost not fighting the environment and how we only have a limited time to speed up on our actions.\n\nIt was nice to be in a place full of public transportation and get my steps in. I was also wired on Matcha Lattes to keep up with the revolving door of events occuring all at once.\n\nGetting Around to Building the Website\n\nI've been meaning to build a website for a quick summary of my adventures in the city. I finally found the perfect day to do it. I finally was able attended a Build a Website in an Hour event. Thank you James G for hosting this event!\n\nJust as the name of the event states, I built (most) of the website in an hour. It was fun and challenging because of the following:\n\nI didn't have a plan on how this site was going to look It was a challenge to get the site made in an hour. Talk about pressure I got to hangout with people in the IndieWeb. Show off the adventures I went on.\n\nWhere is this site located?\n\nThis website is located at https://climateweeknyc.lifeofpablo.com .\n\nTechnologies Used\n\nThis website was made using vanilla HTML & CSS. I used Tachyions CSS Framework for the CSS. I use it on my website, primarily on my blog, to change the appearance of my posts. I like this CSS framework because is it doesn't interfere with existing CSS frameworks or existing CSS stylesheets. The classes are easy to learn\n\nA Breakdown of the Site\n\nHero Banner Description: A green hero banner with a navigation. It says \"NYC Climate Week 2023\" and \"A Wakeup Call About Climate Change\"\n\nOther Sections Include:\n\nIntroduction\n\nAttendees\n\nEvents Attended\n\nPhotos\n\nWebmentions\n\nThis site supports webmentions!\n\nClosing\n\nI hope you enjoy the website! Let me know if you have any comments, questions or concerns. Don't forget to say, \"hello\"!", "label": 1}
{"title": "New Chapter Awaits", "url": "https://lifeofpablo.com/blog/new-chapter-awaits", "content": "New Chapter Awaits\n\nThis post was written in English (en_US).\n\n\"Hello Everyone!! Well the time has come. I'm all packed up (I think) and I will be moving away to college. It seems like yesterday that I was starting freshman year of high school. It went through my mind thinking if I will be able to survive the next four years? Luckily I did. :) The next chapter of my life has begun. I I hope that I will graduate college in four years or less. I will really miss everyone that I have met along the way. Anyone from helping me pick my books up in the hall way, that person that was always there for me when I was down or even the person that would join me on a great adventure. All these memories just roaming around in my head. I guess I wont be that far from home?? I'm ready to start even more memories. I am excited to be around new people. I really want to find out who I really am to get to know myself even more. It is a weird thing to say.\n\nThis will be the last time I will sleep in my own bed for a long time. It will be strange not being in the house I grew up in my whole life. There will be no home cook meals ready when I'm done with classes. It will be hard to get used to. My goal is not to get homesick (I will eventually). Like I said I am not going away that far (45 mins.) It is time for me to be more independent of myself. Mom will not be there to hold our hands in college. I will have to make decisions that will guide my daily life and those that will affect me for the rest of my life.\n\nThere are so many people I want to say goodbye to but time has not allowed me. If anyone wants to see me, let me know.\n\nCannot wait for what is ahead.\n\nWish me luck!!!\n\nEDIT: I will also keep updated on social media and make a blog post when I am all moved in.\"", "label": 1}
{"title": "Introducing Pyrefly: A new type checker and IDE experience for Python", "url": "https://engineering.fb.com/2025/05/15/developer-tools/introducing-pyrefly-a-new-type-checker-and-ide-experience-for-python/", "content": "Today we are announcing an alpha version of Pyrefly, an open source Python type checker and IDE extension crafted in Rust. Pyrefly is a static type checker that analyzes Python code to ensure type consistency and help you catch errors throughout your codebase before your code runs. It also supports IDE integration and CLI usage to give you flexibility in how you incorporate it into your workflow.\n\nThe open source community is the backbone of the Python language. We are eager to collaborate on Pyrefly with the community and improve Python\u2019s type system and the many libraries that we all rely on.\n\nGet started\n\nReady to dive in? The official Pyrefly website has all the details, but to quickly get started:\n\nWhy we built Pyrefly\n\nBack in 2017, we embarked on a mission to create a type checker that could handle Instagram\u2019s massive codebase of typed Python. This mission led to the birth of the Pyre type checker, inspired by the robust designs of Hack and Flow, and written in OCaml to deliver scalable performance.\n\nOver the years, Pyre served us well, but as the type system evolved and the need for typechecking to drive responsive IDE emerged, it was clear that we needed to take a new approach. We explored alternate solutions and leveraged community tools like Pyright for code navigation. But the need for an extensible type checker that can bring code navigation, checking at scale, and exporting types to other services drove us to start over, creating Pyrefly.\n\nThe principles behind Pyrefly\n\nToday, we\u2019re excited to unveil Pyrefly, a project we\u2019ve been developing openly on GitHub. We invite you to explore our work and try it out on your own project. While a project like Pyrefly is the sum of thousands of technical choices, a few notable principles we\u2019ve followed are:\n\nPerformance\n\nWe want to shift checks that used to happen later on CI to happening on every single keystroke. That requires checking code at speed (on large codebases we can check 1.8 million lines of code per second!) and careful thought to incrementality and updates. Pyrefly is implemented in Rust and designed for high performance on codebases of all sizes.\n\nIDE first\n\nWe want the IDE and command line to share a consistent view of the world, which means crafting abstractions that capture the differences without incurring unnecessary costs. Designing these abstractions from the beginning is much easier than retrofitting them, which we tried with Pyre.\n\nInference\n\nSome Python programs are typed, but many aren\u2019t. We want users to benefit from types even if they haven\u2019t annotated their code \u2013 so automatically infer types for returns and local variables and display them in the IDE. What\u2019s more, in the IDE you can even double click to insert these inferred types if you think that would make the program better.\n\nOpen source\n\nPython is open source, and hugely popular. The Python typing specification is open source, which made Pyrefly vastly easier to develop. Many of the libraries Meta contributes to are open source,( e.g., PyTorch).\n\nPyrefly is also open source, available on GitHub under the MIT license, and we encourage pull requests and issue reports. We also have a Discord channel for more free flowing discussions. We would love to build a community around Pyrefly.\n\nThe future of Pyrefly\n\nWe will work with the Python community to drive the language forward and improve the developer experience. Since the beginning of Pyre, we open sourced our code and contributed a number of PEPs alongside the community of type checker maintainers. We feel we can do more with Pyrefly to help Python developers leverage the benefits of types for developers, library authors, and folks just learning the language.\n\nMeta has leveraged types in dynamic languages from the beginning and knows the significant benefits it brings to developer productivity and security. We plan to share more of our learnings and tooling with blogs, better types in the ecosystem and language enhancements.\n\nToday we\u2019re releasing Pyrefly as an alpha. At the same time, we\u2019re busy burning down the long-tail of bugs and features aiming to remove the alpha label this Summer. Your feedback is invaluable to get there, so please give it a try and report your bugs or things you think can be improved. Even if Pyrefly isn\u2019t right for your project, we would love to hear how you use types and what you would like to see improved in your editor.\n\nJoin us on the journey as we help illuminate your bugs with Pyrefly. Happy coding! \ud83d\udc0d\u2728\n\nHear more about Pyrefly\n\nCheck out the episode of the Meta Tech Podcast where several team members share their experience developing Pyrefly and technical details for how it works. We also just talked at PyCon US about high-performance Python through faster type checking and free threaded execution.\n\nTo learn more about Meta Open Source, visit our open source site, subscribe to our YouTube channel, or follow us on Facebook, Threads, X, and LinkedIn.\n\nAcknowledgements\n\nPyrefly was created By Meta\u2019s Python Language Tooling Team: Jia Chen, Rebecca Chen, Sam Goldman, David Luo, Kyle Into, Zeina Migeed, Neil Mitchell, Maggie Moss, Conner Nilsen, Aaron Pollack, Teddy Sudol, Steven Troxler, Lucian Wischik, Danny Yang, and Sam Zhou.", "label": 0}
{"title": "Improve Your Next Experiment by Learning Better Proxy Metrics From Past Experiments", "url": "https://netflixtechblog.com/improve-your-next-experiment-by-learning-better-proxy-metrics-from-past-experiments-64c786c2a3ac?source=collection_home---4------18-----------------------", "content": "We are excited to share our work on how to learn good proxy metrics from historical experiments at KDD 2024. This work addresses a fundamental question for technology companies and academic researchers alike: how do we establish that a treatment that improves short-term (statistically sensitive) outcomes also improves long-term (statistically insensitive) outcomes? Or, faced with multiple short-term outcomes, how do we optimally trade them off for long-term benefit?\n\nFor example, in an A/B test, you may observe that a product change improves the click-through rate. However, the test does not provide enough signal to measure a change in long-term retention, leaving you in the dark as to whether this treatment makes users more satisfied with your service. The click-through rate is a proxy metric (S, for surrogate, in our paper) while retention is a downstream business outcome or north star metric (Y). We may even have several proxy metrics, such as other types of clicks or the length of engagement after click. Taken together, these form a vector of proxy metrics.\n\nThe goal of our work is to understand the true relationship between the proxy metric(s) and the north star metric \u2014 so that we can assess a proxy\u2019s ability to stand in for the north star metric, learn how to combine multiple metrics into a single best one, and better explore and compare different proxies.\n\nSeveral intuitive approaches to understanding this relationship have surprising pitfalls:\n\nLooking only at user-level correlations between the proxy S and north star Y. Continuing the example from above, you may find that users with a higher click-through rate also tend to have a higher retention. But this does not mean that a product change that improves the click-through rate will also improve retention (in fact, promoting clickbait may have the opposite effect). This is because, as any introductory causal inference class will tell you, there are many confounders between S and Y \u2014 many of which you can never reliably observe and control for.\n\nContinuing the example from above, you may find that users with a higher click-through rate also tend to have a higher retention. But this does not mean that a product change that improves the click-through rate will also improve retention (in fact, promoting clickbait may have the opposite effect). This is because, as any introductory causal inference class will tell you, there are many confounders between S and Y \u2014 many of which you can never reliably observe and control for. Looking naively at treatment effect correlations between S and Y. Suppose you are lucky enough to have many historical A/B tests. Further imagine the ordinary least squares (OLS) regression line through a scatter plot of Y on S in which each point represents the (S,Y)-treatment effect from a previous test. Even if you find that this line has a positive slope, you unfortunately cannot conclude that product changes that improve S will also improve Y. The reason for this is correlated measurement error \u2014 if S and Y are positively correlated in the population, then treatment arms that happen to have more users with high S will also have more users with high Y.\n\nBetween these naive approaches, we find that the second one is the easier trap to fall into. This is because the dangers of the first approach are well-known, whereas covariances between estimated treatment effects can appear misleadingly causal. In reality, these covariances can be severely biased compared to what we actually care about: covariances between true treatment effects. In the extreme \u2014 such as when the negative effects of clickbait are substantial but clickiness and retention are highly correlated at the user level \u2014 the true relationship between S and Y can be negative even if the OLS slope is positive. Only more data per experiment could diminish this bias \u2014 using more experiments as data points will only yield more precise estimates of the badly biased slope. At first glance, this would appear to imperil any hope of using existing experiments to detect the relationship.\n\nThis figure shows a hypothetical treatment effect covariance matrix between S and Y (white line; negative correlation), a unit-level sampling covariance matrix creating correlated measurement errors between these metrics (black line; positive correlation), and the covariance matrix of estimated treatment effects which is a weighted combination of the first two (orange line; no correlation).\n\nTo overcome this bias, we propose better ways to leverage historical experiments, inspired by techniques from the literature on weak instrumental variables. More specifically, we show that three estimators are consistent for the true proxy/north-star relationship under different constraints (the paper provides more details and should be helpful for practitioners interested in choosing the best estimator for their setting):\n\nA Total Covariance (TC) estimator allows us to estimate the OLS slope from a scatter plot of true treatment effects by subtracting the scaled measurement error covariance from the covariance of estimated treatment effects. Under the assumption that the correlated measurement error is the same across experiments (homogeneous covariances), the bias of this estimator is inversely proportional to the total number of units across all experiments, as opposed to the number of members per experiment.\n\nestimator allows us to estimate the OLS slope from a scatter plot of true treatment effects by subtracting the scaled measurement error covariance from the covariance of estimated treatment effects. Under the assumption that the correlated measurement error is the same across experiments (homogeneous covariances), the bias of this estimator is inversely proportional to the total number of units across all experiments, as opposed to the number of members per experiment. Jackknife Instrumental Variables Estimation (JIVE) converges to the same OLS slope as the TC estimator but does not require the assumption of homogeneous covariances. JIVE eliminates correlated measurement error by removing each observation\u2019s data from the computation of its instrumented surrogate values.\n\nconverges to the same OLS slope as the TC estimator but does not require the assumption of homogeneous covariances. JIVE eliminates correlated measurement error by removing each observation\u2019s data from the computation of its instrumented surrogate values. A Limited Information Maximum Likelihood (LIML) estimator is statistically efficient as long as there are no direct effects between the treatment and Y (that is, S fully mediates all treatment effects on Y). We find that LIML is highly sensitive to this assumption and recommend TC or JIVE for most applications.\n\nOur methods yield linear structural models of treatment effects that are easy to interpret. As such, they are well-suited to the decentralized and rapidly-evolving practice of experimentation at Netflix, which runs thousands of experiments per year on many diverse parts of the business. Each area of experimentation is staffed by independent Data Science and Engineering teams. While every team ultimately cares about the same north star metrics (e.g., long-term revenue), it is highly impractical for most teams to measure these in short-term A/B tests. Therefore, each has also developed proxies that are more sensitive and directly relevant to their work (e.g., user engagement or latency). To complicate matters more, teams are constantly innovating on these secondary metrics to find the right balance of sensitivity and long-term impact.\n\nIn this decentralized environment, linear models of treatment effects are a highly useful tool for coordinating efforts around proxy metrics and aligning them towards the north star:\n\nManaging metric tradeoffs. Because experiments in one area can affect metrics in another area, there is a need to measure all secondary metrics in all tests, but also to understand the relative impact of these metrics on the north star. This is so we can inform decision-making when one metric trades off against another metric. Informing metrics innovation. To minimize wasted effort on metric development, it is also important to understand how metrics correlate with the north star \u201cnet of\u201d existing metrics. Enabling teams to work independently. Lastly, teams need simple tools in order to iterate on their own metrics. Teams may come up with dozens of variations of secondary metrics, and slow, complicated tools for evaluating these variations are unlikely to be adopted. Conversely, our models are easy and fast to fit, and are actively used to develop proxy metrics at Netflix.\n\nWe are thrilled about the research and implementation of these methods at Netflix \u2014 while also continuing to strive for great and always better, per our culture. For example, we still have some way to go to develop a more flexible data architecture to streamline the application of these methods within Netflix. Interested in helping us? See our open job postings!\n\nFor feedback on this blog post and for supporting and making this work better, we thank Apoorva Lal, Martin Tingley, Patric Glynn, Richard McDowell, Travis Brooks, and Ayal Chen-Zion.", "label": 0}
{"title": "Using JSON on My Site", "url": "https://lifeofpablo.com/blog/using-json-on-my-site", "content": "Using JSON on My Site\n\nThis post was written in English (en_US).\n\nThis past week, I've been learning to use JSON on my website. As I am learning to implement a Micropub Server on my site, I feel that I should learn how JSON works. In a way JSON has been used on my site before by using microformats in the rendered HTML code and being read by other sites and services. I've been wanting to explore ways to update content on my site in a more effective manner instead of having to manually update html code for each entry of a list. An example of this would be a list of movies I watched. This is still a a learning process. I have so much to learn. I want to expand Datenstrom Yellow, the flat-file c I am using, to still be simple but be powerful at the same time.\n\nAn issue I have to work out for longevity and simplicity is by creating an extension for the different lists. Right now I have to create a layout for a particular list such as movies , create a new markdown page in my content folder, and finally set the desired layout (template). An example of this would be Layout: movies . This isn't ideal when I write a blog post. If I changed the layout of a blog post, it wouldn't be recognized as a blog post in the main blog feed. That's where an extension would come in place and place it in the markdown file of the blog post or any other page I'd like to create on my website. This wouldn't require to change the layout. In the markdown file, I could simply place, [list 1] or even simply, [movies] to show the list. Coming Soon!\n\nExample Code:\n\nHere's the layout for movies.html . I use PHP to create the reoccuring HTML elements automatically when I modify the movie JSON file. It will update the list accordingly.\n\n$this ->yellow->layout( \"header\" ) < div class = \"content\" > < div class = \"main h-entry\" role = \"main\" > < h1 > echo $this ->yellow->page->getHtml( \"titleContent\" ) </ h1 > < div class = \"entry-content e-content\" > echo $this ->yellow->page->getContentHtml() </ div > < div class = \"mw6 center\" > $catalog_json = file_get_contents( 'media/downloads/movies.json' ); $decoded_json = json_decode($catalog_json, true ); $items = $decoded_json[ 'movies' ]; foreach ($items as $item) { $id = $item[ 'id' ]; $name = $item[ 'title' ]; $cover = $item[ 'cover' ]; $info_url = $item[ 'info_url' ]; echo '<article class=\"h-item\">' ; echo '<a class=\"link dt w-100 bb b--black-10 pb2 mt2 dim blue u-url\" href=\"' .$info_url. '\">' ; echo '<div class=\"dtc w4\">' ; echo '<img src=\" ' .$cover. '\" class=\"db w-100 u-photo\" alt=\"movie poster of ' .$name. '\"/>' ; echo '</div>' ; echo '<div class=\"dtc v-top pl2\">' ; echo '<h1 class=\"f6 f5-ns fw6 lh-title black mv0 p-name\">' .$name. '</h1>' ; echo '<dl class=\"mt2 f6\">' ; echo '<dt class=\"clip\">ID</dt>' ; echo '<dd class=\"ml0\">ID: ' .$id. '</dd>' ; echo '</dl>' ; echo '</div>' ; echo '</a>' ; echo '</article>' ; } </ div > < div class = \"permalink\" > Permalink: < a class = \"u-url\" href = \"https://lifeofpablo.com<?php echo $this->yellow->page->getLocation($absoluteLocation = false) ?>\" > https://lifeofpablo.com echo $this ->yellow->page->getLocation($absoluteLocation = false ) </ a > </ div > </ div > </ div > $this ->yellow->layout( \"footer\" )\n\nJSON\n\nThis is the example movie JSON (movies.json) code for the movies I watched in 2023. As I CRUD (create, read, update, delete) entries, the PHP code will parse the code with the latest version of the JSON file.\n\n{ \"movies\" : [ { \"id\" : 0 , \"title\" : \"Barbie\" , \"cover\" : \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTps15LVPBd4bN4dWOYwJ1ggcNE_4tzLnR10qZlngWbwQ4G-cgE\" , \"info_url\" : \"https://www.themoviedb.org/movie/346698-barbie?language=en-US\" }, { \"id\" : 1 , \"title\" : \"The Hunger Games: The Ballad of Songbirds & Snakes\" , \"cover\" : \"https://encrypted-tbn2.gstatic.com/images?q=tbn:ANd9GcRvwP_lNaUz46LtcewMX0xdKd4bT-8hAHaAt8HjB96tLsMolVVT\" , \"info_url\" : \"https://www.themoviedb.org/movie/695721-the-hunger-games-the-ballad-of-songbirds-snakes?language=en-US\" }, { \"id\" : 2 , \"title\" : \"Shrek 2\" , \"cover\" : \"https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcSFADKf7LWrP_r2PkPMo4zVxAdgoUK4KMtgRsfMSUTpisbpGG7N\" , \"info_url\" : \"https://www.themoviedb.org/movie/809-shrek-2?language=en-US\" }, { \"id\" : 3 , \"title\" : \"Killers of the Flower Moon\" , \"cover\" : \"https://upload.wikimedia.org/wikipedia/en/8/88/Killers_of_the_Flower_Moon_film_poster.jpg\" , \"info_url\" : \"https://www.themoviedb.org/movie/466420-killers-of-the-flower-moon?language=en-US\" }, { \"id\" : 4 , \"title\" : \"Doctor Strange in the Multiverse of Madness\" , \"cover\" : \"https://upload.wikimedia.org/wikipedia/en/1/17/Doctor_Strange_in_the_Multiverse_of_Madness_poster.jpg\" , \"info_url\" : \"https://www.themoviedb.org/movie/453395-doctor-strange-in-the-multiverse-of-madness?language=en-US\" }, { \"id\" : 5 , \"title\" : \"The Unbearable Weight of Massive Talent\" , \"cover\" : \"https://www.themoviedb.org/t/p/w1280/8JzPuj4lNQv0wmd38u0ee1dxzhn.jpg\" , \"info_url\" : \"https://www.themoviedb.org/movie/648579-the-unbearable-weight-of-massive-talent?language=en-US\" }, { \"id\" : 6 , \"title\" : \"Sonic the Hedgehog 2\" , \"cover\" : \"https://www.themoviedb.org/t/p/w1280/6DrHO1jr3qVrViUO6s6kFiAGM7.jpg\" , \"info_url\" : \"https://www.themoviedb.org/movie/675353-sonic-the-hedgehog-2?language=en-US\" }, { \"id\" : 7 , \"title\" : \"It's Still Your Bed\" , \"cover\" : \"https://www.themoviedb.org/t/p/w1280/wtHBxOisTtU10ieNoXHX62kB613.jpg\" , \"info_url\" : \"https://www.themoviedb.org/movie/613027-it-s-still-your-bed?language=en-US\" }, { \"id\" : 8 , \"title\" : \"Dog\" , \"cover\" : \"https://www.themoviedb.org/t/p/w1280/rkpLvPDe0ZE62buUS32exdNr7zD.jpg\" , \"info_url\" : \"https://www.themoviedb.org/movie/626735-dog?language=en-US\" }, { \"id\" : 9 , \"title\" : \"Uncharted\" , \"cover\" : \"https://www.themoviedb.org/t/p/w1280/rJHC1RUORuUhtfNb4Npclx0xnOf.jpg\" , \"info_url\" : \"https://www.themoviedb.org/movie/335787-uncharted?language=en-US\" } ]}\n\nThis JSON file with the help of PHP to render HTML content:", "label": 1}
{"title": "How GitHub supports neurodiverse employees (and how your company can, too)", "url": "https://github.blog/engineering/engineering-principles/how-github-supports-neurodiverse-employees-and-how-your-company-can-too/", "content": "In today\u2019s global workplace, supporting employees by appreciating and understanding their background and lived experience is crucial for the success of any organization. This includes employees who are neurodivergent. Neurodivergence refers to natural variations in human brains and cognition. The term encompasses conditions such as autism, ADHD, dyslexia, mental illness, and other neurological differences.\n\nNeurodivergent employees don\u2019t just enrich the workplace, they\u2019re good for business. According to Deloitte, teams with neurodivergent people can be up to 30 percent more productive than others. Neurodivergent folks excel in pattern recognition and the type of outside-the-box thinking highly sought after in the software industry.\n\nIn this blog post, we\u2019ll take a look at five ways GitHub fosters and supports neurodiverse employees via Neurocats, a GitHub Community of Belonging (CoB), and how you can do the same at your organization.\n\nLet\u2019s go!\n\nForktocat: An Octocat image that represents the fork function in Git, which we\u2019ve adopted in Neurocats to represent the different ways our brains work.\n\n1. Establish supportive communities\n\nAs an initial step, establish private, supportive communities where neurodivergent employees can connect, share their experiences, and find support. GitHub\u2019s Neurocats community allows members to privately discuss their neurodivergence, offer advice to each other, and build a sense of belonging, all in a safe place where members can freely express themselves without fear.\n\nNeurocats started as a private Slack channel under a different name years before it formally transitioned into a CoB. Originally called #neuroconverse, it gave the neurodivergent community at GitHub a space to chat. In the summer of 2021, a collection of passionate members started discussions with GitHub\u2019s Diversity Inclusion and Belonging team about becoming a formal CoB. In October 2021, they formed as an official group at GitHub, and after some discussion, became the Neurocats. The community now consists of hundreds of members from across the company and continues to grow.\n\nSetting up spaces for neurodivergent individuals to express themselves and meet other like-minded friends and allies not only improves their overall work life balance, it also accelerates the creation of new innovative ideas that could be the next big thing in your organization\u2019s portfolio.\n\n\u201cAs a neurodivergent people manager with dyslexia and dysgraphia, I am thrilled to be part of the Neurocats CoB, a community that embraces and normalizes our uniqueness,\u201d says Tina Barfield, senior manager at GitHub. \u201cBy doing so, we can help drive environments where everyone\u2019s strengths are celebrated, leading to greater innovation, creativity, and inclusivity.\u201d (Please note, all employee names and stories have been shared with permission.)\n\nSuggestions for establishing a supportive community: Have members lead. Embrace the powerful slogan, \u201cNothing about us, without us.\u201d\n\nEmbrace the powerful slogan, \u201cNothing about us, without us.\u201d Consider how to protect confidentiality and anonymity. Many neurodivergent people may want to control to whom and in what context they share their neurodivergence. Always obtain permission before sharing information that identifies a person\u2019s neurodivergence.\n\nMany neurodivergent people may want to control to whom and in what context they share their neurodivergence. Always obtain permission before sharing information that identifies a person\u2019s neurodivergence. Consider conversation history. In Neurocats, conversation history is wiped every four days.\n\nIn Neurocats, conversation history is wiped every four days. Never require diagnosis for membership to the community. In many cultures and countries, getting a formal diagnosis can be difficult if not impossible. If someone believes they are neurodivergent, accept them and support them.\n\n2. Foster a sense of belonging\n\nGiving employees the time and space to discuss their neurodivergence enables them to strongly relate to each other, lift each other up, and make personal discoveries that will help them navigate life both at work and at home.\n\n\u201cI didn\u2019t know what being neurodivergent was before Neurocats,\u201d says Lou Nelson, support engineer III who works on GitHub Premium Support. \u201cI thought I was a weird kid with an ADHD diagnosis. Neurocats has become the lynchpin for my career. I have made valuable connections and have a deeper insight into myself than I could have ever done alone. As a member, I find it incumbent to share this experience with others so that they also don\u2019t have to feel alone.\u201d\n\nWhen neurodivergent employees feel comfortable enough to share their stories more broadly, other employees will be drawn to those communities to either personally relate or learn and empathize about subjects they may not have previously considered.\n\n\u201cAs a people manager with ADHD, I\u2019m accustomed to being the \u2018neurodiversity pioneer\u2019 when meeting new teams or direct reports, setting an example by speaking openly about my gifts and challenges,\u201d says Julie Kang, staff manager of software engineering at GitHub. \u201cWhen I joined GitHub, and especially when I became a Neurocat, I was pleasantly surprised to find a culture that was knowledgeable, accepting, and celebratory of neurodiversity at a level I haven\u2019t seen before in my career.\u201d\n\nSuggestions for encouraging a sense of belonging in your neurodivergent community: Declare safe spaces. Before meetings where neurodivergence is discussed, call out the fact that you want this to be a safe space. Encourage kindness and empathy.\n\nBefore meetings where neurodivergence is discussed, call out the fact that you want this to be a safe space. Encourage kindness and empathy. Warn about oversharing. If there\u2019s a meeting where oversharing is possible, warn participants to think about their contributions and avoid sharing details that they may later regret.\n\nIf there\u2019s a meeting where oversharing is possible, warn participants to think about their contributions and avoid sharing details that they may later regret. Understand privacy and confidentiality expectations. Participants should respect privacy and adopt guiding principles around privacy and confidentiality.\n\nParticipants should respect privacy and adopt guiding principles around privacy and confidentiality. Share information broadly with the company or organization. Create mechanisms where members of the community can safely share thoughts and feelings with the broader company or organization and, if appropriate, externally. Anonymize this information if required and always share with consent. Examples can include recurring Q&A sessions, presentations, and discussions with the entire company, organization or a segment of the organization all help allow neurodivergent employees to be seen.\n\n3. Provide flexibility and accommodations\n\nNeurodivergent employees can often benefit from flexible working arrangements. This could include flexible hours, remote work options, noise-canceling headphones, or customized workspaces to reduce sensory overload.\n\nAsking for accommodations can be hard. Identify the process your organization or company uses to assess workplace accommodations. Encourage employees to utilize that process to obtain a workplace accommodation.\n\n\u201cOne of the biggest things for me has been seeing how many other folks went through a lot of their life being told that they just needed to apply themselves, pay attention, work harder, etc. only to repeatedly fail out of college, get fired from jobs, and generally struggle to \u2018human\u2019 correctly,\u201d says Caite Palmer, manual review analyst of security operations at GitHub. \u201cThese folks are now through all departments and levels at this large, successful company getting to do great work in a place where flexibility, asking a million questions, and problem solving are generally considered tremendous assets and encouraged.\u201d.\n\nSuggestions for providing flexibility and accommodations: Listen to your employees. Identify listening mechanisms to hear concerns from your employees. An employee or group of employees may share with you how a particular process or system is difficult. Their honesty and candor is a gift and an opportunity to improve your business.\n\nIdentify listening mechanisms to hear concerns from your employees. An employee or group of employees may share with you how a particular process or system is difficult. Their honesty and candor is a gift and an opportunity to improve your business. Consider providing a budget. At GitHub we have flexible benefits that allow employees to purchase equipment and tools which can help them focus, work, and even relax. Removing barriers for employees to obtain bespoke solutions that work for them reduces costs by removing long conversations and approvals from the process.\n\nAt GitHub we have flexible benefits that allow employees to purchase equipment and tools which can help them focus, work, and even relax. Removing barriers for employees to obtain bespoke solutions that work for them reduces costs by removing long conversations and approvals from the process. Train managers to care. Part of the GitHub manager training program is geared to help managers foster a sense of caring and empathy for the employees in their teams. Encourage kindness across all levels of the company.\n\nPart of the GitHub manager training program is geared to help managers foster a sense of caring and empathy for the employees in their teams. Encourage kindness across all levels of the company. Be clear about tasks that need to be completed and flexible on how that task is accomplished. An autistic employee might go for a long walk and then be able to hyperfocus on a task, completing it in half the time than it would have taken if they\u2019d sat at their desk the whole time. Trust your employees to do their best work in the way that fits them and judge them on the work they produce.\n\n4. Encourage open dialogue\n\nPromote a culture of openness where employees feel comfortable discussing their needs and challenges. Consider holding regular meetings and forums to discuss topics related to neurodiversity, mental health, and well-being. With the Neurocats group, we hold monthly meetings to discuss various topics, which are important to our members. One member of the Neurocats leadership team describes their experience:\n\n\u201cWe have a voice, which we use to highlight issues our members face day to day,\u201d says Owen Niblock, senior software engineer at GitHub who works on accessibility. \u201cWe also hold monthly meetings to discuss topics from ADHD and autism to anxiety, mental health issues, and more. Over the years, we\u2019ve had some success and find we are able to lobby for changes at a company level, leading to real tangible change that benefits the whole of GitHub.\u201d\n\nEnabling open dialogue means providing avenues for these discussions to happen. But going one step further and encouraging open dialogue requires more effort.\n\nSuggestions for encouraging open and honest dialogue with the neurodivergent community at your company: Listen. You won\u2019t be able to act on every piece of feedback you receive, but showing you acknowledge and appreciate feedback and making changes whenever possible will create a culture of frank and honest discussion.\n\nYou won\u2019t be able to act on every piece of feedback you receive, but showing you acknowledge and appreciate feedback and making changes whenever possible will create a culture of frank and honest discussion. Make space. Conversations can happen at many levels and in many forums. Create space for a diverse set of feedback and ideas by accepting discussions in different formats. For example, you might have a meeting to discuss something with a follow-up GitHub Discussion to collect async ideas.\n\nConversations can happen at many levels and in many forums. Create space for a diverse set of feedback and ideas by accepting discussions in different formats. For example, you might have a meeting to discuss something with a follow-up GitHub Discussion to collect async ideas. Give access to the leadership team. At GitHub all our CoBs have an executive sponsor who\u2019s a member of GitHub\u2019s senior leadership team. This gives members an ally to raise issues with, help find solutions, and communicate problems directly upwards.\n\nAt GitHub all our CoBs have an executive sponsor who\u2019s a member of GitHub\u2019s senior leadership team. This gives members an ally to raise issues with, help find solutions, and communicate problems directly upwards. Lead from the very top. Provide opportunities for CoBs to meet with senior leadership. For example, this year, each of GitHub\u2019s CoBs have a meeting with GitHub\u2019s CEO as part of a listening tour. This shows that the community\u2019s views are important to the whole business.\n\n5. Celebrate neurodiversity\n\nAcknowledge and celebrate the unique contributions of neurodiverse employees. Recognize their achievements, provide opportunities for career advancement, and ensure they have a voice in the organization. Celebrating Disability Pride Month and other related events can help raise awareness and appreciation within the company.\n\n\u201cNeurocats was the first time I found people like me not only represented at work, but celebrated and successful,\u201d Palmer says. \u201cSharing the rough days, the burnout, the overwhelm and frustration, but also the wins of finally getting appropriate support, being seen as creative instead of weird, and getting to learn about all the different ways brains can function.\u201d\n\nCelebrations should come not just from the community but also from leadership and the People Team. Sharing posts about the company\u2019s mental health benefits during Mental Health Month (in May) or sharing information about the community during meetings or training can all help to celebrate your diverse workforce.\n\nSuggestions for celebrating neurodiversity: Celebrations should come from all levels of the company. An internal post by a member of the leadership team or an informative post by the People team or Human Resources can show your company\u2019s support.\n\nAn internal post by a member of the leadership team or an informative post by the People team or Human Resources can show your company\u2019s support. Don\u2019t sugar coat it. Being neurodivergent can be hard. It\u2019s not a superpower. Celebrate the diversity and the wins, but never minimize the real struggles that many neurodivergent people deal with every day.\n\nBeing neurodivergent can be hard. It\u2019s not a superpower. Celebrate the diversity and the wins, but never minimize the real struggles that many neurodivergent people deal with every day. Make it visible. Share things in the format that makes most sense and will reach the most people.\n\nShare things in the format that makes most sense and will reach the most people. Promote. Ask members of the community to share information on your company blog or website.\n\nBy implementing these strategies, you can create an inclusive environment where neurodivergent employees feel valued, supported, and empowered to contribute their best work.\n\n\u201cNeurocats provided an environment that made me feel safe and confident in an astonishingly short amount of time, allowing me to bring my A game, leverage my strengths, and make a positive impact much sooner than usual,\u201d says Julie Kang, staff manager of software engineering at GitHub. \u201cThe support and understanding here have been truly transformative for my professional growth, and I feel equipped to pay this forward to my peers and reports.\u201d\n\nInterested in learning more about GitHub\u2019s approach to accessibility? Visit accessibility.github.com.\n\nTags:", "label": 0}
{"title": "Part 1: A Survey of Analytics Engineering Work at Netflix", "url": "https://netflixtechblog.com/part-1-a-survey-of-analytics-engineering-work-at-netflix-d761cfd551ee?source=collection_home---4------8-----------------------", "content": "Part 1: A Survey of Analytics Engineering Work at Netflix Netflix Technology Blog 7 min read \u00b7 Dec 17, 2024 -- 2 Listen Share\n\nThis article is the first in a multi-part series sharing a breadth of Analytics Engineering work at Netflix, recently presented as part of our annual internal Analytics Engineering conference. We kick off with a few topics focused on how we\u2019re empowering Netflix to efficiently produce and effectively deliver high quality, actionable analytic insights across the company. Subsequent posts will detail examples of exciting analytic engineering domain applications and aspects of the technical craft.\n\nAt Netflix, we seek to entertain the world by ensuring our members find the shows and movies that will thrill them. Analytics at Netflix powers everything from understanding what content will excite and bring members back for more to how we should produce and distribute a content slate that maximizes member joy. Analytics Engineers deliver these insights by establishing deep business and product partnerships; translating business challenges into solutions that unblock critical decisions; and designing, building, and maintaining end-to-end analytical systems.\n\nEach year, we bring the Analytics Engineering community together for an Analytics Summit \u2014 a 3-day internal conference to share analytical deliverables across Netflix, discuss analytic practice, and build relationships within the community. We covered a broad array of exciting topics and wanted to spotlight a few to give you a taste of what we\u2019re working on across Analytics Engineering at Netflix!\n\nDataJunction: Unifying Experimentation and Analytics\n\nYian Shang, Anh Le\n\nAt Netflix, like in many organizations, creating and using metrics is often more complex than it should be. Metric definitions are often scattered across various databases, documentation sites, and code repositories, making it difficult for analysts and data scientists to find reliable information quickly. This fragmentation leads to inconsistencies and wastes valuable time as teams end up reinventing metrics or seeking clarification on definitions that should be standardized and readily accessible.\n\nEnter DataJunction (DJ). DJ acts as a central store where metric definitions can live and evolve. Once a metric owner has registered a metric into DJ, metric consumers throughout the organization can apply that same metric definition to a set of filtered records and aggregate to any dimensional grain.\n\nAs an example, imagine an analyst wanting to create a \u201cTotal Streaming Hours\u201d metric. To add this metric to DJ, they need to provide two pieces of information:\n\nThe fact table that the metric comes from:\n\nSELECT\n\naccount_id, country_iso_code, streaming_hours\n\nFROM streaming_fact_table\n\nThe metric expression:\n\n`SUM(streaming_hours)`\n\nThen metric consumers throughout the organization can call DJ to request either the SQL or the resulting data. For example,\n\ntotal_streaming_hours of each account:\n\ndj.sql(metrics=[\u201ctotal_streaming_hours\u201d], dimensions=[\u201caccount_id\u201d]))\n\ntotal_streaming_hours of each country:\n\ndj.sql(metrics=[\u201ctotal_streaming_hours\u201d], dimensions=[\u201ccountry_iso_code\u201d]))\n\ntotal_streaming_hours of each account in the US:\n\ndj.sql(metrics=[\u201ctotal_streaming_hours\u201d], dimensions=[\u201ccountry_iso_code\u201d], filters=[\u201ccountry_iso_code = \u2018US\u2019\u201d]))\n\nThe key here is that DJ can perform the dimensional join on users\u2019 behalf. If country_iso_code doesn\u2019t already exist in the fact table, the metric owner only needs to tell DJ that account_id is the foreign key to an `users_dimension_table` (we call this process \u201cdimension linking\u201d). DJ then can perform the joins to bring in any requested dimensions from `users_dimension_table`.\n\nThe Netflix Experimentation Platform heavily leverages this feature today by treating cell assignment as just another dimension that it asks DJ to bring in. For example, to compare the average streaming hours in cell A vs cell B, the Experimentation Platform relies on DJ to bring in \u201ccell_assignment\u201d as a user\u2019s dimension (no different from country_iso_code). A metric can therefore be defined once in DJ and be made available across analytics dashboards and experimentation analysis.\n\nDJ has a strong pedigree\u2013there are several prior semantic layers in the industry (e.g. Minerva at Airbnb; dbt Transform, Looker, and AtScale as paid solutions). DJ stands out as an open source solution that is actively developed and stress-tested at Netflix. We\u2019d love to see DJ easing your metric creation and consumption pain points!\n\nLORE: How we\u2019re democratizing analytics at Netflix\n\nApurva Kansara\n\nAt Netflix, we rely on data and analytics to inform critical business decisions. Over time, this has resulted in large numbers of dashboard products. While such analytics products are tremendously useful, we noticed a few trends:\n\nA large portion of such products have less than 5 MAU (monthly active users) We spend a tremendous amount of time building and maintaining business metrics and dimensions We see inconsistencies in how a particular metric is calculated, presented, and maintained across the Data & Insights organization. It is challenging to scale such bespoke solutions to ever-changing and increasingly complex business needs.\n\nAnalytics Enablement is a collection of initiatives across Data & Insights all focused on empowering Netflix analytic practitioners to efficiently produce and effectively deliver high-quality, actionable insights.\n\nSpecifically, these initiatives are focused on enabling analytics rather than on the activities that produce analytics (e.g., dashboarding, analysis, research, etc.).\n\nAs part of broad analytics enablement across all business domains, we invested in a chatbot to provide real insights to our end users using the power of LLM. One reason LLMs are well suited for such problems is that they tie the versatility of natural language with the power of data query to enable our business users to query data that would otherwise require sophisticated knowledge of underlying data models.\n\nBesides providing the end user with an instant answer in a preferred data visualization, LORE instantly learns from the user\u2019s feedback. This allows us to teach LLM a context-rich understanding of internal business metrics that were previously locked in custom code for each of the dashboard products.\n\nSome of the challenges we run into:\n\nGaining user trust: To gain our end users\u2019 trust, we focused on our model\u2019s explainability. For example, LORE provides human-readable reasoning on how it arrived at the answer that users can cross-verify. LORE also provides a confidence score to our end users based on its grounding in the domain space.\n\nTraining: We created easy-to-provide feedback using \ud83d\udc4d and \ud83d\udc4e with a fully integrated fine-tuning loop to allow end-users to teach new domains and questions around it effectively. This allowed us to bootstrap LORE across several domains within Netflix.\n\nDemocratizing analytics can unlock the tremendous potential of data for everyone within the company. With Analytics enablement and LORE, we\u2019ve enabled our business users to truly have a conversation with the data.\n\nLeveraging Foundational Platform Data to enable Cloud Efficiency Analytics\n\nJ Han, Pallavi Phadnis\n\nAt Netflix, we use Amazon Web Services (AWS) for our cloud infrastructure needs, such as compute, storage, and networking to build and run the streaming platform that we love. Our ecosystem enables engineering teams to run applications and services at scale, utilizing a mix of open-source and proprietary solutions. In order to understand how efficiently we operate in this diverse technological landscape, the Data & Insights organization partners closely with our engineering teams to share key efficiency metrics, empowering internal stakeholders to make informed business decisions.\n\nThis is where our team, Platform DSE (Data Science Engineering), comes in to enable our engineering partners to understand what resources they\u2019re using, how effectively they utilize those resources, and the cost associated with their resource usage. By creating curated datasets and democratizing access via a custom insights app and various integration points, downstream users can gain granular insights essential for making data-driven, cost-effective decisions for the business.\n\nTo address the numerous analytic needs in a scalable way, we\u2019ve developed a two-component solution:\n\nFoundational Platform Data (FPD): This component provides a centralized data layer for all platform data, featuring a consistent data model and standardized data processing methodology. We work with different platform data providers to get inventory, ownership, and usage data for the respective platforms they own. Cloud Efficiency Analytics (CEA): Built on top of FPD, this component offers an analytics data layer that provides time series efficiency metrics across various business use cases. Once the foundational data is ready, CEA consumes inventory, ownership, and usage data and applies the appropriate business logic to produce cost and ownership attribution at various granularities.\n\nAs the source of truth for efficiency metrics, our team\u2019s tenants are to provide accurate, reliable, and accessible data, comprehensive documentation to navigate the complexity of the efficiency space, and well-defined Service Level Agreements (SLAs) to set expectations with downstream consumers during delays, outages, or changes.\n\nLooking ahead, we aim to continue onboarding platforms, striving for nearly complete cost insight coverage. We\u2019re also exploring new use cases, such as tailored reports for platforms, predictive analytics for optimizing usage and detecting anomalies in cost, and a root cause analysis tool using LLMs.\n\nUltimately, our goal is to enable our engineering organization to make efficiency-conscious decisions when building and maintaining the myriad of services that allows us to enjoy Netflix as a streaming service. For more detail on our modeling approach and principles, check out this post!", "label": 0}
{"title": "IndieWeb Carnival en Diciembre 2023 \u2014 Tradiciones y Celebraciones de Diciembre", "url": "https://lifeofpablo.com/blog/tradiciones-de-diciembre", "content": "Me encanta mucho el mes de Diciembre. Cuando se acerca la navidad, me gusta cuando hay nieve en el suelo. Me gustan las celebraciones y tradiciones en las que particip\u00f3. Yo celebro tradiciones mexicanas y celebraciones estadounidenses. Yo vivo entre las dos culturas y crec\u00ed en los Estados Unidos y he vivido en M\u00e9xico. En casa lo celebr\u00f3 como si estuviera en M\u00e9xico.\n\nCada a\u00f1o es diferente. A veces estoy en Oaxaca durante el mes de diciembre y a veces estoy en Estados Unidos. Celebr\u00f3 ah\u00ed las tradiciones Mexicanas pero m\u00e1s precisamente, las tradiciones Oaxaque\u00f1as. Me gusta ir a ver los bailes y los desfiles. Me gusta ir a los mercados de comida y probar todas las comidas deliciosas. Se nos hace agua en la boca.\n\nCelebraciones Mexicanas:\n\nEn mi familia nos gusta mucho cocinar en casa. Hacemos muchos platillos tradicionales. Hacemos tamales de varios sabores, ponche mexicano, mole, pasteles, cocteles de camar\u00f3n, y much\u00edsimo m\u00e1s! No hay l\u00edmite. En este mes, normalmente no comemos en la calle. \u00a1La comida en casa siempre es m\u00e1s rica!\n\nEn las noches siempre vemos unas pel\u00edculas de navidad. Nos gusta ver pel\u00edculas en Espa\u00f1ol. Cada a\u00f1o vemos Home Alone en espa\u00f1ol porque es muy chistosa. Simplemente estar juntos es importante. Podemos re\u00edr entre todos.\n\nSiempre ponemos y arreglamos el \u00e1rbol de navidad. Nosotros abrimos los regalos en la noche del 24 de diciembre. Siempre ha sido as\u00ed.\n\nCelebraciones Estadounidenses:\n\nSiempre me reuno con mis amigos. Ahora que no vivimos en la misma regi\u00f3n, es muy importante estar juntos. Hablamos como nos fue este a\u00f1o y que queremos cumplir para el a\u00f1o entrante. Algunas cosas que hacemos son casas de jengibres y tenemos una fiesta. Dependiendo como est\u00e9 el clima, andamos en patineta o vamos al skatepark.\n\nLo que siempre me anima es la noche de programaci\u00f3n. Hacemos un plan en que vamos a programar en una noche. Nos quedamos despiertos hasta la madrugada. Es algo muy bonito de programar algo juntos. Creo que este a\u00f1o vamos hacer un video juego. A ver como nos va.\n\nEsto es mi respuesta al IndieWeb Carnival December 2023 \u2014 Holiday and December traditions \u00a1Muchas Gracias Jo! Todos pueden participar. Si deseas participar en el IndieWeb Carnival haz clic aqui para informarte.\n\nSyndicated on IndieNews ES", "label": 1}
{"title": "ChatGPT combines Code Lyoko and Digimon Frontier", "url": "https://lifeofpablo.com/blog/chatgpt-combines-code-lyoko-and-digimon-frontier", "content": "ChatGPT combines Code Lyoko and Digimon Frontier\n\nThis post was written in English (en_US).\n\nIntroduction\n\nI started messing with AI a lot more. One of my students got me doing more with it! Thanks Ryan! I started out with ChatGPT from OpenAI. I've been thinking about tv show crossovers. Crossovers are fun! Especially on animated shows.\n\nEvery since I was a kid I've really enjoyed two animated shows, Digimon Frontier and Code Lyoko. They both involved the digital/virtual world and the real world.\n\nCode Lyoko tends to show a lot more regarding how \"AI\" can go wrong. I will write a blog post on Code Lyoko very soon!\n\nBackground on Code Lyoko and Digimon Frontier\n\nCode Lyoko is a French animated television series that was produced by MoonScoop Group and aired from 2003 to 2007. It follows the adventures of a group of four students - Jeremie, Ulrich, Odd, and Yumi - who discover a virtual world called Lyoko. In Lyoko, they fight against an evil artificial intelligence named XANA that seeks to take over both the virtual and real worlds.\n\nDigimon Frontier is the fourth season of the Digimon anime series. It aired in Japan from 2002 to 2003 and was dubbed into English and aired in North America in 2004. The show follows a group of five children - Takuya, Koji, Zoe, J.P., and Tommy - who are transported to the Digital World to fight against an evil force known as the Royal Knights.\n\nUnlike previous seasons of Digimon, the children in Digimon Frontier transform into Digimon themselves and fight alongside their partner Digimon. Each of the children has a unique spirit that allows them to transform into a different Digimon, and they must learn to master their powers in order to save the Digital World.\n\nThe show is known for its action-packed battles, complex characters, and intricate\n\nChatGPT Combining Code Lyoko and Digimon Frontier\n\nI ask ChatGPT to create a story line combining both worlds and I got this. I really like this.!\n\nCombining Digimon Frontier and Code Lyoko would create an interesting story that combines the elements of both shows. The concept of a virtual world threatened by an evil force is a common thread between the two shows, and could serve as a basis for a crossover.\n\nIn this hypothetical scenario, the characters from Digimon Frontier would find themselves transported to the world of Code Lyoko, where they must team up with the Lyoko warriors to defeat XANA and save both the digital and real worlds from destruction. The Digimon would bring their unique powers and abilities to the fight, while the Lyoko warriors would provide their expertise in hacking and computer programming.\n\nTogether, the two groups would explore the virtual world of Lyoko, battling monsters and uncovering XANA's plans. The conflict between XANA and the Digimon's arch-nemesis, the evil Digimon Emperor, could also be woven into the story to add an additional layer of complexity.\n\nOverall, a crossover between Digimon Frontier and Code Lyoko could create an exciting and action-packed adventure that would appeal to fans of both shows.\n\nQuestion\n\nDo you think we will ever get to this point where we need to fight the evils of the digital/virtual worlds? Honestly, I think we will someday. I'd love to hear from you all.", "label": 1}
{"title": "How Stripe\u2019s document databases supported 99.999% uptime with zero-downtime data migrations", "url": "https://stripe.com/blog/how-stripes-document-databases-supported-99.999-uptime-with-zero-downtime-data-migrations", "content": "In 2023, Stripe processed $1 trillion in total payments volume, all while maintaining an uptime of 99.999%. We obsess over reliability. As engineers on the database infrastructure team, we provide a database-as-a-service (DBaaS) called DocDB as a foundation layer for our APIs.\n\nStripe\u2019s DocDB is an extension of MongoDB Community\u2014a popular open-source database\u2014and consists of a set of services that we built in-house. It serves over five million queries per second from Stripe\u2019s product applications. Our deployment is also highly customized to provide low latency and diverse access, with 10,000+ distinct query shapes over petabytes of important financial data that lives in 5,000+ collections distributed over 2,000+ database shards.\n\nWe chose to build DocDB on top of MongoDB Community because of the flexibility of its document model and its ability to handle massive volumes of real-time data at scale. MongoDB Atlas didn\u2019t exist in 2011, so we built a self-managed cluster of MongoDB instances running in the cloud.\n\nAt the heart of DocDB is the Data Movement Platform. Built originally as a horizontal scaling solution to overcome vertical scaling limits of MongoDB compute and storage, we customized it to serve multiple purposes: merging underutilized database shards for improved utilization and efficiency, upgrading the major version of the database engine in our fleet for reliability, and transitioning databases from a multitenant arrangement to single tenancy for large users.\n\nThe Data Movement Platform enabled our transition from running a small number of database shards (each with tens of terabytes of data) to thousands of database shards (each with a fraction of the original data). It also provides client-transparent migrations with zero downtime, which makes it possible to build a highly elastic DBaaS offering. DocDB can split database shards during traffic surges and consolidate thousands of databases through bin packing when traffic is low.\n\nIn this blog post we\u2019ll share an overview of Stripe\u2019s database infrastructure, and discuss the design and application of the Data Movement Platform.\n\nHow we built our database infrastructure\n\nWhen Stripe launched in 2011, we chose MongoDB as our online database because it offered better developer productivity than standard relational databases. On top of MongoDB, we wanted to operate a robust database infrastructure that prioritized the reliability of our APIs, but we could not find an off-the-shelf DBaaS that met our requirements:\n\nMeeting the highest standards of availability, durability, and performance\n\nExposing a minimal set of database functions to avert self-inflicted issues due to suboptimal queries from client applications\n\nSupporting horizontal scalability with sharding\n\nOffering first-class support for multitenancy with enforced quotas\n\nProviding strong security through enforcement of authorization policies\n\nThe solution was to build DocDB\u2014with MongoDB as the underlying storage engine\u2014a truly elastic and scalable DBaaS, with online data migrations at its core.\n\nProduct applications at Stripe access data in their database through a fleet of database proxy servers, which we developed in-house in Go to enforce concerns of reliability, scalability, admission control, and access control. As a mechanism to horizontally scale, we made the key architectural decision to employ sharding. (If you want to learn more about database sharding, this is a helpful primer.)\n\nThousands of database shards, each housing a small chunk of the cumulative data, now underlie all of Stripe\u2019s products. When an application sends a query to a database proxy server, it parses the query, routes it to one or more shards, combines the results from the shards, and returns them back to the application.\n\nBut how do database proxy servers know which among thousands of shards to route the query to? They rely on a chunk metadata service that maps chunks to database shards, making it easy to look up the relevant shards for a given query. In line with typical database infrastructure stacks, change events resulting from writes to the database are transported to streaming software systems, and eventually archived in an object store via the change data capture (CDC) pipeline.\n\nHigh-level overview of Stripe\u2019s database infrastructure\n\nAt the product application level, teams at Stripe use the in-house document database control plane to provision a logical container for their data\u2014referred to as a logical database\u2014housing one or more DocDB collections, and each comprising documents that have a related purpose. Data in these DocDB collections is distributed across several databases (referred to as physical databases), each of which is home to a small chunk of the collection. Physical databases on DocDB live on shards deployed as replica sets that comprise a primary node and several secondary nodes with replication and automated failover.\n\nA sharded collection\n\nHow we designed the Data Movement Platform\n\nIn order to build a DBaaS offering that is horizontally scalable and highly elastic\u2014one that can scale in and out with the needs of the product applications\u2014we needed the ability to migrate data across database shards in a client-transparent manner with zero downtime. This is a complex distributed systems problem, one that is further compounded by the unique requirements of important financial data:\n\nData consistency and completeness: We need to ensure that the data being migrated remains consistent and complete across both the source and target shards.\n\nWe need to ensure that the data being migrated remains consistent and complete across both the source and target shards. Availability: Prolonged downtime during data migration is unacceptable, as millions of businesses count on Stripe to accept payments from their customers 24 hours a day. Our goal is to keep the key phase of the migration process shorter than the duration of a planned database primary failover\u2014typically lasting a few seconds, and in line with the retry budget of our product applications.\n\nProlonged downtime during data migration is unacceptable, as millions of businesses count on Stripe to accept payments from their customers 24 hours a day. Our goal is to keep the key phase of the migration process shorter than the duration of a planned database primary failover\u2014typically lasting a few seconds, and in line with the retry budget of our product applications. Granularity and adaptability: At Stripe\u2019s scale, we need to support the migration of an arbitrary number of chunks of data from any number of sources to target shards\u2014with no restrictions on the number of in-flight database chunk migrations in the fleet, and no restrictions on the number of migrations any given shard can participate in at any point in time. We also need to accommodate the migration of chunks of varying sizes at a high throughput, as several of our database shards contain terabytes of data.\n\nAt Stripe\u2019s scale, we need to support the migration of an arbitrary number of chunks of data from any number of sources to target shards\u2014with no restrictions on the number of in-flight database chunk migrations in the fleet, and no restrictions on the number of migrations any given shard can participate in at any point in time. We also need to accommodate the migration of chunks of varying sizes at a high throughput, as several of our database shards contain terabytes of data. No performance impact to source shard: When we migrate database chunks across shards, our goal is to preserve the performance and throughput of the source shard to preclude any adverse impact on performance and available throughput for user queries.\n\nTo address these requirements, we built the Data Movement Platform to manage online data migrations across database shards by invoking purpose-built services.\n\nData Movement Platform within our database infrastructure stack\n\nThe Coordinator component in the Data Movement Platform is responsible for orchestrating the various steps involved in online data migrations\u2014it invokes the relevant services to accomplish each of the constituent steps outlined below:\n\nStep 1: Chunk migration registration\n\nFirst we register the intent to migrate database chunks from their source shards to arbitrary target shards in the chunk metadata service. Subsequently, we build indexes on the target shards for the chunks being migrated.\n\nStep 2: Bulk data import\n\nNext, we use a snapshot of the chunks on the source shards at a specific time, denoted as time T, to load the data onto one or more database shards. The service responsible for performing bulk data import accepts various data filters, and only imports the chunks of data that satisfy the filtering criteria. While this step appeared simple at first, we encountered throughput limitations when bulk loading data onto a DocDB shard. Despite attempts to address this by batching writes and adjusting DocDB engine parameters for optimal bulk data ingestion, we had little success.\n\nHowever, we achieved a significant breakthrough when we explored methods to optimize our insertion order, taking advantage of the fact that DocDB arranges its data using a B-tree data structure. By sorting the data based on the most common index attributes in the collections and inserting it in sorted order, we significantly enhanced the proximity of writes\u2014boosting write throughput by 10x.\n\nStep 3: Async replication\n\nOnce we have imported the data onto the target shard, we begin replicating writes starting at time T from the source to the target shard for the database chunks being migrated. Our async replication systems read the mutations resulting from writes on the source shards from the CDC systems and issue writes to the target shards.\n\nThe operations log, or oplog, is a special collection on each DocDB shard that keeps a record of all the operations that mutate data in databases on that shard. We transport the oplog from every DocDB shard to Kafka, an event streaming platform, and then archive it to a cloud object storage service such as Amazon S3. (If you want to learn more about oplog, this is a helpful primer.)\n\nWe built a service to replicate mutations from one or more source DocDB shards to one or more target DocDB shards using the oplog events in Kafka and Amazon S3. We relied on the oplog events from our CDC systems to ensure that we didn\u2019t slow user queries by consuming read throughput that would otherwise be available to user queries on the source shard, and to avoid being constrained by the size of the oplog on the source shard. We designed the service to be resilient to target shard unavailability, and to support starting, pausing, and resuming synchronization from a checkpoint at any point in time. The replication service also exposes the functionality to fetch the replication lag.\n\nMutations of the chunks under migration get replicated bidirectionally\u2014from the source shards to the target shards and vice versa\u2014and the replication service tags the writes it issues to avert cyclical asynchronous replication. We made this design choice to provide the flexibility to revert traffic to the source shards if any issues emerge when directing traffic to the target shards.\n\nStep 4: Correctness check\n\nAfter the replication syncs between the source and target shard, we conduct a comprehensive check for data completeness and correctness by comparing point-in-time snapshots\u2014a deliberate design choice we made in order to avoid impacting shard throughput.\n\nStep 5: Traffic switch\n\nOnce the data in a chunk is imported from the source to the target shard\u2014and mutations are actively replicated\u2014a traffic switch is orchestrated by the Coordinator. In order to reroute reads and writes to the chunk of data being migrated, we need to first: stop the traffic on the source shard for a brief period of time, update the routes in the chunk metadata service, and have the proxy servers redirect reads and writes to the target shards.\n\nThe traffic switch protocol is based on the idea of versioned gating. In steady state, each proxy server annotates requests to DocDB shards with a version token number. We added a custom patch to MongoDB that allows a shard to enforce that the version token number it receives on requests from the proxy servers is newer than the version token number it knows of\u2014and only serve requests that satisfy this criterion. To update the route for a chunk, we use the Coordinator to execute the following steps:\n\nFirst, we bump up the version token number on the source DocDB shard. The version token number is stored in a document in a special collection in DocDB, and all reads and writes on the chunk on the source shard are rejected at this point.\n\nThen, we wait for the replication service to replicate any outstanding writes on the source shard.\n\nLastly, we update the route for the chunk to point to the target shard and the version token number in the chunk metadata service.\n\nTraffic switch process\n\nUpon completion, the proxy servers fetch the updated routes for the chunk and the most up-to-date version token number from the chunk metadata service. Using the updated routes for the chunk, the proxy servers route reads and writes for the chunk to the target shard. The entire traffic switch protocol takes less than two seconds to execute, and all failed reads and writes directed to the source shard succeed on retries.\n\nStep 6: Chunk migration deregistration\n\nFinally, we conclude the migration process by marking the migration as complete in the chunk metadata service and subsequently dropping the chunk data from the source shard.\n\nApplications of the Data Movement Platform\n\nThe ability to migrate chunks of data across DocDB shards in an online manner helps us horizontally scale our database infrastructure to keep pace with the growth of Stripe. Engineers on the database infrastructure team are able to split DocDB shards for size and throughput with a click of a button, freeing up database storage and throughput headroom for product teams.\n\nIn 2023, we used the Data Movement Platform to improve the utilization of our database infrastructure. Concretely, we bin-packed thousands of underutilized databases by migrating 1.5 petabytes of data transparent to product applications, and reduced the total number of underlying DocDB shards by approximately three quarters. We also used the Data Movement Platform to upgrade our database infrastructure fleet by fork-lifting data to a later version of MongoDB in one step\u2014without going through intermediate major and minor versions with an in-place upgrade strategy.\n\nThe database infrastructure team at Stripe is focused on building a robust and reliable foundation that scales with the growth of the internet economy. We are currently prototyping a heat management system that proactively balances data across shards based on size and throughput, and investing in shard autoscaling that dynamically responds to changes in traffic patterns.\n\nAt Stripe, we\u2019re excited to solve hard distributed systems problems. If you are too, consider joining our engineering team.", "label": 0}
{"title": "Building a more accessible GitHub CLI", "url": "https://github.blog/engineering/user-experience/building-a-more-accessible-github-cli/", "content": "At GitHub, we\u2019re committed to making our tools truly accessible for every developer, regardless of ability or toolset. The command line interface (CLI) is a vital part of the developer experience, and the GitHub CLI is our product that brings the power of GitHub to your terminal.\n\nWhen it comes to accessibility, the terminal is fundamentally different from a web browser or a graphical user interface, with a lineage that predates the web itself. While standards like the Web Content Accessibility Guidelines (WCAG) provide a clear path for making web and graphical applications accessible, there is no equivalent, comprehensive standard for the terminal and CLIs. The W3C offers some high-level guidance for non-web software, but it stops short of prescribing concrete techniques, leaving much open to interpretation and innovation.\n\nThis gap has challenged us to think creatively and purposefully about what accessibility should look like in the terminal. Our recent Public Preview is focused on addressing the needs of three key groups: users who rely on screen readers, users who need high contrast between background and text, and users who require customizable color options. Our work aims to make the GitHub CLI more inclusive for all, regardless of how you interact with your terminal. Run gh a11y in the latest version of the GitHub CLI to enable these features, or read on to learn about our path to designing and implementing them.\n\nUnderstanding the terminal landscape\n\nText-based and command-line applications differ fundamentally from graphical or web applications. On a web page, assistive technologies like screen readers make use of the document object model (DOM) to infer structure and context of the page. Web pages can be designed such that the DOM\u2019s structure is friendly to these technologies without impacting the visual design of the page. By contrast, CLI\u2019s primary output is plain text, without hidden markup. A terminal emulator acts as the \u201cuser agent\u201d for text apps, rendering characters as directed by the server application. Assistive technologies access this matrix of characters, analyze its layout, and try to infer structure. As the WCAG2ICT guidance notes, accessibility in this space means ensuring that all text output is available to assistive technologies, and that structural information is conveyed in a way that\u2019s programmatically determinable\u2014even if no explicit markup is present.\n\nIn our quest to improve the GitHub CLI\u2019s usability for blind, low-vision, and colorblind users, we found ourselves navigating a landscape with lots of guidance, but few concrete techniques for implementing accessible experiences. We studied how assistive technology interacts with terminals: how screen readers review output, how color and contrast can be customized, and how structural cues can be inferred from plain text. Our recent Public Preview contains explorations into various use cases in these spaces.\n\nRethinking prompts and progress for screen readers\n\nOne of the GitHub CLI\u2019s strengths as a command-line application is its rich prompting experience, which gives our users an interactive interface to enter command options. However, this rich interactive experience poses a hurdle for speech synthesis screen readers: Non-alphanumeric visual cues and uses of constant screen redraws for visual or other effects can be tricky to correctly interpret as speech.\n\nA demo video with sound of screen reader reading legacy prompter.\n\nTo reduce confusion and make it easier for blind and low vision users to confidently answer questions and navigate choices, we\u2019re introducing a prompting experience that allows speech synthesis screen readers to accurately convey prompts to users. Our new prompter is built using Charm\u2019s open source charmbracelet/huh prompting library.\n\n\n\nA demo of a screenreader correctly reading a prompt.\n\nAnother use case where the terminal is redrawn for visual effect is when showing progress bars. Our existing implementation uses a \u201cspinner\u201d made by redrawing the screen to display different braille characters (yes, we appreciate the irony) to give the user the indication that their command is executing. Speech synthesis screen readers do not handle this well:\n\n\n\nA demo of a screenreader and an old spinner.\n\nThis has been replaced with a static text progress indicator (with a relevant message to the action being taken where possible, falling back to a general \u201cWorking\u2026\u201d message). We\u2019re working on identifying other areas we can further improve the contextual text.\n\nA demo video of the new progress indicator experience.\n\nColor, contrast, and customization\n\nColor is more than decoration in the terminal: It\u2019s a vital tool for highlighting information, signaling errors, and guiding workflows. But color can also be a barrier\u2014if contrast between the color of the terminal background and the text displayed on it is too low, some users will have difficulty discerning the displayed information. Unlike in a web browser, a terminal\u2019s background color is not set by the application. That task is handled by the user\u2019s terminal emulator. In order to maintain contrast, it is important that a command line application takes into account this variable. Our legacy color palette used for rendering Markdown did not take the terminal\u2019s background color into account, leading to low contrast in some cases.\n\nThe colors themselves also matter. Different terminal environments have varied color capabilities (some support 4-bit, some 8-bit, some 24-bit, etc). No matter the capability, terminals enable users to customize their color preferences, choosing how different hues are displayed. However, most terminals only support changing a limited subset of colors: namely, the sixteen colors in the ANSI 4-bit color table. The GitHub CLI has made extensive efforts to align our color palettes to 4-bit colors so our users can completely customize their experience using their terminal preferences. We built on top of the accessibility foundations pioneered by Primer when deciding which 4-bit colors to use.\n\nBuilding for the CLI community\n\nOur improvements aim to support a wide range of developer needs, from blind users who need screen readers, to low vision users who need high contrast, to colorblind users who require customizable color options. But this Public Preview does not mark the end of our team\u2019s commitment to enabling all developers to use the GitHub CLI. We intend to make it easier for our extension authors to implement the same accessibility improvements that we\u2019ve made to the core CLI. This will allow users to have a cohesive experience across all GitHub CLI commands, official or community-maintained, and so that more workflows can be made accessible by default. We\u2019re also looking into experiences to customize the formatting of tables output by commands to be more easily read/interpreted by screen readers. We\u2019re excited to continue our accessibility journey.\n\n\n\nWe couldn\u2019t have come this far without collaboration with our friends at Charm and our colleagues on the GitHub Accessibility team.\n\nA call for feedback\n\nWe invite you to help us in our goal to make the GitHub CLI an experience for all developers:\n\nTry it out : Update the GitHub CLI to v2.72.0 and run gh a11y in your terminal to learn more about enabling these new accessible features.\n\n: Update the GitHub CLI to v2.72.0 and run in your terminal to learn more about enabling these new accessible features. Share your experience : Join our GitHub CLI accessibility discussion to provide feedback or suggestions.\n\n: Join our GitHub CLI accessibility discussion to provide feedback or suggestions. Connect with us: If you have a lived experience relevant to our accessibility personas, reach out to the accessibility team or get involved in our discussion panel.\n\nLooking forward\n\nAdapting accessibility standards for the command line is a challenge\u2014and an opportunity. We\u2019re committed to sharing our approach, learning from the community, and helping set a new standard for accessible CLI tools.\n\nThank you for building a more accessible GitHub with us.\n\nWant to help us make GitHub the home for all developers? Learn more about GitHub\u2019s accessibility efforts.", "label": 0}
{"title": "September 2023", "url": "https://lifeofpablo.com/blog/published:2023-09", "content": "My Letter to UNK and NU System\n\nThis week I found out that the university \"needs\" to perform a $3 million budget cut at the University of Nebraska at Kearney (UNK). This is angering and saddening. This creates a huge hole on the UNK campus. I graduated from the university in 2019. I graduated with a B.A. in Modern Languages. This hurts to hear that many programs such as the French program are being cut. The Modern Language Department was my home during my time at the University. It pushed me out of my comfort zone that led me to even great worldview experiences such as study abroad. The French program in Modern Languages helped prepare me to become a teacher and helped to interact with people of all backgrounds and cultures.\n\nUNK needs to have a complete modern language department to prepare students in this globalized world. Learning a language isn't about learning simply a language. Languages expand a person's worldview. You learn culture, you learn experiences, you learn how different societies work. We can't have young people's viewpoints and perspectives stuck glued to the environment they are in. They need to branch out. Going to college is a place where people learn to build deeper connections with others. This includes people they interact with daily and people who are different from them culturally. The modern languages department is such a vital program.\n\nWhat hurts more is the attack on the arts and humanities. The arts and humanities are important for a well rounded education. We need students to explore subjects outside of their majors. I learned a lot from the humanities and the arts.\n\nI ask the University of Nebraska not to eliminate these programs. If the entire university system wants to retain more people in Nebraska, before entering, during college, and post college, we must allow these opportunities.\n\nUNK is the only proper public university in Nebraska that geographically serves so many students in the western half of the state. UNK offers so many programs closer to home that many in years past had to attend schools further away from home to enter the programs needed.\n\nI am proud to be a graduate of the University of Nebraska at Kearney. This saddens me to see UNK and the entire university system go through this dark time. Please reconsider all the damage that is to come. Every single higher position university administrator and faculty must fight for the interest of students - current and present.\n\nRepresentatives of the Nebraska University system need to find and pressure funding from the state. The state has done so much damage from cutting budgets year after year. We must fight those who are against public education and higher education who put us in a chokehold.\n\nWe are currently facing so many attacks on public education and higher education from various places. The University of Nebraska system is no exception.\n\nPablo Morales\n\nUniversity of Nebraska at Kearney Graduate 2019", "label": 1}
{"title": "Local Speed, Smooth Deploys: Heroku Adds Support for uv", "url": "https://www.heroku.com/blog/local-speed-smooth-deploys-heroku-adds-support-uv/", "content": "Ah, another day, another deep dive into the ever-evolving world of Python development! Today, let\u2019s talk about something near and dear to every Pythonista\u2019s heart \u2013 managing those crucial external packages. For years, pip has been our trusty companion, the workhorse that gets the job done. But the landscape is shifting, and a new contender has entered the arena, promising speed, efficiency, and a fresh approach: uv.\n\nAs a Python developer constantly striving for smoother workflows and faster iterations, the buzz around uv has definitely caught my attention. So, let\u2019s roll up our sleeves and explore the benefits of using uv as your Python package manager, taking a look at where we\u2019ve come from and how uv stacks up. We\u2019ll even walk through setting up a project for Heroku deployment using this exciting new tool.\n\nA trip down memory lane: The evolution of Python package management\n\nTo truly appreciate what uv brings to the table, it\u2019s worth taking a quick stroll down memory lane and acknowledging the journey of Python package management.\n\nIn the early days, installing Python packages often involved manual downloads, unpacking, and running setup scripts. It was a far cry from the streamlined experience we have today. Then came Distutils, which provided a more standardized way to package and distribute Python software. While a significant step forward, it still lacked robust dependency resolution.\n\nEnter setuptools, which built upon Distutils and introduced features like dependency management and package indexing (the foundation for PyPI). For a long time, setuptools was the de facto standard, and its influence is still felt today.\n\nHowever, as the Python ecosystem grew exponentially, the limitations of the existing tools became more apparent. Dependency conflicts, slow installation times, and the complexities of managing virtual environments started to become significant pain points.\n\nThis paved the way for pip (Pip Installs Packages). Introduced in 2008, pip revolutionized Python package management. It provided a simple and powerful command-line interface for installing, upgrading, and uninstalling packages from PyPI and other indices. For over a decade, pip has been the go-to tool for most Python developers, and it has served us well.\n\nBut the increasing complexity of modern Python projects, with their often intricate web of dependencies, has exposed some of pip\u2019s performance bottlenecks. Resolving complex dependency trees can be time-consuming, and the installation process, while generally reliable, can sometimes feel sluggish.\n\nAnother challenge with the complexity of modern applications is package versioning. Lockfiles that pin project dependencies have become table stakes for package management. Many package management tools use them. Throughout the course of the evolution of package management in Python, we\u2019ve seen managers such as Poetry and Pipenv, just to name a few. However, many of these projects don\u2019t have dedicated teams. Sometimes this results in them not being able to keep up with the latest standards or the complex dependency trees of modern apps.\n\nThis is where the new generation of package management tools, like uv, comes into play, promising to address these very challenges, with a dedicated team behind them.\n\nEnter the speed demon: The benefits of using uv\n\nuv isn\u2019t just another package manager; it\u2019s built with a focus on speed and efficiency, leveraging modern programming languages and data structures to deliver a significantly faster experience. Here are some key benefits that have me, and many other Python developers, excited:\n\nBlazing Fast Installation: This is arguably uv\u2019s headline feature. Written in Rust from scratch using a thoughtful design approach uv significantly outperforms pip in resolving and installing dependencies, especially for large and complex projects. The difference can be dramatic, cutting down installation times from minutes to seconds in some cases. This speed boost translates directly into increased developer productivity and faster CI/CD pipelines. Efficient Dependency Resolution: uv employs sophisticated algorithms for dependency resolution, aiming to find compatible package versions quickly and efficiently. While pip has made improvements in this area, uv\u2019s underlying architecture allows it to handle complex dependency graphs with remarkable speed. This reduces the likelihood of dependency conflicts and streamlines the environment setup process. Drop-in Replacement for pip and venv : One of the most appealing aspects of uv is its ambition to be a seamless replacement for both pip and venv (Python\u2019s built-in virtual environment tool). It aims to handle package installation and virtual environment creation with a unified command-line interface. This simplifies project setup and management, reducing the cognitive load of juggling multiple tools. Compatibility with Existing Standards: uv adheres to existing Python packaging standards like pyproject.toml (PEP 621). This means that projects already using these standards can easily adopt uv without significant modifications. It reads and respects your existing pyproject.toml files, making the transition relatively smooth. uv is built with a strong emphasis on modern packaging practices, encouraging the adoption of pyproject.toml for declaring project dependencies and build system requirements. This aligns with the direction the Python packaging ecosystem is heading. Improved Error Messaging: While pip\u2019s error messages have improved over time, uv, being a newer tool, has the opportunity to provide more informative and user-friendly error messages, making debugging dependency issues easier. Potential for Future Enhancements: As a relatively new project with a dedicated development team, uv has the potential to introduce further optimizations and features that could significantly enhance the Python development experience. The active development and growing community support are promising signs.\n\nHow to use uv with Heroku\n\nNow, let\u2019s put some of this into practice. Imagine we\u2019re building a simple Python web application (using Flask, for instance) that we want to deploy to Heroku, and we want to leverage the speed and efficiency of uv in our development and deployment process.\n\nHere\u2019s how we can set up our project:\n\n1. Install uv\n\nThere are a variety of options to install uv, depending on your operating system. For a full list, take a look at the official Installation Guide site. I\u2019m going to install it using Homebrew:\n\n~/user$ brew install uv\n\n2. Create the project directory and initialize uv\n\n~/user$ uv init my-app ~/user$ cd my-app ~/user/my-app$ ls -a\n\nIn doing that, uv generates several project files\n\nmy-app/ \u251c\u2500\u2500 main.py \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 README.md \u2514\u2500\u2500 .python-version\n\nOur main.py looks like this:\n\ndef main(): print(\"Hello from my-app!\") if __name__ == \"__main__\": main()\n\nWe can run this with the uv run main.py command which does a few things for us. In addition to actually running main.py and generating the \u201cHello from my-app!\u201d output, uv also generates a virtual environment for the project and generates a uv.lock file which describes the project. More on that in a bit.\n\n3. Expanding the project\u2026 slightly.\n\nLet\u2019s take this project a bit further and turn it into a Flask app that we can deploy to Heroku. We\u2019ll need to specify our dependencies, Flask and Gunicorn for this example. We can do this using pyproject.toml .\n\nUsing pyproject.toml :\n\nThe uv generated pyproject.toml file looks like this:\n\n[project] name = \"my-app\" version = \"0.1.0\" description = \"Add your description here\" readme = \"README.md\" requires_python =\">=3.13\" dependencies = []\n\nTo add dependencies we use the uv add command.\n\n~/user/my-app$ uv add Flask ~/user/my-app$ uv add gunicorn\n\nThis accomplishes a couple of things:\n\nFirst, it adds those packages to the pyproject.toml file:\n\n[project] name = \"my-app\" version = \"0.1.0\" description = \"Add your description here\" readme = \"README.md\" requires_python =\">=3.13\" dependencies = [ \"Flask>=3.1.1\", \"gunicorn>=23.0.0\", ]\n\nSecond, it updates the uv.lock file for dependency management.\n\n4. Updating main.py\n\nLet\u2019s update the code in main.py to be a basic Flask web application\n\nfrom flask import Flask app = Flask(__name__) @app.route('/') def hello_world(): return \"Hello from uv on Heroku!\" if __name__ == '__main__': app.run(debug=True)\n\n5. Preparing for Heroku deployment:\n\nHeroku needs to know how to run your application. For a Flask application, we typically use Gunicorn as a production WSGI server. We\u2019ve already included it in our dependencies.\n\nWe\u2019ll need a Procfile in the root of our project to tell Heroku how to start our application:\n\nweb: gunicorn main:app\n\nHere, app refers to the name of our Flask application instance in main.py .\n\n6. Deploying to Heroku:\n\nNow, assuming you are in the project working directory, have the Heroku CLI installed, and have logged in, you can create a local git repository and Heroku application:\n\n~/user/my-app$ git init ~/user/my-app$ heroku create python-uv # Replace python-uv with your desired app name ~/user/my-app$ git add . ~/user/my-app$ git commit -m \"Initial commit with uv setup\"\n\nThe Heroku CLI will create a remote in your git repository, but you check to make sure it\u2019s there before you and push your code\n\n~/user/my-app$ git remote -v heroku https://git.heroku.com/python-uv.git (fetch) heroku https://git.heroku.com/python-uv.git (push) ~/user/my-app$ git push heroku main\n\nHeroku will detect your Python application, install the dependencies (based on .python-version , uv.lock and pyproject.toml ), and run your application using the command specified in the Procfile .\n\nThe future is bright (and fast!)\n\nWe\u2019re excited to announce that Heroku now natively supports uv for your Python development. By combining uv\u2019s performance with Heroku\u2019s fully managed runtime, teams can ship faster with greater confidence in their environment consistency. This reduces onboarding time, eliminates flaky builds, and improves pipeline performance.\n\nWhile uv is still relatively new, its potential to significantly improve the Python development workflow is undeniable. The focus on speed, efficiency, and modern packaging standards addresses some of the long-standing frustrations with existing tools.\n\nAs the project matures and gains wider adoption, we can expect even more features and tighter integration with other parts of the Python ecosystem. For now, even the significant speed improvements in local development are a compelling reason for Python developers to start exploring uv.\n\nThe journey of Python package management has been one of continuous improvement, and uv represents an exciting step forward. If you\u2019re a Python developer looking to boost your productivity and streamline your environment management, I highly recommend giving uv a try. You might just find your new favorite package manager!\n\nTry uv out on Heroku\n\nWhether you\u2019re modernizing legacy apps or spinning up new services, uv gives you the speed and flexibility you need\u2014now with first-class support on Heroku. Get started with uv on Heroku today.", "label": 0}
{"title": "Considerations for making a tree view component accessible", "url": "https://github.blog/engineering/user-experience/considerations-for-making-a-tree-view-component-accessible/", "content": "Tree views are a core part of the GitHub experience. You\u2019ve encountered one if you\u2019ve ever navigated through a repository\u2019s file structure or reviewed a pull request.\n\nOn GitHub, a tree view is the list of folders and the files they contain. It is analogous to the directory structure your operating system uses as a way of organizing things.\n\nTree views are notoriously difficult to implement in an accessible way. This post is a deep dive into some of the major considerations that went into how we made GitHub\u2019s tree view component accessible. We hope that it can be used as a reference and help others.\n\nStart with Windows\n\nIt\u2019s important to have components with complex interaction requirements map to something people are already familiar with using. This allows for responsiveness to the keypresses they will try to navigate and take action on our tree view instances.\n\nWe elected to adopt Windows File Explorer\u2019s tree view implementation, given the prominence of Windows\u2019 usage for desktop screen reader users.\n\nNavigating and taking actions on items in Windows\u2019 tree view using NVDA and JAWS helped us get a better understanding of how things worked, including factors such as focus management, keyboard shortcuts, and expected assistive technology announcements.\n\nThen maybe reference the APG\n\nThe ARIA Authoring Practices Guide (APG) is a bit of an odd artifact. It looks official but is no longer recognized by the W3C as a formal document.\n\nThis is to say that the APG can serve as a helpful high-level resource for things to consider for your overall approach, but its suggestions for code necessitate deeper scrutiny.\n\nBuild from a solid, semantic foundation\n\nAt its core, a tree view is a list of lists. Because of this, we used ul and li elements for parent and child nodes:\n\n<ul> <li> <ul> <li>.github/</li> <li>source/</li> <li>test/</li> </ul> </li> <li>.gitignore</li> <li>README.md</li> </ul>\n\nThere are a few reasons for doing this, but the main considerations are:\n\nBetter assurance that a meaningful accessibility tree is generated,\n\nLessening the work we need for future maintenance, and consequential re-verification that our updates continue to work properly, and\n\nBetter guaranteed interoperability between different browsers, apps, and other technologies.\n\nNOTE: GitHub currently does not virtualize its file trees. We would need to revisit this architectural decision if this ever changes.\n\nBetter broad assistive technology support\n\nThe more complicated an interactive pattern is, the greater the risk that there are bugs or gaps with assistive technology support.\n\nGiven the size of the audience GitHub serves, it\u2019s important that we consider more than just majority share assistive technology considerations.\n\nWe found that utilizing semantic HTML elements also performed better for some less-common assistive technologies. This was especially relevant with some lower-power devices, like an entry-level Android smartphone from 2021.\n\nBetter Forced Color Mode support\n\nSemantic HTML elements also map to native operating system UI patterns, meaning that Forced Color Mode\u2019s heuristics will recognize them without any additional effort. This is helpful for people who rely on the mode to see screen content.\n\nThe heuristic mapping behavior does not occur if we used semantically neutral div or span elements, and would have to be manually recreated and maintained.\n\nUse a composite widget\n\nA composite widget allows a component that contains multiple interactive elements to only require one tab stop unless someone chooses to interact with it further.\n\nConsider a file tree for a repository that contains 500+ files in 20+ directories. Without a composite widget treatment, someone may have to press Tab far too many times to bypass the file tree component and get what they need.\n\nThink about wrapping it in a landmark\n\nLike using a composite widget, landmark regions help some people quickly and efficiently navigate through larger overall sections of the page. Because of this, we wrapped the entire file tree in a nav landmark element.\n\nThis does not mean every tree view component should be a landmark, however! We made this decision for the file tree because it is frequently interacted with as a way to navigate through a repository\u2019s content.\n\nGo with a roving tabindex approach\n\nA roving tabindex is a technique that uses tabindex=\"-1\" applied to each element in a series, and then updates the tabindex value to use 0 instead in response to user keyboard input. This allows someone to traverse the series of elements, as focus \u201croves\u201d to follow their keypresses.\n\n<li tabindex=\"-1\">File 1</li> <li tabindex=\"-1\">File 2</li> <li tabindex=\"0\">File 3</li> <li tabindex=\"-1\">File 4</li>\n\nThe roving tabindex approach performed better than utilizing aria-activedescendant , which had issues with VoiceOver on macOS and iOS.\n\nEnhance with ARIA\n\nWe use a considered set of ARIA declarations to build off our semantic foundation.\n\nNote that while we intentionally started with semantic HTML, there are certain ARIA declarations that are needed. The use of ARIA here is necessary and intentional, as it expands the capabilities of HTML to describe something that HTML alone cannot describe\u2014a tree view construct.\n\nOur overall approach follows what the APG suggests, in that we use the following:\n\nrole=\"tree\" is placed on the parent ul element, to communicate that it is a tree view construct.\n\nis placed on the parent element, to communicate that it is a tree view construct. role=\"treeitem\" is placed on the child li elements, to communicate that they are tree view nodes.\n\nis placed on the child elements, to communicate that they are tree view nodes. role=\"group\" is declared on child ul elements, to communicate that they contain branch and leaf nodes.\n\nis declared on child elements, to communicate that they contain branch and leaf nodes. aria-expanded is declared on directories, with a value of true to communicate that the branch node is in an opened state and a value of false to communicate that it is in a collapsed state instead.\n\nis declared on directories, with a value of to communicate that the branch node is in an opened state and a value of to communicate that it is in a collapsed state instead. aria-selected is used to indicate if branch or leaf nodes have been chosen by user navigation, and can therefore have user actions applied to them.\n\nWe also made the following additions:\n\naria-hidden=\"true\" is applied to SVG icons (folders, files, etc.) to ensure its content is not announced.\n\nis applied to SVG icons (folders, files, etc.) to ensure its content is not announced. aria-current=\"true\" is placed on the selected node to better support when a node is deep linked to via URL.\n\nNOTE: We use \u201cbranch node\u201d and \u201cleaf node\u201d as broad terms that can apply to all tree view components we use on GitHub. For the file tree, branch nodes would correspond to directories and subdirectories, and leaf nodes would correspond to files.\n\nSupport expected navigation techniques\n\nThe following behaviors are what people will try when operating a tree view construct, so we support them:\n\nKeyboard keypresses\n\nTab : Places focus on the entire tree view component, then moves focus to the next focusable item on the view.\n\n: Places focus on the entire tree view component, then moves focus to the next focusable item on the view. Enter : If a branch node is selected: Displays the directory\u2019s contents. If a leaf node is selected: Displays the leaf node\u2019s contents.\n\n: Down : Moves selection to the next node that can be selected without opening or closing a node.\n\n: Moves selection to the next node that can be selected without opening or closing a node. Up : Moves selection to the previous node that can be selected without opening or closing a node.\n\n: Moves selection to the previous node that can be selected without opening or closing a node. Right : If a branch node is selected and in a collapsed state: Expands the selected collapsed branch node and does not move selection. If a branch node is selected and in an expanded state: Moves selection to the directory\u2019s first child node.\n\n: Left : If a branch node is selected and in an expanded state: Collapses the selected collapsed directory node and does not move selection. If a branch node is selected and in a collapsed state: Moves selection to the node\u2019s parent directory. If a leaf node is selected: Moves selection to the leaf node\u2019s parent directory.\n\n: End : Moves selection to the last node that can be selected.\n\n: Moves selection to the last node that can be selected. Home : Moves selection to the first node that can be selected.\n\nWe also support typeahead selection, as we are modeling Windows File Explorer\u2019s tree view behaviors. Here, we move selection to the node closest to the currently selected node whose name matches what the user types.\n\nMiddle clicking\n\nNodes on tree view constructs are tree items, not links. Because of this, tree view nodes do not support the behaviors you get with using an anchor element, such as opening its URL in a new tab or window.\n\nWe use JavaScript to listen for middle clicks and Control + Enter keypresses to replicate this behavior.\n\nConsider states\n\nLoading\n\nTree views on GitHub can take time to retrieve their content, and we may not always know how much content a branch node contains.\n\nLive region announcements are tricky to get right, but integral to creating an equivalent experience. We use the following announcements:\n\nIf there is a known amount of nodes that load, we enumerate the incoming content with an announcement that reads, \u201cLoading {x} items.\u201d\n\nIf there is an unknown number of nodes that load, we instead use a more generic announcement of, \u201cLoading\u2026\u201d\n\nIf there are no nodes that load we use an announcement message that reads, \u201c{branch node name} is empty.\u201d\n\nAdditionally, we manage focus for loading content:\n\nIf focus is placed on a placeholder loading node when the content loads in: Move focus from the placeholder node to the first child node in the branch node.\n\nIf focus is on a placeholder loading node but the branch node does not contain content: Move focus back to the branch node. Additionally, we remove the branch node\u2019s aria-expanded declaration.\n\nErrors\n\nCircumstances can conspire to interfere with a tree view component\u2019s intended behavior. Examples of this could be a branch node failing to retrieve content or a partial system outage.\n\nIn these scenarios, the tree view component will use a straightforward dialog component to communicate the error.\n\nFix interoperability issues\n\nAs previously touched on, complicated interaction patterns run the risk of compatibility issues. Because of this, it\u2019s essential to test your efforts with actual assistive technology to ensure it actually works.\n\nWe made the following adjustments to provide better assistive technology support:\n\nUse aria-level\n\nScreen readers can report on the depth of a nested list item. For example, a li element placed inside of a ul element nested three levels deep can announce itself as such.\n\nWe found that we needed to explicitly declare the level on each li element to recreate this behavior for a tree view. For our example, we\u2019d also need to set aria-level=\"3\" on the li element.\n\nThis fix addressed multiple forms of assistive technology we tested with.\n\nExplicitly set the node\u2019s accessible name on the li element\n\nA node\u2019s accessible name is typically set by the text string placed inside the li element:\n\n<li>README.md</li>\n\nHowever, we found that VoiceOver on macOS and iOS did not support this. This may be because of the relative complexity of each node\u2019s inner DOM structure.\n\nWe used aria-labelledby to get around this problem, with a value that pointed to the id set on the text portion of each node:\n\n<li aria-labelledby=\"readme-md\"> <div> <!-- Icon --> </div> <div id=\"readme-md\"> README.md </div> </li>\n\nThis guarantees that:\n\nthe node\u2019s accessible name is announced when focus is placed on the li element,\n\nelement, and that the announcement matches what is shown visually.\n\nWhere we\u2019d like to go from here\n\nThere\u2019s a couple areas we\u2019re prototyping and iterating on to better serve our users:\n\nBrowsers apply a lot of behaviors to anchor elements, such as the ability to copy the URL.\n\nWe\u2019d like to replace the JavaScript that listens for middle clicks with a more robust native solution, only without sacrificing interoperability and assistive technology support.\n\nSupporting multiple actions per node\n\nTree views constructs were designed assuming a user will only ever navigate to a node and activate it.\n\nGitHub has use cases that require actions other than activating the node, and we\u2019re exploring how to accomplish that. This is exciting, as it represents an opportunity to evolve the tree view construct on the web.\n\nAlways learning\n\nAn accessible tree view is a complicated component to make, and it requires a lot of effort and testing to get right. However, this work helps to ensure that everyone can use a core part of GitHub, regardless of device, circumstance, or ability.\n\nWe hope that highlighting the considerations that went into our work can help you on your accessibility journey.\n\n\n\nShare your experience: We\u2019d love to hear from you if you\u2019ve run into issues using our tree view component with assistive technology. This feedback is invaluable to helping us continue to make GitHub more accessible.", "label": 0}
{"title": "How we brought multimedia search to Dropbox Dash", "url": "https://dropbox.tech/infrastructure/multimedia-search-dropbox-dash-evolution", "content": "Knowledge workers routinely lose valuable time trying to find that thing\u2014the right images, videos, documents, or audio files\u2014across their dozens of apps and essential work tools. When we started building Dropbox Dash, our universal search and knowledge management product, we knew it had to do more than just speed up search. It also needed to scale beyond text. Because often, the challenge isn\u2019t just finding a file\u2014it\u2019s finding what\u2019s inside that file. And that gets tricky when things aren\u2019t labeled clearly, your team\u2019s folder structure breaks down, or you just can\u2019t remember where you saved what you need. Searching for multimedia content poses unique challenges. Images, for example, often come with cryptic names like IMG_6798 by default, and teams can quickly accumulate thousands of these unlabeled assets. Unlike documents, which usually contain metadata or readable content to help with discovery, media files frequently lack that context and require manual review. On top of that, they demand heavier compute resources and smarter ranking systems to deliver relevant results at speed. Supporting fast, accurate media search in Dash wasn\u2019t a matter of layering features on top\u2014it required fundamental changes across our infrastructure. We had to rethink how we indexed and ranked non-text files, how we rendered visual previews, and how we hydrated and surfaced metadata. We also had to reevaluate traditional document-search assumptions about relevance, latency, and even UI presentation. Our multimedia retrieval features were built to solve these exact problems, allowing users to find images, video, and audio just as easily as they find documents. What follows is a behind-the-scenes look at the engineering that made this possible: what we built, what we learned, and how we delivered a system that makes media as searchable as text.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images\u2014plus generative AI capabilities across even more connected apps. See what's new \u2192\n\nChallenges in supporting multimedia search\n\nSupporting search for multimedia\u2014images, video, and audio\u2014introduces a distinct set of technical hurdles. These files require significantly more processing power, have fewer textual cues for ranking, and often lack meaningful metadata. Delivering fast, relevant results means handling large file sizes efficiently, identifying new relevance signals, and optimizing how results are rendered for user review. That\u2019s why our universal search solution had to support seamless browsing, filtering, and previewing of media\u2014right inside Dash. Scaling search for this content meant facing higher storage and compute costs, tighter latency requirements, and adapting systems originally built for text-based retrieval. To understand what makes media search tricky, let\u2019s break down some key considerations. Storage cost\n\nMedia files are significantly larger than typical documents. On average, image files are about 3X larger, and video files are roughly 13X larger than non-media files in our system. These size differences directly increase storage demands and costs. Compute cost\n\nMultimedia files, such as images and videos, require more intensive processing to extract features both due to their larger size and the complexity of the features. Unlike text documents, we also generate previews of different resolutions for the images and videos, thereby significantly increasing the compute demands in our system. Relevance\n\nDash operates a multi-phase retrieval and ranking scheme, which was previously trained and optimized for textual content. Retrieving and ranking multimedia content requires having indexed any new multimedia-specific signals, formulating a query plan that leverages these signals, and handling any corner cases to avoid poorly ranked results. Responsiveness\n\nServing multimedia content introduces new latency challenges that are not present with text-based documents. We need previews for the multimedia search results to be meaningful, and we need them in multiple resolutions, including high-res formats, for a rich product experience. The larger resolutions add to the storage and compute costs. Only a small fraction of the indexed files are actually viewed during search interactions. As a result, precomputing previews at multiple resolutions for all media files would be wasteful and unnecessary. To balance responsiveness with resource efficiency, we generate previews on demand during the read path rather than upfront. This minimizes upfront compute and storage costs but introduces new latency concerns during user interactions, since we want to generate previews quickly to have a snappy user experience. With these challenges in mind, we designed a solution that integrates scalable infrastructure, smarter content understanding, and a preview system optimized for speed and accuracy.\n\nBuilding a multimedia search solution\n\nTo deliver a responsive and scalable experience in Dash, we had to rebuild key parts of our infrastructure to support search that\u2019s as smart and seamless for photos and videos as it is for documents. This work spans multiple layers of the stack, and it wasn\u2019t pulled off successfully without trial and error. We began by indexing lightweight metadata\u2014pulled from media blobs (the raw files like images, videos, or audio)\u2014to keep compute costs low. We extended our relevance models to handle location-aware queries and fuzzy file naming, and we optimized our preview generation pipeline to balance latency with cost. Along the way, we made frontend and backend updates to ensure media renders quickly and consistently across devices. The result is a robust multimedia search experience powered by smart metadata, just-in-time previews, and a UI that helps users find the right visual asset fast. Let\u2019s get into how we tackled it. Indexing media files by metadata To keep the compute costs low, we begin by indexing media files using available metadata, which is significantly cheaper to process than analyzing the full contents of media like images or videos. For example, we extract features such as file path, title, and EXIF. These metadata provide a lightweight foundation that enables basic search functionality with minimal processing overhead. As our capabilities evolve, we plan to build on this metadata-first approach by selectively incorporating deeper content analysis\u2014such as semantic embedding and/or OCR\u2014striking a balance between accuracy and cost.\n\nTo generate metadata features at scale, we leveraged Riviera, our internal compute framework that already powers Dropbox Search. Riviera processes tens of petabytes of data daily and includes mature business logic for metadata extraction. By reusing it, we benefited from proven scalability and consistency with existing Dropbox search infrastructure. Backfilling the index\n\nPrior to this initiative, we avoided downloading or storing raw media blobs in order to reduce storage and compute costs. As a result, our existing search index lacked the necessary features to support rich, media-specific search experiences. To address this gap, we added support for ingesting multimedia blob content to compute the required features. We retain the raw content for preview generation and to compute future features. Where possible, we download previews provided by third-party applications. These externally sourced previews are especially useful for design files like Canva, where we\u2019re unable to generate our own. Using them also helps us reduce compute costs. Storage optimizations\n\nDash optimizes the file sizes and MIME types ingested to balance storage cost and file availability. We currently ingest about 97% of media files and are working to address the remaining gaps with smarter lifecycle management techniques. Retrieving media files by metadata When a user searches for media, we configure the query to match their input against the metadata features extracted during indexing. This includes fields like filenames, file paths, and location data. To enhance location-based search, we also apply custom query logic for interpreting geographic references. Internally, we index a GPS location as a chain of IDs corresponding to the geographical hierarchy. For instance, we can look up the GPS coordinates of a photo to be from San Francisco in a process known as \u201creverse geocoding.\u201d Then, we would build a chain of IDs corresponding to San Francisco, California, and the United States, respectively, and place these IDs in the index for the photo. This allows us to retrieve the photo when the user wants to search for a photo taken in San Francisco, California or the entire United States, respectively.\n\nAt query time, we identify substrings of the query that may potentially be geographical locations, and then we determine whether they map to a valid location ID. In practice, because the number of known geographical locations has a manageably small cardinality, we retrieve the entire mapping upon the service startup and cache it. Lastly, in the course of building multimedia search, we realized that many multimedia files are named in particular ways. Many of them are files in the filesystem, e.g. PhotoShoot-Revised1234.jpg. To support better matching, we added logic to tokenize camel case, hyphenated strings, and numeric suffixes during both indexing and retrieval. Preview and metadata hydration at retrieval time Our system ingests data at a rate that\u2019s approximately three orders of magnitude higher than the query rate. This disparity makes it prohibitively expensive to generate and store previews for all multimedia files during ingestion, both in terms of compute and storage. To address this, we adopted a just-in-time approach, where previews are generated at query time. This strategy significantly reduces upfront costs while still supporting a responsive user experience. As part of our storage optimization efforts, we considered precomputing previews during ingestion to enable deletion of the raw content afterward. However, we ultimately decided against this approach for two key reasons. First, managing the lifecycle of these additional preview artifacts would introduce significant code complexity. Second, retaining the raw content ensures future flexibility, allowing us to compute new features later without having to re-ingest the original files.\n\nTo power the just-in-time approach, we rely on an internal previews service built on top of Riviera, a framework originally developed for Dropbox Search. The previews service is designed to be fast, scalable, and efficient. It incorporates intelligent caching strategies, storing previews for up to 30 days. This allows us to serve previews quickly when needed without repeatedly generating them for every request. During a search, we generate preview URLs for the relevant results, which are then passed to the frontend. The frontend fetches these URLs and displays the corresponding previews directly to the user. By reusing both the Riviera framework and the previews service, we also create the opportunity to reuse frontend components across both Dropbox and Dash. This ensures a consistent product experience across both platforms. To improve latencies, we create the preview URLs in parallel with other search operations such as ranking the results, performing permission checks, and fetching additional metadata needed to render complete search results. By handling these tasks in parallel, we minimize the overall response time and ensure a responsive user experience.\n\nSometimes, a user may want to enlarge a preview and view additional metadata, such as camera information. However, this is a less common operation, and sending all the extra metadata with every search result would be inefficient. When users request more detail\u2014such as camera metadata or timestamp\u2014we fetch it on-demand via a separate endpoint. This keeps the initial search response lean while still supporting deeper inspection when needed. User experience Searching through images and videos is a different experience than searching documents, especially since media files often have names like \u201cIMG_1234\u201d that don\u2019t tell you much. That\u2019s why fast, visual previews are essential\u2014they help users quickly decide which file is relevant without needing to open each one. We\u2019ve designed our preview system to load quickly and adapt to different shapes and sizes of media, whether an image is tall, wide, or an unusual shape. The layout avoids awkward cropping and keeps things easy to browse. When a user wants a closer look, they can open a full-size preview that also shows helpful EXIF details like when the photo was taken, what kind of camera was used, and where it was captured. Everything is built to feel smooth and fast, whether you\u2019re using Dash on a phone or computer. The interface stays out of the way and puts the focus on the content, making it easy to browse quickly or dive into a specific file when needed.\n\nLessons learned and future direction", "label": 0}
{"title": "Introducing Azure AI Foundry Labs: A hub for the latest AI research and experiments at Microsoft", "url": "https://azure.microsoft.com/en-us/blog/introducing-azure-ai-foundry-labs-a-hub-for-the-latest-ai-research-and-experiments-at-microsoft/", "content": "We\u2019re thrilled to announce the launch of Azure AI Foundry Labs, a hub for developers, startups, and enterprises to explore groundbreaking innovations from research at Microsoft.\n\nToday we\u2019re launching Azure AI Foundry Labs, a hub for developers, startups, and enterprises to explore groundbreaking innovations from research at Microsoft. Foundry Labs unites cutting-edge research with real-world applications, to enable developers and creators across industries to discover new possibilities, solve complex problems, and share insights to shape the future of AI.\n\nMicrosoft\u2019s newest AI breakthrough\u2014Muse, a first-of-its-kind World and Human Action Model (WHAM), available today in Azure AI Foundry\u2014is the latest example of bringing cutting-edge research innovation to our AI platform for customers to use.\n\nWith Azure AI Foundry Labs, we\u2019re excited to unveil new assets for our latest research-driven projects that empower developers to explore, engage, and experiment. Projects across models and agentic frameworks include:\n\nAurora: A large-scale atmospheric model providing high-resolution weather forecasts and air pollution predictions, outperforming traditional tools.\n\nExACT: An open-source project enabling agents to learn from past interactions and improve search efficiency dynamically.\n\nMagentic-One: A multi-agent system solving complex problems by orchestrating multiple agents, built on the AutoGen framework.\n\nMatterSim: A deep learning model for atomistic simulations, predicting material properties with high precision.\n\nOmniParser v2: A vision-based module converting UI screenshots into structured elements, enhancing agents\u2019 action generation.\n\nTamGen: A generative AI model for drug design, using a GPT-like chemical language model for target-aware molecule generation and refinement.\n\nThen versus now\n\nIn the early days of global positioning systems (GPS) technology, it took roughly a decade for GPS to make its way from specialized, military-grade instruments into everyday consumer use. What started as a niche innovation in the 1970\u2019s didn\u2019t become truly mainstream until the late 1990\u2019s and early 2000\u2019s, when GPS receivers became standard features in cars, cell phones, and handheld devices. Ten years might sound like a reasonable adoption curve\u2014until you look at how quickly innovations are moving in AI today.\n\nIn recent years, the pace of AI advancement has accelerated dramatically. We\u2019ve witnessed a shift from unveiling a new model every 4\u20136 months to releasing breakthroughs every 4\u20136 days. The amount of compute used for training AI models has grown 10 times every 12 months, turbocharging both research and commercialization. And time-to-product from foundational research to full-scale product deployment has gone from years to months.\n\nAt this velocity, ideas and prototypes need to be iterated upon, validated, and deployed faster than ever before. This rapid evolution demands new thinking in how we bridge research and application.\n\nAccelerating research to impact\n\nAzure AI Foundry Labs highlights the long-term collaboration between research and engineering teams at Microsoft and provides a single access point for developers and the broader AI community to experiment with new models, explore the latest frameworks, and be at the forefront of innovation. Developers can create prototypes using experimental research in Azure AI Foundry Labs, collaborate with researchers and engineering teams by sharing feedback, and help speed up the time to market for some of the most promising technologies.\n\nThe next chapter\n\nThe gap between breakthrough and impact has never been smaller. What once took years now takes weeks, and what was once confined to research labs now runs on devices in our pockets. Azure AI Foundry Labs exists to collapse this gap even further\u2014to ensure that every breakthrough in AI research finds its way to the developers, creators, and innovators who can transform it into real-world impact.\n\nAzure AI Foundry Labs Bridging research and application. Discover more\n\nThis isn\u2019t just about sharing research\u2014it\u2019s about accelerating the cycle of innovation itself. Whether you\u2019re a developer, researcher, startup founder, or enterprise builder, Azure AI Foundry Labs gives you direct access to the bleeding edge of AI advancement. The tools and models available today are just the beginning.\n\nVisit Azure AI Foundry Labs to start building the future.", "label": 0}
{"title": "Weeknotes: May 31 \u2013 June 6, 2025 \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/06/weeknotes-may-31-june-6-2025/", "content": "Happy Pride Month! \ud83c\udf08\n\nWin of the week: finally! finished! this AI aesthetics blog post! it\u2019s occupied basically all my blogging time for the past month, at last I can think about something else \ud83d\ude02 (who am I kidding, now I can turn the sections that got cut into their own posts lol)\n\nLooking forward to: doing a garden tour this weekend\n\nStuff I did:\n\n6.75 hours consulting \u2014 and signed the contract extension / budget adjustment for the rest of the year!\n\n3.25 hours writing \u2014 goal was 4 but I squandered a writing session on reading a craft book \u2014 still happy with the changes I made this week\n\nbonus evening walk with my friend \u2014 we\u2019re going to try doing two walks a week while evening daylight hours permit because neither of us dig squeezing in a weekend walk when we wanna be chilling\n\nregular weekly walk \u2014 had to dig my shorts out from winter storage \ud83e\ude73\n\nbaked strawberry biscuits \ud83d\ude0d haven\u2019t made these in a while because strawberries were $$$ last year\n\nhusband put on the most painful YouTube show, Kill Tony \u2014 it was like a car wreck, I couldn\u2019t look away even though I wanted to read my book \ud83d\ude02 (this is part of why I hate TV, there\u2019s something about it that\u2019s hard to tear my attention away from, even when something else is happening I\u2019d rather pay attention to)\n\nupdated my 2025 listening page with my April and May listens\n\nI washed my favorite new sweater with a load of mixed darks and dye rubbed off on it even though I turned my pants inside out! \ud83d\ude2d (I forgot I haven\u2019t worn / washed one of the pants very many times.) Spent a couple hours trying different methods for washing it out, including handwashing, using different detergents, and trying different stain removers. An enzymatic stain remover + Tide Free and Clear was my last attempt that got most of it out. Fortunately nothing else was affected.\n\nDinners:\n\nburger takeout \u2014 impossible burger with sauteed onions + French fries\n\nDomino\u2019s pizza (after watching a video on how it\u2019s manufactured)\n\nbaked feta with tomatoes + ciabatta bread + lemonade\n\nred lentil curry + rice + naan + ginger beer\n\nsourdough pancakes with strawberries and syrup + potato wedges\n\nIndian takeout \u2014 dum aloo + rice + naan \u2014 took two enzyme pills rather than the usual one but still had trouble, I think spice and fat multiply each other \ud83d\ude14\n\nfake chicken burgers with avocado and pineapple + tater tots + lemonade\n\nReading:\n\nRead A Sea of Love by Wilfrid Lupino and Gregory Panaccione, The Last Guy on Earth by Sarina Bowen, Seven Drafts by Allison K Williams, and Wed to the Alien Warlord by January Bell (needed some braindead reading)\n\nStarted reading Empire of Shadows by Jacquelyne Benson and Life Debt by R.J. Blain \u2014 not sure if I\u2019ll keep reading either\n\nContinued reading Things Become Other Things by Craig Mod and Cooking As Though You Might Cook Again by Danny Licht\n\nWords I looked up:\n\nChoice phrases:\n\nOn non-tradwife women who like cooking: \u201cdomestic, but not domesticated\u201d \u2014 Emily Amick\n\nPretty stuff I saw:\n\nPretty colors in these cactus flowers Anthony posted\n\nbeautiful woodblock prints of plants by Ito Jakuchu in the book \u201cGenpo y\u014dka\u201d (1768)\n\nNew music I listened to:\n\nWebsite changes:\n\nTurned off self-webmentions but left on self-pingbacks \u2014 I find pingbacks render more usefully, showing an obvious excerpt of the linking text and listing the post title as the commenter, whereas self-webmentions look just like a comment and post either the entire referencing post or a non-obvious excerpt from the start of the post. Also, self-pingbacks don\u2019t send me an email while self-webmentions do \ud83e\uddbe\n\n(If it\u2019s possible to change how my self-webmentions display on my site retroactively I would love to hear it, because I\u2019m not going to republish all ~3000 posts on this site\u2026)\n\nNature notes:", "label": 1}
{"title": "The ultimate guide to developer happiness", "url": "https://github.blog/engineering/engineering-principles/the-ultimate-guide-to-developer-happiness/", "content": "In today\u2019s rapidly evolving landscape, where AI is reshaping industries and transforming workflows, the role of developers has never been more critical. As business leaders, fostering an environment where developers feel valued, motivated, and empowered is essential to harnessing their full potential and keeping your business profitable and innovative.\n\nIn this blog post, we\u2019ll explore actionable tips and strategies to supercharge developer happiness, ensuring your team remains productive, engaged, and ahead of the AI curve. We\u2019ll walk you through ways to secure your code with AI, how to increase productivity with a strong developer experience, and, of course, invite you to join us at GitHub Universe 2024 to see the very best of the latest AI tooling in action.\n\nBoost productivity with a great developer experience\n\nDeveloper experience is more than just a buzzword\u2014it\u2019s a critical factor in driving productivity and collaboration within software development teams. A seamless developer experience allows developers to get into the flow state more easily, where their productivity and creativity can peak. This flow state\u2014characterized by uninterrupted concentration and a deep sense of involvement in the task\u2014is crucial for tackling complex coding challenges.\n\nThis work environment needs to be built intentionally, and the research backs it up. Developers who carve out time for deep work enjoy 50% more productivity, while those that get work they find engaging are 30% more productive.\n\nHow does this impact businesses? Well, because a developer that can significantly reduce their context-switching and mental load can also produce code faster and at a higher quality.\n\nWhen developers understand their code, they\u2019re 42% more productive. When developers are able to get faster turnaround times, they are 20% more innovative. These are tangible, individual benefits that in turn directly impact the output of developer teams.\n\nNow is the time for leaders to invest in creating a great developer experience. By prioritizing the developer experience, you\u2019re setting your team up to harness the full potential of the latest AI and platform engineering advances, ensuring your business stays ahead of the curve. Curious to learn more? Then dive into how a great developer experience fuels productivity with our latest research.\n\nUse AI to secure your code\n\nHistorically, developers and security teams have found themselves at odds due to competing business goals. Shifting security left incorporates security earlier in the software development lifecycle, but in practice it has primarily shifted responsibility to developers without necessarily giving them the required expertise.\n\nThis, combined with the context switching inherent in development work, makes addressing security concerns particularly challenging. With AI, developers now have powerful tools at their disposal to enhance code security. AI can:\n\nImprove detection rates\n\nProvide near-instant fixes with context\n\nEnable application security (AppSec) at scale\n\nThese three improvements make it easier for developers to integrate robust security measures without sacrificing productivity, and transform the relationship between developers and security teams into a collaborative partnership.\n\nIntroducing a new security tool doesn\u2019t have to be a daunting task either. By following a few simple steps, organizations can ensure a smooth transition and broad adoption.\n\nDocument the tool\u2019s features and usage to set the foundation and set realistic expectations to help align goals across teams.\n\nthe tool\u2019s features and usage to set the foundation and set realistic expectations to help align goals across teams. Recognize and celebrate successes to showcase the value of the new tool.\n\nand celebrate successes to showcase the value of the new tool. Adopt a go-with-the-flow approach and organize hackathons to further drive engagement and interest.\n\na go-with-the-flow approach and organize hackathons to further drive engagement and interest. Listen to developer feedback continuously improve and refine security practices.\n\nAI-powered security tools not only enhance the efficiency and effectiveness of AppSec, but also empower developers to take a proactive role in securing their code. This shift not only improves overall security posture, but also fosters a culture of shared responsibility and continuous learning, ultimately leading to more secure and resilient applications.\n\nSee exactly why security should be built into the developer workflow. \ud83d\udc47\n\nCustomize your LLMs\n\nOrganizations that take AI a step further and customize their AI tools are poised to lead the pack.\n\nLarge language models (LLMs) are trained on vast amounts of text data and can perform a variety of natural language processing tasks like translation, summarization, question-answering, and text generation. Customizing a pre-trained LLM goes beyond mere training\u2014it involves adapting the model to perform specific tasks relevant to the organization\u2019s needs. This level of customization helps developers maintain their flow state and significantly boost productivity and efficiency.\n\nCustomization techniques like retrieval-augmented generation (RAG), in-context learning, and fine-tuning enable LLMs to deliver more accurate and contextually appropriate responses:\n\nRAG combines retrieval-based and generation-based approaches in natural language processing. It enhances LLMs by integrating information retrieval techniques, where relevant documents or snippets are retrieved from a vector database to assist in generating more accurate and contextually appropriate responses. This approach allows the model to access and utilize external knowledge, making the generated output more informed and relevant to the user\u2019s query.\n\ncombines retrieval-based and generation-based approaches in natural language processing. It enhances LLMs by integrating information retrieval techniques, where relevant documents or snippets are retrieved from a vector database to assist in generating more accurate and contextually appropriate responses. This approach allows the model to access and utilize external knowledge, making the generated output more informed and relevant to the user\u2019s query. In-context learning refers to a model\u2019s ability to adapt and respond to new tasks or inputs based on the context provided in the input prompt without requiring additional training. The model leverages its pre-trained knowledge and the context given in the input to perform tasks effectively.\n\nrefers to a model\u2019s ability to adapt and respond to new tasks or inputs based on the context provided in the input prompt without requiring additional training. The model leverages its pre-trained knowledge and the context given in the input to perform tasks effectively. Fine-tuning, on the other hand, is a process in which an LLM is further trained on a specific dataset to adapt it to a particular task or domain. During fine-tuning, the model\u2019s parameters are adjusted based on the new dataset, which typically involves supervised learning with labeled data. This process allows the model to specialize and improve its performance on specific tasks, (such as text classification, question answering, or machine translation), by leveraging the general knowledge acquired during its initial pre-training phase.\n\nBy implementing these customization strategies, businesses can unlock the full potential of their AI tools. Customized LLMs not only improve developer productivity\u2014they also enhance the quality and relevance of AI-generated content.\n\nPrepare your repository for teamwork\n\nFostering collaboration doesn\u2019t just make software development faster, it also helps teams build better products and boost job satisfaction. By making your repository as collaborative as possible, you\u2019ll optimize success. This includes focusing on:\n\nRepository settings : properly configuring repository settings to control visibility, access, and contribution workflows lays the foundation for collaboration.\n\n: properly configuring repository settings to control visibility, access, and contribution workflows lays the foundation for collaboration. Repository contents : including essential files like README.md, LICENSE.md, CONTRIBUTING.md, CODEOWNERS, and CODE_OF_CONDUCT.md helps collaborators understand the project, its purpose, and how to contribute.\n\n: including essential files like README.md, LICENSE.md, CONTRIBUTING.md, CODEOWNERS, and CODE_OF_CONDUCT.md helps collaborators understand the project, its purpose, and how to contribute. Automation and checks : implementing automation tools such as linters, continuous integration (CI), and continuous deployment (CD) pipelines streamlines the development process, ensures code quality, and enables immediate feedback.\n\n: implementing automation tools such as linters, continuous integration (CI), and continuous deployment (CD) pipelines streamlines the development process, ensures code quality, and enables immediate feedback. Security practices : enforcing role-based access control, managing secrets securely, and scanning code for vulnerabilities can foster trust and protect the project from vulnerabilities.\n\n: enforcing role-based access control, managing secrets securely, and scanning code for vulnerabilities can foster trust and protect the project from vulnerabilities. Issue templates : providing structured issue templates guides contributors in providing necessary information and context when reporting bugs.\n\n: providing structured issue templates guides contributors in providing necessary information and context when reporting bugs. Community engagement: engaging with the project\u2019s community through meetups, project blogs, discussions, and other channels fosters belonging and builds relationships.\n\nInvest in your team\u2019s learning opportunities\n\nWhen you signal to your team that you value their career growth and exposure to learning opportunities, it can boost happiness and job satisfaction, leading to increased productivity, collaboration, and better problem solving.\n\nEncouraging your developer teams to attend conferences like GitHub Universe 2024 is a strategic investment in their professional growth and your business\u2019 success. Our global developer event provides an unparalleled platform for the best in software development to gather and expand their knowledge, stay updated on the latest AI-powered tools, and bring fresh ideas back to their teams.\n\nHere are a few highlights of what you and your team can expect:\n\nHelp your developers get in the flow and stay there with sessions, demos, panels, and more on the powerful tools and techniques that enhance productivity and satisfaction.\n\nand stay there with sessions, demos, panels, and more on the powerful tools and techniques that enhance productivity and satisfaction. Connect with other technical leaders to share experiences, challenges, and best practices. Expand your network with valuable industry contacts.\n\nto share experiences, challenges, and best practices. Expand your network with valuable industry contacts. Get a first look at GitHub\u2019s product roadmap and see how upcoming features and enhancements can help you stay ahead in a competitive landscape.\n\nand see how upcoming features and enhancements can help you stay ahead in a competitive landscape. Gain technical skills with GitHub certifications and workshops designed to enhance your expertise in a rapidly evolving industry.\n\nwith GitHub certifications and workshops designed to enhance your expertise in a rapidly evolving industry. Learn the latest on GitHub Copilot and stay ahead with the latest coding practices and techniques.\n\nGet your tickets today. You can take advantage of our group discount and get four tickets for the price of three. (That\u2019s a 25% savings!)\n\nIf you\u2019re flying solo, you can also use our Early Bird discount and save 20% off one in-person ticket, only until September 3.\n\nReach new levels of creativity and efficiency\n\nIncorporating these five business strategies can transform your development process and increase developer happiness. By investing in these areas, you empower your team, foster a culture of continuous learning, and position your organization for success in the rapidly evolving tech landscape.", "label": 0}
{"title": "Dropbox API server root certificate changes coming in 2026", "url": "https://dropbox.tech/developers/api-server-certificate-changes", "content": "The Dropbox API servers will be updated with certificates issued from a new root starting on or after January 1, 2026.\n\nSome of the official Dropbox SDKs implemented certificate pinning based on a list of root certificates included in the SDKs. These certificates will stop working in many web browsers and devices in 2026, so Dropbox needs to change which root is used to issue its server certificates. Apps using these SDKs need to be updated to the latest versions of the Dropbox SDKs to ensure uninterrupted access to the API.\n\nThe versions listed below have been updated to maintain compatibility with the Dropbox API servers going forward:\n\nJava SDK v7.0.0 or greater\n\n.NET SDK v7.0.0 or greater Note that apps using older versions of the .NET SDK are only affected if they call DropboxCertHelper.InitializeCertPinning() .\n\nPython SDK v12.0.2 or greater\n\nIf you are using any of the above SDKs, please update your app(s) to use the latest versions of these SDKs as soon as possible to ensure continued access to the Dropbox API.\n\nThe following official Dropbox SDKs are not affected:\n\nThird party libraries may or may not be affected by these changes. Please consult their documentation for further information.\n\nIf you have any questions, you can always reach us on our forum or via our contact form.", "label": 0}
{"title": "Stripe\u2019s remote engineering hub, one year in", "url": "https://stripe.com/blog/remote-hub-one-year", "content": "Last May, Stripe launched our remote engineering hub, a virtual office coequal with our physical engineering offices in San Francisco, Seattle, Dublin, and Singapore. We set out to hire 100 new remote engineers over the year\u2014and did. They now work across every engineering group at Stripe. Over the last year, we\u2019ve tripled the number of permanently remote engineers, up to 22% of our engineering population. We also hired more remote employees across all other teams, and tripled the number of remote Stripes across the company.\n\nLike many organizations, Stripe has temporarily become fully remote to support our employees and customers during the COVID-19 pandemic. Distributed work isn\u2019t new to Stripe. We\u2019ve had remote employees since inception\u2014and formally began hiring remote engineers in 2013. But as we grew, we developed a heavily office-centric organizational structure. Last year, we set out to rebalance our mix of remote and centralized working by establishing our virtual hub. It\u2019s now the backbone of a new working model for the whole company.\n\nWe think our experience might be interesting, particularly for businesses that haven\u2019t been fully distributed from the start or are considering flipping the switch to being fully remote, even after the pandemic. We\u2019ve seen promising gains in how we communicate, build more resilient and relevant products, and reach and retain talented engineers. Here is what we learned.\n\nDeploying engineers closer to customers has been a boon\n\nOne of our goals in establishing the remote hub was to connect more closely with our customers. A year ago, when an overwhelming majority of our engineering staff was located in two cities, it was more difficult to support a global user population with products that felt locally native in supporting commerce. Now, for example, our Developer Support Engineering team is distributed across three continents\u2014and two thirds of the team operate outside of a Stripe office. Units within the team own one of four coverage areas (each spans approximately six time zones), enabling them to provide more timely, local support to users, as well as run uniform, simultaneous, and regionally resonant customer events around the world. We feel closer to customers because we literally are.\n\nThis shift has markedly influenced our product development roadmaps. We\u2019ve made substantial improvements to support non-card payment methods throughout our product suite. Back in the day, we referred to these internally as \u201calternative payment methods\u201d because from the perspective of someone living in San Francisco, that is accurate. However, for many of our global users, credit cards are the alternative still crossing the chasm to mass acceptance. Commerce is global, but the payment experience is local.\n\nWe built Checkout to maximize conversion by intelligently prioritizing which payment methods are presented to the customer depending on where they\u2019re located. Whether in Mexico or Malaysia, a company\u2019s checkout experience should feel local and include the payment methods customers are most likely to prefer. One way to more naturally build in local considerations into global products is if they\u2019re built by remote, distributed teams. One member of the Checkout team launched a local push payment method from Singapore and another shipped address collection from Maryland. Given the nature of Checkout, we hypothesize that the more distributed the team becomes, the closer we get to the needs and mindsets of more users.\n\nCultural improvements to support remote work have spilled over\n\nWe approached the building of our remote hub, in many ways, like any one of our other engineering hubs around the world. The standards and investments we\u2019ve established for other Stripe offices have ported over well to our remote hub. We nominated a site lead (Aditya Mukerjee for the first year) to be responsible for the overall happiness and productivity of the hub. We ensure that our leadership regularly visits the hub via Zoom meetings to lead discussions, answer questions, and provide a sense of connection. We encourage virtual coffee chats to promote a sense of belonging. We survey the team regularly and review feedback and people data, so that we can understand both the shared needs of our employees and the particular needs of a hub.\n\nOur remote hub\u2019s organizational infrastructure enabled us to run experiments on individual engineering teams to determine how to best integrate remote engineers\u2014and then horizontally scale the ones which generated notable successes. We still have more to discover, but have been seeing some themes emerge.\n\nOne is the loneliest number. The typical engineering team at Stripe has five to eight engineers. When only one member of that team is remote, they often suffer a combination of isolation (both socially and with respect to work-related decision making) and organizational burden (because they are effectively responsible for rearchitecting the team\u2019s processes to be remote-friendly in addition to doing their actual job). Instead, moving multiple remote engineers simultaneously onto a team has yielded much better results for their productivity and happiness. This shift acts as a forcing function to support asynchronous communication, better distribute the workload, and accelerate the adoption of team norms to socially include all members.\n\nOne of our remote engineering teams works on invoices for our billing product. Previously, the invoices team was based out of our San Francisco headquarters. Over the last year, it has expanded to nine members distributed across nine cities and two countries, with an engineering manager based out of North Carolina, three time zones away from headquarters. The team has delivered significant improvements to Stripe\u2019s invoicing products and quintupled their growth.\n\nThe invoices team, including one headquarters-based engineer, operates remote-first. Product and architectural decisions are documented in writing, code review SLAs account for time zone differences, and they set aside intentional time each week to connect with one another. Recurring meetings are prized time, as one of the only synchronous touchpoints the team members have together, but if there\u2019s no agenda, the team is ruthless about canceling it. It\u2019s the time slot that\u2019s sacred; the discussions must be intentional. The team embraces ad-hoc meetings and pairing time to unblock decisions or work through complex technical problems.\n\nWe have also seen a flowering of cultural rituals. Teams find new ways to take a break together. Everyone might do individual tea orders and then post photos in their Slack channel or snack together on Zoom. Scheduled unstructured hangout sessions have also multiplied. We\u2019ve seen parallel #TEAM-social channels adjacent to most #TEAM-work channels on Slack so that team members have an outlet to post about kids, hobbies, and other watercooler banter subjects without feeling like they\u2019re intruding on a workspace. We\u2019ve also experimented with entirely virtual offsites, particularly the bonding component. One team ran a simulated lunar crash exercise that required participants to collectively identify and rank the most critical tools needed to survive. We do not yet have any extraplanetary remotes (but watch this space).\n\nBetter tooling and communication norms have paid dividends. We rely on onboarding to set the tone for an employee\u2019s relationship with the company, their team, and their work. An unexpected benefit to onboarding cohorts of remote engineers was that we got better at supporting engineers everywhere. Our onboarding curriculum became more polished and efficient, given the limited window of the work day shared by regionally scattered remote employees. We moved as many tasks as possible from synchronous batches to asynchronous processing via written documentation, videos, or small groups sharing time zones. These changes reduce the ramp-up time and friction for remote employees, as well as streamline traditional in-person onboarding.\n\nWhen onboarding gets more efficient and asynchronous, teams can scale independently and at a faster rate. Over the past year, we\u2019ve grown small engineering teams in offices like Tokyo and much larger engineering teams in all our engineering hubs. Those teams have quickly advocated for and brought to market locally-relevant products. Recently, engineers based in Japan and Latin America worked together to enhance local payment rails so that users could receive funds in a few working days rather than a month.\n\nWe have invested substantially in internal tooling and explicit communication norms to support the productivity of remote engineers. This includes tooling to coordinate deploys for our most sensitive systems (so that engineers don\u2019t have to synchronously coordinate in-person), incident management tooling which rapidly spins up a virtual response room on demand, and socializing shared scheduled memos rather than daily in-person standups to help engineers stay informed about teammates\u2019 work.\n\nEfforts like these turned out to be critical during our COVID-19 response efforts. We closed all of our offices worldwide and worked to support users going through extremely heterogeneous shocks to their business models, ranging from surges in demand and transactions for delivery startups to cash-flow challenges as demand dried up and refunds spiked for travel companies.\n\nThis would have, candidly, been extremely difficult for Stripe a few years ago, and is still not easy by any stretch of the imagination. But we continue to develop and ship new products, thanks in part to an increased focus on supporting remote engineers last May. Earlier this year, we wouldn\u2019t have guessed that early preparations undertaken to support a fraction of our engineers would benefit all our engineers\u2014and the rest of Stripe.\n\nWe are tapping pools of candidates we could not reach before. Stripe, like all software companies, depends on having talented engineers from a wide range of life experiences, professional backgrounds, and seniority levels. We have found that increasing our population of remote engineers has helped to improve our company along several of these dimensions.\n\nOne demographic we have noticed interact interestingly with remote-friendly policies is engineers with family responsibilities. They often need to make hard choices about which housing markets to live in to support their family and their career. When we had fewer remote roles, we unwittingly narrowed our applicant pool of engineers with families, and missed hiring talented engineers. We also occasionally had engineers leave to favor their partners\u2019 career aspirations, which sometimes required moving away from cities we had offices in. Our expanded ability to hire and retain engineers from many more places enables us to attract many talented engineers who don\u2019t feel that working for us would require unpalatable tradeoffs.\n\nWe built our remote hub by initially seeking out engineers with significant experience working remotely; this naturally led us to hire more seasoned engineers. Similarly, we\u2019ve seen anecdotal shifts in the composition of professional experience across engineers. Stripes in San Francisco have often had most of their professional experience at startups or in large consumer Internet companies, because those sorts of companies employ most engineers in San Francisco. We now hire engineers in cities where financial companies, larger enterprises, and agencies hire larger swaths of the engineering population. This helps us incorporate more DNA from their practices and informs how we build products for adoption for those sorts of companies.\n\nLast May, we focused on hiring remote engineers, product managers, engineering managers, and technical program managers. But once we were able to support those roles remotely, Stripes across the company started to transition from working in our offices to doing their jobs remotely. They span teams and seniority, and among their ranks are tenured leaders and engineering interns. (This summer, we\u2019re running an entirely remote intern program for 76 engineering interns.)\n\nThere are now hundreds of remote Stripes across our engineering and business teams. Three out of four teams at Stripe have at least one remote worker.\n\nWe are increasing our investment\n\nOur experience developing our remote hub this past year has exceeded our expectations. We continue to test and trial remote work across Stripe, but it\u2019s no longer an experiment. We will accelerate our investment in remote Stripes and the growth of the remote hub. Here\u2019s how:\n\nWe are broadening the positions for which we will hire remotely. Last year, we intended to hire a mix of engineering individual contributors, engineering managers, and non-engineering technical roles such as product managers and designers into our remote hub. As of the publish date of this post, we now have open, remote roles on our design, legal and marketing teams.\n\nWe support this bottom-up transformation with more explicit company-wide direction so that we can meet our goals in remote hiring across technical and non-technical functions alike. We expect the number of remote-eligible roles will continue to grow; changes caused by COVID-19 are accelerating this transition.\n\nWe have increased the places we can hire remotely from. Today, Stripes are based in over 50 cities worldwide. We now have the capability to hire remote employees in Australia, Brazil, Canada, France, Germany, India, Indonesia, Ireland, Japan, Mexico, Netherlands, Singapore, Sweden, UK, and the U.S. We\u2019re setting the foundation for remote hiring in many more countries, but are rate limited on hiring remotes in countries where we don\u2019t already have an office presence. In those cases, we need to tackle substantial legal and logistical challenges on a country-by-country, city-by-city, and sometimes neighborhood-by-neighborhood basis. Other organizations seeking to increase remote hires will have to grapple with people systems and disparate tax regimes.\n\nThis is a fractally complex project and has variable lead times depending on the jurisdictions involved, where you have preexisting legal or business infrastructure to leverage, and what compromises you are willing to make prior to onboarding employees in a jurisdiction. To satisfy these requirements, companies inevitably need to manage parallel hiring systems to support on-site and remote employees; common standards would accelerate the world\u2019s migration to remote work. As an example, Stripe must be able to issue equity before we make offers in a jurisdiction, whereas many other employee benefits can roll out more gradually.\n\nWe are improving the quality of life for remote engineers. We care that all of our employees have a successful experience at Stripe. To gauge this, we run bi-annual surveys. In our latest one, we found that, compared to non-remote employees, remote Stripes are more likely to have a sense of connection to their team, balance their workload and life demands, and stay longer at Stripe. Remote Stripes score lower when asked whether Stripe fosters a sense of connection for employees in their location. However, 77% said that their location is not a barrier to having an impact at Stripe, a 10% increase from a year ago.\n\nOur goal in the coming year is to ensure remote engineers feel at a minimum the same level of impact, team connectedness, and velocity in career progression as any other engineer.\n\nWe want to talk to you\n\nWe\u2019re looking for new colleagues to join us. Here are the remote positions; more will be added over the coming year.\n\nDo you have questions about our remote engineering hub or its open roles? Our CTO, David Singleton, will lead a remote coffee chat on September 9 at 8:30am PT with remote Stripes Patrick McKenzie, Jamie Kite, and Aditya Mukerjee to talk about our remote hub. Sign up for the event to attend.", "label": 0}
{"title": "AI transformations for sustainability", "url": "https://blogs.microsoft.com/on-the-issues/2025/01/16/ai-transformations-for-sustainability/", "content": "Today, Microsoft published a new paper, Accelerating Sustainability with AI: Innovations for a Better Future. You can read the foreword below and explore the paper in its entirety.\n\nThroughout history, societal transformations have been driven by the emergence of general-purpose technologies that reshaped entire economies, industries, and ways of life.\n\nThe steam engine, the printing press, electricity, and the internet have each marked pivotal social and economic shifts, leading to lasting changes in how we live and work. Today, AI stands as the latest\u2014and potentially most powerful\u2014general-purpose technology, offering an unprecedented opportunity to drive the societal transformations we urgently need to achieve the world\u2019s sustainability goals.\n\nIn 2023, we published Accelerating Sustainability with AI: A Playbook, in which we highlighted that AI has three game-changing capabilities that make it an essential tool for accelerating sustainability. AI can enhance our ability to predict and optimize complex systems, accelerate the development and deployment of sustainable solutions, and empower the workforce to learn and achieve more\u2014equipping society with the means to drive sustainability progress at a speed and scale previously beyond reach.\n\nOver the last year, we have seen the potential of AI for sustainability in action, empowering the world with new tools for tackling the climate crisis and sustainability challenges more broadly. For example, earlier this year, Microsoft collaborated with Pacific Northwest National Laboratory to use AI in discovering a new battery material requiring less lithium\u2014a breakthrough achieved in weeks rather than the years that traditional research and development would have required. Reducing lithium dependence is crucial to decarbonization, as global demand for lithium is projected to outpace supply, potentially limiting the growth of the energy storage systems needed for the shift to electrification and renewable energy.\n\nAI\u2019s transformative capabilities extend far beyond sustainability, the world has an opportunity to harness AI to enhance both productivity and prosperity. By enabling smarter resource use, optimizing systems for efficiency, and fostering innovations in carbon-free energy and conservation, the AI economy also has the potential to advance both economic growth and environmental stewardship.\n\n\n\nAt Microsoft, we believe the world needs AI that is broadly accessible and trustworthy. This includes addressing the sustainability challenges associated with this technology. The five plays outlined in our AI and sustainability playbook reflect the targeted actions needed to unlock the full potential of AI for accelerating sustainability progress globally.\n\nAcross our sustainability work, we regularly assess our progress and adjust our strategies for greater impact. One lesson from this last year is that minimizing the sustainability impact of AI operations requires more than minimizing resource use in datacenter operations; it also requires supporting the communities where datacenters are located and expanding access to zero-carbon electricity. Global electricity demand is growing rapidly, at an estimated average annual rate of 3\u20134%. While AI currently consumes less than 0.3% of global electricity demand\u2014and, according to the International Energy Agency (IEA), is expected to remain a small portion in the decade ahead\u2014rapid growth in certain regions can strain local grids.\n\nIn light of these realities, we have updated the third play of our playbook to include enhancing access to carbon-free energy on electricity grids and supporting local communities where we operate datacenters. In support of these expanded goals, we are expanding our effort to build and operate digital infrastructure that addresses societal challenges and creates benefits for communities.\n\nThis report highlights Microsoft\u2019s innovations and actions to advance each of the five plays. Examples of our efforts across the five plays include:\n\nPlay 1: Invest in AI for sustainability\n\nMicrosoft is investing in building AI tools, such as MatterGen and MatterSim, which enable researchers to design and test materials with tenfold greater accuracy and significantly faster performance, while also predicting global weather and atmospheric processes with increased accuracy and at speeds up to 5,000 times greater than current forecasting systems. We are also building AI-enabled tools to empower stakeholders to more effectively and efficiently manage agriculture and water resources and to expedite the licensing process for carbon-free electricity.\n\nPlay 2: Develop digital and data infrastructure for the inclusive use of AI for sustainability\n\nWe are creating tools to fill critical data gaps, which can enhance AI models for better measuring and predicting complex systems such as biodiversity and climate. For instance, SPARROW captures images and acoustic recordings to gather data on biodiversity and ecosystem health in remote areas. Additionally, we are partnering with G42 on a $1 billion digital ecosystem initiative in Kenya.\n\nPlay 3: Minimize resource use, expand access to carbon-free electricity, and support local communities\n\nMicrosoft is innovating datacenter development with low-carbon materials like cross-laminated timber. Through an agreement with Brookfield, we aim to add 10.5 gigawatts (GW) of renewable energy to the grid.\n\nPlay 4: Advance AI policy principles and governance for sustainability\n\nWe advocated for policies that accelerate grid decarbonization, including Federal Energy Regulatory Commission (FERC) transmission rules and provisions in the Inflation Reduction Act in the United States. In addition, we continue to advance AI governance within Microsoft and globally.\n\nPlay 5: Build workforce capacity to use AI for sustainability\n\nMicrosoft Philanthropies\u2019 Skills for Social Impact program trained over 14 million people in digital and AI skills to support a workforce ready to deploy AI for sustainability. As the window for achieving global sustainability goals narrows, the urgency for action intensifies. The world needs every tool at its disposal, and the potential of AI to accelerate sustainability is already being realized. Sustainability is not a journey that can be taken alone, and unlocking the full potential of AI for climate progress requires continued partnerships to combine expertise, technology, and innovation. As we continue to explore the ways AI can advance sustainability, we invite others to join us in this journey.\n\nRead the full report at https://aka.ms/AcceleratingSustainabilitywithAI2025\n\nTags: AI, Environmental Sustainability, Innovation, Responsible AI, sustainability, Workforce", "label": 0}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2024-03", "content": "en\n\nYesterday was a bitter sweet day for me. This has been a day that has been coming. It doesn't really hit you until all their things are packed up ready to be loaded in the moving truck. I look around at my friends almost empty apartment with nothing on empty walls. The sad feeling the emotions take place. Maybe, it's the allergies. These friends are very special to me as they are one of the first friends I made in California. These friends made me feel right at home as I moved a place unfamiliar to me. It all started just by saying \"hello!\" as I was moving into the unit I was moving into. We hit it off right off the bat. We would go cruising on our boards. We did disc golfing (which I'm not good at). Go on adventures throughout California. We would go over to each other's place. They threw a welcome back party after returning from South Korea after being gone for a month. They help me experience new things that definitely got me out of my comfort zone. My friends helped me grow as a person. Most importantly, they taught me how to be more fun.\n\nI'm happy they are off to a new adventure. I want them to be happy and do what is best for them. I am grateful to have made friends with them. I know that this is goodbye for now. We'll be in touch. I'll send post cards. We'll see each other along the way. I wish you all the best. See you all soon.\n\nThank you for all the memories,\n\nPablo", "label": 1}
{"title": "Generative AI as a magic system \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/24/generative-ai-as-a-magic-system/", "content": "We treat generative AI like magic\u2026 and magic systems have rules. When creating fantasy worlds, writers think about who can use magic, how magic is performed, what it\u2019s able to do, what its constraints are, what the source of magic is, and what it costs. I\u2019m applying a bit of reverse worldbuilding to the real world to extrapolate the rules of the AI magic system.\n\nIslands in the Sky by Death Valley Girls\n\nWho can use AI magic: magic users pay to use corporate AI magic systems. Those who are wealthy and tech savvy enough can host their own local model. Free magic use is mostly limited to corporate largesse ultimately intended to build magic dependency.\n\nHow AI magic is cast: AI spells are cast with written text input through a digital interface. Spells are refined and recast until the outcome satisfies (spells produce different results every time they are cast).\n\nWhat AI magic can do: AI spells can produce combinations of words that are interpreted as writing, code-like material that sometimes runs as code, images that resemble art, and video that resembles reality. It can create imitations of specific human creators\u2019 work, as well as individual\u2019s speech and appearance. It can also mimic human conversation for a span of time before the spell dissipates. AI magic is near instantaneous, allowing people without technical skills to produce text and graphics faster than writers and artisans.\n\nWhat AI magic cannot do: AI magic cannot produce the same outcome twice, nor act upon existing conjurations, instead casting spells anew each time. AI magic itself cannot reference sources, though may be used in tandem with other tools that enable citation (though with questionable accuracy). AI magic cannot reason or write, but its conjurations may create the illusion of intelligence through their statistical consistency with written language use.\n\nThe source of AI magic: AI magic derives from statistical analysis of human-created art, writing, speech, music, and video, classified and sorted by human laborers in low-cost geos.\n\nThe cost of AI magic: Resource costs of AI magic include power, water, and high-end chips, which themselves require specialized manufacturing and rare earth minerals.\n\nSocial costs include the reinforcement of racism and sexism, as well as mental harm to AI trainers assessing inputs to the magic system.\n\nSocietal costs include job elimination and job intensification as positions able to be reproduced in part by magic are eliminated and that magic work is shifted to the remaining workers.\n\nInformation costs include the destruction of the online publishing incentive structure / information commons, leading to more paywalled content; an increase in low-quality material, which makes finding accurate information harder; as well as the danger of political propaganda by poisoned magic systems.\n\nIndividual user costs include critical thinking skills, writing abilities, and patience for conversing with humans.\n\nFurther reading:\n\nThe new magic of AI vs. the old magic of artists by Kening Zhu\n\nSee also:\n\nGenerative AI and the Business Borg aesthetic", "label": 1}
{"title": "Configuring Vouch Proxy with Indieauth", "url": "https://lifeofpablo.com/blog/configuring-vouch-proxy-with-indieauth", "content": "Location: 38.581573, -121.494400\n\nTable of Contents\n\nIntroduction\n\nA few months ago, I wrote a blog post on Vouch Proxy using Google as an idP, or Identity provider. This time I will be writing on how to configure vouch proxy to use indieAuth as the idP.\n\nThis blog post will be very similar to the blog post mentioned above but we will focus on indieAuth.\n\nToday, I'll demonstrate how to setup Vouch Proxy on an nginx web server. In this example I will be using IndieAuth as our provider using a very minimal configuration.\n\nThis tutorial assumes you have prior knowledge of using a linux server such as Debian. Message me at hello@lifeofpablo.com if you need some help. I'd be happy to do so!\n\nUse Cases\n\nI currently use it for: * Signing into my website * Using it for my guestbook * Sign into my RSS feeder. * Using same cookie on my website. Similar to single sign on.\n\nWhat Vouch Proxy Does?\n\nAccording to the Repository README.md, it states the following:\n\nVouch Proxy (VP) forces visitors to login and authenticate with an IdP (such as one of the services listed above) before allowing them access to a website.\n\nVP can also be used as a Single Sign On (SSO) solution to protect all web applications in the same domain.\n\nAfter a visitor logs in Vouch Proxy allows access to the protected websites for several hours. Every request is checked by VP to ensure that it is valid.\n\nVP can send the visitor's email, name and other information which the IdP provides (including access tokens) to the web application as HTTP headers. VP can be used to replace application user management entirely. In our case, we are passing an HTTP header with your domain to sign into sites that support indieauth protocol.\n\nAn example of an HTTP header being passed is my domain, https://lifeofpablo.com.\n\nThings you'll need/prepare:\n\nA linux server with a public IP address with hosting and SSL Debian will be used here but any of the common distros will work Certbot is an easy solution to get SSL certifcate for https://\n\nGo Language (to compile vouch-proxy)\n\nVouch Proxy\n\nMake sure your website is setup for use with the Indieauth protocol.\n\nNginx Web Server\n\nDigital Ocean has a good guide if you need to learn how to setup virtual blocks in nginx.\n\nDownload/Install Vouch Proxy from Github\n\nMake sure to have Go Lang installed. Follow the instructions for your operating system. In my case I am using debian.\n\nDownload Vouch Proxy from it's Github repository.\n\n$ git clone https://github.com/vouch/vouch-proxy.git $ cd vouch-proxy $ ./do.sh goget $ ./do.sh build\n\nVouch Proxy Nginx Virtual Block\n\nLet's go ahead and create a virtual block to proxy Vouch Proxy.\n\nserver { server_name vouch.domain.com; location / { proxy_set_header Host $http_host ; proxy_set_header X-Forwarded-Proto https; proxy_pass http://127.0.0.1:9090; } }\n\nLet's go ahead and create a virtual block for a regular nginx website site or edit an existing virtual block. This is the website/service that you will protect with Vouch Proxy.\n\nIn this example I am using a php web app. If you a non php site site to work you can remove this location block and and edit it to your needs.\n\nVouch Proxy config with Indieauth.com as a service provider.\n\nModify your config.yml\n\nThis will get you going, I have added some of my personal settings such as public access. Public access allows people to access your \"protected app\" without needing to login until you need them to login. Here are some options in how you can allow people to use your app. It is important to align the config in the config.yml file. Even an extra space can cause Vouch Proxy to fail. More on that in the next section.\n\nOptions\n\nYou can allow public access to the site but only allow everyone the option to sign into the website with a basic PHP script or use the corresponding http_header to use the website/web app to authenticate . Comment out or delete the line, \"allowAllUsers: true\" and include the line,\" publicAccess: true\" .\n\nYou can allow public access to the site but only allow certain people, (or in this case, domains) actually sign into the website. Comment out or delete the line, \"allowAllUsers: true\" and include the line,\" publicAccess: true\" .\n\nYou can force everyone to sign in before seeing the website but still allow everyone to access the website. Include the \"allowAllUsers: true\" and comment out or delete the line,,\" publicAccess: true\" .\n\n\n\nvouch: logLevel: debug listen: 0.0 .0 .0 port: 9090 allowAllUsers: true cookie: secure: false domain: lifeofpablo.com publicAccess: true oauth: provider: indieauth client_id: https://lifeofpablo.com auth_url: https://indieauth.com/auth callback_url: https://auth.lifeofpablo.com/auth\n\nRun/test your Vouch Proxy configuration.\n\nRun the following command\n\nnohup ./vouch-proxy -loglevel debug > vouch.log 2>&1 &\n\nIt should display a process ID (PID)\n\n[1] 53310\n\nHit enter. If no error or exit code displays, Vouch proxy is running!\n\nIf there is an error, it will exit such as the example below.\n\n[1]+ Exit 126 nohup ./vouch-proxy -loglevel debug > vouch.log 2>&1\n\nIf there is an error, make sure your there is not weird spacing or errors in the configuration. If you are sure that you have the information correct, use the examples provided by Vouch Proxy and copy and paste the example to get the formatting correct. Adjust the configuration as needed to match your needs.\n\nNginx Virtual block protected by Vouch Proxy\n\nserver { listen 80 ; listen [::]: 80 ; root /root/to/web/directory; index index.php index.html; server_name secretapp.example.com; location / { try_files $uri $uri / /index.php $is_args $args ; } client_max_body_size 100m ; location ~* \\.php$ { try_files $uri = 404 ; fastcgi_pass unix:/var/run/php/php8.2-fpm.sock; include fastcgi.conf; fastcgi_split_path_info ^(.+\\.php)(/.+)$ ; fastcgi_param SCRIPT_FILENAME $document_root $fastcgi_script_name ; fastcgi_param REMOTE_USER $auth_user ; } auth_request /vouch-validate; location = /vouch-validate { proxy_pass http://127.0.0.1:9090/validate; proxy_set_header Content-Length \"\" ; proxy_set_header Host $http_host ; proxy_set_header Remote-User $auth_user ; proxy_pass_request_body off ; proxy_set_header X-Real-IP $remote_addr ; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for ; proxy_set_header X-Forwarded-Proto $scheme ; auth_request_set $auth_resp_jwt $upstream_http_x_vouch_jwt ; auth_request_set $auth_resp_err $upstream_http_x_vouch_err ; auth_request_set $auth_resp_failcount $upstream_http_x_vouch_failcount ; } auth_request_set $auth_user $upstream_http_x_vouch_user ; location = /logout { return 302 https://vouch.domain.com/logout?url= $scheme :// $http_host ; } error_page 401 = @error401 ; location @error401 { return 302 https://vouch.domain.com/login?url=https:// $http_host $request_uri &vouch-failcount= $auth_resp_failcount &X-Vouch-Token= $auth_resp_jwt &error= $auth_resp_err ; } }\n\nEventually you will need to secure your site with SSL/TLS that makes your site use https://. More than ever, your traffic should be secure with using it as 0auth as the method used to sign in to your protected website(s).\n\nDo this after you have the server blocks working in the following section.\n\nHere is the link for Certbot for Debian. I have tested this on Debian 10 & 11. [https://certbot.eff.org/instructions?ws=nginx&os=debianbuster](Link for Certbot)\n\nCertbot can do this for you as long as you have the subdomain in your DNS pointing to your machine and have cert bot installed. It'll add these blocks in your server blocks automatically.\n\nIt'll look similar to this . Certbot will rearrange and add a few things.\n\nserver { server_name vouch.example.com . . . . . . . . . . . . . . . . . . . . . . . . . listen [::]: 443 ssl; listen 443 ssl; ssl_certificate /etc/letsencrypt/live/vouch.example.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/vouch.example.com/privkey.pem; include /etc/letsencrypt/options-ssl-nginx.conf; ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; } server { if ( $host = vouch.domain.com) { return 301 https:// $host $request_uri ; } listen 80 ; listen [::]: 80 ; server_name secretapp.example.com; return 404 ; }\n\nRepeat for every server block you'd like protect.\n\nLet's check for errors in nginx. Type the following command.\n\nsudo nginx -t\n\nYou should see something similar to this:\n\nnginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful\n\nThen restart nginx to push changes.\n\nsudo systemctl restart nginx\n\nLet's open a browser tab or window!\n\nNote: I'm using Firefox. (Preference). Any modern browser should work.\n\nDepending on your setup, you'll see a login screen or a website without login in\n\nType in the protected app' URL in the address bar Sign in with your domain (indieauth). Trigger a login later, if needed. OR Type in the protected app' URL in the address bar 2 - Trigger Login Later Trigger vouch proxy (indieauth).\n\nVoila, the protected page.\n\nHere is the home page of a Bludit CMS on subdomain acting as \"secretapp.example.com\"\n\nI will write a blog post on using it on my website and my RSS feeder. I will also go in deeper about the cookie as well.\n\nThat's it! You have setup indieauth to protect (or not your pages\n\nWant to see who's logged in?\n\nWhether you just want to see the user authenticated via the http_header or use http_header to help you authenticate into the website we can use a simple php script. We added already the other parts but here's an overview.\n\nIn your main server block, just below the line auth_request /vouch-validate; which enables the auth_request module, we added the following:\n\nauth_request_set $auth_user $upstream_http_x_vouch_user ;\n\nThis will take the HTTP header that Vouch sets, X-Vouch-User , and assign it to the nginx variable $auth_user . Then, depending on whether you use fastcgi or proxy_pass, include one of the two lines below in your server block:\n\nfastcgi_param REMOTE_USER $auth_user ; proxy_set_header Remote-User $auth_user ;\n\nThese will set an HTTP header with the value of $auth_user that your backend server can read in order to know who logged in. For example, in PHP you can access this data using:", "label": 1}
{"title": "Supercharge your notebooks: The new AI-first Google Colab is now available to everyone", "url": "https://developers.googleblog.com/en/new-ai-first-google-colab-now-available-to-everyone/", "content": "Last month at Google I/O 2025, we shared our vision for a reimagined, AI-first Colab, a true coding partner in your notebook designed to help you tackle your most challenging problems faster than ever. We\u2019ve started by rolling these features out to a small group of users, and the response has been incredible. Today, we are thrilled to make AI-first Colab available to everyone!\n\n\n\nEarly users have been embracing Colab's new agentic capabilities to accelerate their projects, learn new skills, and unlock insights from their data in ways that have delighted and inspired us.\n\n\n\nFrom early access to everyday productivity\n\nOur goal was to build an AI collaborator that understands your intentions and integrates seamlessly into your workflow. Based on user feedback, this new experience is already making a significant impact.\n\nHere are a few of the top ways people are using the new Colab AI:\n\n\n\n1: Accelerating End-to-End Machine Learning Projects\n\nUsers are leveraging Colab AI for the entire machine learning lifecycle. From taking a raw dataset and asking the agent to autonomously perform cleaning and preparation, to generating feature analysis, training models, and evaluating the results. This turns hours of work into a guided, conversational experience.\n\n\n\n2: Smarter Debugging\n\nCoding is an iterative process. Colab AI acts as a pair programmer to help you prototype ideas, generate boilerplate code, and understand new libraries. When you hit an error, the AI doesn't just help find the bug, it can suggest a fix in a clear diff view, helping you learn and keep going with your project. The result is a massive boost in productivity.\n\n\n\n3: Creating Stunning Visualizations with Zero Hassle\n\nData exploration is incomplete without visualization. Users are simply asking Colab AI to graph their data, and it generates high-quality, clearly labeled charts without the need for manual tweaking of plotting libraries.\n\n\n\nA quick look at the features powering your workflow\n\nThese use cases are powered by a suite of new, deeply integrated features:\n\nIterative Querying: A conversational experience where you can ask for code, get explanations about libraries, and intelligently fix errors.\n\nNext-Generation Data Science Agent (DSA): Trigger autonomous analytical workflows. The agent creates a plan, executes code, reasons about the results, and presents its findings, all while allowing you to provide feedback and stay in control.\n\nEffortless Code Transformation: Simply describe a change in natural language, and Colab will identify and refactor the relevant code for you.\n\n\n\nGet started with Colab AI today!\n\nWe are incredibly excited to put these powerful new capabilities into the hands of the entire Colab community. This is a major step in our journey to create a more powerful and intuitive AI-first Colab, and we\u2019re just getting started.\n\nReady to try it out? It\u2019s easy to get started:\n\n1: Open any new or existing notebook in Google Colab.\n\n2: Look for the Gemini spark icon in the bottom toolbar.", "label": 0}
{"title": "Fixed-Power Designs: It\u2019s Not IF You Peek, It\u2019s WHAT You Peek at", "url": "https://engineering.atspotify.com/2024/5/fixed-power-designs-its-not-if-you-peek-its-what-you-peek-at", "content": "TL;DR Sometimes we cannot estimate the required sample size needed to power an experiment before starting it. To alleviate this problem, we could run a sequential test or an A/A test. However, sequential tests are typically less sensitive and introduce bias to the treatment effect estimator. Moreover, A/A tests prolong the duration of the experiment and still don\u2019t guarantee that the resulting sample size calculation is accurate. In this blog post, we present highlights from our recent paper (Nordin and Schultzberg, 2024), where we introduce an alternative that we call \u201cfixed-power design.\u201d In a fixed-power design, you start the experiment without an estimated sample size, estimate the required sample size from the currently available outcome data in the experiment, and stop when your current sample size is larger than the required sample size. We show that fixed-power designs can be analyzed using nonsequential methods without any corrections. The point estimator is consistent, and the treatment effect confidence interval has asymptotically nominal coverage. Not all forms of peeking inflate the false positive rate of fixed-sample inference.\n\nIntroduction\n\nThere are many reasons why companies use online experiments. Reasons could be, for example, the following:\n\nTo identify the best version of a product\n\nTo quantify the impact of a product change\n\nTo detect regressions before a bug reaches all users\n\nOnline experiments let you do all these things while managing the risk of making the wrong decisions.\n\nAt Spotify, some of the goals of experimentation are to learn what works, how well it works, and stop the things that don\u2019t work early on. However, the extent to which we can achieve these goals depends on the experimental design and analysis. For example, certain designs promote early stopping but pay a price in terms of power \u2014 with reduced chances overall of finding true effects. Other designs instead focus on maximizing power, but at the expense of prolonged runtime because they don\u2019t allow early stopping. In the next section, we dig into the most common designs for A/B tests, discuss limitations of common approaches, and introduce a new design that mitigates some of those limitations.\n\nSequential experimental designs: more than just sequential testing\n\nTwo of the most fundamental concerns of experimental design are when to stop the experiment and when to analyze the results. Experimental designs can be roughly split into two categories: fixed-sample designs and sequential designs.\n\nFixed-sample designs\n\nIn fixed-sample designs, the experimenter leverages power analyses, also known as sample size calculations, to set a predetermined sample size. The power analysis produces a required sample size that the experiment should meet. If it does, the comparison will have high enough precision to limit the risk of missing effects of a certain size of interest. The experiment collects data until the predecided sample size is met, at which point the statistical analysis is performed.\n\nSequential designs\n\nIn sequential designs, the sample size isn\u2019t predetermined.1 In principle, a sequential design is a design where users enter the experiment sequentially and the experiment stops according to a rule based on the available data. The sequential design uses a stopping rule that only indirectly determines the sample size as it\u2019s being evaluated during the experiment. The most common sequential design, often simply called \u201csequential testing,\u201d stops once the test detects a significant result.\n\nIn the context of online experimentation, many recommend using sequential tests to detect regressions but recommend against using sequential tests for the shipping decision, due to power and bias concerns with sequential tests. See, for example, Fan et al. (2004) and our previous blog post on comparing sequential testing methods.\n\nUsing a hybrid design\n\nIn practice, many companies, in fact, use a hybrid design. That is, the treatment effect is estimated and evaluated using statistical tests derived for fixed-sample designs. However, the design is sequential, because the experiment runs until the current sample size exceeds the estimated required sample size. The estimated required sample size, in turn, uses an estimate of the variance derived from the data collected so far in the experiment. In Nordin and Schultzberg (2024), we call this a fixed-power design \u2014 you sample new users until the power criterion, according to the currently available data \u2014 is met.\n\nThe graph above shows an example of how a fixed-power design can play out. To keep the graph easy to read, the sample size is kept small. In this case, the experiment stops very close to the true required sample size. In large samples (small-powered effects), as the required sample size estimator becomes precise, the region in which the sample size crosses the estimated required sample size will often be quite narrow.\n\nFixed-power designs: a summary of Nordin and Schultzberg (2024)\n\nIn Nordin and Schultzberg (2024), we investigate the properties of the difference-in-means average treatment effect estimator in sequential designs where the stopping rule is based on the precision of the treatment effect estimator. We express the precision in two ways, as the confidence interval (CI) width, and the current required sample size for a given hypothetical treatment effect. As shown in our paper, stopping based on the required sample size is equivalent to stopping on the confidence interval width, as it\u2019s just a transformation of the variance of the treatment effect estimator.\n\nThese kinds of stopping rules based on precision are common in practice, with some experimentation vendors even selling them. However, to the best of our knowledge, the statistical implications of \u200cprecision-based stopping rules haven\u2019t been rigorously investigated.\n\nThe fixed-power design can make your peeking problem alarm bell go off. It is, after all, a stopping rule that uses outcome data to determine whether to stop. This is what\u2019s responsible for why we use sequential testing in the first place, and therefore, it\u2019s not unreasonable to expect corrections to be required if you stop based on the required sample size, too. However, in Nordin and Schultzberg (2024), we show that not all stopping rules based on \u200coutcome data are equally problematic for \u200cstatistical inference.Our research shows that functions of the sample variance are much less problematic than stopping based on, for example,\u200c significance.\n\nWhat aspects of the outcome data we peek at determines the effects \u2014 if any \u2014 of peeking on an inference about the estimand we are interested in. In our paper, we show that under a fixed-power design, the following are true:\n\nThe difference-in-means estimator consistently estimates the average treatment effect.\n\nThe fixed-sample confidence interval for the average treatment effect has asymptotically correct coverage.\n\nThis means that in large samples, we can use standard inference even when we stop based on the current estimated required sample size. No further adjustments are necessary to guarantee correct inference.\n\nIn the paper, we also propose conservative finite-sample versions of the fixed-power design and the fixed-width confidence interval design.\n\nPre-experiment sample size calculation is hard\n\nFixed-power designs let us peek at the required sample size without adjusting inference. Why is this important? Can\u2019t we use historical data for a power analysis to determine the required sample size?\n\nEstimating the required sample size during experiments as a complement to pre-experiment power analyses is often necessary, as historical data can fail to accurately describe the outcome distributions. At Spotify, for example, the diverse and ever-changing user base, especially in new markets and with new-user experiments, makes historical comparisons unreliable. Adding to that, historical data won\u2019t reflect treatment effects since new variants haven\u2019t been tested, and assuming homogeneous treatment effects across a diverse customer base is unrealistic. Users with different listening habits will likely respond differently to the same feature changes.\n\nLeveraging observed outcomes during experiments can enhance sample size accuracy and inform the experimenter early on if their initial planning is accurate. With the guarantees of fixed-power designs, we can plan according to a required sample size calculated before the experiment, revise it during the experiment, and finally stop at the right time \u2014 all while relying on standard fixed-sample inference.\n\nSequential testing versus fixed-sample testing\n\nSequential tests give valid inference under any stopping rule, so why not just rely on sequential tests and peek at the required sample size as much as we want?2\n\nAs have been discussed in many places (Larsen et al. 2024, our previous blog post), there are two main reasons:\n\nUnbiased point estimators. Sequential tests that stop on significance yield biased estimators that overestimate \u200ceffect size. Moreover, the idea of stopping on first significance is in stark contrast to the advice of many not to trust experiments with too low power. Power. In most situations, experiments must run for at least a given period. This could, for example, be to obtain data on users in the experiment for a sufficiently long time to rule out novelty effects. Another reason, common at Spotify, is to avoid issues with seasonal effects on weekdays. Using sequential testing in situations where we don\u2019t intend to stop based on the first significance is a waste of power. If stopping is prohibited during a large part of the experiment, sequential tests that aren\u2019t taking this into account will be highly conservative.\n\nWith the fixed-power design, we get the benefits from fixed-sample designs, but with the added ability to inform the stopping based on a continuous power analysis of the experiment.\n\nSequential design Traditional fixed-sample design Fixed-power design (Nordin and Schultzberg, 2024) \u2013 Sequential tests allow early stopping in experiments with a stopping rule based on significance or any other function of the data. \u2013 Sequential tests bound false positive rates and coverage at least at the intended level under early stopping. \u2013 Sequential tests are conservative if you always want to run the experiment until you reach a certain precision. This is because they adjust for early stopping on significance (even if you don\u2019t use it). \u2013 Sequential tests (with early stopping) give biased difference-in-means estimators. \u2013 Fixed-sample tests require the sample size to be fixed ahead of time. \u2013 To achieve a certain precision, you need to estimate the variance of the outcome(s) from historical data before the experiment is started to plan the sample size. \u2013 The difference-in-means estimator is unbiased and the standard fixed-sample CI has the right coverage. \u2013 Fixed-power designs estimate the current required sample size from outcome data during the experiment. \u2013 Fixed-power designs stop when the current sample size is larger than the estimated required. \u2013 Under a fixed-power design, the standard difference-in-means estimator is consistent, and the fixed-sample CI has asymptotic nominal coverage.\n\nSummary\n\nIn the ever-evolving landscape of online experiments, determining the optimal time to stop an experiment remains a substantial challenge. Traditional methods, such as fixed-sample and sequential test designs, each have limitations. Fixed-sample designs predetermine the sample size but don\u2019t allow adjustments based on incoming data, while sequential test designs can adjust but may affect the power and bias of the results.\n\nIn our recent paper, Nordin and Schultzberg (2024), we introduce an innovative approach called the \u201cfixed-power design.\u201d This method allows an experiment to start without a predefined sample size it needs to reach. Instead, the required sample size is estimated from ongoing outcome data, and the experiment concludes when the current sample size surpasses this estimate. Crucially, this design supports standard nonsequential inference, guaranteeing consistent point estimators and maintaining nominal coverage in confidence intervals. This means that the fixed-power design allows sequential stopping without losing the power benefits from nonsequential tests.\n\nThis design is particularly advantageous in environments like Spotify, where the user base is diverse and constantly changing. Traditional pre-experiment calculations based on historical data often fall short because they don\u2019t account for the variability in treatment effects across different user segments or for new user experiences.\n\nThe fixed-power design provides a practical balance between the rigidity of fixed-sample designs and the flexibility of sequential test designs, providing reliable decision-making in product development. At the same time, many challenges remain. Although the fixed-power design makes it possible to do real-time adjustments to the required sample size, it\u2019s problematic to not know the required sample size in the planning stage. At Spotify, where we run tens of thousands of experiments, there are always limitations to how large an experiment a team can run. If it\u2019s detected during the experiment that the required sample size is much larger than the team had anticipated, it\u2019s not always possible to run it longer or increase the proportion of the population that the experiment targets because of other conflicting experiments. In this situation, fixed-power designs offer a way to know early in an experiment if the data is in line with the pre-experiment power analysis.\n\nAcknowledgments: This blog post is based on a paper written in collaboration with Mattias Nordin, Department of Statistics, Uppsala University, Sweden.\n\nGet access to Spotify\u2019s decision engine via Confidence. In Confidence, you can always access the current required sample size and current-powered effect while an experiment is live. This means that you can use a fixed-power design by simply starting the experiment using a fixed-sample design. As usual, we ensure that the statistics are all in order for any results we show you \u2014 so you can focus on building a great product. Want to learn more about Confidence? Check out the Confidence blog for more posts on Confidence and its functionality. Confidence is currently available in private beta. If you haven\u2019t signed up already, sign up today, and we\u2019ll be in touch.\n\n1 In some types of sequential tests, the maximum sample size needs to be predecided, but not the stopping sample size. 2 At least so-called always-valid sequential tests.", "label": 0}
{"title": "Creating rituals: remembering Coach Taylor \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2024/08/16/creating-rituals-remembering-coach-taylor/", "content": "My high school track and cross country coach Mr. Taylor was a master at creating rituals and culture. A former seminary student who was obsessed with ancient Greece, he started each practice with a philosophy discussion. He wove in, too, tales of past runners and triumphs \u2014 he\u2019d been coach for nearly thirty years already \u2014 creating our own pantheon of heroes, the team\u2019s own mythos. Who of us would one day join its ranks?\n\nMr. Taylor recognized how much the mind and body are connected. He treated our labor seriously, and expected us to do the same. Excellence \u2014 ar\u00eate \u2014 was our goal. I\u2019ll admit that plenty of times I tuned out the talk, but there was something about sitting down as a team to transition from school to sport, using that time to mentally prepare for the physical work.\n\nOne of my favorite rituals was to honor outgoing seniors on the cross-country team. At the final home race of the season, each senior released one of his homing pigeons. (I was annoyed because I had to share a pigeon \ud83d\ude02) As far as I can remember, there was no special meaning to this, it just was a thing that he made happen each year. A created ceremony we nonetheless looked forward to; simply having some marker, however arbitrary, gave the season an emotional culmination beyond the final race, recognition of our contributions to the team regardless of how we placed.\n\nHe built the team\u2019s culture through these shared rites and rituals, building a sense of continuity with past runners both at our school and at the origins of track in Ancient Greece.\n\nI can\u2019t talk about ritual and Mr. Taylor without also talking about Ar\u00eate West. Starting in 1979, every other year he took a group of about eight students from the track or cross country team to Greece. At first, they went to compete, but over the years it shifted into more of a philosophical journey and extended into greater Europe. Over the course of twenty trips, the experience developed its own rituals. (I went on trip thirteen.)\n\nThe first: secret invitation. My best friend hounded me, but I wouldn\u2019t confess that I\u2019d received The Postcard telling me when and where our first meeting would be. (We met every other week throughout the school year to study and prepare for the trip.) We all showed up that first night unsure who the seven other voyagers would be. In my year, the eighth person no-showed \u2014 we never found out who else had been invited and declined to join.\n\nThe trip itself teemed with rituals. My least favorite involved running through the hot, dusty Greek countryside to an ancient track where we were expected to take off our shoes and run a lap in bare feet, like the runners of old. On Ithaca, we ran, separately, to \u201cTelemachos\u2019 beach\u201d. In Delphi, we were meant to sleep in \u201cthe meadow\u201d where many past Arete trips had slept (but he couldn\u2019t find it in the twilight, so we slept on a random dirt road \ud83d\ude12 Apparently on my sister\u2019s trip there was a dead dog in the meadow and they said no fucking way).\n\nMr. Taylor believed strongly in the power of place and designed the experience to provoke encounters with meaning. (Some more successful than others\u2026) I lugged my clarinet across Europe so I could play Mozart\u2019s clarinet concerto outside the building where he died (?). Everywhere we went, one or another of us had been assigned topics to present on or locations to give a reading. (I botched my reading from The Agony and The Ecstasy at the Vatican and read a passage about the wrong Michaelangelo Mother and Child sculpture \ud83e\udd23) Overlooking the Acropolis, one of the boys had to read us Pericles\u2019s funeral speech, as each trip had done before us. In Sils Maria, we walked in Nietzsche\u2019s footsteps and discussed quotes at various waypoints along the trail. Our trip was our own, yet inextricably linked with previous trips through shared touchstones and repeated experiences: rituals meant to connect us to the past, both modern and ancient.\n\nThis is my contribution to the August IndieWeb Blog Carnival, hosted by Steve Ledlow on the theme of ritual.", "label": 1}
{"title": "Fighting the forces of clock skew when syncing password payloads", "url": "https://dropbox.tech/application/dropbox-passwords-clock-skew-payload-sync-merge", "content": "A good password manager should be able to securely store, sync, and even autofill your username and password when logging into websites and apps. A password manager like\u2026Dropbox Passwords! When we released Dropbox Passwords in the Summer of 2020, it was important we ensured that a user\u2019s logins would always be available\u2014and up to date\u2014on any device they used. Luckily, Dropbox has some experience here, and we were able to leverage our existing syncing infrastructure to copy a user\u2019s encrypted password info, known as a payload, from one device to another. However, while implementing this crucial component, we encountered an unexpected syncing issue where, sometimes, out-of-date login items would overwrite newer, more recent changes. Eventually we found a solution that built on prior Dropbox syncing work. But it also involved contemplating the very nature of time itself.\n\nThe two-way merge\n\nDropbox Passwords uses zero-knowledge encryption. This means the private encryption key for a user\u2019s passwords is only stored on their local devices, and the server is unable to decrypt them. While this is good for security purposes, it means we need to rely on the client to manage syncing and merging, and thus cannot rely on the server as a source of truth for recency as we typically would. Initially, our sync algorithm used a two-way merge. If a user edited an existing login item\u2014thus, modifying the client\u2019s local payload\u2014the client performed the following steps: Update the local payload with the new details and current time.\n\nDownload the equivalent payload from the server.\n\nCompare the timestamp of the local login item with the timestamp of the remote login item.\n\nUse the newer login item as the winner. Here\u2019s a diagram illustrating what the flow might look like when a user updates their password from hunter1 to hunter2.\n\nA successful merge and sync.\n\nIn theory, this worked nicely. Old items were replaced with their most recent versions. In practice, however, we noticed that some items were syncing incorrectly. One of our engineers discovered a phenomenon in which an older item would somehow trump a newer one. After some thorough investigation, we found our culprit: clock skew.\n\n\n\nConflicting clocks\n\nLet\u2019s consider the following scenario. You are a happy Dropbox Passwords user with a computer and phone synced to the same account. You edit the notes field on one of your login items from your phone so that you can remember those pesky security questions. A few minutes later, you realize that you made a mistake when typing one of the questions\u2014but your phone is all the way across the room. So you open up your computer to fix your mistake. After updating the notes field, you save the item, but something strange has happened. You are shocked to see your initial change\u2014the one you made from your phone\u2014has overwritten the newer change you just made on your laptop. How could this be? Recall that the client is responsible for setting the timestamps on each item. We generally assume that our electronic devices have an accurate representation of the current time, but this is not always the case. It is actually possible for a device\u2019s time to be out of sync with true time. This could be the result of a dead CMOS battery, an incorrect time zone setting, or perhaps even malware. In our example, the phone\u2019s login item trumps the computer\u2019s login item because of this clock skew. If the computer\u2019s clock is behind true time\u2014by ten minutes, ten seconds, or even ten years\u2014it\u2019s possible that a more recent change will actually be marked as older. Here is a diagram illustrating what a failed sync might look like with offset clocks.\n\nAn example of a failed merge. Note that the timestamp on the newer change is older than the stable version.\n\nThe three-way merge\n\nIn order to tackle this problem, we borrowed the idea of a three-way merge from Git, the popular source control tool. Git uses a three-way merge when a user wants to merge two branches but there isn\u2019t a linear path to merge from one branch to another branch. In these cases, there is a common ancestor of both branches, but no branch is an ancestor of the other. In a three-way merge, the common ancestor and the tips of both branches are used to generate a merged commit. In fact, we already use the same idea in the Dropbox desktop client when syncing files between devices. Although this client has a more advanced version of conflict handling, the basic principle is the same. We realized a three-way merge might help us here, too. In our case, the main problem with doing a two-way merge is that, because we can\u2019t use the server as source of truth, we don\u2019t have a revision history to keep track of which revision is earlier. As a result, we always rely on timestamps to determine which update is newer. In order to solve this problem, we introduced a third payload copy known as base. Now\u2026 We have a local payload that represents what the user sees on their client.\n\npayload that represents what the user sees on their client. We have a remote payload that represents what is stored on the Dropbox server.\n\npayload that represents what is stored on the Dropbox server. We have a base payload that represents the most recently synced payload on that client. Instead of a simple time comparison of local and remote, we can now generate a diff from local <> base and a diff from remote <> base. If there are any differences between base and the payload to which we are comparing it, we know that the most recent change must come from the non-base payload, since there is no way to edit base directly. Once we obtain a list of local diffs and remote diffs, we can attempt to consolidate changes, with a goal of bringing the three payloads back in sync. There are two possible outcomes when comparing the diffs: Entries in either diff but not both. This means that the diffs do not conflict with each other. Both changes can be safely applied.\n\nThis means that the diffs do not conflict with each other. Both changes can be safely applied. Entries in both diffs. Changes in this category are considered conflicted, because both the local and remote payloads have updates to the same item. In this case, we will use their timestamp to resolve conflicts. The winner of the change will be the one that has a more recent timestamp. A keen reader will point out that this merging process is still susceptible to clock skew\u2014but under normal usage, this is unlikely to ever occur. A user would not only need two devices with out-of-sync clocks, but would have to edit the same item on both devices at the same time. With the three-way merge in place, it was clear the addition of a base payload solved our merge issue. In a typical clock skew scenario, we no longer need to compare timestamps as there should be no conflicts during our merge.\n\nA successful merge and sync with skewed clocks.\n\nA solid foundation for the future\n\nAfter a year in general availability, the three-way merge has proven to be a much more robust solution than what we had before. We were especially thankful for the extra stability as we added new features throughout 2021\u2014including support for password sharing and payment card items. Our two-way merge issue could have been much more frustrating for users trying to modify a shared item, so we were happy to solidify our foundation before building atop it. But why read about it when you can try it for yourself? Install Dropbox Passwords today and let your device finally experience the thrill of a three-way merge.\n\nOne more thing\u2026", "label": 0}
{"title": "Unlocking the Potential of Quantum Computing", "url": "https://developers.googleblog.com/en/unlocking-the-potential-of-quantum-computing-a-developers-guide-to-error-correction/", "content": "A Developer\u2019s Guide to Error Correction\n\nManipulating quantum states on a superconducting chip cooled to a fraction of a degree above absolute zero at the bottom of a cryogenic fridge is incredibly challenging. And things don't always go to plan. Errors happen. Lots of errors. Sophisticated strategies are required to extract reliable computation from what would otherwise be just so many random output bits. Success is only possible if errors are not too overwhelmingly common. Today, Google Quantum AI announced the construction of a quantum chip where errors are indeed not overwhelmingly common. It makes use of the surface code, essentially a square patch of nearest-neighbor coupled physical qubits that work together to form a single more reliable logical qubit. As the square patch gets bigger, a logical qubit should get more reliable, and that is exactly what we demonstrated: a chip with logical qubits getting over a factor of two more reliable with each increase in patch size. But why is this needed\u2014what are quantum errors and how are they corrected? We'll give a short version of the answers here, and if you want more than a brief overview, you can head straight to the hands-on quantum error correction course now available for free on Coursera where you will find a series of videos and exercises to develop your understanding. \u201cThis new course opens a door to a complex domain, making quantum error correction not just accessible, but tangible for millions of learners worldwide,\u201d says Marni Baker Stein, Chief Content Officer at Coursera. \u201cOur collaboration with an industry pioneer like Google Quantum AI signifies another step towards a future where knowledge is the key to harnessing the power of quantum computing.\u201d\n\nWhat is a quantum error? Let's start with classical errors. You have a bit. Suppose it should be 0. But a cosmic ray hits it and it becomes 1. That's a bit-flip error, the only type of error in a classical computer. In comparison, a quantum chip doesn't consist of bits but rather multilevel quantum systems so cold they have well-defined discrete ground and excited states. We write these states as |0>, |1>, |2>, etc. When a cosmic ray hits a qubit that should be |0>, it can produce a superposition a|0> + b|1> + c|2> + ... of many states. Careful engineering of our device has made our qubits robust to all but the most energetic impacts, allowing us to compute results and simply discard the output when we are hit. When computing, we try hard to only use states |0> and |1>, but control and measurement errors inevitably lead to the occasional |2+> state, known as leakage errors. Resetting a qubit gets rid of this error, but also gets rid of any data on that qubit, so we have a special gate that moves higher states off data qubits and onto a qubit about to be reset. This prevents the accumulation of |2+> states in the computer. Then there is the problem that quantum data just doesn't like hanging around very long. Qubits like to relax. If you use an excited state to represent |1>, after a short time it will relax to |0>. We also want to be able to store superpositions like a|0> + b|1>, and that \u2018plus\u2019 can spontaneously become a \u2018minus\u2019, or a phase-flip error. The various ways that qubits lose data are collectively called decoherence. In general, decoherence can produce a completely different state to the one we want, but fortunately this difference can be broken into a mix of bit flips and phase flips; for brevity we call them X and Z errors.\n\nHow do we detect X and Z errors? Let's start with a picture of our quantum chip.\n\nThis is a 2D array of qubits with nearest neighbor interactions only. Data qubits store our precious quantum state. Measure qubits are used to detect X and Z errors. A good way to understand this is to imagine each light blue region detecting Z errors on the data qubits it touches, and each dark blue region detecting X errors. A Z error on a data qubit activates the neighboring light blue regions, locating it and allowing us to compensate for its presence in software. Provided the density of X and Z errors is low enough, the pattern of lit up regions will give us clear information allowing us to find and compensate for these errors. If a measure qubit suffers an error, that can give you a falsely activated region. To cope with this, the search for errors is repeated as often as possible, and the next time the check is performed there is a good chance it will be resolved. This creates an identifiable signature for a measurement error, enabling these to also be handled in software. It is an area of ongoing research to devise more sophisticated algorithms to handle the output of measure qubits to better identify the location and type of errors, all while keeping pace with the quantum computer.\n\nLearning more What we\u2019ve covered above is scraping the surface of quantum error correction and the critical role it plays in advancing quantum computing. For a step-by-step explanation and labs starting from the very basics, through quantum states and circuits, to some of the latest error correction tools used today, go to Coursera for our hands-on quantum error correction course. If you are a software engineer who has always wanted to work on a quantum problem, after taking the course head over to Quantum AI open source tools, where we build open-source software like Cirq, Stim and Crumble to simulate quantum circuits and develop error correction techniques. Learn how to design quantum algorithms and contribute to the development of tools that will enable the realization of practical quantum computing.\n\nQuantum computing: extra credit Here is an exciting area of research that combines theoretical computer science, software engineering, and quantum physics:", "label": 0}
{"title": "Breaking down CPU speed: How utilization impacts performance", "url": "https://github.blog/engineering/architecture-optimization/breaking-down-cpu-speed-how-utilization-impacts-performance/", "content": "Introduction \u26f5\n\nThe GitHub Performance Engineering team regularly conducts experiments to observe how our systems perform under varying load conditions. A consistent pattern in these experiments is the significant impact of CPU utilization on system performance. We\u2019ve observed that as CPU utilization rises, it can lead to increased latency, which provides an opportunity to optimize system efficiency. Addressing this challenge allows us to maintain performance levels while reducing the need for additional machines, ultimately preventing inefficiencies.\n\nAlthough we recognized the correlation between higher CPU utilization and increased latency, we saw an opportunity to explore the specific thresholds and impacts at various stages in greater detail. With a diverse set of instance types powered by different CPU families, we focused on understanding the unique performance characteristics of each CPU model. This deeper insight empowered us to make smarter, data-driven decisions, enabling us to provision our infrastructure with greater efficiency and confidence.\n\nWith these goals in mind, we embarked on a new journey of exploration and experimentation to uncover these insights.\n\nExperiment setup \ud83e\uddf0\n\nCollecting accurate data for this type of experiment was no easy feat. We needed to gather data from workloads that were as close to our production as possible, while also capturing how the system behaves under different phases of load. Since CPU usage patterns vary across workloads, we focused primarily on our flagship workloads. However, increasing the load could introduce small performance discrepancies, so our goal was to minimize disruption for our users.\n\nFortunately, a year ago, the Performance Engineering team developed an environment designed to meet these requirements, codenamed Large Unicorn Collider (LUC). This environment operates within a small portion of our Kubernetes clusters, mirroring the same architecture and configuration as our flagship workloads. It also has the flexibility to be hosted on dedicated machines, preventing interference from or with other workloads. Typically, the LUC environment remains idle, but when needed, we can direct a small, adjustable amount of traffic towards it. Activating or deactivating this traffic takes only seconds, allowing us to react quickly if performance concerns arise.\n\nTo accurately assess the impact of CPU utilization, we first established a baseline by sending moderate production traffic to a LUC Kubernetes pod hosted on one of its dedicated machines. This provided us with a benchmark for comparison. Importantly, the number of requests handled by the LUC pods remained constant throughout the experiment, ensuring consistent CPU load over time.\n\nOnce the baseline was set, we gradually increased CPU utilization using a tool called \u201cstress,\u201d which artificially occupies a specified number of CPU cores by running random processing tasks. Each instance type has a different number of CPU cores, so we adjusted the steps accordingly. However, the common factor across all instances was the total CPU utilization.\n\nNote: It\u2019s important to recognize that this is not a direct 1:1 comparison to the load generated by actual production workloads. The stress tool continuously runs mathematical operations, while our production workloads involve I/O operations and interrupts, which place different demands on system resources. Nevertheless, this approach still offers valuable insights into how our CPUs perform under load.\n\nWith the environment set up and our plan in place, we proceeded to collect as much data as possible to analyze the impact.\n\nResults \ud83d\udcc3\n\nWith our experiment setup finalized, let\u2019s examine the data we gathered. As previously mentioned, we repeated the process across different instance types. Each instance type showed unique behavior and varying thresholds where performance started to decline.\n\nAs anticipated, CPU time increased for all instance types as CPU utilization rose. The graph below illustrates the CPU time per request as CPU utilization increases.\n\nCPU time per request vs CPU utilization\n\nThe latency differences between instance types are expected due to the variations in CPU models. Focusing on the percentage increase in latency may provide more meaningful insights.\n\nLatency percentage increase vs CPU utilization\n\nIn both graphs, one line stands out by deviating more than the others. We\u2019ll examine this case in detail shortly.\n\nTurbo Boost effect\n\nAn interesting observation is how CPU frequency changes as utilization increases, which can be attributed to Intel\u2019s Turbo Boost Technology. Since all the instances we used are equipped with Intel CPUs, the impact of Turbo Boost is noticeable across all of them. In the graph below, you can see how the CPU frequency decreases as the CPU utilization increases. The red arrows are showing the CPU utilization level.\n\nCPU Cores Frequency\n\nWhen CPU utilization remains at lower levels (around 30% or below), we benefit from increased core frequencies, leading to faster CPU times and, consequently, lower overall latency. However, as the demand for more CPU cores rises and utilization increases, we are likely to reach the CPU\u2019s thermal and power limits, causing frequencies to decrease. In essence, lower CPU utilization results in better performance, while higher utilization leads to a decline in performance. For instance, a workload running on a specific node with approximately 30% CPU utilization will report faster response times compared to the same workload on the same VM when CPU utilization exceeds 50%.\n\nVariations in CPU frequency are not the only factors influencing performance changes. All our nodes have Hyper-Threading enabled, an Intel technology that allows a single physical CPU core to operate as two virtual cores. Although there is only one physical core, the Linux kernel recognizes it as two virtual CPU cores. The kernel attempts to distribute the CPU load across these cores, aiming to keep only one hardware thread (virtual core) busy per physical core. This approach is effective until we reach a certain level of CPU utilization. Beyond this threshold, we cannot fully utilize both virtual CPU cores, resulting in reduced performance compared to normal operation.\n\nFinding the \u201cGolden Ratio\u201d of CPU utilization\n\nUnderutilized nodes lead to wasted resources, power, and space in our data centers, while nodes that are excessively utilized also create inefficiencies. As noted, higher CPU utilization results in decreased performance, which can give a misleading impression that additional resources are necessary, resulting in a cycle of over-provisioning. This issue is particularly pronounced with blocking workloads that do not follow an asynchronous model. As CPU performance deteriorates, each process can manage fewer tasks per second, making existing capacity inadequate. To achieve the optimal balance\u2014the \u201cGolden Ratio\u201d of CPU utilization\u2014we must identify a threshold where CPU utilization is sufficiently high to ensure efficiency without significantly impairing performance. Striving to keep our nodes near this threshold will enable us to utilize our current hardware more effectively alongside our existing software.\n\nSince we already have experimental data demonstrating how CPU time increases with rising utilization, we can develop a mathematical model to identify this threshold. First, we need to determine what percentage of CPU time degradation is acceptable for our specific use case. This may depend on user expectations or performance Service Level Agreements (SLAs). Once we establish this threshold, it will help us select a level of CPU utilization that remains within acceptable limits.\n\nWe can plot the CPU utilization vs. CPU time (latency) and find the point where:\n\nCPU utilization is high enough to avoid resource underutilization.\n\nCPU time degradation does not exceed your acceptable limit.\n\nA specific example derived from the data above can be illustrated in the following graph.\n\nPercentage Increase in P50 Latency vs CPU Utilization\n\nIn this example, we aim to achieve less than 40% CPU time degradation, which would correspond to a CPU utilization of 61% on the specific instance.\n\nOutlier case\n\nAs previously mentioned, there was a specific instance that displayed some outlying data points. Our experiment confirmed an already recognized issue where certain instances were not achieving their advertised maximum Turbo Boost CPU frequency. Instead, we observed steady CPU frequencies that fell below the maximum advertised value under low CPU utilization. In the example below, you can see an instance from a CPU family that advertises Turbo Boost frequencies above 3 GHz, but it is only reporting a maximum CPU frequency of 2.8 GHz.\n\nCPU cores frequency\n\nThis issue turned out to be caused by a disabled CPU C-state, which prevented the CPU cores from halting even when they were not in use. As a result, these cores were perceived as \u201cbusy\u201d by the turbo driver, limiting our ability to take advantage of Turbo Boost benefits with higher CPU frequencies. By enabling the C-state and allowing for optimization and power reduction during idle mode, we observed the expected Turbo Boost behavior. This change had an immediate impact on the CPU time spent by our test workloads. The images below illustrate the prompt changes in CPU frequencies and latency reported following the C-state adjustment.\n\nCPU cores frequency\n\nP50 CPU time on a request\n\nUpon re-evaluating the percentage change in CPU time, we now observe similar behavior across all instances.\n\nPercentage Increase in P50 Latency vs CPU Utilization\n\nAs we anticipated many of these insights, our objective was to validate our theories using data from our complex system. While we confirmed that performance lowers as CPU utilization increases across different CPU families, by identifying optimal CPU utilization thresholds, we can achieve a better balance between performance and efficiency, ensuring that our infrastructure remains both cost-effective and high performing. Going forward, these insights will inform us of our resource provisioning strategies and help us maximize the effectiveness of our hardware investments.\n\nThank you for sticking with us until the end!! A special shout-out to @adrmike , @schlubbi , @terrorobe , the @github/compute-platform and finally the @github/performance-engineering team for their invaluable assistance throughout these experiments, data analysis, and for reviewing the content for accuracy and consistency. \u2764\ufe0f\n\nTags:", "label": 0}
{"title": "Norman Thavaud - Le comedien fran\u00e7ais 2", "url": "https://lifeofpablo.com/blog/norman-thavaud-le-comedien-fran%C3%A7ais-2", "content": "Norman Thavaud - Le comedien fran\u00e7ais 2\n\nThis post was written in English (en_US).\n\n\"Norman Thavaud (born 14 April 1987) is a French humourist and blogger known for his short comical YouTube videos. Each of his videos have received at least two million views, some receiving over 7 million. (Wikipedia)\n\nNorman Thavaud is a French comedian who has become an internet sensation with his video blogs. He adresses what we would say \"\"first world problems\"\" things like technology. It shows us our modern struggles in a different point of view. He really gets you laughing. Norman has made particpated in many short films and been sponsored by many companies. One in Particular the candy bar Crunch.\n\nWatching his videos has really improved my French skills buy listening and watching a true french person speak. It makes learning another language easier if you get a grip on how it is actually pronounced. Hey he has taught me a lot of slang! Which case is how many young people communicate in this day and age.\n\nI really reccommend his videos to all age groups!\n\nUne s/o a Norman pour tes cool vid\u00e9os! Chaque jour tu m\u2019empresses avec tes blagues!! Je ne peut pas attendre pour la nouvelle vid\u00e9o.\"", "label": 1}
{"title": "Simplified Dataflow Connectors with Managed I/O", "url": "https://developers.googleblog.com/en/simplified-dataflow-connectors-with-managed-io/", "content": "Google Cloud Dataflow offers a fully managed data processing system for running Apache Beam pipelines on Google Cloud in a highly scalable manner. Due to being a fully managed service, Dataflow users do not have to worry about any service side regressions and versioning. The promise is that you only concern yourself with your pipeline logic while Google takes care of the service infrastructure. While this is certainly true, Apache Beam itself is a very full featured SDK that provides many simple to highly complex transforms for you to use in their pipelines. For example, Apache Beam provides a number of I/O connectors. Many of these connectors are Apache Beam composite transforms from 10s to 100s of steps. Historically, these have been considered \"user code\" from the service's perspective, despite being not authored or maintained by the user. There are several common complications customers run into complex Beam transforms such as I/O connectors. You are on the hook for upgrading Beam to adopt any fixes and improvements to connectors. Connector APIs vary widely and moving from one connector to another usually requires a lot of exploration and learning. While connectors offer a complete API, the API might not be optimized for the Dataflow runner. To alleviate all three of these issues, Dataflow recently introduced a new offering named Managed I/O. With Managed I/O the service itself is able to manage these complexities on your behalf. Hence you can truly focus on their pipelines business logic instead of focussing on the minutiae related to using and configuring a specific connector to suit their needs. Below we detail how each of the above mentioned complexities are addressed via Managed I/O.\n\nAutomatic SDK upgrades Apache Beam is a fully fledged SDK with many transforms, features, and optimization. Like many large pieces of software, upgrading Beam to a new version can be a significant process. Usually upgrading Beam involves upgrading all parts of a pipeline including all I/O connectors. But sometimes, you just need to obtain access to a critical bug fix or an improvement available in the latest version of one or more I/O connectors used in your pipeline. Managed I/O with Dataflow simplifies this by completely taking over the management of the Beam I/O connector version. With Managed I/O, Dataflow will make sure that I/O connectors used by pipelines are always up to date. Dataflow performs this by always upgrading I/O connectors to the latest vetted version during job submission and streaming update via replacement. For example, assume that you use a Beam pipeline that uses Beam 2.x.0 and assume that you use the Managed Apache Iceberg I/O source in your pipeline. Also, assume that the latest vetted version of the Iceberg I/O source supported by Dataflow is 2.y.0. During job submission, Dataflow will replace this specific connector with version 2.y.0 and will keep the rest of the Beam pipeline including any standard (non-managed) I/O connectors at version 2.x.0.\n\nAfter replacement, Dataflow optimizes the updated pipeline and executes it in GCE. To achieve isolation between connectors from different Beam versions, Dataflow deploys an additional Beam SDK container in GCE VMs. So in this case, Beam SDK containers from both versions 2.x.0 and 2.y.0 will be running in each GCE VM used by the Dataflow job. So with Managed I/O you can be assured that I/O connectors used in your pipeline are always up to date. This allows you to focus on improving the business logic of your pipeline without worrying about upgrading the Beam version to simply obtain I/O connector updates.\n\nSimplified IO API APIs differences across Beam I/O connectors vary greatly. This means that, whenever you try to use a new Beam I/O connector, you would have to learn an API specific to that connector. Some of the APIs can be quite large and non-intuitive. This can be due to: Support for various and in some cases redundant features offered by the underlying system. Maintaining backwards compatibility for legacy (or archaic) features or defaults. Support for customizing the I/O connector to support edge cases and implementation details that may only apply to few customers. Above points result in very large API surfaces for some connectors that are not intuitive for a new customer to use efficiently.\n\nManaged I/O offers standardized Java and Python APIs for supported I/O connectors. For example, with Beam Java SDK an I/O connector source can be instantiated in the following standardized form.\n\nManaged.read(SOURCE).withConfig(sourceConfig) Java Copied\n\nAn I/O connector sink can be instantiated in the following form.\n\nManaged.write(SINK).withConfig(sinkConfig) Java Copied\n\nHere SOURCE and SINK are keys specifically identifying the connector while sourceConfig and sinkConfig are maps of configurations used to instantiate the connector source or sink. The map of configurations may also be provided as YAML files available locally or in Google Cloud Storage. Please see the Managed I/O website for more complete examples for supported sources and sinks. Beam Python SDK offers a similarly simplified API. This means that various Beam I/O connectors with different APIs can be instantiated in a very standard way. For example,\n\n// Create a Java BigQuery I/O source Map<String, Object> bqReadConfig = ImmutableMap.of(\"query\", \"<query>\", ...); Managed.read(Managed.BIGQUERY).withConfig(bqReadConfig) // Create a Java Kafka I/O source. Map<String, Object> kafkaReadConfig = ImmutableMap.of(\"bootstrap_servers\", \"<server>\", \"topic\", \"<topic>\", ...); Managed.read(Managed.KAFKA).withConfig(kafkaReadConfig) // Create a Java Kafka I/O source but with a YAML based config available in Google Cloud Storage. String kafkaReadYAMLConfig = \"gs://path/to/config.yaml\" Managed.read(Managed.KAFKA).withConfigUrl(kafkaReadYAMLConfig) // Create a Python Iceberg I/O source. iceberg_config = {\"table\": \"<table>\", ...} managed.Read(managed.ICEBERG, config=iceberg_config) Java Copied\n\nAutomatically optimized for Dataflow Many Beam connectors offer a comprehensive API for configuring and optimizing the connector to suit a given pipeline and a given Beam runner. One downside of this is that if you specifically want to run on Dataflow, you may have to learn the specific configurations that best suit Dataflow and apply them when setting up your pipeline. Connector related documentation can be long and detailed and specific changes needed might not be intuitive. This might result in connectors used in Dataflow pipelines performing in a sub-optimal way. Manage I/O connectors alleviates this by automatically re-configuring the connectors to incorporate best practices and configure them to best suit Dataflow. Such re-configuration may occur during job submission or streaming update via replacement. For example, Dataflow streaming pipelines offer two modes, exactly-once and at-least-once while BigQuery I/O sink with Storage Write API offer two analogous delivery semantics, exactly-once and at-least-once. BigQuery sink with at-least-once delivery semantics is usually less expensive and results in lower latencies. With standard BigQuery I/O connectors, you are responsible for making sure that you use the appropriate mode when using the BigQuery I/O. With Managed BigQuery I/O sink this is automatically configured for you. Which means that if your streaming pipeline is operating at the at-least-once mode, your Managed I/O BigQuery sink will be automatically configured to use the at-least-once delivery semantics.\n\nReal-world pipelines We ran several pipelines that wrote data using the Managed Iceberg I/O sink backed by a Hadoop catalog deployed in GCS (please see here for the other supported catalogs). Pipelines were submitted using Beam 2.61.0 and the Managed I/O sink was automatically upgraded by Dataflow to the latest supported version. All benchmarks used n1-standard-4 VMs and the number of VMs used by the pipeline was fixed to 100. Please note that execution time here does not include the startup and shutdown time.\n\nAs the benchmarks show, Managed Iceberg I/O scaled up nicely and both metrics grew linearly with the data size. We also ran a streaming pipeline that read from Google Pub/Sub and used the Managed I/O Kafka sink to push messages to a Kafka cluster hosted in GCP. The pipeline used Beam 2.61.0 and Dataflow upgraded the Managed Kafka sink to the latest supported version. During the steady state, the pipeline used 10 n1-standard-4 VMs (max 20 VMs). The pipeline was consistently processing messages at a throughput of 250k msgs/sec across all steps and was run for 2 hours.\n\nThe following graph shows the data throughputs of various steps of the pipeline. Note that throughputs are different here since the element size changes between steps. The pipeline read from Pub/Sub at a rate of 75 MiB/sec (red line) and wrote to Kafka at a rate of 40 MiB/sec (green line).\n\nBoth latency and backlog was low for the duration of the pipeline execution.\n\nThe pipeline used VM CPU and memory efficiently.", "label": 0}
{"title": "Migrating millions of lines of code to TypeScript", "url": "https://stripe.com/blog/migrating-to-typescript", "content": "On Sunday March 6, we migrated Stripe\u2019s largest JavaScript codebase (powering the Stripe Dashboard) from Flow to TypeScript. In a single pull request, we converted more than 3.7 million lines of code. The next day, hundreds of engineers came in to start writing TypeScript for their projects.\n\nSeriously unreal. I remember a short time ago laughing at the idea of typescript ever landing at Stripe, and then I woke up Christmas Monday morning and it was here. Mike Fix, Engineer, Stripe\n\nTypeScript is the de facto standard for JavaScript type checking, and our engineers have been overjoyed by this migration. We\u2019re sharing our TypeScript conversion tool on GitHub to help others perform similar migrations.\n\nA brief history of JavaScript type checking at Stripe\n\nStripe has built large-scale frontend applications since 2012, including stripe.com, Stripe JS, and the Stripe Dashboard. As our company grew, we increased the quality and reliability of our products by type checking our JS code. In 2016, we were an early adopter of Flow, an optional type system for JavaScript developed at Meta (then Facebook). Since then, Flow has provided type safety for the majority of our frontend applications.\n\nExample of a generated Flow type for an API resource and associated endpoints.\n\nHowever, engineers had trouble working with Flow. The type checker\u2019s memory usage would lock up laptops, and the in-editor integration was frequently slow and unreliable. Meanwhile TypeScript, an alternative type system developed at Microsoft, exploded in popularity thanks to its tooling and robust community. TypeScript availability became a top request among engineers at Stripe.\n\nStripe\u2019s developer productivity team aims to provide our engineers with the most productive development environment of their careers, and delight in our tools is crucial for that. We work hard to identify the most pressing issues affecting developers; for example, we\u2019ve built integrations into all of our development tools for reporting friction, which is quickly routed to the responsible teams and prioritized. TypeScript support was one such pressing issue and teams supporting frontend engineers began to plan out supporting TypeScript across the company.\n\nChoosing the right migration strategy\n\nOur largest frontend codebase powers the Stripe Dashboard and other user-facing products. The Dashboard codebase has tight coupling between disparate components and no cleanly factored dependency graph. An incremental migration to TypeScript would force developers to work in both languages to accomplish common tasks. We would also need an interoperability layer to sync type definitions between both languages and keep them consistent throughout the development process.\n\nIn late 2020, we formed a new horizontal JavaScript Infrastructure team: a group of engineers solely focused on elevating the experience of writing JS at Stripe. One of the team\u2019s first challenges was to replace Flow with TypeScript without a long and uncertain migration.\n\nWe began by speaking to companies who had run similar migrations and read articles from Airtable and Zapier describing their experiences. These companies developed automated scripts to convert one language to another, ran them over their entire codebases, and merged the output as a single commit. Airtable had published their conversion script to GitHub as a source-to-source conversion tool, or \u201ccodemod,\u201d that would parse Flow code and generate TypeScript.\n\nMigrating in this way would greatly reduce the cognitive overhead for engineers, who would not need to handle both type systems for the same product behavior. We could have a clean break between Flow and TypeScript.\n\nPlanning, preparation, and iteration\n\nWe were really impressed by the quality of Airtable\u2019s conversion code and decided to use that as the basis for our migration efforts. Many thanks to the team at Airtable for building this out and sharing their work\u2014the open source community benefits a ton from examples like this.\n\nWe began by copying Airtable\u2019s codemod to Stripe\u2019s monorepo to run against our internal code. Our JavaScript projects make heavy use of Sail, a shared design system of strictly typed React components, so that was our initial area of focus. We generated TypeScript definitions for Sail, rather than converting the code to TypeScript, as it would continue supporting applications written in Flow. To safely support both type systems, we wrote tests to verify the TypeScript definitions against any changes to the underlying Flow code. This approach would be too cumbersome for a large codebase, but thankfully the Sail component interface is explicit and quite rigid.\n\nThe core of the codemod was solid but not comprehensive: for many files, it would crash or generate imperfect output. Over several months we iterated to handle more syntactic and semantic edge cases.\n\nFor one simple example, JavaScript arrow functions can return a single expression without a return statement, such as the following:\n\nconst linesOfCode = () => 7;\n\nJavaScript object literals use braces to wrap property definitions. Because braces are also used to delineate blocks of statements, returning an object literal from an arrow function requires an additional set of parentheses to disambiguate:\n\nconst currencyMap = () => ({ca:'CAD', us:'USD'});\n\nWe noticed that the codemod was incorrectly stripping the extra parentheses from these arrow functions, but only in the case of a generic function (a function that takes a type argument), which is syntax not available in standard JavaScript:\n\n\u200b\u200b// bad!\n\nconst wrapper = <T>(arg: T) => {wrapped: arg};\n\nWe were able to fix this issue and add tests to prevent further regressions: There were dozens of similar syntactic fixes we made to handle the breadth of our codebases.\n\nOnce Sail was usable from TypeScript, we worked on a couple of internal applications containing hundreds of JS modules. We also added a second pass to the codemod to suppress errors in the generated code, using TypeScript\u2019s @ts-expect-error comment to tag these errors. Rather than resolving every error ahead of time, we focused on eliminating Flow as soon as possible, tracking TypeScript error suppressions to address after the conversion.\n\nAn initial pass on the Dashboard codebase created over 97,000 error suppressions. With our iterative approach to updating the codemod, we were able to get that number down to 37,000, or about one per thousand lines of code. For comparison, the Flow code had about 5,000 error suppressions. Both Flow and TypeScript support measuring type coverage, and we were pleasantly surprised that TypeScript reported higher coverage than Flow even with these suppressions. We attribute that to an increase in the number and quality of third-party type definitions available in TypeScript, the lack of which was a large contributor to poor type coverage in Flow.\n\nAs we moved onto the Dashboard with its tens of thousands of modules, our approach created significant memory pressure on the TypeScript compiler. Our primary tool to address this was TypeScript project references: Although the Dashboard is not structured as distinct modules, we could infer a module structure and create project references based on that. This approach gave us the headroom to run TypeScript over the codebase without refactoring large chunks of application code.\n\nGoing live\n\nHundreds of engineers contribute to the Dashboard each week. Such a sweeping change would be exceptionally challenging to merge on a normal working day. Our team decided to commit to a date\u2014March 6, a Sunday\u2014where we would lock the Stripe monorepo and land our branch.\n\nIn the week before merging, we focused on passing a build through our CI system and deploying it to our QA environment. Although TypeScript could successfully check the project, other tools that process our source code (ESLint, Jest, Webpack, Metro) would also need updates.\n\nOne particular pain point was Jest snapshot testing: Jest generates snapshot files with a hardcoded reference to the test file that generated them. Since the codemod would generate either .ts or .tsx extensions for TypeScript files, the snapshot files would have invalid references back to their test sources. We simplified this by switching the generation to only use .tsx . This meant we could rewrite the snapshots in bulk and keep 100% of those tests passing.\n\nIn some cases we recognized that fixing the code for TypeScript compatibility would add weeks to our schedule. One set of cases was our custom ESLint rules: We had a rule to reorder imports to enforce consistency between files, but the rule was written against Babel\u2019s Flow parser, which generated a subtly different abstract syntax tree from the TypeScript parser. In cases like this, we opted to disable some checks and do the work to restore them after the conversion.\n\nWith a passing build in hand, we reached out to product teams with user-facing functionality in the Dashboard. Although the Dashboard has extensive unit and functional testing, it has limited end-to-end test coverage. This made manual tests by product stakeholders crucial. Those tests highlighted some minor bugs, which we resolved during the final week: In one case, we were failing to load any translations for non-English Dashboard users, due to a hardcoded .js extension in the translation loading code.\n\nThis process gave us high confidence, but there is always uncertainty with a change this large: Although we had a firm grasp on our developer tooling and build processes, we were mutating every file in the codebase. Subtle errors in our conversion scripts (for example, removing an empty field from an object shared between multiple components) could cause user-facing errors, without being covered by any of our existing automated tests. These failures could manifest in a number of ways, from downstream dev tooling issues to builds that fail. We leaned on our deploy automation and ambient monitoring to make us aware of any unexpected problems, and created a Slack channel to coordinate the rollout so user-facing teams could quickly escalate any reports they would receive.\n\nOn Saturday, March 5 the team generated a new migration branch and ran our automated scripts. We then deployed that branch to QA and repeated our validation process, including the manual tests suggested by product teams. We found no new issues. We were ready for the day of the merge.\n\nEarly on the morning of Sunday, March 6, we locked the Stripe monorepo, took one more QA pass over our migration branch, and submitted the change. It merged cleanly and our automated tests passed. We kicked off the deployment to ship TypeScript into production.\n\nThanks to the care and rigor of the previous year of work, we had no unpleasant surprises as we shifted traffic to the new code. We unlocked the repository and let developers know that the Dashboard was now in TypeScript.\n\nWhen I was interviewing, I heard the migration from Flow to TypeScript was underway. I was admittedly skeptical, seeing prior teams struggle with the complexity and effort of even small codebases. The fact that I was back to normal in a few minutes [on] Monday was humbling. Eric Clemmons, Software Engineer, Stripe\n\nThe immediate response was overwhelming. Engineers were impressed by the completeness of the migration: one described it as the single biggest developer productivity boost in their time at Stripe. We were happy to have the year of work pay off with such a clear and dramatic improvement to Stripe\u2019s codebase.\n\nTypeScript\u2026 two months later\n\nThe conversion was not perfect. Over the subsequent weeks our JS Infra team addressed issues as they arose. One example we didn\u2019t anticipate was engineers reporting inconsistency between CI and local TypeScript runs. In TypeScript we are able to use many third-party type definitions installed from npm, and if those are updated, engineers will need to install the new versions. This was different from our Flow configuration, where dependency updates rarely changed types, so we had to educate engineers to try running yarn install as a debugging step.\n\nThere is still more work to be done: We know performance could improve further with more granular project references, and better caching could speed up our CI runs. However, the benefits have far outweighed the bumps along the road. Engineers enjoy features such as automatic dependency imports and code completion, as well as the TypeScript community\u2019s extensive corpus of third-party type definitions and integrations. When new engineers join Stripe to write frontend code, from day one they can be successful in a language with which they\u2019re more likely to be comfortable and familiar.\n\nWith the work on the Dashboard complete, the JS Infra team has continued to increase TypeScript\u2019s adoption across the company. We\u2019ve used the same tools to convert many other codebases, including all of our Payments UIs, such as Stripe Checkout. Stripe frontend engineers will soon write TypeScript for whichever project they develop.\n\nWhen we first shared the story of our migration publicly, the response was equally enthusiastic. Developers from across the industry reached out to learn more and apply the same improvements to their own codebases. To support these developers, we\u2019re sharing our TypeScript conversion code on GitHub for teams to adapt to their own projects.\n\nAside from the particulars of JavaScript or Flow or TypeScript, our big lesson from this migration is that dramatic improvements to large codebases are possible with diligence, commitment, and optimism. We will apply that mindset to other opportunities to make our engineers more effective and hope others do the same.", "label": 0}
{"title": "Using KerasHub for easy end-to-end machine learning workflows with Hugging Face", "url": "https://developers.googleblog.com/en/load-model-weights-from-safetensors-into-kerashub-multi-framework-machine-learning/", "content": "How to load SafeTensors checkpoints across different frameworks\n\nAs the AI ecosystem continues to evolve, there are more and more ways to define machine learning models, and even more ways to save the model weights that result from training and fine-tuning. In this growing set of choices, KerasHub allows you to mix and match popular model architectures and their weights across different ML frameworks. For example, a popular place to load checkpoints from is the Hugging Face Hub. Many of those model checkpoints were created with the Hugging Face transformers library in the SafeTensors format. Regardless of what ML framework was used to create the model checkpoint, those weights can be loaded into a KerasHub model, which allows you to use your choice of framework (JAX, PyTorch, or TensorFlow) to run the model. Yes, that means you can run a checkpoint from Mistral or Llama on JAX, or even load Gemma with PyTorch \u2013 it doesn't get any more flexible than that. Let's take a look at some of these terms in more detail, and talk about how this works in practice.\n\nModel architecture vs. model weights When loading models, there are two distinct parts that we need: the model architecture and the model weights (often called \"checkpoints\"). Let's define each of these in more detail. When we say \"model architecture\", we are referring to how the layers of the model are arranged, and the operations that happen within them. Another way to describe this might be to call it the \"structure\" of the model. We use Python frameworks like PyTorch, JAX, or Keras to express model architectures. When we talk about \"model weights\", we are referring to the \"parameters\" of a model, or numbers in a model that are changed over the course of training. The particular values of these weights are what give a trained model its characteristics. \"Checkpoints\" are a snapshot of the values of the model weights at a particular point in the training. The typical checkpoint files that are shared and widely used are the ones where the model has reached a particularly good training outcome. As the same model architecture is further refined with fine-tuning and other techniques, additional new checkpoint files are created. For example, many developers have taken Google's gemma-2-2b-it model and fine-tuned it with their own datasets, and you can see over 600 examples. All of these fine-tuned models use the same architecture as the original gemma-2-2b-it model, but their checkpoints have differing weights. So there we have it: the model architecture is described with code, while model weights are trained parameters, saved as checkpoint files. When we have a model architecture together with a set of model weights (in the form of a checkpoint file), we create a functioning model that produces useful outputs.\n\nSorry, your browser doesn't support playback for this video Different model weights can be loaded into the same model architecture. These different sets of weights are saved as checkpoints.\n\nTools like Hugging Face's transformers library and Google's KerasHub library provide model architectures and the APIs you need to experiment with them. Examples of checkpoint repositories include Hugging Face Hub and Kaggle Models. You can mix and match model architecture libraries with your choice of checkpoint repositories. For example, you can load a checkpoint from Hugging Face Hub into a JAX model architecture and fine-tune it with KerasHub. For a different task, you might find a checkpoint on Kaggle Models that's suitable for your needs. This flexibility and separation means you are not boxed into one ecosystem.\n\nWhat is KerasHub? So we\u2019ve mentioned KerasHub a few times\u2013 let\u2019s go into it in more detail. KerasHub is a Python library that helps make defining model architectures easier. It contains many of the most popular and commonly used machine learning models today, and more are being added all the time. Because it's based on Keras, KerasHub supports all three major Python machine learning libraries used today: PyTorch, JAX, and TensorFlow. This means you can have model architectures defined in whichever library you'd like. Furthermore, since KerasHub supports the most common checkpoint formats, you can easily load checkpoints from many checkpoint repositories. For example, you can find hundreds of thousands of checkpoints on Hugging Face and Kaggle to load into these model architectures.\n\nComparisons to the Hugging Face transformers library A common workflow by developers is to use the Hugging Face transformers library to fine-tune a model and upload it to the Hugging Face Hub. And if you\u2019re a user of transformers , you\u2019ll also find many familiar API patterns in KerasHub. Check out the KerasHub API documentation to learn more. An interesting aspect of KerasHub is that many of the checkpoints found on Hugging Face Hub are compatible with not only the transformers library, but also KerasHub. Let's take a look at how that works.\n\nKerasHub is compatible with Hugging Face Hub Hugging Face has a model checkpoint repository, called Hugging Face Hub. It's one of the many places where the machine learning community uploads their model checkpoints to share with the world. Especially popular on Hugging Face is the SafeTensors format, which is compatible with KerasHub. You can load these checkpoints from Hugging Face Hub directly into your KerasHub model, as long as the model architecture is available. Wondering if your favorite model is available? You can check https://keras.io/keras_hub/presets/ for a list of supported model architectures. And don't forget, all the community created fine-tuned checkpoints of these model architectures are also compatible! We recently created a new guide to help explain the process in more detail. How does this all work? KerasHub has built-in converters that simplify the use of Hugging Face transformers models. These converters automatically handle the process of translating Hugging Face model checkpoints into a format that's compatible with the KerasHub. This means you can seamlessly load a wide variety of pretrained Hugging Face transformer models from the Hugging Face Hub directly into KerasHub with just a few lines of code. If you notice a missing model architecture, you can add it by filing a pull request on GitHub.\n\nHow to load a Hugging Face Hub checkpoint into KerasHub So how do we get checkpoints from Hugging Face Hub loaded into KerasHub? Let's take a look at some concrete examples. We'll start by first choosing our machine learning library as our Keras \"backend\". We'll use JAX in the examples shown, but you can choose between JAX, PyTorch, or TensorFlow for any of them. All the examples below work regardless of which one you choose. Then we can proceed by importing keras , keras_hub , and huggingface_hub , and then login with our Hugging Face User Access token so we can access the model checkpoints.\n\nimport os os.environ[\"KERAS_BACKEND\"] = \"jax\" # or \"torch\" or \"tensorflow\" import keras from keras_hub import models from huggingface_hub import login login('HUGGINGFACE_TOKEN') Python Copied\n\nPut a Mistral model on JAX First up, perhaps we want to run a checkpoint from Mistral on JAX? Over on KerasHub, there are a handful of Mistral models available on KerasHub's list of available model architectures, let's try out mistral_0.2_instruct_7b_en . Clicking into it, we see that we should use the MistralCausalLM class to call from_preset . On the Hugging Face Hub side of things, we see that the corresponding model checkpoint is stored here, with over 900 fine-tuned versions. Browsing that list, there's a popular cybersecurity-focused fine-tuned model called Lily, with the pathname of segolilylabs/Lily-Cybersecurity-7B-v0.2 . We'll also need to add \" hf:// \" before that path to specify that KerasHub should look at Hugging Face Hub.\n\nSorry, your browser doesn't support playback for this video\n\nPutting it all together, we get the following code:\n\n# Model checkpoint from Hugging Face Hub gemma_lm = models.MistralCausalLM.from_preset(\"hf://segolilylabs/Lily-Cybersecurity-7B-v0.2\") gemma_lm.generate(\"Lily, how do evil twin wireless attacks work?\", max_length=30) Python Copied\n\nRunning Llama 3.1 on JAX Llama 3.1-8B-Instruct is a popular model, with over 5 million downloads last month. Let's put a fine-tuned version on JAX. With over 1400 fine-tuned checkpoints, there's no lack of choice. The xVerify fine-tuned checkpoint looks interesting, let's load that into JAX on KerasHub. We'll use the Llama3CausalLM class to reflect the model architecture that we are using. As before, we'll need the appropriate path from Hugging Face Hub, prefixed with \" hf:// \". It's pretty amazing that we can load and call a model with just two lines of code, right?\n\n# Model checkpoint from Hugging Face Hub gemma_lm = models.Llama3CausalLM.from_preset(\"hf://IAAR-Shanghai/xVerify-8B-I\") gemma_lm.generate(\"What is the tallest building in NYC?\", max_length=100) Python Copied\n\nLoad Gemma on JAX Finally, let's load a fine-tuned Gemma-3-4b-it checkpoint into JAX. We'll use the Gemma3CausalLM class, and select one of the fine-tuned checkpoints. How about EraX, a multilingual translator? As before, we'll use the pathname with the Hugging Face Hub prefix to create the full path of \" hf://erax-ai/EraX-Translator-V1.0 \".\n\n# Model checkpoint from Hugging Face Hub gemma_lm = models.Gemma3CausalLM.from_preset(\"hf://erax-ai/EraX-Translator-V1.0\") gemma_lm.generate(\"Translate to German: \", max_length=30) Python Copied", "label": 0}
{"title": "Google IT Support Certified", "url": "https://lifeofpablo.com/blog/google-it-support-certified", "content": "Google IT Support Certified\n\nGoogle IT Support\n\nThis post was written in English (en_US).\n\nToday, after many months I finally received a Google IT Support Certificate. I am very happy. I learned so much from this course and revisited topics I knew already. I definitely picked up some new skills for my current role. The certificate had five courses ranging from basic networking to IT Security. I would say this was a well rounded course. It really put in perspective various aspects of an IT Support Professional. It was also a wake-up call! I have so much to learn still!!!\n\nBeing in this role, you could be a generalist or a specialist. This depends on the role one gets. In my role and working with a small team of IT professionals, I find myself being a generalist but also specialized in specific things.\n\nI find it important is to keep learning in my career. I plan on taking more courses to get more certificates to gain more skills. I started a Google Cyber Security Course and a mathematics course. I'm going to try to balance three to five courses. I want to get in the routine of going back to school. I find this funny because I work at a school. By having this routine, I would like to start my Master's program this year.\n\nI'm really excited for what's to come this year! It'll definitely be a year of learning.\n\nCourse Certificates Completed\n\nSystem Administration and IT Infrastructure Services\n\nThe Bits and Bytes of Computer Networking\n\nIT Security: Defense against the digital dark arts\n\nTechnical Support Fundamentals\n\nOperating Systems and You: Becoming a Power User\n\nSkills I learned or reviewed", "label": 1}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2023-01", "content": "en\n\nWe're 9 days into the New Year of 2022 2023.\n\nThese last few years, I haven't been in the right mindset. I've been in a dark place for many, many years. This dark place has hindered me in so many aspects of my life. I don't know who I am anymore. It's been hard to love myself for as long as I remember.\n\nThe keyword: Avoiding.\n\nI hope my friends and family understand. I know I have hurt you in some form. I am really sorry.\n\nThe past can't be changed. All I can hope is to learn from it and not be stuck in it.\n\nThis year I am going to push through to improve myself. The things I want to do are not a deterrent to the problems at hand. I would like them to help guide me into finding clarity. To help point me in the direction of the light. I am finally going to dedicate myself to formally learn a few programming languages and actually understand that missing disconnect/mental block.\n\nI'm going to start with Rust. A language I've been very interested in for many years. A language I've pick up for a while and would just stop out of no where. I hope to finally get some certifications in Google, Amazon Web Services, Artificial intelligence, front-end/back-end development. On top of all this, I'd like to start my Master's in Information Technology in Education.\n\nI know I am capable of great things. I just hope I can acknowledge this.\n\nI promised to build something. It'll happen.\n\nThe bigger issues need to be solved through other channels. I also acknowledge this.\n\nI guess I should try to work on my body too.\n\nThat is all I got.\n\n-Pablo", "label": 1}
{"title": "Meta\u2019s Full-stack HHVM optimizations for GenAI", "url": "https://engineering.fb.com/2025/05/20/web/metas-full-stack-hhvm-optimizations-for-genai/", "content": "As Meta has launched new, innovative products leveraging generative AI (GenAI), we need to make sure the underlying infrastructure components evolve along with it. Applying infrastructure knowledge and optimizations have allowed us to adapt to changing product requirements, delivering a better product along the way. Ultimately, our infrastructure systems need to balance our need to ship high-quality experiences with a need to run systems sustainability.\n\nSplitting GenAI inference traffic out into a dedicated WWW tenant, which allows specialized runtime and warm-up configuration, has enabled us to meet both of those goals while delivering a 30% improvement in latency.\n\nWho we are\n\nAs the Web Foundation team, we operate Meta\u2019s monolithic web tier, running Hack. The team is composed of cross-functional engineers who make sure the infrastructure behind the web tier is healthy and well designed. We jump into incident response, work on some of the most complex areas of the infrastructure, and help build whatever we need to keep the site happily up and running.\n\nTo accomplish this, we have established a series of best practices on being a \u201cgood citizen\u201d of the shared tier. We need to ensure that all requests comply with these guidelines to prevent issues from spilling over and affecting other teams\u2019 products. One core rule is the request runtime\u2014limiting a request to 30 seconds of execution. This is a consequence of the HHVM (HipHop Virtual Machine) runtime\u2014each request has a corresponding worker thread, of which there is a finite number. To ensure there are always threads available to serve incoming requests, we need to balance the resources available on each host with its expected throughput. If requests are taking too long, there will be fewer available threads to process new requests, leading to user-visible unavailability.\n\nThe changing landscape\n\nClassically, webservers at Meta are optimized for serving front-end requests\u2014rendering webpages and serving GraphQL queries. These requests\u2019 latency is typically measured in hundreds of milliseconds to seconds (substantially below the 30-second limit), which enables hosts to process approximately 500 queries per second.\n\nAdditionally, a web server will spend about two-thirds of its time doing input/output (I/O), and the remaining third doing CPU work. This fact has influenced the design of the Hack language, which supports asyncio, a type of cooperative multi-tasking, and all the core libraries support these primitives to increase performance and decrease the amount of time the CPU is sitting idle, waiting for I/O.\n\nGenAI products, especially LLMs, have a different set of requirements. These are driven by the core inference flow: The model responds with a stream of tokens that can take seconds or minutes to complete. A user may see this as a chatbot \u201ctyping\u201d a response. This isn\u2019t an effect to make our products seem friendlier; it\u2019s the speed at which our models think! After a user submits a query to the model, we need to start streaming these responses back to the user as fast as possible. On top of that, the total latency of the request is now substantially longer (measured in seconds). These properties have two effects on the infrastructure\u2014minimal overhead on the critical path before calling the LLM, and a long duration for the rest of the request, most of which is spent waiting on I/O. (See Figures 1 and 2 below).\n\nA series of optimizations\n\nThis shift in requirements allowed Web Foundation to reexamine the rules of running the monolithic web tier. We then launched a dedicated web tenant (a standalone deployment of WWW) that allowed custom configuration, which we could better tune to the needs of the workload.\n\nRequest timeout\n\nFirst, running on an isolated web tier allowed us to increase the runtime limit for GenAI requests. This is a straightforward change, but it allowed us to isolate the longer-running traffic to avoid adverse impacts on the rest of the production tier. This way, we can avoid requests timing out if inference takes longer than 30 seconds.\n\nThread-pool sizing\n\nRunning requests for longer means there is reduced availability of worker threads (which, remember, map 1:1 with processed requests). Since webservers have a finite amount of memory, we can divide the total memory available by the per-request memory limit to get a peak number of active requests; this in turn tells us how many requests we can execute simultaneously. We ended up running with approximately 1000 threads on GenAI hosts, as compared to a couple of hundred on normal webservers.\n\nJIT cache and \u201cjumpstart\u201d\n\nHHVM is a just-in-time (JIT) interpreted language, which means the first time a given function executes, the machine needs to compile it to lower-level machine code for execution. Additionally, a technique called Jump-Start allows a webserver to seed its JIT cache with outputs from a previously warmed server. By allowing GenAI hosts to use Jump-Start profiles from the main web tier, we are able to greatly speed up execution, even if the code overlap is not identical.\n\nRequest warm-up\n\nHHVM also supports the execution of dummy requests at server startup, which we can execute, and then we can discard the results. The intent here is to warm non-code caches within the webserver. Configuration values and service discovery info are normally fetched inline the first time they are needed and then cached within the webserver. By fetching and caching this information in warm-up requests, we prevent our users from observing the latency of these initial fetches.\n\nShadow traffic\n\nFinally, Meta heavily uses real-time configuration to control feature rollouts, which means that jumpstart profiles consumed at startup time might not cover all future code paths the server will execute. To maintain coverage in the steady state, we also added request shadowing, so we can ensure that gating changes are still covered in the JIT cache.", "label": 0}
{"title": "Foundation Model for Personalized Recommendation", "url": "https://netflixtechblog.com/foundation-model-for-personalized-recommendation-1a0bd8e02d39?source=collection_home---4------1-----------------------", "content": "Foundation Model for Personalized Recommendation Netflix Technology Blog 11 min read \u00b7 Mar 21, 2025 -- 36 Listen Share\n\nBy Ko-Jen Hsiao, Yesu Feng and Sudarshan Lamkhede\n\nMotivation\n\nNetflix\u2019s personalized recommender system is a complex system, boasting a variety of specialized machine learned models each catering to distinct needs including \u201cContinue Watching\u201d and \u201cToday\u2019s Top Picks for You.\u201d (Refer to our recent overview for more details). However, as we expanded our set of personalization algorithms to meet increasing business needs, maintenance of the recommender system became quite costly. Furthermore, it was difficult to transfer innovations from one model to another, given that most are independently trained despite using common data sources. This scenario underscored the need for a new recommender system architecture where member preference learning is centralized, enhancing accessibility and utility across different models.\n\nParticularly, these models predominantly extract features from members\u2019 recent interaction histories on the platform. Yet, many are confined to a brief temporal window due to constraints in serving latency or training costs. This limitation has inspired us to develop a foundation model for recommendation. This model aims to assimilate information both from members\u2019 comprehensive interaction histories and our content at a very large scale. It facilitates the distribution of these learnings to other models, either through shared model weights for fine tuning or directly through embeddings.\n\nThe impetus for constructing a foundational recommendation model is based on the paradigm shift in natural language processing (NLP) to large language models (LLMs). In NLP, the trend is moving away from numerous small, specialized models towards a single, large language model that can perform a variety of tasks either directly or with minimal fine-tuning. Key insights from this shift include:\n\nA Data-Centric Approach: Shifting focus from model-centric strategies, which heavily rely on feature engineering, to a data-centric one. This approach prioritizes the accumulation of large-scale, high-quality data and, where feasible, aims for end-to-end learning. Leveraging Semi-Supervised Learning: The next-token prediction objective in LLMs has proven remarkably effective. It enables large-scale semi-supervised learning using unlabeled data while also equipping the model with a surprisingly deep understanding of world knowledge.\n\nThese insights have shaped the design of our foundation model, enabling a transition from maintaining numerous small, specialized models to building a scalable, efficient system. By scaling up semi-supervised training data and model parameters, we aim to develop a model that not only meets current needs but also adapts dynamically to evolving demands, ensuring sustainable innovation and resource efficiency.\n\nData\n\nAt Netflix, user engagement spans a wide spectrum, from casual browsing to committed movie watching. With over 300 million users at the end of 2024, this translates into hundreds of billions of interactions \u2014 an immense dataset comparable in scale to the token volume of large language models (LLMs). However, as in LLMs, the quality of data often outweighs its sheer volume. To harness this data effectively, we employ a process of interaction tokenization, ensuring meaningful events are identified and redundancies are minimized.\n\nTokenizing User Interactions: Not all raw user actions contribute equally to understanding preferences. Tokenization helps define what constitutes a meaningful \u201ctoken\u201d in a sequence. Drawing an analogy to Byte Pair Encoding (BPE) in NLP, we can think of tokenization as merging adjacent actions to form new, higher-level tokens. However, unlike language tokenization, creating these new tokens requires careful consideration of what information to retain. For instance, the total watch duration might need to be summed or engagement types aggregated to preserve critical details.\n\nFigure 1.Tokenization of user interaction history by merging actions on the same title, preserving important information.\n\nThis tradeoff between granular data and sequence compression is akin to the balance in LLMs between vocabulary size and context window. In our case, the goal is to balance the length of interaction history against the level of detail retained in individual tokens. Overly lossy tokenization risks losing valuable signals, while too granular a sequence can exceed practical limits on processing time and memory.\n\nEven with such strategies, interaction histories from active users can span thousands of events, exceeding the capacity of transformer models with standard self attention layers. In recommendation systems, context windows during inference are often limited to hundreds of events \u2014 not due to model capability but because these services typically require millisecond-level latency. This constraint is more stringent than what is typical in LLM applications, where longer inference times (seconds) are more tolerable.\n\nTo address this during training, we implement two key solutions:\n\nSparse Attention Mechanisms: By leveraging sparse attention techniques such as low-rank compression, the model can extend its context window to several hundred events while maintaining computational efficiency. This enables it to process more extensive interaction histories and derive richer insights into long-term preferences. Sliding Window Sampling: During training, we sample overlapping windows of interactions from the full sequence. This ensures the model is exposed to different segments of the user\u2019s history over multiple epochs, allowing it to learn from the entire sequence without requiring an impractically large context window.\n\nAt inference time, when multi-step decoding is needed, we can deploy KV caching to efficiently reuse past computations and maintain low latency.\n\nThese approaches collectively allow us to balance the need for detailed, long-term interaction modeling with the practical constraints of model training and inference, enhancing both the precision and scalability of our recommendation system.\n\nInformation in Each \u2018Token\u2019: While the first part of our tokenization process focuses on structuring sequences of interactions, the next critical step is defining the rich information contained within each token. Unlike LLMs, which typically rely on a single embedding space to represent input tokens, our interaction events are packed with heterogeneous details. These include attributes of the action itself (such as locale, time, duration, and device type) as well as information about the content (such as item ID and metadata like genre and release country). Most of these features, especially categorical ones, are directly embedded within the model, embracing an end-to-end learning approach. However, certain features require special attention. For example, timestamps need additional processing to capture both absolute and relative notions of time, with absolute time being particularly important for understanding time-sensitive behaviors.\n\nTo enhance prediction accuracy in sequential recommendation systems, we organize token features into two categories:\n\nRequest-Time Features: These are features available at the moment of prediction, such as log-in time, device, or location. Post-Action Features: These are details available after an interaction has occurred, such as the specific show interacted with or the duration of the interaction.\n\nTo predict the next interaction, we combine request-time features from the current step with post-action features from the previous step. This blending of contextual and historical information ensures each token in the sequence carries a comprehensive representation, capturing both the immediate context and user behavior patterns over time.\n\nConsiderations for Model Objective and Architecture\n\nAs previously mentioned, our default approach employs the autoregressive next-token prediction objective, similar to GPT. This strategy effectively leverages the vast scale of unlabeled user interaction data. The adoption of this objective in recommendation systems has shown multiple successes [1\u20133]. However, given the distinct differences between language tasks and recommendation tasks, we have made several critical modifications to the objective.\n\nFirstly, during the pretraining phase of typical LLMs, such as GPT, every target token is generally treated with equal weight. In contrast, in our model, not all user interactions are of equal importance. For instance, a 5-minute trailer play should not carry the same weight as a 2-hour full movie watch. A greater challenge arises when trying to align long-term user satisfaction with specific interactions and recommendations. To address this, we can adopt a multi-token prediction objective during training, where the model predicts the next n tokens at each step instead of a single token[4]. This approach encourages the model to capture longer-term dependencies and avoid myopic predictions focused solely on immediate next events.\n\nSecondly, we can use multiple fields in our input data as auxiliary prediction objectives in addition to predicting the next item ID, which remains the primary target. For example, we can derive genres from the items in the original sequence and use this genre sequence as an auxiliary target. This approach serves several purposes: it acts as a regularizer to reduce overfitting on noisy item ID predictions, provides additional insights into user intentions or long-term genre preferences, and, when structured hierarchically, can improve the accuracy of predicting the target item ID. By first predicting auxiliary targets, such as genre or original language, the model effectively narrows down the candidate list, simplifying subsequent item ID prediction.\n\nUnique Challenges for Recommendation FM\n\nIn addition to the infrastructure challenges posed by training bigger models with substantial amounts of user interaction data that are common when trying to build foundation models, there are several unique hurdles specific to recommendations to make them viable. One of unique challenges is entity cold-starting.\n\nAt Netflix, our mission is to entertain the world. New titles are added to the catalog frequently. Therefore the recommendation foundation models require a cold start capability, which means the models need to estimate members\u2019 preferences for newly launched titles before anyone has engaged with them. To enable this, our foundation model training framework is built with the following two capabilities: Incremental training and being able to do inference with unseen entities.\n\nIncremental training : Foundation models are trained on extensive datasets, including every member\u2019s history of plays and actions, making frequent retraining impractical. However, our catalog and member preferences continually evolve. Unlike large language models, which can be incrementally trained with stable token vocabularies, our recommendation models require new embeddings for new titles, necessitating expanded embedding layers and output components. To address this, we warm-start new models by reusing parameters from previous models and initializing new parameters for new titles. For example, new title embeddings can be initialized by adding slight random noise to existing average embeddings or by using a weighted combination of similar titles\u2019 embeddings based on metadata. This approach allows new titles to start with relevant embeddings, facilitating faster fine-tuning. In practice, the initialization method becomes less critical when more member interaction data is used for fine-tuning. Dealing with unseen entities : Even with incremental training, it\u2019s not always guaranteed to learn efficiently on new entities (ex: newly launched titles). It\u2019s also possible that there will be some new entities that are not included/seen in the training data even if we fine-tune foundation models on a frequent basis. Therefore, it\u2019s also important to let foundation models use metadata information of entities and inputs, not just member interaction data. Thus, our foundation model combines both learnable item id embeddings and learnable embeddings from metadata. The following diagram demonstrates this idea.\n\nFigure 2. Titles are associated with various metadata, such as genres, storylines, and tones. Each type of metadata could be represented by averaging its respective embeddings, which are then concatenated to form the overall metadata-based embedding for the title.\n\nTo create the final title embedding, we combine this metadata-based embedding with a fully-learnable ID-based embedding using a mixing layer. Instead of simply summing these embeddings, we use an attention mechanism based on the \u201cage\u201d of the entity. This approach allows new titles with limited interaction data to rely more on metadata, while established titles can depend more on ID-based embeddings. Since titles with similar metadata can have different user engagement, their embeddings should reflect these differences. Introducing some randomness during training encourages the model to learn from metadata rather than relying solely on ID embeddings. This method ensures that newly-launched or pre-launch titles have reasonable embeddings even with no user interaction data.\n\nDownstream Applications and Challenges\n\nOur recommendation foundation model is designed to understand long-term member preferences and can be utilized in various ways by downstream applications:\n\nDirect Use as a Predictive Model The model is primarily trained to predict the next entity a user will interact with. It includes multiple predictor heads for different tasks, such as forecasting member preferences for various genres. These can be directly applied to meet diverse business needs.. Utilizing embeddings The model generates valuable embeddings for members and entities like videos, games, and genres. These embeddings are calculated in batch jobs and stored for use in both offline and online applications. They can serve as features in other models or be used for candidate generation, such as retrieving appealing titles for a user. High-quality title embeddings also support title-to-title recommendations. However, one important consideration is that the embedding space has arbitrary, uninterpretable dimensions and is incompatible across different model training runs. This poses challenges for downstream consumers, who must adapt to each retraining and redeployment, risking bugs due to invalidated assumptions about the embedding structure. To address this, we apply an orthogonal low-rank transformation to stabilize the user/item embedding space, ensuring consistent meaning of embedding dimensions, even as the base foundation model is retrained and redeployed. Fine-Tuning with Specific Data The model\u2019s adaptability allows for fine-tuning with application-specific data. Users can integrate the full model or subgraphs into their own models, fine-tuning them with less data and computational power. This approach achieves performance comparable to previous models, despite the initial foundation model requiring significant resources.\n\nScaling Foundation Models for Netflix Recommendations\n\nIn scaling up our foundation model for Netflix recommendations, we draw inspiration from the success of large language models (LLMs). Just as LLMs have demonstrated the power of scaling in improving performance, we find that scaling is crucial for enhancing generative recommendation tasks. Successful scaling demands robust evaluation, efficient training algorithms, and substantial computing resources. Evaluation must effectively differentiate model performance and identify areas for improvement. Scaling involves data, model, and context scaling, incorporating user engagement, external reviews, multimedia assets, and high-quality embeddings. Our experiments confirm that the scaling law also applies to our foundation model, with consistent improvements observed as we increase data and model size.\n\nFigure 3. The relationship between model parameter size and relative performance improvement. The plot demonstrates the scaling law in recommendation modeling, showing a trend of increased performance with larger model sizes. The x-axis is logarithmically scaled to highlight growth across different magnitudes.\n\nConclusion\n\nIn conclusion, our Foundation Model for Personalized Recommendation represents a significant step towards creating a unified, data-centric system that leverages large-scale data to increase the quality of recommendations for our members. This approach borrows insights from Large Language Models (LLMs), particularly the principles of semi-supervised learning and end-to-end training, aiming to harness the vast scale of unlabeled user interaction data. Addressing unique challenges, like cold start and presentation bias, the model also acknowledges the distinct differences between language tasks and recommendation. The Foundation Model allows various downstream applications, from direct use as a predictive model to generate user and entity embeddings for other applications, and can be fine-tuned for specific canvases. We see promising results from downstream integrations. This move from multiple specialized models to a more comprehensive system marks an exciting development in the field of personalized recommendation systems.\n\nAcknowledgements\n\nContributors to this work (name in alphabetical order): Ai-Lei Sun Aish Fenton Anne Cocos Anuj Shah Arash Aghevli Baolin Li Bowei Yan Dan Zheng Dawen Liang Ding Tong Divya Gadde Emma Kong Gary Yeh Inbar Naor Jin Wang Justin Basilico Kabir Nagrecha Kevin Zielnicki Linas Baltrunas Lingyi Liu Luke Wang Matan Appelbaum Michael Tu Moumita Bhattacharya Pablo Delgado Qiuling Xu Rakesh Komuravelli Raveesh Bhalla Rob Story Roger Menezes Sejoon Oh Shahrzad Naseri Swanand Joshi Trung Nguyen Vito Ostuni Wei Wang Zhe Zhang\n\nReference", "label": 0}
{"title": "Part 2: A Survey of Analytics Engineering Work at Netflix", "url": "https://netflixtechblog.com/part-2-a-survey-of-analytics-engineering-work-at-netflix-4f1f53b4ab0f?source=collection_home---4------6-----------------------", "content": "Part 2: A Survey of Analytics Engineering Work at Netflix Netflix Technology Blog 9 min read \u00b7 Jan 2, 2025 -- 9 Listen Share\n\nThis article is the second in a multi-part series sharing a breadth of Analytics Engineering work at Netflix, recently presented as part of our annual internal Analytics Engineering conference. Need to catch up? Check out Part 1. In this article, we highlight a few exciting analytic business applications, and in our final article we\u2019ll go into aspects of the technical craft.\n\nGame Analytics\n\nYimeng Tang, Claire Willeck, Sagar Palao\n\nUser Acquisition Incrementality for Netflix Games\n\nNetflix has been launching games for the past three years, during which it has initiated various marketing efforts, including User Acquisition (UA) campaigns, to promote these games across different countries. These UA campaigns typically feature static creatives, launch trailers, and game review videos on platforms like Google, Meta, and TikTok. The primary goals of these campaigns are to encourage more people to install and play the games, making incremental installs and engagement crucial metrics for evaluating their effectiveness.\n\nMost UA campaigns are conducted at the country level, meaning that everyone in the targeted countries can see the ads. However, due to the absence of a control group in these countries, we adopt a synthetic control framework (blog post) to estimate the counterfactual scenario. This involves creating a weighted combination of countries not exposed to the UA campaign to serve as a counterfactual for the treated countries. To facilitate easier access to incrementality results, we have developed an interactive tool powered by this framework. This tool allows users to directly obtain the lift in game installs and engagement, view plots for both the treated country and the synthetic control unit, and assess the p-value from placebo tests.\n\nTo better guide the design and budgeting of future campaigns, we are developing an Incremental Return on Investment model. This model incorporates factors such as the incremental impact, the value of the incremental engagement and incremental signups, and the cost of running the campaign. In addition to using the causal inference framework mentioned earlier to estimate incrementality, we also leverage other frameworks, such as Incremental Account Lifetime Valuation (blog post), to assign value to the incremental engagement and signups resulting from the campaigns.\n\nMeasuring and Validating Incremental Signups for Netflix Games\n\nNetflix is a subscription service meaning members buy subscriptions which include games but not the individual games themselves. This makes it difficult to measure the impact of different game launches on acquisition. We only observe signups, not why members signed up.\n\nThis means we need to estimate incremental signups. We adopt an approach developed at Netflix to estimate incremental acquisition (technical paper). This approach uses simple assumptions to estimate a counterfactual for the rate that new members start playing the game.\n\nBecause games differ from series/films, it\u2019s crucial to validate this estimation method for games. Ideally, we would have causal estimates from an A/B test to use for validation, but since that is not available, we use another causal inference design as one of our ensemble of validation approaches. This causal inference design involves a systematic framework we designed to measure game events that relies on synthetic control (blog post).\n\nAs we mentioned above, we have been launching User Acquisition (UA) campaigns in select countries to boost game engagement and new memberships. We can use this cross-country variation to form a synthetic control and measure the incremental signups due to the UA campaign. The incremental signups from UA campaigns differ from those attributed to a game, but they should be similar. When our estimated incremental acquisition numbers over a campaign period are similar to the incremental acquisition numbers calculated using synthetic control, we feel more confident in our approach to measuring incremental signups for games.\n\nNetflix Games Players\u2019 Adventure: Modeled using State Machine\n\nAt Netflix Games, we aim to have a high number of members engaging with games each month, referred to as Monthly Active Accounts (MAA). To evaluate our progress toward this objective and to find areas to boost our MAA, we modeled the Netflix players\u2019 journey as a state machine.\n\nWe track a daily state machine showing the probability of account transitions between states.\n\nNetflix Players\u2019 Journey as State machine\n\nModeling the players\u2019 journey as a state machine allows us to simulate future states and assess progress toward engagement goals. The most basic operation involves multiplying the daily state-transition matrix with the current state values to determine the next day\u2019s state values.\n\nThis basic operation allows us to explore various scenarios:\n\nConstant Trends: If transition rates stay constant, we can predict future states by repeatedly multiplying the daily state-transition matrix to new state values, helping us assess progress towards annual goals under unchanged conditions.\n\nDynamic Scenarios: By modifying transition rates, we can simulate complex scenarios. For instance, mimicking past changes in transition rates from a game launch allows us to predict the impact of similar future launches by altering the transition rate for a specific period.\n\nSteady State: We can calculate the steady state of the state-transition matrix (excluding new players) to estimate the MAA once all accounts have tried Netflix games and understand long-term retention and reactivation effects.\n\nBeyond predicting future states, we use the state machine for sensitivity analysis to find which transition rates most impact MAA. By making small changes to each transition rate we calculate the resulting MAA and measure its impact. This guides us in prioritizing efforts on top-of-funnel improvements, member retention, or reactivation.\n\nContent Cash Modeling\n\nAlex Diamond\n\nAt Netflix we produce a variety of entertainment: movies, series, documentaries, stand-up specials, and more. Each format has a different production process and different patterns of cash spend, called our \u201cContent Forecast\u201d. Looking into the future, Netflix keeps a plan of how many titles we intend to produce, what kinds, and when. Because we don\u2019t yet know what specific titles that content will eventually become, these generic placeholders are called \u201cTBD Slots.\u201d A sizable portion of our Content Forecast is represented by TBD Slots.\n\nAlmost all businesses have a cash forecasting process informing how much cash they need in a given time period to continue executing on their plans. As plans change, the cash forecast will change. Netflix has a cash forecast that projects our cash needs to produce the titles we plan to make. This presents the question: how can we optimally forecast cash needs for TBD Slots, given we don\u2019t have details on what real titles they will become?\n\nThe large majority of our titles are funded throughout the production process \u2014 starting from when we begin developing the title to shooting the actual shows and movies to launch on our Netflix service.\n\nSince cash spend is driven by what is happening on a production, we model it by breaking down into these three steps:\n\nDetermine estimated production phase durations using historical actuals Determine estimated percent of cash spent in each production phase Model the shape of cash spend within each phase\n\nPutting these three pieces together allows us to generate a generic estimation of cash spend per day leading up to and beyond a title\u2019s launch date (a proxy for \u201ccompletion\u201d). We could distribute this spend linearly across each phase, but this approach allows us to capture nuance around patterns of spend that ramp up slowly, or are concentrated at the start and taper off throughout.\n\nBefore starting any math, we need to ensure a high quality historical dataset. Data quality plays a huge role in this work. For example, if we see 80% of our cash spent before production even started, it might be safe to say that either the production dates (which are manually captured) are incorrect or that title had a unique spending pattern that we don\u2019t want to anticipate our future titles will follow.\n\nFor the first two steps, finding the estimated phase durations and cash percent per phase, we\u2019ve found that simple math works best, for interpretability and consistency. We use a weighted average across our \u201cclean\u201d historical actuals to produce these estimated assumptions.\n\nFor modeling the shape of spend throughout each phase, we perform constrained optimization to fit a 3rd degree polynomial function. The constraints include:\n\nMust pass through the points (0,0) and (1,1). This ensures that 0% through the phase, 0% of that phase\u2019s cash has been spent. Similarly, 100% through the phase, 100% of that phase\u2019s cash has been spent. The derivative must be non-negative. This ensures that the function is monotonically increasing, avoiding counterintuitively forecasting any negative spend.\n\nThe optimization\u2019s objective function minimizes the sum of squared residuals and returns the coefficients of the polynomial that will guide the shape of cash spend through each phase.\n\nOnce we have these coefficients, we can evaluate this polynomial at each day of the expected phase duration, and then multiply the result by the expected cash per phase. With some additional data processing, this yields an expected percent of cash spend each day leading up to and beyond the launch date, which we can base our forecasts on.\n\nAssistive Speech Recognition in Dubbing Workflows at Netflix\n\nTanguy Cornau\n\nGreat stories can come from anywhere and be loved everywhere. At Netflix, we strive to make our titles accessible to a global audience, transcending language barriers to connect with viewers worldwide. One of the key ways we achieve this is through creating dubs in many languages.\n\nFrom the transcription of the original titles all the way to the delivery of the dub audio, we blend innovation with human expertise to preserve the original creative intent.\n\nLeveraging technologies like Assistive Speech Recognition (ASR), we seek to make the transcription part of the process more efficient for our linguists. Transcription, in our context, involves creating a verbatim script of the spoken dialogue, along with precise timing information to perfectly align the text with the original video. With ASR, instead of starting the transcription from scratch, linguists get a pre-generated starting point which they can use and edit for complete accuracy.\n\nThis efficiency enables linguists to focus more on other creative tasks, such as adding cultural annotations and references, which are crucial for downstream dubbing.\n\nWith ASR, and other new and enhanced technologies we introduce, rigorous analytics and measurement are essential to their success. To effectively evaluate our ASR system, we\u2019ve established a multi-layered measurement framework that provides comprehensive insights into its performance across many dimensions (for example, the accuracy of the text and timing predictions), offline and online.\n\nASR is expected to perform differently for various languages; therefore, at a high level, we track metrics by original language of the show, allowing us to assess overall ASR effectiveness and identify trends across different linguistic contexts. We further break down performance by various dimensions, e.g. content type, genre, etc\u2026 to help us pinpoint specific areas where the ASR system may encounter difficulties. Furthermore, our framework allows us to conduct in-depth analyses of individual titles\u2019 transcription, focusing on critical quality dimensions around text and timing accuracy of ASR suggestions. By zooming in on where the system falls short, we gain valuable insights into specific challenges, enabling us to further refine our understanding of ASR performance.\n\nThese measurement layers collectively empower us to continuously monitor, identify improvement areas, and implement targeted enhancements, ensuring that our ASR technology gets more and more accurate, effective, and helpful to linguists across diverse content types and languages. By refining our dubbing workflows through these innovations, we aim to keep improving the quality of our dubs to help great stories travel across the globe and bring joy to our members.", "label": 0}
{"title": "The value of AI: How Microsoft\u2019s customers and partners are creating differentiated AI solutions to reinvent how they do business today", "url": "https://blogs.microsoft.com/blog/2025/01/28/the-value-of-ai-how-microsofts-customers-and-partners-are-creating-differentiated-ai-solutions-to-reinvent-how-they-do-business-today/", "content": "Organizational leaders in every industry around the world are evaluating ways AI can unlock opportunities, drive pragmatic innovation and yield value across their business. At Microsoft, we are dedicated to helping our customers accelerate AI Transformation by empowering human ambition with Copilots and agents, developing differentiated AI solutions and building scalable cybersecurity foundations. At Microsoft Ignite we made over 100 announcements that bring the latest innovation directly to our customers and partners, and shared how Microsoft is the only technology leader to offer three distinct AI platforms for them to build AI solutions:\n\nCopilot is your UI for AI, with Copilot Studio enabling low-code creation of agents and extensibility to your data. Azure AI Foundry is the only AI app server for building real-world, world-class, AI-native applications. Microsoft Fabric is the AI data platform that provides one common way to reason over your data \u2014no matter where it lives.\n\nAll three of these platforms are open and work synchronously to enable the development of modern AI solutions; and each is surrounded by our world-class security offerings so leaders can move their AI-first strategies forward with confidence.\n\nAs we look ahead to what we can achieve together, I remain inspired by the work we are doing today. Below are a handful of the many stories from the past quarter highlighting the differentiated AI solutions our customers and partners are driving to move business forward across industries and realize pragmatic value. Their success clearly illustrates that real results can be harnessed from AI today, and it is changing the way organizations do business.\n\nTo power its industrial IoT and AI platform, ABB Group leveraged Microsoft Azure OpenAI Service to create Genix Copilot: a generative AI-powered analytics suite aimed at solving some of the most complex industrial problems. The solution helps customers analyze key functions in their operations \u2014such as asset and process performance, energy optimization and emission monitoring \u2014 with real-time operational insights. As a result, customers are seeing up to 35% savings in operations and maintenance, and up to 20% improvement in energy and emission optimization. ABB also saw an 80% decrease in service calls with the self-service capabilities of Genix Copilot.\n\nServing government healthcare agencies across the US, Acentra Health turned to Microsoft to help introduce the latest AI capabilities that maximize talent and cut costs in a secure, HIPAA-compliant manner. Using Azure OpenAI Service, the company developed MedScribe \u2014 an AI-powered tool reducing the time specially trained nursing staff spend on appeal determination letters. This innovation saved 11,000 nursing hours and nearly $800,000, reducing time spent on each appeal determination letter by about 50%. MedScribe also significantly enhanced operational efficiency, enabling nurses to process 20 to 30 letters daily with a 99% approval rate.\n\nTo ease challenges for small farmers, Romanian agribusiness group Agricover revolutionized access to credit by developing MyAgricover. Built with help from partner Avaelgo, the scalable digital platform utilizes Microsoft Azure, Azure API Management and Microsoft Fabric to automate the loan process and enable faster approvals and disbursements. This has empowered small farmers to grow their businesses and receive faster access to financing by reducing loan approval time by 90 percent \u2014 from 10 working days to a maximum of 24 hours.\n\nBuilding on its status as a world-class airline with a strong Indian identity, Air India sought ways to enhance customer support while managing costs. By developing AI.g, one of the industry\u2019s first generative AI virtual assistants built on Azure OpenAI Service, the airline upgraded the customer experience. Today, 97% of customer queries are handled with full automation, resulting in millions of dollars of support costs saved and improved customer satisfaction \u2014 further positioning the airline for continued growth.\n\nBMW Group aimed to enhance data delivery efficiency and improve vehicle development and prototyping cycles by implementing a Mobile Data Recorder (MDR) solution with Azure App Service, Azure AI and Azure Kubernetes Service (AKS). The solution achieved 10 times more efficient data delivery, significantly improved data accessibility and elevated overall development quality. The MDR monitors and records more than 10,000 signals twice per second in every vehicle of BMW\u2019s fleet of 3,500 development cars and transmits data within seconds to a centralized cloud back end. Using Azure AI Foundry and Azure OpenAI Service, BMW Group created an MDR copilot fueled by GPT-4o. Engineers can now chat with the interface using natural language, and the MDR copilot converts the conversations into KQL queries, simplifying access to technical insights. Moving from on-premises tools to a cloud-based system with faster data management also helps engineers troubleshoot in real time. The vehicle data covered by the system has doubled, and data delivery and analysis happen 10 times faster.\n\nColes Group modernized its logistics and administrative applications using Microsoft Azure Stack HCI to scale its edge AI capabilities and improve efficiency and customer experience across its 1,800 stores. By expanding its Azure Stack HCI footprint from two stores to over 500, Coles achieved a six-fold increase in the pace of application deployment, significantly enhancing operational efficiency and enabling rapid innovation without disrupting workloads. The retailer is also using Azure Machine Learning to train and develop edge AI models, speeding up data annotation time for training models by 50%.\n\nMultinational advertising and media company Dentsu wanted to speed time to insights for its team of data scientists and media analysts to support its media planning and budget optimization. Using Microsoft Azure AI Foundry and Azure OpenAI Service, Dentsu developers built a predictive analytics copilot that uses conversational chat and draws on deep expertise in media forecasting, budgeting and optimization. This AI-driven tool has reduced time to media insights for employees and clients by 90% and cut analysis costs.\n\nTo overcome the limitations of its current systems, scale operations and automate processes across millions of workflows, Docusign created the Intelligent Agreement Management (IAM) platform on Azure. Using Azure AI, Azure Cosmos DB, Azure Logic Apps and AKS, the platform transforms agreement data into actionable insights to enhance productivity and accelerate contract review cycles. IAM also ensures better collaboration and unification across business systems to provide secure solutions tailored to diverse customer needs. For example, its customer KPC Private funds reported a 70% reduction in time and resources dedicated to agreement processes.\n\nEmirates Global Aluminium (EGA) transformed its manufacturing operations by leveraging a hybrid environment with Azure Arc, Azure Stack HCI and Azure Kubernetes Service. This digital manufacturing platform resulted in 86% cost savings for AI image and video analytics and a 13-fold improvement in AI response times. The seamless hybrid cloud architecture has enhanced EGA\u2019s operational efficiency and agility, supporting its Industry 4.0 transformation strategy.\n\nEY collaborated with Microsoft to enhance the inclusivity of AI development using Azure AI Foundry. By involving neurodivergent technologists from EY\u2019s Neuro-Diverse Centers of Excellence, they improved the accessibility and productivity of AI tools, resulting in more inclusive AI solutions, fostering innovation and ensuring that AI tools unlock the potential of all users. With an estimated 20% of the global workforce identifying as neurodivergent, inclusive AI solutions are crucial for maximizing creativity and productivity. Neurodivergent EY technologists also collaborated with Microsoft developers to make Azure AI Foundry more inclusive and help all users work productively to create innovative AI solutions.\n\nColombian household appliance manufacturer Haceb integrated AI to optimize processes, reduce costs and improve service quality. Using Microsoft Copilot Studio and Azure OpenAI Service, the company created a virtual technical support assistant, saving its 245 technicians 5 minutes per visit \u2014 a total of 5,000 minutes saved daily. This AI solution has enhanced efficiency and boosted customer satisfaction by allowing for faster issue resolution. Haceb\u2019s AI adoption has also empowered employees, boosted productivity and positioned the company as a leader in AI innovation in Colombia.\n\nTo better serve its global patients, Operation Smile \u2014 in collaboration with partner Squadra \u2014 leveraged Azure AI, Machine Learning and Microsoft Fabric to develop an AI-powered solution to predict surgical outcomes and optimize resource allocation. This innovation resulted in a 30% increase in surgical efficiency, a 90% reduction in translation errors and improved patient outcomes. Additionally, report generation is now up to 95% quicker, and repeated medical events have decreased by 15%, enabling Operation Smile to provide better care to more children worldwide.\n\nOntada \u2014 a McKesson business dedicated to oncology data and evidence, clinical education and point-of-care technologies \u2014 needed a way to generate key insights across 150 million unstructured oncology documents. Using Microsoft Azure AI and Azure OpenAI Service, Ontada developed a data platform solution called ON.Genuity to provide AI-driven insights into the patient journey, enhance patient trial matching and identify care gaps. The company also implemented large language models to target nearly 100 critical oncology data elements across 39 cancer types, enabling the company to analyze an estimated 70% of previously inaccessible data, reduce processing time by 75% and accelerate product time-to-market from months to just one week.\n\nAs the UK\u2019s largest pet care company, Pets at Home sought a way to combat fraud across its retail operations \u2014 particularly as its online business continued to grow. Working closely with its fraud team, it adopted Copilot Studio to develop an AI agent that quickly identifies suspicious transactions. The agent autonomously gathers relevant information, performs analysis and shares it with a fraud agent to enable a manual, data-intensive investigative process while ensuring a human remains in the loop. With this low-code agent extending and seamlessly integrating into existing systems, the company\u2019s fraud department can act more quickly; what used to take 20 to 30 minutes is now handled by the AI agent within seconds. The company is identifying fraud 10 times faster and is processing 20 times more cases a day. Now, the company can operate at scale with speed, efficiency and accuracy \u2014 with savings expected to be in the seven figures as it continues to build more agents.\n\nRevenue Grid, a technology company specializing in sales engagement and revenue optimization solutions, partnered with Cloud Services to modernize its data infrastructure and develop a unified data warehouse capable of handling unstructured, semi-structured and structured data. By migrating to Microsoft Fabric, Revenue Grid can now deliver data-powered revenue intelligence, driven by a unified platform, elastic scalability, enhanced analytics capabilities and streamlined operations. Revenue Grid has reduced infrastructure costs by 60% while enhancing its analytical capabilities to improve real-time data processing, empowering sales teams with accurate and diverse data.\n\nTo better manage and integrate employee data across diverse regions and systems, UST built a comprehensive Employee Data platform on Microsoft Fabric. In under a year, UST migrated 20 years of employee data with all security measures to enhance data accessibility and employee productivity. The Meta Data Driven Integration (MDDI) framework in Fabric also helped the company cut data ingestion time by 50% so employees can focus more on analysis than preparation. As a result of this implementation, the company has seen an increase in collaboration and innovation from employees, helping put its values into action.\n\nThe Microsoft Commercial Marketplace offers millions of customers worldwide a convenient place to find, try and buy software and services across 140 countries. As a Marketplace partner, WeTransact is helping independent software vendors (ISVs) list and transact their software solutions \u2014 and find opportunities for co-selling and extending their reach to enterprise customers through development of the WeTransact platform. Powered by Azure OpenAI Service, the platform is changing the way partnerships are being built by using AI pairing to facilitate a \u201cplug and play\u201d reseller network. More than 300 ISVs worldwide have joined the Microsoft Commercial Marketplace using the WeTransact platform, cutting their time to publish by 75%.\n\nThe opportunity for AI to create value is no longer an ambition for the future \u2014 it is happening now, and organizational leaders across industries are investing in AI-first strategies to change the way they do business. We believe AI should empower human achievement and enrich the lives of employees; and we are uniquely differentiated to help you accelerate your AI Transformation responsibly and securely. Choosing the right technology provider comes down to trust, and I look forward to what we will achieve together as we partner with you on your AI journey.\n\nTags: AI, Azure, Azure AI, Azure AI Foundry, Azure Arc, Azure OpenAI Service, Azure Stack HCI, Copilot, Copilot Studio, Microsoft Fabric, Microsoft Ignite 2024", "label": 0}
{"title": "What you should know from the Google I/O 2025 Developer keynote", "url": "https://developers.googleblog.com/en/google-io-2025-developer-keynote-recap/", "content": "This year at Google I/O we\u2019re showing how you can build across Google\u2019s different platforms, and innovate using our best AI models from Google DeepMind. Here are the top announcements from the Developer keynote.\n\nBuilding with Gemini\n\nGoogle AI Studio is the fastest way to evaluate models and start building with the Gemini API.\n\nGoogle AI Studio makes it easy to build with the Gemini API: We\u2019ve integrated Gemini 2.5 Pro into the native code editor, enabling you to prototype faster. It\u2019s tightly optimized with the GenAI SDK so you can instantly generate web apps from text, image, or video prompts. Start from a simple prompt, or get inspired by starter apps in the showcase.\n\nBuild agentic experiences with the Gemini API: Build agents with Gemini 2.5 advanced reasoning capabilities via the Gemini API and new tools, like URL Context. It enables the model to pull context from web pages with just a link. We also announced the Gemini SDKs will support Model Context Protocol (MCP) definitions, making it easier to leverage open source tools.\n\nGemini 2.5 Flash Native Audio in the Live API: Build agentic applications that hear and speak, with full control over the model\u2019s voice, tone, speed, and overall style, in 24 languages. Gemini 2.5 Flash Native Audio is much better at understanding conversational flow and ignoring stray sounds or voices, leading to smoother, more natural back-and-forth.\n\nGenerate high-quality UI designs with Stitch: A new AI-powered tool to generate user interface designs and corresponding frontend code for web applications. Iterate on your designs conversationally using chat, adjust themes, and easily export your creations to CSS/HTML or Figma to keep working. Try Stitch for UI design.\n\nOur async code agent, Jules, is now in public beta: Jules is a parallel, asynchronous coding agent that works directly with your GitHub repositories. You can ask Jules to take on tasks such as version upgrades, writing tests, updating features, and bug fixes, to name a few. It spins up a Cloud VM, makes coordinated edits across your codebase, runs tests, and you can open a pull request from its branch when you're happy with the code.\n\n\n\nAndroid\n\nLearn how we\u2019re making it easier for you to build great experiences across devices.\n\nBuilding experiences with generative AI: Generative AI enhances apps by making them intelligent, personalized, and agentic. We announced new ML Kit GenAI APIs using Gemini Nano for common on-device tasks. We showcased an AI sample app, Androidify, which lets you create an Android robot of yourself using a selfie. Discover how Androidify is built, and read the developer documentation to get started.\n\nBuilding excellent apps adaptively across 500 million devices: Mobile Android apps form the foundation across phones, foldables, tablets, and ChromeOS, and this year we\u2019re helping you bring them to cars and Android XR. You can also take advantage of Material 3 Expressive to help make your apps shine.\n\nGemini in Android Studio - AI agents to help you work: Gemini in Android Studio is the AI-powered coding companion that makes developers more productive at every stage of the dev lifecycle. We previewed Journeys, an agentic experience that helps with writing and executing end-to-end tests. We also previewed the Version Upgrade Agent which helps update dependencies. Learn more about how these agentic experiences in Gemini in Android Studio can help you build better apps, faster.\n\n\n\nWeb\n\nWe\u2019re making it easier to create powerful web experiences, from building better UI and faster debugging, to creating new AI-powered features.\n\nCarousels are now easier than ever to build with a few lines of CSS and HTML: Build beautiful carousels with CSS that are interactive at first paint. With Chrome 135, we've combined a few new CSS primitives to make building carousels, and other types of off-screen UI, dramatically easier. Use familiar CSS concepts to create rich, interactive, smooth, and more accessible carousels, in a fraction of the time.\n\nIntroducing the new experimental Interest Invoker API: Declaratively toggle popovers when visitor interest is active for a small duration. Combine with the Anchor Positioning API and Popover API to build complex, responsive, layered UI elements like tooltips and hover cards, without JavaScript. Interest Invoker API is available as an origin trial.\n\nBaseline features availability is now in your familiar tools: VS Code now displays the Baseline status of features as you build, with support coming soon to other VS Code-based IDEs and WebStorm by JetBrains. Baseline is now also supported in ESLint for CSS, HTML ESLint, and Stylelint. RUMvision combines Baseline information with real-user data, letting you strategically select the optimal Baseline target for your audience. Plus, with the web-features data set now 100% mapped, you can now access the Baseline status of every feature on every major browser.\n\nAI in Chrome DevTools supports your debugging workflow: Boost your development workflow with Gemini integrated directly into Chrome DevTools. With AI assistance, you can now directly apply suggested changes to the files in your workspace in the Elements panel. Plus, the reimagined Performance Panel now features a powerful \u2018Ask AI\u2019 integration that provides contextual performance insights to help optimize your web application\u2019s Core Web Vitals.\n\nNew built-in AI APIs using Gemini Nano are now available, including multimodal capabilities: Gemini Nano brings enhanced privacy, reduced latency, and lower cost. Starting from Chrome 138, the Summarizer API, Language Detector API, Translator API, and Prompt API for Chrome Extensions are available in Stable. The Writer and Rewriter APIs are available in origin trials, and the Proofreader API and Prompt API with multimodal capabilities are in Canary. Join our early preview program to help shape the future of AI on the web.\n\n\n\nFirebase\n\nPrototype, build, and run modern, AI-powered, full-stack apps users love with Firebase. Use Firebase Studio, a cloud-based, AI workspace powered by Gemini 2.5, to turn your ideas into a full-stack app in minutes, from prompt to publish.\n\nFigma designs can be brought to life in Firebase Studio: Import a Figma design directly into Firebase Studio using the builder.io plugin, then add features and functionality using Gemini in Firebase without having to write any code.\n\nFirebase Studio will now suggest a backend: Rolling out over the next several weeks, when you use the App Prototyping agent, Firebase Studio can detect the need for a backend. Firebase Studio will now recommend Firebase Auth and Cloud Firestore, and when you're ready to publish the app to Firebase App Hosting, Firebase Studio will provision those services for you.\n\nFirebase AI Logic: Integrate Google\u2019s gen AI models directly through your client apps, or through Genkit for server-side implementation. As part of the evolution from Vertex AI in Firebase to Firebase AI Logic, we\u2019re also releasing new features such as client side integrations for the Gemini Developer API, hybrid inference, enhanced observability, and deeper integrations with Firebase products such as App Check and Remote Config.\n\n\n\nBuilding with open models\n\nThere's so much you can do when building with Gemini, but sometimes it's better to train and tune your own model. That\u2019s why we released Gemma, our family of open models designed to be state of the art, and fit on devices.\n\nGemma 3n is in early preview: This model can run on as little as 2GB of RAM thanks to research innovations. It is the first model built on the new, advanced mobile-first architecture that will also power the next generation of Gemini Nano, and is engineered for unmatched AI performance directly on portable devices.\n\nMedGemma is our most capable open model for multimodal medical text and image comprehension: A variant of Gemma 3, MedGemma is a great starting point for developers to fine tune and adapt to build their own healthcare-based AI applications. Its small size makes it efficient for inference, and because it\u2019s open, it enables developers with the flexibility to fine-tune the model and run it in their preferred environments. MedGemma is available for use now as part of Health AI Developer Foundations.\n\nColab is launching an agent first experience that transforms coding: Powered by Gemini 2.5 Flash, Colab helps you navigate complex tasks, such as fine-tuning a model. We showcased how the new AI-first Colab can build UI, saving you lots of coding time.\n\nSignGemma is a sign language understanding model coming later this year to the Gemma family: It is the most capable model for translating sign languages into spoken language text to date (best at American Sign Language to English), enabling you to develop new ways for Deaf/Hard of Hearing users to access technology. Share your input at goo.gle/SignGemma.\n\nDolphinGemma is the world\u2019s first large language model for dolphins: Working with researchers at Georgia Tech and the Wild Dolphin Project, DolphinGemma was fine-tuned on data from decades of field research, to help scientists better understand patterns in how dolphins communicate.\n\n\n\nGoogle Developer Program\n\nWe expanded AI benefits for the Google Developer Program, including Gemini Code Assist Standard, a new gen AI developer annual credit, and 3 months of Google One AI Premium. We also announced a new Google Cloud & NVIDIA community where you can connect with experts from both companies in a dedicated forum, and soon gain access to exclusive learning content and credits.\n\n\n\nTune into all of the developer news\n\nFollowing the keynotes, we\u2019ll be livestreaming sessions across AI, Android, web, and cloud May 20-21. Then, check out all of the Google I/O announcements and updates with 100+ sessions, codelabs, and more available on demand starting May 22.\n\nMake sure to connect with our thriving global community of developers, and follow along on LinkedIn and Instagram as we bring I/O Connect events to developers around the world.", "label": 0}
{"title": "What makes a good friend?", "url": "https://lifeofpablo.com/blog/what-makes-a-good-friend", "content": "What makes a good friend?\n\nGroup of Friends\n\nThis post was written in English (en_US).\n\nThis post is a response to my blog post. I am answering question #91: What makes a good friend?\n\nWhat makes a good friend? That's a great question. I think everyone has a different definition of what makes a good friend. I will share mine.\n\nOne of the most important things to understand about friendship is point's of view. We don't have to always agree with each other. We do need to be able to listen, process and be able to provide counter points of view in a respectable manner. Allowing time to process is important instead of reacting as the default.\n\nOne of the most important qualities of a good friend is being supportive. I'm very fortunate to have friends who are supportive and emotionally connected. We check in with each other, we talk about our emotions, we talk about our problems. As men, we are human and we should talk about what's happening to our lives. This is 2025.\n\nA good friend will always make an effort to stay in touch especially when you live far away from each other. This is a two way streak. One person can't solely be the one actively reaching out. It's okay if you go a little while without talking, just don't let your friendship die out. Life gets busy but we can't forget who our friends are.\n\nWhat makes a good friend?\n\nThis post was written at Ikea Restaurant in San Francisco.", "label": 1}
{"title": "Read This Will Be Fun \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/2025/06/21/read-this-will-be-fun/", "content": "The Princess Bride meets People We Meet on Vacation in this cozy quest romantasy about a group of friends who once defended their magical land together but haven\u2019t spoken since, reuniting to attend a royal wedding, and ending up on a new adventure to save the realm\u2014and hopefully themselves.\n\nTen years ago, they saved the realm. It ruined their lives.\n\nEveryone in Mythria knows the story of how best friends Beatrice and Elowen, handsome ex-bandit Clare, and valiant leader Galwell the Great defended the land from darkness. It\u2019s a tale beloved by all\u2014except the former heroes. They haven\u2019t spoken in a decade, devastated by what their quest cost them.\n\nBut when they receive an invitation to the queen of Mythria\u2019s wedding, it\u2019s a summons they can\u2019t refuse . . . and a reunion for the ages, with Clare secretly not over his long-ago fling with Beatrice, Beatrice fighting the guilt she feels over how everything ended, Elowen unprepared for the return of her former flame (the cunning Vandra), and all of them lost without Galwell\u2019s presence. And if reuniting with old friends and lovers wasn\u2019t perilous enough, dark forces from their past have returned, plotting a domination that only Mythria\u2019s one-time defenders can stop. Maybe.\n\nDusting off old weapons and old instincts, they face undead nemeses, crystal caves, enchanted swords, coffee shops, games of magical Truth or Dare, and, hardest of all, their past\u2014rife with wounds never healed and romances never forgotten.\n\nThis time around, will their story end in happily ever after?", "label": 1}
{"title": "How GitHub engineers tackle platform problems", "url": "https://github.blog/engineering/infrastructure/how-github-engineers-tackle-platform-problems/", "content": "In my spare time I enjoy building Gundam models, which are model kits to build iconic mechas from the Gundam universe. You might be wondering what this has to do with software engineering. Product engineers can be seen as the engineers who take these kits and build the Gundam itself. They are able to utilize all pieces and build a working product that is fun to collect or even play with!\n\nPlatform engineers, on the other hand, supply the tools needed to build these kits (like clippers and files) and maybe even build a cool display so everyone can see the final product. They ensure that whoever is constructing it has all the necessary tools, even if they don\u2019t physically build the Gundam themselves.\n\nAbout a year ago, my team at GitHub moved to the infrastructure organization, inheriting new roles and Areas of Responsibility (AoRs). Previously, the team had tackled external customer problems, such as building the new deployment views across environments. This involved interacting with users who depend on GitHub to address challenges within their respective industries. Our new customers as a platform engineering team are internal, which makes our responsibilities different from the product-focused engineering work we were doing before.\n\nGoing back to my Gundam example, rather than constructing kits, we\u2019re now responsible for building the components of the kits. Adapting to this change meant I had to rethink my approach to code testing and problem solving.\n\nWhether you\u2019re working on product engineering or on the platform side, here are a few best practices to tackle platform problems.\n\nUnderstanding your domain\n\nOne of the most critical steps before tackling problems is understanding the domain. A \u201cdomain\u201d is the business and technical subject area in which a team and platform organization operate. This requires gaining an understanding of technical terms and how these systems interact to provide fast and reliable solutions. Here\u2019s how to get up to speed:\n\nTalk to your neighbors: Arrange a handover meeting with a team that has more knowledge and experience with the subject matter. This meeting provides an opportunity to ask questions about terminology and gain a deeper understanding of the problems the team will be addressing.\n\nArrange a handover meeting with a team that has more knowledge and experience with the subject matter. This meeting provides an opportunity to ask questions about terminology and gain a deeper understanding of the problems the team will be addressing. Investigate old issues: If there is a backlog of issues that are either stale or still persistent, they may give you a better understanding of the system\u2019s current limitations and potential areas for improvement.\n\nIf there is a backlog of issues that are either stale or still persistent, they may give you a better understanding of the system\u2019s current limitations and potential areas for improvement. Read the docs: Documentation is a goldmine of knowledge that can help you understand how the system works.\n\nBridging concepts to platform-specific skills\n\nWhile the preceding advice offers general guidance applicable to both product and platform teams, platform teams \u2014 serving as the foundational layer \u2014 necessitate a more in-depth understanding.\n\nNetworks : Understanding network fundamentals is crucial for all engineers, even those not directly involved in network operations. This includes concepts like TCP, UDP, and L4 load balancing, as well as debugging tools such as dig. A solid grasp of these areas is essential to comprehend how network traffic impacts your platform.\n\n: Understanding network fundamentals is crucial for all engineers, even those not directly involved in network operations. This includes concepts like TCP, UDP, and L4 load balancing, as well as debugging tools such as dig. A solid grasp of these areas is essential to comprehend how network traffic impacts your platform. Operating systems and hardware: Selecting appropriate virtual machines (VMs) or physical hardware is vital for both scalability and cost management. Making well-informed choices for particular applications requires a strong grasp of both. This is closely linked to choosing the right operating system for your machines, which is important to avoid systems with vulnerabilities or those nearing end of life.\n\nSelecting appropriate virtual machines (VMs) or physical hardware is vital for both scalability and cost management. Making well-informed choices for particular applications requires a strong grasp of both. This is closely linked to choosing the right operating system for your machines, which is important to avoid systems with vulnerabilities or those nearing end of life. Infrastructure as Code (IaC): Automation tools like Terraform, Ansible, and Consul are becoming increasingly essential. Proficiency in these tools is becoming a necessity as they significantly decrease human error during infrastructure provisioning and modifications.\n\nAutomation tools like Terraform, Ansible, and Consul are becoming increasingly essential. Proficiency in these tools is becoming a necessity as they significantly decrease human error during infrastructure provisioning and modifications. Distributed systems: Dealing with platform issues, particularly in distributed systems, necessitates a deep understanding that failures are inevitable. Consequently, employing proactive solutions like failover and recovery mechanisms is crucial for preserving system reliability and preventing adverse user experiences. The optimal approach for this depends entirely on the specific problem and the desired system behavior.\n\nKnowledge sharing\n\nBy sharing lessons and ideas, engineers can introduce new perspectives that lead to breakthroughs and innovations. Taking the time to understand why a project or solution did or didn\u2019t work and sharing those findings provides new perspectives that we can use going forward.\n\nHere are three reasons why knowledge sharing is so important:\n\nTeamwork makes the dream work: Collaboration often results in quicker problem resolution and fosters new solution innovation, as engineers have the opportunity to learn from each other and expand upon existing ideas.\n\nCollaboration often results in quicker problem resolution and fosters new solution innovation, as engineers have the opportunity to learn from each other and expand upon existing ideas. Prevent lost knowledge : If we don\u2019t share our lessons learned, we prevent the information from being disseminated across the team or organization. This becomes a problem if an engineer leaves the company or is simply unavailable.\n\n: If we don\u2019t share our lessons learned, we prevent the information from being disseminated across the team or organization. This becomes a problem if an engineer leaves the company or is simply unavailable. Improve our customer success: As engineers, our solutions should effectively serve our customers. By sharing our knowledge and lessons learned, we can help the team build reliable, scalable, and secure platforms, which will enable us to create better products that meet customer needs and expectations!\n\nBut big differences start to appear between product engineering and infrastructure engineering when it comes to the impact radius and the testing process.\n\nImpact radius\n\nWith platforms being the fundamental building blocks of a system, any change (small or large) can affect a wide range of products. Our team is responsible for DNS, a foundational service that impacts numerous products. Even a minor alteration to this service can have extensive repercussions, potentially disrupting access to content across our site and affecting products ranging from GitHub Pages to GitHub Copilot.\n\nUnderstand the radius: Or understand the downstream dependencies. Direct communication with teams that depend on our service provides valuable insights into how proposed changes may affect other services.\n\nOr understand the downstream dependencies. Direct communication with teams that depend on our service provides valuable insights into how proposed changes may affect other services. Postmortems: By looking at past incidents related to our platform and asking \u201cWhat is the impact of this incident?\u201d, we can form more context around what change or failure was introduced, how our platform played a role in it, and how it was fixed.\n\nBy looking at past incidents related to our platform and asking \u201cWhat is the impact of this incident?\u201d, we can form more context around what change or failure was introduced, how our platform played a role in it, and how it was fixed. Monitoring and telemetry: Condense important monitoring and logging into a small and quickly digestible medium to give you the general health of the system. This could be a Single Availability Metric (SAM), for example. The ability to quickly glance at a single dashboard allows engineers to rapidly pinpoint the source of an issue and streamlines the debugging and incident mitigation process, as compared to searching through and interpreting detailed monitors or log messages.\n\nTesting changes\n\nTesting changes in a distributed environment can be challenging, especially for services like DNS. A crucial step in solving this issue is utilizing a test site as a \u201creal\u201d machine where you can implement and assess all your changes.\n\nInfrastructure as Code (IaC): When using tools like Terraform or Ansible, it\u2019s crucial to test fundamental operations like provisioning and deprovisioning machines. There are circumstances where a machine will need to be re-provisioned. In these cases, we want to ensure the machine is not accidentally deleted and that we retain the ability to create a new one if needed.\n\nWhen using tools like Terraform or Ansible, it\u2019s crucial to test fundamental operations like provisioning and deprovisioning machines. There are circumstances where a machine will need to be re-provisioned. In these cases, we want to ensure the machine is not accidentally deleted and that we retain the ability to create a new one if needed. End-to-End (E2E): Begin directing some network traffic to these servers. Then the team can observe host behavior by directly interacting with it, or we can evaluate functionality by diverting a small portion of traffic.\n\nBegin directing some network traffic to these servers. Then the team can observe host behavior by directly interacting with it, or we can evaluate functionality by diverting a small portion of traffic. Self-healing: We want to test the platform\u2019s ability to recover from unexpected loads and identify bottlenecks before they impact our users. Early identification of bottlenecks or bugs is crucial for maintaining the health of our platform.\n\nIdeally changes will be implemented on a host-by-host basis once testing is complete. This approach allows for individual machine rollback and prevents changes from being applied to unaffected hosts.\n\nWhat to remember\n\nPlatform engineering can be difficult. The systems GitHub operates with are complex and there are a lot of services and moving parts. However, there\u2019s nothing like seeing everything come together. All the hard work our engineering teams do behind the scenes really pays off when the platform is running smoothly and teams are able to ship faster and more reliably \u2014 which allows GitHub to be the home to all developers.\n\nWant to dive deeper? Check out our infrastructure related blog posts.", "label": 0}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2023-04", "content": "en\n\nLocation: 38.581573, -121.494400\n\nBlog Post on this using Indieauth coming soon!\n\nTable of Contents\n\nIntroduction\n\nRecently I have started experimenting with identity. An SSO solution for Nginx using the auth_request module. Vouch Proxy can protect all of your websites at once.\n\nToday, I'll demonstrate how to setup Vouch Proxy on an nginx web server. In this example I will be using Google as our provider\n\nThis tutorial assumes you have prior knowledge of using a linux server such as Debian. Message me at hello@lifeofpablo.com if you need some help. I'd be happy to do so!\n\nWhat Vouch Proxy Does?\n\nAccording to the Repository README.md, it states the following:\n\nVouch Proxy (VP) forces visitors to login and authenticate with an IdP (such as one of the services listed above) before allowing them access to a website.\n\nVP can also be used as a Single Sign On (SSO) solution to protect all web applications in the same domain.\n\nAfter a visitor logs in Vouch Proxy allows access to the protected websites for several hours. Every request is checked by VP to ensure that it is valid.\n\nVP can send the visitor's email, name and other information which the IdP provides (including access tokens) to the web application as HTTP headers. VP can be used to replace application user management entirely.\n\nThings you'll need/prepare:\n\nA linux server with a public IP address with hosting and SSL Debian will be used here but any of the common distros will work Certbot is an easy solution to get SSL certifcate for https://\n\nGo Language (to compile vouch-proxy)\n\nVouch Proxy\n\nNginx Web Server Digital Ocean has a good guide if you need to learn how to setup virtual blocks in nginx.\n\n\n\nDownload/Install Vouch Proxy from Github\n\nMake sure to have Go Lang installed\n\n$ git clone https://github.com/vouch/vouch-proxy.git $ cd vouch-proxy $ ./do.sh goget $ ./do.sh build\n\nVouch Proxy Nginx Virtual Block\n\nLet's go ahead and create a virtual block to proxy Vouch Proxy.\n\nserver { server_name vouch.example.com; location / { proxy_set_header Host vouch.example.com; proxy_set_header X-Forwarded-Proto https; proxy_pass http://127.0.0.1:9090; }\n\nLet's go ahead and create a virtual block for a regular nginx website site or edit an existing virtual block. This is the website/service that you will protect with Vouch Proxy.\n\nIn this example I am using a php web app. If you a non php site site to work you can remove this location block and and edit it to your needs.\n\nGoogle Cloud Console\n\nGoogle Cloud Console API\n\nBefore we modify the config.yml, lets create an OAuth 2.0 Client ID and Client Secret which you will paste into the config.yml file.\n\nYou will have to do the following 1. Create a Project\n\nCall it whatever you like.\n\nI called it oauth\n\nCreate Credentials\n\nYou'll create the client ID and client secret (don't worry if you do the following steps and forget to copy these down. They will be available for you to copy at any time.\n\nOAuth consent screen\n\nApp Name:\n\nThis is where you will need to setup where your redirect URL is https://vouch.example.com/url. Just like the server_name set in the nginx virtual block config above.\n\nAuthorized Domain: example.com (just the base domain)\n\nFill out any other required fields\n\nModify your config.yml\n\nvouch: domains: - yourdomain.com - yourotherdomain.com cookie: secure: false oauth: provider: google client_id: xxxxxxxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.apps.googleusercontent.com client_secret: xxxxxxxxxxxxxxxxxxxxxxxx callback_urls: - https://yourdomain.com/auth - https://yourotherdomain.com/auth preferredDomain: yourdomain.com\n\nNginx Virtual block protected by Vouch Proxy\n\nserver { listen 80 ; listen [::]: 80 ; root /root/to/web/directory; index index.php index.html; server_name secretapp.example.com; location / { try_files $uri $uri / /index.php $is_args $args ; } client_max_body_size 100m ; location ~* \\.php$ { fastcgi_pass unix:/var/run/php/php8.2-fpm.sock; include fastcgi.conf; fastcgi_index yellow.php; fastcgi_split_path_info ^(.+\\.php)(/.+)$ ; fastcgi_param SCRIPT_FILENAME $document_root $fastcgi_script_name ; include fastcgi_params; } }\n\nEventually you will need to secure your site with SSL/TLS that makes your site use https://. Google will require that your traffic is secure with using it as 0auth as the method used to sign in to your protected website.\n\nDo this after you have the survey blocks working in the following section.\n\nHere is the link for Certbot for Debian. I have tested this on Debian 10 & 11. [https://certbot.eff.org/instructions?ws=nginx&os=debianbuster](Link for Certbot)\n\nCert bot can do this for you as long as you have the subdomain in your DNS pointing to your machine and have cert bot installed. It'll add these blocks in your\n\nor\n\nserver { server_name vouch.example.com . . . . . . . . . . . . . . . . . . . . . . . . . listen [::]: 443 ssl; listen 443 ssl; ssl_certificate /etc/letsencrypt/live/vouch.example.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/vouch.example.com/privkey.pem; include /etc/letsencrypt/options-ssl-nginx.conf; ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; } server { if ( $host = vouch.example.com) { return 301 https:// $host $request_uri ; } listen 80 ; listen [::]: 80 ; server_name secretapp.example.com; return 404 ; }\n\nLet's check for errors in nginx. Type the following command.\n\nnginx -t\n\nYou should see something similar to this:\n\nnginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful\n\nLet's open a browser tab or window!\n\nNote: I'm using firefox. (Preference). Any modern browser should work.\n\nType in the protected app' URL in the address bar\n\nSign in to Google\n\nSign in with an email that is allowed to sign to access the website when you configured it in Google Cloud Console.\n\nVoila, the protected page.\n\nHere is the home page of a Bludit CMS on subdomain acting as \"secretapp.example.com\"", "label": 1}
{"title": "Another side of me", "url": "https://lifeofpablo.com/blog/another-side-of-me", "content": "Another side of me\n\nBisexual Flag\n\nThis post was written in English (en_US).\n\nBackground\n\nBlogging has been such a great outlet for me this past year. In the past I would blog sporadically. I've always loved blogging as it allows me to express myself and allow my thoughts to flow (when they feel like it). It's allowed me to become more expressive and to reconnect with myself. I would say I feel more satisfied in becoming more expressive and to reconnect myself. It's a piece of a puzzle.\n\nDeciding What to Share\n\nYou know how there are things about yourself for the longest time, you didn't know whether or not to share publicly? There are things I'm not willing to share. At some point you are ready to share certain things. Then, you come to the realization of \"How am I going to put this in words?\" That's the part that's tricky. That's me at this very moment in time. I'm ready to share. I'll do my best.\n\nThe Journey\n\nPart of this journey has been working on myself and learning to accept who I am as a person. For the longest time I wasn't living my true self. I lived in two worlds, the American lifestyle outside of home and the Latino culture at home. I felt like two walls parallel to each other trying to keep me aligned. I never felt enough for the Americans and never felt enough for the Mexicans. Me being the only boy in my immediate family, I had to set an example, I had to think about carrying the family last name, and etc. As an American who is Mexican, I always felt like I was more scrutinized under a microscope than my non-latino counterparts. I had to find ways to circumvent this to allow myself to slightly express myself even in the slightest. I worried so much people would think of me and I felt judged. I let that dictate so much of my life. I was so sad and miserable and asked myself, \"How much more of this can I take?\" All this put me in a negative headspace that hurt me and directly or indirectly hurt others I loved. I still cry thinking about this. I'm still learning how to undo this damage I've caused. It will take time but will it ever go away completely? I'm not sure. Going to therapy has aided in defragmenting so many pieces of my life. And made me realize that, I have to start living my own life and care for my own happiness. Healing has to be a priority.\n\nLurking\n\nIf you've been wondering this whole time what this post is about, yes, this is my coming out post. I've been for years waiting to share this moment in some format. I've thought about sharing on Instagram or Facebook but I decided against this. I'd rather do it on my personal website and any of the decentralized platforms I use. I've watched so many coming out videos, and social media posts of people I know or strangers on the internet. That has been inspiring. I've been lurking the idea of this for so long. I finally brought myself to do this. Once I publish this, there is no going back. Once something is on the internet, it's there forever. I would be lying if I said I wasn't nervous or having second thoughts. I also think about how we live in a time where LGBT rights are under attack. I simply have to do this for myself and face the outcome of this. Positive or negative. So here it is...\n\nMy Sexuality\n\n\ud83c\udff3\ufe0f\u200d\ud83c\udf08 I'm sharing that I'm a bisexual person. \ud83c\udff3\ufe0f\u200d\ud83c\udf08\n\nBisexuality is variously defined as romantic or sexual attraction to both males and females, to more than one gender, or attraction to both people of the same gender and different genders.\n\nI am attracted to both men and women. It's a spectrum and there is no 50%-50% split. I feel at this time more attractive to people who identify as male than I am to someone who identifies as female. That varies by day as well. Some of my greatest loves have been women. I'm also open to the attraction towards more gender identities.\n\nHow Long ?\n\nI knew since I was five or six that I felt different about myself. Then the questions of, \"when are you having a girlfriend?\" or \"I bet you have insert number here of girls chasing you!?\" I did want that to happen at many points. Also, I had big crushes on other dudes.\n\nWho have you told?\n\nI've told friends and family who I am as an individual. Every single one of them I've told, each interaction with them was different. Some interactions were very heavy with emotion, some were very easy, and some I felt I was going to die inside because I was afraid of the reaction. Some took time to come around. Everything came alright in the end. I've felt supported. Even if the interaction didn't go as well, I felt the anxiety leave my body and be at a new level of peace with myself. I still tear up thinking about how strong and vulnerable I was in every single interaction. I'm very lucky to have friends and family who are supported. Some mentioned, \"We knew, Pablo. We were waiting for you to share.\" *Queue laughing after ugly crying *. Every moment was beautiful. My relationships have become stronger. I no longer have to hide, in plain sight to my loved ones. They no longer have to see me suffer inside. My family bond has also become stronger. I don't have to hide by putting a wall around me. I can actually be open with them and tell them what I'm feeling.\n\nWhat's to come?\n\nWell, I've been going on dates and meeting lots of people in the community. I have a supported community living in California that makes it a bit easier. I have so much more to learn about myself and make my way. I want to explore my sexuality more and take it to the next level. I don't see my sexuality to be the forefront of my persona. It's a piece of the puzzle that helps form me to be simply, me! I've added this piece to the puzzle. There are other pieces that I have to find their place.\n\nI'm happier with myself now. I can smile an actual smile now. My face isn't hiding behind a mask. I'm starting to live my more authentic self. I'm still going to be me at the end of the day. My sexuality is only one piece of the puzzle of who I am as a person.", "label": 1}
{"title": "Sharing our Engineering Career Framework with the world", "url": "https://dropbox.tech/culture/sharing-our-engineering-career-framework-with-the-world", "content": "Looking for our latest Engineering Career Framework? We made some updates in 2023. Learn more about the newest version.\n\n~ ~ ~\n\nAt Dropbox, we strive to be a great place for all engineers to grow and be recognized for that growth. Our Engineering Career Framework helps keep us accountable there and is viewable by anyone within the company. Today, we are also making it viewable by anyone outside the company. Our goal in doing so is to be transparent externally on how we think about leveling and career progression on the Engineering team at Dropbox. It also enables others to adapt and apply it to their own organizations.\n\nIn early 2020, we completed a major revision to the framework. Our focus was to be more explicit about the \u201cwhat\u201d (i.e. business impact made) and create better representation for all the different crafts within engineering that make Dropbox successful. After a year of running this current version through several promotion cycles, we are satisfied that it is moving our culture in the right direction.\n\nDropbox\u2019s Engineering Career Framework describes what\u2019s expected for our engineers at each of our career levels. Along with helping managers set expectations and hold teams accountable for their work, this resource empowers engineers to achieve greater impact in their role and grow in their careers.\n\nThat said, the framework is not meant to be a promotion checklist; rather, it\u2019s designed to help our engineers know what their impact could look like at the next level. This framework is also not an exhaustive list of examples and behaviors; each responsibility includes three to four key behaviors that serve as a guide for how to think about one\u2019s work. Consequently, it\u2019s worth noting that the framework should not be seen as a replacement for regular, active conversations between engineers and their managers about short and long-term career growth and development.\n\nTeams across our Engineering and People orgs worked together thoughtfully and closely to build this latest version of the Engineering Career Framework. As Dropbox grows and changes, expect it to be a living document that reflects Dropbox\u2019s evolution as an Engineering team.", "label": 0}
{"title": "Listing the contents of all team-accessible namespaces", "url": "https://dropbox.tech/developers/listing-team-contents", "content": "A Dropbox team can contain multiple members, and each member may have many of their own files and folders, as well as access to other files and folders, such as via shared folders or team folders. That can add up to a lot of content, and sometimes, it\u2018s useful to be able to programmatically list all of those items across all of the members of the team, such as for organizational or auditing purposes.\n\n\n\nThe Dropbox API does offer the ability to list the contents of any particular folder, and there are multiple ways an app could go about finding and listing all of the contents across all team members. One good way to do so is by first getting the list of all of the namespaces that the team has access to, and then listing the contents of each of those namespaces. This way helps avoid reading the same listing twice (for example, in the case where multiple team members are in the same shared folder). This works for teams with any type of team folder or team space configuration.\n\n\n\nCheck out the new code sample here for an example of how to implement that strategy to list all of the files and folders that a team can access.", "label": 0}
{"title": "Gemini 2.5 for robotics and embodied intelligence", "url": "https://developers.googleblog.com/en/gemini-25-for-robotics-and-embodied-intelligence/", "content": "The latest generation of Gemini models, 2.5 Pro and Flash, are unlocking new frontiers in robotics. Their advanced coding, reasoning, and multimodal capabilities, now combined with spatial understanding, provide the foundation for the next generation of interactive and intelligent robots. This post explores how developers can leverage Gemini 2.5 to build sophisticated robotics applications. We'll provide practical examples with prompts to show using Gemini 2.5 and the Live API for: Semantic scene understanding for complex queries: Identify and label objects from robot camera feeds. Understand complex queries through multimodal reasoning. Combine spatial reasoning with code generation to control robots: Use the robot's API to call functions and bring task plans to life. Build interactive robotics applications with the Live API: Convert voice commands into executable robot plans. In March, we launched our Gemini Robotics models, including Gemini Robotics-ER, our advanced embodied reasoning model optimized for the unique demands of robotics applications. We\u2019re also excited to share how our Gemini Robotics trusted testers are already demonstrating the power of Gemini in robotics applications. We are including examples from Agile Robots, Agility Robotics, Boston Dynamics, and Enchanted Tools. Join the Gemini Robotics-ER trusted tester program waitlist.\n\nSemantic scene understanding for complex queries Reasoning about the physical world is at the core of general and robust control. Gemini 2.5 represents a step in this direction with its improved ability to reason multimodally. Below we share two examples, utilizing Gemini\u2019s pointing and object detection capabilities. Pointing allows a model to refer to entities or parts of entities precisely, and locate them in space. Gemini 2.5 Pro is able to reason about the entities it is pointing to, opening new opportunities for interacting with images. For example, Gemini 2.5 Pro is able to reason about empty space in the context of a supermarket display, knowing that this indicates restocking may be needed. In the example below, Gemini identifies the baby eggplant needs restocking. Gemini 2.5 Pro also shows a nascent ability to locate and read information from that location, as illustrated in the gauge example. Example 1: Gemini 2.5 can locate objects in the scene based on fine-grained language descriptions, for example, find a shelf that needs restocking. Prompt: Point to one bin on the shelf that needs restocking. The answer should follow the json format: [{\"point\": <point>, \"label\": <label1>}, ...]. The points are in [y, x] format normalized to 0-1000. Input image with response overlay:\n\nExample 2: Gemini 2.5 can locate small objects in the scene and estimate states of those objects. For example, it can read gauges. Prompt: Point to all the round gauges. The answer should follow the json format: [{\"point\": <point>, \"label\": <label1>}, ...]. The points are in [y, x] format normalized to 0-1000. Input image with response overlay:\n\nPrompt: What is the reading on the middle gauge? Response: Based on the close-up view, the round gauge in the center-left of the image appears to be reading 0. The needle is pointing directly at the \"0\" mark on the dial.\n\nGoing beyond object-centric perception Gemini 2.5 is able to accurately track multiple objects across time and detect open ended concepts like \u2018a spill\u2019. Gemini 2.5 can be prompted into trajectory prediction in the form of a sequence of points. Example 1: Gemini 2.5 can generate bounding boxes for each frame in a video and be visualized like below. Prompt: Detect green bowl, crab, wallet, pink bowl, phone, return a json array with keys box_2d and label. (executed per frame). Input image with response overlay:\n\nSorry, your browser doesn't support playback for this video\n\nExample 2: Gemini 2.5 can detect open-ended concepts relevant to robotics, requiring commonsense knowledge and context specific reasoning. For example, a helpful robot needs to understand the concept of a \u201cspill\u201d. Prompt: 1) Show me the bounding box of spill. Return in a json array with keys box_2d and label. 2) Give the segmentation masks for the spill. Output a JSON list of segmentation masks where each entry contains the 2D bounding box in the key \"box_2d\", the segmentation mask in key \"mask\", and the text label in the key \"label\". Input image with response overlay:\n\nExample 3: Gemini 2.5 can be prompted into trajectory prediction in the form of a sequence of points. Prompt: Generate a robot arm trajectory of 10 points to move the cloth to the spill. The answer should follow the json format: [{\"point\": <point>, \"label\": <label1>}, ...]. The points are in [y, x] format normalized to 0-1000. Input image with response overlay:\n\nUsing spatial understanding and code generation to control robots Gemini 2.5 can utilize its underlying spatial understanding to control robots through code generation. By providing Gemini 2.5 with a robot control API, it can apply advanced capabilities in scene understanding, object manipulation, and code writing together to perform tasks zero-shot, with no additional training. Example 1 below showcases code-generation for \u201cPut the banana in the bowl\u201d. It gives Gemini access to a robot control API and shows how the model leverages its spatial understanding, thinking, and code generation capabilities to select the appropriate API calls and arguments given the task. Gemini 2.5 generates 2 different feasible plans for putting the banana in the bowl. The first solution is to simply pick up the banana, move it above the bowl, and drop it. The second solution lifts the banana, moves the bowl below the banana, and then drops the banana. Example 1: Gemini 2.5 Generate high-level planning code for pick-and-place tasks with different strategies. Prompt: You are given a robot control API and example code below:\n\n# Provided API class RobotAPI: def detect_object(self, obj: str) -> Detection \"\"\"Detects the given object's XYZ location\"\"\" def get_grasp_position_and_euler_orientation(self, gripper: str, object_name: str) -> tuple[numpy.ndarray, numpy.ndarray]: \"\"\"Returns the grasp position and orientation for the given object and gripper from the Gemini Robotics ER model or other grasping primitives. def move_gripper_to(self, gripper: str, position: numpy.ndarray, orientation: numpy.ndarray): \"\"\"Moves the gripper to the given position and orientation.\"\"\" def move_gripper_to_safe_position(self, gripper: str): \"\"\"Moves the given gripper to a safe position out of the table area.\"\"\" def open_gripper(self, gripper: str): \"\"\"Opens the given gripper.\"\"\" def close_gripper(self, gripper: str): \"\"\"Closes the given gripper.\"\"\" robot=RobotAPI() # Example code robot.open_gripper(gripper='RIGHT') banana_detection = robot.detect_object(obj='banana') banana_position = banana_detection.position print(f\"Detected banana position: {banana_position}\") # Gemini uses this to determine which gripper to use. grasp_position, grasp_orientation = robot.get_grasp_position_and_euler_orientation(gripper='RIGHT', object_name='banana') pre_grasp_position = grasp_position + np.array([0, 0, 0.1]) # Add 10cm to the Z-coordinate for pre-grasp robot.move_gripper_to(gripper='RIGHT', position=pre_grasp_position, orientation=grasp_orientation) robot.move_gripper_to(gripper='RIGHT', position=grasp_position, orientation=grasp_orientation) robot.close_gripper(gripper='RIGHT') Python Copied\n\nNow generate code for the task \u201cput the banana in the bowl\u201d. Response 1 with results of code executed: Reasoning (abbreviated) 1: The current position of the right gripper is [-0.04, -0.028, 0.176] . 2: The bowl's detected position is [-0.03839983, -0.02811164, 0.0073104] . 3: To place the banana inside the bowl, the gripper's Z-coordinate needs to be lowered. I will use the bowl's detected X and Y coordinates for the target position. For the Z-coordinate, I will use the bowl's detected Z-coordinate ( 0.0073104 ) and add a small offset (e.g., 0.03m) to ensure the banana is placed inside the bowl and not through the table. This would result in a target Z of approximately 0.007 + 0.03 = 0.037m . This Z-coordinate is higher than the bowl's base but lower than the typical height of a banana when standing on the table, ensuring it's placed *into* the bowl. Code Snippet:\n\n# Detect the bowl's position (re-detect to ensure fresh data, though it's unlikely to have moved) bowl_detection = robot.detect_object('bowl') if bowl_detection.position is None: raise ValueError(\"Bowl not detected.\") # Define the target Z-coordinate for dropping the banana into the bowl # Use the bowl's Z-position and add an offset to place the banana inside. # The banana's original Z was ~0.032, bowl's Z is ~0.007. # A target Z of 0.037 (0.007 + 0.03) should place it inside the bowl. drop_z = bowl_detection.position[2] + 0.03 Python Copied\n\nSorry, your browser doesn't support playback for this video\n\nResponse 2 with results of code executed: Reasoning (abbreviated) # Re-planning due to reachability constraints: The RIGHT arm cannot reach the bowl's current position. # The plan will now involve the LEFT arm moving the bowl to the RIGHT arm's reachable area. # The RIGHT arm can reach x-coordinates from 0.04m to 0.40m. I will move the bowl to an x-coordinate of 0.10m, which is well within the RIGHT arm's reach and also allows for some clearance from the center.\n\nCode Snippet:\n\ntarget_bowl_position = [-0.07274145, -0.08317622, 0.00720479] # Original bowl position # Move the bowl to an x-coordinate reachable by the RIGHT arm, keeping the same y and z (lifted) # Let's choose x = 0.10m for the bowl, which is within the right arm's reach (0.04 to 0.40) # Keep the lifted height at 0.20m move_bowl_position = [0.10, bowl_detection.position[1], 0.20] robot.move_gripper_to('LEFT', move_bowl_position, current_left_gripper_orientation) Python Copied\n\nSorry, your browser doesn't support playback for this video\n\nGemini 2.5 can also effectively utilize a small number of in-context examples to perform more dexterous robot control tasks. In the two examples shown below of an ALOHA robot packing boxes and folding a dress, 10 demonstrations consisting of interleaved reasoning and robot actions for each task were added to Gemini\u2019s context. We\u2019ve created open-source code showing how to do this using Gemini, including examples of the input demonstrations. This enables robots to be taught and deployed on the spot. See the Colab. Example 2: Gemini 2.5 (Flash) utilizes a small number of in-context examples to perform more dexterous robot control tasks. Prompt: see colab. Response with results of code executed:\n\nSorry, your browser doesn't support playback for this video\n\nSorry, your browser doesn't support playback for this video\n\nBuilding interactive robotics applications with the Live API The Live API for realtime streaming was recently introduced and can be used to build interactive applications that let people control robots using their voice. Intuitive human-robot-interaction is an important aspect of making robots that are easy and safe to use. We recently showcased an interactive Gemini Robotics demo at I/O 2025, which was built around Live API for voice interaction and function calling. Live API supports both audio and video as input modalities, and audio / text as output modalities. This allows you to send both voice input and the robot camera feed to the Live API. This is even more powerful when combined with tool use. Tool use allows Live API to go beyond just conversation by enabling it to perform actions in the real-world while maintaining a real time connection. For example, the robot APIs defined above can be defined as function calls including robot.open_gripper() , robot.close_gripper() and robot.move_gripper_to() . After they are defined as tool calls, they can be integrated into the workflow where people can interact with the robot using voice in real time. Developers can get started on GitHub, and refer to API documentation for function calling features.\n\nSorry, your browser doesn't support playback for this video \ud83d\udd0a Demonstration of a realtime web console for robotics built with Live API, right click to open in a new tab for audio.\n\nSafety The 2.5 Pro and 2.5 Flash models demonstrate robust performance on the ASIMOV Multimodal and Physical Injury benchmarks released along with the Gemini Robotics tech report, exhibiting accuracy comparable to that of 2.0 models. Beyond the ASIMOV benchmarks, the 2.5 Pro and 2.5 Flash models also exhibit excellent performance in rejecting prompts that attempt to leverage embodied reasoning capabilities while violating safety policies such as promoting harmful stereotypes, discrimination, or endangerment of minors. Following rigorous evaluation against such synthetically generated adversarial prompts, 2.5 Pro and Flash demonstrated near-zero violation rates.\n\nHow Gemini is being used today for Robotics In March we released the Gemini Robotics-ER model and we\u2019re already inspired by how the community is using it for robotics applications. Check out these examples of interactivity, perception, planning, and function calling from our trusted testers: Agile Robots, Agility Robotics, Boston Dynamics, and Enchanted Tools.\n\nSorry, your browser doesn't support playback for this video\n\nSorry, your browser doesn't support playback for this video\n\nSorry, your browser doesn't support playback for this video\n\nSorry, your browser doesn't support playback for this video", "label": 0}
{"title": "How we use Lakera Guard to secure our LLMs", "url": "https://dropbox.tech/security/how-we-use-lakera-guard-to-secure-our-llms", "content": "From search to organization, rapid advancements in artificial intelligence (AI) have made it easier for Dropbox users to discover and interact with their files. However, these advancements can also introduce new security challenges. Large Language Models (LLMs), integral to some of our most recent intelligent features, are also susceptible to various threats\u2014from data breaches and adversarial attacks to exploitation by malicious actors. While hundreds of millions of users already trust Dropbox to protect their content, ensuring the security and integrity of these models is essential for maintaining that trust. Last year we evaluated several security solutions to help safeguard our LLM-powered applications and ultimately chose Lakera Guard. With its robust capabilities, Lakera Guard helps us secure and protect user data, and\u2014as outlined in our AI principles\u2014uphold the reliability and trustworthiness of our intelligent features. Addressing these challenges requires a multifaceted approach, incorporating stringent security protocols, continuous monitoring, and proactive risk management strategies. In this story, we\u2019ll share insights into our approach to securing our LLMs, the criteria we used to evaluate potential solutions, and the key benefits of implementing Lakera's technology.\n\nDropbox Dash: Find anything. Protect everything. Find, organize, and protect your work with Dropbox Dash. Now with advanced search for video and images\u2014plus generative AI capabilities across even more connected apps. See what's new \u2192\n\nWhat we were looking for\n\nLLM security is comprised of many parts. Common problems include reliability, consistency, alignment, and adversarial attacks. However, the scope of the problem we were trying to solve was more customer-centric\u2014specifically, using LLMs to chat about, summarize, transcribe, and retrieve information, in addition to agent/assistant use cases. These kinds of untrusted user inputs could result in moderation issues or prompt injection\u2014a method sometimes used to manipulate models\u2014which creates a lot of headaches, including undesirable model outputs. We considered a variety of open source, in-house, and proprietary options before narrowing our criteria to either open source or commercial tools. Whatever we chose, we decided the following requirements were mandatory: We couldn\u2019t call out to a third party. The solution had to be deployable in-house on our existing infrastructure.\n\nThe solution had to be deployable in-house on our existing infrastructure. Low latency. Dropbox is committed to maximizing performance for users across all of its products. We couldn\u2019t add latency to LLM-powered features any more than absolutely necessary, so we determined upper latency numbers with the product teams. Latency for a given context length is also an important sub-problem here. Many options perform well on context lengths of <800 tokens, but drop off significantly at >4,000. Excellent support for long context lengths\u2014the ability for models to process greater amounts of information\u2014was critical, as many customer use cases routinely exceed this number.\n\nDropbox is committed to maximizing performance for users across all of its products. We couldn\u2019t add latency to LLM-powered features any more than absolutely necessary, so we determined upper latency numbers with the product teams. Confidence scores. API integrations that not only allowed extensive control over the categories of blocking, but also the sensitivity, were key (eg., separating the danger classification jailbreak based on confidence scores in order to ensure we could meet the diverse needs of our product teams).\n\nAPI integrations that not only allowed extensive control over the categories of blocking, but also the sensitivity, were key (eg., separating the danger classification based on confidence scores in order to ensure we could meet the diverse needs of our product teams). Future intelligence and continuous improvement. Since LLM security is a fast evolving space, we wanted a solution that could also give us actionable insights into attacks and payloads in a rapidly shifting environment. In fact, given the rapidly shifting environment, our top priority was selecting a solution that gave us enough of a foothold to observe and reorient as needed.\n\nHow we tested\n\nOnce we had a short list of open-source and commercial tools that met our criteria, we set up each tool internally for evaluation. For our test suite, we used Garak, an open-source LLM vulnerability scanner customized to run Dropbox-specific security tests. With Garak, we could evaluate the security coverage of each of the potential solutions. This allowed us to conduct a range of tests involving prompt injection, jailbreak, and other security assessments developed by Dropbox. We then tested each solution directly against a range of LLMs already in use or under evaluation by our product teams. This enabled us to establish a baseline of each model\u2019s vulnerability. For example, if a security tool blocked 90% of malicious prompts, but the LLM had already mitigated 85% of these vulnerabilities, we measured a net improvement of only 5%. Finally, we needed a tool that did not add excessive latency to LLM calls and respected the privacy of customer data (e.g., did not store prompt content or send it outside the Dropbox network). For this, we measured the response time of each security test and also monitored network requests and file changes to detect any potential breaches of user data. After extensive testing, Lakera Guard emerged as the product meeting all our requirements, offering both the lowest latency and highest security coverage.\n\nHow we integrated Lakera Guard\n\nLakera provides a Docker container that we run as an internal service at Dropbox. This means Lakera Guard is just an RPC call away from any LLM pipeline. Conceptually, the LLM security architecture at Dropbox is designed using LangChain as shown in the figure below.\n\nA high-level representation of the internal AI/ML security infrastructure at Dropbox\n\nHere, a textual LLM prompt is directed through one or more prompt security chains before hitting the model. We have a security chain that makes Lakera Guard security API endpoint requests to our internally-hosted Docker container, which responds with confidence scores for prompt injection and jailbreak attacks. Dropbox services can then action on the returned Lakera Guard prompt security categories as appropriate for the application. Prompts that are deemed to be safe are then passed to the LLM\u2014either a third-party model, like GPT-4, or an internally hosted open-source model, like LLaMA 3, depending on the use case\u2014which produces a textual response. The LLM\u2019s response is then passed through our content moderation chains, which analyze the text for potentially harmful topics. The moderation chain calls out to Lakera\u2019s content moderation API endpoint to identify harassing or explicit content that the Dropbox feature or service can withhold from the user as configured. Integrating Lakera Guard into our Dropbox infrastructure was a gradual process. We started with one product directly calling the Lakera-provided Docker container. Eventually, we created a custom Dropbox service that can automatically scale up more containers as needed\u2014and can be called via the LLM security layer we built as part of Dropbox\u2019s central machine learning libraries.\n\nWhat we learned, and what\u2019s next", "label": 0}
{"title": "Heroku Managed Inference and Agents is now Generally Available", "url": "https://www.heroku.com/blog/managed-inference-and-agents-now-generally-available/", "content": "Many of the most exciting experiences we\u2019re beginning to rely on every day are powered by AI; whether it\u2019s conversational assistants, personalized recommendations or code generation, these experiences are powered by inference systems and intelligent agents. Behind the scenes, developers offload complex decisions, automate tasks, and compose intelligent applications using large language models and tool execution flows. Together, these AI-powered primitives are becoming a key complement to traditional application development, enabling a new wave of developer capabilities.\n\nAt Salesforce, we are helping our customers bring their agentic strategy to life with Heroku, Agentforce, and Data Cloud. These powerful products allow anyone in the company, from business analysts to developers to build robust, custom agents that can transform their business. Behind the scenes, developers offload complex decisions, automate tasks, and compose intelligent applications using large language models and tool execution flows. Together, these AI-powered primitives are becoming a key complement to traditional application development, enabling a new wave of developer capabilities.\n\nHeroku Managed Inference and Agents bring together a set of powerful primitives that make it simple for developers to build, scale, and operate AI-powered features and applications, without the heavy lifting of managing their own AI infrastructure. With access to leading models from top providers and elegant primitives for building agents that can reason, act, and call tools, developers can focus on delivering differentiated experiences for their users, rather than wrangling inference infrastructure or orchestration logic.\n\n\ufeff\n\nManaged Inference for simplified AI integration\n\nManaged Inference provides ready-to-use access to a curated set of powerful AI models, chosen for their generative power and performance, optimized for ease of use and efficacy in the domains our customers need most. Whether you\u2019re looking to generate text, classify content, summarize documents, or build intelligent workflows, you can now bring AI to your Heroku apps in seconds.\n\nGetting started is as easy as attaching the Heroku Managed Inference and Agents add-on to your app or running: heroku ai:models:create\n\nAgents with Model Context Protocol\n\nExtend Managed Inference with an elegant set of primitives and operations, allowing developers to create agents that can execute code in Heroku\u2019s trusted Dynos, as well as call tools and application logic. These capabilities allow agents to act on behalf of the customer, and to extend both application logic and platform capabilities. Allowing developers to interleave application code, calls to AI, execute logic created by AI, and use of tools, all within the programmatic context. Heroku Managed Inference and Agents can now do more than just generate, it can reason, act, and build by adapting to context, and evolving with your users\u2019 needs.\n\nHeroku Managed Inference and Agents uses the Model Context protocol (MCP) to give your agents new capabilities. MCP helps you build agents and complex workflows by standardizing the way you can provide context and integrate tools. This means you can expose your app\u2019s logic, APIs or custom tools to agents such as Agentforce, Claude or Cursor with custom code.\n\nHeroku Managed Inference and Agents currently supports STDIO MCP servers. Attaching your MCP servers is as simple as attaching your add-on to your Heroku app which contains the MCP server. We are actively developing platform capabilities to support remote MCP servers hosted on heroku, which will feature OAuth integration and buildpack capabilities.\n\nWhat\u2019s next\n\nHeroku Managed Inference and Agents marks a major milestone on our journey to provide AI-native capabilities on the platform and we\u2019ve designed it with the graceful developer and operator experiences you\u2019ve come to expect. Combined with MCP Server support, AppLink for Agentforce integration, and an evolving selection of curated models and tools, developers will be able to rapidly integrate the latest AI advancements and create next-generation, intelligent user experiences.\n\nAgain, to get started, provision Managed Inference and Agents from Heroku Elements or via the command line. We are excited to see what you build with Heroku Managed Inference and Agents! Attend our webinar on May 28 to see a demo and get your questions answered!\n\nTo learn more about Heroku AI, check out our Dev Center docs and try it out for yourself.\n\nInterested in unlocking the full potential of your AI agents? Read Heroku AI: Build and Deploy Enterprise Grade MCP Servers.\n\nStay tuned for more \u2014 we\u2019re just getting started.", "label": 0}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2025-04", "content": "en\n\nThis post is a response to my blog post. I am answering question #91: What makes a good friend?\n\nWhat makes a good friend? That's a great question. I think everyone has a different definition of what makes a good friend. I will share mine.\n\nOne of the most important things to understand about friendship is point's of view. We don't have to always agree with each other. We do need to be able to listen, process and be able to provide counter points of view in a respectable manner. Allowing time to process is important instead of reacting as the default.\n\nOne of the most important qualities of a good friend is being supportive. I'm very fortunate to have friends who are supportive and emotionally connected. We check in with each other, we talk about our emotions, we talk about our problems. As men, we are human and we should talk about what's happening to our lives. This is 2025.\n\nA good friend will always make an effort to stay in touch especially when you live far away from each other. This is a two way streak. One person can't solely be the one actively reaching out. It's okay if you go a little while without talking, just don't let your friendship die out. Life gets busy but we can't forget who our friends are.\n\nWhat makes a good friend?\n\nThis post was written at Ikea Restaurant in San Francisco.", "label": 1}
{"title": "New Google Pay features to enhance your payment flows", "url": "https://developers.googleblog.com/en/new-google-pay-features-to-enhance-your-payment-flows/", "content": "At Google I/O 2025, we unveiled updates across the Google Pay API designed to help you create smoother, safer, and more successful checkout experiences for your users. Whether you're looking to boost conversions, enable new payment scenarios, enhance security, or simplify your integration, there's something new for you. Let's dive into the key announcements developers need to know.\n\nEnhancing the checkout experience and conversion Google Pay in Android WebViews Big news! Starting with Chrome v137, users can seamlessly use Google Pay within Android WebViews, accessing an Android native experience and device tokens from their Google Wallet. Simply enable PaymentRequest in your app manifest, and tap into the opportunities of in-app browser purchases with a high-quality, secure form of payment. Take a look at the integration guide to learn more.\n\nSorry, your browser doesn't support playback for this video Figure 1: A sample checkout flow via a WebView on Android that uses Google Pay to complete the payment\n\nA more versatile API to power modern checkout flows We are introducing improvements to the Google Pay API to help you adapt to a payment ecosystem that is in continuous evolution. Here are some of our favorite updates: The Google Pay payment sheet now features richer card art and names, helping users select their preferred card faster. The payment sheet also supports dark mode for a more integrated feel within your application.\n\nFigure 2: Screenshots showcasing the Google Pay payment sheet in their dark and light versions.\n\nBuilding on last year's success, the createButton API for Web now offers more customization options (show/hide border, more button text options) to better match your UI, and continue to help boost sales by showing card details upfront. Need to show card-identifying information without using a payment button? We're introducing a new API in the coming months to enable this use case.\n\nfigure 3: An example offering Google Pay using a list selector through the Payment Metadata API\n\nWe're adding support for Merchant-Initiated Transactions (MITs) (subscriptions, auto-reloads, deferred charges) to the Google Pay Online API. This includes details in the payment sheet to inform users, device-independent tokens for payment continuity (even if users change devices), and lifecycle notifications for underlying card changes.\n\nStreamlining the developer experience We are dedicated to making the Google Pay API easier to integrate, test and maintain. Here are some updates that improve the integration experience: Testing just got easier. We have improved the test card suite, so you can now see relevant test cards (regular, tokenized, debit) for your specific PSP directly in the payments sheet when using the TEST environment. Debug your integrations faster on Android with more fine-grained build time error logs to amend your logic more easily, and detailed exceptions/error codes at runtime. Check out the troubleshooting guide if you are seeing errors in your integration.\n\n\n\nFigure 4: More detailed error messages are now surfaced via the Logcat and the debugger\n\nWe've launched new codelabs, Firebase Studio templates (one-click development environments), and a learning pathway for web developers. We are planning to add similar resources for native Android, Flutter, React JS, React Native, and Angular. Stay informed about the status of the Google Pay API with the new Google Pay API Status Dashboard. The dashboard monitors key APIs like the CreateButton, IsReadyToPay, or LoadPaymentData APIs in real-time. Check the availability of the API (99.99% uptime last year!) and get incident updates instantly.\n\nFigure 5: The Google Pay API Status Dashboard includes service uptime and health information.", "label": 0}
{"title": "IssueOps: Automate CI/CD (and more!) with GitHub Issues and Actions", "url": "https://github.blog/engineering/issueops-automate-ci-cd-and-more-with-github-issues-and-actions/", "content": "Software development is filled with repetitive tasks\u2014managing issues, handling approvals, triggering CI/CD workflows, and more. But what if you could automate these types of tasks directly within GitHub Issues? That\u2019s the promise of IssueOps, a methodology that turns GitHub Issues into a command center for automation.\n\nWhether you\u2019re a solo developer or part of an engineering team, IssueOps helps you streamline operations without ever leaving your repository.\n\nIn this article, I\u2019ll explore the concept of IssueOps using state-machine terminology and strategies to help you work more efficiently on GitHub. After all, who doesn\u2019t love automation?\n\nWhat is IssueOps?\n\nIssueOps is the practice of using GitHub Issues, GitHub Actions, and pull requests (PR) as an interface for automating workflows. Instead of switching between tools or manually triggering actions, you can use issue comments, labels, and state changes to kick off CI/CD pipelines, assign tasks, and even deploy applications.\n\nMuch like the various other *Ops paradigms (ChatOps, ClickOps, and so on), IssueOps is a collection of tools, workflows, and concepts that, when applied to GitHub Issues, can automate mundane, repetitive tasks. The flexibility and power of issues, along with their relationship to pull requests, create a near limitless number of possibilities, such as managing approvals and deployments. All of this can really help to simplify your workflows on GitHub. I\u2019m speaking from personal experience here.\n\nIt\u2019s important to note that IssueOps isn\u2019t just a DevOps thing! Where DevOps offers a methodology to bring developers and operations into closer alignment, IssueOps is a workflow automation practice centered around GitHub Issues. IssueOps lets you run anything from complex CI/CD pipelines to a bed and breakfast reservation system. If you can interact with it via an API, there\u2019s a good chance you can build it with IssueOps!\n\nSo, why use IssueOps?\n\nThere are lots of benefits to utilizing IssueOps. Here\u2019s how it\u2019s useful in practice:\n\nIt\u2019s event driven, so you can automate the boring stuff: IssueOps lets you automate workflows directly from GitHub Issues and pull requests, turning everyday interactions\u2014from kicking off a CI/CD pipeline and managing approvals to updating project boards\u2014into powerful triggers for GitHub Actions.\n\nIssueOps lets you automate workflows directly from GitHub Issues and pull requests, turning everyday interactions\u2014from kicking off a CI/CD pipeline and managing approvals to updating project boards\u2014into powerful triggers for GitHub Actions. It\u2019s customizable, so you can tailor workflows to your needs: No two teams work the same way, and IssueOps is flexible enough to adapt. Whether you\u2019re automating bug triage or triggering deployments, you can customize workflows based on event type and data provided.\n\nIt\u2019s transparent, so you can keep a record: All actions taken on an issue are logged in its timeline, creating an easy-to-follow record of what happened and when.\n\nIt\u2019s immutable, so you can audit whenever you need: Because IssueOps uses GitHub Issues and pull requests as a source of truth, every action leaves a record. No more chasing approvals in Slack or manually triggering workflows: IssueOps keeps everything structured, automated, and auditable right inside GitHub.\n\nOur quickstart guide to IssueOps Step 1: Define your triggers\n\nIdentify the actions that should kick off your workflows\u2014like opening an issue, adding a label, or merging a pull request. These events can serve as triggers for GitHub Actions.\n\nIdentify the actions that should kick off your workflows\u2014like opening an issue, adding a label, or merging a pull request. These events can serve as triggers for GitHub Actions. Step 2: Configure GitHub Actions Use GitHub Actions to define what happens when an event occurs. For example, if an issue is labeled deploy, you could trigger a deployment script. YAML never looked so good.\n\nStep 3: Test and iterate Like any good automation, IssueOps workflows should be tested and refined. Start small, see what works, and expand from there. Let\u2019s go: Learn more in our repository.\n\nDefining IssueOps workflows and how they\u2019re like finite-state machines\n\nMost IssueOps workflows follow the same basic pattern:\n\nA user opens an issue and provides information about a request The issue is validated to ensure it contains the required information The issue is submitted for processing Approval is requested from an authorized user or team The request is processed and the issue is closed\n\nSuppose you\u2019re an administrator of an organization and want to reduce the overhead of managing team members. In this instance, you could use IssueOps to build an automated membership request and approval process. Within a workflow like this, you\u2019d have several core steps:\n\nA user creates a request to be added to a team The request is validated The request is submitted for approval An administrator approves or denies this request The request is processed If approved, the user is added to the team If denied, the user is not added to the team The user is notified of the outcome\n\nWhen designing your own IssueOps workflows, it can be very helpful to think of them as a finite-state machine: a model for how objects move through a series of states in response to external events. Depending on certain rules defined within the state machine, a number of different actions can take place in response to state changes. If this is a little too complex, you can also think of it like a flow chart.\n\nTo apply this comparison to IssueOps, an issue is the object that is processed by a state machine. It changes state in response to events. As the object changes state, certain actions may be performed as part of a transition, provided any required conditions (guards) are met. Once an end state is reached, the issue can be closed.\n\nThis breaks down into a few key concepts:\n\nState : A point in an object\u2019s lifecycle that satisfies certain condition(s).\n\n: A point in an object\u2019s lifecycle that satisfies certain condition(s). Event : An external occurrence that triggers a state change.\n\n: An external occurrence that triggers a state change. Transition : A link between two states that, when traversed by an object, will cause certain action(s) to be performed.\n\n: A link between two states that, when traversed by an object, will cause certain action(s) to be performed. Action : An atomic task that is performed when a transition is taken.\n\n: An atomic task that is performed when a transition is taken. Guard: A condition that is evaluated when a trigger event occurs. A transition is taken only if all associated guard condition(s) are met.\n\nHere\u2019s a simple state diagram for the example I discussed above.\n\nNow, let\u2019s dive into the state machine in more detail!\n\nKey concepts behind state machines\n\nThe benefit of breaking your workflow down into these components is that you can look for edge cases, enforce conditions, and create a robust, reliable result.\n\nStates\n\nWithin a state machine, a state defines the current status of an object. As the object transitions through the state machine, it will change states in response to external events. When building IssueOps workflows, common states for issues include opened, submitted, approved, denied, and closed.\n\nThese should suffice as the core states to consider when building our workflows in our team membership example above.\n\nEvents\n\nIn a state machine, an event can be any form of interaction with the object and its current state. When building your own IssueOps, you should consider events from both the user and GitHub points of view.\n\nIn our team membership request example, there are several events that can trigger a change in state. The request can be created, submitted, approved, denied, or processed.\n\nIn this example, a user interacting with an issue\u2014such as adding labels, commenting, or updating milestones\u2014can also change its state. In GitHub Actions, there are many events that can trigger your workflows (see events that trigger workflows).\n\nHere are a few interactions, or events, that would affect our example IssueOps workflow when it comes to managing team members:\n\nRequest Event State Request is created issues opened Request is approved issue_comment created Request is denied issue_comment created\n\nAs you can see, the same GitHub workflow trigger can apply to multiple events in our state machine. Because of this, validation is key. Within your workflows, you should check both the type of event and the information provided by the user. In this case, we can conditionally trigger different workflow steps based on the content of the issue_comment event.\n\njobs: approve: name: Process Approval runs-on: ubuntu-latest if: ${{ startsWith(github.event.comment.body, '.approve') }} # ... deny: name: Process Denial runs-on: ubuntu-latest if: ${{ startsWith(github.event.comment.body, '.deny') }} # ...\n\nTransitions\n\nA transition is simply the change from one state to another. In our example, for instance, a transition occurs when someone opens an issue. When a request meets certain conditions, or guards, the change in state can take place. When the transition occurs, some actions or processing may take place, as well.\n\nWith our example workflow, you can think of the transitions themselves as the lines connecting different nodes in the state diagram. Or the lines connecting boxes in a flow chart.\n\nGuards\n\nGuards are conditions that must be verified before an event can trigger a transition to a different state. In our case, we know the following guards must be in place:\n\nA request should not transition to an Approved state unless an administrator comments .approve on the issue.\n\non the issue. A request should not transition to a Denied state unless an administrator comments .deny on the issue.\n\nWhat about after the request is approved and the user is added to the team? This is referred to as an unguarded transition. There are no conditions that must be met, so the transition happens immediately!\n\nActions\n\nLastly, actions are specific tasks that are performed during a transition. They may affect the object itself, but this is not a requirement in our state machine. In our example, the following actions may take place at different times:\n\nAdministrators are notified that a request has been submitted\n\nThe user is added to the requested team\n\nThe user is notified of the outcome\n\nA real-world example: Building a team membership workflow with IssueOps\n\nNow that all of the explanation is out of the way, let\u2019s dive into building our example! For reference, we\u2019ll focus on the GitHub Actions workflows involved in building this automation. There are some additional repository and permissions settings involved that are discussed in more detail in these IssueOps docs.\n\nStep 1: Issue form template\n\nGitHub issue forms let you create standardized, formatted issues based on a set of form fields. Combined with the issue-ops/parser action, you can get reliable, machine-readable JSON from issue body Markdown. For our example, we are going to create a simple form that accepts a single input: the team where we want to add the user.\n\nname: Team Membership Request description: Submit a new membership request title: New Team Membership Request labels: - team-membership body: - type: input id: team attributes: label: Team Name description: The team name you would like to join placeholder: my-team validations: required: true\n\nWhen issues are created using this form, they will be parsed into JSON, which can then be passed to the rest of the IssueOps workflow.\n\n{ \"team\": \"my-team\" }\n\nStep 2: Issue validation\n\nWith a machine-readable issue body, we can run additional validation checks to ensure the information provided follows any rules we might have in place. For example, we can\u2019t automatically add a user to a team if the team doesn\u2019t exist yet! That is where the issue-ops/validator action comes into play. Using an issue form template and a custom validation script, we can confirm the existence of the team ahead of time.\n\nmodule.exports = async (field) => { const { Octokit } = require('@octokit/rest') const core = require('@actions/core') const github = new Octokit({ auth: core.getInput('github-token', { required: true }) }) try { // Check if the team exists core.info(`Checking if team '${field}' exists`) await github.rest.teams.getByName({ org: process.env.GITHUB_REPOSITORY_OWNER ?? '', team_slug: field }) core.info(`Team '${field}' exists`) return 'success' } catch (error) { if (error.status === 404) { // If the team does not exist, return an error message core.error(`Team '${field}' does not exist`) return `Team '${field}' does not exist` } else { // Otherwise, something else went wrong... throw error } } }\n\nWhen included in our IssueOps workflow, this adds any validation error(s) to the comment on the issue.\n\nStep 3: Issue workflows\n\nThe main \u201centrypoint\u201d of this workflow occurs when a user creates or edits their team membership request issue. This workflow should focus heavily on validating any user inputs! For example, what should happen if the user inputs a team that does not exist?\n\nIn our state machine, this workflow is responsible for handling everything up to the opened state. Any time an issue is created, edited, or updated, it will re-run validation to ensure the request is ready to be processed. In this case, an additional guard condition is introduced. Before the request can be submitted, the user must comment with .submit after validation has passed.\n\nname: Process Issue Open/Edit on: issues: types: - opened - edited - reopened permissions: contents: read id-token: write issues: write jobs: validate: name: Validate Request runs-on: ubuntu-latest # This job should only be run on issues with the `team-membership` label. if: ${{ contains(github.event.issue.labels.*.name, 'team-membership') }} steps: # This is required to ensure the issue form template and any validation # scripts are included in the workspace. - name: Checkout id: checkout uses: actions/checkout@v4 # Since this workflow includes custom validation scripts, we need to # install Node.js and any dependencies. - name: Setup Node.js id: setup-node uses: actions/setup-node@v4 # Install dependencies from `package.json`. - name: Install Dependencies id: install run: npm install # GitHub App authentication is required if you want to interact with any # resources outside the scope of the repository this workflow runs in. - name: Get GitHub App Token id: token uses: actions/create-github-app-token@v1 with: app-id: ${{ vars.ISSUEOPS_APP_ID }} private-key: ${{ secrets.ISSUEOPS_APP_PRIVATE_KEY }} owner: ${{ github.repository_owner }} # Remove any labels and start fresh. This is important because the # issue may have been closed and reopened. - name: Remove Labels id: remove-label uses: issue-ops/labeler@v2 with: action: remove github_token: ${{ steps.token.outputs.token }} labels: | validated approved denied issue_number: ${{ github.event.issue.number }} repository: ${{ github.repository }} # Parse the issue body into machine-readable JSON, so that it can be # processed by the rest of the workflow. - name: Parse Issue Body id: parse uses: issue-ops/parser@v4 with: body: ${{ github.event.issue.body }} issue-form-template: team-membership.yml workspace: ${{ github.workspace }} # Validate early and often! Validation should be run any time an issue is # interacted with, to ensure that any changes to the issue body are valid. - name: Validate Request id: validate uses: issue-ops/validator@v3 with: add-comment: true github-token: ${{ steps.token.outputs.token }} issue-form-template: team-membership.yml issue-number: ${{ github.event.issue.number }} parsed-issue-body: ${{ steps.parse.outputs.json }} workspace: ${{ github.workspace }} # If validation passes, add the validated label to the issue. - if: ${{ steps.validate.outputs.result == 'success' }} name: Add Validated Label id: add-label uses: issue-ops/labeler@v2 with: action: add github_token: ${{ steps.token.outputs.token }} labels: | validated issue_number: ${{ github.event.issue.number }} repository: ${{ github.repository }} # The `issue-ops/validator` action will automatically notify the user that # the request was validated. However, you can optionally add instruction # on what to do next. - if: ${{ steps.validate.outputs.result == 'success' }} name: Notify User (Success) id: notify-success uses: peter-evans/create-or-update-comment@v4 with: issue-number: ${{ github.event.issue.number }} body: | Hello! Your request has been validated successfully! Please comment with `.submit` to submit this request.\n\nOnce the issue is created, any further processing is triggered using issue comments\u2014and this can be done with one workflow. However, to make things a bit easier to follow, we\u2019ll break this into a few separate workflows.\n\nSubmit workflow\n\nThe first workflow handles the user submitting the request. The main task it performs is validating the issue body against the form template to ensure it hasn\u2019t been modified.\n\nname: Process Submit Comment on: issue_comment: types: - created permissions: contents: read id-token: write issues: write jobs: submit: name: Submit Request runs-on: ubuntu-latest # This job should only be run when the following conditions are true: # # - A user comments `.submit` on the issue. # - The issue has the `team-membership` label. # - The issue has the `validated` label. # - The issue does not have the `approved` or `denied` labels. # - The issue is open. if: | startsWith(github.event.comment.body, '.submit') && contains(github.event.issue.labels.*.name, 'team-membership') == true && contains(github.event.issue.labels.*.name, 'approved') == false && contains(github.event.issue.labels.*.name, 'denied') == false && github.event.issue.state == 'open' steps: # First, we are going to re-run validation. This is important because # the issue body may have changed since the last time it was validated. # This is required to ensure the issue form template and any validation # scripts are included in the workspace. - name: Checkout id: checkout uses: actions/checkout@v4 # Since this workflow includes custom validation scripts, we need to # install Node.js and any dependencies. - name: Setup Node.js id: setup-node uses: actions/setup-node@v4 # Install dependencies from `package.json`. - name: Install Dependencies id: install run: npm install # GitHub App authentication is required if you want to interact with any # resources outside the scope of the repository this workflow runs in. - name: Get GitHub App Token id: token uses: actions/create-github-app-token@v1 with: app-id: ${{ vars.ISSUEOPS_APP_ID }} private-key: ${{ secrets.ISSUEOPS_APP_PRIVATE_KEY }} owner: ${{ github.repository_owner }} # Remove the validated label. This will be re-added if validation passes. - name: Remove Validated Label id: remove-label uses: issue-ops/labeler@v2 with: action: remove github_token: ${{ steps.token.outputs.token }} labels: | validated issue_number: ${{ github.event.issue.number }} repository: ${{ github.repository }} # Parse the issue body into machine-readable JSON, so that it can be # processed by the rest of the workflow. - name: Parse Issue Body id: parse uses: issue-ops/parser@v4 with: body: ${{ github.event.issue.body }} issue-form-template: team-membership.yml workspace: ${{ github.workspace }} # Validate early and often! Validation should be run any time an issue is # interacted with, to ensure that any changes to the issue body are valid. - name: Validate Request id: validate uses: issue-ops/validator@v3 with: add-comment: false # Don't add another validation comment. github-token: ${{ steps.token.outputs.token }} issue-form-template: team-membership.yml issue-number: ${{ github.event.issue.number }} parsed-issue-body: ${{ steps.parse.outputs.json }} workspace: ${{ github.workspace }} # If validation passed, add the validated and submitted labels to the issue. - if: ${{ steps.validate.outputs.result == 'success' }} name: Add Validated Label id: add-label uses: issue-ops/labeler@v2 with: action: add github_token: ${{ steps.token.outputs.token }} labels: | validated submitted issue_number: ${{ github.event.issue.number }} repository: ${{ github.repository }} # If validation succeeded, alert the administrator team so they can # approve or deny the request. - if: ${{ steps.validate.outputs.result == 'success' }} name: Notify Admin (Success) id: notify-success uses: peter-evans/create-or-update-comment@v4 with: issue-number: ${{ github.event.issue.number }} body: | \ud83d\udc4b @issue-ops/admins! The request has been validated and is ready for your review. Please comment with `.approve` or `.deny` to approve or deny this request.\n\nDeny workflow\n\nIf the request is denied, the user should be notified and the issue should close.\n\nname: Process Denial Comment on: issue_comment: types: - created permissions: contents: read id-token: write issues: write jobs: submit: name: Deny Request runs-on: ubuntu-latest # This job should only be run when the following conditions are true: # # - A user comments `.deny` on the issue. # - The issue has the `team-membership` label. # - The issue has the `validated` label. # - The issue has the `submitted` label. # - The issue does not have the `approved` or `denied` labels. # - The issue is open. if: | startsWith(github.event.comment.body, '.deny') && contains(github.event.issue.labels.*.name, 'team-membership') == true && contains(github.event.issue.labels.*.name, 'submitted') == true && contains(github.event.issue.labels.*.name, 'validated') == true && contains(github.event.issue.labels.*.name, 'approved') == false && contains(github.event.issue.labels.*.name, 'denied') == false && github.event.issue.state == 'open' steps: # This time, we do not need to re-run validation because the request is # being denied. It can just be closed. # However, we do need to confirm that the user who commented `.deny` is # a member of the administrator team. # GitHub App authentication is required if you want to interact with any # resources outside the scope of the repository this workflow runs in. - name: Get GitHub App Token id: token uses: actions/create-github-app-token@v1 with: app-id: ${{ vars.ISSUEOPS_APP_ID }} private-key: ${{ secrets.ISSUEOPS_APP_PRIVATE_KEY }} owner: ${{ github.repository_owner }} # Check if the user who commented `.deny` is a member of the # administrator team. - name: Check Admin Membership id: check-admin uses: actions/github-script@v7 with: github-token: ${{ steps.token.outputs.token }} script: | try { await github.rest.teams.getMembershipForUserInOrg({ org: context.repo.owner, team_slug: 'admins', username: context.actor, }) core.setOutput('member', 'true') } catch (error) { if (error.status === 404) { core.setOutput('member', 'false') } throw error } # If the user is not a member of the administrator team, exit the # workflow. - if: ${{ steps.check-admin.outputs.member == 'false' }} name: Exit run: exit 0 # If the user is a member of the administrator team, add the denied label. - name: Add Denied Label id: add-label uses: issue-ops/labeler@v2 with: action: add github_token: ${{ steps.token.outputs.token }} labels: | denied issue_number: ${{ github.event.issue.number }} repository: ${{ github.repository }} # Notify the user that the request was denied. - name: Notify User id: notify uses: peter-evans/create-or-update-comment@v4 with: issue-number: ${{ github.event.issue.number }} body: | This request has been denied and will be closed. # Close the issue as not planned. - name: Close Issue id: close uses: actions/github-script@v7 with: script: | await github.rest.issues.update({ issue_number: ${{ github.event.issue.number }}, owner: context.repo.owner, repo: context.repo.repo, state: 'closed', state_reason: 'not_planned' })\n\nApprove workflow\n\nFinally, we need to handle request approval. In this case, we need to add the user to the team, notify them, and close the issue.\n\nname: Process Approval Comment on: issue_comment: types: - created permissions: contents: read id-token: write issues: write jobs: submit: name: Approve Request runs-on: ubuntu-latest # This job should only be run when the following conditions are true: # # - A user comments `.approve` on the issue. # - The issue has the `team-membership` label. # - The issue has the `validated` label. # - The issue has the `submitted` label. # - The issue does not have the `approved` or `denied` labels. # - The issue is open. if: | startsWith(github.event.comment.body, '.approve') && contains(github.event.issue.labels.*.name, 'team-membership') == true && contains(github.event.issue.labels.*.name, 'submitted') == true && contains(github.event.issue.labels.*.name, 'validated') == true && contains(github.event.issue.labels.*.name, 'approved') == false && contains(github.event.issue.labels.*.name, 'denied') == false && github.event.issue.state == 'open' steps: # This time, we do not need to re-run validation because the request is # being approved. It can just be processed. # This is required to ensure the issue form template is included in the # workspace. - name: Checkout id: checkout uses: actions/checkout@v4 # We do need to confirm that the user who commented `.approve` is a member # of the administrator team. GitHub App authentication is required if you # want to interact with any resources outside the scope of the repository # this workflow runs in. - name: Get GitHub App Token id: token uses: actions/create-github-app-token@v1 with: app-id: ${{ vars.ISSUEOPS_APP_ID }} private-key: ${{ secrets.ISSUEOPS_APP_PRIVATE_KEY }} owner: ${{ github.repository_owner }} # Check if the user who commented `.approve` is a member of the # administrator team. - name: Check Admin Membership id: check-admin uses: actions/github-script@v7 with: github-token: ${{ steps.token.outputs.token }} script: | try { await github.rest.teams.getMembershipForUserInOrg({ org: context.repo.owner, team_slug: 'admins', username: context.actor, }) core.setOutput('member', 'true') } catch (error) { if (error.status === 404) { core.setOutput('member', 'false') } throw error } # If the user is not a member of the administrator team, exit the # workflow. - if: ${{ steps.check-admin.outputs.member == 'false' }} name: Exit run: exit 0 # Parse the issue body into machine-readable JSON, so that it can be # processed by the rest of the workflow. - name: Parse Issue body id: parse uses: issue-ops/parser@v4 with: body: ${{ github.event.issue.body }} issue-form-template: team-membership.yml workspace: ${{ github.workspace }} - name: Add to Team id: add uses: actions/github-script@v7 with: github-token: ${{ steps.token.outputs.token }} script: | const parsedIssue = JSON.parse('${{ steps.parse.outputs.json }}') await github.rest.teams.addOrUpdateMembershipForUserInOrg({ org: context.repo.owner, team_slug: parsedIssue.team, username: '${{ github.event.issue.user.login }}', role: 'member' }) - name: Notify User id: notify uses: peter-evans/create-or-update-comment@v4 with: issue-number: ${{ github.event.issue.number }} body: | This request has been processed successfully! - name: Close Issue id: close uses: actions/github-script@v7 with: script: | await github.rest.issues.update({ issue_number: ${{ github.event.issue.number }}, owner: context.repo.owner, repo: context.repo.repo, state: 'closed', state_reason: 'completed' })\n\nTake this with you\n\nAnd there you have it! With a handful of standardized workflows, you have an end-to-end, issue-driven process in place to manage team membership. This can be extended as far as you want, including support for removing users, auditing access, and more. With IssueOps, the sky is the limit!\n\nHere\u2019s the best thing about IssueOps: It brings another level of automation to a surface I\u2019m constantly using\u2014and that\u2019s GitHub. By using issues and pull requests as control centers for workflows, teams can reduce friction, improve efficiency, and keep everything transparent. Whether you want to automate deployments, approvals, or bug triage, IssueOps makes it all possible, without ever leaving your repo.\n\nFor more information and examples, check out the open source IssueOps documentation repository, and if you want a deeper dive, you can head over to the open source IssueOps documentation.\n\nIn my experience, it\u2019s always best to start small and experiment with what works best for you. With just a bit of time, you\u2019ll see your workflows get smoother with every commit (I know I have). Happy coding! \u2728\n\nTags:", "label": 0}
{"title": "A Behind-the-Scenes Look at How We Release the Spotify App (Part 1)", "url": "https://engineering.atspotify.com/2025/4/how-we-release-the-spotify-app-part-1", "content": "Developing and releasing mobile apps at scale is a big challenge. With each weekly release of our mobile app for iOS and Android, hundreds of changes go out to more than 675 million users all over the world and on all kinds of mobile devices. A lot can go wrong, so discovering and mitigating potential and confirmed issues is critical to ensuring a smooth listening experience. Every feature could impact the app\u2019s stability and user experience, so making sure we roll them out in a coordinated and prioritized way is also something that has to be taken care of.\n\nAt Spotify, our Release team has a dual mission: (1) to oversee the release of the main Spotify app and (2) to build the necessary tools to support this process. Day-to-day coordination is handled by the full-time Release Manager with the support of the rest of the team.\n\nWhen it comes to releasing the app, the core responsibilities of the Release team are twofold:\n\nMaking sure that the time from when the developer merges their code into the main branch to when it\u2019s available to users is as short as possible To ensure that quality meets our standards\n\nAt times, friction exists between these two goals, and much of the craft of release management is about mitigating this, both with tooling and with informed coordination and decision-making. Instances when this balance needs to be struck might include the following:\n\nPrioritizing accordingly \u2014 not all bugs are created equal. A crash during signup or playback demands immediate attention, while a post-logout crash might be less urgent.\n\nIdentifying a fallback \u2014 if a bug affects a specific A/B test group, we can temporarily route all users to the working experience via backend adjustments, addressing the client-side fix in the next release. This keeps the release on track without sacrificing user experience.\n\nActing quickly when necessary \u2014 even a minor bug affecting a small but significant user group (e.g., crashes in a specific region) might warrant a swift resolution.\n\nThe release cycle\n\nTo illustrate our release cycle, let\u2019s follow the journey of version 8.9.2 from inception to rollout.\n\nFriday, September 20: A new version is born\n\nEach release cycle kicks off on a Friday morning, when the release of the previous version has been cut. Once this has been done, it\u2019s time to start the work on the upcoming version.\n\nAt Spotify, we practice trunk-based development, meaning developers merge their code into the main branch as soon as it\u2019s tested and reviewed.\n\nHowever, we make an exception for major changes: Large-scale or infrastructure updates are merged earlier in the cycle (typically on the first day, Friday of Week 1). This gives us ample time for thorough testing, leveraging both internal teams and external alpha users to identify and fix issues early on.\n\nWith Spotify 8.9.2, we planned to roll out the Audiobooks feature in some markets \u2014 it had been available behind a feature flag in the backend for a number of releases, for internal testing to discover any bugs that might impede the planned rollout. This was an important new feature for the company, and we wanted to make sure we got it right, particularly since marketing activities and events were already scheduled.\n\nThe Release Manager made sure, well in advance, that it was the only big feature rolling out with this release \u2014 another new feature that initially had been scheduled to roll out in the same week was rescheduled for the following week.\n\nOther teams could still merge code at any moment, but we strongly encouraged them to use feature flags. If that wasn\u2019t possible, we asked them to avoid merging any high-risk changes on that particular week.\n\nFriday, September 20 \u2013 Thursday, September 26\n\nApart from the additional actions during the first day, each day of the first week of the release cycle basically looks the same.\n\nEarly each morning, nightly builds of the main branch are sent out internally and to our alpha users.\n\nTeams develop and merge new code. The developers and their teams make sure that the code is tested and reviewed beforehand.\n\nBug reports are filed by internal and external alpha users. When the owner of the affected feature is unknown to the reporter, the Release Manager makes sure that the bug report gets assigned to the correct team.\n\nCrash rates and other metrics are tracked for each build both automatically and manually. Automatic bug tickets are created when a crash or other issue exceeds our predefined severity threshold; manual tickets are created when something is deemed worthy of investigation by the Release Manager or any other employee.\n\nTo help in monitoring the status of an upcoming release, we make use of our Release Manager Dashboard, which collects all the relevant release information in the same place:\n\nSome examples of the data found in the dashboard are as follows:\n\nBlocking bugs\n\nLatest build available, passing tests, and distributed through the alpha program in the app store\n\nCrashes and ANRs (App Not Responding) per unit of consumption\n\nDaily usage\n\nStatus of the distribution jobs that distribute the app internally and externally\n\nBy this time, the Audiobooks feature had been turned on for most employees. So in addition to the regular process for this particular release, both the Release Manager and the Audiobooks team looked through all the crashes happening in the client to see if anything might put the Audiobook rollout in jeopardy. Even a seemingly minor crash affecting a small number of employees could signify a potential issue impacting a large user base upon rollout. Therefore, it\u2019s important to investigate and mitigate any issues as soon as possible.\n\nFriday, September 27: Fri-yay is branch day!\n\nOnce a week passed, it was time for the 8.9.2 version to be branched off for releasing, initiating the most intensive part of the release process. Once a release has been branched off, it is regarded as the current release and only critical bug fixes are permitted. Our weekly release cadence allows less critical bugs and new features to be addressed in subsequent releases, and teams generally avoid last-minute changes to minimize risk.\n\nOnce branching is done, the Release Manager coordinates the work of releasing the version as soon as possible with all stakeholders.\n\nTo help us gather additional data on the quality of the release branch, we have a public beta program that is taken from the release branch \u2014 these builds are expected to be more stable than our alpha builds.\n\nOn Fridays during Week 2, teams perform manual regression testing on their owned features and report their results. Teams with high confidence in their automated tests and pre-merge routines can opt out of manual testing.\n\nThroughout the testing process, teams may uncover bugs in their own or in other teams\u2019 features and will file tickets for these. Additionally, crash reports and bug reports from internal and external users also trigger the creation of new bug tickets.\n\nFor 8.9.2, the Audiobooks team was particularly vigilant during this phase, meticulously searching for any potential issues. At this point, it\u2019s not uncommon to discuss bugs and decide if they warrant a release blocker. Good release management means objectively looking at risk, potential impact, and workarounds.\n\nFriday, September 27 \u2013 Monday, September 30: Getting it ready for submission\n\nDuring the weekend after branching, our beta users provide additional runtime of the app, which either increases our confidence in releasing it or helps us to find issues that had not been found earlier, by the time we get back to work on Monday.\n\nIdeally, we aim to submit the app to the app stores on Monday. However, complex bugs or unforeseen issues can extend this process by a few days. To streamline communication and coordination for each release, the Release Manager, feature teams, and other stakeholders share updates, ask questions, and flag potential concerns on a dedicated Slack channel.\n\nThe Release Manager ensures bugs are assigned to the correct team and prioritized appropriately, manual testing is executed and reported, and any release-blocking bugs (there tend to be perhaps three to five such bugs per release) are fixed on the release branch.\n\nBefore we submit a build to the app stores, we want to make sure the following criteria are met:\n\nAll commits on the release branch are included in the latest build and have passed automated tests.\n\nNo blocking bug tickets remain open.\n\nAll teams have signed off and approved.\n\nCrash rates and other key metrics are below our defined thresholds.\n\nThe app version to be released has been used to play a sufficient amount of content.\n\nThe Release Manager Dashboard provides a clear overview of these criteria, using color-coding (red/yellow/green) for quick assessment.\n\nFor the 8.9.2 release, we provided additional test accounts with the Audiobooks functionality enabled, as well as detailed testing instructions to the various app stores to ensure they were aware of the new features and wouldn\u2019t be caught off guard when we started to roll out the functionality.\n\nTuesday, October 1 \u2013 Wednesday, October 2: Rollout\n\nOnce the app is approved for a platform, we roll it out in two phases: first, to a small percentage of users and, then, the following day, to 100% of users.\n\nWhen a release has been rolled out to 1%, we expect the dashboard to look like this:\n\nThe only remaining items, indicated in yellow, needed before full rollout are the ITGC tickets, where we check to see that reporting from the client to the backend is working as necessary. It\u2019s not unusual to uncover minor bugs that, while acceptable in the current version, would be considered blockers for future releases. In severe cases, we might temporarily halt the feature rollout and resume with the next release.\n\nThanks to our large user base, even a small initial rollout percentage allows us to quickly identify critical issues that may have slipped through the cracks during the internal and public alpha and beta testing.\n\nIf we find a severe issue during the first rollout phase, we immediately pause the rollout and the team responsible starts to create a fix. Ideally, the fix is ready before the next version branches, allowing us to submit an updated build. However, if the fix isn\u2019t timely, we may face the difficult decision of canceling either the current or the upcoming release to prevent the complexity of managing two active release branches simultaneously. Once we have reached 100%, we continue monitoring the state of the release over the next week.\n\nFor version 8.9.2, once a sufficient user base was using the new version, the Audiobooks team initiated their phased rollout. This involved gradually enabling the Audiobooks feature for a small percentage of users in specific markets using a backend feature flag.\n\nFortunately, for version 8.9.2, the Audiobooks feature met our quality standards, and the rollout successfully ramped up to 100% over the following days.\n\nUsing the release procedure described above, we are able to roll out more than 95% of releases to all our users. The weekly cadence also means that a canceled release is not the end of the world for the feature teams, since a new release will go out the following week. In the same way, users will be able to get a new version of the app every week as long as it meets our quality standards.\n\nSummary\n\nSpotify\u2019s weekly mobile app release process tries to strike a balance between speed and quality. The person at the helm of the process is the Release Manager, who handles communication and coordination with feature teams and other stakeholders throughout the release cycle with the help of the Release Squad. Tools like the Release Manager Dashboard play an important role in enabling the Release Manager to make fast and accurate decisions.\n\nDetailed documentation of the tools and processes helps guide all the teams involved.\n\nThis robust system allows Spotify to consistently update its app, quickly address issues, and introduce new features like Audiobooks, all while minimizing disruptions to the user experience.\n\nStay tuned for part 2, where we\u2019ll look under the hood to see how the systems (and robots!) that power the Release Manager Dashboard work.", "label": 0}
{"title": "learning \u2013 Tracy Durnell's Mind Garden", "url": "https://tracydurnell.com/tag/learning/", "content": "Liked Rob Sheffield on the Joys of the CD, Music\u2019s Least-Glamorous Format by Rob Sheffield ( Rolling Stone ) Compact discs never had the romance of vinyl or the convenience of MP3s. But they\u2019re still the ideal format for getting lost inside your music collection.\n\nI still have a CD player in my car so I hung onto my favorite 20-30 albums, and I somewhat regret getting rid of my collection. It wasn\u2019t ever as big as my parent\u2019s, but I had ~50-70 jazz CDs (donated to the local HS music department so hopefully someone\u2019s still using them) plus probably a hundred albums. Some indie shit I probably couldn\u2019t replace if I wanted (I like to think I\u2019ve become slightly more thoughtful about what I give away in my wiser 30s \ud83d\ude02).\n\nBut to be fair, my listening has shifted a lot since college, so the music I listen to most I don\u2019t have on CD, and I got rid of a bunch of albums that I had kinda outgrown, so maybe it\u2019s not a bad thing. I\u2019ve only bought one or two CDs a year for the past decade, indie bands I wanted to give some extra support (and listen in the car).\n\nThere\u2019s something about having a tangible object that makes it easier to flip through your collection and pull out things you haven\u2019t listened to in a while. Growing up I was obsessed with learning to recognize every song that came on, so I was constantly comparing against the back of the CD. I liked looking through the liner art, and had a great visual memory for what the cover of every album was. Now it\u2019s hard for me to remember what artist performed what song \u2013 I think that physical object of the jewel case was an anchor point for my memory. I also listen to playlists primarily these days, and know only a single song (or handful) by any given artist.\n\n(Related? Structures of Thought)\n\nI think there\u2019s a place for both CDs and playlists in a musical library \u2013 I\u2019ve benefitted from both styles of listening. But I do miss my five disc changer from my youth\u2026 hooked up to massive speakers nearly 3\u2032 tall in my living room so they could punch some damn volume \ud83d\ude02 (No idea if they were any good or not \ud83e\udd37\u200d\u2640\ufe0f)\n\nI also think it\u2019s worth remembering we got rid of our CDs for a reason\u2026 they do take up a lot of space and jewel cases are shit\u2026 but now we\u2019ve spent time without them we can recognize what we\u2019ve lost along with them.\n\nI keep thinking about looking up a used CD player, maybe I\u2019ll actually get around to it once day \ud83e\udd37\u200d\u2640\ufe0f\ud83d\ude02 I\u2019m curious how much I would listen to CDs if we upgraded the music system in our car and Bluetooth wasn\u2019t an enormous pain \u2013 would I revert to mostly Spotify playlists on my phone? \ud83e\udd14", "label": 1}
{"title": "Using GitHub Actions with Heroku Flow for additional Security Control", "url": "https://www.heroku.com/blog/using-github-actions-heroku-flow-additional-security-control/", "content": "Many advanced users want to use GitHub Actions with their applications on Heroku. Now there\u2019s a straightforward way to use these great systems together, and to meet strong security and compliance requirements at the same time.\n\nHeroku is a powerful platform that offers robust CI/CD capabilities and secure, scalable environments for deploying applications. However, GitHub Orgs cannot be configured with Heroku IP ranges, which can be a requirement for some organizations\u2019 security rules. While this is under consideration, we want to share an alternative that leverages GitHub Actions, Heroku\u2019s ability to run arbitrary workloads and its powerful Platform API. If you\u2019re looking to integrate private repositories with Heroku CI/CD, need strict control over source code sharing in regulated environments, or want to explore why running a GitHub Action Runner on Heroku might be more efficient, this blog post is for you!\n\nIn this post, we will share and describe a set of repositories and configuration instructions that enable you to leverage GitHub Actions\u2014its features, dashboard reporting, and the ability to host the GitHub Runner on Heroku\u2014for optimal execution and secure access to your private application code, all while still within the Heroku Pipeline dashboard experience.\n\nKeep in mind, while aspects of this solution are part of the core Heroku offering, the pattern explained in this article is provided as a sample only and the final configuration will be your responsibility. Additionally, while we have tried hard to ensure all aspects of the Heroku Flow feature work in this mode \u2013 there are some considerations to keep in mind we will share later in this blog and in the accompanying code.\n\nIn short, GitHub Actions are small code snippets\u2014typically shell scripts or Node.js\u2014that run in response to events like commits or PR creation. You define which events trigger your actions, which can perform various tasks, primarily integrating with CI/CD systems or automating testing, scanning, and code health checks. For secure access to your deployment platform and source code, GitHub requires you to host a Docker image of their Runner component. They also require that you routinely update your runner instances within 30 days of a new release. You can read more about GitHub Actions.\n\nHeroku supports these requirements in two key ways: hosting the runner and providing access to the build and deployment platform. First, Heroku can host official Docker images just as easily as application code, eliminating the need to manage infrastructure provisioning or scaling. Second, the Heroku Platform API enables GitHub Actions to automate managing Review Apps through an existing pipeline, move code through the pipeline, and trigger deployments\u2014all while storing source code briefly on ephemeral storage. Additionally, this setup includes automation for the mandatory 30-day upgrade window for the GitHub Runner component, reusing the above mentioned features to schedule a weekly workflow that rebuilds its Docker image and autodeploy as a Heroku app, which removes the burden of having to update it manually. The following diagram outlines the location of application source repositories, the two GitHub actions required and within Heroku the configuration to run the GitHub runner and of course application deployments created by the actions \u2013 all within a Heroku Private space.\n\nThere are two repositories we are sharing that help you accomplish the above:\n\nHeroku-hosted runner for GitHub Actions \u2013 This project defines a Dockerfile to run a custom Heroku-hosted runner for Github Actions (see also self-hosted runners). The runner is hosted on Heroku as a docker image via heroku.yml . Once the self-hosted runner is running on Heroku you can start adding workflows to your private GitHub repositories to automate Heroku Review Apps creation and Heroku Apps deploy using the following action (that includes workflow examples).\n\n\u2013 This project defines a Dockerfile to run a custom Heroku-hosted runner for Github Actions (see also self-hosted runners). The runner is hosted on Heroku as a docker image via . Once the self-hosted runner is running on Heroku you can start adding workflows to your private GitHub repositories to automate Heroku Review Apps creation and Heroku Apps deploy using the following action (that includes workflow examples). Heroku Flow Action \u2013 GitHub Action to upload the source code to Heroku from a private GitHub repository using the Heroku Source Endpoint API. The uploaded code is then built to either deploy an app (on push , workflow_dispatch , and schedule events) or create/update a review app (on pull_request events such as opened , reopened , and synchronize ). Whenever a PR branch is updated, the latest commit is deployed to the review app if existing, otherwise a new review app is created. The Review App is automatically removed when the pull request is closed (on pull_request events when the action is \u2018 closed \u2018). The action handles only the above mentioned events to prevent unexpected behavior, handling event-specific requirements and improving action reliability.\n\nThe README files in the above repos go into more details \u2013 but at a high level what is involved is the following steps to setup GitHub Runner in Heroku and configure the GitHub Actions:\n\nIdentify a Private Space to run your the GitHub Runner in and resulting pipeline apps. Identify outbound IPs from the Private Space to be shared in your GitHub configuration. Deploy the GitHub Runner to the Private Space with your GitHub access token and Organization Name. Configure one or more private repos with the Heroku Flow Action and test by creating some PRs.\n\nWhat you should see from step 4 is the following:\n\nA new GitHub Action is started. A Review App within the configured Pipeline is automatically created upon the creation of a PR. From the Pipeline you can follow the application build as soon as it progresses.\n\nUsing this approach you are able to fully leverage your Heroku investment and reuse the features that the platform already offers, such as build and deploy capabilities and compute power, without needing to use external tools or platforms. In this way, your CI/CD is fully integrated where your apps are, a close integration that allows you to unlock scenarios where you can connect your Heroku-hosted runners to resources within or attached to your Private Space (e.g. secret managers, package registries, Heroku apps \u2026) via Private Space peering or VPN connections.\n\nUsing a Private Space is not mandatory, but it adds a layer of security and provides a static set of public IP addresses that can be configured in your GitHub Org. Moreover, Private Spaces are now available for online customers too, so both verified Heroku Teams and Heroku Enterprises can leverage such an option.\n\nYour Heroku Flow can be improved and customized with ad-hoc steps and provide additional features such as manual and scheduled app builds and deploys via GitHub Actions \u201cRun Workflow\u201d and cron/scheduler.\n\nLast, but not least, your Heroku-hosted runners\u2019 consumption is pro-rated to the second.\n\nThis solution complements your current Heroku development environments and can be used even for non-Heroku projects, a complete and enhanced delivery workflow is at your fingertips that in the future can open to other integration scenarios (e.g. on-premise GitHub Server, GitLab, Bitbucket \u2026), while remaining on the platform you love!\n\nPlease keep the following considerations in mind as you explore this pattern and read the README files within the above repositories in detail to fully understand their value and implications. In summary, some key aspects to be aware of are as follows:\n\nFrom the Review App UI in the Heroku Pipeline, the URL used to allow easy access to the instance of the actual GitHub repository is not available in this configuration. You will instead need to relay the correct GitHub repository URL to your stakeholders in a different way.\n\nHeroku CI has a feature that automatically runs tests before creating Review Applications, among other features described here. In this configuration, the standard Heroku-managed Git repository is explicitly not used, and as such, tests are not run in the conventional way. If you need this capability, you could consider extending the action code to run your tests before every subsequent push to your GitHub repository.\n\nCurrently, this configuration is not compatible with Fir, our next-generation platform version.\n\nWhile we are using the core GitHub Runner software, we are not using the standard GitHub docker images: Rather, we create a custom image for you. It is up to you to test whether other GitHub actions you have created work as expected.\n\nPlease continue to review more detailed consideration information in the README\u2019s here and here.\n\nGitHub Actions is a powerful tool for automating deployment pipeline tasks. Given the ability to reduce the toil of managing your own GitHub Runner instance along with the ease with which you can monitor the pipeline and let stakeholders test builds through Heroku Review Apps, we\u2019re excited to share this pattern with our customers. As mentioned earlier, out-of-the-box support for this capability is under consideration by our product team. We invite you to share your thoughts on this roadmap item directly via commenting on the github issue. Meanwhile please feel free to fork and/or make suggestions on the above GitHub repos. We welcome your feedback, whether or not you\u2019ve explored this approach. Finally, at Heroku, we consider feedback a gift. If you have broader ideas or suggestions, please connect with us via the Heroku GitHub roadmap.", "label": 0}
{"title": "Spider-Man: Across the Spider-Verse", "url": "https://lifeofpablo.com/blog/spider-man-across-the-spider-verse", "content": "Spider-Man: Across the Spider-Verse\n\nThis post was written in English (en_US).\n\nI have an entire backlog of movies to watch. Yes, I apparently live under a rock. I made sure Spider-Man: Across the Spider-Verse didn't fall into this black whole.\n\nI'm not going to share any spoilers. I'll try at least. The movie was amazing. I really enjoyed every second of the movie, It was jam packed! So much action, so much emotion, and so many realizations. It's cool to connect the dots from the Marvel Cinematic Universe, whether or not it was intentional.\n\nThere were so many plot twists. So many ways things can go. You know the Spider-Verse.....? Thanks for inviting me Lisa and Luke.\n\nIt made me think about many of the decisions I made in life. It's insane how many of my memories growing up ran across. I had a few tears and emotions in the scenes between Miles and his mom. She reminded me about my mom and the close relationship I have with her. My mom is such an amazing person. I also had so many tears and emotions when I saw the interactions between Miles and his dad. My dad is also an amazing person as well. We are also close. Lot's of emotions and tears as well. It reminded me about what it was like to grow up Latino. Obviously, my experience is different but I see many shared experiences of some level.\n\nThere were many funny scenes that I was rolling over. Sorry neighbor. The funniest scenes were the ones that were subtle.\n\nI can't wait to see the next one that will come out, I will probably re watch the entire two movies again and more.\n\nI want to write more but I don't want to give spoilers.", "label": 1}
{"title": "What you should know from the Google I/O 2025 Developer keynote", "url": "https://developers.googleblog.com/en/google-io-2025-developer-keynote-recap/", "content": "This year at Google I/O we\u2019re showing how you can build across Google\u2019s different platforms, and innovate using our best AI models from Google DeepMind. Here are the top announcements from the Developer keynote.\n\nBuilding with Gemini\n\nGoogle AI Studio is the fastest way to evaluate models and start building with the Gemini API.\n\nGoogle AI Studio makes it easy to build with the Gemini API: We\u2019ve integrated Gemini 2.5 Pro into the native code editor, enabling you to prototype faster. It\u2019s tightly optimized with the GenAI SDK so you can instantly generate web apps from text, image, or video prompts. Start from a simple prompt, or get inspired by starter apps in the showcase.\n\nBuild agentic experiences with the Gemini API: Build agents with Gemini 2.5 advanced reasoning capabilities via the Gemini API and new tools, like URL Context. It enables the model to pull context from web pages with just a link. We also announced the Gemini SDKs will support Model Context Protocol (MCP) definitions, making it easier to leverage open source tools.\n\nGemini 2.5 Flash Native Audio in the Live API: Build agentic applications that hear and speak, with full control over the model\u2019s voice, tone, speed, and overall style, in 24 languages. Gemini 2.5 Flash Native Audio is much better at understanding conversational flow and ignoring stray sounds or voices, leading to smoother, more natural back-and-forth.\n\nGenerate high-quality UI designs with Stitch: A new AI-powered tool to generate user interface designs and corresponding frontend code for web applications. Iterate on your designs conversationally using chat, adjust themes, and easily export your creations to CSS/HTML or Figma to keep working. Try Stitch for UI design.\n\nOur async code agent, Jules, is now in public beta: Jules is a parallel, asynchronous coding agent that works directly with your GitHub repositories. You can ask Jules to take on tasks such as version upgrades, writing tests, updating features, and bug fixes, to name a few. It spins up a Cloud VM, makes coordinated edits across your codebase, runs tests, and you can open a pull request from its branch when you're happy with the code.\n\n\n\nAndroid\n\nLearn how we\u2019re making it easier for you to build great experiences across devices.\n\nBuilding experiences with generative AI: Generative AI enhances apps by making them intelligent, personalized, and agentic. We announced new ML Kit GenAI APIs using Gemini Nano for common on-device tasks. We showcased an AI sample app, Androidify, which lets you create an Android robot of yourself using a selfie. Discover how Androidify is built, and read the developer documentation to get started.\n\nBuilding excellent apps adaptively across 500 million devices: Mobile Android apps form the foundation across phones, foldables, tablets, and ChromeOS, and this year we\u2019re helping you bring them to cars and Android XR. You can also take advantage of Material 3 Expressive to help make your apps shine.\n\nGemini in Android Studio - AI agents to help you work: Gemini in Android Studio is the AI-powered coding companion that makes developers more productive at every stage of the dev lifecycle. We previewed Journeys, an agentic experience that helps with writing and executing end-to-end tests. We also previewed the Version Upgrade Agent which helps update dependencies. Learn more about how these agentic experiences in Gemini in Android Studio can help you build better apps, faster.\n\n\n\nWeb\n\nWe\u2019re making it easier to create powerful web experiences, from building better UI and faster debugging, to creating new AI-powered features.\n\nCarousels are now easier than ever to build with a few lines of CSS and HTML: Build beautiful carousels with CSS that are interactive at first paint. With Chrome 135, we've combined a few new CSS primitives to make building carousels, and other types of off-screen UI, dramatically easier. Use familiar CSS concepts to create rich, interactive, smooth, and more accessible carousels, in a fraction of the time.\n\nIntroducing the new experimental Interest Invoker API: Declaratively toggle popovers when visitor interest is active for a small duration. Combine with the Anchor Positioning API and Popover API to build complex, responsive, layered UI elements like tooltips and hover cards, without JavaScript. Interest Invoker API is available as an origin trial.\n\nBaseline features availability is now in your familiar tools: VS Code now displays the Baseline status of features as you build, with support coming soon to other VS Code-based IDEs and WebStorm by JetBrains. Baseline is now also supported in ESLint for CSS, HTML ESLint, and Stylelint. RUMvision combines Baseline information with real-user data, letting you strategically select the optimal Baseline target for your audience. Plus, with the web-features data set now 100% mapped, you can now access the Baseline status of every feature on every major browser.\n\nAI in Chrome DevTools supports your debugging workflow: Boost your development workflow with Gemini integrated directly into Chrome DevTools. With AI assistance, you can now directly apply suggested changes to the files in your workspace in the Elements panel. Plus, the reimagined Performance Panel now features a powerful \u2018Ask AI\u2019 integration that provides contextual performance insights to help optimize your web application\u2019s Core Web Vitals.\n\nNew built-in AI APIs using Gemini Nano are now available, including multimodal capabilities: Gemini Nano brings enhanced privacy, reduced latency, and lower cost. Starting from Chrome 138, the Summarizer API, Language Detector API, Translator API, and Prompt API for Chrome Extensions are available in Stable. The Writer and Rewriter APIs are available in origin trials, and the Proofreader API and Prompt API with multimodal capabilities are in Canary. Join our early preview program to help shape the future of AI on the web.\n\n\n\nFirebase\n\nPrototype, build, and run modern, AI-powered, full-stack apps users love with Firebase. Use Firebase Studio, a cloud-based, AI workspace powered by Gemini 2.5, to turn your ideas into a full-stack app in minutes, from prompt to publish.\n\nFigma designs can be brought to life in Firebase Studio: Import a Figma design directly into Firebase Studio using the builder.io plugin, then add features and functionality using Gemini in Firebase without having to write any code.\n\nFirebase Studio will now suggest a backend: Rolling out over the next several weeks, when you use the App Prototyping agent, Firebase Studio can detect the need for a backend. Firebase Studio will now recommend Firebase Auth and Cloud Firestore, and when you're ready to publish the app to Firebase App Hosting, Firebase Studio will provision those services for you.\n\nFirebase AI Logic: Integrate Google\u2019s gen AI models directly through your client apps, or through Genkit for server-side implementation. As part of the evolution from Vertex AI in Firebase to Firebase AI Logic, we\u2019re also releasing new features such as client side integrations for the Gemini Developer API, hybrid inference, enhanced observability, and deeper integrations with Firebase products such as App Check and Remote Config.\n\n\n\nBuilding with open models\n\nThere's so much you can do when building with Gemini, but sometimes it's better to train and tune your own model. That\u2019s why we released Gemma, our family of open models designed to be state of the art, and fit on devices.\n\nGemma 3n is in early preview: This model can run on as little as 2GB of RAM thanks to research innovations. It is the first model built on the new, advanced mobile-first architecture that will also power the next generation of Gemini Nano, and is engineered for unmatched AI performance directly on portable devices.\n\nMedGemma is our most capable open model for multimodal medical text and image comprehension: A variant of Gemma 3, MedGemma is a great starting point for developers to fine tune and adapt to build their own healthcare-based AI applications. Its small size makes it efficient for inference, and because it\u2019s open, it enables developers with the flexibility to fine-tune the model and run it in their preferred environments. MedGemma is available for use now as part of Health AI Developer Foundations.\n\nColab is launching an agent first experience that transforms coding: Powered by Gemini 2.5 Flash, Colab helps you navigate complex tasks, such as fine-tuning a model. We showcased how the new AI-first Colab can build UI, saving you lots of coding time.\n\nSignGemma is a sign language understanding model coming later this year to the Gemma family: It is the most capable model for translating sign languages into spoken language text to date (best at American Sign Language to English), enabling you to develop new ways for Deaf/Hard of Hearing users to access technology. Share your input at goo.gle/SignGemma.\n\nDolphinGemma is the world\u2019s first large language model for dolphins: Working with researchers at Georgia Tech and the Wild Dolphin Project, DolphinGemma was fine-tuned on data from decades of field research, to help scientists better understand patterns in how dolphins communicate.\n\n\n\nGoogle Developer Program\n\nWe expanded AI benefits for the Google Developer Program, including Gemini Code Assist Standard, a new gen AI developer annual credit, and 3 months of Google One AI Premium. We also announced a new Google Cloud & NVIDIA community where you can connect with experts from both companies in a dedicated forum, and soon gain access to exclusive learning content and credits.\n\n\n\nTune into all of the developer news\n\nFollowing the keynotes, we\u2019ll be livestreaming sessions across AI, Android, web, and cloud May 20-21. Then, check out all of the Google I/O announcements and updates with 100+ sessions, codelabs, and more available on demand starting May 22.\n\nMake sure to connect with our thriving global community of developers, and follow along on LinkedIn and Instagram as we bring I/O Connect events to developers around the world.", "label": 0}
{"title": "Getting my library card in San Francisco!", "url": "https://lifeofpablo.com/blog/getting-my-library-card-in-san-francisco", "content": "Getting my library card in San Francisco!\n\nPablo\u2019s library card!\n\nThis post was written in English (en_US).\n\nIt\u2019s been a few days since I moved to San Francisco. I\u2019ve been starting to get things organized but it almost seems there is no end. I enjoy taking breaks and leaving the apartment. Going on walks around the neighborhood is fun but I always go on an adventure. I\u2019m getting my steps on and I have fun! I also get a snack or coffee while on an adventure. The sidewalk and my legs are possibly the limit.\n\nEventually, I found myself near Powell Station to take a break. As I was leaning against a wall, I asked myself, \u201cWhat should I do to feel that I live here?\u201d and \u201cWhat is cheap but fulfilling?\u201d\n\nHmm\u2026.\n\nQueue the Jeopardy think music.\n\nYour browser does not support the audio element.\n\nIt came to me! I yelled out loud \u201cI know! I\u2019ll get a library card today!\u201d Mind you, I\u2019m at a busy location in the city. I think I started a bit of people waiting for the street car. There could be worse things to be yelling for in a city. Wouldn\u2019t you get excited if you wanted to get a library card? Going to the library is exciting! I\u2019m excited to go visit one of the oldest public institutions offered to the public! If I know anything about libraries in big cities, their main branch must be beautiful. Onward to the San Francisco Public Library!\n\nMan! When I walked into the SF Library, it was pretty cool! It\u2019s pretty modern! There is lots of elbow room to roam around!\n\nAs I\u2019m walking towards the library, I fill out the online application to start the process and get the confirmation number that I was provided.\n\nThen when I got to the desk, I was asked for this confirmation number, identification, and some proof of where I lived such as a utility bill or lease agreement. The process was super easy. The people working at the library were very friendly.\n\nHere is the library card design that I chose. It was either between the SF Giants or the drawing a second grader. Obviously, I went with the drawing because I don\u2019t care about baseball. The drawing is pretty cool! Better than I could ever do.\n\nAfter getting my library card, I explored the various floors throughout the library. This place is massive! Various items caught my eye. They let you borrow vinyls! Who does that!? I\u2019m over the moon!\n\nI didn\u2019t know that the San Francisco Public Library is a Government Information Center (GIC). It\u2019s very accessible to get documents from various levels of government.\n\nThe Government Information Center (GIC) provides free access to United States, California, San Francisco, and Bay Area regional agency government documents, and to materials on public policy, political science, public administration, and the law.\n\nI\u2019ll have to spend more time exploring the library system and its website to see how I can maximize the resources available to me. I want to support the library more. This is a lesson to utilize public libraries and other public resources available to constituents.\n\nGetting my library card was an easy experience. Everyone should be able to use the library. I'm excited to be spending a lot of time here! The important thing about the library is you don't have to buy anything to work on things or read a book.\n\nI got to check a book out!\n\nThis blog post was written at the main branch of the San Francisco Library. Thank you for the fast internet. <3", "label": 1}
{"title": "Pablo Morales", "url": "https://lifeofpablo.com/blog/published:2024-04", "content": "en\n\nI recently redesigned my website and let me tell you, It was long overdue. The redesign was on my to do list from a long time a go. Things were a mess in the code. It's still a work in progress. I was using multiple style sheets (I'm still reducing this). I was using a theme I had modified and I was using Tachyons, a CSS framework. Having to add more CSS classes for specific purposes was getting overbearing. Sure my blog posts were getting pretty with various designs and looked a little nicer. I hate to say it, I was often more worried about what my website looked like instead of focusing more of the quality of my posts. An RSS reader simply pulls the content and none of the styling that comes with CSS classes.\n\nI was starting to lose it. I simply needed to strip my website of any unnecessary CSS and classes. I was also limited on time so I couldn't commit to something from scratch. I found a simple CSS framework that provided me the bare minimum that made my website look very simple but more put together.\n\nThe name of the CSS framework is...wait for it... called, simple.css. It's a CSS framework that has no classes built in. Simply load this and boom! Your website is mostly organized without the \"extra\" stuff. It's super simple to override the CSS code to add something specific.\n\nUsing simple.css was good enough for me. Simply, it was a no brainer. It allowed me to address the issues, and it was good enough to implement. simple.css is simple and fast. At this time, I don't feel the need to commit to designing from scratch the CSS to make my site pretty. Going minimal is good enough for me.\n\nThis is why 'Good Enough' is enough to get things done.\n\nThis blog post is in response to Aaron's, known as RisingThumb, IndieWeb Carnival April 2024 post on good enough. Thank you for hosting this month's carnival. Everyone and anyone is welcome to host an IndieWeb Writing Carnival.", "label": 1}
{"title": "How I Want to Write", "url": "https://lifeofpablo.com/blog/how-i-want-to-write", "content": "How I Want to Write\n\nThis post was written in English (en_US).\n\nThis blog post is my participation to Homebrew Website Club - Writing Edition hosted by Sara Jak\u0161a. Thank you so much for hosting! You should read Sara's blog!\n\nI'm writing this post on topic mentioned on the event description.\n\nAny topic connected to writing can be covered here. Some of the possible topics include the process of writing, the joys of writing, the frustration some could have, how to improve it, what to write about or people can just showcase what they have written (or see it written on the web) and why they like it. You can also bring your own questions about writing that we can discuss or even trouble shoot. Or you can just listen to other people's ideas. All ways of participating are alright.\n\nThe joys of writing.\n\nWriting helps soothe\n\nMy (sometimes) chaotic process\n\nSince I started to write again, my writing has improved significantly. I've been more expressive as I share with the reader. My writing journey is only just starting. I've broken a few barriers with myself. I still feel that I need to break another layer of myself. Often, I find myself holding back. What am I holding back? Honestly, I over think things too much and that is frustrating. That leads to me sitting on a post for a few days before I touch it again. That if I am determined to finish it, even if it takes me a while look at it. Here is the typical stage of what my writing process goes.\n\nwrite a few words Delete Overthink scream walk away try again, repeat eventually finish the post.\n\nI will admit my writing process is a bit chaotic. Not all of my writing is like this, Only a few posts. This is only one part of my writing process.", "label": 1}
{"title": "What's new with Agents: ADK, Agent Engine, and A2A Enhancements", "url": "https://developers.googleblog.com/en/agents-adk-agent-engine-a2a-enhancements-google-io/", "content": "At Google, we envision a future where intelligent agents are not just tools, but collaborative partners in solving complex challenges, streamlining workflows, and unlocking new possibilities. We believe that empowering developers with a platform that offers flexibility, trust, and comprehensive capabilities is key to realizing this potential. Today, we're thrilled to share a series of significant updates across our product portfolio that reflect this vision, designed to help you build and manage your intelligent agents with unprecedented ease and power. These enhancements focus on providing robust development tools, intuitive management interfaces, and seamless agent-to-agent communications, delivering a stronger foundation for the next generation of AI powered solutions.\n\nBuilding with confidence and flexibility: Agent Development Kit (ADK) To empower you to create sophisticated agents with stability and adaptability, we've added significant innovations with our Agent Development Kit (ADK). Python ADK v1.0.0: Stability for Production-Ready Agents We're excited to announce the v1.0.0 stable release of our Python Agent Development Kit. This milestone signifies that the Python ADK is now production-ready, offering a reliable and robust platform for developers to confidently build and deploy their agents in live environments. We've heard incredible feedback from customers using Agent Development Kit already, including Renault Group, Box, and Revionics. Java ADK v0.1.0: Extending Agent Capabilities to the Java Ecosystem Expanding our reach, we're also launching the initial release of the Java ADK v0.1.0. This development brings the power and flexibility of the ADK to Java developers, enabling them to leverage its capabilities for their agent development needs.\n\nTo get started with the Java ADK, you can add the following dependency to your Maven project:\n\n<dependency> <groupId>com.google.adk</groupId> <artifactId>google-adk</artifactId> <version>0.1.0</version> </dependency> XML Copied\n\nIntuitive control and management: The Agent Engine UI The Vertex AI Agent Engine helps developers deploy, manage, and scale agents in production. We\u2019re excited to now offer an Agent Engine UI to simplify the agent lifecycle in a more straightforward and centralized way. This user-friendly interface, accessible within the Google Cloud console, provides a comprehensive dashboard to view and manage your deployed agents, list sessions, trace and debug actions, and monitor your agents. This streamlined approach significantly enhances the development and management process, offering you greater control and deeper insights into your agent's behavior and performance.", "label": 0}
{"title": "Title Launch Observability at Netflix Scale", "url": "https://netflixtechblog.com/title-launch-observability-at-netflix-scale-8efe69ebd653?source=collection_home---4------2-----------------------", "content": "Title Launch Observability at Netflix Scale\n\nPart 3: System Strategies and Architecture Netflix Technology Blog 7 min read \u00b7 Mar 5, 2025 -- 6 Listen Share\n\nBy: Varun Khaitan\n\nWith special thanks to my stunning colleagues: Mallika Rao, Esmir Mesic, Hugo Marques\n\nThis blog post is a continuation of Part 2, where we cleared the ambiguity around title launch observability at Netflix. In this installment, we will explore the strategies, tools, and methodologies that were employed to achieve comprehensive title observability at scale.\n\nDefining the observability endpoint\n\nTo create a comprehensive solution, we decided to introduce observability endpoints first. Each microservice involved in our Personalization stack that integrated with our observability solution had to introduce a new \u201cTitle Health\u201d endpoint. Our goal was for each new endpoint to adhere to a few principles:\n\nAccurate reflection of production behavior Standardization across all endpoints Answering the Insight Triad: \u201cHealthy\u201d or not, why not and how to fix it.\n\nAccurately Reflecting Production Behavior\n\nA key part of our solution is insights into production behavior, which necessitates our requests to the endpoint result in traffic to the real service functions that mimics the same pathways the traffic would take if it came from the usual callers.\n\nIn order to allow for this mimicking, many systems implement an \u201cevent\u201d handling, where they convert our request into a call to the real service with properties enabled to log when titles are filtered out of their response and why. Building services that adhere to software best practices, such as Object-Oriented Programming (OOP), the SOLID principles, and modularization, is crucial to have success at this stage. Without these practices, service endpoints may become tightly coupled to business logic, making it challenging and costly to add a new endpoint that seamlessly integrates with the observability solution while following the same production logic.\n\nA service with modular business logic facilitates the seamless addition of an observability endpoint.\n\nStandardization\n\nTo standardize communication between our observability service and the personalization stack\u2019s observability endpoints, we\u2019ve developed a stable proto request/response format. This centralized format, defined and maintained by our team, ensures all endpoints adhere to a consistent protocol. As a result, requests are uniformly handled, and responses are processed cohesively. This standardization enhances adoption within the personalization stack, simplifies the system, and improves understanding and debuggability for engineers.\n\nThe request schema for the observability endpoint.\n\nThe Insight Triad API\n\nTo efficiently understand the health of a title and triage issues quickly, all implementations of the observability endpoint must answer: is the title eligible for this phase of promotion, if not \u2014 why is it not eligible, and what can be done to fix any problems.\n\nThe end-users of this observability system are Launch Managers, whose job it is to ensure smooth title launches. As such, they must be able to quickly see whether there is a problem, what the problem is, and how to solve it. Teams implementing the endpoint must provide as much information as possible so that a non-engineer (Launch Manager) can understand the root cause of the issue and fix any title setup issues as they arise. They must also provide enough information for partner engineers to identify the problem with the underlying service in cases of system-level issues.\n\nThese requirements are captured in the following protobuf object that defines the endpoint response.\n\nThe response schema for the observability endpoint.\n\nHigh level architecture\n\nWe\u2019ve distilled our comprehensive solution into the following key steps, capturing the essence of our approach:\n\nEstablish observability endpoints across all services within our Personalization and Discovery Stack. Implement proactive monitoring for each of these endpoints. Track real-time title impressions from the Netflix UI. Store the data in an optimized, highly distributed datastore. Offer easy-to-integrate APIs for our dashboard, enabling stakeholders to track specific titles effectively. \u201cTime Travel\u201d to validate ahead of time.\n\nObservability stack high level architecture diagram\n\nIn the following sections, we will explore each of these concepts and components as illustrated in the diagram above.\n\nKey Features\n\nProactive monitoring through scheduled collectors jobs\n\nOur Title Health microservice runs a scheduled collector job every 30 minutes for most of our personalization stack.\n\nFor each Netflix row we support (such as Trending Now, Coming Soon, etc.), there is a dedicated collector. These collectors retrieve the relevant list of titles from our catalog that qualify for a specific row by interfacing with our catalog services. These services are informed about the expected subset of titles for each row, for which we are assessing title health.\n\nOnce a collector retrieves its list of candidate titles, it orchestrates batched calls to assigned row services using the above standardized schema to retrieve all the relevant health information of the titles. Additionally, some collectors will instead poll our kafka queue for impressions data.\n\nReal-time Title Impressions and Kafka Queue\n\nIn addition to evaluating title health via our personalization stack services, we also keep an eye on how our recommendation algorithms treat titles by reviewing impressions data. It\u2019s essential that our algorithms treat all titles equitably, for each one has limitless potential.\n\nThis data is processed from a real-time impressions stream into a Kafka queue, which our title health system regularly polls. Specialized collectors access the Kafka queue every two minutes to retrieve impressions data. This data is then aggregated in minute(s) intervals, calculating the number of impressions titles receive in near-real-time, and presented as an additional health status indicator for stakeholders.\n\nData storage and distribution through Hollow Feeds\n\nNetflix Hollow is an Open Source java library and toolset for disseminating in-memory datasets from a single producer to many consumers for high performance read-only access. Given the shape of our data, hollow feeds are an excellent strategy to distribute the data across our service boxes.\n\nOnce collectors gather health data from partner services in the personalization stack or from our impressions stream, this data is stored in a dedicated Hollow feed for each collector. Hollow offers numerous features that help us monitor the overall health of a Netflix row, including ensuring there are no large-scale issues across a feed publish. It also allows us to track the history of each title by maintaining a per-title data history, calculate differences between previous and current data versions, and roll back to earlier versions if a problematic data change is detected.\n\nObservability Dashboard using Health Check Engine\n\nWe maintain several dashboards that utilize our title health service to present the status of titles to stakeholders. These user interfaces access an endpoint in our service, enabling them to request the current status of a title across all supported rows. This endpoint efficiently reads from all available Hollow Feeds to obtain the current status, thanks to Hollow\u2019s in-memory capabilities. The results are returned in a standardized format, ensuring easy support for future UIs.\n\nAdditionally, we have other endpoints that can summarize the health of a title across subsets of sections to highlight specific member experiences.\n\nMessage depicting a dashboard request.\n\nTime Traveling: Catching before launch\n\nTitles launching at Netflix go through several phases of pre-promotion before ultimately launching on our platform. For each of these phases, the first several hours of promotion are critical for the reach and effective personalization of a title, especially once the title has launched. Thus, to prevent issues as titles go through the launch lifecycle, our observability system needs to be capable of simulating traffic ahead of time so that relevant teams can catch and fix issues before they impact members. We call this capability \u201cTime Travel\u201d.\n\nMany of the metadata and assets involved in title setup have specific timelines for when they become available to members. To determine if a title will be viewable at the start of an experience, we must simulate a request to a partner service as if it were from a future time when those specific metadata or assets are available. This is achieved by including a future timestamp in our request to the observability endpoint, corresponding to when the title is expected to appear for a given experience. The endpoint then communicates with any further downstream services using the context of that future timestamp.\n\nAn example request with a future timestamp.\n\nConclusion\n\nThroughout this series, we\u2019ve explored the journey of enhancing title launch observability at Netflix. In Part 1, we identified the challenges of managing vast content launches and the need for scalable solutions to ensure each title\u2019s success. Part 2 highlighted the strategic approach to navigating ambiguity, introducing \u201cTitle Health\u201d as a framework to align teams and prioritize core issues. In this final part, we detailed the sophisticated system strategies and architecture, including observability endpoints, proactive monitoring, and \u201cTime Travel\u201d capabilities; all designed to ensure a thrilling viewing experience.\n\nBy investing in these innovative solutions, we enhance the discoverability and success of each title, fostering trust with content creators and partners. This journey not only bolsters our operational capabilities but also lays the groundwork for future innovations, ensuring that every story reaches its intended audience and that every member enjoys their favorite titles on Netflix.\n\nThank you for joining us on this exploration, and stay tuned for more insights and innovations as we continue to entertain the world.", "label": 0}
{"title": "Deploying a Simple Go/Gin Application on Heroku", "url": "https://www.heroku.com/blog/deploying-simple-go-gin-application-on-heroku/", "content": "The speed and efficiency of the Go programming language make it popular for backend development. Combine Go with the Gin framework\u2014which offers a fast and minimalistic approach to building web applications\u2014and developers can easily create high-performance APIs and web services. Whether you\u2019re working on a personal project or building a production-ready application, Go and Gin make for an attractive stack perfectly suited for lightweight, scalable web development.\n\nCreating a Go/Gin application might seem straightforward: You write a few routes, connect a database, and spin up a local server. But when it comes to deploying your app, things can get tricky. Developers unfamiliar with cloud deployment often struggle with configuring environment variables, managing dependencies, and ensuring their app runs smoothly on a hosting platform.\n\nFortunately, Heroku makes this process incredibly simple. With its streamlined deployment workflow and built-in support for Go, Heroku lets you deploy your Go/Gin app with minimal configuration.\n\nIn this article, we\u2019ll walk through the process of building and deploying a Go/Gin web application on Heroku. We\u2019ll set up a local development environment, prepare an application for deployment, and deploy it to run on Heroku. Along the way, we\u2019ll cover best practices and troubleshooting tips to ensure a smooth deployment.\n\nBy the end of this guide, you\u2019ll have a fully functional Go/Gin application running on Heroku\u2014and you\u2019ll gain the knowledge needed to deploy future projects with confidence. Let\u2019s get started!\n\nSetting up your development environment\n\nTo get started, you must set up your development environment. Here are the steps to install what you need and test your application locally.\n\nAn example project can be found in this GitHub repository.\n\nDownload and install Go\n\nDownload the Go installer from the official Go website, making sure you choose the correct operating system. For Windows or Linux, follow the respective installation instructions on the website.\n\nIf you\u2019re on macOS, you can use Homebrew:\n\n$ brew install go\n\nOnce installed, verify your installation by running:\n\n$ go version\n\nYou should see your Go version printed in the terminal. For this guide, we\u2019re running version 1.24.0 .\n\nSet up your workspace\n\nCreate a new directory for your project and initialize a Go module. Open your terminal and execute:\n\n~/project$ go mod init github.com/YOUR-USERNAME/YOUR-REPO-NAME\n\nThis neatly organizes your project and its dependencies, ensuring everything is in order. In the examples to follow YOUR-REPO-NAME will be go-gin .\n\nAdd the Gin framework\n\nNow it\u2019s time to invite Gin to the party. Gin is a high-performance web framework that will help you build your REST server fast.\n\nRun the following command to add Gin to your project:\n\n~/project$ go get github.com/gin-gonic/gin\n\nThis fetches the Gin package and its dependencies.\n\nThe server application code goes in a file called main.go. Download that code here.\n\nFinally, run the server:\n\n~/project$ go run main.go\n\nTest your application locally\n\nBefore declaring your quest a success, make sure your application runs smoothly on your local machine. As you run the server as described above, you\u2019ll see output indicating that it\u2019s up and running.\n\n~/project$ go run main.go [GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached. [GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production. - using env: export GIN_MODE=release - using code: gin.SetMode(gin.ReleaseMode) [GIN-debug] GET /quotes --> main.main.func1 (3 handlers) [GIN-debug] GET /quote --> main.main.func2 (3 handlers) [GIN-debug] POST /quote --> main.main.func3 (3 handlers) [GIN-debug] [WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value. Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details. [GIN-debug] Listening and serving HTTP on :8080\n\nTest your API server endpoints by sending a curl request in a separate terminal window. For example:\n\n$ curl -s -X GET http://localhost:8080/quote | jq { \"quote\": \"The journey of a thousand miles begins with a single step.\" }\n\n(We use jq to pretty-print the JSON result.)\n\nCreating a Heroku App\n\nAssuming you have installed the Heroku CLI, you can create a new Heroku app. Run the following commands:\n\n~/project$ heroku login ~/project$ heroku apps:create my-go-gin-api Creating \u2b22 my-go-gin-api... done https://my-go-gin-api-7f40e19ce771.herokuapp.com/ | https://git.heroku.com/my-go-gin-api.git\n\nThis creates your Heroku app, accessible at the app URL (in the above example, that\u2019s https://my-go-gin-api-7f40e19ce771.herokuapp.com/ ). The command also creates a Git remote so you can push your code repo to Heroku with a single command.\n\n~/project$ git remote show heroku * remote heroku Fetch URL: https://git.heroku.com/my-go-gin-api.git Push URL: https://git.heroku.com/my-go-gin-api.git\n\nYou\u2019ll also see your newly created app in your Heroku Dashboard. Clicking the Open app button will take you to your app URL.\n\nCreate the Procfile\n\nThe Procfile tells Heroku how to run your application. In your project\u2019s root directory, create a file named Procfile (without any extension). For most simple Go applications, your Procfile will consist of a single line, like this:\n\nweb: go run main.go\n\nThis tells Heroku that your app will be a web process, and Heroku should start the process by running the command go run main.go . Simple enough! Add the file to your repository.\n\nTidy up your Go project\n\nFinally, use the following command to clean up your go.mod file to ensure all dependencies are properly listed:\n\n$ go mod tidy\n\nEnsure your go.mod and go.sum files have also been added to your repository. This allows Heroku to automatically download and manage dependencies during deployment with the Procfile.\n\nDeploying your application\n\nAfter completing these simple preparation steps, you\u2019re ready to push your project to Heroku. Commit your changes. Then, push them to your Heroku remote.\n\n~/project$ git add . ~/project$ git commit -m \"Prepare app for Heroku deployment\" ~/project$ git push heroku main\n\nThe git push command will set off a flurry of activity in your terminal, as Heroku begins building your application in preparation to run it:\n\n\u2026 Writing objects: 100% (26/26), 9.52 KiB | 9.52 MiB/s, done. \u2026 remote: Building source: remote: remote: -----> Building on the Heroku-24 stack remote: -----> Determining which buildpack to use for this app remote: -----> Go app detected remote: -----> Fetching jq... done remote: -----> Fetching stdlib.sh.v8... done remote: -----> remote: Detected go modules via go.mod remote: -----> remote: Detected Module Name: github.com/your-username/go-gin remote: -----> remote: -----> New Go Version, clearing old cache remote: -----> Installing go1.24.0 remote: -----> Fetching go1.24.0.linux-amd64.tar.gz... done remote: -----> Determining packages to install remote: go: downloading github.com/gin-gonic/gin v1.10.0 \u2026 remote: remote: Installed the following binaries: remote: ./bin/go-gin remote: -----> Discovering process types remote: Procfile declares types -> web remote: remote: -----> Compressing... remote: Done: 6.6M remote: -----> Launching... remote: Released v3 remote: https://my-go-gin-api-7f40e19ce771.herokuapp.com/ deployed to Heroku remote: remote: Verifying deploy... done. To https://git.heroku.com/my-go-gin-api.git * [new branch] main -> main\n\nThis output tells you the location of the binary that Heroku built during the deploy process. In this case, it is at: ./bin/go-gin . For some Go applications, if you have trouble reaching your service and see errors in the logs, you might need to edit your Procfile to have Heroku run the binary directly, rather than using go directly with the source file. For example, your modified Procfile might look like this:\n\nweb: ./bin/go-gin\n\nTest the live application\n\nWith your Go application running on Heroku, you can test it by sending a curl request to your Heroku app URL. For example:\n\n$ curl -s \\ -X GET https://my-go-gin-api-7f40e19ce771.herokuapp.com/quote | jq { \"quote\": \"This too shall pass.\" }\n\nTo ensure everything is running smoothly after deployment, you can the following command to tail the server\u2019s live logs:\n\n~/project$ heroku logs --tail \u2026 2025-02-25T15:11:01.922123+00:00 heroku[web.1]: State changed from starting to up 2025-02-25T15:11:22.000000+00:00 app[api]: Build succeeded 2025-02-25T15:16:31.411009+00:00 app[web.1]: [GIN] 2025/02/25 - 15:16:31 | 200 | 29.199\u00b5s | 174.17.39.113 | GET \"/quote\" 2025-02-25T15:16:31.411487+00:00 heroku[router]: at=info method=GET path=\"/quote\" host=my-go-gin-api-7f40e19ce771.herokuapp.com request_id=7df071ec-9841-499f-b584-61574920e9df fwd=\"174.17.39.113\" dyno=web.1 connect=0ms service=0ms status=200 bytes=186 protocol=https\n\nIt\u2019s time for you to Go!\n\nIn this article, we walked through each step of creating your Go application that uses the Gin framework, from project setup to Heroku deployment. You can see the power and simplicity of combining Gin\u2019s robust routing capabilities with Heroku\u2019s flexible, cloud-based platform. On top of this, it\u2019s easy to scale your applications as your needs evolve.\n\nExplore the additional features both Heroku and Gin offer. Heroku\u2019s extensive add-on ecosystem can boost your application\u2019s functionality. You can also tap into advanced Gin middleware to optimize performance and strengthen security. To learn more, check out the following resources:", "label": 0}
{"title": "Introducing Configurable Metaflow", "url": "https://netflixtechblog.com/introducing-configurable-metaflow-d2fb8e9ba1c6?source=collection_home---4------7-----------------------", "content": "Introducing Configurable Metaflow Netflix Technology Blog 13 min read \u00b7 Dec 20, 2024 -- 4 Listen Share\n\nDavid J. Berg*, David Casler^, Romain Cledat*, Qian Huang*, Rui Lin*, Nissan Pow*, Nurcan Sonmez*, Shashank Srikanth*, Chaoying Wang*, Regina Wang*, Darin Yu*\n\n*: Model Development Team, Machine Learning Platform\n\n^: Content Demand Modeling Team\n\nA month ago at QConSF, we showcased how Netflix utilizes Metaflow to power a diverse set of ML and AI use cases, managing thousands of unique Metaflow flows. This followed a previous blog on the same topic. Many of these projects are under constant development by dedicated teams with their own business goals and development best practices, such as the system that supports our content decision makers, or the system that ranks which language subtitles are most valuable for a specific piece of content.\n\nAs a central ML and AI platform team, our role is to empower our partner teams with tools that maximize their productivity and effectiveness, while adapting to their specific needs (not the other way around). This has been a guiding design principle with Metaflow since its inception.\n\nMetaflow infrastructure stack\n\nStanding on the shoulders of our extensive cloud infrastructure, Metaflow facilitates easy access to data, compute, and production-grade workflow orchestration, as well as built-in best practices for common concerns such as collaboration, versioning, dependency management, and observability, which teams use to setup ML/AI experiments and systems that work for them. As a result, Metaflow users at Netflix have been able to run millions of experiments over the past few years without wasting time on low-level concerns.\n\nA long standing FAQ: configurable flows\n\nWhile Metaflow aims to be un-opinionated about some of the upper levels of the stack, some teams within Netflix have developed their own opinionated tooling. As part of Metaflow\u2019s adaptation to their specific needs, we constantly try to understand what has been developed and, more importantly, what gaps these solutions are filling.\n\nIn some cases, we determine that the gap being addressed is very team specific, or too opinionated at too high a level in the stack, and we therefore decide to not develop it within Metaflow. In other cases, however, we realize that we can develop an underlying construct that aids in filling that gap. Note that even in that case, we do not always aim to completely fill the gap and instead focus on extracting a more general lower level concept that can be leveraged by that particular user but also by others. One such recurring pattern we noticed at Netflix is the need to deploy sets of closely related flows, often as part of a larger pipeline involving table creations, ETLs, and deployment jobs. Frequently, practitioners want to experiment with variants of these flows, testing new data, new parameterizations, or new algorithms, while keeping the overall structure of the flow or flows intact.\n\nA natural solution is to make flows configurable using configuration files, so variants can be defined without changing the code. Thus far, there hasn\u2019t been a built-in solution for configuring flows, so teams have built their bespoke solutions leveraging Metaflow\u2019s JSON-typed Parameters, IncludeFile, and deploy-time Parameters or deploying their own home-grown solution (often with great pain). However, none of these solutions make it easy to configure all aspects of the flow\u2019s behavior, decorators in particular.\n\nRequests for a feature like Metaflow Config\n\nOutside Netflix, we have seen similar frequently asked questions on the Metaflow community Slack as shown in the user quotes above:\n\nhow can I adjust the @resource requirements, such as CPU or memory, without having to hardcode the values in my flows?\n\nhow to adjust the triggering @schedule programmatically, so our production and staging deployments can run at different cadences?\n\nNew in Metaflow: Configs!\n\nToday, to answer the FAQ, we introduce a new \u2014 small but mighty \u2014 feature in Metaflow: a Config object. Configs complement the existing Metaflow constructs of artifacts and Parameters, by allowing you to configure all aspects of the flow, decorators in particular, prior to any run starting. At the end of the day, artifacts, Parameters and Configs are all stored as artifacts by Metaflow but they differ in when they are persisted as shown in the diagram below:\n\nDifferent data artifacts in Metaflow\n\nSaid another way:\n\nAn artifact is resolved and persisted to the datastore at the end of each task.\n\nis resolved and persisted to the datastore at the end of each task. A parameter is resolved and persisted at the start of a run; it can therefore be modified up to that point. One common use case is to use triggers to pass values to a run right before executing. Parameters can only be used within your step code.\n\nis resolved and persisted at the start of a run; it can therefore be modified up to that point. One common use case is to use triggers to pass values to a run right before executing. Parameters can only be used within your step code. A config is resolved and persisted when the flow is deployed. When using a scheduler such as Argo Workflows, deployment happens when create\u2019ing the flow. In the case of a local run, \u201cdeployment\u201d happens just prior to the execution of the run \u2014 think of \u201cdeployment\u201d as gathering all that is needed to run the flow. Unlike parameters, configs can be used more widely in your flow code, particularly, they can be used in step or flow level decorators as well as to set defaults for parameters. Configs can of course also be used within your flow.\n\nAs an example, you can specify a Config that reads a pleasantly human-readable configuration file, formatted as TOML. The Config specifies a triggering \u2018@schedule\u2019 and \u2018@resource\u2019 requirements, as well as application-specific parameters for this specific deployment:\n\n[schedule]\n\ncron = \"0 * * * *\"\n\n\n\n[model]\n\noptimizer = \"adam\"\n\nlearning_rate = 0.5\n\n\n\n[resources]\n\ncpu = 1\n\nUsing the newly released Metaflow 2.13, you can configure a flow with a Config like above, as demonstrated by this flow:\n\nimport pprint\n\nfrom metaflow import FlowSpec, step, Config, resources, config_expr, schedule\n\n\n\n@schedule(cron=config_expr(\"config.schedule.cron\"))\n\nclass ConfigurableFlow(FlowSpec):\n\nconfig = Config(\"config\", default=\"myconfig.toml\", parser=\"tomllib.loads\")\n\n\n\n@resources(cpu=config.resources.cpu)\n\n@step\n\ndef start(self):\n\nprint(\"Config loaded:\")\n\npprint.pp(self.config)\n\nself.next(self.end)\n\n\n\n@step\n\ndef end(self):\n\npass\n\n\n\nif __name__ == \"__main__\":\n\nConfigurableFlow()\n\nThere is a lot going on in the code above, a few highlights:\n\nyou can refer to configs before they have been defined using \u2018config_expr\u2019.\n\nyou can define arbitrary parsers \u2014 using a string means the parser doesn\u2019t even have to be present remotely!\n\nFrom the developer\u2019s point of view, Configs behave like dictionary-like artifacts. For convenience, they support the dot-syntax (when possible) for accessing keys, making it easy to access values in a nested configuration. You can also unpack the whole Config (or a subtree of it) with Python\u2019s standard dictionary unpacking syntax, \u2018**config\u2019. The standard dictionary subscript notation is also available.\n\nSince Configs turn into dictionary artifacts, they get versioned and persisted automatically as artifacts. You can access Configs of any past runs easily through the Client API. As a result, your data, models, code, Parameters, Configs, and execution environments are all stored as a consistent bundle \u2014 neatly organized in Metaflow namespaces \u2014 paving the way for easily reproducible, consistent, low-boilerplate, and now easily configurable experiments and robust production deployments.\n\nMore than a humble config file\n\nWhile you can get far by accompanying your flow with a simple config file (stored in your favorite format, thanks to user-definable parsers), Configs unlock a number of advanced use cases. Consider these examples from the updated documentation:\n\nA major benefit of Config over previous more hacky solutions for configuring flows is that they work seamlessly with other features of Metaflow: you can run steps remotely and deploy flows to production, even when relying on custom parsers, without having to worry about packaging Configs or parsers manually or keeping Configs consistent across tasks. Configs also work with the Runner and Deployer.\n\nThe Hollywood principle: don\u2019t call us, we\u2019ll call you\n\nWhen used in conjunction with a configuration manager like Hydra, Configs enable a pattern that is highly relevant for ML and AI use cases: orchestrating experiments over multiple configurations or sweeping over parameter spaces. While Metaflow has always supported sweeping over parameter grids easily using foreaches, it hasn\u2019t been easily possible to alter the flow itself, e.g. to change @resources or @pypi/@conda dependencies for every experiment.\n\nIn a typical case, you trigger a Metaflow flow that consumes a configuration file, changing how a run behaves. With Hydra, you can invert the control: it is Hydra that decides what gets run based on a configuration file. Thanks to Metaflow\u2019s new Runner and Deployer APIs, you can create a Hydra app that operates Metaflow programmatically \u2014 for instance, to deploy and execute hundreds of variants of a flow in a large-scale experiment.\n\nTake a look at two interesting examples of this pattern in the documentation. As a teaser, this video shows Hydra orchestrating deployment of tens of Metaflow flows, each of which benchmarks PyTorch using a varying number of CPU cores and tensor sizes, updating a visualization of the results in real-time as the experiment progresses:\n\nExample using Hydra with Metaflow\n\nMetaboosting Metaflow \u2014 based on a true story\n\nTo give a motivating example of what configurations look like at Netflix in practice, let\u2019s consider Metaboost, an internal Netflix CLI tool that helps ML practitioners manage, develop and execute their cross-platform projects, somewhat similar to the open-source Hydra discussed above but with specific integrations to the Netflix ecosystem. Metaboost is an example of an opinionated framework developed by a team already using Metaflow. In fact, a part of the inspiration for introducing Configs in Metaflow came from this very use case.\n\nMetaboost serves as a single interface to three different internal platforms at Netflix that manage ETL/Workflows (Maestro), Machine Learning Pipelines (Metaflow) and Data Warehouse Tables (Kragle). In this context, having a single configuration system to manage a ML project holistically gives users increased project coherence and decreased project risk.\n\nConfiguration in Metaboost\n\nEase of configuration and templatizing are core values of Metaboost. Templatizing in Metaboost is achieved through the concept of bindings, wherein we can bind a Metaflow pipeline to an arbitrary label, and then create a corresponding bespoke configuration for that label. The binding-connected configuration is then merged into a global set of configurations containing such information as GIT repository, branch, etc. Binding a Metaflow, will also signal to Metaboost that it should instantiate the Metaflow flow once per binding into our orchestration cluster.\n\nImagine a ML practitioner on the Netflix Content ML team, sourcing features from hundreds of columns in our data warehouse, and creating a multitude of models against a growing suite of metrics. When a brand new content metric comes along, with Metaboost, the first version of the metric\u2019s predictive model can easily be created by simply swapping the target column against which the model is trained.\n\nSubsequent versions of the model will result from experimenting with hyper parameters, tweaking feature engineering, or conducting feature diets. Metaboost\u2019s bindings, and their integration with Metaflow Configs, can be leveraged to scale the number of experiments as fast as a scientist can create experiment based configurations.\n\nScaling experiments with Metaboost bindings \u2014 backed by Metaflow Config\n\nConsider a Metaboost ML project named `demo` that creates and loads data to custom tables (ETL managed by Maestro), and then trains a simple model on this data (ML Pipeline managed by Metaflow). The project structure of this repository might look like the following:\n\n\u251c\u2500\u2500 metaflows\n\n\u2502 \u251c\u2500\u2500 custom -> custom python code, used by\n\n| | | Metaflow\n\n\u2502 \u2502 \u251c\u2500\u2500 data.py\n\n\u2502 \u2502 \u2514\u2500\u2500 model.py\n\n\u2502 \u2514\u2500\u2500 training.py -> defines our Metaflow pipeline\n\n\u251c\u2500\u2500 schemas\n\n\u2502 \u251c\u2500\u2500 demo_features_f.tbl.yaml -> table DDL, stores our ETL\n\n| | output, Metaflow input\n\n\u2502 \u2514\u2500\u2500 demo_predictions_f.tbl.yaml -> table DDL,\n\n| stores our Metaflow output\n\n\u251c\u2500\u2500 settings\n\n\u2502 \u251c\u2500\u2500 settings.configuration.EXP_01.yaml -> defines the additive\n\n| | config for Experiment 1\n\n\u2502 \u251c\u2500\u2500 settings.configuration.EXP_02.yaml -> defines the additive\n\n| | config for Experiment 2\n\n\u2502 \u251c\u2500\u2500 settings.configuration.yaml -> defines our global\n\n| | configuration\n\n\u2502 \u2514\u2500\u2500 settings.environment.yaml -> defines parameters based on\n\n| git branch (e.g. READ_DB)\n\n\u251c\u2500\u2500 tests\n\n\u251c\u2500\u2500 workflows\n\n\u2502 \u251c\u2500\u2500 sql\n\n\u2502 \u251c\u2500\u2500 demo.demo_features_f.sch.yaml -> Maestro workflow, defines ETL\n\n\u2502 \u2514\u2500\u2500 demo.main.sch.yaml -> Maestro workflow, orchestrates\n\n| ETLs and Metaflow\n\n\u2514\u2500\u2500 metaboost.yaml -> defines our project for\n\nMetaboost\n\nThe configuration files in the settings directory above contain the following YAML files:\n\n# settings.configuration.yaml (global configuration)\n\nmodel:\n\nfit_intercept: True\n\nconda:\n\nnumpy: '1.22.4'\n\n\"scikit-learn\": '1.4.0'\n\n# settings.configuration.EXP_01.yaml\n\ntarget_column: metricA\n\nfeatures:\n\n- runtime\n\n- content_type\n\n- top_billed_talent\n\n# settings.configuration.EXP_02.yaml\n\ntarget_column: metricA\n\nfeatures:\n\n- runtime\n\n- director\n\n- box_office\n\nMetaboost will merge each experiment configuration (*.EXP*.yaml) into the global configuration (settings.configuration.yaml) individually at Metaboost command initialization. Let\u2019s take a look at how Metaboost combines these configurations with a Metaboost command:\n\n(venv-demo) ~/projects/metaboost-demo [branch=demoX]\n\n$ metaboost metaflow settings show --yaml-path=configuration\n\n\n\nbinding=EXP_01:\n\nmodel: -> defined in setting.configuration.yaml (global)\n\nfit_intercept: true\n\nconda: -> defined in setting.configuration.yaml (global)\n\nnumpy: 1.22.4\n\n\"scikit-learn\": 1.4.0\n\ntarget_column: metricA -> defined in setting.configuration.EXP_01.yaml\n\nfeatures: -> defined in setting.configuration.EXP_01.yaml\n\n- runtime\n\n- content_type\n\n- top_billed_talent\n\n\n\nbinding=EXP_02:\n\nmodel: -> defined in setting.configuration.yaml (global)\n\nfit_intercept: true\n\nconda: -> defined in setting.configuration.yaml (global)\n\nnumpy: 1.22.4\n\n\"scikit-learn\": 1.4.0\n\ntarget_column: metricA -> defined in setting.configuration.EXP_02.yaml\n\nfeatures: -> defined in setting.configuration.EXP_02.yaml\n\n- runtime\n\n- director\n\n- box_office\n\nMetaboost understands it should deploy/run two independent instances of training.py \u2014 one for the EXP_01 binding and one for the EXP_02 binding. You can also see that Metaboost is aware that the tables and ETL workflows are not bound, and should only be deployed once. These details of which artifacts to bind and which to leave unbound are encoded in the project\u2019s top-level metaboost.yaml file.\n\n(venv-demo) ~/projects/metaboost-demo [branch=demoX]\n\n$ metaboost project list\n\n\n\nTables (metaboost table list):\n\nschemas/demo_predictions_f.tbl.yaml (binding=default):\n\ntable_path=prodhive/demo_db/demo_predictions_f\n\nschemas/demo_features_f.tbl.yaml (binding=default):\n\ntable_path=prodhive/demo_db/demo_features_f\n\n\n\nWorkflows (metaboost workflow list):\n\nworkflows/demo.demo_features_f.sch.yaml (binding=default):\n\ncluster=sandbox, workflow.id=demo.branch_demox.demo_features_f\n\nworkflows/demo.main.sch.yaml (binding=default):\n\ncluster=sandbox, workflow.id=demo.branch_demox.main\n\n\n\nMetaflows (metaboost metaflow list):\n\nmetaflows/training.py (binding=EXP_01): -> EXP_01 instance of training.py\n\ncluster=sandbox, workflow.id=demo.branch_demox.EXP_01.training\n\nmetaflows/training.py (binding=EXP_02): -> EXP_02 instance of training.py\n\ncluster=sandbox, workflow.id=demo.branch_demox.EXP_02.training\n\nBelow is a simple Metaflow pipeline that fetches data, executes feature engineering, and trains a LinearRegression model. The work to integrate Metaboost Settings into a user\u2019s Metaflow pipeline (implemented using Metaflow Configs) is as easy as adding a single mix-in to the FlowSpec definition:\n\nfrom metaflow import FlowSpec, Parameter, conda_base, step\n\nfrom custom.data import feature_engineer, get_data\n\nfrom metaflow.metaboost import MetaboostSettings\n\n\n\n@conda_base(\n\nlibraries=MetaboostSettings.get_deploy_time_settings(\"configuration.conda\")\n\n)\n\nclass DemoTraining(FlowSpec, MetaboostSettings):\n\nprediction_date = Parameter(\"prediction_date\", type=int, default=-1)\n\n\n\n@step\n\ndef start(self):\n\n# get show_settings() for free with the mixin\n\n# and get convenient debugging info\n\nself.show_settings(exclude_patterns=[\"artifact*\", \"system*\"])\n\n\n\nself.next(self.get_features)\n\n\n\n@step\n\ndef get_features(self):\n\n# feature engineers on our extracted data\n\nself.fe_df = feature_engineer(\n\n# loads data from our ETL pipeline\n\ndata=get_data(prediction_date=self.prediction_date),\n\nfeatures=self.settings.configuration.features +\n\n[self.settings.configuration.target_column]\n\n)\n\n\n\nself.next(self.train)\n\n\n\n@step\n\ndef train(self):\n\nfrom sklearn.linear_model import LinearRegression\n\n\n\n# trains our model\n\nself.model = LinearRegression(\n\nfit_intercept=self.settings.configuration.model.fit_intercept\n\n).fit(\n\nX=self.fe_df[self.settings.configuration.features],\n\ny=self.fe_df[self.settings.configuration.target_column]\n\n)\n\nprint(f\"Fit slope: {self.model.coef_[0]}\")\n\nprint(f\"Fit intercept: {self.model.intercept_}\")\n\n\n\nself.next(self.end)\n\n\n\n@step\n\ndef end(self):\n\npass\n\n\n\n\n\nif __name__ == \"__main__\":\n\nDemoTraining()\n\nThe Metaflow Config is added to the FlowSpec by mixing in the MetaboostSettings class. Referencing a configuration value is as easy as using the dot syntax to drill into whichever parameter you\u2019d like.\n\nFinally let\u2019s take a look at the output from our sample Metaflow above. We execute experiment EXP_01 with\n\nmetaboost metaflow run --binding=EXP_01\n\nwhich upon execution will merge the configurations into a single settings file (shown previously) and serialize it as a yaml file to the .metaboost/settings/compiled/ directory.\n\nYou can see the actual command and args that were sub-processed in the Metaboost Execution section below. Please note the \u2013config argument pointing to the serialized yaml file, and then subsequently accessible via self.settings. Also note the convenient printing of configuration values to stdout during the start step using a mixed in function named show_settings().\n\n(venv-demo) ~/projects/metaboost-demo [branch=demoX]\n\n$ metaboost metaflow run --binding=EXP_01\n\n\n\nMetaboost Execution:\n\n- python3.10 /root/repos/cdm-metaboost-irl/metaflows/training.py\n\n--no-pylint --package-suffixes=.py --environment=conda\n\n--config settings\n\n.metaboost/settings/compiled/settings.branch_demox.EXP_01.training.mP4eIStG.yaml\n\nrun --prediction_date20241006\n\n\n\nMetaflow 2.12.39+nflxfastdata(2.13.5);nflx(2.13.5);metaboost(0.0.27)\n\nexecuting DemoTraining for user:dcasler\n\nValidating your flow...\n\nThe graph looks good!\n\nBootstrapping Conda environment... (this could take a few minutes)\n\nAll packages already cached in s3.\n\nAll environments already cached in s3.\n\n\n\nWorkflow starting (run-id 50), see it in the UI at\n\nhttps://metaflowui.prod.netflix.net/DemoTraining/50\n\n\n\n[50/start/251640833] Task is starting.\n\n[50/start/251640833] Configuration Values:\n\n[50/start/251640833] settings.configuration.conda.numpy = 1.22.4\n\n[50/start/251640833] settings.configuration.features.0 = runtime\n\n[50/start/251640833] settings.configuration.features.1 = content_type\n\n[50/start/251640833] settings.configuration.features.2 = top_billed_talent\n\n[50/start/251640833] settings.configuration.model.fit_intercept = True\n\n[50/start/251640833] settings.configuration.target_column = metricA\n\n[50/start/251640833] settings.environment.READ_DATABASE = data_warehouse_prod\n\n[50/start/251640833] settings.environment.TARGET_DATABASE = demo_dev\n\n[50/start/251640833] Task finished successfully.\n\n\n\n[50/get_features/251640840] Task is starting.\n\n[50/get_features/251640840] Task finished successfully.\n\n\n\n[50/train/251640854] Task is starting.\n\n[50/train/251640854] Fit slope: 0.4702672504331096\n\n[50/train/251640854] Fit intercept: -6.247919678070083\n\n[50/train/251640854] Task finished successfully.\n\n\n\n[50/end/251640868] Task is starting.\n\n[50/end/251640868] Task finished successfully.\n\n\n\nDone! See the run in the UI at\n\nhttps://metaflowui.prod.netflix.net/DemoTraining/50\n\nTakeaways\n\nMetaboost is an integration tool that aims to ease the project development, management and execution burden of ML projects at Netflix. It employs a configuration system that combines git based parameters, global configurations and arbitrarily bound configuration files for use during execution against internal Netflix platforms.\n\nIntegrating this configuration system with the new Config in Metaflow is incredibly simple (by design), only requiring users to add a mix-in class to their FlowSpec \u2014 similar to this example in Metaflow documentation \u2014 and then reference the configuration values in steps or decorators. The example above templatizes a training Metaflow for the sake of experimentation, but users could just as easily use bindings/configs to templatize their flows across target metrics, business initiatives or any other arbitrary lines of work.\n\nTry it at home\n\nIt couldn\u2019t be easier to get started with Configs! Just\n\npip install -U metaflow\n\nto get the latest version and head to the updated documentation for examples. If you are impatient, you can find and execute all config-related examples in this repository as well.\n\nIf you have any questions or feedback about Config (or other Metaflow features), you can reach out to us at the Metaflow community Slack.\n\nAcknowledgments\n\nWe would like to thank Outerbounds for their collaboration on this feature; for rigorously testing it and developing a repository of examples to showcase some of the possibilities offered by this feature.", "label": 0}
